Avec la croissance sans précédent des publications sur les plateformes de micro-blogging, trouver du contenu intéressant pour un utilisateur est devenu un enjeu majeur.
Cependant, en raison des propriétés intrinsèques des plateformes de micro–blogging, comme le flux gigantesque de messages arrivant tous les jours et leur faible durée de vie, il est difficile d'appliquer les méthodes traditionnelles de recommandation comme la factorisation matricielle.
Après une étude approfondie d'un large jeu de donnée issu de Twitter, nous présentons un modèle de propagation qui repose sur l'homophilie présente dans le réseau pour recommander des messages aux utilisateurs.
Notre approche s'appuie sur la construction d'un graphe de similarités lié aux interactions des utilisateurs.
Nous présentons plusieurs expérimentations pour démontrer la qualité de prédiction de notre modèle et sa capacité à passer à l'échelle.
Il semble que, de façon contre intuitive, dans la majorité des cas les systèmes de recommandation ouvrent les perspectives des utilisateurs.
Cependant, une minorité de personnes est concerné par l'effet de bulle et nous proposons donc un modèle reposant sur les liens entre communautés pour adapter les recommandations afin d'être plus en accord avec leur profil communautaire.
La reconnaissance de motifs est une tâche cruciale pour les êtres vivants, exécutée avec efficacité par le cerveau.
Les réseaux de neurones profonds artificiels reproduisent de mieux en mieux ces performances, avec des applications telles que la reconnaissance d'images ou le traitement du langage.
Ils nécessitent cependant un apprentissage intensif sur de grands jeux de données et couteux en calculs.
Les réseaux de neurones à impulsions, plus proches du fonctionnement du cerveau avec des neurones émettant des impulsions et des lois d' apprentissage dites STDP dépendant du temps entre deux impulsions, constituent une alternative intéressante.
Ils permettent un apprentissage non supervisé et ont déjà été utilisés pour la reconnaissance visuelle ou auditive, mais les applications restent limitées par rapport à l'apprentissage profond classique.
Il est d'autant plus intéressant de développer de nouvelles applications pour ces réseaux qu'ils peuvent être implémentés sur des circuits neuromorphiques connaissant aujourd'hui des développements importants, notamment avec les composants analogiques « memristifs » qui miment la plasticité synaptique.
Ici, nous avons choisi de développer un réseau STDP pour un problème crucial en neuroscience : le spike-sorting.
Les implants cérébraux composés de matrices de microélectrode permettent d'enregistrer l'activité individuelle de multiples neurones, prenant la forme de pics de potentiel dans le signal, appelés potentiels d'action.
Une même électrode enregistre l'activité de plusieurs neurones.
Le spike-sorting a pour but de détecter et trier cette activité, en utilisant le fait que la forme d'un potentiel d'action dépend du neurone qui l'a émis.
Les méthodes classiques de spike-sorting consistent en trois étapes : la détection des potentiels d'action, l'extraction de traits caractéristiques de leurs formes, et le tri de ces caractéristiques en groupes correspondant alors aux différentes cellules neurales.
Bien que les méthodes onlines existent, les méthodes les plus répandues nécessitent un traitement offline, qui n'est pas compatible avec les applications temps réelles telles que les interfaces cerveau-machine (BCI).
Utiliser un réseau STDP apporte une nouvelle méthode pour répondre à ces besoins.
Le réseau que nous avons conçu prend en entrée le signal de l'électrode et produit en sortie un train d'impulsions qui correspond à l'activité des cellules enregistrées.
Il est organisé en différentes couches, connectées en série, chacune effectuant une étape du traitement.
La première couche, constituée de neurones senseurs, convertit le signal d'entrée en train d'impulsions.
Les couches suivantes apprennent les motifs générés par la couche précédente grâce aux lois STDP.
Chaque couche est améliorée par l'implémentation de différents mécanismes, tels que le STDP avec ressources, l'adaptation de seuil, la plasticité déclenchée par l'inhibition, ou un modèle de neurone déchargeant par rebond.
Un mécanisme d'attention permet au réseau de ne traiter que les parties du signal contenant des potentiels d'action.
Ce réseau a été conçu dans un premier temps pour traiter des données mono-électrode, puis adapté pour traiter des signaux provenant d'électrodes multiples.
Il a été testé d'abord sur des données simulées qui permettent de comparer la sortie du réseau à la vérité, puis sur des enregistrements réels de microélectrodes associés à des enregistrements intracellulaires donnant une vérité partielle.
Les différentes versions du réseau ont été ainsi évaluées et comparées à d'autres algorithmes, donnant des résultats très satisfaisants.
Suite à ces résultats simulés sur ordinateur, nous avons travaillé à une implémentation FPGA, constituant une première étape vers une implémentation embarquée neuromorphique.
La présente étude s'inscrit dans le cadre d'un essai de traitement automatique de la langue malgache visant à construire un dictionnaire électronique.
Notre travail est articulé autour de deux axes principaux : une étude descriptive de la langue d'une part et une modélisation informatique d'autre part.
Après un chapitre préliminaire consacré à une présentation générale, nous présentons dans la première partie une analyse des termes malgaches portant tant sur les formes de la structure morphologique ou morphématique que sur leur comportement syntaxique.
Nous décrivons les différentes voix des énoncés malgaches et les modalités : le temps, l'aspect et le mode.
Dans la deuxième partie, nous formalisons les données linguistiques dégagées dans la partie descriptive pour en faire une implantation informatique d'un système d'Analyseur-Générateur des Termes prédicatifs Malgaches (AGTM).
Nous terminons cette partie pratique par le bilan de notre étude et les perspectives d'avenir.
La thèse est complétée par des annexes qui rassemblent le module de programmation et les résultats obtenus.
Ce travail porte sur le rôle de la polysémie dans la structuration du lexique.
La thèse propose de faire une évaluation qualitative de la polysémie, en la comparant aux autres relations qui structurent le lexique.
Cette entreprise doit permettre de vérifier que les liens de polysémie ne doivent pas être modélisés indépendamment des liens de dérivation ou de conversion.
Les résultats de l'évaluation montrent que la frontière entre polysémie et conversion est poreuse.
Les comparaisons entre relations utilisent les propriétés de l'analogie, bien adaptée pour caractériser les rapports entre relations.
Ce sont les liens qui connectent les lexies qui font l'objet d'une comparaison.
Un lien de polysémie est ce qui connecte deux lexies en relation de polysémie.
Ce lien peut être comparé à un lien qui connecte deux autres lexies en relation de conversion.
La langue d'étude est le wolof, langue atlantique d'Afrique de l'ouest.
Cette langue est un terrain propice à une telle recherche.
La modélisation de la langue naturelle est l¿un des défis fondamentaux de l¿intelligence artificielle et de la conception de systèmes interactifs, avec applications dans les systèmes de dialogue, la génération de texte et la traduction automatique.
Nous proposons un modèle log-linéaire discriminatif donnant la distribution des mots qui suivent un contexte donné.
Le résultat est un modèle efficace qui capte suffisamment les dépendances longues sans occasionner une forte augmentation des ressources en espace ou en temps.
Dans un modèle log-linéaire, les phases d¿apprentissage et de tests deviennent de plus en plus chères avec un nombre croissant de classes.
Le nombre de classes dans un modèle de langue est la taille du vocabulaire, qui est généralement très importante.
Une astuce courante consiste à appliquer le modèle en deux étapes : la première étape identifie le cluster le plus probable et la seconde prend le mot le plus probable du cluster choisi.
Cette idée peut être généralisée à une hiérarchie de plus grande profondeur avec plusieurs niveaux de regroupement.
Cependant, la performance du système de classification hiérarchique qui en résulte dépend du domaine d¿application et de la construction d¿une bonne hiérarchie.
Nous étudions différentes stratégies pour construire la hiérarchie des catégories de leurs observations.
Cette thèse développe un formalisme théorique pour la sémantique du discours.
Il s'appuie sur l'extension des grammaires de Montague, sur la notion de continuation et sur les mécanismes de levée et de traitement des exceptions.
Le formalisme permet de traiter des phénomènes dynamiques tels que les anaphores d'une phrase à l'autre, les présuppositions déclenchées par des référents et les projections présuppositions.
Partant du postulat que ce parti politique français est un parti désormais ancré dans le système mais qui, paradoxalement, se revendique comme un parti « antisystème » , nous étudions la mise en discours de cette opposition.
Pour réaliser cette recherche, nous avons créé, sur le logiciel Hyperbase, une plateforme d'analyse de données textuelles, un vaste corpus de plus de 3 millions d'occurrences structurées en 5 bases de données.
Plus de 300 discours lepéniens et trois campagnes présidentielles
Pour répondre à notre interrogation fondamentale, trois champs de pertinence – correspondant chacun à une approche linguistique – se succèdent et progressent d'une analyse infra-textuelle, centrée sur l'étude des spécificités lexicales et syntaxiques du discours FN, vers une analyse textuelle consacrée à la cohésion inter- et supra-phrastique de la textualité lepénienne, pour aboutir à l'organisation discursive et aux relations que pose le discours du FN localement et globalement aux autres discours.
La présente thèse a pour objet l'analyse automatique des formes lexicales dans les textes écrits en lituanien, sur la base d'une heuristique forme - valeur qui s'inscrit dans une approche symbolique du traitement automatique des langues.
Cette étude accorde une attention spécifique à l'éxploitation optimale des indices formels et s'appuie principalement sur deux domaines de la linguistique, la graphématique et la morphologie.
Le point d'entrée formaliste couplé à l'objectif d'automatisation a réclamé une révision de la perspective grammaticale traditionnelle, qui nous a conduit à esquisser un renouvellement de la description relative à plusieurs aspects du système linguistique, notamment les parties du discours, la structure lexicale et la suffixation.
Le modèle linguistique, qui reste à développer, a servi de fondement à la réalisation d'un analyseur de formes lexicales nommé ALeksas.
Ce logiciel possède une structure hybride principalement basée sur des automates à nombre fini d'états.
ALeksas, qui est encore à l'état expérimental, assure l'analyse grammaticale des mots formes selon une approche indépendante d'une base de données lexicale permettant de formuler des hypothèses d'interprétation sur des critères formels.
Le prototype a fait l'objet d'une mise à l'épreuve par confrontation à un corpus de textes authentiques variés, afin d'évaluer ses capacités, notamment par rapport aux outils comparables, et de mieux cerner les améliorations nécessaires.
Historiquement, les Modèles Graphiques Probabilistes (PGMs) sont une solution d'apprentissage à partir des données incertaines et plates, appelées aussi données propositionnelles ou représentations attribut-valeur.
Au début des années 2000, un grand intérêt a été adressé au traitement des données relationnelles présentant un grand nombre d'objets participant à des différentes relations.
Les Modèles Probabilistes Relationnels (PRMs) présentent une extension des PGMs pour le contexte relationnel.
Avec l'évolution rapide issue de l'internet, des innovations technologiques et des applications web, les données sont devenues de plus en plus variées et complexes.
D'où l'essor du Big Data.
Toutefois, tous les travaux d'apprentissage des PRMs sont consacrés à apprendre à partir des données bien structurées et stockées dans des bases de données relationnelles.
Les bases de données graphe sont non structurées et n'obéissent pas à un schéma bien défini.
Les arcs entre les noeuds peuvent avoir des différentes signatures.
En effet, les relations qui ne correspondent pas à un modèle ER peuvent exister dans l'instance de base de données.
Ces relations sont considérées comme des exceptions.
Dans ce travail de thèse, nous nous intéressons à ce type de bases de données.
Nous étudions aussi deux types de PRMs à savoir, Direct Acyclic Probabilistic Entity Relationship (DAPER) et chaines de markov logiques (MLNs).
Nous proposons deux contributions majeures.
Premièrement, Une approche d'apprentissage des DAPERs à partir des bases de données graphe partiellement structurées.
Une deuxième approche consiste à exploiter la logique de premier ordre pour apprendre les DAPERs en utilisant les MLNs pour prendre en considération les exceptions qui peuvent parvenir lors de l'apprentissage.
Nous menons une étude expérimentale permettant de comparer nos méthodes proposées avec les approches déjà existantes.
Cette thèse est composée de trois études distinctes qui examinent empiriquement le rôle des informations divulguées sur la dépréciation de la survaleur pour les principales parties prenantes de l'entreprise (c'est-à-dire les analystes financiers, les entreprises homologues et les auditeurs externes).
Dans la première étude, j'examine l'effet de la transparence de la divulgation sur le désaccord entre les analystes, et le désaccord entre les analystes et les gestionnaires, dans le contexte de la dépréciation du goodwill.
La deuxième étude examine si la déclaration d'une dépréciation significative du goodwill par une entreprise (entreprise dépréciée) affecte le comportement d'investissement d'autres entreprises du même secteur (entreprises homologues).
La troisième étude examine l'impact de la divulgation élargie des rapports d'audit sur les décisions des entreprises en matière de divulgation financière.
Plus précisément, j'examine si les entreprises adaptent les niveaux de divulgation sur la dépréciation des écarts d'acquisition lorsque les auditeurs signalent la dépréciation des écarts d'acquisition comme un risque d'inexactitudes importantes dans le rapport d'audit élargi.
Cette étude contribue au débat sur l'utilité du rapport d'audit élargi en identifiant le mécanisme par lequel le rapport d'audit élargi a un impact sur l'information financière et les décisions des entreprises.
La dysarthrie est un trouble de la parole affectant la réalisation motrice de la parole causée par des lésions du système nerveux central ou périphérique.
Elle peut être liée à différentes pathologies : la maladie de Parkinson, la Sclérose Latérale Amyotrophique (SLA), un Accident Vasculaire Cérébral (AVC), etc.
Plusieurs travaux de recherche ont porté sur la caractérisation des altérations liées à chaque pathologie afin de les regrouper dans des classes de dysarthrie.
La classification la plus répandue est celle établie par F. L. Darley comportant 6 classes en 1969, (complétée par deux classes supplémentaires en 2005).
Actuellement, l'évaluation perceptive (à l'oreille) reste le standard utilisé dans la pratique clinique pour le diagnostique et le suivi thérapeutique des patients.
Cette approche est néanmoins reconnue comme étant subjective, non reproductible et coûteuse en temps.
Ces limites la rendent inadaptée à l'évaluation de larges corpus (dans le cadre d'études phonétiques par exemple) ou pour le suivi longitudinal de l'évolution des patients dysarthriques.
Le travail présenté dans ce rapport s'inscrit dans ce cadre et étudie l'apport que peuvent avoir ces outils dans l'évaluation de la parole dysarthrique, et plus généralement pathologique.
Dans ce travail, une approche pour la détection automatique des phonèmes anormaux dans la parole dysarthrique est proposée et son comportement est analysé sur différents corpus comportant différentes pathologies, classes dysarthriques, niveaux des évérité de la maladie et styles de parole.
Contrairement à la majorité des approches proposées dans la littérature permettant des évaluations de la qualité globale de la parole(évaluation de la sévérité, intelligibilité, etc.), l'approche proposée se focalise sur le niveau phonème dans le but d'atteindre une meilleure caractérisation de la dysarthrieet de permettre un feed-back plus précis et utile pour l'utilisateur (clinicien, phonéticien,patient).
L'approche s'articule autours de deux phases essentielles : (1) une première phase d'alignement automatique de la parole au niveau phonème (2) une classification de ces phonèmes en deux classes : phonèmes normaux et anormaux.
L'évaluation de l'annotation réalisée par le système par rapport à une évaluation perceptive d'un expert humain considérée comme "référence" montre des résultats très encourageants et confirme la capacité de l'approche à detecter les anomalies au niveau phonème.
L'approche s'est aussi révélée capable de capter l'évolution de la sévérité de la dysarthrie suggérant une potentielle application lors du suivi longitudinal des patients ou pour la prédiction automatique de la sévérité de leur dysarthrie.
Aussi, l'analyse du comportement de l'outil d'alignement automatique de la parole face à la parole dysarthrique a révélé des comportements dépendants des pathologies et des classes dysarthriques ainsi que des différences entre les catégories phonétiques.
De plus, un effet important du style de parole (parole lue et spontanée) a été constaté sur les comportements de l'outil d'alignement de la parole et de l'approche de détection automatique d'anomalies.
Finalement, les résultats d'une campagne d'évaluation de l'approche de détection d'anomalies par un jury d'experts sont présentés et discutés permettant une mise en avant des points forts et des limites du système.
Les bases de données médico-administratives sont des bases de données de santé particulièrement exhaustives.
De nouveaux modèles de processus et un algorithme de process mining adapté sont présentés, modélisant les transitions et leurs temporalités.
Une solution de prétraitement des journaux d'événements est également proposée, permettant une représentation des évènements complexes caractérisés par de multiples codes appartenant à différents systèmes de codage, organisés en structures hiérarchiques.
Cette méthode de clustering par auto-encodage permet de regrouper dans l'espace latent les événements similaires et produit automatiquement des labels pertinents pour le process mining, explicables médicalement.
Un premier algorithme de prédiction adapté aux parcours est alors proposé, produisant via une procédure d'optimisation un modèle de processus utilisé pour classifier les parcours directement à partir des données de journaux d'événements.
Ce modèle de processus sert également de support pour expliquer les patterns de parcours distinctifs entre deux populations.
Une seconde méthode de prédiction est présentée, avec un focus particulier sur les événements médicaux récurrents.
Avec l'essor du Big Data, le traitement du Volume, de la Vélocité (croissance et évolution) et de la Variété de la donnée concentre les efforts des différentes communautés pour exploiter ces nouvelles ressources.
Ces nouvelles ressources sont devenues si importantes, que celles-ci sont considérées comme le nouvel « or noir » .
Au cours des dernières années, le volume et la vélocité sont des aspects de la donnée qui sont maitrisés contrairement à la variété qui elle reste un défi majeur.
Cette thèse présente deux contributions dans le domaine de mise en correspondance de données hétérogènes, avec un focus sur la dimensions spatiale.
La première contribution repose sur un processus de mise en correspondance de données textuelles hétérogènes divisé en deux étapes : la géoreprésentation et le géomatching.
Dans la première phase, nous proposons de représenter la dimension spatiale de chaque document d'un corpus à travers une structure dédiée, la Spatial Textual Representation (STR).
Cette représentation de type graphe est composée des entités spatiales identifiées dans le document, ainsi que les relations spatiales qu'elles entretiennent.
Pour identifier les entités spatiales d'un document et leurs relations spatiales, nous proposons une ressource dédiée, nommée Geodict.
La seconde phase, le géomatching, consiste à mesurer la similarité entre les représentations générées (STR).
Pour évaluer la pertinence d'une correspondance, nous proposons un ensemble de 6 critères s'appuyant sur une définition de la similarité spatiale entre deux documents.
La seconde contribution repose sur la dimension thématique des données textuelles et sa participation dans le processus de mise en correspondance spatiale.
Nous proposons d'identifier les thèmes apparaissant dans la même fenêtre contextuelle que certaines entités spatiales.
L'objectif est d'induire certaines des similarités spatiales implicites entre les documents.
Pour cela, nous proposons d'étendre la structure de la STR à l'aide de deux concepts : l'entité thématique et de la relation thématique.
L'entité thématique représente un concept propre à un domaine particulier (agronome, médical) et représenté selon différentes orthographes présentes dans une ressource terminologique, ici un vocabulaire.
Une relation thématique lie une entité spatiale à une entité thématique si celles-ci apparaissent dans une même fenêtre contextuelle.
Les vocabulaires choisis ainsi que la nouvelle forme de la STR intégrant la dimension thématique sont évalués selon leur couverture sur les corpus étudiés, ainsi que leurs contributions dans le processus de mise en correspondance spatiale.
Cette thèse explore l'utilisation de fonctions de perte structurées dans deux domaines distincts.
Dans la première contribution, nous nous intéressons à l'apprentissage par renforcement multi-agent, dans le contexte d'environnements qui peuvent être séparés en plusieurs tâches faiblement dépendantes.
On s'attache à trouver des politiques qui se généralisent à plus d'agents et de tâches que les scénarios d'entraînement, permettant ainsi d'augmenter la taille des problèmes qui peuvent être approchés.
Notre solution affecte les agents aux tâches en résolvant un problème d'optimisation centralisé dont la fonction objectif est paramétrée par un réseau de neurones.
On montre que l'expressivité du problème d'optimisation et celle du réseau de neurones influencent la capacité du modèle à généraliser, et qu'avec les bons choix, la politique peut généraliser à plus de 5 fois plus d'agents que pendant l'entraînement.
Dans la seconde contribution, nous formulons la détection d'objets comme un problème de prédiction d'ensemble, et nous concevons un modèle dans cette optique.
Notre solution utilise un réseau convolutionel profond, comme souvent en vision par ordinateur, et un encodeur-décodeur de Transformer, une architecture qui a récemment permis d'importants progrès en traitement du langage.
Remarquablement, notre solution n'incorpore que peu de biais inductif, et ne nécessite donc pas de composants spécifiques à la détection d'objets, tels que les ancres de détection.
Avec un nombre de paramètres comparable, notre modèle égale la performance de modèles de référence, tels que Retinanet et Faster R-CNN sur le dataset de détection COCO.
Pour finir, nous montrons que la méthode peut naturellement être étendue à la segmentation panoptique, où elle surpasse les approches concurrentes, démontrant ainsi sa généralité.
La tâche de segmentation et de regroupement en locuteur (SRL) consiste à déterminer le nombre de locuteurs ainsi que leurs interventions dans un document audio.
Cette tâche intéresse de nombreuses entreprises qui souhaitent indexer leurs contenus audiovisuels.
En particulier, l'institut national de l'audiovisuel (INA) désire appliquer cette tâche sur ses archives afin d'en améliorer l'accessibilité mais également l'annotation.
Cependant, les usages de l'institut requièrent une qualité minimum qui n'est, la plupart du temps, pas encore atteinte par les systèmes automatiques de SRL à l'état de l'art.
Pour atteindre les performances voulues, un humain peut corriger la sortie d'un système de SRL.
Néanmoins, une intervention humaine est généralement chronophage et coûteuse.
Afin de réduire ces coûts, une solution possible est d'utiliser un système assisté par l'humain : un humain donne des informations à un système afin qu'il améliore ses prédictions pour faire décroître son coût de correction.
Le présent manuscrit s'articule autour de la SRL assistée par l'humain.
Il propose une mesure afin d'évaluer le coût d'intervention humain pour corriger une SRL, un protocole pour évaluer les interactions d'un humain pour la SRL, un automate simulant les corrections humaines à faire pour une SRL et des systèmes de SRL assistés réduisant le coût d'intervention humain total.
Plus précisément, les systèmes de SRL assistés présentés réévaluent soit uniquement le regroupement en locuteurs, soit la segmentation et le regroupement en locuteurs.
Elles ont donné lieu à de nombreuses études en Traitement Automatique du Langage Naturel.
En effet, leur étude et leur identification précise sont primordiales, sur les plans théorique et applicatif.
Cependant, la majorité des travaux de recherche sur le sujet portent sur des usages de langage quotidien : dialogues " à bâtons rompus ", demandes d'horaire, discours, etc.
Mais qu'en est-il des productions orales spontanées produites dans un cadre contraint ?
Aucune étude n'a à notre connaissance été menée dans ce contexte.
Or, on sait que l'utilisation d'une " langue de spécialité " dans le cadre d'une tâche donnée entraîne des comportements spécifiques.
Notre travail de thèse est consacré à l'étude linguistique et informatique des disfluences dans un tel cadre.
Il s'agit de dialogues de contrôle de trafic aérien, aux contraintes pragmatiques et linguistiques.
Nous effectuons une étude exhaustive des phénomènes de disfluences dans ce contexte.
Dans un premier temps nous procédons à l'analyse fine de ces phénomènes.
Ensuite, nous les modélisons à un niveau de représentation abstrait, ce qui nous permet d'obtenir les patrons correspondant aux différentes configurations observées.
Enfin nous proposons une méthodologie de traitement automatique.
Celle-ci consiste en plusieurs algorithmes pour identifier les différents phénomènes, même en l'absence de marqueurs explicites.
Elle est intégrée dans un système de traitement automatique de la parole.
Enfin, la méthodologie est validée sur un corpus de 400 énoncés.
Conduite dans le but de rendre les robots comme socio-communicatifs, les chercheurs ont cherché à mettre au point des robots dotés de compétences sociales et de « bon sens » pour les rendre acceptables.
Cette intelligence sociale ou « sens commun » du robot est ce qui finit par déterminer son acceptabilité sociale à long terme.
Cependant, ce n'est pas commun.
Les robots peuvent donc seulement apprendre à être acceptables avec l'expérience.
Cependant, en enseignant à un humanoïde, les subtilités d'une interaction sociale ne sont pas évidentes.
Même un échange de dialogue standard intègre le panel le plus large possible de signes qui interviennent dans la communication et sont difficiles à codifier (synchronisation entre l'expression du corps, le visage, le ton de la voix, etc.).
Dans un tel scénario, l'apprentissage du modèle comportemental du robot est une approche prometteuse.
Cet apprentissage peut être réalisé avec l'aide de techniques d'IA.
Cette étude tente de résoudre le problème de l'apprentissage des modèles comportementaux du robot dans le paradigme automatisé de planification et d'ordonnancement (APS) de l'IA.
Dans le domaine de la planification automatisée et de l'ordonnancement (APS), les agents intelligents nécessitent un modèle d'action (plans d'actions dont les exécutions entrelacées effectuent des transitions de l'état système) afin de planifier et résoudre des problèmes réels.
Au cours de cette thèse, nous présentons deux nouveaux systèmes d'apprentissage qui facilitent l'apprentissage des modèles d'action et élargissent la portée de ces nouveaux systèmes pour apprendre les modèles de comportement du robot.
Ces techniques peuvent être classées dans les catégories non optimale et optimale.
Les techniques non optimales sont plus classiques dans le domaine, ont été traitées depuis des années et sont de nature symbolique.
Cependant, ils ont leur part de quirks, ce qui entraîne un taux d'apprentissage moins élevé que souhaité.
Les techniques optimales sont basées sur les progrès récents dans l'apprentissage en profondeur, en particulier la famille à long terme (LSTM) de réseaux récurrents récurrents.
Ces techniques sont de plus en plus séduisantes par la vertu et produisent également des taux d'apprentissage plus élevés.
Cette étude met en vedette ces deux techniques susmentionnées qui sont testées sur des repères d'IA pour évaluer leurs prouesses.
Ils sont ensuite appliqués aux traces HRI pour estimer la qualité du modèle de comportement du robot savant.
Ceci est dans l'intérêt d'un objectif à long terme d'introduire l'autonomie comportementale dans les robots, afin qu'ils puissent communiquer de manière autonome avec les humains sans avoir besoin d'une intervention de « magicien » .
Un corpus aussi vaste et riche que celui représenté par le conte de tradition orale de la province de Valladolid justifie la création d'un outil lexicostatistique.
L'outil informatique permet la visualisation de données précises et exhaustives et la mise en évidence des différentes unités lexicales exploitées au sein des contes extraits d'une part de l'oeuvre de Joaquin DIAZ et d'autre part de l'oeuvre de Aurelio M. ESPINOSA (hijo).
Dans un premier temps cette étude s'attache à présenter le contexte géographique et terminologique.
Le travail de recherche consiste ensuite en la création d'une base de données lexicales -fruit du dépouillement de l'intégralité du corpus de référence- laquelle représente une vaste banque terminologique répertoriée en différents champs lexicosémantiques, en la comptabilisation et la représentation graphique des données obtenues et en l'interprétation des résultats enregistrés selon une approche sociolinguistique et socioculturelle.
A terme, ce travail permet la réalisation d'une classification des récits favorisée par une nouvelle méthode de gestion textuelle.
Ces travaux s'intéressent à la modélisation formelle de la sémantique des langues naturelles.
Pour cela, nous suivons le principe de compositionnalité qui veut que le sens d'une expression complexe soit une fonction du sens de ses parties.
Ces fonctions sont généralement formalisées à l'aide du [lambda]-calcul.
Cependant, ce principe est remis en cause par certains usages de la langue, comme les pronoms anaphoriques ou les présuppositions.
Ceci oblige à soit abandonner la compositionalité, soit modifier les structures du sens.
Dans le premier cas, le sens n'est alors plus obtenu par un calcul qui correspond à des fonctions mathématiques, mais par un calcul dépendant du contexte, ce qui le rapproche des langages de programmation qui manipulent leur contexte avec des effets de bord.
Dans le deuxième cas, lorsque les structures de sens sont ajustées, les nouveaux sens ont tendance à avoir une structure de monade.
Ces dernières sont elles-mêmes largement utilisées en programmation fonctionnelle pour coder des effets de bord, que nous retrouvons à nouveau.
Par ailleurs, s'il est souvent possible de proposer le traitement d'un unique phénomène, composer plusieurs traitements s'avère être une tâche complexe.
Nos travaux proposent d'utiliser les résultats récents autour des langages de programmation pour parvenir à combiner ces modélisations par les effets de bord.
Dans la première partie de la thèse, nous démontrons les propriétés fondamentales de ce calcul (préservation de type, confluence et terminaison).
Dans la seconde partie, nous montrons comment utiliser le calcul pour le traitement de plusieurs phénomènes linguistiques : deixis, quantification, implicature conventionnelle, anaphore et présupposition.
Enfin, nous construisons une unique grammaire qui gère ces phénomènes et leurs interactions.
Les plateformes en ligne telles que les blogs et les réseaux sociaux permettent aux internautes de s'exprimer sur des sujets d'une grande variété (produits commerciaux, politique, services, etc.).
Cet important volume de données d'opinions peut être exploré et exploité grâce à des techniques de fouille de texte connues sous le nom de fouille d'opinions ou analyse de sentiments.
Contrairement à la majorité des travaux actuels en fouille d'opinions, qui se focalisent sur les opinions simplement positives ou négatives (ou un intermédiaire entre ces deux extrêmes), nous nous intéressons dans cette thèse aux points de vue.
La fouille de point de vue généralise l'opinion au delà de son acception usuelle liée à la polarité (positive ou négative) et permet l'étude d'opinions exprimées plus subtilement, telles que les opinions politiques.
Dans notre première contribution, nous avons exploré l'idée de différencier mots d'opinions (spécifiques à la fois à un point de vue et à un thème) et mots thématiques (dépendants du thème mais neutres vis-à-vis des différents points de vue) en nous basant sur les parties de discours, inspirée par des pratiques similaires dans la littérature de fouille d'opinions classique - restreinte aux opinions positives et négatives.
Notre seconde contribution se focalise quant à elle sur les points de vue exprimés sur les réseaux sociaux.
Notre objectif est ici d'analyser dans quelle mesure l'utilisation des interactions entre utilisateurs, en outre de leur contenu textuel généré, est bénéfique à l'identification de leurs points de vue.
Nos différentes contributions ont été évaluées et comparées à l'état de l'art sur des collections de documents réels.
La reconnaissance des actions à partir de vidéos est l'un des principaux problèmes de vision par ordinateur.
Malgré des recherches intensives, la différenciation et la reconnaissance d'actions similaires restent un défi.
Cette thèse porte sur la classification des gestes sportifs à partir de vidéos, avec comme cadre applicatif le tennis de table.
Nous proposons une méthode d'apprentissage profond pour segmenter et classifier automatiquement les différents coup de Tennis de Table.
Les annotations consistent en une description des coups effectués (début, fin et type de coup).
Au total, 20 différents coups de tennis de table sont considérés plus une classe de rejet.
La reconnaissance des actions similaires présente des différences avec la reconnaissance d'actions classique.
En effet, dans les bases de données classiques, le contexte de l'arrière plan fournit souvent des informations discriminantes que les méthodes peuvent utiliser pour classer l'action plutôt que de se concentrer sur l'action elle-même.
Dans notre cas, la similarité entre classes est élevée, les caractéristiques visuelles discriminantes sont donc plus difficiles à extraire et le mouvement joue un rôle clef dans la caractérisation de l'action.
Dans cette thèse, nous introduisons un réseau de neurones spatio-temporel convolutif avec une architecture Jumelle.
Ce réseau d'apprentissage profond prend comme entrées une séquence d'images RVB et son flot optique estimé.
Les données RVB permettent à notre modèle de capturer les caractéristiques d'apparence tandis que le flot optique capture les caractéristiques de mouvement.
Ces deux flux sont traités en parallèle à l'aide de convolutions 3D, et sont fusionnés à la dernière étape du réseau.
Notre méthode obtient une performance de classification de 93.2% sur l'ensemble des données tests.
Appliquée à la tâche jointe de détection et de classification, notre méthode atteint une précision de 82.6%.Nous étudions les performances en fonction des types de données utilisés en entrée et la manière de les fusionner.
Différents estimateurs de flot optique ainsi que leur normalisation sont testés afin d'améliorer la précision.
Les caractéristiques de chaque branche de notre architecture sont également analysées afin de comprendre le chemin de décision de notre modèle.
Enfin, nous introduisons un mécanisme d'attention pour aider le modèle à se concentrer sur des caractéristiques discriminantes et aussi pour accélérer le processus d'entraînement.
Nous comparons notre modèle avec d'autres méthodes sur TTStroke-21 et le testons sur d'autres ensembles de données.
Nous constatons que les modèles fonctionnant bien sur des bases de données d'actions classiques ne fonctionnent pas toujours aussi bien sur notre base de données d'actions similaires.
Les travaux présentés dans cette thèse ont été validés par des publications dans une revue internationale, cinq papiers de conférences internationales, deux papiers d'un workshop international et une tâche reconductible dans le workshop MediaEval où les participants peuvent appliquer leurs méthodes de reconnaissance d'actions à notre base de données TTStroke-21.
Deux autres papiers de workshop internationaux sont en cours de préparation, ainsi qu'un chapitre de livre.
Avec notre thèse, nous voulons poser les fondements linguistiques et didactiques nécessaires à la future élaboration d'un programme ou d'une méthode en intercompréhension slave, en prenant l'exemple des langues slaves de l'ouest et du sud-ouest et en fournissant une analyse linguistique de trois langues : le tchèque, le slovène et le croate.
Dans notre travail, nous cherchons principalement à fournir deux éléments : - Une série d'hypothèses linguistiques ayant pour objectif de déterminer les points à enseigner dans une méthode d'intercompréhension concernant le tchèque, le slovène et le croate ; - Une présentation de programmes et de supports en didactique de l'intercompréhension réalisés et testés dans le cadre de notre cursus.
Dans notre travail, nous constatons que la didactique de l'intercompréhension slave diffère en de nombreux points avec les apprentissages classiques.
Dans le cas de l'intercompréhension, de nombreux points normalement lourds et complexes à maîtriser peuvent n'être que survolés.
Grâce à nos analyses, tant sur le plan linguistique que didactique, nous avons pu fournir une réflexion sur l'une des formes que pourra prendre une formation en intercompréhension slave dans le futur.
Nous préconisons particulièrement l'utilisation de ressources en ligne, via, par exemple, le site www.rozrazum.eu, développé dans le cadre de cette thèse afin de tester des activités respectant la méthodologie proposée par Eurom 5 (Bonvino et al. 2001).
Ce site pourra servir, dans un premier temps, de plate-forme de test et de mise au point d'approches didactiques, tout en étant fonctionnel, et donc disponible à un public d'apprenants.
L'intelligence épidémiologique a pour but de détecter, d'analyser et de surveiller au cours du temps les potentielles menaces sanitaires.
Ce processus de surveillance repose sur des sources dites formelles, tels que les organismes de santé officiels, et des sources dites informelles, comme les médias.
Cette thèse se concentre sur l'extraction et la combinaison d'informations épidémiologiques extraites d'articles de presse en ligne, dans le cadre de la veille des maladies infectieuses animales.
Dans ce manuscrit, nous proposons de nouvelles représentations textuelles fondées sur la sélection, l'expansion et la combinaison de descripteurs épidémiologiques.
Nous montrons que l'adaptation et l'extension de méthodes de fouille de texte et de classification permet d'améliorer l'utilisation des articles en ligne tant que source de données sanitaires.
Nous mettons en évidence le rôle de l'expertise quant à la pertinence et l'interprétabilité de certaines des approches proposées.
Bien que nos travaux soient menés dans le contexte de la surveillance de maladies en santé animale, nous discutons des aspects génériques des méthodes proposées, vis-à-vis de de maladies inconnues et dans un contexte
La traduction automatique des documents est considérée comme l'une des tâches les plus difficiles en traitement automatique des langues et de la parole.
Les particularités linguistiques de certaines langues, comme la langue arabe, rendent la tâche de traduction automatique plus difficile.
Notre objectif dans cette thèse est d'améliorer les systèmes de traduction de l'arabe vers le français et vers l'anglais.
Les principales recherches portent à la fois sur la construction de corpus parallèles, le prétraitement de l'arabe et sur l'adaptation des modèles de traduction et de langue.
Tout d'abord, un corpus comparable journalistique a été exploré pour en extraire automatiquement un corpus parallèle.
Ensuite, différentes approches d'adaptation du modèle de traduction sont exploitées, soit en utilisant le corpus parallèle extrait automatiquement soit en utilisant un corpus parallèle construit automatiquement.
Nous démontrons que l'adaptation des données du système de traduction permet d'améliorer la traduction.
Un texte en arabe doit être prétraité avant de le traduire et ceci à cause du caractère agglutinatif de la langue arabe.
Nous présentons notre outil de segmentation de l'arabe, SAPA (Segmentor and Part-of-speech tagger for Arabic), indépendant de toute ressource externe et permettant de réduire les temps de calcul.
Cet outil permet de prédire simultanément l'étiquette morpho-syntaxique ainsi que les proclitiques (conjonctions, prépositions, etc.) pour chaque mot, ensuite de séparer les proclitiques du lemme (ou mot de base).
Nous décrivons également dans cette thèse notre outil de détection des entités nommées, NERAr (Named Entity Recognition for Arabic), et nous examions l'impact de l'intégration de la détection des entités nommées dans la tâche de prétraitement et la pré-traduction de ces entités nommées en utilisant des dictionnaires bilingues.
Nous présentons par la suite plusieurs méthodes pour l'adaptation thématique des modèles de traduction et de langue expérimentées sur une application réelle contenant un corpus constitué d'un ensemble de phrases multicatégoriques.
Ces expériences ouvrent des perspectives importantes de recherche comme par exemple la combinaison de plusieurs systèmes lors de la traduction pour l'adaptation thématique.
Il serait également intéressant d'effectuer une adaptation temporelle des modèles de traduction et de langue.
Finalement, les systèmes de traduction améliorés arabe-français et arabe-anglais sont intégrés dans une plateforme d'analyse multimédia et montrent une amélioration des performances par rapport aux systèmes de traduction de base.
Cette recherche se concentre sur un modèle causal de certains antécédents et les conséquences des dimensions des relations BRQ (chaud vs froid) : un test empirique dans un contexte Vietnamien.
Les objectifs de cette recherche sont : premièrement, déterminer les effets de la personnalité de la marque sur les antécédents de deux composantes de la BRQ dans le contexte Vietnamien.
Deuxièmement, étudier l'impact des antécédents et les conséquences des deux composantes de BRQ sur l'intention d'achat de la marque dans le contexte Vietnamien.
Un modèle structurel a été développé illustrant la relation entre la personnalité de la marque (antidépresseur) et les conséquences d'une relation de qualité de marque (BRQ).
Cela a entraîné le développement de vingt hypothèses.
Pour répondre aux objectifs de cette recherche, des données ont été recueillies, axées sur six classes de produits et, au final, 634 questionnaires ont été recueillis.
Tout d'abord, les résultats de cette étude révèlent que la personnalité de la marque a une influence positive sur les deux variables de l'auto-congruence et de la qualité des partenaires, mais on voit clairement qu'il existe une différence entre le niveau d'influence et d'importance.
Deuxièmement, étant donné que l'auto-congruence a un effet plus important sur le chaud que froid BRQ, d'autre part, la qualité des partenaires a un effet plus important sur le froid que chaud BRQ.
Cependant, en fonction du coefficient de chemin de l'auto-congruence et de la qualité des partenaires, les résultats révèlent que l'auto-congruence a un effet positif sur les chauds et froids BRQ par rapport à la qualité des partenaires.
En ce qui concerne les résultats de la condamnation des deux composantes de BRQ, nous avons constaté que la taille de l'ensemble de considération et le bouche-à-oreille n'ont aucune relation avec l'intention de la marque, alors que WTP a un effet positif sur la marque.
Les principales contributions de cette recherche permettent de mieux comprendre le comportement des consommateurs sur le marché Vietnamien.
Cependant, le froid BRQ a considérablement influencé le bouche-à-oreille du consommateur.
Par conséquent, le chaud BRQ, qui est la qualité de la relation émotionnelle, augmente principalement le comportement de fidélité des clients.
En revanche, le froid BRQ aide à attirer de nouveaux clients' grâce à une communication positive de bouche-à-oreille des clients.
Le maintien des clients actuels et l'attrait des clients potentiels sont des moteurs essentiels pour la survie d'une marque ou d'un produit.
Les gestionnaires ont besoin, par conséquent, d'avoir une incidence positive sur le chaud et froid BRQ de leurs clients.
En outre, en fonction des résultats de la recherche, ils devraient se concentrer sur une volonté de payer un prix haut de gamme afin d'augmenter leur intention d'achat de marque.
Le génome bactérien est classiquement pensé comme constitué de “chromosomes”, éléments génomiques essentiels pour l'organisme, stables et à évolution lente, et de “plasmides”, éléments génomiques accessoires, mobiles et à évolution rapide.
La distinction entre plasmides et chromosomes a récemment été mise en défaut avec la découverte dans certaines lignées bactériennes d'éléments génomiques intermédiaires, possédant à la fois des caractéristiques de chromosomes et de plasmides.
Cependant, leur véritable nature et les mécanismes permettant leur intégration dans le génome stable reste à caractériser.
En utilisant les protéines liées aux Systèmes de Transmission de l'Information Génétique (STIG) comme variables descriptives des éléments génomiques bactériens (ou réplicons), une étude globale de génomique comparative a été conduite sur l'ensemble des génomes bactériens disponibles.
Les avancées récentes des Technologies de l'Information et de la Communication (TIC) ont entraîné des transformations radicales de plusieurs secteurs de l'industrie.
L'adoption des technologies du Web Sémantique a démontré plusieurs avantages, surtout dans une application de Recherche d'Information (RI) : une meilleure représentation des données et des capacités de raisonnement sur celles-ci.
Cependant, il existe encore peu d'applications industrielles car il reste encore des problèmes non résolus, tels que la représentation de documents hétérogènes interdépendants à travers des modèles de données sémantiques et la représentation des résultats de recherche accompagnés d'informations contextuelles.
Dans cette thèse, nous abordons deux défis principaux.
Le deuxième défi porte sur la construction des résultats de RI, à partir de ce corpus de documents hétérogènes, aidant les utilisateurs à mieux interpréter les informations pertinentes de leur recherche surtout quand il s'agit d'exploiter les relations inter/intra-documentaires.
Pour faire face à ces défis, nous proposons tout d'abord une représentation sémantique du corpus de documents hétérogènes à travers un modèle de graphe sémantique couvrant à la fois les dimensions structurelle et métier du corpus.
Ensuite, nous définissons une nouvelle structure de données pour les résultats de recherche, extraite à partir de ce graphe, qui incorpore les informations pertinentes directes ainsi qu'un contexte structurel et métier.
Afin d'exploiter cette nouvelle structure dans un modèle de RI novateur, nous proposons une chaine de traitement automatique de la requête de l'utilisateur, allant du module d'interprétation de requête, aux modules de recherche, de classement et de présentation des résultats.
Cependant, dans cette thèse, les expérimentations ont été appliquées au domaine du Bâtiment et Travaux Publics (BTP), en s'appuyant sur des projets de construction.
Cette thèse porte sur l'adaptation de la synthèse paramétrique de la parole à partir d'un texte écrit à la langue arabe.
Pour ce faire, différentes méthodes ont été développées afin de mettre en place des systèmes de synthèse.
Ces méthodes sont basées sur une description du signal de parole par un ensemble de paramètres acoustiques et prosodiques.
De même, chaque son est représenté par un ensemble de descripteurs contextuels contenant toutes les informations affectant la prononciation de celui-ci.
Une partie de ces descripteurs dépend de la langue et de ses particularités, ainsi, afin d'adapter l'approche de synthèse paramétrique à l'arabe, une étude des particularités phonologiques de l'arabe était nécessaire.
L'accent a été mis sur deux phénomènes : la gémination et la longueur des voyelles (courte/longue).
Deux descripteurs associés à ces deux phénomènes ont été ajoutés à l'ensemble des descripteurs contextuels.
Quatre combinaisons de modélisation sont possibles en alternant la différentiation ou la fusion des consonnes simples et géminées d'une part et des voyelles courtes et longues d'autres part.
Un ensemble des tests perceptifs et objectifs a été conduit afin d'évaluer l'effet des quatre approches de modélisation des unités sur la qualité de la parole synthétisée.
Les évaluations ont été faites dans le cas de synthèse paramétrique par HMM (Hidden Markov Model) puis dans le cas de la synthèse paramétrique par DNN.
Les résultats subjectifs sont montrés que dans le cas de l'approche par HMM, les quatre approches produisent des signaux de qualité similaire, une conclusion qui a été confirmée par les mesures objectives calculées pour évaluer la prédiction des durées des unités de parole.
Cependant, les résultats des évaluations objectives dans le cas de l'approche par DNN ont montré que la différentiation des consonnes simples (respectivement des voyelles courtes) des consonnes géminées (respectivement des voyelles longues) permet d'avoir une prédiction des durées légèrement meilleure qu'avec les autres des approches de modélisation.
Une dernière partie de la thèse a été consacrée à la comparaison de l'approche de synthèse par HMM à celle par DNN.
L'ensemble des tests conduits ont montré que l'utilisation des DNN a amélioré la qualité perçue des signaux générés.
Cette "mémoire syntaxique" se construit à partir de l'expérience et notamment de l'observation de séquences, suites d'objets dont l'organisation séquentielle obéit à des règles syntaxiques.
Elle doit pouvoir être utilisée ultérieurement pour générer des séquences valides, c'est-à-dire respectant ces règles.
Cette production de séquences valides peut se faire de façon explicite, c'est-à-dire en évoquant les règles sous-jacentes, ou de façon implicite, quand l'apprentissage a permis de capturer le principe d'organisation des séquences sans recours explicite aux règles.
Bien que plus rapide, plus robuste et moins couteux en termes de charge cognitive que le raisonnement explicite, le processus implicite a pour inconvénient de ne pas donner accès aux règles et de ce fait, de devenir moins flexible et moins explicable.
Au début, l'expert réalise un choix pour suivre explicitement les règles du métier.
Mais ensuite, à force de répétition, le choix se fait automatiquement, sans évocation explicite des règles sous-jacentes.
Ce changement d'encodage des règles chez un individu en général et particulièrement chez un expert métier peut se révéler problématique lorsqu'il faut expliquer ou transmettre ses connaissances.
Si les concepts métiers peuvent être formalisés, il en va en général de tout autre façon pour l'expertise.
Dans nos travaux, nous avons souhaité nous pencher sur les séquences de composants électriques et notamment la problématique d'extraction des règles cachées dans ces séquences, aspect important de l'extraction de l'expertise métier à partir des schémas techniques.
Nous nous plaçons dans le domaine connexionniste, et nous avons en particulier considéré des modèles neuronaux capables de traiter des séquences.
Nous avons évalué ces deux modèles sur différentes grammaires artificielles (grammaire de Reber et ses variations) au niveau de l'apprentissage, de leurs capacités de généralisation de celui-ci et leur gestion de dépendances séquentielles.
Finalement, nous avons aussi montré qu'il était possible d'extraire les règles encodées (issues des séquences) dans le réseau récurrent doté de LSTM, sous la forme d'automate.
Ces systèmes sont entraînés sur des gros corpus de données, que l'on nomme big data et qui sont devenus le nouveau pétrole du 21e siècle.
Les systèmes réussissent à capturer des éléments que l'humain n'est pas capable de mémoriser, en extrayant des régularités de ces grandes quantités de données.
Nous nous proposons donc, dans cette optique, d'étudier les liens entre apprentissage et performances des systèmes de traitement automatique de la parole en se concentrant sur la répartition homme/femme.
Le choix de la parole est motivé par la tendance actuelle faisant de la voix la nouvelle interface homme/machine.
Nous souhaitons donc proposer non pas des catégories, mais un continuum qui nous permettra de caractériser nos corpus non plus en terme de genre, mais en terme de diversité vocale.
Cette représentation permettra de mettre en évidence la variabilité vocale et de remettre en question la pertinence du genre en tant que distinction catégorielle, tout en permettant une étude socio-phonétique des rôles des locuteurs et des interactions.
Si le concept de décentralisation est en quelque sorte inscrit dans le principe même de l'Internet, son urbanisme actuel n'intègre ce principe que de manière limitée.
En cherchant les meilleures solutions, certains développeurs se retournent vers les qualités persistantes d'une ancienne technologie, le pair-à-pair (P2P), qui replonge dans la topologie de l'Internet pré-commercial, mettant à profit les ressources des « nains » du réseau – ses marges ou sa périphérie.
Cette thèse explore l'approche distribuée et décentralisée de l'architecture technique des services Internet.
Il s'agit de comprendre ce que dessine une architecture de réseau décentralisée du point de vue de l'articulation des acteurs et des contenus, de la répartition de responsabilités, de l'organisation du marché et de la capacité à exercer du contrôle, des formes d'existence et des rôles d'entités telles que les nœuds du réseau, ses usagers, ses unités centrales.
Ce travail analyse sous quelles conditions un réseau qui répartit à ses marges la responsabilité de son propre fonctionnement, et qui suit un modèle non hiérarchisé ou hybride, peut se développer dans l'Internet d'aujourd'hui.
La thèse suit les développeurs de trois services Internet – un moteur de recherche, un service de stockage et une application pour le streaming vidéo – construits sur un modèle de réseau décentralisé, les collectifs d'usagers pionniers qui se développent avec ces services, et ponctuellement, les arènes politiques où l'on discute de l'organisation et de la gouvernance de l'Internet à moyen et long terme.
Cette thèse contribue à l'étude de la fiabilité et de la sécurité-innocuité des systèmes informatisés, modélisés par des systèmes à événements discrets.
Les principales contributions concernent la théorie des Systèmes de Contrôle (notés C Systems) et l'approche par Monitoring des modèles.
Dans la première partie de la thèse, nous étudions la théorie des Systèmes de Contrôle qui combine et étend de façon significative, les systèmes de réécriture de la théorie des langages et le contrôle supervisé.
Un système de contrôle est une structure générique qui contient deux composants : le composant contrôlé et le composant contrôlant qui restreint le comportement du composant contrôlé.
Les deux composants sont exprimés en utilisant le même formalisme comme des automates ou des grammaires.
Nous considérons différentes classes de systèmes de contrôle basés sur différents formalismes comme, par exemple, les automates, les grammaires, ainsi que leurs versions infinies et concurrentes.
Ensuite, une application de cette théorie est présentée.
Les systèmes de contrôle basés sur les automates de Büchi sont utilisés pour vérifier par model-checking, des propriétés définissant la correction sur des traces d'exécution spécifiées par une assertion de type nevertrace.
Dans la seconde partie de la thèse, nous investiguons l'approche de monitoring des modèles dont la théorie des systèmes de contrôle constitue les fondations formelles.
Le principe pivot de cette approche est la « spécification de propriétés comme contrôleur » .
En d'autres termes, pour un système, les exigences fonctionnelles, d'une part, et des propriétés, d'autre part, sont modélisées et implantées séparément, les propriétés spécifiées contrôlant le comportement issu des exigences fonctionnelles.
De cette approche découle ainsi deux techniques alternatives, respectivement nommées monitoring de modèle et génération de modèle.
Cette approche peut être utilisée de diverses manières pour améliorer la fiabilité et la sécurité-innocuité de divers types de systèmes.
Nous présentons quelques applications qui montrent l'intérêt pratique de cette contribution théorique.
Tout d'abord, cette approche aide à prendre en compte les évolutions des spécifications des propriétés.
En second lieu, elle fournit une base théorique à la sécurité fonctionnelle, popularisée par la norme IEC 61508.
En troisième lieu, l'approche peut être utilisée pour formaliser et vérifier l'application de guides de bonnes pratiques ou des règles de modélisation appliquées par exemple pour des modèles UML.Ces résultats constituent les bases pour des études futures de dispositifs plus perfectionnés, et fournissent une nouvelle voie pour s'assurer de la fiabilité et de la sécurité-innocuité des systèmes
Les interactions requièrent une confiance mutuelle des parties impliquées.
Les entités d'Internet endossent plusieurs rôles, chacun ayant ses propres intérêts et motivations ; conduisant à des conflits qui doivent être adressés afin de permettre confiance et sécurité.
Dans cette thèse nous nous concentrons sur la dynamique de frappe au clavier afin de résoudre quelques de ces conflits.
La manière de taper au clavier est une modalité biométrique sans coûts et transparente, elle ne requiert ni capteurs ni actions additionnels.
Malheureusement, elle permet aussi le profilage des utilisateurs (s.a. identification, âge, sexe), contre leur consentement et connaissance.
Afin de protéger la vie privée des utilisateurs, nous proposons d'anonymiser la dynamique de frappe au clavier.
Cependant, cette information peut être légitimement requise afin de renforcer l'authentification des utilisateurs
Nous proposons ainsi un Code Personnel d'Identité Respectueux de la Vie Privée, permettant l'authentification biométrique des utilisateurs, sans menacer leur vie privée.
Nous proposons aussi une preuve sociale d'identité permettant de vérifier des déclarations d'identités ainsi que la génération synthétique de dynamique de frappe au clavier.
Notre travail de recherche est consacré à l'analyse, d'un point de vue aspectuel, du rôle de la morphologie dérivationnelle dans le processus de construction du sens aspectuel lexical.
Le préfixe RE- est très intéressant parce que sa structure processuelle est complexe.
Il implique une relation entre un procès présupposé et un procès posé et cette relation est établie par un troisième procès, intermédiaire, qui crée un lien de continuité, de reprise ou d'interruption.
L'activation de ces liens est notamment dépendante du sens de la base lexicale du dérivé et du sens aspectuel de cette même base.
L'un des aspects d'une interface homme-machine réussie (p. ex. interaction homme-robot, chatbots, parole, écriture manuscrite, etc.) est la possibilité d'avoir une interaction personnalisée.
Cela affecte l'expérience humaine globale et permet une interaction plus fluide.
Actuellement, il y a beaucoup de travaux qui utilisent l'apprentissage machine afin de modéliser de telles interactions.
Cependant, ces modèles n'abordent pas la question du comportement personnalisé : ils tentent de faire la moyenne des différents exemples provenant de différentes personnes.
L'identification des styles humains (persona) ouvre la possibilité de biaiser la sortie des modèles pour prendre en compte la préférence humaine.
Dans cette thèse, nous nous sommes concentrés sur le problème des styles dans le contexte de l'écriture manuscrite.
L'objectif de cette thèse est d'étudier ces problèmes de styles, dans le domaine de l'écriture.
Nous disposons d'un jeu de données IRONOFF, un jeu de données d'écriture manuscrite en ligne, avec 410 rédacteurs, avec ~25K exemples de dessins en majuscules, minuscules et chiffres.
Pour le problème de l'apprentissage par transfert, nous avons utilisé un jeu de données supplémentaire, QuickDraw !, un jeu de données de dessin d'esquisses contenant environ 50 millions de dessins sur 345 catégories.
Les principales contributions de ma thèse sont :
1) Proposer un pipeline de travail pour étudier le problème des styles d'écriture.
Il s'agit de proposer une méthodologie, des repères et des paramètres d'évaluation (et de fonder ces paramètres d'évaluation).
Nous choisissons le paradigme des modèles génératifs temporels dans l'apprentissage profond afin de générer des dessins et d'évaluer leur proximité/pertinence par rapport aux dessins de vérité voulus/de terrain.
Nous avons proposé deux métriques, pour évaluer la courbure et la longueur des dessins générés.
Afin d'enraciner ces métis, nous avons proposé de multiples repères - dont nous connaissons le pouvoir relatif à l'avance -, puis vérifié que les mesures respectent effectivement la relation de pouvoir relatif.
2) Proposer un cadre pour l'étude et l'extraction des styles, et vérifier son avantage par rapport aux repères proposés précédemment.
Nous nous sommes mis d'accord sur l'idée d'utiliser un auto-encodeur conditionné en profondeur pour résumer et extraire les informations de style, sans avoir besoin de nous concentrer sur l'identité de la tâche (puisqu'elle est donnée comme une condition).
Nous validons ce cadre par rapport au repère proposé précédemment à l'aide de nos paramètres d'évaluation.
Nous visualisons également les styles extraits, ce qui nous permet d'obtenir des résultats passionnants !
3) En utilisant le cadre proposé, proposer un moyen de transférer l'information sur les styles entre les différentes tâches, et un protocole afin d'évaluer la qualité du transfert.
Nous avons exploité le codeur automatique conditionné profond utilisé précédemment, en extrayant la partie codeur - qui, selon nous, contenait les informations pertinentes sur les styles - et en l'utilisant dans de nouveaux modèles formés sur de nouvelles tâches.
Nous testons intensivement ce paradigme sur une gamme différente de tâches, à la fois sur les ensembles de données IRONOFF et QuickDraw !.
Nous montrons que nous pouvons transférer avec succès les informations de style entre différentes tâches.
L'étude 1 nous a permis de mettre en évidence des vitesses de saisie de texte chez les personnes tétraplégiques et d'étudier l'influence de leurs aides techniques d'accès à l'outil informatique sur cette vitesse.
L'étude 2 nous a permis de mettre en avant l'hétérogénéité des résultats d'un logiciel de prédiction de mots sur la vitesse de saisie de texte sur une population hétérogène et sans paramétrage de ces logiciels.
L'étude 3 nous a permis d'étudier les habitudes de préconisations et de paramétrages des logiciels de prédictions de mots par les professionnels.
Les études 4 et 5 nous ont permis d'évaluer l'influence des paramétrages (nombre de mots affichés dans la liste de prédiction et l'adaptation du logiciel au vocabulaire de l'utilisateur) sur cette saisie de texte.
Enfin, l'étude 6 nous a permis d'étudier l'influence d'un entraînement dirigé par des professionnels sur les logiciels de prédictions de mots chez des personnes tétraplégiques, sur la vitesse de saisie de texte.
Les paramétrages (nombre de mots affichés dans la liste de prédiction et l'adaptation du logiciel au vocabulaire de l'utilisateur) ont une influence différente en fonction du niveau lésionnel des personnes tétraplégiques sur la vitesse de saisie de texte, le nombre d'erreurs ou le confort.
De plus, une différence entre l'importance donnée aux paramétrages par les professionnels préconisateurs et les paramétrages effectivement réglés a été mise en évidence.
Enfin, l'influence d'un entraînement dirigé sur la vitesse de saisie de texte a été mise en évidence sur la vitesse de saisie de texte.
Au regard de l'ensemble de ces résultats, il apparait nécessaire de paramétrer les logiciels de prédictions de mots, mais aussi de connaitre l'influence des différents réglages et de diffuser cette information au sein des réseaux professionnels.
Une systématisation des entraînements dirigés sur les logiciels de prédiction de mots nécessite une réflexion et une validation sur les modalités et la nature de ces accompagnements.
Dans ce cadre, nous proposons une mise en oeuvre statistique de la sémantique textuelle interprétative sur un corpus de textes de Pierre Mendès France (1922-1982) : sont construits des parcours interprétatifs qui vont du global – le corpus – vers le local – ses partitions contrastives – en considérant l'influence de la situation historico-sociale sur les textes.
Ainsi, deux grandes variables qui traversent le corpus sont testées : la variable chronologique et la variable générique.
Nous établissons que le corpus est structuré par ses hautes fréquences et étudions, de manière privilégiée, le vocabulaire banal.
On y applique deux traitements cooccurrentiels : l'environnement cotextuel et la cooccurrence asymétrique.
Le premier aboutit à la description de fonds textuels et intertextuels sur lesquels se détachent des formes sémantiques.
La seconde décrit des contours rythmiques de variation lexicale qui semblent associés, dans notre corpus, à des visées argumentatives distinctes : informatives ou persuasives-explicatives.
Définissons une préférence spécifique comme une préférence qui ne serait partagée pour aucun groupe d'utilisateurs.
Un utilisateur possédant plusieurs préférences spécifiques qu'il ne partage avec aucun autre utilisateur sera probablement mal servi par une approche de FC classique.
Il s'agit du problème des Grey Sheep Users (GSU).
Dans cette thèse, je réponds à trois questions distinctes.
1) Qu'est-ce qu'une préférence spécifique ?
J'apporte une réponse en proposant des hypothèses associées que je valide expérimentalement.
2) Comment identifier les GSU dans les données ?
Cette identification est importante afin d'anticiper les mauvaises recommandations qui seront fournies à ces utilisateurs.
Je propose des mesures numériques permettant d'identifier les GSU dans un jeu de données de recommandation sociale.
Ces mesures sont significativement plus performantes que celles de l'état de l'art.
Enfin, comment modéliser ces GSU pour améliorer la qualité des recommandations qui leurs sont fournies ?
Je propose des méthodes inspirées du domaine de l'apprentissage automatique et dédiées à la modélisation des GSU permettant d'améliorer la qualité des recommandations qui leurs sont fournies.
Ce travail de thèse se situe dans le difficile contexte de la linguistique et de l'informatique.
Plus précisément, il s'agit de montrer l'intérêt de la prise en compte simultanée de la structure du document et des connaissances linguistiques pour la classification de documents suivant leur style.
Pour cela, nous avons défini de nouveaux descripteurs, qui, combinés avec des descripteurs linguistiques exploitant la hiérarchie textuelle, sont pertinents pour caractériser des types de documents.
Puis, nous avons proposée une méthode de classification fondée sur l'absence des motifs dans les documents.
Une des originalités de notre travail est d'associer des méthodes linguistiques et d'apprentissage automatique à des techniques de recherche de motifs locaux.
Cette hiérarchisation représente la structure logique du document fondée sur le principe que différentes fenêtres d'observation correspondent à des différents types d'information.
Ces derniers sont reliés entre eux par le biais de la notion de l'héritage du contexte afin de préserver la cohérence globale du document.
D'autre part, des hypothèses liées à la tâche de catégorisation sont émergées telle que l'exploitation de l'absence totale ou partielle de motifs sous certaines contraintes, qui peut servir à construire de nouvelles analogies pour la catégorisation des documents.
Alors, en analysant par évidence les motifs à fréquences faibles ou nulles, une nouvelle approche de catégorisation par exclusion-inclusion a été proposée en introduisant une nouvelle notion telle que les motifs exclusifs
Depuis une trentaine d'années, les images scientifiques font l'objet d'un intérêt croissant dans le champ des sciences humaines et sociales, de l'histoire de l'art, de l'histoire des sciences et de l'épistémologie de manière générale.
Ce qui a favorisé la naissance, puis l'essor, dans le monde anglo-saxon, des visual studies et plus particulièrement d'une branche qui intéresse spécifiquement l'épistémocritique : les visual studies of science.
Les sciences entretiennent en effet un lien intime dans leur production et leur diffusion du savoir avec les représentations visuelles : diagrammes, schémas, photographies, figures géométriques, équations, etc.
Cette articulation entre le « voir » et le « savoir » a ouvert de nouvelles voies à la recherche en épistémocritique qui noue désormais des relations fécondes avec les études visuelles.
Ces dernières ont insisté à la fois sur le rôle crucial de l'image dans le processus d'acquisition des connaissances (modélisation, force de propositions paradigmatiques, support visuel à vocation didactique, document-témoin des pratiques scientifiques, etc.).
Au cœur de notre projet, se trouve un type d'image particulier, les diagrammes qui sont des outils heuristiques permettant à un savoir encore tâtonnant de surgir et de prendre forme.
Formes hybrides, faites d'image et d'écriture les diagrammes posent à leur manière la question des rapports entre la connaissance et les procédures de visualisation dans la démarche scientifique, question qui a pris une acuité particulière avec la crise du langage et de la représentation suscitée par la révolution scientifique au tournant du XIXème et du XXème siècle.
Traditionnellement, on oppose le concept qui est du côté de l'argumentation discursive au diagramme qui est du côté de l'intuition et de l'imagination.
Dans ce domaine, le début du XXème siècle est aussi marqué par des innovations formelles, notamment par le recours à des procédures de visualisation et de spatialisation qui visent à renouveler les modes de représentation.
Si l'on considère que ces innovations sont le produit d'une imagination diagrammatique, alors on peut envisager cette dernière comme un pont entre la démarche scientifique et la démarche littéraire.
Le diagramme est une représentation visuelle de concepts ou d'idées mis en relation.
Il peut aussi montrer les relations constitutives d'un objet ou les relations entre des objets hétérogènes.
Comme les théoriciens du diagramme l'ont bien montré, le diagramme n'est pas une illustration : il n'est pas une mise en forme spatialisée d'une idée qui lui préexisterait mais il fait advenir l'idée à travers une représentation visuelle.
La relation analogique avec l'objet (relation iconique) sur laquelle se fonde le diagramme concerne les relations des parties entre elles et avec le tout.
Par ailleurs, les diagrammes se caractérisent par leur caractère hybride qui mêle texte et image.
Prenant pour objet la pensée diagrammatique, notre projet s'intéressera à tous les dispositifs où texte et image se combinent dans des figures graphiques qui entretiennent une relation iconique avec leur objet.
Plus largement, on fera l'hypothèse que l'imagination diagrammatique met en jeu une série de tensions qui recouvrent en partie les tensions entre littérature et science : tension entre image et concept, entre imagination et raison, entre intelligible et sensible, entre textuel et visuel, entre visible et lisible, etc.
Nos œuvres dessineront donc un corpus synchronique dans la période du fin XIXème-XXème qui correspond à celle où les sciences, et les mathématiques en particulier, sont confrontées à une crise de la représentation qui interroge à nouveau frais les relations entre le langage et le monde.
Nous nous demanderons comment les textes littéraires enregistrent et répercutent cette crise, comment ils la mettent en scène avec leurs propres moyens, et comment la pensée mathématique peut devenir l'origine même du processus de création.
Dans le contexte de changement climatique qui voit se multiplier les épisodes de sécheresse et les territoires sujets à de forts stress hydriques, cette thèse s'intéresse à la manière de gérer le déséquilibre entre disponibilité de la ressource en eau et demande croissante à Phoenix et à Tucson, dans l'Ouest aride des États-Unis.
Pour ce faire, la thèse mobilise les outils et concepts de l'urban political ecology afin d'observer les luttes de pouvoir entre les acteurs de la gestion de l'eau dans un contexte où le système de grandes infrastructures hydrauliques sous-tendant la croissance urbaine est de plus en plus remis en question.
Ce travail de recherche met en œuvre des protocoles d'enquête quantitatifs et qualitatifs : analyse des discours à partir de corpus de textes extraits de la presse et de Twitter pour déconstruire le discours dominant sur l'eau ; entretiens semi-directifs avec les acteurs de l'eau (institutions et militants écologistes) et observation participante pour questionner les tensions entre discours et changements entrepris dans les pratiques urbaines à l'échelle locale pour économiser l'eau.
La thèse montre d'une part que les stratégies d'adaptation sont mises en œuvre par les acteurs dominants dans le cadre de fixes socio-écologiques afin de maintenir la trajectoire de croissance de villes particulièrement attractives.
Dans ce thèse, nous présentons les implicatures conversationnelles de manière intuitive en nous appuyant sur le concept très large de granularité de Jerry Hobbs.
Nous présentons ensuite les implicatures conversationnelles d'un point de vue interdisciplinaire en débutant par ses origines Gricéennes et en passant par la sociologie à travers la théorie de la politesse, l'inférence par l'abduction et les systèmes de dialogues par la théorie des actes de langage.
Enfin, nous motivons les deux lignes d'approches de cette thèse pour l'étude des implicatures conversationnelles : l'analyse empirique d'un corpus de conversation située et finalisée, et l'analyse par synthèse dans le cadre d'un jeu d'aventure textuel.
La disponibilité de quantités massives de données, comme des images dans les réseaux sociaux, des signaux audio de téléphones mobiles, ou des données génomiques ou médicales, a accéléré le développement des techniques d'apprentissage automatique.
Récemment, les systèmes d'apprentissage profond ont émergé comme des algorithmes d'apprentissage très efficaces.
Ces modèles multi-couche effectuent leurs prédictions de façon hiérarchique, et peuvent être entraînés à très grande échelle avec des méthodes de gradient.
Leur succès a été particulièrement marqué lorsque les données sont des signaux naturels comme des images ou des signaux audio, pour des tâches comme la reconnaissance visuelle, la détection d'objets, ou la reconnaissance de la parole.
Pour de telles tâches, l'apprentissage profond donne souvent la meilleure performance empirique, mais leur compréhension théorique reste difficile à cause du grand nombre de paramètres, et de la grande dimension des données.
Leur succès est souvent attribué à leur capacité d'exploiter des structures des signaux naturels, par exemple en apprenant des représentations invariantes et multi-échelle de signaux naturels à travers un bon choix d'architecture, par exemple avec des convolutions et des opérations de pooling.
Néanmoins, ces propriétés sont encore mal comprises théoriquement, et l'écart entre la théorique et pratique en apprentissage continue à augmenter.
Cette thèse vise à réduire cet écart grâce à l'étude d'espaces de fonctions qui surviennent à partir d'une certaine architecture, en particulier pour les architectures convolutives.
Notre approche se base sur les méthodes à noyaux, et considère des espaces de Hilbert à noyaux reproduisant (RKHS) associés à certains noyaux construits de façon hiérarchique selon une architecture donnée.
Cela nous permet d'étudier précisément des propriétés de régularité, d'invariance, de stabilité aux déformations du signal, et d'approximation des fonctions du RKHS.
Ces propriétés sur la représentation sont aussi liées à des questions d'optimisation pour l'entraînement de réseaux profonds à très grand nombre de neurones par descente de gradient, qui donnent lieu à de tels noyaux.
Cette théorie suggère également des nouvelles stratégies pratiques de régularisation qui permettent d'obtenir une meilleure performance en généralisation pour des petits jeux de données, et une performance état de l'art pour la robustesse à des perturbations adversariales en vision.
Une Ligne de Produits Logiciels (LPL) supporte la gestion d'une famille de logiciels.
Cette approche se caractérise par une réutilisation systématique des artefacts communs qui réduit le coût et le temps de mise sur le marché et augmente la qualité des logiciels.
Cependant, une LPL exige un investissement initial coûteux.
Cependant, l'efficacité de cette pratique se dégrade proportionnellement à la croissance de la famille de produits, qui devient difficile à maintenir.
Dans cette thèse, nous proposons une approche hybride qui utilise à la fois une LPL et l'approche C&amp;O pour faire évoluer une famille de produits logiciels.
Le développeur peut alors réduire ces possibilités en exprimant ses préférences (e.g. produits, artefacts) et en utilisant les estimations de coûts sur les opérations que nous proposons.
Nous avons étayé cette thèse en développant le framework SUCCEED (SUpporting Clone-and-own with Cost-EstimatEd Derivation) et l'avons appliqué à une étude de cas sur des familles de portails web.
Depuis les travaux fondateurs de R. Bolt « Mets ça là » combinant la voix et le geste, les modalités d'interaction se sont multipliés, diversifiés et améliorés.
Les récents paradigmes d'interaction comme les interfaces tangibles incarnées ou la réalité augmentée, couplés aux progrès des systèmes de localisation, à la miniaturisation des dispositifs, à la qualité des réseaux sans fils, à l'amélioration de la reconnaissance de la parole ou de gestes ouvrent un vaste champ de possibilités d'interaction pour les systèmes multimodaux.
Ce travail de thèse aborde ce problème de conception et de développement pour la multimodalité en entrée (de l'utilisateur vers le système informatique).
Nous décrivons un modèle conceptuel de la multimodalité qui organise dans un canevas unificateur les modalités et leurs formes de combinaison.
Basé sur ce modèle, nous définissons une approche générique à composants logiciels, notée ICARE, facilitant et accélérant la conception, le développement et le maintien des interfaces multimodales.
Nous démontrons l'apport de cette approche par l'outil ICARE qui est une opérationnalisation de notre approche à composants.
Un éditeur graphique est fourni, simplifiant la phase d'assemblage des composants et générant automatiquement le code correspondant à l'interaction multimodale.
La planification de tournées de véhicules dans des environnements urbains denses est un problème difficile qui nécessite des solutions robustes et flexibles.
Au lieu de cela, nous proposons dans cette thèse d'étudier l'application de méthodes d'apprentissage par renforcement multi-agent (MARL) aux DS-VRPs en s'appuyant sur des réseaux de neurones profonds (DNNs).
Nous avons ensuite proposé un nouveau modèle de décision séquentiel en relâchant la contrainte d'observabilité partielle que nous avons baptisé MDP multi-agent séquentiel (sMMDP).
Ce modèle permet de décrire plus naturellement les DS-VRPs, dans lesquels les véhicules prennent la décision de servir leurs prochains clients à l'issu de leurs précédents services, sans avoir à attendre les autres.
Cependant, l'implémentation de cette technologie dans des applications réelles est entravée par la grande dégradation des performances en présence de nuisances acoustiques en phase d'utilisation.
Un grand effort a été investi par la communauté de recherche en RAL dans la conception de techniques de compensation des nuisances acoustiques.
Ces techniques opèrent à différents niveaux : signal, paramètres acoustiques, modèles ou scores.
Afin de mettre en œuvre cette méthodologie, des exemples de données propres / corrompues sont générés artificiellement et utilisés pour construire des algorithmes de compensation des nuisances acoustiques.
Ce procédé permet d'éviter les dérivations qui peuvent être complexes, voire très approximatives.
La deuxième classe de techniques n'utilise aucun modèle de distorsion dans le domaine des i
Des expériences ont été réalisées sur les données bruitées ainsi que les données de courte durée ; donnés de NIST SRE 2008 bruitées/découpées artificiellement ainsi que les données du challenge SITW bruitées naturellement / de courte durée.
Même avec le récent passage à 280 caractères, les messages de Twitter considérés dans leur singularité, sans information additionnelle exogène, peuvent confronter leurs lecteurs à des difficultés d'interprétation.
L'ajout d'une contextualisation à ces messages s'avère donc une voie de recherche prometteuse pour faciliter l'accès à leur contenu informationnel.
Dans la dernière décennie, la majorité des travaux se sont concentrés sur la construction de résumés à partir de sources d'information complémentaires telles que Wikipédia.
Nous avons choisi dans cette thèse une voie complémentaire différente qui s'appuie sur l'analyse des conversations sur Twitter afin d'extraire des informations utiles à la contextualisation d'un tweet.
Ces informations ont été intégrées dans un prototype qui, pour un tweet donné, propose une visualisation d'un sous-graphe du graphe de conversation associé au tweet.
Ce sous-graphe extrait automatiquement à partir de l'analyse des distributions des indicateurs structurels, permet de mettre en évidence notamment des individus qui jouent un rôle majeur dans la conversation et des tweets qui ont contribué à la dynamique des échanges.
Ce prototype a été testé sur un panel d'utilisateurs, pour valider son apport et ouvrir des perspectives d'amélioration.
Dans un second temps, nous considérons le problème des bandits corrompus, dans lequel un processus de corruption stochastique perturbe le retour d'information.
Pour ce problème aussi, nous concevons des algorithmes pour obtenir un regret cumulatif asymptotiquement optimal.
Cette thèse a pour objectif la détection du langage figuratif dans les réseaux sociaux.
Nous nous focalisons en particulier sur l'ironie et le sarcasme dans Twitter et proposons une approche basée sur l'apprentissage supervisée afin de prédire si le message véhiculé dans un tweet est ironique ou non.
Les résultats obtenus pour cette tâche extrêmement complexe sont très encourageants et permettrons d'améliorer sensiblement la détection de polarité lors de l'analyse de sentiments.
La publication scientifique dans les revues spécialisées et les actes de conférences permet de communiquer les progrès en sciences.
Les comités de rédaction et de programme sous-jacents représentent la clé de voûte du processus d'évaluation.
Avec le développement des revues et le nombre croissant de conférences scientifiques organisées chaque année, rechercher des experts pour participer à ces comités est une activité chronophage mais critique.
Cette thèse se focalise sur la tâche de suggestion de membres de comité de programme (CP) pour des conférences scientifiques.
Elle comporte trois volets.
Premièrement, nous proposons une modélisation basée sur un graphe hétérogène pondéré de l'expertise scientifique multifacette des chercheurs.
Deuxièmement, nous définissons des indicateurs scientométriques pour quantifier les critères impliqués dans la constitution de CP.
Troisièmement, nous concevons une approche de suggestion de membres de CP pour une conférence donnée, en combinant les résultats des indicateurs scientométriques susmentionnés.
Notre approche est expérimentée pour une des conférences de premier plan de notre communauté de recherche : SIGIR, en considérant ses éditions de 1971 à 2015, ainsi que les conférences proches thématiquement.
Dans une interaction humain-agent, l'engagement de l'utilisateur est un élément essentiel pour atteindre l'objectif de l'interaction.
Dans cette thèse, nous étudions comment l'engagement de l'utilisateur pourrait être favorisé par le comportement de l'agent.
Basé sur les résultats de la dernière étude, nous proposons un Gestionnaire de Sujets axé sur l'engagement (modèle computationnel) qui personnalise les sujets d'une interaction dans des conversations où l'agent donne des informations à un utilisateur humain.
Le Modèle de Sélection des Sujets du Gestionnaire de Sujets décide sur quoi l'agent devrait parler et quand.
Pour cela, il prend en compte la perception par l'agent de l'utilisateur, qui est dynamiquement mis à jour, ainsi que l'état mental et les préférences de l'agent.
Le Modèle de Transition de Sujets du Gestionnaire de Sujet, basé sur une étude empirique, calcule comment l'agent doit présenter les sujets dans l'interaction en cours sans perdre la cohérence de l'interaction.
Nous avons implémenté et évalué le Gestionnaire de Sujets dans un agent virtuel conversationnel qui joue le rôle d'un visiteur dans un musée.
Depuis le premier séquençage du génome humain au début des années 2000, de grandes initiatives se sont lancé le défi de construire la carte des variabilités génétiques inter-individuelles, ou bien encore celle des altérations de l'ADN tumoral.
Ces projets ont posé les fondations nécessaires à l'émergence de la médecine de précision, dont le but est d'intégrer aux dossiers médicaux conventionnels les spécificités génétiques d'un individu, afin de mieux adapter les traitements et les stratégies de prévention.
La traduction des variations et des altérations de l'ADN en prédictions phénotypiques constitue toutefois un problème difficile.
Les séquenceurs ou puces à ADN mesurent plus de variables qu'il n'y a d'échantillons, posant ainsi des problèmes statistiques.
Les données brutes sont aussi sujettes aux biais techniques et au bruit inhérent à ces technologies.
Enfin, les vastes réseaux d'interactions à l'échelle des protéines obscurcissent l'impact des variations génétiques sur le comportement de la cellule, et incitent au développement de modèles prédictifs capables de capturer un certain degré de complexité.
Cette thèse présente de nouvelles contributions méthodologiques pour répondre à ces défis.
Tout d'abord, nous définissons une nouvelle représentation des profils de mutations tumorales, qui exploite leur position dans les réseaux d'interaction protéine-protéine.
Pour certains cancers, cette représentation permet d'améliorer les prédictions de survie à partir des données de mutations, et de stratifier les cohortes de patients en sous-groupes informatifs.
Nous présentons ensuite une nouvelle méthode d'apprentissage permettant de gérer conjointement la normalisation des données et l'estimation d'un modèle linéaire.
Nos expériences montrent que cette méthode améliore les performances prédictives par rapport à une gestion séquentielle de la normalisation puis de l'estimation.
Pour finir, nous accélérons l'estimation de modèles linéaires parcimonieux, prenant en compte des interactions deux à deux, grâce à un nouvel algorithme.
L'accélération obtenue rend cette estimation possible et efficace sur des jeux de données comportant plusieurs centaines de milliers de variables originales, permettant ainsi d'étendre la portée de ces modèles aux données des études d'associations pangénomiques.
Notre étude porte sur les verbes pronominaux en français, en anglais et en allemand.
La première partie est une évaluation des travaux précédents, depuis les travaux "classiques" en terminologie, grammaire et typologies jusqu'aux travaux faits dans le domaine du TALN et de la TAO.
Ces énoncés sont soumis à différents tests et transformations en vue de dégager des constantes de comportement.
La typologie résultante est exposée en partie 3 et sert de base à la formalisation précédant la phase d'implémentation effectuée sous Sygmart, un modèle de transfert de structures.
Enfin, la double lecture (lecture réfléchie et lecture réciproque) fait problème pour certains énoncés d'ambiguïté extrême, à moins qu'une base de connaissance complète ne soit intégrée dans la chaîne
Cette thèse se propose d'analyser les effets d'une formation plurilingue en ligne sur l'acquisition de compétences langagières et, en particulier, en langue cible (français) pour des apprenants de lycée, de Barcelone.
Pour pouvoir répondre à l'hypothèse selon laquelle la formation développerait les compétences des apprenants d'une part dans les langues de la formation mais également en français, la méthodologie adoptée a été la réalisation d'un protocole (quasi-) expérimental auprès de lycéens d'un établissement de Barcelone.
Ce protocole a consisté à faire participer un groupe d'élèves à une session sur la plate-forme Galanet, de façon intégrée à leurs enseignements scolaires, ainsi qu'à la réalisation de tests pré- et post-formation.
Un profil langagier a été complété par les sujets en amont de la formation.
Chacun des tests présente le même type d'exercices, portant sur les mêmes compétences, et pour chacune des situations (initiale ou finale), il y a un test dont les supports sont en français et un autre dont les supports sont dans d'autres langues romanes (italien, portugais, roumain et occitan vivarois).
Les compétences ciblées par les tests sont la cohérence/cohésion textuelle, la compréhension globale et fine, l'identification du degré de perception de l'interculturalité, la traduction/transposition, la segmentation et la reconnaissance/application d'un modèle discursif.
Les données recueillies lors des tests ont fait l'objet d'analyses quantitatives et qualitatives, suivies de traitement statistiques quand il s'est agi d'établir, en particulier, des corrélations.
En parallèle, le degré d'interactivité intercompréhensif (prenant en compte l'alternance des langues dans ce contexte bi-plurilingue) des messages de forum déposés lors de la formation plurilingue a été déterminé par le traitement semi-automatique du corpus de messages par la plate-forme Calico.
C'est cette double analyse, basée sur des données concrètes expérientielles qui a permis de réaliser une étude de cas et de définir par l'étude longitudinale de leurs différents résultats si le fait de suivre de manière satisfaisante, en terme de participation, une formation en intercompréhension a une incidence positive sur l'acquisition des compétences évaluées par les tests, sur quelles compétences particulières et de quelle manière.
Les questions des élèves sont utiles pour leur apprentissage et l'adaptation pédagogique des enseignants.
Nous abordons cette problématique principalement dans le cadre d'une formation hybride dans lequel chaque semaine les étudiants posent des questions en ligne, selon une approche de classe inversée, pour aider les enseignants à préparer leur séances de questions-réponses en présentiel.
Notre objectif est d'outiller l'enseignant pour qu'il détermine les types de questions posées par les différents groupes d'apprenants.
Pour mener ce travail, nous avons développé un schéma de codage de questions guidé par l'intention des élèves et la réaction pédagogique de l'enseignant.
Plusieurs outils de classification automatique ont été conçus, évalués et combinés pour catégoriser les questions.
Nous avons montré comment un modèle dérivé de clustering des données et entraîné sur des sessions antérieures peut être utilisé pour prédire le profil des élèves en ligne et établir des liens avec leurs questions.
Ces résultats nous ont permis de proposer trois organisations de questions aux enseignants (basées sur les catégories de questions et profils des apprenants) qui ouvrent des perspectives de traitement différent lors des séances de questions-réponses.
Nous avons testé et montré la possibilité d'adapter notre schéma de codage et les outils associés au contexte très différent d'un MOOC, ce qui suggère une certaine généricité de notre approche.
Les véhicules autonomes sont des systèmes assez complexes où plusieurs types de défaillances peuvent se produire, enclenchant une fausse action sur la route.
Chaque composant devra donc être testé rigoureusement pour anticiper et éliminer les potentielles défaillances.
Des techniques de simulation numérique sont utilisées pour complémenter les essais de conduite réelle dans la validation.
Les contributions de cette thèse sont organisées selon trois objectifs : détection d'un nombre maximal de défaillances de la loi de commande, détection de scénarios près de la frontière entre zones défaillantes et sûres, et construction de modèles explicables de la frontière pour l'identifier aussi précisément que possible.
Des techniques d'apprentissage (forêt aléatoire) et d'optimisation (CMA-ES) sont utilisées pour satisfaire la contrainte industrielle de réduire la puissance de calcul, et trois approches sont considérées pour construire des modèles de frontière en analysant leurs performances et explicabilités : réseaux de neurones, programmation mathématique linéaire, et programmation génétique appliquée à la régression symbolique.
Le travail développé dans cette thèse porte sur l'analyse de séquences vidéo.
Cette dernière est basée sur 3 taches principales : la détection, la catégorisation et le suivi des objets.
Le développement de solutions fiables pour l'analyse de séquences vidéo ouvre de nouveaux horizons pour plusieurs applications telles que les systèmes de transport intelligents, la vidéosurveillance et la robotique.
Dans cette thèse, nous avons mis en avant plusieurs contributions pour traiter les problèmes de détection et de suivi d'objets multiples sur des séquences vidéo.
Les techniques proposées sont basées sur l'apprentissage profonds et des approches de transfert d'apprentissage.
Dans une première contribution, nous abordons le problème de la détection multi-objets en proposant une nouvelle technique de transfert d'apprentissage basé sur le formalisme et la théorie du filtre SMC (Sequential Monte Carlo) afin de spécialiser automatiquement un détecteur de réseau de neurones convolutionnel profond (DCNN) vers une scène cible.
Dans une deuxième contribution, nous proposons une nouvelle approche de suivi multi-objets original basé sur des stratégies spatio-temporelles (entrelacement / entrelacement inverse) et un détecteur profond entrelacé, qui améliore les performances des algorithmes de suivi par détection et permet de suivre des objets dans des environnements complexes (occlusion, intersection, fort mouvement).
Dans une troisième contribution, nous fournissons un système de surveillance du trafic, qui intègre une extension du technique SMC afin d'améliorer la précision de la détection de jour et de nuit et de spécialiser tout détecteur DCNN pour les caméras fixes et mobiles.
Tout au long de ce rapport, nous fournissons des résultats quantitatifs et qualitatifs.
Sur plusieurs aspects liés à l'analyse de séquences vidéo, ces travaux surpassent les cadres de détection et de suivi de pointe.
En outre, nous avons implémenté avec succès nos infrastructures dans une plate-forme matérielle intégrée pour la surveillance et la sécurité du trafic routier.
Les larges ontologies biomédicales décrivent généralement le même domaine d'intérêt, mais en utilisant des modèles de modélisation et des vocabulaires différents.
Les systèmes de matching d'ontologies combinent différents types de matcher pour résoudre ces problèmes.
Le nouvel algorithme de partitionnement est basé sur le clustering hiérarchique par agglomération (CHA).
L'application de techniques de sélection de caractéristiques à chaque classificateur local augmente la valeur de rappel pour chaque tâche d'alignement local.
Vu les ambiguïtés que créent les anaphores au niveau des langages naturelle, plusieurs chercheurs du domaine du traitement automatique des langues (TAL) ont mis en place divers approches pour résoudre le problème.
Nous nous proposons d'adapter ces approches à la résolution automatique des anaphores pronominales dans RESUMAN.
Dans la première partie sont présentés les modèles cognitifs, linguistiques et textuelles qui rendent compte du fonctionnement et de l'interprétation des pronoms.
Dans la deuxième partie, les procédures d'interprétation des anaphores pronominales sont exposées : la procédure de distance minimale, la procédure des fonctions parallèles, la procédure du sujet ou la procédure thématique, ainsi que les procédures d'analyse morphologique, sémantique et pragmatique.
Dans la troisième partie, une nouvelle version d'algorithme, se basant sur une approche statistique, est présentée.
L'outil RESUMAN permet d'améliorer les performances d'un TAL en cas de textes denses en anaphores.
Les performances de ce logiciel sont évaluées et ses limites sont commentées.
La génomique comparative est un sujet essentiel pour éclaircir la biologie évolutionnaire.
La première étape pour dépasser une connaissance seulement descriptive est de développer une méthode pour représenter le contenu du génome.
Nous avons choisi la représentation modulaire des génomes pour étudier les lois quantitatives qui réglementent leur composition en unités élémentaires de type fonctionnel ou évolutif.
La première partie de la thèse se fonde sur l'observation que le nombre de domaines ayant la même fonction est lié à la taille du génome par une loi de puissance.
Puisque les catégories fonctionnelles sont des agrégats de familles de domaines, on se demande comment le nombre de domaines dans la même catégorie fonctionnelle est lié à l'évolution des familles.
Le résultat est que les familles suivent également une loi de puissance.
Le deuxième partie présente un modèle positif qui construit une réalisation à partir des composants liés dans un réseau de dépendance.
L'ensemble de toutes les réalisations reproduit la distribution des composants partagés et la relation entre le nombre de familles distinctes et la taille du génome.
Le dernier chapitre étend l'approche modulaire aux écosystèmes microbiens.
Sur la base des constatations que nous avons faites sur les lois de puissance pour les familles de domaines, nous avons analysé comment le nombre de familles dans un metagénome en est influencé.
Par conséquence, nous avons défini une nouvelle observable dont la forme fonctionnelle comprend des informations quantitatives sur la composition originelle du metagénome.
Ces dernières années, le contenu disponible sur le Web a augmenté de manière considérable dans ce qu'on appelle communément le Web social.
Pour l'utilisateur moyen, il devient de plus en plus difficile de recevoir du contenu de qualité sans se voir rapidement submergé par le flot incessant de publications.
Pour les fournisseurs de service, le passage à l'échelle reste problématique.
L'objectif de cette thèse est d'aboutir à une meilleure expérience utilisateur à travers la mise en place de systèmes de filtrage et de recommandation.
Le filtrage consiste à offrir la possibilité à un utilisateur de ne recevoir qu'un sous ensemble des publications des comptes auxquels il est abonné.
Tandis que la recommandation permet la découverte d'information à travers la suggestion de comptes à suivre sur des sujets donnés.
Nous avons élaboré MicroFilter un système de filtrage passant à l'échelle capable de gérer des flux issus du Web ainsi que RecLand, un système de recommandation qui tire parti de la topologie du réseau ainsi que du contenu afin de générer des recommandations pertinentes.
Cette thèse porte sur des analyses acoustiques et prosodiques du français à partir de grandes masses de données orales illustrant différents styles de parole (préparée et spontanée).
Nous nous sommes intéressées aux attributs acoustiques et prosodiques qui pourraient caractériser la prononciation.
Une classification automatique (CA) a été effectuée pour discriminer deux paires homophones ('et/est'et 'à/a') par des propriétés acoustiques et prosodiques.
Les résultats de la CA ont montré que le paire 'et/est'était plus dissociable.
La CA par des attributs prosodiques et inter-segmentaux (15 attributs) s'est avérée aussi performante que celle utilisant la totalité des 62 attributs.
Un test perceptif a été également effectué pour vérifier si les humains utilisaient eux aussi ces paramètres.
Les résultats ont suggéré que des informations acoustiques et prosodiques pourraient être utiles pour effectuer un choix correct de mots dans des structures syntaxiquement ambigües.
Ensuite, nous avons examiné des propriétés prosodiques globales aux niveaux du nom et du syntagme nominal.
La comparaison entre mots lexicaux et grammaticaux a montré que la fréquence fondamentale (F0) montante et l'allongement vocalique de la dernière syllabe caractérisent les mots lexicaux, par opposition aux mots grammaticaux.
Ainsi, le profil de F0 moyenne d'un syntagme nominal de longueur n pourrait être différent de celui du nom avec une valeur de F0 basse au début du syntagme.
Les profils prosodiques peuvent être utiles pour localiser frontières de mots.
Les résultats de ce travail pourront servir à localiser le focus et les entités-nommées par des classifieurs discriminants, et de manière plus générale à améliorer les techniques de localisation des frontières des mots pour la RAP.
Les malwares, autrement dit programmes malicieux ont grandement évolué ces derniers temps et sont devenus une menace majeure pour les utilisateurs grand public, les entreprises et même le gouvernement.
Afin de limiter ces problèmes, les chercheurs spécialisés dans les malwares ont proposé différentes approches comme l'exploration des données (data mining) ou bien l'apprentissage automatique (machine learning) pour détecter et classifier les échantillons de malwares en fonction de leur propriétés statiques et dynamiques.
De plus les méthodes proposées sont efficaces sur un petit ensemble de malwares, le passage à l'échelle de ses méthodes pour des grands ensembles est toujours en recherche et n'a pas été encore résolu.
Il est évident aussi que la majorité des malwares sont une variante des précédentes versions.
Par conséquent, le volume des nouvelles variantes créées dépasse grandement la capacité d'analyse actuelle.
C'est pourquoi développer la classification des malwares est essentiel pour lutter contre cette augmentation pour la communauté informatique spécialisée en sécurité.
Le challenge principal dans l'identification des familles de malware est de réussir à trouver un équilibre entre le nombre d'échantillons augmentant et la précision de la classification.
Pour atteindre notre objectif, premièrement nous avons développé une version portable, évolutive et transparente d'analyse de malware appelée VirMon pour analyse dynamique de malware visant les OS windows.
Deuxièmement, nous avons mis en place un cluster de 5 machines pour notre module d'apprentissage en ligne (Jubatus);qui permet de traiter une quantité importante de données.
Cette configuration permet à chaque machine d'exécuter ses tâches et de délivrer les résultats obtenus au gestionnaire du cluster.
Notre outil proposé consiste essentiellement en trois niveaux majeures.
Le premier niveau permet l'extraction des comportements des échantillons surveillés et observe leurs interactions avec les ressources de l'OS.
Durant cette étape, le fichier exemple est exécuté dans un environnement « sandbox » .
Notre outil supporte deux « sandbox » : VirMon et Cuckoo.
Durant le second niveau, nous appliquons des fonctionnalités d'extraction aux rapports d'analyses.
Le label de chaque échantillon est déterminé Virustotal, un outil regroupant plusieurs anti-virus permettant de scanner en ligne constitués de 46 moteurs de recherches.
Enfin au troisième niveau, la base de données de malware est partitionnée en ensemble de test et d'apprentissage.
L'objectif de cette thèse est de concevoir et de construire, un système Text-To-Speech (TTS) haute qualité à base de HMM (Hidden Markov Model) pour le vietnamien, une langue tonale.
Le système est appelé VTED (Vietnamese TExt-to-speech Development system).
Au vu de la grande importance de tons lexicaux, un tonophone” – un allophones dans un contexte tonal – a été proposé comme nouvelle unité de la parole dans notre système de TTS.
Un nouveau corpus d'entraînement, VDTS (Vietnamese Di-Tonophone Speech corpus), a été conçu à partir d'un grand texte brut pour une couverture de 100% de di-phones tonalisés (di-tonophones) en utilisant l'algorithme glouton.
Un total d'environ 4000 phrases ont été enregistrées et pré-traitées comme corpus d'apprentissage de VTED.
Dans la synthèse de la parole sur la base de HMM, bien que la durée de pause puisse être modélisée comme un phonème, l'apparition de pauses ne peut pas être prédite par HMM.
Les niveaux de phrasé ne peuvent pas être complètement modélisés avec des caractéristiques de base.
Cette recherche vise à obtenir un découpage automatique en groupes intonatifs au moyen des seuls indices de durée.
Des blocs syntaxiques constitués de phrases syntaxiques avec un nombre borné de syllabes (n), ont été proposés pour prévoir allongement final (n = 6) et pause apparente (n = 10).
Des améliorations pour allongement final ont été effectuées par des stratégies de regroupement des blocs syntaxiques simples.
La qualité du modèle prédictive J48-arbre-décision pour l'apparence de pause à l'aide de blocs syntaxiques, combinée avec lien syntaxique et POS (Part-Of-Speech) dispose atteint un F-score de 81,4 % (Précision = 87,6 %, Recall = 75,9 %), beaucoup mieux que le modèle avec seulement POS (F-score=43,6%) ou un lien syntaxique (F-score=52,6%).
L'architecture du système a été proposée sur la base de l'architecture HTS avec une extension d'une partie traitement du langage naturel pour le Vietnamien.
L'apparence de pause a été prédit par le modèle proposé.
Les caractéristiques contextuelles incluent les caractéristiques d'identité de “tonophones”, les caractéristiques de localisation, les caractéristiques liées à la tonalité, et les caractéristiques prosodiques (POS, allongement final, niveaux de rupture).
Mary TTS a été choisi comme plateforme pour la mise en oeuvre de VTED.
Dans le test MOS (Mean Opinion Score), le premier VTED, appris avec les anciens corpus et des fonctions de base, était plutôt bonne, 0,81 (sur une échelle MOS 5 points) plus élevé que le précédent système – HoaSung (lequel utilise la sélection de l'unité non-uniforme avec le même corpus) ; mais toujours 1,2-1,5 point de moins que le discours naturel.
La qualité finale de VTED, avec le nouveau corpus et le modèle de phrasé prosodique, progresse d'environ 1,04 par rapport au premier VTED, et son écart avec le langage naturel a été nettement réduit.
Dans le test d'intelligibilité, le VTED final a reçu un bon taux élevé de 95,4%, seulement 2,6% de moins que le discours naturel, et 18% plus élevé que le premier.
Le taux d'erreur du premier VTED dans le test d'intelligibilité générale avec le carré latin test d'environ 6-12% plus élevé que le langage naturel selon des niveaux de syllabe, de ton ou par phonème.
Le résultat final ne s'écarte de la parole naturelle que de 0,4-1,4%.
Les images hyperspectrales suscitent un intérêt croissant depuis une quinzaine d'années.
Elles fournissent une information plus détaillée d'une scène et permettent une discrimination plus précise des objets que les images couleur RVB ou multi-spectrales.
Les travaux de cette thèse s'inscrivent dans le cadre de la réduction et de partitionnement des images hyperspectrales de grande dimension spatiale.
L'approche proposée se compose de deux étapes : calcul d'attributs et classification des pixels.
Une nouvelle approche d'extraction d'attributs à partir des matrices de tri-occurrences définies sur des voisinages cubiques est proposée en tenant compte de l'information spatiale et spectrale.
Une étude comparative a été menée afin de tester le pouvoir discriminant de ces nouveaux attributs par rapport aux attributs classiques.
Concernant la classification, nous nous intéressons ici au partitionnement des images par une approche de classification non supervisée et non paramétrique car elle présente plusieurs avantages : aucune connaissance a priori, partitionnement des images quel que soit le domaine applicatif, adaptabilité au contenu informationnel des images.
Une étude comparative des principaux classifieurs semi-supervisés (connaissance du nombre de classes) et non supervisés (C-moyennes, FCM, ISODATA, AP) a montré la supériorité de la méthode de propagation d'affinité (AP).
Nous avons proposé une approche qui apporte des solutions à ces deux problèmes.
Pour estimer le nombre de classes, la méthode AP utilise de manière implicite un paramètre de préférence p dont la valeur initiale correspond à la médiane des valeurs de la matrice de similarité.
Cette valeur conduisant souvent à une sur-segmentation des images, nous avons introduit une étape permettant d'optimiser ce paramètre en maximisant un critère lié à la variance interclasse.
L'approche proposée a été testée avec succès sur des images synthétiques, mono et multi-composantes.
Elle a été également appliquée et comparée sur des images hyperspectrales de grande taille spatiale (1000 × 1000 pixels × 62 bandes) avec succès dans le cadre d'une application réelle pour la détection des plantes invasives.
AF) primaire, cooccurrent à la frontière prosodique, s'effacerait perceptivement en frontière prosodique majeure.
L'Accent Initial (AI), dit secondaire et optionnel, serait un accent rythmique apparaissant sur les longs constituants.
L'existence d'un Syntagme Intermédiaire (ip) est en revanche controversée.
L'objectif de cette étude est d'explorer l'organisation du phrasé prosodique en français.
Dans ce cadre, nous proposons une étude perceptive, via un corpus de parole contrôlée manipulant des structures syntaxiques ambiguës, où 80 participants ont effectué 3 tâches de perception : proéminence, frontière et groupement.
Les événements prosodiques perçus ont ensuite été mis en relation avec leurs réalités acoustiques.
Les résultats montrent que les auditeurs sont capables de percevoir des niveaux de granularité de frontières plus fins que ce que les descriptions traditionnelles du français prédisent.
Par ailleurs, les mots lexicaux sont systématiquement réalisés par un marquage bipolaire (AI+AF) de même force métrique.
AI joue également un rôle plus structurel que rythmique, en marquant la structure prosodique de manière plus privilégiée qu'AF.
Enfin, AF ne s'efface pas perceptivement en frontière prosodique majeure, et garde au contraire une trace métrique au niveau du mot lexical, qui ne varie pas strictement en fonction du niveau de constituance.
Afin d'optimiser la langue existante, nous avons entrepris d'évaluer les niveaux appropriés de simplification qui permettraient une compréhension plus précise et plus rapide, réduisant ainsi le temps de formation des pilotes.
Nous avons tout d'abord exploré le domaine des langues contrôlées afin d'avoir un aperçu des langues contrôlées existantes, de leur contexte et de leurs règles.
À partir de cette recherche, nous avons tenté de trouver des solutions d'optimisation, tout en nous efforçant d'apporter une contribution originale à ce domaine.
Les agents virtuels conversationnels ayant un comportement social reposent souvent sur au moins deux disciplines différentes : l'informatique et la psychologie.
Dans la plupart des cas, les théories psychologiques sont converties en un modèle informatique afin de permettre aux agents d'adopter des comportements crédibles.
Nous nous intéressons aux agents conversationnels orientés tâche, qui sont utilisés dans un contexte professionnel pour produire des réponses à partir d'une base de connaissances métier.
Nous proposons un modèle affectif pour ces agents qui s'inspire des mécanismes affectifs chez l'humain.
L'approche que nous avons choisie de mettre en œuvre dans notre modèle s'appuie sur la théorie des Tendances à l'Action en psychologie.
Ce modèle a été implémenté dans une architecture d'agent conversationnel développée au sein de l'entreprise DAVI.
Afin de confirmer la pertinence de notre approche, nous avons réalisé plusieurs études expérimentales.
La deuxième porte sur l'impact des différentes stratégies de régulation possibles sur la perception de l'agent par l'utilisateur.
Enfin, la troisième étude porte sur l'évaluation des agents affectifs en interaction avec des participants.
Nous montrons que le processus de régulation que nous avons implémenté permet d'augmenter la crédibilité et le professionnalisme perçu des agents, et plus généralement qu'ils améliorent l'interaction.
Nos résultats mettent ainsi en avant la nécessité de prendre en considération les deux mécanismes émotionnels complémentaires : la génération et la régulation des réponses émotionnelles.
Ils ouvrent des perspectives sur les différentes manières de gérer les émotions et leur impact sur la perception de l'agent.
Dans cette thèse, j'étudie, dans une perspective à la fois théorique et historique, l'apparition de la crise macroéconomique.
En m'appuyant sur l'économie narrative, les techniques text mining et les théories des systèmes complexes, j'explore les mécanismes qui alimentent la crise systémique et je montre que c'est l'architecture du système économique qui crée la vulnérabilité des économies et les ingrédients d'une crise à grande échelle plutôt que la nature des chocs.
et une approche narrative pour créer des données structurées, reflétant les perspectives macroéconomiques des membres du FMI depuis le début des années 1950.
En utilisant une combinaison de techniques d'apprentissage supervisé et non supervisé, nous créons un vaste échantillon de 30 000 rapports du FMI.
Cet échantillon permet ensuite de caractériser chaque document dans environ 20 catégories de crise économiquement pertinentes, allant de la défaillance d'un État souverain, d'une crise monétaire ou d'une crise bancaire à des épidémies ou des conflits violents.
La base de données et les outils quantitatifs seront disponibles dans un package R (bientôt disponible sur github).
En m'appuyant les modèles écologiques de résilience, je montre que les systèmes macroéconomiques sont des systèmes en perpétuel déséquilibre avec des crises récurrentes (les déclencheurs) qui affectent de manière hétérogène le cœur du système (les propagateurs) et finalement les nœuds critiques (les points chauds) dans une dynamique similaire à celle des incendies de forêt.
Dans un troisième chapitre, j'adopte une perspective plus étroite en examinant les épisodes où le FMI est intervenu dans le cadre d'un accord de prêt.
Dans ce chapitre intitulé "Les instruments du prêteur en dernier ressort : guérir les fondamentaux ou calmer la panique", je donne un aperçu historique des 70 années d'intervention du FMI pour fournir une assistance aux États souverains en détresse.
Dans un quatrième chapitre, j'apporte la preuve de l'application d'une règle de Taylor par le FMI lors de la conception des accords de prêt destinés à aider les pays en détresse.
Dans l'ensemble, ce travail apporte de nouveauw éléments sur l'apparition de la crise systémique et sur la dynamique des perturbations mondiales que certains événements inattendus peuvent générer.
Ces travaux tentent de mettre au centre de l'analyse la nature narrative de la crise économique et l'importance de l'architecture du système de crise et visent à attirer l'attention sur l'importance de trouver le juste équilibre entre le développement économique, la complexité des systèmes économiques et la gravité de la crise.
Dans le contexte d'un système économique international très fragile et vulnérable, combiné à l'émergence croissante de crises non critiques sur le plan économique et écologique, une attention particulière devrait être accordée à l'élaboration de nouvelles normes de gestion de crise visant à réduire l'intensité de la crise tout en atténuant la vulnérabilité due à l'interconnexion des secteurs économiques, en augmentant la resilience décentralisée des différents nœuds économiques et en isolant les éléments les plus critiques du système afin d'éviter des effondrements économiques systémiques à grande échelle.
Le récit est un outil de communication qui permet aux individus de donner un sens au monde qui les entoure.
Il représente une plate-forme pour comprendre et partager leur culture, connaissances et identité.
Le récit porte une série d'événements réels ou imaginaires, en provoquant un ressenti, une réaction ou même, déclenche une action.
Pour cette raison, il est devenu un sujet d'intérêt pour différents domaines au-delà de la Littérature (Éducation, Marketing, Psychologie, etc.) qui cherchent d'atteindre un but particulier au travers de lui (Persuader, Réfléchir, Apprendre, etc.).
Cependant, le récit reste encore sous-développé dans le contexte informatique.
Il existent des travaux qui visent son analyse et production automatique.
Les algorithmes et implémentations, par contre, restent contraintes à imiter le processus créatif derrière des textes littéraires provenant de sources textuelles.
Ainsi, il n'existent pas des approches qui produisent automatiquement des récits dont 1) la source est constitué de matériel non formatées et passé dans la réalité et 2) et le contenu projette une perspective qui cherche à transmettre un message en particulier.
Travailler avec des données brutes devient relevante vu qu'elles augmentent exponentiellement chaque jour grâce à l'utilisation d'appareils connectés.
Ainsi, vu le contexte du Big Data, nous présentons une approche de génération automatique de récits à partir de données ambiantes.
L'objectif est de faire émerger l'expérience vécue d'une personne à partir des données produites pendant une activité humaine.
Tous les domaines qui travaillent avec des données brutes pourraient bénéficier de ce travail, tels que l'Éducation ou la Santé.
Il s'agit d'un effort interdisciplinaire qui inclut le Traitement Automatique de Langues, la Narratologie, les Sciences Cognitives et l'Interaction Homme-Machine.
Cette approche est basée sur des corpus et modèles et comprend la formalisation de ce que nous appelons le récit d'activité ainsi qu'une démarche de génération adaptée.
Elle a est composé de 4 étapes : la formalisation des récits d'activité, la constitution de corpus, la construction de modèles d'activité et du récit, et la génération de texte.
Chacune a été conçue pour surmonter des contraintes liées aux questions scientifiques posées vue la nature de l'objectif : la manipulation de données incertaines et incomplètes, l'abstraction valide d'après l'activité, la construction de modèles avec lesquels il soit possible la transposition de la réalité gardée dans les données vers une perspective subjective et la rendue en langage naturel.
Nous avons utilisé comme cas d'usage le récit d'activité, vu que les pratiquant se servent des appareils connectés, ainsi qu'ils ont besoin de partager son expérience.
Les résultats obtenus sont encourageants et donnent des pistes qui ouvrent beaucoup de perspectives de recherche.
Le travail realise au cours de cette these a essentiellement pour objectif la valorisation de donnees existantes et leur acces par l'ensemble de la communaute parole.
Les aspects importants de la constitution de cette base sont la sauvegarde des donnees.
Sa gestion d'acces et l'interfacage avec les outils de traitement et de visualisation des donnees.
Nous avons propose un modele de representation des donnees parole dans un systeme de gestion de base de donnees oriente-objet.
Pour cela, nous avons introduit le concept de donnees primaires : donnees brutes (signaux et images cineradiographiques) ou bien donnees descriptives des locuteurs, des corpus et des conditions d'enregistrements, et des donnees derivees extraites des donnees brutes.
Afin de montrer l'interet d'une base de donnees cineradiographiques, nous avons exploite un corpus enregistre par un locuteur de langue francaise.
La methode d'analyse detaillee des traces du conduit vocal s'est averee interessante, particulierement pour recuperer l'ensemble des gestes articulatoires impliques dans la production caque sequence vcv.
Cette methode nous a aide a suivre de pres les mouvements des articulateurs au niveau de chaque trace et a detecter les debuts des mouvements d'anticipation des levres et de la langue dans les sequences vcv analysees.
L'analyse de ses sequences plaide en faveur des hypotheses du modele d'anticipation m. E. M propose par abry &amp;
Associée à la recherche d'information, la notion de contexte est de plus en plus mobilisée dans les domaines des sciences de l'information, de l'ingénierie des connaissances, des sciences cognitives et de l'informatique.
En effet, le sens d'une expression linguistique, la lecture d'un document la stratégie mise en œuvre dans l'activité de recherche d'information, le raisonnement adopté dans l'opération de classement d'un document, le choix d'avoir recours à tel dispositif, varient fortement d'un contexte à l'autre.
Notre contribution cherche ainsi à éclairer l'environnement informationnel de ce groupe d'acteurs et propose quelques axes de réflexion pour accompagner la construction d'une démarche instrumentée de gestion de l'information en entreprise.
Ce travail associe différents acteurs (chercheurs et ingénieurs) et participe à un projet de recherche appliquée (ANR MIIPA-Doc).
Le peuplement de base de connaissance (KBP) est une tâche importante qui présente de nombreux défis pour le traitement automatique des langues.
Nous nous sommes intéressés à la reconnaissance de relations entre entités.
Généralement, ces relations sont extraites en fonction des informations lexicales et syntaxiques au niveau de la phrase.
Cependant, l'exploitation d'informations globales sur les entités n'a pas encore été explorée.
Nous proposons d'extraire un graphe d'entités du corpus global et de calculer des caractéristiques sur ce graphe afin de capturer des indices des relations entre paires d'entités.
Pour évaluer la pertinence des fonctionnalités proposées, nous les avons testées sur une tâche de validation de relation dont le but est de décider l'exactitude de relations extraites par différents systèmes.
Les résultats expérimentaux montrent que les caractéristiques proposées conduisent à améliorer les résultats de l'état de l'art.
Le travail de thèse consiste en la réalisation de modèles et outils d'analyse sémantique pour l'exploitation de corpus d'entretiens réalisés par une société d'aide à l'innovation.
Ces entretiens sont analysés afin de déterminer si un projet d'innovation répond ou non à plusieurs critères d'acceptabilité.
L'automatisation de cette analyse, au-delà des difficultés d'ambiguïtés généralement rencontrées en analyse sémantique des opinions, doit permettre de traiter des contenus relevant d'une multitude de langues de spécialité.
Il s'agira donc de réaliser des modèles et outils intégrant des procédés généraux de traitement automatique des langues (analyse syntaxique et sémantique) et des ontologies permettant d'intégrer différentes terminologies.
Les plate-formes de Calcul Haute Performance (High Performance Computing, HPC) augmentent en taille et en complexité.
De manière contradictoire, la demande en énergie de telles plates-formes a également rapidement augmenté. Les supercalculateurs actuels ont besoin d'une puissance équivalente à celle de toute une centrale d'énergie.
Dans le but de faire un usage plus responsable de ce puissance de calcul, les chercheurs consacrent beaucoup d'efforts à la conception d'algorithmes et de techniques permettant d'améliorer différents aspects de performance, tels que l'ordonnancement et la gestion des ressources.
Cependent, les responsables des plate-formes HPC hésitent encore à déployer des méthodes d'ordonnancement à la fine pointe de la technologie et la plupart d'entre eux recourent à des méthodes heuristiques simples, telles que l'EASY Backfilling, qui repose sur un tri naïf premier arrivé, premier servi.
Les nouvelles méthodes sont souvent complexes et obscures, et la simplicité et la transparence de l'EASY Backfilling sont trop importantes pour être sacrifiées.
Dans un premier temps, nous explorons les techniques d'Apprentissage Automatique (Machine Learning, ML) pour apprendre des méthodes heuristiques d'ordonnancement online de tâches parallèles.
À l'aide de simulations et d'un modèle de génération de charge de travail, nous avons pu déterminer les caractéristiques des applications HPC (tâches) qui contribuent pour une réduction du ralentissement moyen des tâches dans une file d'attente d'exécution.
La modélisation de ces caractéristiques par une fonction non linéaire et l'application de cette fonction pour sélectionner la prochaine tâche à exécuter dans une file d'attente ont amélioré le ralentissement moyen des tâches dans les charges de travail synthétiques.
Appliquées à des traces de charges de travail réelles de plate-formes HPC très différents, ces fonctions ont néanmoins permis d'améliorer les performances, attestant de la capacité de généralisation des heuristiques obtenues.
Dans un deuxième temps, à l'aide de simulations et de traces de charge de travail de plusieurs plates-formes HPC réelles, nous avons effectué une analyse approfondie des résultats cumulés de quatre heuristiques simples d'ordonnancement (y compris l'EASY Backfilling).
Nous avons également évalué des outres effets tels que la relation entre la taille des tâches et leur ralentissement, la distribution des valeurs de ralentissement et le nombre de tâches mises en calcul par backfilling, par chaque plate-forme HPC et politique d'ordonnancement.
Nous démontrons de manière expérimentale que l'on ne peut que gagner en remplaçant l'EASY Backfilling par la stratégie SAF (Smallest estimated Area First) aidée par backfilling, car elle offre une amélioration des performances allant jusqu'à 80% dans la métrique de ralentissement, tout en maintenant la simplicité et la transparence d'EASY Backfilling.
La SAF réduit le nombre de tâches à hautes valeurs de ralentissement et, par l'inclusion d'un mécanisme de seuillage simple, nous garantonts l'absence d'inanition de tâches.
Dans l'ensemble, nous avons obtenu les remarques suivantes :
(i) des heuristiques simples et efficaces sous la forme d'une fonction non linéaire des caractéristiques des tâches peuvent être apprises automatiquement, bien qu'il soit subjectif de conclure si le raisonnement qui sous-tend les décisions d'ordonnancement de ces heuristiques est clair ou non.
(ii) La zone (l'estimation du temps d'exécution multipliée par le nombre de processeurs) des tâches semble être une propriété assez importante pour une bonne heuristique d'ordonnancement des tâches parallèles, car un bon nombre d'heuristiques (notamment la SAF) qui ont obtenu de bonnes performances ont la zone de la tâche comme entrée.
(iii) Le mécanisme de backfilling semble toujours contribuer à améliorer les performances, bien que cela ne remédie pas à un meilleur tri de la file d'attente de tâches, tel que celui effectué par SAF.
Cette thèse a pour objet l'extraction d'informations relationnelles à partir de documents scientifiques en sciences de la vie, c'est-à-dire la transformation de texte non structuré en information structurée exploitable par une machine.
L'extraction de relations sémantiques spécialisées entre entités détectées dans le texte rend explicite et formalise les structures sous-jacentes.
Les méthodes actuelles à l'état de l'art s'appuient sur de l'apprentissage supervisé.
L'apprentissage supervisé, et particulièrement les méthodes récentes d'apprentissage profond, ont besoin de beaucoup d'exemples d'apprentissages qui sont coûteux à produire, d'autant plus dans les domaines spécialisés comme les sciences de la vie.
Nous faisons l'hypothèse que combiner l'information et la connaissance disponibles dans les domaines spécialisés avec les derniers modèles d'apprentissage profond de plongements lexicaux (word embeddings) peut pallier l'absence ou le nombre réduit de données d'entraînement annotées.
Dans ce but, cette thèse concevra une représentation riche des textes qui s'appuie à la fois sur des informations linguistiques obtenues par analyse syntaxique et sur des connaissances du domaine issues de graphes de connaissance tels que des ontologies.
L'utilisation d'ontologies dans le processus d'extraction d'information facilitera en outre l'intégration de l'information avec d'autres données, telles que des données expérimentales ou analytiques.
Les technologies de l'information et de la communication impactent largement de nombreuses branches du droit.
Le droit des obligations n'y fait pas exception et de nombreux contrats sont désormais conclus en ligne, quel que soit le terminal utilisé.
Le recours à ce moyen de communication n'est pas sans influence sur la perfection du contrat, en particulier sur les modes d'expression de la volonté dans l'univers numérique.
En effet, ce dernier offre de vastes perspectives en termes d'instantanéité, d'immatérialité et d'automatisation de l'expression du consentement contractuel, conduisant à s'interroger sur la validité des contrats formés par voie électronique.
L'observation des pratiques qui se sont installées sur l'internet permet de mesurer aujourd'hui le net impact du numérique sur l'expression du consentement contractuel, c'est-à-dire sur les volontés des internautes cocontractants, ainsi que sur le mécanisme de rencontre de celles-ci.
Les volontés individuelles se sont ainsi vues soumises à un processus constitué d'une série d'étapes obligatoires, supposées limiter les cas dans lesquels la perfection de la convention interviendrait par erreur.
Ce découpage ouvre toutefois la voie à l'automatisation de l'expression des volontés et de leur rencontre, annonçant alors l'ère de contrats conclus voire exécutés en un trait de temps grâce aux récentes avancées de l'intelligence artificielle appliquée au domaine juridique.
La localisation est le processus d'estimation de la position d'une entité dans un système de coordonnées local ou global.
Les applications de localisation sont largement réparties dans des contextes différents.
Dans les événements, le suivi des participants peut sauver des vies pendant des crises.
Dans le domaine de la santé, les personnes âgées peuvent être suivies pour répondre à leurs besoins dans des situations critiques comme les chutes.
Dans les entrepôts, les robots transférant des produits d'un endroit à un autre nécessitent une connaissance précise de ses positions, la position des produits ainsi que des autres robots.
Dans un contexte industriel, la localisation est essentielle pour réaliser des processus automatisés qui sont assez flexibles pour être reconfiguré à diverses missions.
La localisation est considérée comme un sujet de grand intérêt tant dans l'industrie que dans l'académie, en particulier avec l'avènement de la 5G avec son "Enhanced Mobile Broadband (eMBB)" qui devrait atteindre 10 Gbits/s, "Ultra-Reliable Low-Latency Communication (URLLC)" qui est moins d'une milliseconde et "massive Machine-Type Communication (mMTC)" permettant de connecter environ 1 million d'appareils par kilomètre.
Dans ce travail, nous nous concentrons sur deux principaux types de localisation ; la localisation basée sur la distance entre des appareils et la localisation basée sur les empreintes digitales.
Dans la localisation basée sur la distance, un réseau d'appareils avec une distance de communication maximale estime les valeurs de distances par rapport à leurs voisins.
Ces distances ainsi que la connaissance des positions de quelques nœuds sont utilisées pour localiser d'autres nœuds du réseau à l'aide d'une solution basée sur la triangulation.
La méthode proposée est capable de localiser environ 90% des nœuds d'un réseau avec un degré moyen de 10.
Dans la localisation basée sur les empreintes digitales, les réponses des chaînes sans fil sont utilisées pour estimer la position d'un émetteur communiquant avec une antenne MIMO.
Dans ce travail, nous appliquons des techniques d'apprentissage classiques (K-nearest neighbors) et des techniques d'apprentissage en profondeur (Multi-Layer Perceptron Neural Network et Convolutional Neural Networks) pour localiser un émetteur dans des contextes intérieurs et extérieurs.
Notre travail a obtenu le premier prix au concours de positionnement préparé par IEEE Communication Theory Workshop parmi 8 équipes d'universités de grande réputation du monde entier en obtenant une erreur carrée moyenne de 2,3 cm.
Cette thèse se concentre sur le développement de modèles de représentation dense d'objets 3-D à partir d'images.
L'objectif de ce travail est d'améliorer les modèles surfaciques 3-D fournispar les systèmes de vision par ordinateur, en utilisant de nouveaux éléments tirés des images, plutôt que les annotations habituellement utilisées, ou que les modèles basés sur unedivision de l'objet en différents parties.
Des réseaux neuronaux convolutifs (CNNs) sont utilisés pour associer de manière dense les pixels d'une image avec les coordonnées 3-D d'un modèle de l'objet considéré.
Cette méthode permet de résoudre très simplement une multitude de tâches de vision par ordinateur,telles que le transfert d'apparence, la localisation de repères ou la segmentation sémantique, en utilisant la correspondance entre une solution sur le modèle surfacique 3-D et l'image 2-D considérée.
On démontre qu'une correspondance géométrique entre un modèle 3-D et une image peut être établie pour le visage et le corps humains.
Cette étude vise à développer un système d'aide au diagnostic (CAD) pour la détection de lésions épileptogènes, reposant sur l'analyse de données de neuroimagerie, notamment, l'IRM T1 et FLAIR.
L'approche adoptée, introduite précédemment par Azami et al., 2016, consiste à placer la tâche de détection dans le cadre de la détection de changement à l'échelle du voxel, basée sur l'apprentissage d'un modèle one-class SVM pour chaque voxel dans le cerveau.
Les caractéristiques manuelles ne sont pas forcément les plus pertinentes pour la tâche visée.
Notre première contribution porte sur l'intégration de différents réseaux profonds non-supervisés, pour extraire des caractéristiques dans le cadre du problème de détection de changement.
Nous introduisons une nouvelle configuration des réseaux siamois, mieux adaptée à ce contexte.
Le système CAD proposé a été évalué sur l'ensemble d'images IRM T1 des patients atteints d'épilepsie.
Afin d'améliorer la performance obtenue, nous avons proposé d'étendre le système pour intégrer des données multimodales qui possèdent des informations complémentaires sur la pathologie.
Notre deuxième contribution consiste donc à proposer des stratégies de combinaison des différentes modalités d'imagerie dans un système pour la détection de changement.
Ce système multimodal a montré une amélioration importante sur la tâche de détection de lésions épileptogènes sur les IRM T1 et FLAIR.
Notre dernière contribution se focalise sur l'intégration des données TEP dans le système proposé.
Etant donné le nombre limité des images TEP, nous envisageons de synthétiser les données manquantes à partir des images IRM disponibles.
Nous démontrons que le système entraîné sur les données réelles et synthétiques présente une amélioration importante par rapport au système entraîné sur les images réelles uniquement.
Le raisonnement par analogie est reconnu comme une des principales caractéristiques de l'intelligence humaine.
En tant que tel, il a pendant longtemps été étudié par les philosophes et les psychologues, mais de récents travaux s'intéressent aussi à sa modélisation d'un point de vue formel à l'aide de proportions analogiques, permettant l'implémentation de programmes informatiques.
Nous nous intéressons ici à l'utilisation des proportions analogiques à des fins prédictives, dans un contexte d'apprentissage artificiel.
Dans de récents travaux, les classifieurs analogiques ont montré qu'ils sont capables d'obtenir d'excellentes performances sur certains problèmes artificiels, là où d'autres techniques traditionnelles d'apprentissage se montrent beaucoup moins efficaces.
Partant de cette observation empirique, cette thèse s'intéresse à deux axes principaux de recherche.
Le premier sera de confronter le raisonnement par proportion analogique à des applications pratiques, afin d'étudier la viabilité de l'approche analogique sur des problèmes concrets.
Le second axe de recherche sera d'étudier les classifieurs analogiques d'un point de vue théorique, car jusqu'à présent ceux-ci n'étaient connus que grâce à leurs définitions algorithmiques.
Les propriétés théoriques qui découleront nous permettront de comprendre plus précisément leurs forces, ainsi que leurs faiblesses.
Comme domaine d'application, nous avons choisi celui des systèmes de recommandation.
On reproche souvent à ces derniers de manquer de nouveauté ou de surprise dans les recommandations qui sont adressées aux utilisateurs.
Le raisonnement par analogie, capable de mettre en relation des objets en apparence différents, nous est apparu comme un outil potentiel pour répondre à ce problème.
Du côté théorique, une contribution majeure de cette thèse est de proposer une définition fonctionnelle des classifieurs analogiques, qui a la particularité d'unifier les approches préexistantes.
Cette définition fonctionnelle nous permettra de clairement identifier les liens sous-jacents entre l'approche analogique et l'approche par k plus
De plus, nous avons pu identifier un critère qui rend l'application de notre principe d'inférence analogique parfaitement certaine (c'est-à-dire sans erreur), exhibant ainsi les propriétés linéaires du raisonnement par analogie.
Notre objectif a été le développement de biomarqueurs quantitatifs pour caractériser l'organisation géométrique des quatre variants de FN à partir d'images de microscopie confocale 2D, puis de comparer les tissus sains et cancéreux.
Premièrement, nous avons montré à travers deux pipelines de classification fondés sur les curvelets et sur l'apprentissage profond, que les variants peuvent être distingués avec une performance similaire à celle d'un annotateur humain.
Nous avons ensuite construit une représentation des fibres (détectées avec des filtres Gabor) fondée sur des graphes.
Les variantes ont été classés en utilisant des attributs spécifiques aux graphes, prouvant que ceux-ci intègrent des informations pertinentes dans les images confocales.
De plus, nous avons identifié différentes techniques capables de différencier les graphes, afin de comparer les variants de FN quantitativement et qualitativement.
Une analyse des performances sur des exemples simples a montré la capacité des méthodes fondées sur l'appariement de graphes et le transport optimal, de comparer les graphes.
Nous avons ensuite proposé différentes méthodologies pour définir le graphe représentatif d'une certaine classe.
De plus, l'appariement de graphes nous a permis de calculer des cartes de déformation des paramètres entre tissus sains et cancéreux.
Ces cartes ont ensuite été analysées dans un cadre statistique montrant si la variation du paramètre peut être expliquée ou non par la variance au sein d'une même classe.
Dans ce travail, nous étudions ces deux aspects de la mobilité humaine en proposant des algorithmes de machine learning adapté aux sources d'information disponibles dans chacun des contextes.
Pour l'étude de la mobilité en environnement extérieur, nous utilisons les données de coordonnées GPS collectées pour découvrir les schémas de mobilité quotidiens des utilisateurs.
Cette méthode de segmentation est basée sur l'estimation des densités de probabilité des trajectoires, ce qui atténue les problèmes causés par le bruit des données.
Concernant l'étude de la mobilité humaine dans les bâtiments, nous utilisons les données d'empreintes digitales WiFi collectées par les smartphones.
De plus, en ce qui concerne la localisation précise en intérieur, nous supposons qu'il existe une distribution latente régissant l'entrée et la sortie en même temps.
Sur la base de cette hypothèse, nous avons développé un modèle d'apprentissage semi-supervisé basé sur le variational autoencoder (VAE).
Dans la procédure d'apprentissage non supervisé, nous utilisons un modèle VAE pour apprendre une distribution latente de l'entrée qui est composée de données d'empreintes digitales WiFi.
Dans la procédure d'apprentissage supervisé, nous utilisons un réseau de neurones pour calculer la cible, coordonnées par l'utilisateur.
De plus, sur la base de la même hypothèse utilisée dans le modèle d'apprentissage semi-supervisé basé sur le VAE, nous exploitons la théorie des goulots d'étranglement de l'information pour concevoir un modèle basé sur le variational information bottleneck (VIB).
Il s'agit d'un modèle d'apprentissage en profondeur de bout en bout plus facile à former et offrant de meilleures performances.
La TA contextuelle a pour objectif de dépasser cette limitation en explorant différentes méthodes d'intégration du contexte extra-phrastique dans le processus de traduction.
Les phrases environnantes (contexte linguistique) et le contexte de production des énoncés (contexte extra-linguistique) peuvent fournir des informations cruciales pour la traduction, notamment pour la prise en compte des phénomènes discursifs et des mécanismes référentiels.
La prise en compte du contexte est toutefois un défi pour la traduction automatique.
Évaluer la capacité de telles stratégies à prendre réellement en compte le contexte et à améliorer ainsi la qualité de la traduction est également un problème délicat, les métriques d'évaluation usuelles étant pour cela inadaptées voire trompeuses.
Dans cette thèse, nous proposons plusieurs stratégies pour intégrer le contexte, tant linguistique qu'extra-linguistique, dans le processus de traduction.
Nos expériences s'appuient sur des méthodes d'évaluation et des jeux de données que nous avons développés spécifiquement à cette fin.
Nous explorons différents types de stratégies : les stratégies par pré-traitement, où l'on utilise le contexte pour désambiguïser les données fournies en entrée aux modèles ; les stratégies par post-traitement, où l'on utilise le contexte pour modifier la sortie d'un modèle non-contextuel, et les stratégies où l'on exploite le contexte pendant la traduction proprement dite.
Nous nous penchons sur de multiples phénomènes contextuels, et notamment sur la traduction des pronoms anaphoriques, la désambiguïsation lexicale, la cohésion lexicale et l'adaptation à des informations extra-linguistiques telles que l'âge ou le genre du locuteur.
Nos expériences, qui relèvent pour certaines de la TA statistique et pour d'autres de la TA neuronale, concernent principalement la traduction de l'anglais vers le français, avec un intérêt particulier pour la traduction de dialogues spontanés.
Une deuxième difficulté qui handicape grandement les systèmes de dialogue est le fort taux d'erreur du système de reconnaissance vocale.
Certaines propositions récentes permettent de réduire la complexité du modèle.
Un premier suivi de croyance (belief update) est réalisé dans l'espace maitre (en intégrant des observations probabilistes sous forme de listes nbest).
Les tests réels sur des utilisateurs humains montrent qu'un système optimisé par renforcement obtient cependant de meilleures performances sur le critère pour lequel il a été optimisé.
La présence relativement récente de migrants transnationaux invite à porter un regard renouvelé sur les dynamiques d'identification entre « autochtones » et « allochtones » et, plus particulièrement, à en considérer les implications au niveau éducatif.
En donnant des appareils photos jetables à des collégien-nes de la ville de Bozen-Bolzano avec la mission de photographier « les langues du quartier » pour en faire une exposition, il s'agit d'interroger les façons dont, dans une ville qui semble incarner une certaine hybridité, des jeunes interprètent la pluralité qui les environne.
Fournir les infrastructures de calcul nécessaires à la résolution des problèmes complexes de la société moderne constitue un défi stratégique.
Les organisations y répondent classiquement en mettant en place de larges infrastructures de calcul parallèle et distribué.
Les vendeurs de systèmes de Calcul Hautes Performances sont incités par la compétition à produire toujours plus de puissance de calcul et de stockage, ce qui mène à des plateformes "Petascale" spécifiques et sophistiquées, et bientôt à des machines "Exascale".
Ces systèmes sont gérés de manière centralisée à l'aide desolutions logicielles de gestion de jobs et de resources dédiées.
Un problèmecrucial auquel répondent ces logiciels est le problème d'ordonnancement, pourlequel le gestionnaire de resources doit choisir quand, et sur quellesresources exécuter quelle tache calculatoire.
Cette thèse fournit des solutions à ce problème.
Toutes les plateformes sont différentes.
En effet, leur infrastructure, le comportement de leurs utilisateurs et les objectifs del'organisation hôte varient.
Nous soutenons donc que les politiques d'ordonnancement doivent s'adapter au comportement des systèmes.
Dans ce manuscrit, nous présentons plusieurs manières d'obtenir cette adaptativité.
A travers une approche expérimentale, nous étudions plusieurs compromis entre la complexité de l'approche, le gain potentiel, et les risques pris.
Notre problématique porte sur les liens entre logique et topoï, sémantique et topoï et enfin entre logique et sémantique.
Le travail consiste donc à mettre en relief la pérennité de la notion de topos, son évolution et son intégration dans la logique, dans la sémantique et même dans l'informatique.
Dès lors, topoï, logique et sémantique sont les trois facettes de cette problématique.
Il consiste aussi à montrer que cette notion est présente dans toutes les approches linguistiques et que la sémantique est l'axe fédérateur de toutes les recherches linguistiques.
Nous n'avons pas l'intention de donner une définition nouvelle, mais nous avons essayé de faire la synthèse des caractéristiques principales des topoï et de dresser un tableau des recherches passées et actuelles élaborées sur cette notion.
L'approche que nous avons adoptée est constructive et diachronique.
Elle a mis en relief l'évolution des topoï, de leurs formes, de leurs sens et de leurs usages.
La notion de topoï continue à exister de l'antiquité jusqu'à nos jours, puisqu'elle a des fondements sémantiques, qui sont inhérents à toutes les langues naturelles.
La première expose la conception terminologique de la notion de topoï et son aspect évolutif.
Elle met en évidence les différents emplois, les différentes définitions et l'aspect évolutif des topoï.
Dans cette première partie, nous avons essayé de dresser à grands traits l'historique de la notion de topoï, l'emploi qui en a été fait par les logiciens, les rhéteurs, les linguistes, les pragmaticiens et les informaticiens.
La deuxième est consacrée à la présentation des fondements sémantiques de la logique et des topoï et à la relation de complémentarité de ces trois notions, à savoir logique, sémantique et topoï.
Nous avons précisé que la théorie des topoï est une théorie du sens.
Et la troisième partie traite les fondements lexicaux des topoï et les exploitations modernes de cette notion, à savoir le traitement automatique, la théorie des cadres et l'ontologie, tout en présentant les différentes théories sémantiques argumentatives.
Nous avons présenté un petit exemple, un échantillon d'extraction de l'information à l'aide de la plateforme NooJ.
La compréhension du langage naturel repose souvent sur des raisonnements de sens commun, pour lesquels la connaissance de relations sémantiques, en particulier entre prédicats verbaux, peut être nécessaire.
Cette thèse porte sur la problématique de l'utilisation d'une méthode distributionnelle pour extraire automatiquement les informations sémantiques nécessaires à ces inférences de sens commun.
Des associations typiques entre des paires de prédicats et un ensemble de relations sémantiques (causales, temporelles, de similarité, d'opposition, partie/tout) sont extraites de grands corpus, par l'exploitation de la présence de connecteurs du discours signalant typiquement ces relations.
Afin d'apprécier ces associations, nous proposons plusieurs mesures de signifiance inspirées de la littérature ainsi qu'une mesure novatrice conçue spécifiquement pour évaluer la force du lien entre les deux prédicats et la relation.
La pertinence de ces mesures est évaluée par le calcul de leur corrélation avec des jugements humains, obtenus par l'annotation d'un échantillon de paires de verbes en contexte discursif.
Nous évaluons le potentiel de ces représentations pour plusieurs applications.
Concernant l'analyse du discours, les tâches de la prédiction d'attachement entre unités du discours, ainsi que la prédiction des relations discursives spécifiques les reliant, sont explorées.
En utilisant uniquement les traits provenant de notre ressource, nous obtenons des améliorations significatives pour les deux tâches, par rapport à plusieurs bases de référence, notamment des modèles utilisant d'autres types de représentations lexico-sémantiques.
Nous proposons également de définir des ensembles optimaux de connecteurs mieux adaptés à des applications sur de grands corpus, en opérant une réduction de dimension dans l'espace des connecteurs, au lieu d'utiliser des groupes de connecteurs composés manuellement et correspondant à des relations prédéfinies.
Ces applications diverses démontrent les contributions prometteuses amenées par notre approche permettant l'extraction non supervisée de relations sémantiques.
Cette recherche aborde l'étude d'un sous-ensemble de formules expressives de la conversation (FEC) (tu parles, à quoi bon, c'est bon, ça m'étonne) appréhendées sous l'angle de leur analyse et leur description syntaxique, sémantique, pragmatique et discursive.
Cette étude permet d'examiner des questions complexes telles que la terminologie choisie, les valeurs sémantiques, le comportement syntaxique, le rôle du contexte ou les fonctionnements pragmatiques.
Nous nous proposons aussi de réfléchir sur le statut des FEC dans la lexicographie, ce qui permet de mieux approfondir leur description par rapport à ce que proposent les dictionnaires généraux et spécialisés.
Nous avons mené cette étude en quatre temps.
Tout d'abord, sur le plan théorique, nous avons présenté une synthèse des travaux existants en montrant les difficultés de nature terminologique et définitoire liées à ce phénomène linguistique pour lequel une délimitation s'impose.
Pour notre part, nous avons choisi le terme formule expressive et nous avons justifié notre choix à travers la sélection des critères permettant de circonscrire cette notion.
Par la suite, du point de vue méthodologique, l'étude se situe dans l'approche de la linguistique de corpus dans une démarche qualitative et quantitative.
Elle s'appuie sur des corpus envisagés dans plusieurs cadres : l'écrit (littérature — tweets) et l'oral (Orféo).
Après avoir présenté le cadre théorique et méthodologique, nous avons procédé à l'étude concrète à travers l'analyse discursive des FEC dans deux sous-corpus différents.
L'objectif primordial de cette étude est d'aboutir à des critères bien déterminés limitant cette sous-classe de phraséologismes pragmatique par rapport aux autres sous-types proposés par les chercheurs dans ce domaine linguistique.
Finalement, nous avons proposé le traitement lexicographique de certaines formules sélectionnées dans le cadre du projet Polonium.
L'enjeu est ici d'aboutir à un modèle lexicographique fonctionnel pour le traitement lexicographique des FEC.Nous pensons être parvenue à réaliser les enjeux que nous nous étions fixés au départ.
Par une analyse discursive des FEC dans le texte, nous avons voulu affiner les traits définitoires et d'opter pour une définition exhaustive de ce sous-ensemble.
Par l'examen de leur statut dans le domaine de la lexicographie, nous avons retenu une méthode descriptive détaillée et prometteuse des FEC.
La population des villes devrait doubler d'ici le milieu du siècle, selon les estimations de l'OMS.
Cette augmentation rapide de la population a un impact sur les transports et la croissance économique, et accroîtra les responsabilités des autorités de gestion locales.
Nous vivons une transformation des villes en villes intelligentes offrant de nouveaux services à la population, en optimisant l'utilisation des ressources disponibles.
Qu'il s'agisse de données provenant des citoyens, de données gouvernementales ouvertes ou d'autres sources en ligne, une pluralité de sources de données peut permettre la création d'outils intelligents pour gérer efficacement les activités quotidiennes.
De plus, grâce au progrès d'Internet et des technologies mobiles, les plateformes de réseaux sociaux (Twitter) sont devenues des modes de communication populaires.
Elles permettent aux utilisateurs de partager un large éventail d'informations, y compris des données spatio-temporelles.
Ainsi, Il est aisé d'accéder, en temps réel, à des connaissances provenant de différents types de données disponibles, riches, géo-référencées et issues de sources multiples et de les intégrer sur une carte.
Dans cette thèse, nous proposons d'abord un système de recommandation d'itinéraires, tenant compte des contraintes temps réel, en l'absence d'infrastructure physique ; en exploitant les données géolocalisées issues de réseaux sociaux (twitter) pour identifier les contraintes de trafic temps réel et, par conséquent, recommander un chemin optimisé.
Nous avons mis en œuvre un système d'indexation à base de grille spatiale pour notre modèle de prédiction en quasi-temps réel.
Ensuite, nous avons introduit le concept de "cartes intelligentes " intégrant la représentation visuelle de couches de « connaissances pertinentes » par le biais de la collecte, la gestion et l'intégration de sources de données hétérogènes.
Contrairement aux cartes conventionnelles, les cartes intelligentes extraient des informations à partir des événements annoncés et découverts en temps réel (concerts, incidents,...), les offres en ligne et les analyses statistiques (zones dangereuses, …) en encapsulant les données entrantes semi-structurées et non structurées dans des paquets génériques structurés.
Cette méthodologie ouvre la voie à la fourniture de services et applications intelligents.
De plus, le développement de ''cartes intelligentes'' nécessite un traitement efficace et évolutif et la visualisation de couches basées sur les connaissances à plusieurs échelles cartographiques, permettant ainsi une navigation fluide et sans encombre.
Enfin, nous présentons Hadath, un système évolutif et efficace qui extrait les événements sociaux d'une multitude de flux de données non structurés.
Le système comprend un composant de gestion et de prétraitement des différents types de sources de données et génère des paquets de données structurés à partir de flux non structurés.
Par conséquent, les événements détectés sont affichés à différentes résolutions spatio-temporelles, ce qui permet une navigation fluide.
Enfin, pour valider notre approche, nous avons mené des expériences sur des flux de données réelles.
Le résultat final du système proposé, nommé Hadath crée une expérience unique et dynamique de navigation cartographique.
Ce manuscrit porte sur la reconnaissance automatique du locuteur indépendante du texte en utilisant des paramètres de régression linéaire par maximum de vraisemblance (MLLR).
Ces paramètres sont obtenus par l'adaptation d'un modèle acoustique indépendant du locuteur aux données de parole d'un locuteur et sont des indices pertinents qui caractérisent ce locuteur.
Nous utilisons le paradigme MLLR-SVM qui classifie ces coefficients avec une Machine à Vecteurs Support (SVM).
Nous proposons une approche purement acoustique qui n'utilise pas de transcriptions tout en évitant de dépendre de la langue en utilisant des transformations MLLR contraintes (CMLLR) et l'apprentissage d'un modèle du monde adapté au locuteur (SAT).
Nous explorons les systèmes multi-classe (C)MLLR-SVM basés sur des modèles acoustiques phonémiques.
Une étude expérimentale complète des schémas d'adaptation est réalisée sur de multiples axes tels que le type de paramètres cepstraux, le type et le nombre de transformations, le type de modèle et la méthode d'apprentissage.
Pourquoi et comment les grandes entreprises traitent-elles les réclamations de leur clientèle ?
Quels effets ce traitement a-t-il sur les régulations internes des firmes ?
Que peut espérer le réclamant ?
Cette thèse se propose de traiter ce faisceau de questions par une enquête ethnographique menée dans deux grandes entreprises françaises.
S'appuyant sur l'outillage analytique développé par Albert O. Hirschman, elle propose une description historique et sociologique des pratiques de traitement des réclamations.
Ainsi, elle souhaite contribuer à la problématique de l'influence du destinataire final d'une marchandise sur les entreprises qui la produisent et la vendent.
Une jurisprudence est un corpus de décisions judiciaires représentant la manière dont sont interprétées les lois pour résoudre un contentieux.
Elle est indispensable pour les juristes qui l'analysent pour comprendre et anticiper la prise de décision des juges.
Son analyse exhaustive est difficile manuellement du fait de son immense volume et de la nature non-structurée des documents.
L'estimation du risque judiciaire par des particuliers est ainsi impossible car ils sont en outre confrontés à la complexité du système et du langage judiciaire.
L'automatisation de l'analyse des décisions permet de retrouver exhaustivement des connaissances pertinentes pour structurer la jurisprudence à des fins d'analyses descriptives et prédictives.
Afin de rendre la compréhension d'une jurisprudence exhaustive et plus accessible, cette thèse aborde l'automatisation de tâches importantes pour l'analyse métier des décisions judiciaires.
En premier, est étudiée l'application de modèles probabilistes d'étiquetage de séquences pour la détection des sections qui structurent les décisions de justice, d'entités juridiques, et de citations de lois.
Ensuite, l'identification des demandes des parties est étudiée.
L'approche proposée pour la reconnaissance des quanta demandés et accordés exploite la proximité entre les sommes d'argent et des termes-clés appris automatiquement.
Nous montrons par ailleurs que le sens du résultat des juges est identifiable soit à partir de termes-clés prédéfinis soit par une classification des décisions.
Enfin, pour une catégorie donnée de demandes, les situations ou circonstances factuelles où sont formulées ces demandes sont découvertes par regroupement non supervisé des décisions.
A cet effet, une méthode d'apprentissage d'une distance de similarité est proposée et comparée à des distances établies.
Cette thèse discute des résultats expérimentaux obtenus sur des données réelles annotées manuellement.
Le mémoire propose pour finir une démonstration d'applications à l'analyse descriptive d'un grand corpus de décisions judiciaires françaises.
Les matériels mobiles actuels, et les téléphones mobiles en particulier, sont équipés de différentes technologies sans fil qui augmentent et diversifient leurs capacités de communication.
L'utilisation combinée et efficace de ces technologies offre des possibilités variées et accrues en termes de services et d'applications.
Néanmoins elle requiert la réalisation d'analyses fines en matières de sécurité et de choix du mode de communication à utiliser en fonction de critères dépendant du contexte : coût énergétique, coût financier, préférences des entités impliquées, préservation de la vie privée, etc.
Notre contribution à ce projet est la création d'applications collaboratives qui utilisent de façon appropriée la gamme des technologies sans fil disponibles sur les matériels considérés.
En d'autres termes, on cherche à utiliser les moyens de transmission les plus appropriés (au sens des critères indiqués plus haut) que deux ou plusieurs équipements mobiles peuvent utiliser pour réaliser leurs échanges, qui plus est, sans que cela ne nécessite de connaître leurs positions respectives.
La transparence de la localisation des cibles devient ainsi une règle.
On peut synthétiser la question centrale que nous avons choisie d'étudier de la manière suivante : comment faire communiquer un ensemble de terminaux mobiles (des téléphones portables en particulier) de façon sécurisée en utilisant la technologie la plus adaptée en fonction du contexte ?
Notre objectif est de proposer une réponse à cette question en définissant une plate-forme multi-niveaux prenant en compte les différentes technologies disponibles sur les équipements considérés.
Il s'agit en particulier d'identifier l'ensemble des éléments à prendre en compte dans la conception de la plate-forme, de les modéliser, de développer des applications de référence et de valider la pertinence des solutions proposées par des tests, ainsi que des évaluations qualitatives et quantitatives.
Cette thèse propose une nouvelle approche des écrits scientifiques en prenant comme point de départ les marqueurs discursifs (MD).
Elle s'inscrit dans le cadre du Français sur Objectif Universitaire (FOU).
Dans ce travail, nous nous intéressons tout particulièrement aux MD polylexicaux et les intégrons dans une conception large de la phraséologie.
La particularité de cette recherche réside dans le fait de relier les descriptions linguistiques des MD et la transposition didactique de ces unités lexicales à l'aide de corpus, ce qui est encore peu abordé dans le champ de la didactique francophone.
Nous cherchons à répondre à des objectifs à la fois linguistiques et didactiques.
Pour les objectifs linguistiques, nous mettons en place un modèle d'analyse des MD polylexicaux associant les propriétés syntaxiques et sémantiques et qui est tout à fait réadaptable à d'autres MD.
Les analyses linguistiques des MD serviront par la suite à l'enseignement/apprentissage de ces unités.
Pour les objectifs didactiques, cette recherche vise à concevoir une méthodologie d'enseignement/apprentissage des MD à partir de l'observation de corpus.
Les considérations méthodologiques proposées dans le cadre de cette thèse ouvrent des pistes intéressantes pour l'enseignement/apprentissage de ces éléments linguistiques ainsi que pour faciliter l'accès aux écrits scientifiques auprès des étudiants non-natifs.
Les humains sont au coeur de nombreux problèmes de vision par ordinateur, tels que les systèmes de surveillance ou les voitures sans pilote.
Ils sont également au centre de la plupart des contenus visuels, pouvant amener à des jeux de données très larges pour l'entraînement de modèles et d'algorithmes.
Par ailleurs, si les données stéréoscopiques font l'objet d'études depuis longtemps, ce n'est que récemment que les films 3D sont devenus un succès commercial.
Dans cette thèse, nous étudions comment exploiter les données additionnelles issues des films 3D pour les tâches d'analyse des personnes.
Nous explorons tout d'abord comment extraire une notion de profondeur à partir des films stéréoscopiques, sous la forme de cartes de disparité.
En s'appuyant sur la relative facilité de la tâche de détection de personne dans les films 3D, nous développons une méthode pour collecter automatiquement des exemples de personnes dans les films 3D afin d'entraîner un détecteur de personne pour les films non 3D. Nous nous concentrons ensuite sur la segmentation de plusieurs personnes dans les vidéos.
Nous formulons ce problème comme un problème d'étiquetage de graphe multi-étiquettes, et notre méthode intègre un modèle des occlusions pour produire une segmentation multi-instance par plan.
Après avoir montré l'efficacité et les limitations de cette méthode, nous proposons un second modèle, qui ne repose lui que sur des détections de personne à travers la vidéo, et pas sur des estimations de posture.
Nous formulons ce problème comme la minimisation d'un coût quadratique sous contraintes linéaires.
Ces contraintes encodent les informations de localisation fournies par les détections de personne.
Cette méthode ne nécessite pas d'information de posture ou des cartes de disparité, mais peut facilement intégrer ces signaux supplémentaires.
Elle peut également être utilisée pour d'autres classes d'objets.
Nous évaluons tous ces aspects et démontrons la performance de cette nouvelle méthode.
Les relations d'équivalences entre les différentes requêtes permettent de réunir les participants au sein de communautés d'intérêt.
Celles-ci forment alors une abstraction permettant de séparer le problème d'organisation du système en plusieurs sous-problèmes plus simples et de taille réduite.
Afin de garantir une généricité vis-à-vis du langage, l'organisation repose sur une API simple et modulable.
Le choix entre ces différentes possibilités est ensuite effectué à l'aide d'un modèle de coût paramétrable.
Les relations entre communautés sont concrétisées par un échange de ressources entre elles, un participant de l'une venant contribuer à l'autre.
Cela permet de s'affranchir des limitations de capacités au niveau abstrait, tout en en tenant hautement compte pour la mise en relation effective des participants.
Au sein des communautés, un arbre de diffusion permet à l'ensemble des participants de récupérer les résultats requis.
L'approche, mise en œuvre de manière incrémentale, permet une réduction efficace des coûts de calcul et de diffusion (l'optimalité est atteinte, notamment, dans le cas de l'inclusion de requête) pour un coût d'organisation limité et une latence raisonnable.
Les expérimentations réalisées ont montré une grande adaptabilité aux variations concernant les requêtes exprimées et les capacités des participants.
Le démonstrateur mis en place peut être utilisé à la fois pour des simulations (automatiques ou interactives) et pour un déploiement réel, par une implémentation commune générique vis-à-vis du langage.
L'extraction d'information non supervisée en domaine ouvert est une évolution récente de l'extraction d'information adaptée à des contextes dans lesquels le besoin informationnel est faiblement spécifié.
Dans ce cadre, la thèse se concentre plus particulièrement sur l'extraction et le regroupement de relations entre entités en se donnant la possibilité de traiter des volumes importants de données.
L'extraction de relations se fixe plus précisément pour objectif de faire émerger des relations de type non prédéfini à partir de textes.
Leur extraction est réalisée en deux temps : des relations candidates sont d'abord extraites sur la base de critères simples mais efficaces pour être ensuite filtrées selon des critères plus avancés.
Ce filtrage associe lui-même deux étapes : une première étape utilise des heuristiques pour éliminer rapidement les fausses relations en conservant un bon rappel tandis qu'une seconde étape se fonde sur des modèles statistiques pour raffiner la sélection des relations candidates.
Il est réalisé dans le cas présent selon une stratégie multiniveau permettant de prendre en compte à la fois un volume important de relations et des critères de regroupement élaborés.
Un premier niveau de regroupement, dit de base, réunit des relations proches par leur expression linguistique grâce à une mesure de similarité vectorielle appliquée à une représentation de type « sac-de-mots » pour former des clusters fortement homogènes.
Un second niveau de regroupement est ensuite appliqué pour traiter des phénomènes plus sémantiques tels que la synonymie et la paraphrase et fusionner des clusters de base recouvrant des relations équivalentes sur le plan sémantique.
Ce second niveau s'appuie sur la définition de mesures de similarité au niveau des mots, des relations et des clusters de relations en exploitant soit des ressources de type WordNet, soit des thésaurus distributionnels.
Enfin, le travail illustre l'intérêt de la mise en œuvre d'un clustering des relations opéré selon une dimension thématique, en complément de la dimension sémantique des regroupements évoqués précédemment.
Ce clustering est réalisé de façon indirecte au travers du regroupement des contextes thématiques textuels des relations.
Pour les mesures externes, une méthode interactive est proposée pour construire manuellement un large ensemble de clusters de référence.
Son application sur un corpus journalistique de grande taille a donné lieu à la construction d'une référence vis-à-vis de laquelle les différentes méthodes de regroupement proposées dans la thèse ont été évaluées.
L'analyse linguistique est une étape fondamentale et essentielle pour le traitement automatique des langues.
En effet, elle permet d'étiqueter les mots avec des catégories morphosyntaxiques et d'identifier des entités nommées pour pouvoir réaliser des applications du plus haut niveau, par exemple la recherche d'information, la traduction automatique, la question réponse, etc.
Puisque le mot est l'unité essentielle d'une langue, une segmentation des phrases en mots est indispensable pour le traitement du chinois.
Parmi des études existantes, la segmentation, l'étiquetage morphosyntaxique et l'identification des entités nommées sont souvent enchaînés comme les étapes différentes.
C'est pourquoi des modèles statistiques qui réalisent la segmentation, l'étiquetage morphosyntaxique et l'identification des entités nommées ou la segmentation et l'un des deux autres traitements simultanément, ont été créés.
Par conséquent, cette approche n'est pas approprie pour créer des systèmes d'analyse automatique multilingue.
L'objectif de mon étude consiste à intégrer l'analyse automatique du chinois dans un système d'analyse
D'abord, des traitements pour le chinois doivent être compatibles avec ceux d'autres langues.
Ensuite, pour garder la cohérence et l'unité du système, il est favorable d'employer au maximum des modules de traitement en commun pour toutes les langues traitées par le système.
En conséquence, le choix s'est porté sur l'utilisation des modules séparés pour la segmentation, l'étiquetage morphosyntaxique et l'identification des entités nommées.
Néanmoins, ce type de méthodes enchaînant des trois traitements ne prend pas en compte des dépendances entre eux.
Pour surmonter ce défaut, nous utilisons les informations fournies par l'analyse morphosyntaxique, par l'identification des entités nommées et par des connaissances linguistiques afin d'améliorer la segmentation.
Etant donné ces interdépendances, trois traitements spécifiques sont rajoutés au système : un prétraitement avant la segmentation basée sur le modèle de cooccurrence, une tokenization de termes liés aux chiffres écrits en caractères chinois et un traitement complémentaire pour la segmentation en identifiant certaines entités nommées entre l'étape de la segmentation et celle de l'étiquetage morphosyntaxique.
Les systèmes déterminatifs du français et du polonais sont différents.
Nos descriptions sont formalisées dans des bases de données dédiées au traitement automatique des langues.
Nous rendons compte systématiquement du fonctionnement et des propriétés des déterminants polonais.
Nous retenons tout particulièrement leurs propriétés syntactico-sémantiques selon qu'ils relèvent de la détermination prédicative et de la détermination argumentale.
Le tout contribue à l'élaboration des dictionnaires électroniques du LDI.
Afin d'accompagner les professionnels de santé dans leur démarche clinique, plusieurs systèmes de suivi et deprise en charge médicale ont été construits et déployés dans le milieu hospitalier.
Ces systèmes permettent principalement de collecter des données médicales sur les patients, de les analyser et de présenter les résultats de différentes manières.
Ils représentent un appui et une aide aux professionnels de santé dans leur prise de décision par rapport à l'évolution de l'état de santé des patients suivis.
L'utilisation de tels systèmes nécessite systématiquement une adaptation à la fois au domaine médical concerné et au mode d'intervention.
Il est nécessaire, dans un milieu hospitalier, que ces systèmes puissent s'adapter et évoluer d'une manière simple, en limitant toute maintenance corrective ou évolutive.
Ils doivent être en mesure de prendre en compte dynamiquement des connaissances théoriques et empiriques du domaine issues des experts médicaux.
Cette approche permet notamment l'organisation de la collecte des données médicales, en tenant compte du contexte du patient, la représentation des connaissances du domaine à base d'ontologies ainsi que leur exploitation associée aux guides de bonnes pratiques et à l'expérience clinique.
Dans la continuité des travaux précédemment réalisés au sein de notre équipe de recherche, nous avons choisi d'enrichir, avec notre approche, la plateforme E-care qui est dédiée au suivi et à la détection précoce de toute anomalie de l'état de patients atteints de maladies chroniques.
Nous avons pu ainsi adapter facilement la plateforme E-care aux différentes expérimentations qui sont été menées notamment dans des EPHAD de la Mutualité Française en Anjou-Mayenne, au CHU de Hautepierre et au CHUV à Lausanne.
Les résultats de ces expérimentations ont montré l'efficacité de l'approche proposée.
L'adaptation de la plateforme par rapport au domaine et au mode d'intervention de chacune de ces expérimentations se limite à de la simple configuration.
De plus, l'approche proposée a suscité l'intérêt du personnel médical par rapport à l'organisation de la collecte des données, qui tient compte du contexte du patient, et par rapport à l'exploitation des connaissances médicales qui apporte aux professionnels de santé une assistance pour une meilleure prise de décision.
Nous exposons d'abord une méthode de suivi basée sur un filtre particulaire, permettant de déterminer à tout moment la position de la tête, des coudes, du buste et des mains d'un signeur dans une vidéo monovue.
Cette méthode a été adaptée à la LSF pour la rendre plus robuste aux occultations, aux sorties de cadre et aux inversions des mains du signeur.
Ensuite, l'analyse de données issues de capture de mouvements nous permet d'aboutir à une catégorisation de différents mouvements fréquemment utilisés dans la production de signes.
Nous en proposons un modèle paramétrique que nous utilisons dans le cadre de la recherche de signes dans une vidéo, à partir d'un exemple vidéo de signe.
L'analyse en dépendances est une composante essentielle de nombreuses applications de TAL (Traitement Automatique des Langues),
Ce type d'analyse est dès lors limité à quelques langues seulement, qui disposent des ressources adéquates.
Pour les langues peu dotées, la production de données annotées est une tâche impossible le plus souvent, faute de moyens et d'annotateurs disponibles.
Afin de résoudre ce problème, la thèse examine trois méthodes d'amorçage, à savoir (1) l'apprentissage par transfert multilingue, (2) les plongements vectoriels contextualisés profonds et (3) le co-entrainement.
La première idée, l'apprentissage par transfert multilingue, permet de transférer des connaissances d'une langue pour laquelle on dispose de nombreuses ressources, et donc de traitepments efficaces, vers une langue peu dotée.
Les plongements vectoriels contextualisés profonds, quant à eux, permettent une représentation optimale du sens des mots en contexte, grâce à la notion de modèle de langage.
Nos approches ne nécessitent qu'un petit dictionnaire bilingue ou des ressources non étiquetées faciles à obtenir (à partir de Wikipedia par exemple) pour améliorer la précision de l'analyse pour des langues où les ressources disponibles sont insuffisantes.
Nous avons évalué notre analyseur syntaxique sur 57 langues à travers la participation aux campagnes d'évaluation proposées dans le cadre de la conférence CoNLL.
Notre système a obtenu des résultats très compétitifs lors de campganes d'évaluation officielles, notamment lors des campagnes CoNLL 2017 et 2018.
Cette thèse offre donc des perspectives intéressantes pour le traitement automatique des langues peu dotées, un enjeu majeur pour le TAL dans les années à venir.
Une entreprise, et particulièrement une PME ou une PMI, doit être apte à évoluer sur des secteurs d'activités souvent très concurrentiels qui évoluent rapidement, par exemple, en fonction d'une clientèle volatile et soucieuse de trouver des produits et des services moins chers et plus adaptés à ses besoins.
La PME se trouve alors confrontée à des problèmes de réactivité et de flexibilité face à cette clientèle.
Par effet direct, elle recherche à réduire les délais et les coûts de réalisation tout en privilégiant aussi la qualité et le degré d'innovation des biens et des services qu'elle propose.
Le système d'information de cette PME est un enjeu essentiel pour mettre en œuvre cette stratégie et maximiser donc la réactivité et la flexibilité mais aussi la rentabilité et la qualité recherchées.
Ce sont des qualités incontournables, garantes d'une autonomie et d'une reconnaissance dont la PME a grand besoin.
Une partie de ce système d'information est de fait informatisée.
Celui-ci supporte, mémorise et traite les informations nécessaires aux différents processus de décision, métier et support qui tapissent l'organisation pour servir la stratégie de l'entreprise.
Les fonctionnalités, les interfaces et les données qui forment ce système informatisé sont donc cruciales à comprendre, à développer en accord avec les besoins de la PME, à améliorer au fur et à mesure de l'évolution de ces besoins.
La PME est donc tentée de se lancer, seule ou accompagnée, dans des projets dits d'informatisation i.e. des projets visant le développement ou l'amélioration de son système informatisé.
Nous nous intéressons ici à des projets visant à développer des applicatifs de gestion et de pilotage de la PME.
La PME – prenant alors le rôle de la maîtrise d'ouvrage (MOA) – tout comme la société de services qui l'accompagne – prenant alors en charge le rôle de maîtrise d'œuvre (MOE) – doivent partager une vision commune des besoins d'informatisation.
Elles sont alors appelées à mener en commun des activités d'ingénierie des besoins et des exigences (IBE).
L'IBE guide et accompagne la PME pour arriver à décrire et formaliser ses besoins.
Elle permet ensuite à la société de service de spécifier de manière plus formelle ces besoins sous forme d'exigences qui définissent alors les travaux de développement souhaités.
L'IBE est souvent réalisée avec une assistance à maitrise d'ouvrage.
Cette étape cruciale reste cependant difficile pour une PME.
Elle est de plus souvent réalisée par la MOE elle-même pour faire face au manque de moyens, de temps et de compétences de la PME.
Or, l'implication des collaborateurs de la PME est primordiale pour la réussite de tout projet d'informatisation, surtout si celui-ci impacte durablement le fonctionnement de la PME.
Ces travaux, développés dans le cadre d'une collaboration Industrie/recherche avec la SSII RESULIS, ont consisté à développer une méthode d'IBE qui offre aux PME des concepts, des langages et des moyens de modélisation et de vérification simples mais suffisants tout en tant aisément manipulables de manière intuitive et donnant lieu à une formalisation pertinente pour la MOE.
Cette méthode est basée sur le croisement et la complémentarité de principes issus de la Modélisation d'Entreprise et de l'Ingénierie Système pour l'élicitation de besoins.
Des moyens de vérification et de validation semi-formels sont appliqués pour garantir certaines qualités attendues des exigences résultantes.
La méthode s'intègre également au cycle de développement basé sur les modèles pour permettre a posteriori d'accélérer la production de prototypes et de rendre interopérables les langages et outils de la MOA et de la MOE.
Les sciences participatives, et en particulier la myriadisation (crowdsourcing) bénévole, représentent un moyen peu exploité de créer des ressources langagières pour certaines langues encore peu dotées, et ce malgré la présence de locuteurs sur le Web.
Nous présentons dans ce travail les expériences que nous avons menées pour permettre la myriadisation de ressources langagières dans le cadre du développement d'un outil d'annotation automatique en parties du discours.
Nous avons appliqué cette méthodologie à trois langues non standardisées, en l'occurrence l'alsacien, le créole guadeloupéen et le créole mauricien.
Pour des raisons historiques différentes, de multiples pratiques (ortho)graphiques co-existent en effet pour ces trois langues.
Les difficultés posées par l'existence de cette variation nous ont menée à proposer diverses tâches de myriadisation permettant la collecte de corpus bruts, d'annotations en parties du discours, et de variantes graphiques.
L'analyse intrinsèque et extrinsèque de ces ressources, utilisées pour le développement d'outils d'annotation automatique, montrent l'intérêt d'utiliser la myriadisation dans un cadre linguistique non standardisé : les locuteurs ne sont pas ici considérés comme un ensemble uniforme de contributeurs dont les efforts cumulés permettent d'achever une tâche particulière, mais comme un ensemble de détenteurs de connaissances complémentaires.
Les ressources qu'ils produisent collectivement permettent de développer des outils plus robustes à la variation rencontrée.
Les plateformes développées, les ressources langagières, ainsi que les modèles de taggers entraînés sont librement disponibles.
Depuis son émergence au début des années 1990, la notion d'ontologie s'est rapidement diffusée dans un grand nombre de domaines de recherche.
Compte tenu du caractère prometteur de cette notion, de nombreux travaux portent sur l'utilisation des ontologies dans des domaines aussi divers que la recherche d'information, le commerce électronique, le web sémantique, l'intégration de données, etc.
L'efficacité de tous ces travaux présuppose l'existence d'une ontologie de domaine susceptible d'être utilisée.
Or, la conception d'une telle ontologie s'avère particulièrement difficile si l'on souhaite qu'elle fasse l'objet de consensus.
S'il existe des outils utilisés pour éditer une ontologie supposée déjà conçue, et s'il existe également plusieurs plate-formes de traitement automatique de la langue permettant d'analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il est difficile de trouver une procédure globalement acceptée, ni a fortiori un ensemble d'outils supports permettant de concevoir une ontologie de domaine de façon progressive, explicite et traçable à partir d'un ensemble de ressources informationnelles relevant de ce domaine.
L'objectif du projet ANR DaFOE4App (Differential and Formal Ontologies Editor for Application), dans lequel s'inscrit notre travail, était de favoriser l'émergence d'un tel ensemble d'outils.
Contrairement à d'autres outils de construction d'ontologies, la plate-forme DaFOE, présentée dans cette thèse, ne propose pas un processus de construction figé ni en nombre d'étapes, ni sur la représentation des étapes.
En effet, dans cette thèse nous généralisons le processus de construction d'ontologies pour un nombre quelconque d'étapes.
L'intérêt d'une telle généralisation étant, par exemple, d'offrir la possibilité de raffiner le processus de construction en insérant ou modifiant des étapes.
On peut également souhaiter supprimer certaines étapes à fin de simplifier le processus de construction.
L'objectif de cette généralisation est de minimiser l'impact de l'ajout, suppression ou modification d'une étape dans le processus global de construction d'ontologies, tout en préservant la cohérence globale du processus de construction.
Pour y parvenir, notre approche consiste à utiliser l'Ingénierie Dirigée par les Modèles pour caractériser chaque étape au sein d'un modèle et ensuite ramener le problème du passage d'une étape à l'autre à un problème de mapping de modèles.
Les mappings établis entre les modèles sont ensuite utilisés pour semi-automatiser le processus de construction d'ontologies.
L'originalité du langage MQL se trouve dans sa capacité, au travers de requêtes syntaxiquement compactes, à explorer transitivement tout ou partie du graphe de mappings lors d'une recherche d'informations.
La problématique traitée par la Reconnaissance de la Langue (LR) porte sur la définition découverte de la langue contenue dans un segment de parole.
Cette thèse se base sur des paramètres acoustiques de courte durée, utilisés dans une approche d'adaptation de mélanges de Gaussiennes (GMM-UBM).
Le problème majeur de nombreuses applications du vaste domaine de la re- problème connaissance de formes consiste en la variabilité des données observées.
Dans le contexte de la Reconnaissance de la Langue (LR), cette variabilité nuisible est due à des causes diverses, notamment les caractéristiques du locuteur, l'évolution de la parole et de la voix, ainsi que les canaux d'acquisition et de transmission.
Dans le contexte de la reconnaissance du locuteur, l'impact de la variabilité solution peut sensiblement être réduit par la technique d'Analyse Factorielle (Joint Factor Analysis, JFA).
Dans ce travail, nous introduisons ce paradigme à la Reconnaissance de la Langue.
La deuxième hypothèse, plus technique, est que la variabilité nuisible se situe dans un sous-espace de faible dimension, qui est défini de manière globale.
Dans ce travail, nous analysons le comportement de la JFA dans le contexte d'un dispositif de LR du type GMM-UBM.
Nous introduisons et analysons également sa combinaison avec des Machines à Vecteurs Support (SVM).
Les premières publications sur la JFA regroupaient toute information qui est amélioration nuisible à la tâche (donc ladite variabilité) dans un seul composant.
Celui-ci est supposé suivre une distribution Gaussienne.
En pratique, nous observons que cette hypothèse n'est pas toujours vérifiée.
Nous avons, par exemple, le cas où les données peuvent être groupées de manière logique en deux sous-parties clairement distinctes, notamment en données de sources téléphoniques et d'émissions radio.
Dans ce cas-ci, nos recherches détaillées montrent un certain avantage à traiter les deux types de données par deux systèmes spécifiques et d'élire comme score de sortie celui du système qui correspond à la catégorie source du segment testé.
Afin de sélectionner le score de l'un des systèmes, nous avons besoin d'un analyses détecteur de canal source.
Nous proposons ici différents nouveaux designs pour engendrées de tels détecteurs automatiques.
Dans ce cadre, nous montrons que les facteurs de variabilité (du sous-espace) de la JFA peuvent être utilisés avec succès pour la détection de la source.
Ceci ouvre la perspective intéressante de subdiviser les5données en catégories de canal source qui sont établies de manière automatique.
L'approche JFA permet une réduction de la mesure de coûts allant jusqu'à généraux 72% relatives, comparé au système GMM-UBM de base.
En utilisant des systèmes spécifiques à la source, suivis d'un sélecteur de scores, nous obtenons une amélioration relative de 81%.
Depuis l'explosion du Web, la Recherche d'Information (RI) s'est vue étendue et les moteurs de recherche sur le Web ont vu le jour.
Les méthodes classiques de la RI, surtout destinées à des recherches textuelles simples, se sont retrouvées face à des documents de différents formats et des contenus riches.
L'utilisateur, en réponse à cette avancée, est devenu plus exigeant quant aux résultats retournés par les systèmes de RI.
La personnalisation tente de répondre à ces exigences en ayant pour objectif principal l'amélioration des résultats retournés à l'utilisateur en fonction de sa perception et de ses intérêts ainsi que de ses préférences.
Le présent travail de thèse se situe à la croisée des différents aspects présentés et couvre cette problématique.
Elle a pour objectif principal de proposer des solutions nouvelles et efficaces à cette problématique.
Pour atteindre cet objectif, un système de personnalisation de la recherche spatiale et sémantique sur le Web et intégrant la modélisation de l'utilisateur, a été proposé.
Ce système comprend deux volets : 1/ la modélisation de l'utilisateur ; 2/ la collaboration implicite des utilisateurs à travers la construction d'un réseau de modèles utilisateurs, construit itérativement lors des différentes recherches effectuées en ligne.
Ainsi, nous avons effectué un ensemble d'évaluation, dont les principales sont : a) l'évaluation de la qualité du modèle de l'utilisateur ; b) l'évaluation de l'efficacité de la recherche d'information ; c) l'évaluation de l'efficacité de la recherche d'information intégrant les informations spatiales ; d) l'évaluation de la recherche exploitant le réseau d'utilisateurs.
Les expérimentations menées montrent une amélioration de la personnalisation des résultats présentés par rapport à ceux obtenus par d'autres moteurs de recherche.
Des connaissances sémantiques sont obligatoires pour le Traitement Automatique des Langues.
Malheureusement, les classifications à visée universelle sont une utopie.
Il existe des systèmes d'extraction de connaissances sémantiques des textes de spécialité par des approches terminologiques mais il est largement reconnu qu'il n'est pas possible d'effectuer une telle extraction de textes de la langue dite " générale ".
Cette thèse a pour but de montrer que cette idée est fausse.
Nous montrons qu'une analyse thématique de textes non spécialisés (journaux, dépêches de presse en texte intégral ou pages HTML moissonnées sur le Web) permet la plupart du temps de se ramener dans le cadre d'un problème classique de traitement de corpus spécialisé, tout en nécessitant des interventions humaines très réduites.
Dans notre approche, le thème des segments de textes est détecté par l'analyse statistique des distributions des mots.
Après avoir défini des notions de similarité et d'agrégation, les mots des segments similaires sont agrégés pour former des domaines thématiques dans lesquels les mots de poids élevés décrivent un thème.
On regroupe les noms qui apparaissent comme argument d'un même verbe dans les divers segments de texte appartenant à un certain thème, ce qui forme des classes.
SVETLAN', qui a été testé sur des corpus de plusieurs millions de mots en français et en anglais.
L'analyse empirique des résultats montre que, comme prévu, les mots sont très souvent en relation sémantique forte les uns avec les autres dans les classes obtenues, et ce dans le contexte déterminé par le thème.
Ce projet de thèse porte sur la modélisation des variations acoustico-prosodique des unités informationnelles courtes de la parole spontanée pour permettre leur classification en grandes catégories sémantiques reliées à leur utilisation pragmatique.
Le travail se fonde sur la Théorie du langage en action (Language into Act Theory, désormais L-AcT, Cresti 2000 ; Moneglia and Raso, 2014 ; Cavalcante 2016, 2020), pour laquelle les fonctions informationnelles, y compris illocutoires, des interactions parlées spontanées sont essentiellement orientées par la prosodie.
La parole est segmentée en unités intonatives encapsulant un certain nombre de mots dans la même enveloppe prosodique.
Cela établit un contraste entre les mots des différentes enveloppes prosodiques : la segmentation guide ainsi un découpage fonctionnel du discours (Barth-Weingarten, 2016 ; Barbosa and Raso, 2018 ; Izre'el et al., à paraître-a ; Izre'el et al., à paraître-b).
La L-AcT propose qu'il existe une relation isomorphe entre l'unité intonative et l'unité informationnelle : chaque unité intonative composant l'énoncé acquiert une valeur informationnelle, à l'exception des unités de scansion, produites volontairement (pour relever un point) ou involontairement (problèmes de performance).
La compositionnalité syntaxique est une propriété des unités informationnelles, qui établissent des relations fonctionnelles guidées par les contours intonatifs, sans que la compositionnalité syntaxique y joue un rôle nécessaire (Cresti 2014).
Dans le flux discursif, on distingue les frontières prosodiques à valeur terminale ou non-terminale, qui peuvent être détectées automatiquement de bons niveaux de rappel et de précision, sur la base de leurs corrélats prosodiques (Teixeira, 2018 ; Teixeira, Barbosa and Raso, 2018 ; Raso, Teixeira and Barbosa, à paraître).
Les frontières terminales marquent la fin d'une séquence composée d'unités intonatives non-terminales, marquées par les frontières non-terminales et délimitant les unités informationnelles.
Cette thèse traite de l'analyse des unités informationnelles courtes, c'est-à-dire celles réalisées sur un seul mot phonologique et encapsulées dans une unité intonative (précédées ou suivies d'au moins une frontière prosodique non-terminale).
Les unités courtes peuvent théoriquement avoir pour fonction toutes les valeurs informationnelles, couvrant tous les marqueurs discursifs ainsi que la plupart des unités textuelles : ceci permettra l'observation d'une large gamme de fonctions informationnelles.
En appliquant le processus de classification à ces seules unités courtes, on évite les variations prosodiques dues à d'autres niveaux linguistiques (hiérarchisation, etc.).
De plus, les mots et expressions apparaissant sur les unités courtes sont souvent fréquents dans les corpus oraux, ce qui permet de baser les analyses sur de plus grandes quantités de données.
Les données seront extraites des corpus C-ORAL en portugais brésilien, italien et anglais américain (Cresti and Moneglia, 2005 ; Raso and Mello, 2012 ; Cavalcante and Ramos, 2016), avec une possible application au français (le corpus existe mais doit être resegmenté et informationnellement annoté).
Ces corpus sont segmentés prosodiquement et informationnellement, conformément aux prémisses de la L-AcT.
Des méthodes de TAL ont déjà été appliquées pour l'analyse de différentes unités informationnelles, notamment dans ce cadre théorique (Raso and Vieira, 2016 ; Moneglia and Cresti, 2018 ; Gobbo, 2019 ; Cavalcante, 2020), et ont démontré la faisabilité de cette tâche.
Il s'agira de caractériser les différentes structures prosodiques observées sur les unités cibles, dans la cadre d'une analyse acoustico-prosodique qui aura pour but une discrétisation des formes et une estimation des meilleurs moyens de représentation de celle-ci (les niveaux phonétique et phonologique proposant des formats de descriptions variables).
Sur la base de ces descriptions d'unités prosodiques, un processus d'apprentissage devra modéliser les liens entre formes prosodiques et fonctions linguistiques des unités cibles.
Ces deux processus auront pour but de tester le rôle joué par la prosodie dans l'attribution du signifié informationnel en parole spontanée, et aussi la similarité inter-linguistique des formes signifiantes, si leur usage leur distribution et leur fréquence peuvent varier d'une langue à l'autre.
Dans la première partie de cette thèse, nous présentons une nouvelle méthode de production de variantes de prononciations qui adapte des prononciations standards, c'est-à-dire issues d'un dictionnaire, à un style spontané.
Cette méthode utilise une vaste gamme d'informations linguistiques, articulatoires et acoustiques, ainsi qu'un cadre probabiliste d'apprentissage automatique, à savoir les champs aléatoires conditionnels (CAC) et les modèles de langage.
Nos expériences poussées sur le corpus Buckeye démontrent l'efficacité de l'approche à travers des évaluations objectives et perceptives.
Des tests d'écoutes sur de la parole synthétisée montrent que les prononciations adaptées sont jugées plus spontanées que les prononciations standards, et même que celle réalisées par les locuteurs du corpus étudié.
La seconde partie de la thèse explore une nouvelle approche de production automatique de disfluences dans les énoncés en entrée d'un système de synthèse de la parole.
L'approche proposée offre l'avantage de considérer plusieurs types de disfluences, à savoir des pauses, des répétitions et des révisions.
Pour cela, nous présentons une formalisation novatrice du processus de production de disfluences à travers un mécanisme de composition de ces disfluences.
Nous présentons une première implémentation de notre processus, elle aussi fondée sur des CAC et des modèles de langage, puis conduisons des évaluations objectives et perceptives.
Celles-ci nous permettent de conclure à la bonne fonctionnalité de notre proposition et d'en discuter les pistes principales d'amélioration.
La personnalisation est une tendance envisagée par les musées pour faire face à la diversité des visiteurs et de leurs pratiques de visite.
Afin de favoriser la mise en œuvre de visites personnalisées, nous questionnons l'apport des interactions tangibles, non seulement pour les visiteurs, mais également pour les professionnels du musée.
Comment aider les médiateurs culturels à concevoir des parcours personnalisés prenant en compte la diversité des profils de visiteurs ?
Comment aider les visiteurs à choisir et suivre le parcours qui correspond à leurs envies et besoins ?
Nous avons appliqué un processus de conception centrée utilisateurs avec des musées partenaires afin de concevoir, implémenter et évaluer des outils tangibles permettant de répondre à ces questions.
L'analyse des besoins des utilisateurs (médiateurs culturels et visiteurs) nous a permis de définir six caractéristiques principales à prendre en compte pour la personnalisation des visites ainsi que de faire émerger le concept de personnalisation multicritères.
Nous proposons un concept d'interface permettant d'associer le choix des caractéristiques visiteur constituant un profil et le suivi dynamique de l'avancement de la création des parcours personnalisés pour chaque combinaison pour les médiateurs.
Ce concept a été instancié selon deux modalités d'interaction tangible et tactile, que nous avons comparées à travers une étude expérimentale auprès de médiateurs de musée.
Nous décrivons également la conception itérative de prototypes d'aide au choix de parcours personnalisés pour les visiteurs, ainsi que l'étude pilote menée dans un musée partenaire.
Les prototypes conçus pendant cette thèse mettent en œuvre le paradigme d'interaction token+constraint.
Nous proposons une analyse systématique de la littérature référençant le paradigme d'interaction token+constraint, ainsi qu'une grille heuristique de 24 propriétés réparties en cinq catégories reprenant, synthétisant et illustrant les concepts de l'article séminal, que nous appliquons aux prototypes conçus durant la thèse.
Leur réutilisation peut prendre différentes formes (évolution, alignement, fusion, etc.), et présente plusieurs verrous scientifiques.
L'un des plus importants est la préservation de la consistance de l'ontologie lors de son changement.
Afin d'y répondre, nous nous intéressons dans cette thèse à étudier les changements ontologiques et proposons un cadre formel capable de faire évoluer et de fusionner des ontologies sans affecter leur consistance.
Premièrement, nous proposons TGGOnto (Typed Graph Grammars for Ontologies), un nouveau formalisme permettant la représentation des ontologies et leurs changements par les grammaires de graphes typés.
Deuxièmement, nous proposons EvOGG (Evolving Ontologies with Graph Grammars), une approche d'évolution d'ontologies qui se base sur le formalisme GGTOnto et traite les inconsistances d'une manière a priori.
Nous nous intéressons aux ontologies OWL et nous traitons à la fois : (1) l'enrichissement d'ontologies en étudiant leur niveau structurel et (2) le peuplement d'ontologies en étudiant les changements qui affectent les individus et leurs assertions.
L'approche proposée se décompose en trois étapes : (1) la recherche de similarité entre concepts en se basant sur des techniques syntaxiques, structurelles et sémantiques ; (2) la fusion d'ontologies par l'approche algébrique SPO ; (3) l'adaptation de l'ontologie globale résultante par le biais des règles de réécriture de graphes.
Afin de valider les travaux menés dans cette thèse, nous avons développé plusieurs outils open source basés sur l'outil AGG (Attributed Graph Grammar).
Ces outils ont été appliqués sur un ensemble d'ontologies, essentiellement sur celles développées dans le cadre du projet européen CCAlps (Creatives Companies in Alpine Space) qui a financé les travaux de cette thèse.
Cette thèse fait partie du projet RAPSODIE dont l'objectif est de proposer une reconnaissance vocale spécialisée sur les besoins des personnes sourdes et malentendantes.
Deux axes sont étudiées : la modélisation lexicale et l'extraction d'informations para
Cette détection a comme but de signaler aux personnes sourdes ou malentendantes quand une question leur est adressée
Les travaux de recherche de la thèse portent sur l'étude et la formalisation des interactions émotionnelles Humain-Machine.
Au delà d'une détection d'informations paralinguistiques (émotions, disfluences,...) ponctuelles, il s'agit de fournir au système un profil interactionnel et émotionnel de l'utilisateur dynamique, enrichi pendant l'interaction.
Ce profil permet d'adapter les stratégies de réponses de la machine au locuteur, et il peut également servir pour mieux gérer des relations à long terme.
Le profil est fondé sur une représentation multi-niveau du traitement des indices émotionnels et interactionnels extraits à partir de l'audio via les outils de détection des émotions du LIMSI.
Ainsi, des indices bas niveau (variations de la F0, d'énergie, etc.), fournissent des informations sur le type d'émotion exprimée, la force de l'émotion, le degré de loquacité, etc.
Ces éléments à moyen niveau sont exploités dans le système afin de déterminer, au fil des interactions, le profil émotionnel et interactionnel de l'utilisateur.
Ce profil est composé de six dimensions : optimisme, extraversion, stabilité émotionnelle, confiance en soi, affinité et domination (basé sur le modèle de personnalité OCEAN et les théories de l'interpersonal circumplex).
Le comportement social du système est adapté en fonction de ce profil, de l'état de la tâche en cours, et du comportement courant du robot.
Les règles de création et de mise à jour du profil émotionnel et interactionnel, ainsi que de sélection automatique du comportement du robot, ont été implémentées en logique floue à l'aide du moteur de décision développé par un partenaire du projet ROMEO.
L'implémentation du système a été réalisée sur le robot NAO.
Ces systèmes ont permis de recueillir des données en contrôlant plusieurs paramètres d'élicitation des émotions au sein d'une interaction ; nous présentons les résultats de ces expérimentations, et des protocoles d'évaluation de l'Interaction Humain-Robot via l'utilisation de systèmes à différents degrés d'autonomie.
Grâce aux avancées récentes de l'intelligence artificielle et du Traitement Automatique de la Langue Naturelle, l'objectif du projet pluridisciplinaire IA4Allergie, est de convertir les grandes masses de données textuelles contenues dans les entrepôts de données médicales pour en extraire et déduire de nouvelles connaissances dans l'objectif d'adapter la prise en charge des patients souffrant d'allergiques respiratoires.
Dans cette thèse nous étudions le rôle de la motivation intrinsèque dans l'émergence et le développement des systèmes communicationnels.
Notre objectif est d'explorer comment des populations d'agents artificiels peuvent utiliser un système de motivation computationnel particulier, appelé l'autotelic principle, pour réguler leur développement linguistique et les dynamiques qui en résultent au niveau de la population.
Nous proposons d'abord une mise en œuvre concrète de l'autotelic principle.
La relation entre les deux éléments n'est pas stable mais se déstabilise régulièrement lorsque de nouvelles compétences sont acquises, ce qui permet au système de tenter des défis de plus grande complexité.
Ensuite, nous testons l'utilité de ce système de motivation dans une série d'expériences sur l'évolution du langage.
Dans le premier ensemble d'expériences, une population d'agents artificiels doit développer une langue pour se référer à des objets ayant des caractéristiques discrètes.
Ces expériences se concentrent sur la façon dont les systèmes communicatifs non ambigus peuvent émerger lorsque l'autotelic principle est utilisé pour réguler le développement du langage en étapes de difficulté croissante.
Dans le deuxième ensemble d'expériences, les agents doivent créer un langage artificiel pour communiquer sur des couleurs.
Dans cette partie, on explore comment le système de motivation peut contrôler la complexité linguistique des interactions pour un domaine continu et on examine aussi la validité de l'autotelic principle en tant que mécanisme permettant de réguler simultanément plusieurs stratégies linguistiques de difficulté similaire.
En résumé, nous avons démontré à travers de notre travail que l'autotelic principle peut être utilisé comme un mécanisme général pour réguler la complexité du langage développé de manière autonome en domaines discrets et continus.
La gastronomie et l'onomastique n'ont encore jamais fait l'objet d'étude, de manière conjointe, d'un point de vue linguistique.
L'objectif de cette thèse est de proposer un point de départ à une réflexion sur la structure, la nature et la place du nom propre dans la gastronomie française.
Pour ce faire, après avoir exposé les différentes problématiques liées à la définition des concepts de gastronomie et de nom propre, nous avons réalisé une synthèse des principaux éléments théoriques qui servent de base à la recherche sur les noms de plats, tant sous un angle linguistique, historique qu'artistique ou législatif.
Sera d'abord traitée l'évolution du nom propre dans la gastronomie à travers le prisme de la norme dictionnairique au cours des 70 dernières années, au moyen d'une comparaison lexicographique des deux éditions du Larousse Gastronomique (1938 et 2007).
Pour ce faire, une catégorisation des noms propres relevés dans le corpus a été établie.
Dans un deuxième temps, la place des noms propres dans un corpus témoignant de l'usage a été examinée, en rassemblant des menus de restaurants parisiens ainsi que des dépliants publicitaires de la restauration livrée.
Le travail effectué au cours de ma thèse s'inscrit dans le cadre du Web Sémantique pour rendre l'annotation sémantique.
La vision du Web Sémantique a pour son objectif d'avoir les informations disponibles pour que les utilisateurs puissent les exploiter selon leurs besoins.
Pour cela, les données doivent être étiquetées sémantiquement.
En plus, comparé aux langues flexionnelles comme le Français, la technologie dans le traitement de langue agglutinative comme le Coréen a toujours des manques à cause de la complexité des morphologies et syntaxe.
Cette thèse s'intéresse à la modélisation de la syntaxe et de l'interface syntaxe-sémantique de la phrase, et explore la possibilité de contrôler au niveau des structures de dérivation la surgénération que produit le traitement des dépendances à distance par des types d'ordre supérieur.
À cet effet, nous étudions la possibilité d'étendre le système de typage des Grammaires Catégorielles Abstraites avec les constructions de la somme disjointe, du produit cartésien et du produit dépendant, permettant d'étiqueter les catégories syntaxiques par des structures de traits.
Nous prouvons dans un premier temps que le calcul résultant de cette extension bénéficie des propriétés de confluence et de normalisation, permettant d'identifier les termes beta-équivalents dans le formalisme grammatical.
Nous réduisons de plus le même problème pour la beta-eta-équivalence à un ensemble d'hypothèse de départ.
Dans un second temps, nous montrons comment cette introduction de structures de traits peut être appliquée au contrôle des dépendances à distances, à travers les exemples des contraintes de cas, des îlots d'extraction pour les mouvements explicites et implicites, et des extractions interrogatives multiples, et nous discutons de la pertinence de placer ces contrôles sur les structures de dérivation
Cette thèse porte sur l'identification des expressions polylexicales, abordée au moyen d'une analyse par transitions.
Une expression polylexicale (EP) est une construction linguistique composée de plusieurs éléments dont la combinaison montre une irrégularité à un ou plusieurs niveaux linguistiques.
La tâche d'identification d'EPs consiste à annoter en contexte les occurrences d'EPs dans des textes, i.e à détecter les ensembles de tokens formant de telles occurrences.
L'analyse par transitions est une approche célèbre qui construit une sortie structurée à partir d'une séquence d'éléments, en appliquant une séquence de « transitions » choisies parmi un ensemble prédéfini, pour construire incrémentalement la sortie.
Dans cette thèse, nous proposons un système par transitions dédié à l'identification des EPs au sein de phrases représentées comme des séquences de tokens, et étudions diverses architectures pour le classifieur qui sélectionne les transitions à appliquer, permettant de construire l'analyse de la phrase.
La première variante de notre système utilise un classifieur linéaire de type machine à vecteur support.
Les variantes suivantes utilisent des modèles neuronaux : un simple perceptron multicouche, puis des variantes intégrant une ou plusieurs couches récurrentes.
Le scénario privilégié est une identification d'EPs n'utilisant pas d'informations syntaxiques, alors même que l'on sait les deux tâches liées.
Nous étudions ensuite une approche par apprentissage multitâche, réalisant conjointement l'étiquetage morphosyntaxique, l'identification des EPs par transitions et l'analyse syntaxique en dépendances par transitions.
La thèse comporte une partie expérimentale importante.
Nous avons d'une part étudié quelles techniques de ré-échantillonnage des données permettent une bonne stabilité de l'apprentissage malgré des initialisations aléatoires.
D'autre part, nous avons proposé une méthode de réglage des hyperparamètres de nos modèles par analyse de tendances au sein d'une recherche aléatoire de combinaison d'hyperparamètres.
Nos variantes produisent de très bons résultats, et notamment les scores d'état de l'art pour de nombreuses langues de PARSEME.
L'une des variantes s'est classée première pour la plupart des langues de PARSEME 1.0.
Pourtant, nos modèles ont des performances faibles sur les EPs non vues à l'apprentissage.
Le véhicule autonome est un véhicule qui se conduira, à terme, sans aucune intervention du conducteur, quelle que soit la situation de conduite.
Ce véhicule comprend une nouvelle fonction, nommée fonction AD, pour Autonomous Driving, en charge de la conduite autonome.
Cette fonction peut se trouver dans des états différents (Active, Disponible par exemple) selon l'évolution des conditions environnementales.
Le changement de ses états est géré par une fonction de Supervision, nommée Supervision AD.
Le principal objet de ces travaux consiste à garantir que la fonction AD se trouve constamment dans un état sûr.
Ceci revient à s'assurer que la Supervision AD respecte l'ensemble des exigences fonctionnelles et de sûreté qui spécifient son comportement.
Ces deux disciplines d'ingénierie, bien qu'elles contribuent à la conception d'une même fonction, se distinguent en de nombreux points : objectifs, contraintes, planning, outils...
Dans notre cas d'étude, ces différences s'illustrent par les exigences considérées : les exigences fonctionnelles sont allouées à la fonction AD globale, tandis que les exigences de sûreté spécifient le comportement de sous-fonctions locales redondantes assurant une continuité de service en cas de défaillance.
La mise en cohérence de ces deux perspectives métier au plus tôt dans le cycle de conception et dans un contexte industriel, est la problématique centrale traitée.
Les enjeux de SdF soulevés par le véhicule autonome rendent ce problème primordial pour les constructeurs automobiles.
Afin de répondre à ces préoccupations, nous avons proposé une démarche outillée et collaborative de conception sûre de la Supervision AD.
Cette démarche est intégrée dans les processus normatifs en vigueur (normes ISO 15288 et ISO 26262) ainsi que dans les processus de conception internes chez Renault.
Elle est fondée sur la vérification formelle par model checking, la composition parallèle d'automates finis et l'expertise métier.
Cette démarche prône l'utilisation d'un même formalisme (l'automate à états finis) par les deux métiers pour mener à bien des activités partageant un objectif de modélisation commun : la vérification d'exigences de comportement en phase amont de conception.
Une méthode pour traduire les exigences en propriétés formelles et construire les modèles d'état a été déployée.
Il en résulte une consolidation progressive des exigences traitées, initialement rédigées en langage naturel.
Les potentielles ambigüités, incohérences et incomplétudes sont exhibées et traitées.
Les approches de gestion des processus métier font aujourd'hui partie intégrante de la vie et de la recherche de performance des entreprises.
Néanmoins, ce domaine demeure historiquement cloisonné en particulier vis-à-vis du rôle du système d'information dans l'entreprise.
Les résultats de ces travaux ont été appliqués à un cas industriel réel ce qui a permis de démontrer la pertinence et la nécessité d'une convergence entre un système d'information et la démarche processus d'une entreprise.
La lexicographie contemporaine met à disposition des ressources offrant de multiples possibilités d'exploitation automatique.
Elle débute par une revue de la formalisation et de l'informatisation de l'analogie pour l'étude du lexique, qui définit les principes de l'exploration : les sommets sont des objets disposant d'Attributs, les arcs représentent des Relations.
Deux séries d'expériences viennent ensuite.
La première montre que la formalisation de la ressource permet de détecter des analogies conformes à l'intuition, que différents types d'exploration sont possibles et que l'approche permet de vérifier la cohérence du réseau et de faire émerger des règles lexicales.
La seconde série porte sur la notion de configurations de dérivations lexicales.
Elle montre que le regroupement de sous-graphes analogues fait émerger des connexions récurrentes.
L'état d'avancement du réseau ne permet pas d'obtenir des règles et des modèles aboutis, mais les résultats sont encourageants.
Elle permet d'identifier des phénomènes linguistiques et d'instrumenter l'activité lexicologique.
Cette thèse explique et présente notre démarche de la réalisation d'un système à base de règles de reconnaissance et de classification automatique des EN en arabe.
C'est un travail qui implique deux disciplines : la linguistique et l'informatique.
L'outil informatique et les règles la linguistiques s'accouplent pour donner naissance à une nouvelle discipline ; celle de « traitement automatique des langues » , qui opère sur des niveaux différents (morphosyntaxique, syntaxique, sémantique, syntactico-sémantique etc.).
Ce travail de thèse s'inscrit donc dans un cadre général de traitement automatique des langues, mais plus particulièrement dans la continuité des travaux réalisés au niveau de l'analyse morphosyntaxique par la conception et la réalisation des bases des données lexicales SAMIA et ensuite DIINAR avec l'ensemble de résultats de recherches qui en découlent.
Pour comprendre de quoi il s'agit, il nous était important de commencer par la définition de l'entité nommée.
Et pour mener à bien notre démarche, nous avons distingué entre deux types principaux : pur nom propre et EN descriptive.
Nous avons aussi établi une classification référentielle en se basant sur diverses classes et sous-classes qui constituent la référence de nos annotations sémantiques.
Cependant, nous avons dû faire face à deux difficultés majeures : l'ambiguïté lexicale et les frontières des entités nommées complexes.
Notre système adopte une approche à base de règles syntactico-sémantiques.
Il est constitué, après le Niveau 0 d'analyse morphosyntaxique, de cinq niveaux de construction de patrons syntaxiques et syntactico-sémantiques basés sur les informations linguistique nécessaires (morphosyntaxiques, syntaxiques, sémantique, et syntactico-sémantique).
Ce travail, après évaluation en utilisant deux corpus, a abouti à de très bons résultats en précision, en rappel et en F–mesure.
Les résultats de notre système ont un apport intéressant dans différents application du traitement automatique des langues notamment les deux tâches de recherche et d'extraction d'informations.
En effet, on les a concrètement exploités dans les deux applications (recherche et extraction d'informations).
En plus de cette expérience unique, nous envisageons par la suite étendre notre système à l'extraction et la classification des phrases dans lesquelles, les entités classifiées, principalement les entités nommées et les verbes, jouent respectivement le rôle d'arguments et de prédicats.
Un deuxième objectif consiste à l'enrichissement des différents types de ressources lexicales à l'instar des ontologies.
La réutilisation des données de soins pour la recherche s'est largement répandue avec le développement d'entrepôts de données cliniques.
Ces entrepôts de données sont modélisés pour intégrer et explorer des données structurées liées à des thesaurus.
Ces données proviennent principalement d'automates (biologie, génétique, cardiologie, etc) mais aussi de formulaires de données structurées saisies manuellement.
La production de soins est aussi largement pourvoyeuse de données textuelles provenant des comptes rendus hospitaliers (hospitalisation, opératoire, imagerie, anatomopathologie etc.), des zones de texte libre dans les formulaires électroniques.
Cette masse de données, peu ou pas utilisée par les entrepôts classiques, est une source d'information indispensable dans le contexte des maladies rares.
En effet, le texte libre permet de décrire le tableau clinique d'un patient avec davantage de précisions et en exprimant l'absence de signes et l'incertitude.
Particulièrement pour les patients encore non diagnostiqués, le médecin décrit l'histoire médicale du patient en dehors de tout cadre nosologique.
Cette richesse d'information fait du texte clinique une source précieuse pour la recherche translationnelle.
Cela nécessite toutefois des algorithmes et des outils adaptés pour en permettre une réutilisation optimisée par les médecins et les chercheurs.
Nous présentons dans cette thèse l'entrepôt de données centré sur le document clinique, que nous avons modélisé, implémenté et évalué.
À travers trois cas d'usage pour la recherche translationnelle dans le contexte des maladies rares, nous avons tenté d'adresser les problématiques inhérentes aux données textuelles : (i) le recrutement de patients à travers un moteur de recherche adapté aux données textuelles (traitement de la négation et des antécédents familiaux), (ii) le phénotypage automatisé à partir des données textuelles et (iii) l'aide au diagnostic par similarité entre patients basés sur le phénotypage.
Ces méthodes et algorithmes ont été intégrés dans le logiciel Dr Warehouse développé pendant la thèse et diffusé en Open source depuis septembre 2017.
Les réseaux sociaux ont transformé le Web d'un mode lecture, où les utilisateurs pouvaient seulement consommer les informations, à un mode interactif leur permettant de les créer, partager et commenter.
Un défi majeur du traitement d'information dans les médias sociaux est lié à la taille réduite des contenus, leur nature informelle et le manque d'informations contextuelles.
D'un autre côté, le web contient des bases de connaissances structurées à partir de concepts d'ontologies, utilisables pour enrichir ces contenus.
Cette thèse explore le potentiel d'utiliser les bases de connaissances du Web de données, afin de détecter, classifier et suivre des événements dans les médias sociaux, particulièrement Twitter.
On a abordé 3 questions de recherche : i) Comment extraire et classifier les messages qui rapportent des événements ? ii) Comment identifier des événements précis ? iii) Étant donné un événement, comment construire un fil d'actualité représentant les différents sous-événements ?
Les travaux de la thèse ont contribué à élaborer des méthodes pour la généralisation des entités nommées par des concepts d'ontologies pour mitiger le sur-apprentissage dans les modèles supervisés ; une adaptation de la théorie des graphes pour modéliser les relations entre les entités et les autres termes et ainsi caractériser des événements pertinents ;
l'utilisation des ontologies de domaines et les bases de connaissances dédiées, pour modéliser les relations entre les caractéristiques et les acteurs des événements.
Nous démontrons que l'enrichissement sémantique des entités par des informations du Web de données améliore la performance des modèles d'apprentissages supervisés et non supervisés.
Pour obtenir des hauts taux de détection, les CNNs requièrent d'un grand nombre de paramètres à stocker, et en fonction de l'application, aussi un grand nombre d'opérations.
Cela complique gravement le déploiement de ce type de solutions dans les systèmes embarqués.
Ce manuscrit propose plusieurs solutions à ce problème en visant une coadaptation entre l'algorithme, l'application et le matériel.
Dans ce manuscrit, les principaux leviers permettant de fixer la complexité computationnelle d'un détecteur d'objets basé sur les CNNs sont identifiés et étudies.
Cela devient très coûteux lorsque des petits objets doivent être trouvés dans des images en haute résolution.
Pour rendre la solution efficiente et ajustable, le processus est divisé en deux étapes.
Un premier CNN s'especialise à trouver des régions d'intérêt de manière efficiente, ce qui permet d'obtenir des compromis flexibles entre le taux de détection et le nombre d'opérations.
De plus, les CNN exhibent plusieurs propriétés qui confirment leur surdimensionnement.
Ce surdimensionnement est une des raisons du succès des CNN, puisque cela facilite le processus d'optimisation en permettant un ample nombre de solutions équivalentes.
Cependant, cela complique leur implémentation dans des systèmes avec fortes contraintes computationnelles.
Dans ce sens, une méthode de compression de CNN basé sur une Analyse en Composantes Principales (ACP) est proposé.
L'ACP permet de trouver, pour chaque couche du réseau, une nouvelle représentation de l'ensemble de filtres appris par le réseau en les exprimant à travers d'une base ACP plus adéquate.
Cette base ACP est hiérarchique, ce qui veut dire que les termes de la base sont ordonnés par importance, et en supprimant les termes moins importants, il est possible de trouver des compromis optimales entre l'erreur d'approximation et le nombre de paramètres.
À travers de cette méthode il es possible d'obtenir, par exemple, une réduction x2 sur le nombre de paramètres et opérations d'un réseau du type ResNet-32, avec une perte en accuracy &lt;2%.
Il est aussi démontré que cette méthode est compatible avec d'autres méthodes connues de l'état de l'art, notamment le pruning, winograd et la quantification.
Dans le cas de la détection de vissages, la parallélisation du réseau comprimé par ACP sûr 8 processeurs incrémente la vitesse de calcul d'un facteur x11.68 par rapport au réseau original sûr un seul processeur.
Nous nous intéressons, dans cette thèse, à l'étude des modèles de mélange et des modèles linéaires généralisés, avec une application aux données de co-infection entre les arbovirus et les parasites du paludisme.
Après une première partie consacrée à l'étude de la co-infection par un modèle logistique multinomial, nous proposons dans une deuxième partie l'étude des mélanges de modèles linéaires généralisés.
La méthode proposée pour estimer les paramètres du mélange est une combinaison d'une méthode des moments et d'une méthode spectrale.
Nous proposons à la fin une dernière partie consacrée aux mélanges de valeurs extrêmes en présence de censure.
La méthode d'estimation proposée dans cette partie se fait en deux étapes basées sur la maximisation d'une vraisemblance.
Avec l'émergence du Web de données, notamment les données ouvertes liées, une abondance de données est devenue disponible sur le web.
Cependant, les ensembles de données LOD et leurs sous-graphes inhérents varient fortement par rapport a leur taille, le thème et le domaine, les schémas et leur dynamicité dans le temps au niveau des données.
Dans ce contexte, l'identification des jeux de données appropriés, qui répondent a des critères spécifiques, est devenue une tâche majeure, mais difficile a soutenir, surtout pour répondre a des besoins spécifiques tels que la recherche d'entités centriques et la recherche des liens sémantique des données liées.
Notamment, en ce qui concerne le problème de liage des données, le besoin d'une méthode efficace pour la recommandation des jeux de données est devenu un défi majeur, surtout avec l'état actuel de la topologie du LOD, dont la concentration des liens est très forte au niveau des graphes populaires multi-domaines tels que DBpedia et YAGO, alors qu'une grande liste d'autre jeux de données considérés comme candidats potentiels pour le liage est encore ignorée.
Ce problème est dû a la tradition du web sémantique dans le traitement du problème de " identification des jeux de données candidats pour le liage ".
Bien que la compréhension de la nature du contenu d'un jeu de données spécifique est une condition cruciale pour les cas d'usage mentionnées, nous adoptons dans cette thèse la notion de " profil de jeu de données " - un ensemble de caractéristiques représentatives pour un jeu de données spécifique, notamment dans le cadre de la comparaison avec d'autres jeux de données.
Notre première direction de recherche était de mettre en œuvre une approche de recommandation basée sur le filtrage collaboratif, qui exploite à la fois les prols thématiques des jeux de données, ainsi que les mesures de connectivité traditionnelles, afin d'obtenir un graphe englobant les jeux de données du LOD et leurs thèmes.
Cette approche a besoin d'apprendre le comportement de la connectivité des jeux de données dans le LOD graphe.
Cependant, les expérimentations ont montré que la topologie actuelle de ce nuage LOD est loin d'être complète pour être considéré comme des données d'apprentissage.
Face aux limites de la topologie actuelle du graphe LOD, notre recherche a conduit a rompre avec cette représentation de profil thématique et notamment du concept "apprendre pour classer" pour adopter une nouvelle approche pour l'identification des jeux de données candidats basée sur le chevauchement des profils intensionnels entre les différents jeux de données.
Par profil intensionnel, nous entendons la représentation formelle d'un ensemble d'étiquettes extraites du schéma du jeu de données, et qui peut être potentiellement enrichi par les descriptions textuelles correspondantes.
Cette représentation fournit l'information contextuelle qui permet de calculer la similarité entre les différents profils d'une manière efficace.
Nous identifions le chevauchement de différentes profils à l'aide d'une mesure de similarité semantico-fréquentielle qui se base sur un classement calcule par le tf*idf et la mesure cosinus.
Les expériences, menées sur tous les jeux de données lies disponibles sur le LOD, montrent que notre méthode permet d'obtenir une précision moyenne de 53% pour un rappel de 100%.
Afin d'assurer des profils intensionnels de haute qualité, nous introduisons Datavore - un outil oriente vers les concepteurs de métadonnées qui recommande des termes de vocabulaire a réutiliser dans le processus de modélisation des données.
Datavore fournit également les métadonnées correspondant aux termes recommandés ainsi que des propositions des triples utilisant ces termes.
L'outil repose sur l'écosystème des Vocabulaires Ouverts Lies (LOV) pour l'acquisition des vocabulaires existants et leurs métadonnées.
Les travaux présentés dans ce mémoire s'inscrivent dans le contexte de systèmes d'entreprises collaboratives.
En effet, la non-interopérabilité engendre des coûts non négligeables dus principalement au temps et aux ressources mises en place pour développer des interfaces d'échange des informations.
Ceci influe sur la performance globale des entreprises et précisément sur les coûts et les délais d'obtention des services attendus.
Nous proposons ainsi une approche pour mesurer, a priori, le degré d'interopérabilité (ou de non interopérabilité) entre modèles conceptuels de systèmes d'information d'entreprise, afin de donner à une entreprise la possibilité d'évaluer sa propre capacité à interopérer et donc de prévoir les éventuels problèmes avant la mise en place d'un partenariat.
Il s'agit ainsi de définir des indicateurs et des métriques tant quantitatifs que qualitatifs, permettant de qualifier l'interopérabilité entre les systèmes d'entreprises et de proposer des stratégies d'amélioration lorsque le niveau d'interopérabilité est évalué comme insuffisant
Cette thèse a deux objectifs principaux.
En premier lieu, on voudrait offrir un aperçu des connaissances académiques actuelles, tant théoriques qu'empiriques, des processus d'accommodation linguistique entre interlocuteurs, au sens général, et des caractéristiques rythmiques de la langue espagnole, en particulier.
En second lieu, on présente deux études empiriques conçues pour analyser l'influence de la régularité rythmique au niveau des phrases et de l'arrangement phonologique sur les processus d'accommodation linguistique.
Dans l'ensemble, les données rassemblées dans cette thèse indiquent que les phrases avec un rythme régulier, disposées en groupes accentuels, produisent une plus grande ressemblance entre les hispanophones en matière de rythme et de l'étendue de la F0, par rapport aux phrases avec un rythme irrégulier et aux phrases disposées en pieds accentuels.
De plus, certains faits connus concernant les femmes ayant une moyenne de F0 supérieure, une étendue de F0 plus large, et un débit de parole plus lent quant aux hommes ont également été observés au cours de la première expérience.
En outre, une valeur inférieure de la moyenne de F0 et une étendue de F0 plus étroite ont été observées lors de l'utilisation de phrases avec un rythme régulier et de phrases disposées en groupes accentuels, par rapport aux conditions expérimentales opposées.
Les entreprises, les administrations, et parfois les particuliers, doivent faire face à de nombreuses fraudes sur les documents qu'ils reçoivent de l'extérieur ou qu'ils traitent en interne.
Les factures, les notes de frais, les justificatifs... tout document servant de preuve peut être falsifié dans le but de gagner plus d'argent ou de ne pas en perdre.
En France, on estime les pertes dues aux fraudes à plusieurs milliards d'euros par an.
Étant donné que le flux de documents échangés, numériques ou papiers, est très important, il serait extrêmement coûteux en temps et en argent de les faire tous vérifier par des experts de la détection des fraudes.
C'est pourquoi nous proposons dans notre thèse un système de détection automatique des faux documents.
Si la plupart des travaux en détection automatique des faux documents se concentrent sur des indices graphiques, nous cherchons quant à nous à vérifier les informations textuelles du document afin de détecter des incohérences ou des invraisemblances.
Pour cela, nous avons tout d'abord constitué un corpus de tickets de caisse que nous avons numérisés et dont nous avons extrait le texte.
Après avoir corrigé les sorties de l'OCR et fait falsifier une partie des documents, nous en avons extrait les informations et nous les avons modélisées dans une ontologie, afin de garder les liens sémantiques entre elles.
Les informations ainsi extraites, et augmentées de leurs possibles désambiguïsations, peuvent être vérifiées les unes par rapport aux autres au sein du document et à travers la base de connaissances constituée.
Les liens sémantiques de l'ontologie permettent également de chercher l'information dans d'autres sources de connaissances, et notamment sur Internet.
Nous présentons dans cette thèse deux modèles informatiques développés pour délivrer de l'information structurée et applicables à de grandes bases de données de textes médiévaux.
Les deux modèles, l'un appliqué à la reconnaissance des entités nommées, l'autre à la détection des parties du discours diplomatique, ont suivi un apprentissage supervisé utilisant la méthode des Champs aléatoires conditionnelles (CRF) sur un corpus manuellement annoté de actes médiévaux (Corpus Burgundiae Medii Aevi ou CBMA).
Notre modèle principal de reconnaissance d'entités nommées a prouvé sa robustesse lorsqu'il a été appliqué sur des échantillons de corpus de taille, chronologie et origine très variés.
Le modèle secondaire détectant les parties du discours diplomatique, bien que moins performant, s'est montré valide comme outil de structuration.
Ils peuvent à présent être utilisés pour l'indexation et l'étude d'une grande variété de sources diplomatiques, économisant, ainsi des considérables efforts humains.
Nous avons développé différentes solutions destinées à trouver un juste équilibre entre la dépendance du modèle à son corpus d'origine et sa capacité à être appliqué à d'autres corpus.
De même, différents ajouts et corrections ont été opérés sur le corpus de référence à partir de plusieurs observations de type historique et linguistique concernant les documents utilisés, ce qui a permis d'améliorer la performance initiale.
Nous avons ensuite appliqué les outils ainsi générés à la reconnaissance de noms de personnes, de lieux et de parties du discours diplomatique sur des milliers d'actes du CBMA afin d'étudier différentes questions intéressant la science historique et la diplomatique.
Ces études concernent la datation semi-automatique d'un cartulaire qui en était dépourvu ; l'évolution du vocabulaire spatial dans les actes du Moyen Âge Central ; et l'indexation des documents à partir des modules les intégrant, notamment les formules du protocole des actes.
Par ces études nous poursuivons un double objectif : illustrer différentes stratégies permettant d'abstraire et d'adapter au traitement automatique des données des méthodes de recherche classiques en Histoire ; démontrer que nos outils de traitement massif permettent la génération de connaissances pertinentes pour la science historique.
Les besoins croissants continuels de la médecine et de la santé accentuent la nécessité de systèmes d'alertes médicales bien adaptés.
Ces systèmes d'alerte peuvent être utilisés par une variété de patients et d'acteurs médicaux.
Ils devraient permettre de contrôler un large éventail de variables médicales.
Aux alertes détectées on associées deux indices de qualité.
L'indice d'applicabilité qui exprime dans quelle mesure un patient est concerné par l'alerte, et l'indice de confiance qui exprime la fiabilité de l'alerte concernant la fraicheur des données utilisées lors de sa détection.
Les indices de qualité associés à une alerte détectée se calculent grâce aux informations liées à une situation d'alerte paramétrée par l'utilisateur.
Une situation d'alerte est définie à partir d'une composition de conditions d'activation.
Une condition d'activation est construite à partir d'une valeur linguistique exprimant l'état (par exemple température élevée) ou la tendance (par exemple tension systolique à la hausse) d'une entité observable (la température, la tension systolique, etc.).
Lorsque la situation d'alerte est évaluée, le système utilise les connaissances préparées préalablement par les utilisateurs concernant les valeurs linguistiques.
C'est-à-dire, quelle valeur linguistique représente le mieux une valeur quantitative sous un contexte spécifique.
Les alertes détectées pouvant être nombreuses, nous définissons un politique de notification pour ne notifier que les alertes pertinentes dans le but de ne pas fatiguer les utilisateurs.
D'abord les alertes sont filtrées à partir des indices de qualité.
Parmi les alertes restantes, le système filtre par expressivité : choisir les tendances plus durables et les valeurs linguistiques les plus expressives.
Ensuite, dans le cas d'alertes consécutives, le système ne garde que les alertes qui accomplissent les préférences des utilisateurs, comme celles dont l'indice d'applicabilité augmente.
L'objectif final est de fidéliser l'utilisateur au système d'alertes en lui fournissant un service de qualité.
Il s'approprie le système en même temps qu'il définit les situations d'alerte.
Ainsi, il est capable de l'adapter lui-même au contexte afin d'obtenir des alertes de meilleure qualité.
L'adaptation est guidée par les indices de qualité utilisés pour réduire les faux-positifs et faux-négatifs ainsi que pour contrôler la sur-notification d'alertes.
Nous proposons de nous appuyer sur les systèmes existants en apportant des fonctionnalités de dynamisme et d'évolution, ainsi que des facilités de paramétrage et d'adaptation en temps réel au contexte d'utilisation afin d'exploiter au mieux les observations.
L'extraction d'information de textes médicaux fournit des renseignements très utiles pour identifier les effets indésirables dans la surveillance après consommation (Pharmacovigilance), qui sont plus difficiles à découvrir à travers des études médicales typiques puisque les patients prennent plusieurs traitements en même temps.
Récemment, les techniques de Data Mining ont permis de découvrir les connaissances enregistrées dans de grands ensembles de données, comme les dossiers cliniques collectés par les hôpitaux tout au long de la vie du patient.
Pour cela, nous devons extraire les relations entre les médicaments et Adverses
Ce problème est divisé en tâches de reconnaissance d'entités nommées (NER) et d'extraction de relations.
Aujourd'hui, les approches supervisées basées sur des algorithmes de Deep Learning et Machine Learning résolvent ce problème dans l'état de l'art.
Les méthodes supervisées ont besoin de caractéristiques riches afin d'apprendre des modèles efficaces au cours de la formation, par conséquent, nous nous concentrons sur la construction de représentations de mots larges (l'entrée du réseau neuronal)
La représentation proposée améliore la performance du modèle de référence et le modèle final a atteint les performances des méthodes de pointe.
Ensuite, nous avons extrait des informations contextuelles à travers des modèles de Deep Learning, afin d'identifier les réactions indésirables aux médicaments.
Le modèle proposé a amélioré la précision globale et l'extraction des réactions indésirables aux médicaments obtenu avec le modèle de base, ce qui indique l'efficacité de combiner des modèles de Deep Learning et une vaste ingénierie des caractéristiques.
Cette thèse s'intéresse au Résumé Automatique de texte et plus particulièrement au résumé mis-à-jour.
Cette problématique de recherche vise à produire un résumé différentiel d'un ensemble de nouveaux documents par rapport à un ensemble de documents supposés connus.
Elle intègre ainsi dans la problématique du résumé à la fois la question de la dimension temporelle de l'information et celle de l'historique de l'utilisateur.
Dans ce contexte, le travail présenté s'inscrit dans les approches par extraction fondées sur une optimisation linéaire en nombres entiers (ILP) et s'articule autour de deux axes principaux : la détection de la redondance des informations sélectionnées et la maximisation de leur saillance.
Pour le premier axe, nous nous sommes plus particulièrement intéressés à l'exploitation des similarités inter-phrastiques pour détecter, par la définition d'une méthode de regroupement sémantique de phrases, les redondances entre les informations des nouveaux document set celles présentes dans les documents déjà connus.
Concernant notre second axe, nous avons étudié l'impact de la prise en compte de la structure discursive des documents, dans le cadre de la Théorie de la Structure Rhétorique (RS), pour favoriser la sélection des informations considérées comme les plus importantes.
L'intérêt des méthodes ainsi définies a été démontré dans le cadre d'évaluations menées sur les données des campagnes TAC et DUC.
Enfin, l'intégration de ces critères sémantique et discursif au travers d'un mécanisme de fusion tardive a permis de montrer dans le même cadre la complémentarité de ces deux axes et le bénéfice de leur combinaison.
Depuis l'expansion du web, de nombreuses applications cherchent à répondre aux besoins d'utilisateurs ou de machines aux origines culturelles variées.
De ce contexte de diversité culturelle émerge de nombreux conflits liés à des conceptions du monde différentes.
Proposer des services adaptés requiert l'intégration au sein du système d'une forme de conscience culturelle.
Une conscience culturelle artificielle est composée de représentations et de médiations culturelles formelles offrant au système les moyens pour interpréter les cultures représentées et déterminer leurs différences.
Ces modèles grossiers, bien qu'ils soient adaptés, limitent la compréhension possible des cultures représentées.
Par conséquent ils constituent un goulot d'étranglement dans le développement des systèmes culturellement conscients.
J'étudie la construction, la formalisation et la médiation des représentations culturelles émiques.
Mes contributions principales sont la conception et la validation, d'une part, d'un nouveau processus ethnographique semi-automatique de construction de modèles émiques via la fouille de textes et, d'autre part, d'une conscience culturelle artificielle émique fondée sur l'alignement d'ontologies culturelles issues de ces modèles.
Le premier problème qu'on considère est de détecter un sous graphe Erdős–Rényi G(m,p) plante dans un graphe Erdős–Rényi G(n,q).
Nous dérivons les distributions d'une statistique basée sur les propriétés spectrales d'une matrice définie du graphe.
Pour cela nous utilisons l'algorithme « Belief Propagation » .
Le BP sans informations supplémentaires ne réussit à la récupération qu'avec un SNR effectif lambda au-delà d'un seuil.
Nous prouvons qu'en présence des informations supplémentaires, ce seuil disparaît et le BP réussi pour n'importe quel lambda.
Finalement, nous dérivons des expressions asymptotiques pour PageRank sur une classe de graphes aléatoires non dirigés appelés « fast expanders » , en utilisant des techniques théoriques à la matrice aléatoire.
Nous montrons que PageRank peut être approché pour les grandes tailles du graphe comme une combinaison convexe du vecteur de dégré normalisé et le vecteur de personnalisation du PageRank, lorsque le vecteur de personnalisation est suffisamment délocalisé.
Par la suite, nous caractérisons les formes asymptotiques de PageRank sur le Stochastic Block Model (SBM) et montrons qu'il contient un terme de correction qui est fonction de la structure de la communauté.
L'apprentissage de modèles stochastiques générant des séquences a de nombreuses applications en traitement de la parole, du langage ou bien encore en bio-informatique.
Les algorithmes traditionnels d'apprentissage comme celui de Baum-Welch sont itératifs, lent et peuvent converger vers des optima locaux.
Une alternative récente consiste à utiliser la méthode des moments (MoM) pour concevoir des algorithmes rapides et consistent avec des garanties pseudo-PAC.
Cependant, les algorithmes basés sur la MoM ont deux inconvénients principaux.
Tout d'abord, les garanties PAC ne sont valides que si la dimension du modèle appris correspond à la dimension du modèle cible.
Deuxièmement, bien que les algorithmes basés sur la MoM apprennent une fonction proche de la distribution cible, la plupart ne contraignent pas celle-ci à être une distribution.
Ainsi, un modèle appris à partir d'un nombre fini d'exemples peut renvoyer des valeurs négatives et qui ne somment pas à un.
Ainsi, cette thèse s'adresse à ces deux problèmes.
D'abord, nous proposons un élargissement des garanties théoriques pour les modèles compressés, ainsi qu'un algorithme spectral régularisé qui adapte la taille du modèle aux données.
Puis, une application en guerre électronique est aussi proposée pour le séquencement des écoutes du récepteur superhétérodyne.
Elle s'intéresse En particulier à deux sous-groupes : les dénominaux dont la paraphrase est « faire devenir S » , où S est le substantif de base (appelés BN) ; et les désadjectivaux dont la paraphrase est « faire quelque chose plus A » , où A est l'adjective de base (appelés DPV).
La thèse s'ouvre avec des chapitres de caractère général.
Le premier plaide pour une amélioration des méthodes de collecte de données dans le domaine génératif.
Les deuxième et troisième chapitres décrivent les cadres formels pertinents ainsi que la parasynthèse.
La première partie de la thèse porte sur les BN.
Cette enquête se poursuit par une comparaison avec les données du français, qui se comporte différemment.
La deuxième partie s'ouvre avec une réflexion sur les diagnostics de la stativité, et se poursuit avec l'analyse des DPVs.
Enfin, une application au TALN des diagnostics de stativité est décrite dans le dernier chapitre.
La représentation d'image sous une forme hiérarchique a été largement utilisée dans un contexte de classification.
Une telle représentation est capable de modéliser le contenu d'une image à travers une structure arborescente.
Dans cette thèse, nous étudions les méthodes à noyaux qui permettent de prendre en entrée des données sous une forme structurée et de tenir compte des informations topologiques présentes dans chaque structure en concevant des noyaux structurés.
Nous présentons un noyau structuré dédié aux structures telles que des arbres non ordonnés et des chemins (séquences de noeuds) équipés de caractéristiques numériques.
Le noyau proposé, appelé Bag of Subpaths Kernel (BoSK), est formé en sommant les noyaux calculés sur les sous-chemins (un sac de tous les chemins et des noeuds simples) entre deux sacs.
Nous proposons également une version rapide de notre algorithme, appelé Scalable BoSK (SBoSK), qui s'appuie sur la technique des Random Fourier Features pour projeter les données structurées dans un espace euclidien, où le produit scalaire du vecteur transformé est une approximation de BoSK.
Cette stratégie permet la classification des tuiles ou parties d'image.
En poussant plus loin l'utilisation de (S)BoSK, nous introduisons une nouvelle approche de classification multi-source qui effectue la classification directement à partir d'une représentation hiérarchique construite à partir de deux images de la même scène prises à différentes résolutions, éventuellement selon différentes modalités.
Les évaluations sur plusieurs jeux de données de télédétection disponibles dans la communauté illustrent la supériorité de (S)BoSK par rapport à l'état de l'art en termes de précision de classification, et les expériences menées sur une tâche de classification urbaine montrent la pertinence de l'approche de classification multi-source proposée.
Le principal objectif de cette thèse est d'étudier l'incidence des erreurs prépositionnelles sur l'intelligibilité de productions en L2 anglais par des apprenants francophones.
Nous proposons également des solutions pédagogiques pour améliorer l'apprentissage des prépositions en anglais.
Les résultats de cette analyse permettent de voir dans quelle mesure les erreurs prépositionnelles affectent l'intelligibilité du message.
Dans de nombreux domaines, les nouvelles technologies d'acquisition de l'information ou encore de mesure (e.g. serres de phénotypage robotisées) ont engendré une création phénoménale de données.
Nous nous appuyons en particulier sur deux cas d'application réels : les observations de plantes en botanique et les données de phénotypage en biologie.
Cependant, nos contributions peuvent être généralisées aux données du Web.
Par ailleurs, s'ajoute à la quantité des données leur distribution.
Chaque utilisateur stocke en effet ses données sur divers sites hétérogènes (e.g.
Que ce soit pour les observations de botanique ou pour les données de phénotypage en biologie, des solutions collaboratives, comprenant des outils de recherche et de recommandation distribués, bénéficieraient aux utilisateurs.
Les techniques de diversification permettent de présenter aux utilisateurs des résultats offrant une meilleure nouveauté tout en évitant de les lasser par des contenus redondants et répétitifs.
Grâce à la diversité, une distance entre toutes les recommandations est en effet introduite afin que celles-ci soient les plus représentatives possibles de l'ensemble des résultats pertinents.
Peu de travaux exploitent la diversité des profils des utilisateurs partageant les données.
Dans ce travail de thèse, nous montrons notamment que dans certains scénarios, diversifier les profils des utilisateurs apporte une nette amélioration en ce qui concerne la qualité des résultats~ : des sondages montrent que dans plus de 75% des cas, les utilisateurs préfèrent la diversité des profils à celle des contenus.
Par ailleurs, afin d'aborder les problèmes de distribution des données sur des sites hétérogènes, deux approches sont possibles.
La première, les réseaux P2P, consiste à établir des liens entre chaque pair (noeud du réseau) : étant donné un pair p, ceux avec lesquels il a établi un lien représentent son voisinage.
Celui-ci est utilisé lorsque p soumet une requête q, pour y répondre.
Cependant, dans les solutions de l'état de l'art, la redondance des profils des pairs présents dans les différents voisinages limitent la capacité du système à retrouver des résultats pertinents sur le réseau, étant donné les requêtes soumises par les utilisateurs.
Nous montrons, dans ce travail, qu'introduire de la diversité dans le calcul du voisinage, en augmentant la couverture, permet un net gain en termes de qualité.
La seconde approche de la distribution est le multisite.
Généralement, dans les solutions de l'état de l'art, les sites sont homogènes et représentés par de gros centres de données.
Deux versions du prototype ont été réalisées afin de répondre aux deux cas d'application, en s'adaptant notamment aux types des données.
Dans cette thèse, nous nous sommes intéressés à l'impact de la quantité et de la qualité de l'information échangée entre individus d'un groupe sur leurs performances collectives dans deux types de tâches bien spécifiques.
Dans une première série d'expériences, les sujets devaient estimer des quantités séquentiellement, et pouvaient réviser leurs estimations après avoir reçu comme information sociale l'estimation moyenne d'autres sujets.
Nous contrôlions cette information sociale à l'aide de participants virtuels (dont nous contrôlions le nombre) donnant une information (dont nous contrôlions la valeur), à l'insu des sujets.
Nous avons montré que lorsque les sujets ont peu de connaissance préalable sur une quantité à estimer, (les logarithmes de) leurs estimations suivent une distribution de Laplace.
La médiane étant un bon estimateur du centre d'une distribution de Laplace, nous avons défini la performance collective comme la proximité de la médiane (du logarithme) des estimations à la vraie valeur.
Nous avons trouvé qu'après influence sociale, et lorsque les agents virtuels fournissent une information correcte, la performance collective augmente avec la quantité d'information fournie (fraction d'agents virtuels).
Nous avons aussi analysé la sensibilité à l'influence sociale des sujets, et trouvé que celle-ci augmente avec la distance entre l'estimation personnelle et l'information sociale.
Ces analyses ont permis de définir 5 traits de comportement : garder son opinion, adopter celle des autres, faire un compromis, amplifier l'information sociale ou au contraire la contredire.
Nos résultats montrent que les sujets qui adoptent l'opinion des autres sont ceux qui améliorent le mieux leur performance, car ils sont capables de bénéficier de l'information apportée par les agents virtuels.
Nous avons ensuite utilisé ces analyses pour construire et calibrer un modèle d'estimation collective, qui reproduit quantitativement les résultats expérimentaux et prédit qu'une quantité limitée d'information incorrecte peut contrebalancer un biais cognitif des sujets consistant à sous-estimer les quantités, et ainsi améliorer la performance collective.
D'autres expériences ont permis de valider cette prédiction.
Dans une seconde série d'expériences, des groupes de 22 piétons devaient se séparer en clusters de la même "couleur", sans indice visuel (les couleurs étaient inconnues), après une courte période de marche aléatoire.
Pour les aider à accomplir leur tâche, nous avons utilisé un système de filtrage de l'information disponible (analogue à un dispositif sensoriel tel que la rétine), prenant en entrée l'ensemble des positions et couleurs des individus, et retournant un signal sonore aux sujets (émit par des tags attachés à leurs épaules) lorsque la majorité de leurs k plus proches voisins était de l'autre couleur que la leur.
Cette thèse, réalisée en coopération avec l'ONERA, concerne la reconnaissance active d'objets 3D par un agent autonome muni d'une caméra d'observation.
Alors qu'en reconnaissance passive les modalités d'acquisitions des observations sont imposées et génèrent parfois des ambiguïtés, la reconnaissance active exploite la possibilité de contrôler en ligne ces modalités d'acquisition au cours d'un processus d'inférence séquentiel dans le but de lever l'ambiguïté.
L'objectif des travaux est d'établir des stratégies de planification dans l'acquisition de l'information avec le souci d'une mise en œuvre réaliste de la reconnaissance active.
La première partie des travaux se consacre à apprendre à planifier.
La deuxième partie des travaux s'attache à exploiter au mieux les observations au cours de la reconnaissance.
La possibilité d'une reconnaissance active multi-échelles est étudiée pour permettre une interprétation au plus tôt dans le processus séquentiel d'acquisition de l'information.
Les observations sont également utilisées pour estimer la pose de l'objet de manière robuste afin d'assurer la cohérence entre les modalités planifiées et celles réellement atteintes par l'agent visuel.
Cette thèse porte sur la reconnaissance visuelle "zero-shot", qui vise à classifier des images de catégories non rencontrées par le modèle pendant la phase d'apprentissage.
Après avoir classé les méthodes existantes en trois grandes catégories, nous défendons l'idée que les méthodes dites de classement se basent habituellement sur plusieurs hypothèses implicites préjudiciables.
Nous proposons d'adapter leur fonction de coût pour leur permettre d'intégrer des relations inter et intra-classe.
Nous proposons également un processus permettant de diminuer l'écart entre les performances sur les classes vues et non vues dont souffrent fréquemment ces méthodes.
Dans notre évaluation expérimentale, ces contributions permettent à notre modèle d'égaler ou surpasser les performances des méthodes génératives, tant en étant moins restrictif.
Dans un second temps, nous nous intéressons aux représentations sémantiques utilisées dans un contexte d'
Dans ce contexte, l'information sémantique provient généralement de plongements lexicaux des noms de classe.
Nous soutenons que les plongements habituels souffrent d'un manque de contenu visuel dans les corpus servant à leur apprentissage.
Nous proposons donc de nouveaux corpus de texte davantage connotés visuellement, ainsi qu'une méthode permettant d'adapter les modèles de plongement à ces corpus.
Nous proposons en outre de compléter ces représentations non supervisées par de courtes descriptions en langage naturel, dont la production ne requiert qu'un effort minimal comparé à des attributs génériques.
La construction d'ontologies est un processus fastidieux qui nécessite un travail manuel conséquent.
Les textes, en tant que sources de connaissances, peuvent optimiser les recours aux experts du domaine.
Le passage des textes à l'ontologie requiert un double changement de perspective.
Tout d'abord du niveau du discours vers le niveau linguistique (terminologie, hyperonymie, synonymie, etc.), à l'aide d'outils de traitement automatique des langues.
La conceptualisation, manuelle, permet ensuite d'entrer dans le monde des modèles.
Nous montrons que, dans l'état actuel des connaissances, la construction d'ontologies à partir de textes ne peut s'effectuer de manière totalement automatique.
Cette thèse se situe au croisement de deux domaines de recherche, le Traitement Automatique des Langues (TAL) d'une part, et l'Apprentissage Automatique (AP) d'autre part.
Le sujet propose d'exploiter les connaissances expertes et les ressources linguistiques disponibles pour améliorer la performance des modèles de réseaux de neurones profonds pour les domaines et langues peu dotées en données d'apprentissage.
Le sujet de thèse proposé a pour but d'explorer et d'expérimenter de nouvelles approches pour l'intégration de connaissances expertes et de ressources linguistiques dans des modèles neuronaux profonds en vue d'améliorer leur performance pour les domaines et langues peu dotées en données d'apprentissage.
Un premier défi de cette thèse réside dans la définition d'un mécanisme de représentation de ces connaissances expertes et de ces ressources linguistiques de nature variées pour une intégration optimale dans le modèle neuronal.
Le problème s'avère plus difficile face à des connaissances expertes ou des ressources linguistiques hétérogènes.
Nous proposons d'aborder cette problématique selon les axes suivants, dans le prolongement des travaux déjà réalisés au laboratoire LASTI (Laboratoire Analyse Sémantique Texte et Image) :
- Prise en compte de connaissances expertes et de ressources linguistiques hétérogènes : Ontologies, Bases de données terminologiques, Lexiques, Règles de reconnaissance d'entités nommées, Règles d'identification de relations de dépendance syntaxique, etc.
- Développement d'un formalisme permettant de décrire les connaissances expertes et les ressources linguistiques dans un ensemble de représentations disposées par étages.
L'idée est que chaque type de connaissance experte ou de ressource linguistique a une représentation séparée mais que l'ensemble des représentations est décrit dans le même format (modèle).
- Exploration de nouvelles stratégies pour l'intégration des connaissances expertes et des ressources linguistiques dans les réseaux de neurones profonds.
L'idée sous-jacente est de proposer un mécanisme d'intégration qui s'adapte à chaque type de connaissance externe ou de ressource linguistique.
Menée dans le cadre de la Construction Morphology, ma thèse propose une analyse des verbes parasynthétiques de l'italien.
La définition de parasynthèse courante en littérature correspond à 'double affixation simultanée sur une base de dérivation' ([préf+[X]N/A+suff]V, cf. par exemple IMBARCARE 'embarquer').
Cette particularité est motivée par l'impossibilité d'attester 'l'étape intermédiaire'de dérivation entre la base et le verbe construit (cf. BARCA, *IMBARCA, *BARCARE).
Cette définition relève d'une analyse morphemique, incrémentale et concatenative et présuppose que les procédés dérivationnels sont conçus comme des règles orientées.
Dans mon analyse, au contraire, sont définis parasynthétiques tous les verbes construits par préfixation.
Cette définition s'appuie uniquement sur le paramètre d'appartenance au schéma [préf+[X]N/A]V (le suffixe étant de nature flexionnelle).
Selon mon point de vue, la non-attestation d'une forme est un paramètre qui est non seulement insuffisamment fiable d'un point de vue empirique, mais aussi négligeable du point de vue d'une théorie basée sur l'idée de procédés non-orientés.
La base de données utilisée contient 1674 lexèmes tirés de façon automatique du corpus ItWaC.
Les variables structurelles des verbes correspondent (i) au préfixe employé (a-, in-, s-, de-ou dis-), (ii) à la classe flexionnelle (-are ou-ire) et (iii) à la catégorie de la base (N ou A).
Chaque lexème, définit comme construction, est l'association d'une forme (le résultat de la combinaison des variables) et d'une valeur sémantique holistique.
Les valeurs sémantiques suivantes ont été identifiées : (i) changement d'état, (ii) changement de relation locative et (iii) valeur intensive/itérative.
Pour les deux premières valeurs il a été proposé une analyse unifiée dans une même composante sémantique qui exprime un changement (formalisé au moyen du prédicat BECOME), alors que la classe des verbes qui expriment la valeur (iii) est exclue de cette généralisation.
Nos travaux de recherches présentés dans ce manuscrit ont pour objectif, l'optimisation des performances des algorithmes de suivi des formants.
Pour ce faire, nous avons commencé par l'analyse des différentes techniques existantes utilisées dans le suivi automatique des formants.
Cette analyse nous a permis de constater que l'estimation automatique des formants reste délicate malgré l'emploi de diverses techniques complexes.
Vue la non disponibilité des bases de données de référence en langue arabe, nous avons élaboré un corpus phonétiquement équilibré en langue arabe tout en élaborant un étiquetage manuel phonétique et formantique.
Ensuite, nous avons présenté nos deux nouvelles approches de suivi de formants dont la première est basée sur l'estimation des crêtes de Fourier (maxima de spectrogramme) ou des crêtes d'ondelettes (maxima de scalogramme) en utilisant comme contrainte de suivi le calcul de centre de gravité de la combinaison des fréquences candidates pour chaque formant, tandis que la deuxième approche de suivi est basée sur la programmation dynamique combinée avec le filtrage de Kalman.
Finalement, nous avons fait une étude exploratrice en utilisant notre corpus étiqueté manuellement comme référence pour évaluer quantitativement nos deux nouvelles approches par rapport à d'autres méthodes automatiques de suivi de formants.
Nous avons testé la première approche par détection des crêtes ondelette, utilisant le calcul de centre de gravité, sur des signaux synthétiques ensuite sur des signaux réels de notre corpus étiqueté en testant trois types d'ondelettes complexes (CMOR, SHAN et FBSP).
Suite à ces différents tests, il apparaît que le suivi de formants et la résolution des scalogrammes donnés par les ondelettes CMOR et FBSP sont meilleurs qu'avec l'ondelette SHAN.
Afin d'évaluer quantitativement nos deux approches, nous avons calculé la différence moyenne absolue et l'écart type normalisée.
Nous avons fait plusieurs tests avec différents locuteurs (masculins et féminins) sur les différentes voyelles longues et courtes et la parole continue en prenant les signaux étiquetés issus de la base élaborée comme référence.
Les résultats de suivi ont été ensuite comparés à ceux de la méthode par crêtes de Fourier en utilisant le calcul de centre de gravité, de l'analyse LPC combinée à des bancs de filtres de Mustafa Kamran et de l'analyse LPC dans le logiciel Praat.
Cette méthode donne donc un suivi de formants (F1, F2 et F3) pertinent et plus proche de suivi référence.
Elles sont aussi très proches de la méthode par détection de crêtes de Fourier utilisant le calcul de centre de gravité.
Les résultats obtenus dans le cas des locutrices féminins confirment la tendance observée sur les locuteurs
Cette thèse porte sur l'étude de méthodes aléatoires pour l'apprentissage de données en grande dimension.
Nous proposons d'abord une approche non supervisée consistant en l'estimation des composantes principales, lorsque la taille de l'échantillon et la dimension de l'observation tendent vers l'infini.
Cette approche est basée sur les matrices aléatoires et utilise des estimateurs consistants de valeurs propres et vecteurs propres de la matrice de covariance.
Ensuite, dans le cadre de l'apprentissage supervisé, nous proposons une approche qui consiste à, d'abord réduire la dimension grâce à une approximation de la matrice de données originale, et ensuite réaliser une LDA dans l'espace réduit.
La réduction de dimension est basée sur l'approximation de matrices de rang faible par l'utilisation de matrices aléatoires.
Un algorithme d'approximation rapide de la SVD, puis une version modifiée permettant l'approximation rapide par saut spectral sont développés.
Les approches sont appliquées à des données réelles images et textes.
Elles permettent, par rapport à d'autres méthodes, d'obtenir un taux d'erreur assez souvent optimal, avec un temps de calcul réduit.
Enfin, dans le cadre de l'apprentissage par transfert, notre contribution consiste en l'utilisation de l'alignement des sous-espaces caractéristiques et l'approximation de matrices de rang faible par projections aléatoires.
Cette thèse en linguistique contrastive se propose d'étudier les spécificités syntactico-sémantiques des verbes inchoatifs en arabe et les comparer avec celles de commencer (à/par/de) et de se mettre à.
Nous nous sommes appuyé sur un corpus de textes littéraires francophones et arabophones (Mahfouz, Pagnol et Camus).
Notre corpus montre que les verbes inchoatifs peuvent apparaître dans des constructions simples suivantes : (sujet + verbe inchoatif + complément d'objet direct, complément d'objet indirect, adverbe ou sans complément), ainsi que dans des constructions composées (sujet + V1 inchoatif accompli + V2 inaccompli) en arabe, et (S + V1 conjugué + V2 infinitif) en français.
Notre analyse s'inspire des cadres syntaxiques proposés par Peeters (1993).
En effet, l'application de cette théorie et la variation syntaxique des éléments grammaticaux de la phrase arabe nous amènent, tout d'abord, à proposer différents cadres syntaxiques de certains verbes inchoatifs arabes relevés dans notre corpus en déterminant leurs spécificités syntaxiques et sémantiques.
Concernant la forme simple des inchoatifs en arabe, بدأ (bada'a) est le verbe le plus fréquemment utilisé.
Celui-ci est compatible avec certaines prépositions et accepte n'importe quel type du sujet et du complément.
Par conséquent, il se distingue par six constructions grammaticales et quatre cadres syntaxiques.
En revanche, les verbes هم (hamma), شرع (šara῾a) et أخذ ('aẖaḏa), dans leur emploi simple, chacun de ceux-ci ne se régit que par une préposition, de ce fait, leur construction simple ne se caractérise que par un cadre syntaxique.
Par ailleurs, le verbe راح (rāḫa) se combine avec deux cadres syntaxiques.
En ce qui concerne leur forme composée, notons que tous les verbes inchoatifs en arabe se partagent par une construction composée (S + V1 accompli + V2 inaccompli).
Sémantiquement, la majorité de ces verbes se distingue par une valeur imperfective.
Notons que le verbe بدأ (bada'a) véhicule une valeur déterminative ainsi que non déterminative.
Par contre, le verbe شرع (šara῾a) se défini que par une valeur perfective, alors que هم (hamma) par une valeur semi-perfective.
En conséquence, d'après les résultats de Peeters ainsi que les nôtres, nous constatons que, d'une part, le verbe بدأ (bada'a) et commencer (à/par/de) sont presque équivalents syntaxiquement et sémantiquement.
D'autre part, les verbes هم (hamma) et se mettre à servent toujours à indiquer la rapidité et la soudaineté, mais l'action du premier est toujours semi-perfective.
Nous pouvons dire qu'il existe toujours des points de divergence et convergence entre les verbes inchoatifs dans les deux langues (arabe et français).
Enfin, d'après des productions écrites d'un échantillon d'apprenants libyens du FLE, nous avons tenté de dégager des pistes pédagogiques destinées à savoir l'origine des difficultés dans l'emploi des verbes commencer (à/par/de) et se mettre à.
Aujourd'hui, la plupart des internautes ont leurs données dispersées dans plusieurs appareils, applications et services.
La gestion et le contrôle de ses données sont de plus en plus difficiles.
Dans cette thèse, nous adoptons le point de vue selon lequel l'utilisateur devrait se voir donner les moyens de récupérer et d'intégrer ses données, sous son contrôle total.
À ce titre, nous avons conçu un système logiciel qui intègre et enrichit les données d'un utilisateur à partir de plusieurs sources hétérogènes de données personnelles dans une base de connaissances RDF.
Le logiciel est libre, et son architecture innovante facilite l'intégration de nouvelles sources de données et le développement de nouveaux modules pour inférer de nouvelles connaissances.
Nous présentons un algorithme pour retrouver les points de séjour d'un utilisateur à partir de son historique de localisation.
À l'aide de ces données et de données provenant d'autres capteurs de son téléphone, d'informations géographiques provenant d'OpenStreetMap, et des horaires de transports en commun, nous présentons un algorithme de reconnaissance du mode de transport capable de retrouver les différents modes et lignes empruntés par un utilisateur lors de ses déplacements.
L'algorithme reconnaît l'itinéraire pris par l'utilisateur en retrouvant la séquence la plus probable dans un champ aléatoire conditionnel dont les probabilités se basent sur la sortie d'un réseau de neurones artificiels.
Nous montrons également comment le système peut intégrer les données du courrier électronique, des calendriers, des carnets d'adresses, des réseaux sociaux et de l'historique de localisation de l'utilisateur dans un ensemble cohérent.
Pour ce faire, le système utilise un algorithme de résolution d'entité pour retrouver l'ensemble des différents comptes utilisés par chaque contact de l'utilisateur, et effectue un alignement spatio-temporel pour relier chaque point de séjour à l'événement auquel il correspond dans le calendrier de l'utilisateur.
Enfin, nous montrons qu'un tel système peut également être employé pour faire de la synchronisation multi-système/multi-appareil et pour pousser de nouvelles connaissances vers les sources.
L'incursion de l'Islam en Afrique subsaharienne à partir du IXe siècle, s'est opérée via le commerce transsaharien entre les peuples d'Afrique du Nord et ceux du Sahel.
Ce contact entretenu par les caravanes commerciales entre les deux peuples a engendré l'islamisation progressive de la population hausaphone.
Sous l'influence de l'arabe, plusieurs vocables sont introduits dans le lexique du hausa.
Cet effet islamique s'accompagne d'une révolution dans la production de la littérature arabe-ajami.
Sur la base de ces observations, cette thèse se propose d'analyser les emprunts lexicaux arabes dans les œuvres poétiques de l'auteur, et leur intégration dans la langue hausa.
Notre corpus comprend 15 œuvres poétiques que nous avons lemmatisées en préalable aux calculs statistiques à l'aide du logiciel Excel.
Les principaux résultats obtenus sur les formes graphiques, montrent une fréquence d'utilisation très élevée des emprunts arabes.
L'association de l'analyse linguistique et des traitements informatiques, nous a permis ainsi de confirmer, de façon formelle et impartiale, que la plupart des emprunts les plus fréquents relèvent de domaines religieux, et donc liés aux lexiques de situation.
L'utilisation des dossiers médicaux électroniques (DMEs) et la prescription électronique sont des priorités dans les différents plans d'action européens sur la santé connectée.
il capture tous les épisodes symptomatiques dans la vie d'un patient et doit permettre l'amélioration des pratiques médicales et de prises en charge, à la condition de mettre en place des procédures de traitement automatique.
A ce titre nous travaillons sur la prédiction d'hospitalisation à partir des DMEs et après les avoir représentés sous forme vectorielle, nous enrichissons ces modèles afin de profiter des connaissances issues de référentiels, qu'ils soient généralistes ou bien spécifiques dans le domaine médical, et cela, dans le but d'améliorer le pouvoir prédictif d'algorithmes de classification automatique.
Déterminer les connaissances à extraire dans l'objectif de les intégrer aux représentations vectorielles est à la fois une tâche subjective et destinée aux experts, nous verrons une procédure semi-supervisée afin d'automatiser en partie ce processus.
Du fruit de nos recherches, nous avons ébauché un produit destiné aux médecins généralistes afin de prévenir l'hospitalisation de leur patient ou du moins améliorer son état de santé.
Ainsi, par le biais d'une simulation, il sera possible au médecin d'évaluer quels sont les facteurs impliqués dans le risque d'hospitalisation de son patient et de définir les actions préventives à planifier pour éviter l'apparition de cet événement.
Les recherches passées constituent pourtant une source d'information utile pour les nouveaux utilisateurs (nouvelles requêtes).
En raison de l'absence de collections ad-hoc de RI, à ce jour il y a un faible intérêt de la communauté RI autour de l'utilisation des recherches passées.
En effet, la plupart des collections de RI existantes sont composées de requêtes indépendantes.
Ces collections ne sont pas appropriées pour évaluer les approches fondées sur les requêtes passées parce qu'elles ne comportent pas de requêtes similaires ou qu'elles ne fournissent pas de jugements de pertinence.
Par conséquent, il n'est pas facile d'évaluer ce type d'approches.
En outre, l'élaboration de ces collections est difficile en raison du coût et du temps élevés nécessaires.
Une alternative consiste à simuler les collections.
Par ailleurs, les documents pertinents de requêtes passées similaires peuvent être utilisées pour répondre à une nouvelle requête.
De plus, ce principe peut également bénéficier d'un clustering des recherches antérieures selon leurs similarités.
Quatre algorithmes probabilistes pour la réutilisation des résultats de recherches passées sont ensuite proposés et évalués.
Enfin, une nouvelle mesure dans un contexte de clustering est proposée.
ANALYSE DIACHRONIQUE DU TRÉSOR DE LA LANGUE FRANÇAISEET DE L'OXFORD ENGLISH DICTIONARY : LE TRAITEMENT DES EMPRUNTS RÉSUMÉ
Il n'est pas de langue dont le lexique ne s'enrichisse au gré des emprunts, qui permettent d'accroître et de renouveler le fonds lexical au fur et à mesure que se développent les relations entre les pays et entre leurs cultures.
Les langues anglaise et française, en raison de leur rayonnement sur tous les continents, ont acquis un contingent très important de mots venus d'ailleurs, qu'elles se sont en outre souvent partagé.
En effet, du fait de leur proximité géographique et d'une histoire commune d'une grande richesse, l'anglais et le français ont été amenés à s'interpénétrer pendant plus de dix siècles.
Nous avons voulu, dans cette étude, montrer l'impact des emprunts sur les deux langues, et analyser la façon dont ils sont traités dans les dictionnaires les plus extensifs qui soient de part et d'autre de la Manche : le Trésor de la Langue Française et l'Oxford English Dictionary.
Dans une première partie, nous étudions la constitution des lexiques anglais et français au fil du temps en fonction des apports étrangers, avant de définir la notion même d'emprunt et d'en montrer la complexité.
Enfin, nous présentons le corpus sur lequel repose ce travail.
La seconde partie est consacrée à la présentation du Trésor de la Langue Française et de l'Oxford English Dictionary.
Après avoir retracé l'histoire des dictionnaires de langue et la genèse de ces deux dictionnaires, leurs caractéristiques sont mises en évidence et leur constitution finement analysée, tant sur le plan macrostructurel que sur le plan microstructurel.
Nous avons également montré les atouts que représente leur informatisation.
Dans cette recherche, nous abordons le problème de le recherche de services qui répondent à des besoins des utilisateurs exprimés sous forme de requête en texte libre.
Notre objectif est de résoudre les problèmes qui affectent l'efficacité des modèles de recherche d'information existant lorsqu'ils sont appliqués à la recherche de services dans un corpus rassemblant des descriptions standard de ces services.
Ces problèmes sont issus du fait que les descriptions des services sont brèves.
Nous avons adapté une famille de modèles de recherche d'information (IR) dans le but de contribuer à accroître l'efficacité acquise avec les modèles existant concernant la découverte de services.
En outre, nous avons mené des expériences systématiques afin de comparer notre famille de modèles IR avec ceux de l'état de l'art portant sur la découverte de service.
Des résultats des expériences, nous concluons que notre modèle basé sur l'extension des requêtes via un thésaurus co-occurrence est plus efficace en terme des mesures classiques utilisées en IR que tous les modèles étudiés dans cette recherche.
Par conséquent, nous avons mis en place ce modèle dans S3niffer, qui est un moteur de recherche de service basé sur leur description standard.
Les bases de connaissances sont des bases de données déductives où la logique est utilisée pour représenter des connaissances de domaine sur des données existantes.
Dans le cadre des règles existentielles, une base de connaissances est composée de deux couches : la couche de données qui représentent les connaissances factuelle et la couche ontologique qui incorpore des règles de déduction et des contraintes négatives.
L'interrogation de données à l'aide des ontologies est la fonction de raisonnement principale dans ce contexte.
Comme dans la logique classique, les contradictions posent un problème à l'interrogation car « d'une contradiction, on peut déduire ce qu'on veut (ex falso quodlibet) » .
Récemment, des approches d'interrogation tolérantes aux incohérences ont été proposées pour faire face à ce problème dans le cadre des règles existentielles.
Elles déploient des stratégies dites de réparation pour restaurer la cohérence.
Cependant, ces approches sont parfois inintelligibles et peu intuitives pour l'utilisateur car elles mettent souvent en œuvre des stratégies de réparation complexes.
Ce manque de compréhension peut réduire l'utilisabilité de ces approches car elles réduisent la confiance entre l'utilisateur et les systèmes qui les utilisent.
Par conséquent, la problématique de recherche que nous considérons est comment rendre intelligible à l'utilisateur l'interrogation tolérantes aux incohérences.
Pour répondre à cette question de recherche, nous proposons d'utiliser deux formes d'explication pour faciliter la compréhension des réponses retournées par une interrogation tolérante aux incohérences.
Ces deux types d'explication prennent la forme d'un dialogue entre l'utilisateur et le raisonneur au sujet des déductions retournées comme réponses à une requête donnée.
Nous étudions ces explications dans le double cadre de l'argumentation fondée sur la logique et de la dialectique formelle, comme nous étudions leurs propriétés et leurs impacts sur les utilisateurs en termes de compréhension des résultats.
Ces troubles sont courants dans la maladie de Parkinson et associés à une baisse de la qualité de vie des patients ainsi qu'à une augmentation de la charge des aidants.
Pouvoir prédire quels sont les sujets les plus à risque de développer ces troubles et quand ces troubles apparaissent est de grande importance.
L'objectif de cette thèse est d'étudier les troubles du contrôle de l'impulsivité dans la maladie de Parkinson à partir des approches statistique et de l'apprentissage automatique, et se divise en deux parties.
La première partie consiste à analyser la performance prédictive de l'ensemble des facteurs associés à ces troubles dans la littérature.
La seconde partie consiste à étudier l'association et l'utilité d'autres facteurs, en particulier des données génétiques, pour améliorer la performance prédictive.
Depuis de nombreuses années, le déploiement des TIC dans la prise en charge médicale de pathologies chroniques joue un rôle majeur notamment dans l'évolution des pratiques de santé et l'amélioration du bien-être du patient.
Les pathologies chroniques sont de nature longue et évolutive et nécessitent un suivi régulier effectué par une équipe pluridisciplinaire où différents acteurs interviennent auprès du patient.
Le patient à son tour, est amené à respecter à domicile, un protocole de soins défini et personnalisé par cette équipe.
Cependant, la forme dans la quelle le contenu du protocole est représenté n'est pas forcément complète ni facile à comprendre par les patients.
De plus, chaque patient est unique, et la définition du protocole de soins doit être personnalisée et appropriée à ses soins et traitements individuels, parfois même à ses souhaits et contraintes personnelles.
L'expertise du patient sur sa maladie chronique est une information précieuse que nous souhaitons intégrer dans un protocole de soins personnalisé afin d'améliorer la prise en charge médicale, le suivi à distance de la maladie chronique et à terme améliorer la connaissance de la pathologie chronique.
La recherche développée dans cette thèse introduit une approche qualitative pour représenter et raisonner à partir d'entités spatiales dans un espace géographique à deux dimensions.
Les patrons de mouvements entre entités dynamiques sont catégorisés à partir d'un modèle qualitatif de relations topologiques entre une ligne orientée et une région, et de relations d'orientation entre deux lignes orientées, respectivement.
Les mouvements qualitatifs sont dérivés à partir de relations spatio-temporelles qui caractérisent des entités dynamiques conceptualisées comme des points ou des régions dans un espace à deux dimensions.
Cette architecture de raisonnement permet de dériver des configurations de mouvements basiques dérivées à partir d'entités statiques et dynamiques.
L'approche est complétée par une qualification de ces configurations à partir d'expressions du langage naturel.
Les compositions de mouvements sont étudiées tout comme les transitions possibles dans des cas de données incomplètes.
Les tables de compositions sont également explorées et permettent d'étendre les possibilités de raisonnement.
Le modèle est expérimenté dans le contexte de l'analyse de trajectoires aériennes et maritimes.
Les modèles computationnels pour la compréhension automatique des textes ont suscité un vif intérêt en raison de gains de performances inhabituels au cours des dernières années, certains d'entre eux conduisant à des scores d'évaluation surhumains.
Ce succès a conduit à affirmer la création de représentations universelles de phrases.
Dans cette thèse, nous questionnons cette affirmation au travers de deux angles complémentaires.
Premièrement, les réseaux de neurones et les représentations vectorielles sont-ils suffisamment expressifs pour traiter du texte de sorte à pouvoir effectuer un large éventail de tâches complexes ?
Dans cette thèse, nous présenterons les modèles neuronaux actuellement utilisés et les techniques d'entraînement associées.
Deuxièmement, nous aborderons la question de l'universalité dans les représentation des phrases : que cachent en réalité ces prétentions à l'universalité ?
Nous décrivons quelques théories de ce qu'est le sens d'une expression textuelle, et dans une partie ultérieure de cette thèse, nous soutenons que la sémantique (contenu littéral, non situé) par rapport à la pragmatique (la partie du sens d'un texte définie par son rôle et son contexte) est prépondérante dans les données d'entraînement et d'évaluation actuelles des modèles de compréhension du langage naturel.
Pour atténuer ce problème, nous montrons que la prédiction de marqueurs de discours (classification de marqueurs de discours initialement présents entre des phrases) peut être considérée comme un signal d'apprentissage centré sur la pragmatique pour la compréhension de textes.
Nous proposons également un nouvel outil d'évaluation de la compréhension du langage naturel en se basant sur le discours et la pragmatique.
Cet outil pourrait inciter la communauté du traitement des langues à prendre en compte les considérations pragmatiques lors de l'évaluation de modèles de compréhension du langage naturel.
Cette étude porte sur l'analyse du discours identitaire du recueil "Pourquoi as-tu laissé le cheval à sa solitude ?" de Mahmoud Darwich à savoir la nature de la relation que le Moi (celui de Mahmoud Darwich) entretient avec le Moi de l'Autre.
Autrement dit, le recueil en question ressemble à un dialogue où le même et l'autre se plongent dans un discours à titre argumentatif.
La réponse à la question de la relation entre les deux parties du dialogue est abordée, premièrement, d'un point de vue thématique/théorique : la présentation de la figure emblématique du Moi darwichien perçu en tant qu'un, ayant une identité personnelle, culturelle, sociale et nationale, différente de celle dont dispose l'autre un (celui de l'Autre).
Deuxièmement, d'un point de vue pratique, le type de relation entre les deux côtés a été montré à travers l'analyse du corpus du recueil se composant de six groupes.
La communication efficace a tendance à suivre la loi du moindre effort.
Selon ce principe, en utilisant une langue donnée les interlocuteurs ne veulent pas travailler plus que nécessaire pour être compris.
Ce fait mène à la compression extrême de textes surtout dans la communication électronique, comme dans les microblogues, SMS, ou les requêtes dans les moteurs de recherche.
Cependant souvent ces textes ne sont pas auto-suffisants car pour les comprendre, il est nécessaire d'avoir des connaissances sur la terminologie, les entités nommées ou les faits liés.
Le premier objectif de notre travail est d'aider l'utilisateur à mieux comprendre un message court par l'extraction du contexte d'une source externe comme le Web ou la Wikipédia au moyen de résumés construits automatiquement.
Pour cela nous proposons une approche pour le résumé automatique de documents multiples et nous l'appliquons à la contextualisation de messages, notamment à la contextualisation de tweets.
La méthode que nous proposons est basée sur la reconnaissance des entités nommées, la pondération des parties du discours et la mesure de la qualité des phrases.
Contrairement aux travaux précédents, nous introduisons un algorithme de lissage en fonction du contexte local.
De plus, nous avons développé un algorithme basé sur les graphes pour le ré-ordonnancement des phrases.
La méthode a été également adaptée pour la génération de snippets.
Les résultats des évaluations attestent une bonne performance de notre approche.
Les traumas, accidentels ou intentionnels, constituent un problème majeur de santé publique.
En France, jusqu'à récemment, seule la sécurité routière faisait l'objet d'une attention revendiquant l'exhaustivité nationale de la surveillance épidémiologique, coordonnée par le ministère de l'Intérieur et sans lien fort avec le système de santé.
Le système actuel de centralisation des résumés de visites aux services d'urgence (réseau OSCOUR®) constitue un puissant outil de surveillance en temps réel qui permet grâce au codage des diagnostics principaux et secondaires (codes CIM-10) de fournirn des indicateurs sur certaines pathologies saisonnières comme la grippe ou la gastro entérite.
Cependant, ce type d'information n'est pas suffisant pour établir des indicateurs de surveillance liés aux traumatismes car le mécanisme ayant conduit au traumatisme (accident de la route, suicide violence, chute...) n'est pas renseigné.
L'ajout de mécanismes relatifs aux traumatismes permettrait une surveillance épidémiologique nationale des traumatismes,d'évaluer les stratégies de prévention et d'améliorer la gestion prédictive des flux de patients.
Le motif détaillé de la venue aux urgences n'est pas disponible sous forme de base de données standardisée mais se trouve être décrit en détail au sein des notes cliniques rédigée en texte libre et qui sont stockées dans les dossiers médicaux électroniques.
L'objectif global du projet est donc de développer un outil qui permettrait de créer les informations standardisées sur les mécanismes traumatiques à partir de ces textes cliniques.
À cette fin, les techniques les plus récentes de traitement du langage naturel (NLP : Natural Language Processing), branche de l'Intelligence Artificielle, seront testées, comparées et appliquées.
Notre recherche doctorale, qui s'inscrit dans le champ de l'acquisition d'une troisième langue et du plurilinguisme, a pour objectif d'apporter une meilleure compréhension du fonctionnement de la compétence plurilingue.
Elle cherche plus particulièrement à déterminer si les langues d'un plurilingue jouent des rôles distincts dans l'acquisition d'une nouvelle langue au niveau des interactions translinguistiques lexicales et syntaxiques et si ces phénomènes sont associés.
Elle examine également les activités métalinguistiques et translinguistiques permettant aux apprenants de gérer et d'appréhender la langue cible.
Pour ce faire, nous avons réalisé onze études de cas de locuteurs de l'espagnol et de l'anglais avec des niveaux variés en français L3+.
Ces participants ont été soumis à trois tâches de production orale.
Afin prendre en compte la nature complexe des interactions lorsque les langues en contact sont typologiquement proches et lorsque différents domaines linguistiques sont examinés, nous avons adopté une approche mêlant travaux menés dans le champ de l'acquisition d'une L3 à ceux menés dans l'étude des contacts de langues.
Par le biais d'une analyse quantitative et qualitative, notre travail a mis en évidence le fonctionnement de la compétence plurilingue de nos participants.
Ils activent en effet toutes leurs langues à des degrés divers, au niveau de différents types d'ITL, propriétés syntaxiques et domaines linguistiques.
Nos participants mobilisent, par ailleurs, leur conscience métalinguistique et translinguistique afin de mieux appréhender la LC et gérer leur production en recourant à des cognats, à des consultations translinguistiques et à des inférences.
La thèse se situe au carrefour des domaines du traitement automatique de la parole, de la reconnaissance des formes et de la recherche d'informations multimédia : l'indexation des émotions en vue de la recherche par le contenu.
Le travail de thèse est donc orienté vers la reconnaissance et l'indexation de l'émotion indépendante du locuteur.
Une grande partie de la thèse porte sur l'étude des paramètres avec la conclusion de l'efficacité de la combinaison entre la méthode de Sélection Forcée Séquentielle en Avant (SFSA) et la normalisation symbolique proposée.
L'autre partie de la thèse s'appuie sur l'étude des techniques de classification appliquées dans la reconnaissance de l'émotion.
Des études sur la reconnaissance de l'émotion inter-langue (interculturel) ont aussi été effectuées.
Et enfin, sur la base de ces résultats, un moteur de l'indexation sur un corpus réel a été construit.
Dans ces travaux, nous explorons comment les techniques de Générations Automatiques de Langue Naturelle (GLN) peuvent être utilisées pour aborder la tâche de génération (semi-)automatique de matériel et d'activités dans le contexte de l'apprentissage de langues assisté par ordinateur.
En particulier, nous montrons comment un Réalisateur de Surface (RS) basé sur une grammaire peut être exploité pour la création automatique d'exercices de grammaire.
Notre réalisateur de surface utilise une grammaire réversible étendue, à savoir SemTAG, qui est une Grammaire d'Arbre Adjoints à Structure de Traits (FB-TAG) couplée avec une sémantique compositionnelle basée sur l'unification.
Plus précisément, la grammaire FB-TAG intègre une représentation plate et sous-spécifiée des formules de Logique de Premier Ordre (FOL).
Dans la première partie de la thèse, nous étudions la tâche de réalisation de surface à partir de formules sémantiques plates et nous proposons un algorithme de réalisation de surface basé sur la grammaire FB-TAG optimisé, qui supporte la génération de phrases longues étant donné une grammaire et un lexique à large couverture.
L'approche suivie pour l'optimisation de la réalisation de surface basée sur FB-TAG à partir de sémantiques plates repose sur le fait qu'une grammaire FB-TAG peut être traduite en une Grammaire d'Arbres Réguliers à Structure de Traits (FB-RTG) décrivant ses arbres de dérivation.
Le langage d'arbres de dérivation de la grammaire TAG constitue un langage plus simple que le langage d'arbres dérivés, c'est pourquoi des approches de génération basées sur les arbres de dérivation ont déjà été proposées.
Notre approche se distingue des précédentes par le fait que notre encodage FB-RTG prend en compte les structures de traits présentes dans la grammaire FB-TAG originelle, ayant de ce fait des conséquences importantes par rapport à la sur-génération et la préservation de l'interface syntaxe-sémantique.
L'algorithme de génération d'arbres de dérivation que nous proposons est un algorithme de type Earley intégrant un ensemble de techniques d'optimisation bien connues : tabulation, partage-compression (sharing-packing) et indexation basée sur la sémantique.
Dans la seconde partie de la thèse, nous explorons comment notre réalisateur de surface basé sur SemTAG peut être utilisé pour la génération (semi-)automatique d'exercices de grammaire.
Habituellement, les enseignants éditent manuellement les exercices et leurs solutions et les classent au regard de leur degré de difficulté ou du niveau attendu de l'apprenant.
Un courant de recherche dans le Traitement Automatique des Langues (TAL) pour l'apprentissage des langues assisté par ordinateur traite de la génération (semi-)automatique d'exercices.
Principalement, ces travaux s'appuient sur des textes extraits du Web, utilisent des techniques d'apprentissage automatique et des techniques d'analyse de textes (par exemple, analyse de phrases, POS tagging, etc.).
Ces approches confrontent l'apprenant à des phrases qui ont des syntaxes potentiellement complexes et du vocabulaire varié.
En revanche, l'approche que nous proposons dans cette thèse aborde la génération (semi-)automatique d'exercices du type rencontré dans les manuels pour l'apprentissage des langues.
Il s'agit, en d'autres termes, d'exercices dont la syntaxe et le vocabulaire sont faits sur mesure pour des objectifs pédagogiques et des sujets donnés.
Les approches de génération basées sur des grammaires associent les phrases du langage naturel avec une représentation linguistique fine de leur propriété morpho-syntaxiques et de leur sémantique grâce à quoi il est possible de définir un langage de contraintes syntaxiques et morpho-syntaxiques permettant la sélection de phrases souches en accord avec un objectif pédagogique donné.
Les définitions des mots « sentiment » , « opinion » et « émotion » sont toujours très vagues comme l'atteste aussi le dictionnaire qui semble expliquer un mot en utilisant le deux autres.
Tout le monde est affecté par les opinions : les entreprises pour vendre les produits, les gens pour les acheter et, plus en général, pour prendre des décisions, les chercheurs en intelligence artificielle pour comprendre la nature de l'être humain.
Aujourd'hui on a une quantité d'information disponible jamais vue avant, mais qui résulte peu accessible.
Les mégadonnées (en anglais « big data » ) ne sont pas organisées, surtout pour certaines langues – dont la difficulté à les exploiter.
La recherche française souffre d'une manque de ressources « prêt-à-porter » pour conduire des tests.
Cette thèse a l'objectif d'explorer la nature des sentiments et des émotions, dans le cadre du Traitement Automatique du Langage et des Corpus.
Les contributions de cette thèse sont plusieurs : création de nouvelles ressources pour l'analyse du sentiment et de l'émotion, emploi et comparaison de plusieurs techniques d'apprentissage automatique, et plus important, l'étude du problème sous différents points de vue : classification des commentaires en ligne en polarité (positive et négative), Aspect-Based Sentiment Analysis des caractéristiques du produit recensé.
Enfin, un étude psycholinguistique, supporté par des approches lexicales et d'apprentissage automatique, sur le rapport entre qui juge et l'objet jugé.
Si les moteurs de recherche actuels sont efficaces pour trouver des documents correspondant à un besoin d'information large, c'est-à-dire pour fournir une liste de documents sur un sujet donné, ils répondent moins bien à un besoin d'information précise comme " Qui était le président des Etats-Unis en 1978 ? ".
Les systèmes de questions-réponses tentent de répondre à de tels besoins.
La requête par mots-clefs d'un moteur de recherche classique est remplacée par une question, donc une requête en langage naturel, et la sortie est constituée de la réponse précise à la question, au lieu d'une liste de documents à parcourir.
Le domaine de questions-réponses tire ainsi parti des possibilités actuelles en Recherche d'Information (RI), mais lui apporte une interaction facilitée avec l'utilisateur, grâce à la manipulation et la compréhension du langage naturel hérités du Traitement Automatique des Langues (TAL).
L'objectif de ce travail est d'étudier précisément comment accéder à l'information précise recherchée, et en particulier ce qui caractérise une réponse à une question d'un point de vue du TAL.
Nous nous demanderons ainsi ce que peut être une réponse à une question et quel niveau de connaissances linguistiques doit être utilisé par le système pour reconnaître le lien entre une question et une réponse potentielle.
Nous nous sommes intéressée à la problématique de ce lien entre question et réponse sous un angle syntaxique, afin de déterminer l'impact de ce type de connaissances dans un procesus de recherche d'information précises.
La recherche d'information (RI) dans des documents semi-structurés (écrits en XML en pratique) combine des aspects de la RI traditionnelle et ceux de l'interrogation de bases de données.
La structure a une importance primordiale, mais le besoin d'information reste vague.
L'unité de recherche est variable (un paragraphe, une figure, un article complet…).
Par ailleurs, la flexibilité du langage XML autorise des manipulations du contenu qui provoquent parfois des ruptures arbitraires dans le flot naturel du texte.
Les problèmes posés par ces caractéristiques sont nombreux, que ce soit au niveau du pré-traitement des documents ou de leur interrogation.
Face à ces problèmes, nous avons étudié les solutions spécifiques que pouvait apporter le traitement automatique de la langue (TAL).
Nous avons ainsi proposé un cadre théorique et une approche pratique pour permettre l'utilisation des techniques d'analyse textuelle en faisant abstraction de la structure.
Nous avons également conçu une interface d'interrogation en langage naturel pour la RI dans les documents XML, et proposé des méthodes tirant profit de la structure pour améliorer la recherche des éléments pertinents.
Cette thèse traite d'une approche, guidée par une ontologie, conçue pour annoter les documents d'un corpus où chaque document décrit une entité de même type.
Dans notre contexte, l'ensemble des documents doit être annoté avec des concepts qui sont en général trop spécifiques pour être explicitement mentionnés dans les textes.
De plus, les concepts d'annotation ne sont représentés au départ que par leur nom, sans qu'aucune information sémantique ne leur soit reliée.
Enfin, les caractéristiques des entités décrites dans les documents sont incomplètes.
La phase de peuplement (1) ajoute dans l'ontologie des informations provenant des documents du corpus mais aussi du Web des données (Linked Open Data ou LOD).
Le LOD représente aujourd'hui une source prometteuse pour de très nombreuses applications du Web sémantique à condition toutefois de développer des techniques adaptées d'acquisition de données.
Dans le cadre de SAUPODOC, le peuplement de l'ontologie doit tenir compte de la diversité des données présentes dans le LOD : propriétés multiples, équivalentes, multi-valuées ou absentes.
Les correspondances à établir, entre le vocabulaire de l'ontologie à peupler et celui du LOD, étant complexes, nous proposons un modèle pour faciliter leur spécification.
Puis, nous montrons comment ce modèle est utilisé pour générer automatiquement des requêtes SPARQL et ainsi faciliter l'interrogation du LOD et le peuplement de l'ontologie.
Celle-ci, une fois peuplée, est ensuite enrichie(2) avec les concepts d'annotation et leurs définitions qui sont apprises grâce à des exemples de documents annotés.
Un raisonnement sur ces définitions permet enfin d'obtenir les annotations souhaitées.
Des expérimentations ont été menées dans deux domaines d'application, et les résultats, comparés aux annotations obtenues avec des classifieurs, montrent l'intérêt de l'approche.
Cette nouvelle ère de petits UAV qui peuplent actuellement l'espace aérien soulève de nombreuses préoccupations en matière de sécurité, en raison de l'absence de pilote à bord et de la nature moins précise des capteurs.
Cela nécessite des approches intelligentes pour faire face aux situations d'urgence qui se produiront inévitablement pour toutes les catégories d'opérations d'UAV telles que définies par l'AESA (Agence européenne de la sécurité aérienne).
Les limitations matérielles de ces petits véhicules suggèrent l'utilisation de la redondance analytique plutôt que la pratique habituelle de la redondance matérielle dans l'aviation humaine.
Au cours de cette étude, des pratiques d'apprentissage automatique sont mises en œuvre afin de diagnostiquer les défaillances d'un petit drone à voilure fixe afin d'éviter le fardeau de la modélisation précise nécessaire au diagnostic par le modèle.
Une méthode de classification supervisée, SVM (Support Vector Machines), permet de classer les défauts.
Les données utilisées pour diagnostiquer les défauts sont les mesures de gyroscope et d'accéléromètre.
L'idée de restreindre le jeu de données aux mesures d'accéléromètre et de gyroscope est de vérifier la capacité de classification de la méthode, avec un jeu de puces petit et peu coûteux, sans avoir à accéder aux données du pilote automatique, telles que les informations d'entrée de commande.
Ce travail aborde les défauts dans les surfaces de contrôle d'un UAV.
Plus précisément, les défauts considérés sont la surface de contrôle coincée en angle et la perte d'efficacité.
Tout d'abord, un modèle d'aéronef est simulé.
Ce modèle n'est pas utilisé pour la conception d'algorithmes FDD (Fault Detection and Diagnosis), mais est utilisé pour générer des données.
Des données simulées sont utilisées à la place des données de vol pour isoler les effets probables du contrôleur sur le diagnostic, ce qui peut compliquer une étude préliminaire sur les FDD pour les drones.
Les résultats montrent que pour les mesures simulées, SVM donne des résultats très précis sur la classification des défauts de perte d'efficacité sur les surfaces de contrôle.
Ces résultats prometteurs appellent un complément d'investigation afin d'évaluer les performances du SVM en matière de classification des anomalies avec les données de vol.
Des vols réels ont été organisés pour générer des données de vol erronées en manipulant le pilote automatique open source, Paparazzi.
Toutes les données et le code sont disponibles dans le système de partage de code et de versions, Github.
La formation est interrompue en raison du besoin de données étiquetées et du fardeau informatique lié à la phase de réglage des classificateurs.
Les résultats montrent que, d'après les données de vol, SVM donne un score F1 de 0,98 pour la classification des failles bloquées à la surface de contrôle.
En ce qui concerne les défauts de perte d'efficacité, il est nécessaire de recourir à certaines techniques d'ingénierie, impliquant l'ajout de mesures antérieures, pour obtenir les mêmes performances de classification.
Un résultat prometteur est découvert lorsque les spinors sont utilisés comme caractéristiques au lieu de vitesses angulaires.
Les résultats montrent qu'en utilisant les spinors pour la classification, la précision de la classification est considérablement améliorée, en particulier lorsque les classificateurs ne sont pas accordés.
À l'aide de spinors et d'un noyau Gaussian, un classifieur sans accord donne un score F1 de 0,9555, soit 0,2712 lorsque les mesures gyroscopiques ont été utilisées.
En résumé, ce travail montre que SVM donne une performance satisfaisante pour la classification des défauts sur les gouvernes d'un drone à l'aide de données de vol.
L'idée directrice est que la coordination efficace entre les membres d'une équipe améliore la productivité et réduit les erreurs individuelles et collectives.
Cette thèse traite de la mise en place et du maintien de la coordination au sein d'une équipe de travail composée d'agents et d'humains interagissant dans un EVCF.L'objectif de ces recherches est de doter les agents virtuels de comportements conversationnels permettant la coopération entre agents et avec l'utilisateur dans le but de réaliser un but commun.
Nous proposons une architecture d'agents Collaboratifs et Conversationnels, dérivée de l'architecture Belief-Desire-Intention (C2-BDI), qui gère uniformément les comportements délibératifs et conversationnels comme deux comportements dirigés vers les buts de l'activité collective.
Nous proposons un modèle intégré de la coordination fondé sur l'approche des modèles mentaux partagés, afin d'établir la coordination au sein de l'équipe de travail composée d'humains et d'agents.
Nous soutenons que les interactions en langage naturel entre les membres d'une équipe modifient les modèles mentaux individuels et partagés des participants.
Enfin, nous décrivons comment les agents mettent en place et maintiennent la coordination au sein de l'équipe par le biais de conversations en langage naturel.
Afin d'établir un couplage fort entre la prise de décision et le comportement conversationnel collaboratif d'un agent, nous proposons tout d'abord une approche fondée sur la modélisation sémantique des activités humaines et de l'environnement virtuel via le modèle mascaret puis, dans un second temps, une modélisation du contexte basée sur l'approche Information State.
Ces représentations permettent de traiter de manière unifiée les connaissances sémantiques des agents sur l'activité collective et sur l'environnement virtuel ainsi que des informations qu'ils échangent lors de dialogues.
State nous permet de doter les agents C2BDI de capacités communicatives leur permettant de s'engager pro-activement dans des interactions en langue naturelle en vue de coordonner efficacement leur activité avec les autres membres de l'équipe.
De plus, nous définissons les protocoles conversationnels collaboratifs favorisant la coordination entre les membres de l'équipe.
Enfin, nous proposons dans cette thèse un mécanisme de prise de décision s'inspirant de l'approche BDI qui lie les comportements de délibération et de conversation des agents.
Nous avons mis en oeuvre notre architecture dans trois différents scénarios se déroulant dans des EVCF.
Nous montrons que les comportements conversationnels collaboratifs multipartites des agents C2BDI facilitent la coordination effective de l'utilisateur avec les autres membres de l'équipe lors de la réalisation d'une tâche partagée.
De nombreux domaines ont intérêt à étudier les points de vue exprimés en ligne, que ce soit à des fins de marketing, de cybersécurité ou de recherche avec l'essor des humanités numériques.
Nous proposons dans ce manuscrit deux contributions au domaine de la fouille de points de vue, axées sur la difficulté à obtenir des données annotées de qualité sur les médias sociaux.
Notre première contribution est un jeu de données volumineux et complexe de 22853 profils Twitter actifs durant la campagne présidentielle française de 2017.
C'est l'un des rares jeux de données considérant plus de deux points de vue et, à notre connaissance, le premier avec un grand nombre de profils et le premier proposant des communautés politiques recouvrantes.
Ce jeu de données peut être utilisé tel quel pour étudier les mécanismes de campagne sur Twitter ou pour évaluer des modèles de détection de points de vue ou des outils d'analyse de réseaux.
Nous proposons ensuite deux modèles génériques semi-supervisés de détection de points de vue, utilisant une poignée de profils-graines, pour lesquels nous connaissons le point de vue, afin de catégoriser le reste des profils en exploitant différentes proximités inter-profils.
En effet, les modèles actuels sont généralement fondés sur les spécificités de certaines plateformes sociales, ce qui ne permet pas l'intégration de la multitude de signaux disponibles.
En construisant des proximités à partir de différents types d'éléments disponibles sur les médias sociaux, nous pouvons détecter des profils suffisamment proches pour supposer qu'ils partagent une position similaire sur un sujet donné, quelle que soit la plateforme.
Notre premier modèle est un modèle ensembliste séquentiel propageant les points de vue grâce à un graphe multicouche représentant les proximités entre les profils.
En utilisant des jeux de données provenant de deux plateformes, nous montrons qu'en combinant plusieurs types de proximité, nous pouvons correctement étiqueter 98% des profils.
Notre deuxième modèle nous permet d'observer l'évolution des points de vue des profils pendant un événement, avec seulement un profil-graine par point de vue.
Ce modèle confirme qu'une grande majorité de profils ne changent pas de position sur les médias sociaux, ou n'expriment pas leur revirement.
Face à la masse grandissante des données textuelles présentes sur le Web, le résumé automatique d'une collection de documents traitant d'un sujet particulier est devenu un champ de recherche important du Traitement Automatique des Langues.
Les expérimentations décrites dans cette thèse s'inscrivent dans cette perspective.
SemEval 2014 pour la langue anglaise et des ressources que nous avons construites pour la langue française.
Les bonnes performances des mesures proposées nous ont amenés à les utiliser dans une tâche de résumé multi-documents, qui met en oeuvre un algorithme de type PageRank.
Le système a été évalué sur les données de DUC 2007 pour l'anglais et le corpus RPM2 pour le français.
Les résultats obtenus par cette approche simple, robuste et basée sur une ressource aisément disponible dans de nombreuses langues, se sont avérés très encourageants
L'importance des systèmes collaboratifs a considérablement augmenté au cours des dernières années.
La majorité de nouvelles applications sont conçues de manière distribuée pour répondre aux besoins du travail collaboratif.
RCE) qui permettent la manipulation de divers objets partagés, tels que les pages wiki ou les articles scientifiques par plusieurs personnes réparties dans le temps et dans l'espace
Dans cette thèse, nous proposons un modèle de contrôle d'accès générique basé sur l'approche de réplication optimiste du document partagé ainsi que sa politique de contrôle d'accès.
Pour cela, nous proposons une approche optimiste de contrôle d'accès dans la mesure où un utilisateur peut violer temporairement la politique de sécurité.
Vu l'absence d'une solution d'annulation générique et correcte, nous proposons une étude théorique du problème d'annulation et nous concevons une solution générique basée sur une nouvelle sémantique de l'opération identité.
Afin de valider notre approche tous nos algorithmes ont été implémentés en Java et testés sur la plateforme distribuée Grid'5000
Cette thèse s'inscrit dans le domaine de la recherche d'information (RI) et la recommandation de lecture.
Elle a pour objets : — La création de nouvelles approches de recherche de documents utilisant des techniques de combinaison de résultats, d'agrégation de données sociales et de reformulation de requêtes ; — La création d'une approche de recommandation utilisant des méthodes de RI et les graphes entre les documents.
Deux collections de documents ont été utilisées.
Une collection qui provient de l'évaluation CLEF (tâche Social Book Search-SBS) et la deuxième issue du domaine des sciences humaines et sociales (OpenEdition, principalement Revues.org).
La modélisation des documents de chaque collection repose sur deux types de relations : — Dans la première collection (CLEF SBS), les documents sont reliés avec des similarités calculées par Amazon qui se basent sur plusieurs facteurs (achats des utilisateurs, commentaires, votes, produits achetés ensemble, etc.) ; — Dans la deuxième collection (OpenEdition), les documents sont reliés avec des relations de citations (à partir des références bibliographiques).
Le manuscrit est structuré en deux parties.
La première partie « état de l'art » regroupe une introduction générale, un état de l'art sur la RI et sur les systèmes de recommandation.
La deuxième partie « contributions » regroupe un chapitre sur la détection de comptes rendus de lecture au sein de la collection OpenEdition (Revues.org), un chapitre sur les méthodes de RI utilisées sur des requêtes complexes et un dernier chapitre qui traite l'approche de recommandation proposée qui se base sur les graphes.
La recherche de réponses à des questions relève de deux disciplines : le traitement du langage naturel et la recherche d'information.
L'émergence de l'apprentissage profond dans plusieurs domaines de recherche tels que la vision par ordinateur, le traitement du langage naturel etc. a conduit à l'émergence de modèles de bout en bout.
Dans le cadre du projet GoASQ, l'objectif est d'étudier, comparer et combiner différentes approches pour répondre à des questions formulées en langage naturel sur des données textuelles, en domaine ouvert et en domaine biomédical.
Ce travail se concentre principalement sur 1) la construction de modèles permettant de traiter des ensembles de données à petite et à grande échelle, et 2) l'exploitation de connaissances sémantiques pour répondre aux questions par leur intégration dans les différents modèles.
Nous visons à fusionner des connaissances issues de textes libres, d'ontologies, de représentations d'entités, etc.
Afin de faciliter l'utilisation des modèles neuronaux sur des données de domaine de spécialité, nous nous plaçons dans le cadre de l'adaptation de domaine.
Nous avons proposé deux modèles de tâches de QR différents, évalués sur la tâche BIOASQ de réponse à des questions biomédicales.
Nous montrons par nos résultats expérimentaux que le modèle de QR ouvert convient mieux qu'une modélisation de type Compréhension machine.
Nous pré-entrainons le modèle de Compréhension machine, qui sert de base à notre modèle, sur différents ensembles de données pour montrer la variabilité des performances.
Nous constatons que l'utilisation d'un ensemble de données particulier pour le pré-entraînement donne les meilleurs résultats lors du test et qu'une combinaison de quatre jeux de données donne les meilleurs résultats lors de l'adaptation au domaine biomédical.
Nous avons testé des modèles de langage à grande échelle, comme BERT, qui sont adaptés à la tâche de réponse aux questions.
Les performances varient en fonction du type des données utilisées pour pré-entrainer BERT.
Ainsi, le modèle de langue appris sur des données biomédicales, BIOBERT, constitue le meilleur choix pour le QR biomédical.
Nous avons annoté manuellement et automatiquement un jeu de données par les variantes des réponses de BIOASQ et montré l'importance d'apprendre un modèle de QR avec ces variantes.
Ces types sont ensuite utilisés pour mettre en évidence les entités dans les jeux de données, ce qui montre des améliorations sur l'état de l'art.
Par ailleurs l'exploitation de représentations vectorielles d'entités dans les modèles se montre positif pour le domaine ouvert.
Nous faisons l'hypothèse que les résultats obtenus à partir de modèles d'apprentissage profond peuvent être encore améliorés en utilisant des traits sémantiques et des traits collectifs calculés à partir des différents paragraphes sélectionnés pour répondre à une question.
Nous utilisons des modèles de classification binaires pour améliorer la prédiction de la réponse parmi les K candidats à l'aide de ces caractéristiques, conduisant à un modèle hybride qui surpasse les résultats de l'état de l'art.
Enfin, nous avons évalué des modèles de QR ouvert sur des ensembles de données construits pour les tâches de Compréhension machine et Sélection de phrases.
Nous montrons la différence de performance lorsque la tâche à résoudre est une tâche de QR ouverte et soulignons le fossé important qu'il reste à franchir dans la construction de modèles de bout en bout pour la tâche complète de réponse aux questions.
Cette thèse présente : 1) le développement d'une nouvelle approche pour trouver des associations directes entre des paires d'éléments liés indirectement à travers diverses caractéristiques communes, 2) l'utilisation de cette approche pour associer directement des fonctions biologiques aux domaines protéiques (ECDomainMiner et GODomainMiner) et pour découvrir des interactions domaine-domaine, et enfin 3) l'extension de cette approche pour annoter de manière complète à partir des domaines les structures et les séquences des protéines.
En utilisant des associations de domaines ayant acquis des annotations fonctionnelles inférées, et en tenant compte des informations de taxonomie, des milliers de règles d'annotation ont été générées automatiquement.
Ensuite, ces règles ont été utilisées pour annoter des séquences de protéines dans la base de données TrEMBL
Le travail comprend la définition d'un modèle pour les prédicats d'opinion et leurs arguments (la source, le sujet et le message), la création d'un lexique de prédicats d'opinions auxquels sont associées des informations provenant du modèle et la réalisation de trois systèmes informatiques.
En effet ces différents travaux obtiennent des scores qui se situent entre 63% et 89,5%.Par ailleurs, en sus des systèmes réalisés pour l'identification de l'opinion, notre travail a débouché sur la construction de plusieurs ressources pour l'espagnol : un lexique de prédicats d'opinions, un corpus de 13000 mots avec des annotations sur les opinions et un corpus de 40000 mots avec des annotations sur les prédicats d'opinion et les sources.
Transmettre une émotion ou exprimer une sensation par le discours nécessite de l'interlocuteur qu'il construise des stratégies discursives situées aussi bien dans une situation de communication spécifique que dans une langue-culture particulière.
Cette étude sur les langues de spécialité en contexte équatorien met l'accent sur le locuteur et les stratégies qu'il met en place pour exprimer ses émotions pendant une session de dégustation.
En effet, plusieurs interrogations se posent dont celle de la caractérisation d'un genre discursif lié à une praxis sociale : « Existe-t-il un genre de discours dont le protocole de création et le prototype discursif seraient spécifiques à la dégustation ? » ;
celle d'une production discursive liée à l'identité de son locuteur utilisant différents lexiques-grammaires pour communiquer sur un même produit : « Les experts et les consommateurs utilisent-ils les mêmes stratégies discursives ? » ;
celle d'une terminologie située : « Comment les représentations sociales et les pratiques discursives modulent-elles les contributions des locuteurs impliqués ? » .
En effet, ces interrogations doivent permettre de caractériser l'expression de la sensorialité située dans un contexte professionnel spécifique-la dégustation en espagnol dans la culture équatorienne-tout en proposant une lecture scientifique des choix terminologiques utilisés comme descripteurs sensoriels et hédoniques incluant une dimension émotionnelle.
Cette recherche s'inscrit donc dans le cadre théorique de la sémantique cognitive, de la linguistique de corpus et de l'analyse textométrique des pratiques discursives représentatives de praxis socioculturelles.
La méthodologie proposée utilise les outils informatiques et la technique de fouille de texte propres au traitement automatique des langues appliquée au trois corpus suivants.
1. La compilation d'un corpus de textes diachroniques à partir de la production écrite de la presse spécialisée des dix dernières années dans lequel seront identifiés les descripteurs actuellement utilisés pour décrire les sensations et les émotions qui peuvent être perçues lors de la dégustation d'un carré de chocolat.
2. La compilation d'un corpus de textes produits par les locuteurs s'exprimant sur la conceptualisation et la signification qu'ils possèdent de ces mêmes descripteurs.
Ce corpus sera compilé sans contact avec le produit, le chocolat, de façon à mettre l'accent sur la dimension cognitive de la représentation sociale des descripteurs sélectionnés.
3. Et la compilation d'un corpus de textes à partir de productions orales visant la mise en discours de ces descripteurs pour exprimer des émotions et des sensations perçues lors d'une session de dégustation de chocolat.
L'analyse de ces différents corpus offre un objet d'étude authentique et représentatif de la perception que peut avoir le locuteur lorsqu'il met en place des stratégies discursives visant à communiquer une émotion ou exprimer une sensation.
Cette méthode permet une interprétation qualitative à partir d'une analyse quantitative de données authentiques de façon à proposer une description des différentes stratégies d'expression de l'expérience sensorielle.
Parallèlement à la constitution de ces corpus, l'application d'un questionnaire avant la session de dégustation permet de caractériser le locuteur et de collecter des informations sur les représentations sociales qu'il pourrait convoquer dans ses choix discursifs.
Au-delà de la constitution d'un corpus inédit et de son analyse discursive, cette recherche présente l'intérêt de proposer une étude des stratégies utilisées pour exprimer les émotions et les sensations dans un contexte de dégustation, elle permet ainsi d'offrir une description d'un genre discursif spécifique qui peut apporter des améliorations au discours produit pour s'adresser aux consommateurs potentiels.
Le principal objectif du projet constitue également sa principale originalité.
Il consiste à extraire automatiquement la structure narrative des séries TV.
Les séries télévisées actuelles sont basées sur des structures complexes impliquant plusieurs arcs d'histoires entrelacés dans le même épisode.
La première barrière scientifique est donc liée à l'écart dit sémantique entre l'histoire réelle véhiculée par les séries TV et le type d'information qui peut être automatiquement extrait et traité par les programmes informatiques.
Une approche naïve pourrait être basée uniquement sur la recherche d'informations telles que la structure temporelle de la collection, les personnages (qui sont-ils ?
Mais, ce problème difficile ne peut pas être résolu en utilisant uniquement un seul de ces aspects (Qui ?
La thèse, effectuée dans le cadre d'une bourse CIFRE, et prolongeant un des aspects du projet ANR Traouiero, aborde d'abord la production, l'extension et l'amélioration de corpus multilingues par traduction automatique (TA) et post-édition contributive (PE).
Des améliorations fonctionnelles et techniques ont été apportées aux logiciels SECTra et iMAG, et on a progressé vers une définition générique de la structure d'un corpus multilingue, multi-annoté et multimédia, pouvant contenir des documents classiques aussi bien que des pseudo-documents et des méta-segments.
Dans le cadre d'un projet interne sur le site du LIG et d'un projet (TABE-FC) en coopération avec l'université de Xiamen, on a pu démontrer l'intérêt de l'apprentissage incrémental en TA statistique, sous certaines conditions, grâce à une expérience qui s'est étalée sur toute la thèse.
La troisième partie est consacrée à des contributions et mises à disposition de supports informatiques et de ressources.
Les principales se placent dans le cadre du projet COST MUMIA de l'EU et résultent de l'exploitation de la collection CLEF-2011 de 1,5 M de brevets partiellement multilingues.
De grosses mémoires de traductions en ont été extraites (17,5 M segments), 3 systèmes de TA en ont été tirés, et un site Web de support à la RI multilingue sur les brevets a été construit.
On décrit aussi la réalisation en cours de JianDan-eval, une plate-forme de construction, déploiement et évaluation de systèmes de TA.
La modélisation de processus complexes peut impliquer un grand nombre de variables ayant entre elles une structure de corrélation compliquée.
Par exemple, les phénomènes spatiaux possèdent souvent une forte régularité spatiale, se traduisant par une corrélation entre variables d'autant plus forte que les régions correspondantes sont proches.
Le formalisme des graphes pondérés permet de capturer de manière compacte ces relations entre variables, autorisant la formalisation mathématique de nombreux problèmes d'analyse de données spatiales.
Nous présentons une stratégie de préconditionnement pour l'algorithme generalized forward-backward, spécifiquement adaptée à la résolution de problèmes structurés par des graphes pondérés présentant une grande variabilité de configurations et de poids.
Ces algorithmes présentent des performances supérieures à l'état de l'art pour des tâches d'agrégations de données geostatistiques.
La seconde partie de ce document se concentre sur le développement d'un nouveau modèle qui étend les chaînes de Markov à temps continu au cas des graphes pondérés non orientés généraux.
Ce modèle autorise la prise en compte plus fine des interactions entre noeuds voisins pour la prédiction structurée, comme illustré pour la classification supervisée de tissus urbains.
Cette thèse présente une approche d'analyse des textes non-standardisé qui consiste à modéliser une chaine de traitement permettant l'annotation automatique de textes à savoir l'annotation grammaticale en utilisant une méthode d'étiquetage morphosyntaxique et l'annotation sémantique en mettant en œuvre un système de reconnaissance des entités nommées.
Dans ce contexte, nous présentons un système d'analyse du Moyen Français qui est une langue en pleine évolution dont l'orthographe, le système flexionnel et la syntaxe ne sont pas stables.
Les textes en Moyen Français se singularisent principalement par l'absence d'orthographe normalisée et par la variabilité tant géographique que chronologique des lexiques médiévaux.
L'objectif est de mettre en évidence un système dédié à la construction de ressources linguistiques, notamment la construction des dictionnaires électroniques, se basant sur des règles de morphologie.
Ensuite, nous présenterons les instructions que nous avons établies pour construire un étiqueteur morphosyntaxique qui vise à produire automatiquement des analyses contextuelles à l'aide de grammaires de désambiguïsation.
Finalement, nous retracerons le chemin qui nous a conduits à mettre en place des grammaires locales permettant de retrouver les entités nommées.
De ce fait, nous avons été amenés à constituer un corpus MEDITEXT regroupant des textes en Moyen Français apparus entre le fin du XIIIème et XVème siècle.
Nous traitons dans cette thèse la modélisation de la coarticulation par les réseaux de neurones, dans l'objectif de synchroniser l'animation d'un visage virtuel 3D à de la parole.
Nous proposons dans cette thèse un modèle de coarticulation, c'est-à-dire un modèle qui prédit les trajectoires spatiales des articulateurs à partir de la parole.
Nous exploiterons pour cela un modèle séquentiel, les réseaux de neurones récurrents (RNN), et plus particulièrement les Gated Recurrent Units, capables de considérer la dynamique de l'articulation au cœur de leur modélisation.
Malheureusement, la quantité de données classiquement disponible dans les corpus articulatoires et audiovisuels semblent de prime-abord faibles pour une approche deep learning.
Pour pallier cette difficulté, nous proposons une stratégie permettant de fournir au modèle des connaissances sur les gestes articulatoires du locuteur dès son initialisation.
La robustesse des RNNs nous a permis d'implémenter notre modèle de coarticulation pour prédire les mouvements des lèvres pour le français et l'allemand, et de la langue pour l'anglais et l'allemand.
L'évaluation du modèle fut réalisée par le biais de mesures objectives de la qualité des trajectoires et par des expériences permettant de valider la bonne réalisation des cibles articulatoires critiques.
Nous avons également réalisé une évaluation perceptive de la qualité de l'animation des lèvres du visage parlant.
Enfin, nous avons conduit une analyse permettant d'explorer les connaissances phonétiques acquises par le modèle après apprentissage.
Les systèmes de traduction automatique obtiennent aujourd'hui de bons résultats sur certains couples de langues comme anglais – français, anglais – chinois, anglais – espagnol, etc.
Toutefois, la recherche sur la traduction automatique pour des paires de langues dites « peu dotés » doit faire face au défi du manque de données.
Nous avons ainsi abordé le problème d'acquisition d'un grand corpus de textes bilingues parallèles pour construire le système de traduction automatique probabiliste.
L'originalité de notre travail réside dans le fait que nous nous concentrons sur les langues peu dotées, où des corpus de textes bilingues parallèles sont inexistants dans la plupart des cas.
Ce manuscrit présente notre méthodologie d'extraction d'un corpus d'apprentissage parallèle à partir d'un corpus comparable, une ressource de données plus riche et diversifiée sur l'Internet.
Nous proposons trois méthodes d'extraction.
La première méthode suit l'approche de recherche classique qui utilise des caractéristiques générales des documents ainsi que des informations lexicales du document pour extraire à la fois les documents comparables et les phrases parallèles.
Cependant, cette méthode requiert des données supplémentaires sur la paire de langues.
La deuxième méthode est une méthode entièrement non supervisée qui ne requiert aucune donnée supplémentaire à l'entrée, et peut être appliquée pour n'importe quelle paires de langues, même des paires de langues peu dotées.
La dernière méthode est une extension de la deuxième méthode qui utilise une troisième langue, pour améliorer les processus d'extraction de deux paires de langues.
Les méthodes proposées sont validées par des expériences appliquées sur la langue peu dotée vietnamienne et les langues française et anglaise.
Cette thèse décrit les applications du traitement automatique des langues (TAL) à la gestion des risques industriels.
Elle se concentre sur le domaine de l'aviation civile, où le retour d'expérience (REX) génère de grandes quantités de données, sous la forme de rapports d'accidents et d'incidents.
Nous commençons par faire un panorama des différentes types de données générées dans ce secteur d'activité.
Nous analysons les documents, comment ils sont produits, collectés, stockés et organisés ainsi que leurs utilisations.
Nous montrons que le paradigme actuel de stockage et d'organisation est mal adapté à l'utilisation réelle de ces documents et identifions des domaines problématiques ou les technologies du langage constituent une partie de la solution.
Répondant précisément aux besoins d'experts en sécurité, deux solutions initiales sont implémentées : la catégorisation automatique de documents afin d'aider le codage des rapports dans des taxonomies préexistantes et un outil pour l'exploration de collections de rapports, basé sur la similarité textuelle.
En nous basant sur des observations de l'usage de ces outils et sur les retours de leurs utilisateurs, nous proposons différentes méthodes d'analyse des textes issus du REX et discutons des manières dont le TAL peut être appliqué dans le cadre de la gestion de la sécurité dans un secteur à haut risque.
En déployant et évaluant certaines solutions, nous montrons que même des aspects subtils liés à la variation et à la multidimensionnalité du langage peuvent être traités en pratique afin de gérer la surabondance de données REX textuelles de manière ascendante
Cette thèse porte sur l'analyse et la génération de mouvements expressifs pour des personnages humains virtuels.
Sur la base de résultats d'état de l'art issus de trois domaines de recherche différents (la perception des émotions et du mouvement biologique, la reconnaissance automatique des émotions et l'animation de personnages), une représentation en faible dimension des mouvements a été proposée.
Cette représentation est constituée de trajectoires spatio-temporelles des extrémités des chaînes articulées (tête, mains et pieds) et du pelvis.
Nous avons soutenu que cette représentation est à la fois appropriée et suffisante pour caractériser le contenu expressif du mouvement humain et pour contrôler la génération de mouvements corporels expressifs.
Pour étayer ces affirmations, cette thèse propose :
i.) Une nouvelle base de données de mouvements capturés.
Cette base de données a été inspirée par la théorie du théâtre physique et contient des exemples de différentes catégories de mouvements (à savoir des mouvements périodiques, des mouvements fonctionnels, des mouvements spontanés et des séquences de mouvements théâtraux), produit avec des états émotionnels distincts (joie, tristesse, détente, stress et neutre) et interprétés par plusieurs acteurs.
ii.) Une étude perceptuelle et une approche basée classification automatique conçus pour évaluer qualitativement et quantitativement la quantité d'information liée aux émotions encore véhiculée et codée dans la représentation proposée.
Nous avons observé que, bien que de légères différences dans la performance aient été trouvées par rapport à la situation dans laquelle le corps entier a été utilisé, notre représentation conserve la plupart des qualités de mouvements liées à l'expression de l'affect et d'émotions.
iii.) Un système de synthèse de mouvement capable : (a) de reconstruire des mouvements du corps entier à partir de la représentation à faible dimension proposée, (b) de produire de nouvelles trajectoires extrémités expressives (incluant la trajectoire du pelvis).
Une évaluation quantitative et qualitative des mouvements du corps entier générés montre que ces mouvements sont aussi expressifs que les mouvements enregistrés à partir d'acteurs humains.
L'extraction d'information spatiale à partir de données textuelles est désormais un sujet de recherche important dans le domaine du Traitement Automatique du Langage Naturel (TALN).
Elle répond à un besoin devenu incontournable dans la société de l'information, en particulier pour améliorer l'efficacité des systèmes de Recherche d'Information (RI) pour différentes applications (tourisme, aménagement du territoire, analyse d'opinion, etc.).
De tels systèmes demandent une analyse fine des informations spatiales contenues dans les données textuelles disponibles (pages web, courriels, tweets, SMS, etc.).
Cependant, la multitude et la variété de ces données ainsi que l'émergence régulière de nouvelles formes d'écriture rendent difficile l'extraction automatique d'information à partir de corpus souvent peu standards d'un point de vue lexical voire syntaxique.
Afin de relever ces défis, nous proposons, dans cette thèse, des approches originales de fouille de textes permettant l'identification automatique de nouvelles variantes d'entités et relations spatiales à partir de données textuelles issues de la communication médiée.
Ces approches sont fondées sur trois principales contributions qui sont cruciales pour fournir des méthodes de navigation intelligente.
Notre première contribution se concentre sur la problématique de reconnaissance et d'extraction des entités spatiales à partir de corpus de messages courts (SMS, tweets) marqués par une écriture peu standard.
La deuxième contribution est dédiée à l'identification de nouvelles formes/variantes de relations spatiales à partir de ces corpus spécifiques.
Enfin, la troisième contribution concerne l'identification des relations sémantiques associées à l'information spatiale contenue dans les textes.
Dans cette thèse, nous étudions plusieurs modèles de collaboration entre l'ingénierie logiciel et le web sémantique.
L'objectif principal de notre travail est de fournir au développeur des outils pour concevoir la matière déclarative une couche de métier "exécutable" d'une application afin de simuler son fonctionnement et de montrer ainsi la conformité de l'application par rapport aux exigences du client au début du cycle de vie du logiciel.
Un autre avantage de cette approche est de permettre au développeur de partager et de réutiliser la description de la couche de métier d'une application dans un domaine en utilisant l'ontologie.
Celle-ci est appelée "patron d'application".
La réutilisation de la description de la couche de métier d'une application est un aspect intéressant à l'ingénier logiciel.
C'est le point-clé que nous voulons considérer dans cette thèse.
Dans la première partie de notre travail, nous traitons la modélisation de la couche de métier.
Nous présentons d'abord une approche fondée sur l'ontologie pour représenter les processus de métiers et les règles de métiers et nous montrons comment vérifier la cohérence du processus et de l'ensemble des règles de métier.
Puis, nous présentons le mécanisme de vérification automatique de la conformité d'un processus de métier avec un ensemble de règles de métier.
La deuxième partie de cette thèse est consacrée à définir une méthodologie, dite de personnalisation, de création une application à partir d'un "patron d'application".
Cette méthode permettra à l'utilisateur d'utiliser un patron d'application pour créer sa propre application en évitant les erreurs de structures et les erreurs sémantiques.
Nous introduisons à la fin de cette partie, la description d'une plateforme expérimentale permettant d'illustrer la faisabilité des mécanismes proposés dans cette thèse.
A travers cette étude, se présente une perspective dynamique de l'annotation sémantique.
Cette perspective considère le passage du temps et les flux permanents de documents qui font croître les collections et étendre leurs systèmes d'annotation.
Nous apportons également une vision de la qualité des systèmes d'annotations basée sur la notion d'accès à l'information et de cohérence.
Dans notre vision de la qualité, l'information de vocabulaire d'annotation est la complexité à parcourir par un utilisateur à la recherche d'un certain sujet.
Pour répondre au problème de la dynamique dans l'annotation sémantique, cette thèse propose une architecture modulaire pour l'annotation sémantique dynamique.
Cette architecture modélise les activités impliquées dans le processus d'annotation sémantique en modules abstraits avec des considérations particulières en fonction de la tâche spécifique.
Comme cas d'étude, nous prenons l'annotation de blogs.
Nous rassemblâmes un corpus contenant jusqu'à 10 ans de billets de blog annotés avec des catégories et des tags et analysé les habitudes d'annotation observées.
Nous explorons la suggestion automatique de tags et de catégories afin de mesurer l'impact de la dynamique dans le système d'annotation.
Certaines stratégies pour faire face à cet impact ont été évaluées pour caractériser l'importance de l'âge des exemples.
Enfin, nous proposons un cadre de trois mesures de qualité et une méthode interactive pour récupérer la qualité d'un système d'indexation basé sur des annotations sémantiques appuyée par les métriques.
Les mesures ont été évaluées au fil du temps pour observer la dégradation de la qualité de l'indexation.
Une série d'exemples étudiés sont présentés pour observer la performance des mesures visant à guider la restructuration du système d'annotation de l'indexation.
Bien que les vaccins représentent une avancée majeure pour la santé publique, le risque d'effets secondaires constitue une menace réelle pour leur acceptation par le grand public et les professionnels de santé.
La France se classe, d'ailleurs, comme le pays manifestant la plus grande défiance envers le vaccin.
Cela s'est souvent traduit pas des couvertures vaccinales faibles.
L'origine de cette perte de confiance est, entre autres, liée à la polémique intense autour du vaccin anti-hépatite B (HB) et le risque de sclérose en plaques dans les années 1990.
Le but de cette thèse est d'évaluer le lien potentiel entre vaccination et démyélinisation, en considérant deux exemples : les vaccins anti-VHB et anti-papillomavirus (HPV).
Une approche méthodologique, progressive, fondée sur les preuves a été utilisée pour les deux vaccins.
La génération d'hypothèse a considéré la plausibilité biologique, les rapports de cas publiés, les analyses de disproportionnalité conduites dans le système américain de pharmacovigilance des vaccins (i.e., Vaccine Adverse Event Reporting System (VAERS)), et l'analyse des signaux détectés par la surveillance passive.
Concernant la vaccination anti-VHB, des analyses attendu/observé ont également été menées à partir des cas confirmés rapportés à la pharmacovigilance française dans les années 1990.
Les résultats restent mitigés pour les deux vaccins.
Pour la vaccination anti-VHB, une plausibilité biologique faible et indirecte, l'analyse du signal français détecté en 1996 qui a révélé une disjonction complète entre les populations cible et rejointe, ainsi que les résultats des analyses de disproportionnalité dans VAERS sont des éléments en faveur d'une possible association entre démyélinisation centrale et vaccin anti-VHB.
Cependant, ni la méta-analyse, ni les analyses attendu/observé (bien que leurs conclusions puissent être renversées par un facteur modéré de sous-notification), n'ont fourni de résultat statistiquement significatif.
En tout état de cause, si un risque en excès existait, il serait faible et ne concernerait que l'adulte.
Les recommandations actuelles qui minimisent la probabilité d'exposition à l'âge adulte, sont donc plus que justifiées.
Pour la vaccination anti-HPV, le risque de démyélinisation centrale semble, à ce jour, écarté.
Néanmoins, un doute subsiste concernant un possible risque en excès pour le syndrome de Guillain et Barré.
En conclusion, une association forte avec un risque de démyélinisation semble à exclure pour les deux vaccins, rendant la balance bénéfice/risque largement positive pour ces produits, dès lors qu'ils sont utilisés dans leurs populations cibles.
Dans ce contexte, une communication scientifique, indépendante et claire est la clé pour promouvoir les programmes de vaccination et créer la confiance et l'adhésion du grand public.
Le futur de la pharmacovigilance des vaccins pourrait résider dans la mise en place d'un réseau collaboratif entre le patient et son médecin, via l'utilisation de SMS et smartphones, comme cela existe déjà en Australie.
En plus de collecter les effets secondaires des vaccins, cela représenterait une opportunité unique de placer le patient au coeur du système de surveillance, lui offrant une voix et contribuant à restaurer sa confiance envers les vaccins, et même envers les décideurs de santé publique.
Considérant un point de vue général de cette thèse aborde le problème de trouver, à partir d'un ensemble de blocs de construction, un sous-ensemble qui procure une solution à un problème donné.
Ceci est fait en tenant compte de la compatibilité de chacun des blocs de construction par rapport au problème et l'aptitude d'interaction entre ces parties pour former ensemble une solution.
Dans la perspective notamment de la thèse sont les blocs de construction de méta-modèles et le problème donné est une description d'un problème peut être résolu en utilisant un logiciel et d'être résolu en utilisant un système multi-agents.
Il peut également indiquer que le problème ne peut être résolu par ce paradigme.
Le processus adressée par la thèse consiste en les étapes principales suivantes :
(1) A travers un processus de caractérisation on analyse la description du problème pour localiser le domaine de solutions, puis choisissez une liste de candidats des méta-modèles.
(3) On crée un système multi-agents où chaque agent représente un candidat méta-modèle.
Dans cette société les agents interagissent les uns avec les autres pour trouver un groupe de méta-modèles qui est adapté pour représenter une solution donnée.
Les agents utilisent des critères appropriés pour chaque méta-modèle à représenter.
Cette thèse se concentre sur la fourniture d'un processus et un outil prototype pour résoudre plutôt la dernière étape de la liste.
Par conséquent, le chemin proposé a été créé à l'aide de plusieurs concepts de la méta-analyse, l'intelligence artificielle de coopération, de la cognition bayésienne, incertitude, la probabilité et statistique.
La thèse a pour objectif d'observer le fonctionnement des verbes de manière de mouvement en tant que prédicats en polonais et en français.
Il s'avère que les structures argumentales des prédicats sont responsables de certaines caractéristiques et des « comportements » grammaticaux des membres de cette classe verbale dans les deux langues.
Le premier chapitre expose les bases théoriques et méthodologiques adoptées dans les analyses.
Les chapitres suivants sont consacrés spécifiquement aux verbes déterminés polonais (chapitre 2), aux verbes indéterminés en polonais (chapitre 3) et aux verbes de manière de mouvement français (chapitre 4).
Le chapitre 5 est un bilan comparatif.
Par là même, ils acceptent de modifier les propriétés aspectuelles déterminées par le sens du lexème verbal.
La vérification automatique est aujourd'hui devenue un domaine central de recherche en informatique.
Depuis plus de 25 ans, une riche théorie a été développée menant à de nombreux outils, à la fois académiques et industriels, permettant la vérification de propriétés booléennes-celles qui peuvent être soit vraies soit fausses.
Les besoins actuels évoluent vers une analyse plus fine, c'est-à-dire plus quantitative.
L'extension des techniques de vérification aux domaines quantitatifs a débuté depuis 15 ans avec les systèmes probabilistes.
Cependant, de nombreuses autres propriétés quantitatives existent, telles que la durée de vie d'un équipement, la consommation énergétique d'une application, la fiabilité d'un programme, ou le nombre de résultats d'une requête dans une base de données.
Exprimer ces propriétés requiert de nouveaux langages de spécification, ainsi que des algorithmes vérifiant ces propriétés sur une structure donnée.
Cette thèse a pour objectif l'étude de plusieurs formalismes permettant de spécifier de telles propriétés, qu'ils soient dénotationnels-expressions régulières, logiques monadiques ou logiques temporelles-ou davantage opérationnels, comme des automates pondérés, éventuellement étendus avec des jetons.
Un premier objectif de ce manuscript est l'étude de résultats d'expressivité comparant ces formalismes.
En particulier, on donne des traductions efficaces des formalismes dénotationnels vers celui opérationnel.
Ces objets, ainsi que les résultats associés, sont présentés dans un cadre unifié de structures de graphes.
Ils peuvent, entre autres, s'appliquer aux mots et arbres finis, aux mots emboîtés (nested words), aux images ou aux traces de Mazurkiewicz.
Par conséquent, la vérification de propriétés quantitatives de traces de programmes (potentiellement récursifs, ou concurrents), les requêtes sur des documents XML (modélisant par exemple des bases de données), ou le traitement des langues naturelles sont des applications possibles.
On s'intéresse ensuite aux questions algorithmiques que soulèvent naturellement ces résultats, tels que l'évaluation, la satisfaction et le model checking.
En particulier, on étudie la décidabilité et la complexité de certains de ces problèmes, en fonction du semi-anneau sous-jacent et des structures considérées (mots, arbres...).
Finalement, on considère des restrictions intéressantes des formalismes précédents.
Certaines permettent d'étendre l'ensemble des semi-anneau sur lesquels on peut spécifier des propriétés quantitatives.
Une autre est dédiée à l'étude du cas spécial de spécifications probabilistes : on étudie en particulier des fragments syntaxiques de nos formalismes génériques de spécification générant uniquement des comportements probabilistes.
Il est bien connu qu'une nouvelle étude, menée au sein d'une entreprise, est souvent semblable à une étude précédente et par conséquent, peut être structurée selon un processus de référence commun au type d'étude correspondant.
La difficulté majeure réside dans la formalisation de cette démarche métier.
Les approches traditionnelles de capitalisation des connaissances s'appuyant sur des verbalisations d'experts ont montré leur limite. Souvent réalisées en dehors de l'activité réelle, les experts omettent des détails qui peuvent être d'importance.
Notre thèse repose sur l'idée qu'il est possible de construire le processus opérationnel mis en œuvre lors des activités collaboratives de conception, à partir des traces enregistrées lors de l'utilisation de l'outil numérique par les acteurs métier.
Le processus opérationnel ainsi construit pourra aider les acteurs métiers et les experts à prendre du recul sur le travail réel et à formaliser et enrichir les démarches métier de l'entreprise.
Notre travail s'est déroulé au sein du laboratoire ERPI (Équipe de Recherche sur les Processus Innovatifs) de l'Université de Lorraine et en collaboration avec la société TDC Software dans le cadre d'une thèse CIFRE.
Les contributions que nous proposons sont les suivantes :
• Un double cycle de capitalisation pour des activités instrumentées,
• Une approche globale de gestion des démarches métier,
• Une ontologie OntoProcess modélisant des aspects organisationnels génériques (séparant clairement des concepts liés aux traces et d'autres liés aux démarches métier) et des extensions métiers spécifiques à l'outil utilisé,
• Un système multi-agents supportant l'approche globale de gestion des démarches métiers et s'appuyant sur l'ontologie OntoProcess,
• Un système à base de traces permettant de construire un processus opérationnel à partir des traces enregistrées lors d'une étude
La popularisation des réseaux sociaux et des documents numériques a rapidement accru l'information disponible sur Internet.
Cependant, cette quantité massive de données ne peut pas être analysée manuellement.
Nous analysons également d'autres tâches du TALN (la représentation des mots, la similarité sémantique ou encore la compression de phrases et de groupes de phrases) pour générer des résumés cross-lingues plus stables et informatifs.
La plupart des applications du TALN, celle du résumé automatique y compris, utilisent une mesure de similarité pour analyser et comparer le sens des mots, des séquences de mots, des phrases et des textes.
L'une des façons d'analyser cette similarité est de générer une représentation de ces phrases tenant compte de leur contenu.
Le sens des phrases est défini par plusieurs éléments, tels que le contexte des mots et des expressions, l'ordre des mots et les informations précédentes.
En analysant ces problèmes, nous proposons un modèle de réseau de neurones combinant des réseaux de neurones récurrents et convolutifs pour estimer la similarité sémantique d'une paire de phrases (ou de textes) en fonction des contextes locaux et généraux des mots.
Sur le jeu de données analysé, notre modèle a prédit de meilleurs scores de similarité que les systèmes de base en analysant mieux le sens local et général des mots mais aussi des expressions multimots.
Afin d'éliminer les redondances et les informations non pertinentes de phrases similaires, nous proposons de plus une nouvelle méthode de compression multiphrase, fusionnant des phrases au contenu similaire en compressions courtes.
Pour ce faire, nous modélisons des groupes de phrases semblables par des graphes de mots.
Notre approche surpasse les systèmes de base en générant des compressions plus informatives et plus correctes pour les langues française, portugaise et espagnole.
Enfin, nous combinons les méthodes précédentes pour construire un système de résumé de texte cross-lingue.
Notre système génère des résumés cross-lingue de texte en analysant l'information à la fois dans les langues source et cible, afin d'identifier les phrases les plus pertinentes.
Inspirés par les méthodes de résumé de texte par compression en analyse monolingue, nous adaptons notre méthode de compression multiphrase pour ce problème afin de ne conserver que l'information principale.
En analysant les résumés cross-lingues depuis l'anglais, le français, le portugais ou l'espagnol, vers l'anglais ou le français, notre système améliore les systèmes par extraction de l'état de l'art pour toutes ces langues.
La popularité des médias sociaux en ligne (Online Social Media-OSM) est fortement liée à la qualité du contenu généré par l'utilisateur (User Generated Content-UGC) et la protection de la vie privée des utilisateurs.
En se basant sur la définition de la qualité de l'information, comme son aptitude à être exploitée, la facilité d'utilisation des OSM soulève de nombreux problèmes en termes de la qualité de l'information ce qui impacte les performances des applications exploitant ces OSM.
Ces problèmes sont causés par des individus mal intentionnés (nommés spammeurs) qui utilisent les OSM pour disséminer des fausses informations et/ou des informations indésirables telles que les contenus commerciaux illégaux.
La propagation et la diffusion de telle information, dit spam, entraînent d'énormes problèmes affectant la qualité de services proposés par les OSM.
La majorité des OSM (comme Facebook, Twitter, etc.) sont quotidiennement attaquées par un énorme nombre d'utilisateurs mal intentionnés.
Cependant, les techniques de filtrage adoptées par les OSM se sont avérées inefficaces dans le traitement de ce type d'information bruitée, nécessitant plusieurs semaines ou voir plusieurs mois pour filtrer l'information spam.
En effet, plusieurs défis doivent être surmontées pour réaliser une méthode de filtrage de l'information bruitée.
Les défis majeurs sous-jacents à cette problématique peuvent être résumés par : (i) données de masse ; (ii) vie privée et sécurité ; (iii) hétérogénéité des structures dans les réseaux sociaux ; (iv) diversité des formats du UGC ; (v) subjectivité et objectivité.
Notre travail s'inscrit dans le cadre de l'amélioration de la qualité des contenus en termes de messages partagés (contenu spam) et de profils des utilisateurs (spammeurs) sur les OSM en abordant en détail les défis susmentionnés.
Comme le spam social est le problème le plus récurant qui apparaît sur les OSM, nous proposons deux approches génériques pour détecter et filtrer le contenu spam :
i) La première approche consiste à détecter le contenu spam (par exemple, les tweets spam) dans un flux en temps réel. ii) La seconde approche est dédiée au traitement d'un grand volume des données relatives aux profils utilisateurs des spammeurs (par exemple, les comptes Twitter).
Le besoin croissant en assistance humaine a poussé les chercheurs à développer des systèmes de dialogue automatiques, intelligents et infatigables qui conversent avec les humains dans un langage naturel pour devenir soit leurs assistants virtuels ou leurs compagnons.
L'industrie des systèmes de dialogue est devenue populaire cette dernière décennie, ainsi, plusieurs systèmes ont été développés par des industriels comme des académiques.
Dans le cadre de cette thèse, nous étudions les systèmes de dialogue basés sur la recherche de réponse qui cherchant la réponse la plus appropriée à la conversation parmi un ensemble de réponses prédéfini.
Le défi majeur de ces systèmes est la compréhension de la conversation et l'identification des éléments qui décrivent le problème et la solution qui sont souvent implicites.
La plupart des approches récentes sont basées sur des techniques d'apprentissage profond qui permettent de capturer des informations implicites.
Souvent, ces approches sont complexes ou dépendent fortement du domaine.
Nous proposons une approche de recherche de réponse de bout en bout, simple, efficace et indépendante du domaine et qui permet de capturer ces informations implicites.
Nous effectuons également plusieurs analyses afin de déterminer des pistes d'amélioration.
L'objectif de notre travail, qui émane d'une demande de la sous-direction Assurance Qualité du CNES (Centre National d'Études Spatiales), est d'augmenter la clarté des spécifications techniques rédigées par les ingénieurs préalablement à la réalisation de systèmes spatiaux.
L'importance des spécifications (et en particulier des exigences qui les composent) pour la réussite des projets de grande envergure est en effet désormais largement reconnue, de même que les principaux problèmes liés à l'utilisation de la langue naturelle (ambiguïtés, flou, incomplétude) sont bien identifiés.
L'originalité de notre démarche consiste à systématiquement vérifier nos hypothèses sur un corpus d'exigences (constitué à partir d'authentiques spécifications de projets spatiaux) à l'aide de techniques et d'outils de traitement automatique du langage existants, dans l'optique de proposer un ensemble cohérent de règles (nouvelles ou inspirées de règles plus anciennes) qui puissent ainsi être vérifiées semi-automatiquement lors de l'étape de spécification et qui soient conformes aux pratiques de rédaction des ingénieurs du CNES.
Pour cela, nous nous appuyons notamment sur l'hypothèse de l'existence d'un genre textuel, que nous tentons de prouver par une analyse quantitative, ainsi que sur les notions de normalisation et normaison.
Notre méthodologie combine les approches corpus-based et corpus-driven en tenant compte à la fois des règles imposées par deux autres langues contrôlées (dont l'adéquation avec des données réelles est discutée au travers d'une analyse plus qualitative) et des résultats offerts par des outils de text mining.
L'évaluation de l'analyseur a été effectuée sur des textes malais et un texte indonésien.
Cet analyseur utilise : un ensemble de règles, une liste d'exceptions, une liste restreinte de bases dépourvues de toute information linguistique et des techniques de reconnaissance des formes.
L'algorithme d'analyse est non déterministe.
Les bases analysées sont traitées hors contexte.
L'évaluation des résultats de l'analyseur a donné environ 97% d'analyses correctes et un taux d'erreur inférieur à 2%.
Très peu de bases affixées n'ont pas été analysées (taux inférieur à 0,5%).
Le texte a été le moyen dominant de stocker des données dans des systèmes informatiques et d'envoyer des informations sur le Web.
L'extraction de représentations significatives hors du texte a été un élément clé de la modélisation de langage afin de traiter des tâches de la NLP telles que la classification de texte.
Ces représentations peuvent ensuite former des groupes que l'on peut utiliser pour des problèmes d'apprentissage supervisé. Plus spécifiquement, on peut utiliser ces groupes linguistiques à des fins de régularisation.
Enfin, ces structures peuvent être utiles dans un autre domaine important, le calcul de distance entre documents texte.
Tout d'abord, en examinant de nouvelles représentations de texte basées sur des graphes.
Ensuite, nous avons étudié comment des groupes de ces représentations peuvent aider à la régularisation dans des modèles d'apprentissage automatique pour la classification de texte.
Enfin, nous avons traité des ensembles et de la mesure des distances entre les documents, en utilisant les groupes linguistiques que nous avons proposés, ainsi que des approches basées sur des graphes.
Dans la première partie de la thèse, nous avons étudié les représentations de texte basées sur des graphes.
Transformer le texte en graphiques n'est pas anodin et existait avant même que les mots incorporés ne soient introduits dans la communauté NLP.
Dans notre travail, nous montrons que les représentations graphiques de texte peuvent capturer efficacement des relations telles que l'ordre, la sémantique ou la structure syntaxique.
De plus, ils peuvent être créés rapidement tout en offrant une grande polyvalence pour de multiples tâches.
Dans la deuxième partie, nous nous sommes concentrés sur la régularisation structurée du texte.
Les données textuelles souffrent du problème de dimensionnalité, créant de grands espaces de fonctionnalités.
La régularisation est essentielle pour tout modèle d'apprentissage automatique, car elle permet de remédier au surajustement.
Dans notre travail, nous présentons de nouvelles approches pour la régularisation de texte, en introduisant de nouveaux groupes de structures linguistiques et en concevant de nouveaux algorithmes.
Dans la dernière partie de la thèse, nous étudions de nouvelles méthodes pour mesurer la distance dans le mot englobant l'espace.
Premièrement, nous présentons diverses méthodes pour améliorer la comparaison entre des documents constitués de vecteurs de mots.
Ensuite, en présentant la comparaison des documents comme une correspondance bipartite pondérée, nous montrons comment nous pouvons apprendre des représentations cachées et améliorer les résultats pour la tâche de classification de texte.
Enfin, nous conclurons en résumant les principaux points de la contribution totale et en discutant des orientations futures.
Comment les structures sociales et sémantiques d'une communauté scientifique guident-elles les dynamiques de collaboration à venir ?
Dans cette thèse, nous combinons des techniques de traitement automatique des langues et des méthodes provenant de l'analyse de réseaux complexes pour analyser une base de données de publications scientifiques dans le domaine de la linguistique computationnelle : l'ACL Anthology.
Notre objectif est de comprendre le rôle des collaborations entre les chercheurs dans la construction du paysage sémantique du domaine, et, symétriquement, de saisir combien ce même paysage influence les trajectoires individuelles des chercheurs et leurs interactions.
Nous employons des outils d'analyse du contenu textuel pour extraire des textes des publications les termes correspondant à des concepts scientifiques.
Ces termes sont ensuite connectés aux chercheurs pour former un réseau socio-sémantique, dont nous modélisons la dynamique à différentes échelles.
Nous construisons d'abord un modèle statistique, à base de régressions logistiques multivariées, qui permet de quantifier le rôle respectif des propriétés sociales et sémantiques de la communauté sur la dynamique microscopique du réseau socio-sémantique.
Nous reconstruisons par la suite l'évolution du champ de la linguistique computationelle en créant différentes cartographies du réseau sémantique, représentant les connaissances produites dans le domaine, mais aussi le flux d'auteurs entre les différents champs de recherche du domaine.
En résumé, nos travaux ont montré que la combinaison des méthodes issues du traitement automatique des langues et de l'analyse des réseaux complexes permet d'étudier d'une manière nouvelle l'évolution des domaines scientifiques.
La focalisation prosodique désigne le soulignement d'un constituant dans un énoncé au moyen de différentes ressources prosodiques, en particulier l'accentuation et l'intonation.
Plusieurs fonctions sont attribuées à la focalisation : le marquage des différentes catégories de focus, ainsi que des fonctions emphatiques (ici appelées insistance et expressivité).
Cette thèse a pour principal but de savoir si la focalisation et ses fonctions présentent des propriétés spécifiques dans le phonogenre de la parole interprétée, c'est-à-dire l'oralisation d'un texte écrit mémorisé au préalable par le locuteur (généralement un comédien).
Cette question présente un intérêt pour la linguistique et la phonétique à plusieurs titres.
Tout d'abord, les différences de réalisation prosodique entre les fonctions de la focalisation sont encore mal connues.
Par ailleurs, peu d'études ont été consacrées aux caractéristiques prosodiques de la parole interprétée.
Enfin, notre thèse présente un apport sur le plan méthodologique à travers le protocole relativement novateur de ses deux expériences.
Dans une expérience de production, des locuteurs ont reproduit des conversations spontanées en parole lue et en parole interprétée.
Un groupe d'experts en prosodie a ensuite relevé les occurrences de focalisation dans le corpus et a effectué une classification fonctionnelle de ces occurrences.
Nous avons également mené une expérience de perception afin de comparer la réalisation des fonctions de la focalisation indépendamment du phonogenre.
Malgré un taux d'accord entre experts relativement faible (ce qui soulève plusieurs questions méthodologiques et théoriques), nos analyses révèlent plusieurs résultats originaux.
La fréquence d'occurrence de la focalisation est la plus élevée en parole interprétée, suivie de la parole lue.
Ce résultat confirme notre prédiction et suggère que la parole interprétée est un phonogenre favorable à l'étude de la focalisation.
Une forte relation est observée entre la fonction d'insistance et le trait d'accentuation initiale, ce qui confirme de nombreuses études précédentes.
Le phonogenre se révèle en revanche avoir très peu d'influence sur la réalisation de la focalisation et de ses fonctions.
Ce résultat est dû selon nous à un manque de données et au fait que certains traits prosodiques n'ont pas été pris en compte dans l'analyse.
Cette thèse fournit une description du kakabé, une langue mandée parlée en Guinée, basée sur un corpus et avec un focus sur le système phonologique.
Elle contient une brève esquisse grammaticale et deux parties qui portent sur l'analyse phonologique : la phonologie segmentale et la phonologie suprasegmentale.
Le kakabé applique diverses stratégies d'adaptation des emprunts (principalement, du poular et du français), telles que l'épenthèse vocalique, la simplification d'agglomérations consonantiques.
Le kakabé est une langue à ton (H vs. L), avec downdrift, relèvement du ton H, un ton flottant L, et un certain nombre de processus tonals, tels que l'insertion du ton H, la propagation du ton,l'aplatissement du contour HLH.
En conséquence, la distance entre les tons lexicaux sous‐jacents et leur réalisation de surface peut être assez importante.
Chacun des processus tonals est appliqué dans une unité prosodique particulière.
Par conséquent, les processus tonals participent au découpage du discours en unités prosodiques.
Le kakabé comporte des tons de frontière qui servent à signaler la force illocutoire de l'énoncé.
Les tons lexicaux et les tons de frontièrecoexistent avec des opérations intonatives sur la courbe F0.
Les appendices comprennent un dictionnaire kakabé-français, composé de 3400 entrées, et le corpus de 12 heures de textes en kakabé, transcrits, glosés, traduits etaccompagnés des fichiers vidéos et audios.
Dans le cadre du développement international du mouvement du libre accès aux publications scientifiques, cette thèse analyse plus précisément la situation française dans le contexte européen.
Cette analyse a été menée à travers une démarche de recherche-action, au sein d'un groupe d'acteurs du Groupement français des industries de l'information (GFII) concernés par le libre accès.
Nous cherchons tout d'abord à mettre en évidence les forces motrices du développement du libre accès en nous appuyant sur une méthodologie prospective développée au LIPSOR/CNAM.Les résultats nous ont conduit à contribuer à la conception d'un site d'information dont la finalité est l'affichage des politiques des éditeurs nationaux en matière d'auto-archivage afin d'accompagner les pratiques de dépôts au niveau national.
L'analyse prospective a en effet révélé l'importance des embargos pour les équilibres financiers des éditeurs.
De façon plus distanciée, nous amorçons également une réflexion sur l'impact réel du libre accès sur deux moteurs semblant jouer un rôle croissant dans l'économie de la connaissance, à savoir la créativité et l'interdisciplinarité.
Apprendre, c'est extraire et condenser de l'information pertinente à partir d'un certain nombre d'indices concordants.
Pourtant, les indices auxquels un apprenant a, ou pourrait avoir, accès, peuvent sembler insuffisants pour ce à quoi il veut parvenir.
Le caractère partiel ou insuffisant des indices disponibles est souvent évoqué dans le contexte de l'acquisition du langage : le sens des mots et la grammaire que les individus maîtrisent semblent nécessiter davantage d'information que celle présente dans leur environnement.
Si une grande partie de la recherche se focalise sur l'identification de sources d'information négligées ou sous-estimées jusqu'alors, ici nous déplaçons le point de vue pour explorer plutôt les connaissances que ces sources d'information doivent permettre de construire.
Nous proposons dans ce travail que les informations disponibles restent souvent insuffisantes pour atteindre le niveau de connaissance qu'un locuteur compétent pense avoir ; toutefois, la même information peut être suffisante pour qu'un apprenant extraie l'information nécessaire au traitement cognitif du langage.
Plus généralement, il pourrait y avoir un décalage entre ce que l'on pense apprendre et ce que l'on apprend véritablement.
Dans l'introduction de ce travail, nous commençons par une présentation des études qui suggèrent l'existence d'un décalage entre notre intuition de ce qu'est le sens des mots, et la manière dont le sens des mots est effectivement traité par le cerveau.
Nous explorons ce décalage dans le contexte plus général de la manière dont les individus, qui ne disposent que d'informations limitées, tentent de comprendre le monde.
Ensuite, dans le premier chapitre, nous examinons comment une faible quantité d'information peut malgré tout favoriser l'acquisition de la grammaire, en utilisant un paradigme écologique.
Nos résultats démontrent que la connaissance d'une petite poignée de mots peut générer un cercle vertueux dans l'acquisition de la grammaire et du vocabulaire chez le bébé.
Puis, dans le deuxième chapitre, nous tâchons de découvrir si cet ensemble réduit d'informations peut suffire à amorcer l'acquisition d'un savoir productif qui permettra ensuite de généraliser cette connaissance à des situations nouvelles, à travers les âges : du bébé à l'adulte en passant par l'enfant d'âge scolaire.
Nos données suggèrent que les bébés et les adultes généralisent les caractéristiques d'un nouvel élément grammatical afin de comprendre de nouveaux mots, mais que les enfants d'âge scolaire ne généralisent pas.
Nous expliquons nos résultats dans le contexte de la recherche existante sur la généralisation, en soulignant la possibilité que ce qui est considéré comme savoir est peut-être défini par rapport à un seuil, plutôt que par rapport à une définition idéale objective.
Finalement, dans le troisième chapitre, nous étudions si l'information qui donne l'impression de refléter directement l'état des connaissances de quelqu'un d'autre (i.e., entendre une traduction directe : 'Bamoule', ça veut dire 'chat') est véritablement plus utile, ou seulement plus attractive.
Nos résultats suggèrent que ce type d'information 'clé-en-main'augmente systématiquement la confiance de l'apprenant, mais a des effets variables sur la performance objective.
A travers trois chapitres et dix expériences, nous proposons une série de principes qui définissent la connaissance : (1) une petite quantité d'information peut mener loin, (2) combien d'information suffit semble dépendre d'un seuil adaptable, et (3) le cerveau paraît être avide de certitude.
Nous suggérerons que l'ensemble des informations à la disposition d'un individu peut être suffisant pour générer des connaissances, mais pas forcément le type de connaissances que l'on a l'intuition d'avoir.
Ainsi, pour mieux comprendre comment on apprend, nous devons étudier ce que signifie vraiment 'savoir'pour l'apprenant.
L'apprentissage profond (deep learning) est une avancée majeure de l'intelligence artificielle de ces dernières années.
L'apprentissage profond s'est rapidement imposé comme un standard dans de plusieurs domaines en pulvérisant les records des précédentes méthodes de l'état de l'art.
Ses domaines de prédilection sont principalement l'analyse d'image et le traitement du langage naturel.
Dans ce projet, nous nous intéressons plus spécifiquement à la prédiction de phénotypes (diagnostique, pronostique, réponse à un traitement) à partir de données transcriptomiques.
La grande majorité des articles publiés a moins de deux ans et parmi eux, seulement une poignée s'intéresse à la prédiction de phénotypes.
Alors que les réseaux de neurones profonds traitant des images ou du langage naturel sont construits à partir de plusieurs centaines de milliers ou millions d'exemples, les jeux de données transcriptomiques contiennent très peu de patients (&lt;5000).
À cause de ce faible nombre d'exemples, l'apprentissage des réseaux de neurones profonds se heurte à des problèmes de sur-apprentissage.
Nous nous inspirerons des méthodes les plus performantes actuellement sur les images tel que les adversarial autoencoders, les ladder networks, ou les generative adversarial networks, et les adapterons au problème spécifique des données d'expression.
Une autre solution serait de pouvoir d'intégrer des jeux de données issus de différentes plateformes de production de données transcriptomiques.
Ce problème d'intégration n'a pour le moment pas de solutions satisfaisantes en raison des liens complexes entre les mesures des différentes plateformes.
Une mesure d'expression sur une plateforme A peut à la fois se repartir sur plusieurs expressions et se mélanger à d'autres expressions sur une plateforme B. Notre idée est de faire apprendre par un réseau de neurones la correspondance entre deux plateformes.
Pour cela, nous nous inspirerons des méthodes de transfert de domaine telles que CycleGAN.
Un autre défi majeur concerne l'interprétation des réseaux de neurones et de leurs prédictions.
L'Union Européenne a adopté récemment un texte imposant aux utilisateurs d'algorithmes d'apprentissage automatique d'être capables d'expliquer les décisions d'un modèle prédictif.
Il y a donc un réel besoin de rendre les réseaux de neurones plus interprétables et cela est particulièrement vrai dans le domaine médical pour deux raisons.
Premièrement, il est important de s'assurer que le réseau de neurones base ses prédictions sur une représentation fiable des patients et ne se concentre pas sur des artefacts non pertinents présents dans les données d'apprentissage.
Sans explications des prédictions, les médecins ne peuvent pas faire confiance à un réseau de neurones quelles que soient ses performances.
L'interprétation des réseaux de neurones se fera avec des méthodes de perturbation et de rétro-propagation du signal de sortie.
Dans ce projet, nous collaborons avec l'Institut de Cardiométabolisme et de Nutrition (ICAN) qui nous fournira le profil d'expression génétique de plusieurs milliers de patients atteint de maladies cadiométabolomiques.
Ils nous donneront également accès aux données du projet européen Metacardis qui contient les données cliniques et d'expression de gènes de plus de 2000 patients.
Résultat attendu : Nous proposerons de nouvelles approches d'apprentissage profond adaptées à la prédiction à partir de données transcriptomiques et pour l'interprétation biologique des réseaux de neurones.
Nous mettrons également à disposition de la communauté les réseaux que nous aurons construits sur des grands jeux de données.
Par « transfert learning » , les chercheurs pourront utiliser nos réseaux sur des jeux de données plus petits et améliorer leurs résultats.
L'objectif est de fournir pour les données transcriptomiques un équivalent des réseaux VGG ou ResNet en analyse d'image.
Le sujet de cette thèse s'inscrit dans le cadre général de la Recherche d'Information et la gestion des données massives et distribuées.
Elle a pour but de retourner à l'utilisateur d'un système de recherche d'information des objets résultats qui sont riches et porteurs de connaissances.
Ces objets n'existent pas en tant que tels dans les sources.
Ils sont construits par assemblage (ou configuration ou agrégation) de fragments issus de diffèrentes sources.
Les sources peuvent être non spécifiées dans l'expression de la requête mais découvertes dynamiquement lors de la recherche.
Nous nous intéressons particulièrement à l'exploitation des dépendances de données pour optimiser les accès aux sources distribuées.
Dans ce cadre, nous proposons une approche pour l'un des sous processus de systèmes de RIA, principalement le processus d'indexation/organisation des documents.
Nous considérons dans cette thèse, les systèmes de recherche d'information orientés graphes (graphes RDF).
Utilisant les relations dans les graphes, notre travail s'inscrit dans le cadre de la recherche d'information agrégative relationnelle (Relational Aggregated Search) où les relations sont exploitées pour agréger des fragments d'information.
Nous proposons d'optimiser l'accès aux sources d'information dans un système de recherche d'information agrégative.
Ces sources contiennent des fragments d'information répondant partiellement à la requête.
L'objectif est de minimiser le nombre de sources interrogées pour chaque fragment de la requête, ainsi que de maximiser les opérations d'agrégations de fragments dans une même source.
Nous proposons d'effectuer cela en réorganisant la/les base(s) de graphes dans plusieurs clusters d'information dédiés aux requêtes agrégatives.
Ces clusters sont obtenus à partir d'une approche de clustering sémantique ou structurel des prédicats des graphes RDF.
Pour le clustering structurel, nous utilisons les algorithmes d'extraction de sous-graphes fréquents et dans ce cadre nous élaborons une étude comparative des performances de ces algorithmes.
Pour le clustering sémantique, nous utilisons les métadonnées descriptives des prédicats dont nous appliquons des outils de similarité textuelle sémantique.
L'augmentation des données disponibles dans presque tous les domaines soulève la nécessité d'utiliser des algorithmes pour l'analyse automatisée des données.
Cette nécessité est mise en évidence dans la maintenance prédictive, où l'objectif est de prédire les pannes des systèmes en observant continuellement leur état, afin de planifier les actions de maintenance à l'avance.
Ces observations sont générées par des systèmes de surveillance habituellement sous la forme de séries temporelles et de journaux d'événements et couvrent la durée de vie des composants correspondants.
Le principal défi de la maintenance prédictive est l'analyse de l'historique d'observation afin de développer des modèles prédictifs.
Dans ce sens, l'apprentissage automatique est devenu omniprésent puisqu'il fournit les moyens d'extraire les connaissances d'une grande variété de sources de données avec une intervention humaine minimale.
L'objectif de cette thèse est d'étudier et de résoudre les problèmes dans l'aviation liés à la prévision des pannes de composants à bord.
La quantité de données liées à l'exploitation des avions est énorme et, par conséquent, l'évolutivité est une condition essentielle dans chaque approche proposée.
Cette thèse est divisée en trois parties qui correspondent aux différentes sources de données que nous avons rencontrées au cours de notre travail.
Dans la première partie, nous avons ciblé le problème de la prédiction des pannes des systèmes, compte tenu de l'historique des Post Flight Reports.
Nous avons proposé une approche statistique basée sur la régression précédée d'une formulation méticuleuse et d'un prétraitement / transformation de données.
Notre méthode estime le risque d'échec avec une solution évolutive, déployée dans un environnement de cluster en apprentissage et en déploiement.
À notre connaissance, il n'y a pas de méthode disponible pour résoudre ce problème jusqu'au moment où cette thèse a été écrite.
La deuxième partie consiste à analyser les données du livre de bord, qui consistent en un texte décrivant les problèmes d'avions et les actions de maintenance correspondantes.
Le livre de bord contient des informations qui ne sont pas présentes dans les Post Flight Reports bien qu'elles soient essentielles dans plusieurs applications, comme la prédiction de l'échec.
Cependant, le journal de bord contient du texte écrit par des humains, il contient beaucoup de bruit qui doit être supprimé afin d'extraire les informations utiles.
Nous avons abordé ce problème en proposant une approche basée sur des représentations vectorielles de mots.
Notre approche exploite des similitudes sémantiques, apprises par des neural networks qui ont généré les représentations vectorielles, afin d'identifier et de corriger les fautes d'orthographe et les abréviations.
Enfin, des mots-clés importants sont extraits à l'aide du Part of Speech Tagging.
Dans la troisième partie, nous avons abordé le problème de l'évaluation de l'état des composants à bord en utilisant les mesures des capteurs.
Dans les cas considérés, l'état du composant est évalué par l'ampleur de la fluctuation du capteur et une tendance à l'augmentation monotone.
Dans notre approche, nous avons formulé un problème de décomposition des séries temporelles afin de séparer les fluctuations de la tendance en résolvant un problème convexe.
Cette thèse porte sur la capture, l'annotation, la synthèse et l'évaluation des mouvements des mains et des bras pour l'animation d'avatars communiquant en Langues des Signes (LS).
Actuellement, la production et la diffusion de messages en LS dépendent souvent d'enregistrements vidéo qui manquent d'informations de profondeur et dont l'édition et l'analyse sont difficiles.
Les avatars signeurs constituent une alternative prometteuse à la vidéo.
Ils sont généralement animés soit à l'aide de techniques procédurales, soit par des techniques basées données.
L'animation procédurale donne souvent lieu à des mouvements peu naturels, mais n'importe quel signe peut être produit avec précision.
Avec l'animation basée données, les mouvements de l'avatar sont réalistes mais la variété des signes pouvant être synthétisés est limitée et/ou biaisée par la base de données initiale.
Privilégiant l'acceptation de l'avatar, nous avons choisi l'approche basée sur les données mais, pour remédier à sa principale limitation, nous proposons d'utiliser les mouvements annotés présents dans une base de mouvements de LS capturés pour synthétiser de nouveaux signes et énoncés absents de cette base.
Pour atteindre cet objectif, notre première contribution est la conception, l'enregistrement et l'évaluation perceptuelle d'une base de données de capture de mouvements en Langue des Signes Française (LSF) composée de signes et d'énoncés réalisés par des enseignants sourds de LSF.
Notre deuxième contribution est le développement de techniques d'annotation automatique pour différentes pistes d'annotation basées sur l'analyse des propriétés cinématiques de certaines articulations et des algorithmes d'apprentissage automatique existants.
Notre dernière contribution est la mise en œuvre de différentes techniques de synthèse de mouvements basées sur la récupération de mouvements par composant phonologique et sur la reconstruction modulaire de nouveaux contenus de LSF avec l'utilisation de techniques de génération de mouvements, comme la cinématique inverse, paramétrées pour se conformer aux propriétés des mouvements réels.
L'objectif du didacticien est d'élaborer une méthode performante dont le contenu et les outils d'enseignement-apprentissage améliorent les compétences phonétiques en langue étrangère.
Concernant le contenu pédagogique, les travaux ont montré que les sons et les phonèmes d'une langue inconnue sont traités selon l'organisation de l'espace phonétique et phonologique de la langue maternelle.
Les recherches mettent en avant l'intérêt de confronter les systèmes linguistiques afin de prédire les difficultés et les facilités auxquelles seront exposés les apprenants de langue.
S'agissant des outils de transmission, les études montrent les effets bénéfiques de l'interdisciplinarité et le rôle pertinent de la musique sur le développement cognitif et des apprentissages.
Notre objectif de recherche s'inscrit dans ce contexte scientifique.
Notre intérêt est double.
D'abord, nous avons tenté d'identifier quel paramètre, inhérent à l'émission en voix chantée et la différenciant de la voix parlée, pouvait faciliter la perception de voyelles non-natives.
Ensuite, nous avons souhaité comparer les effets sur la compétence de production de voyelles non-natives de deux méthodes de corrections phonétique, l'une des deux exploitant l'outil « voix chantée » .
À travers les résultats de ces études, nous avons essayé de saisir le rôle de l'italien langue maternelle sur la perception et la production du français langue cible.
Nos travaux n'ont pas mis en évidence d'effet des modalités fréquence fondamentale et allongement de la durée vocalique sur la discrimination perceptive des voyelles non natives /y/ et /ø/, mais ils suggèrent un rôle du contexte prévocalique sur la perception de la voyelle non-native /y/ en contraste /u/.
Nous avons trouvé un effet favorable de la méthode de correction phonétique incluant la pratique chantée sur la production du spectre sonore des voyelles fermées du français, mais pas sur l'évolution des catégories phonologiques à l'intérieur de l'espace acoustique vocalique.
Les résultats de ces études soutiennent la théorie que l'enseignement-apprentissage de la phonétique a sa place en classe de langue, et suggèrent que la voix chantée serait, sous certaines conditions, un outil pertinent pour faciliter la perception et la production de voyelles non-natives.
Dans cette thèse, nous traitons le problème de la séparation de sources audio multicanale par réseaux de neurones profonds (deep neural networks, DNNs).
Notre approche se base sur le cadre classique de séparation par algorithme espérance-maximisation (EM) basé sur un modèle gaussien multicanal, dans lequel les sources sont caractérisées par leurs spectres de puissance à court terme et leurs matrices de covariance spatiales.
Nous explorons et optimisons l'usage des DNNs pour estimer ces paramètres spectraux et spatiaux.
À partir des paramètres estimés, nous calculons un filtre de Wiener multicanal variant dans le temps pour séparer chaque source.
Nous étudions en détail l'impact de plusieurs choix de conception pour les DNNs spectraux et spatiaux.
Nous considérons plusieurs fonctions de coût, représentations temps-fréquence, architectures, et tailles d'ensembles d'apprentissage.
Ces fonctions de coût incluent en particulier une nouvelle fonction liée à la tâche pour les DNNs spectraux : le rapport signal-à-distorsion.
Nous présentons aussi une formule d'estimation pondérée des paramètres spatiaux, qui généralise la formulation EM exacte.
Sur une tâche de séparation de voix chantée, nos systèmes sont remarquablement proches de la méthode de l'état de l'art actuel et améliorent le rapport source-interférence de 2 dB.
Sur une tâche de rehaussement de la parole, nos systèmes surpassent la formation de voies GEV-BAN de l'état de l'art de 14%, 7% et 1% relatifs en terme d'amélioration du taux d'erreur sur les mots sur des données à 6, 4 et 2 canaux respectivement
Cette étude présente une typologie des prédicats de mouvement du hongrois.
Elle reflète une perception objective et simple du mouvement et de l'espace.
Le travail s'inscrit dans le cadre de la théorie des classes d'objets que nous avons appliquée au hongrois.
La classification s'appuie sur des propriétés aspectuelles.
Ces propriétés sont complétées par des propriétés morpho-syntaxiques nécessaires au traitement automatique.
L'aspect contrastif de notre étude a permis de proposer une meilleure description des classes de prédicats du hongrois et de relever les différences morpho-syntaxiques et combinatoires spécifiques des deux langues dans l'expression du mouvement, comme le rôle des préfixes verbaux, des compléments locatifs ainsi que l'importance des prédicats nominaux.
La masse d'informations du domaine juridique, qui ne cesse de s'accroitre, a généré un be-soin capital d'organiser et de structurer les contenus des documents disponibles, et les trans-former ainsi en un guide intelligent, capable d'apporter des réponses complètes et immé-diates à des requêtes en langage naturel.
De ce fait, les systèmes de questions-réponses (QASs) répondent parfaitement à ce besoin en offrant les différents mécanismes pour four-nir des réponses adéquates et précises à des questions exprimées en langage naturel.
En effet, ce type de systèmes permet à l'utilisateur de poser une question en langue naturel et de recevoir une réponse précise à sa demande au lieu d'un ensemble de documents jugés relevant, comme c'est le cas pour les moteurs de recherche.
Notre objectif, est de mettre en place un système de questions-réponses opérant dans le domaine juridique au Maroc qui utilise dans la plupart du temps les langues française et arabe, voir la langue anglaise.
Il a pour but de donner une réponse pertinente et concise à des questions dans le domaine juridique, énoncées en langue naturelle par un utilisateur, sans que ce dernier ait à parcourir les documents juridiques pour trouver une réponse à sa ques-tion, ce qui peut être couteux en terme de temps.
Cette thèse a pour objet l'étude de modèles à base d'analogies dans un cadre d'Apprentissage Automatique pour le Traitement Automatique des Langues Naturelles.
L'approche analogique apporte une alternative à la fois aux méthodes déductives (inférence de connaissances particulières à partir de connaissances générales) et aux méthodes inductives (inférence de connaissances générales à partir de connaissances particulières).
Selon ce mode de raisonnement, l'analyse d'une nouvelle entité s'effectue par comparaison avec les données disponibles ; l'inférence s'effectue directement du particulier au particulier.
Dans cette approche, l'abstraction que constitue la connaissance générale impliquée à la fois dans les approches déductives et inductives n'apparaît plus comme une composante nécessaire du modèle.
Par ailleurs, cette approche s'accorde bien avec l'organisation paradigmatique des données linguistiques, qui permet de mettre aisément une entité linguistique en relation avec d'autres selon des schémas spécifiques ; la connaissance linguistique reste alors implicitement représentée dans le corpus accumulé et les relations systématiques qu'entretiennent les entités le composant.
Cette organisation paradigmatique invite en particulier à considérer des proportions analogiques.
Un modèle d'apprentissage est présenté, qui repose sur l'exploitation de proportions analogiques.
Nous introduisons la notion d'extension analogique, qui permet d'exprimer la méthode et d'identifier clairement son biais d'apprentissage.
Nous proposons également un cadre algébrique formel permettant de donner un sens à la notion de proportion analogique entre objets structurés.
Notre perception est par nature multimodale, i.e. fait appel à plusieurs de nos sens.
Pour résoudre certaines tâches, il est donc pertinent d'utiliser différentes modalités, telles que le son ou l'image.
Cette thèse s'intéresse à cette notion dans le cadre de l'apprentissage neuronal profond.
Pour cela, elle cherche à répondre à une problématique en particulier : comment fusionner les différentes modalités au sein d'un réseau de neurones ?
Nous proposons tout d'abord d'étudier un problème d'application concret : la reconnaissance automatique des émotions dans des contenus audio-visuels.
Cela nous conduit à différentes considérations concernant la modélisation des émotions et plus particulièrement des expressions faciales.
Nous proposons ainsi une analyse des représentations de l'expression faciale apprises par un réseau de neurones profonds.
De plus, cela permet d'observer que chaque problème multimodal semble nécessiter l'utilisation d'une stratégie de fusion différente.
Enfin, nous nous intéressons à une vision multimodale du transfert de connaissances.
En effet, nous détaillons une méthode non traditionnelle pour effectuer un transfert de connaissances à partir de plusieurs sources, i.e. plusieurs modèles pré-entraînés.
Pour cela, une représentation neuronale plus générale est obtenue à partir d'un modèle unique, qui rassemble la connaissance contenue dans les modèles pré-entraînés et conduit à des performances à l'état de l'art sur une variété de tâches d'analyse de visages.
À STMicroelectronics, l'équipe de Business Intelligence est confrontée à exploiter quotidiennement des données et des informations pour créer des rapports d'activité afin de superviser la production.
Dans une telle organisation industrielle, les produits changent régulièrement et les données peuvent rapidement devenir obsolètes.
Par conséquent, au fil du temps, le nombre de rapports crées est de plus en plus important, tandis que les connaissances sur leur création sont perdues.
Ceci est illustré dans une évaluation qualitative et quantitative de la partie principale du système de connaissances à STMicroelectronics.
Ainsi, des problèmes d'obsolescence, de duplication, de non-centralisation et de prolifération continuent à surgir.
Son objectif est de capitaliser efficacement et en permanence les connaissances, tout en ciblant les besoins métier et assurant une solution évolutive.
Un système de Business Intelligence pour la Business Intelligence (BI4BI) est proposé.
Comme la connaissance est intégrée non seulement dans les systèmes et les outils, mais aussi détenue par les humains et leurs pratiques, notre solution de capitalisation de connaissances proposée implique aussi les utilisateurs et les organisations : elle propose de recueillir les points de vue des utilisateurs pour les intégrer dans la représentation des connaissances et dans notre système BI4BI.
La théorie des automates est apparue pour résoudre des problèmes aussi bien pratiques que théoriques, et ceci dès le début de l'informatique.
Désormais, les automates font partie des notions fondamentales de l'informatique, et se retrouvent dans la plupart des logiciels.
En 1974, Samuel Eilenberg proposa un modèle de calcul qui unifie la plupart des automates (transducteurs, automates à pile et machines de Turing) et qui a une propriété de modularité intéressante au vu d'applications reposant sur différentes couches d'automates ;
Nous proposons de rendre effectif ce modèle en étudiant des techniques de simulation.
Le simulateur est défini par un programme fonctionnel énumérant progressivement les solutions en explorant un espace de recherche selon différentes stratégies.
Nous introduisons la notion de machines d'Eilenberg finies pour lesquelles nous fournissons une preuve formelle de correction de la simulation.
Récemment, un ensemble de travaux approfondissant la notion de dérivées de Brzozowski, a été la source d'algorithmes efficaces de synthèse d'automates non-déterministes à partir d'expressions régulières.
Ces algorithmes sont de nature algébrique et nous en faisons un état de l'art, tout en donnant une implémentation en OCaml permettant de les comparer les uns aux autres dans un cadre commun.
Le Traitement Automatique du Langage (TAL) est un domaine de recherche qui s'est particulièrement développé ces dernières années.
En effet, la démocratisation des modes de communication à large échelle a entraîné l'apparition d'une immense quantité de donnée à forte valeur ajoutée qu'il est impossible de traiter manuellement.
L'extraction du sens de ces échanges représente un enjeu commercial de premier plan pour beaucoup d'acteurs privés, comme les GAFAMI (réseaux sociaux, retours clients sur des produits distribués par des boutiques en ligne, etc.), et publics.
La recherche académique et privée s'est donc naturellement orientée sur le développement de moyens permettant de déterminer le sens d'un énoncé à moindre coût et avec le moins de contraintes de fonctionnement possibles.
Ces technologies sont notamment intéressantes pour mettre en place un système d'écoute des réseaux sociaux concernant un lieu particulier, comme une gare, un musée ou un aéroport.
En effet, les informations qui peuvent circuler à leur sujet peuvent être exploitées pour répondre à différents besoins.
Par exemple, il est possible d'anticiper les futurs pics d'affluence et de les localiser sur un site afin d'améliorer la gestion de son personnel, ou de détecter des cibles pour d'éventuelles actions malveillantes.
L'élaboration de schémas de circulation en fonction de la langue des usagers peut également permettre d'optimiser l'affichage d'informations destinées aux clients pour le rendre plus facile d'accès.
L'extraction du sens de données textuelles peut cependant se heurter à une barrière linguistique, en particulier si elles sont écrites en différentes langues.
Leur acquisition et leur analyse doivent alors pouvoir s'opérer indépendamment du langage utilisé pour que l'information contenue dans le texte puisse être exploitée.
Dans ce contexte, le développement d'un outil capable d'écouter les échanges sur différents réseaux sociaux pour y identifier des éléments que l'on souhaite quantifier et synthétiser dans différentes langues se révèlerait particulièrement intéressant pour aider à améliorer les services et la sécurité dans un lieu tel qu'un aéroport.
Les évènements présentant un caractère d'urgence sont identifiés car pouvant impacter directement l'activité aéroportuaire.
Par exemple une tempête empêchera les avions d'atterrir ou de décoller et il faut le prévoir pour gérer les passager, les personnels, les correspondances etc.
Un embouteillage quant à lui peut empêcher les passagers d'accéder à l'avion, de sortir de l'aéroport, empêcher les pilotes et les personnels de prendre poste, bloquer les véhicules de service selon leur localisation et leur trajets prévus, etc.
C'est dans ces problématiques que s'inscrit ce projet de recherche.
Nous abordons le problème de la recherche indépendante du modèle pour la Nouvelle Physique (NP), au Grand Collisionneur de Hadrons (LHC) en utilisant le détecteur ATLAS.
Une attention particulière est accordée au développement et à la mise à l'essai de nouvelles techniques d'apprentissage automatique à cette fin.
Le présent ouvrage présente trois résultats principaux.
Tout d'abord, nous avons mis en place un système de surveillance automatique des signatures génériques au sein de TADA, un outil logiciel d'ATLAS.
Nous avons exploré plus de 30 signatures au cours de la période de collecte des données de 2017 et aucune anomalie particulière n'a été observée par rapport aux simulations des processus du modèle standard.
Deuxièmement, nous proposons une méthode collective de détection des anomalies pour les recherches de NP indépendantes du modèle au LHC.
Nous proposons l'approche paramétrique qui utilise un algorithme d'apprentissage semi-supervisé.
Cette approche utilise une probabilité pénalisée et est capable d'effectuer simultanément une sélection appropriée des variables et de détecter un comportement anormal collectif possible dans les données par rapport à un échantillon de fond donné.
Troisièmement, nous présentons des études préliminaires sur la modélisation du bruit de fond et la détection de signaux génériques dans des spectres de masse invariants à l'aide de processus gaussiens (GPs) sans information préalable moyenne.
Deux méthodes ont été testées dans deux ensembles de données : une procédure en deux étapes dans un ensemble de données tiré des simulations du modèle standard utilisé pour ATLAS General Search, dans le canal contenant deux jets à l'état final, et une procédure en trois étapes dans un ensemble de données simulées pour le signal (Z′) et le fond (modèle standard) dans la recherche de résonances dans le cas du spectre de masse invariant de paire supérieure.
Notre étude est une première étape vers une méthode qui utilise les GPs comme outil de modélisation qui peut être appliqué à plusieurs signatures dans une configuration plus indépendante du modèle.
De par leurs rôles écologiques multiples, les macrophytes sont une composante importante des hydrosystèmes, qu'il est essentiel de conserver.
Toutefois, durant la saison estivale, des densités importantes des espèces submergées entraînent des problèmes récurrents dans certains cours d'eau, notamment en milieu urbain pour les usagers et les gestionnaires, et peuvent avoir des conséquences négatives pour la santé des écosystèmes.
Dans un contexte de changements globaux, les enjeux liés à la prolifération des macrophytes submergés incitent à proposer des outils permettant de mieux comprendre la dynamique des herbiers et de prédire leur évolution selon différents scénarios environnementaux.
Dans cette optique, la présente thèse a pour objectif le développement d'une boîte à outils accompagnant un modèle mécaniste multispécifique de production des végétaux aquatiques submergés, le modèle DEMETHER.
Pour ce faire, il nécessite de connaître certains paramètres écophysiologiques et de disposer de données spatialisées de biomasse pour sa calibration, ainsi que de données bathymétriques et de substrat.
La première phase de ce travail a alors consisté à réaliser des relevés sur le terrain pour caractériser le site d'étude et à développer des outils numériques ou expérimentaux pour l'acquisition de ces données.
Le premier outil développé a pour objet le suivi des macrophytes submergés par télédétection.
La méthode explorée a confirmé le potentiel de l'imagerie multispectrale à haute résolution spatiale (50 cm) des satellites Pléiades, traitée par des algorithmes d'apprentissage automatique, pour cartographier la distribution des herbiers et quantifier leur biomasse in situ.
Cette approche nous a par ailleurs conduits à proposer une stratégie d'échantillonnage optimisée des macrophytes en grand cours d'eau pour de futures investigations.
Ce travail ouvre des perspectives intéressantes pour appliquer la méthode à de l'imagerie drone, et poursuivre son développement pour un suivi mensuel automatisé.
En parallèle, un outil de mesure de paramètres écophysiologiques par oxymétrie a été développé et appliqué aux deux espèces d'intérêt.
Les données obtenues renseignent en particulier sur les capacités photosynthétique et respiratoire de chaque espèce en réponse à des facteurs limitants (lumière, température).
La seconde phase de ce travail a consisté en l'application du modèle DEMETHER pour l'exploration de différents scénarios d'évolution climatique.
Des simulations de la dynamique des herbiers en termes de biomasse ont été réalisées pour les conditions thermiques actuelles et pour une hausse des températures prévisible à l'horizon 2041-2070.
Les résultats ont montré l'importance de la sensibilité de certains processus physiologiques à la température pour expliquer les patrons de distribution des deux espèces étudiées, soulignant l'intérêt de la modélisation mécaniste pour comprendre la structuration des communautés de macrophytes.
Les premiers résultats obtenus avec cette boîte à outils ont confirmé sa fonctionnalité.
Toutefois, en vue d'étendre son champ d'application, chacun des outils développés durant la thèse devra encore être amélioré, notamment pour affiner la calibration du modèle DEMETHER.
Des propositions précises ont été formulées dans ce sens
Cette thèse de doctorat traite de la reconnaissance automatique du Langage français Parlé Complété (LPC), version française du Cued Speech (CS), à partir de l'image vidéo et sans marquage de l'information préalable à l'enregistrement vidéo.
Afin de réaliser cet objectif, nous cherchons à extraire les caractéristiques de haut niveau de trois flux d'information (lèvres, positions de la main et formes), et fusionner ces trois modalités dans une approche optimale pour un système de reconnaissance de LPC robuste.
Dans ce travail, nous avons introduit une méthode d'apprentissage profond avec les réseaux neurono convolutifs (CNN)pour extraire les formes de main et de lèvres à partir d'images brutes.
Un modèle de mélange de fond adaptatif (ABMM) est proposé pour obtenir la position de la main.
Toutes ces méthodes constituent des contributions significatives pour l'extraction de caractéristiques du LPC.
En outre, en raison de l'asynchronie des trois flux caractéristiques du LPC, leur fusion est un enjeu important dans cette thèse.
Afin de le résoudre, nous avons proposé plusieurs approches, y compris les stratégies de fusion au niveau données et modèle avec une modélisation HMM dépendant du contexte.
Pour obtenir le décodage, nous avons proposé trois architectures CNNs-HMMs.
Toutes ces architectures sont évaluées sur un corpus de phrases codées en LPC en parole continue sans aucun artifice, et la performance de reconnaissance CS confirme l'efficacité de nos méthodes proposées.
Le résultat est comparable à l'état de l'art qui utilisait des bases de données où l'information pertinente était préalablement repérée.
En même temps, nous avons réalisé une étude spécifique concernant l'organisation temporelle des mouvements de la main, révélant une avance de la main en relation avec l'emplacement dans la phrase.
Néanmoins, jusqu'à présent, aucune théorie systématique n'a pas été construite qui peut fournir en une manière formelle la composition et la gamme des types de conversation actuels et possibles.
Dans cette thèse, nous adoptons une approche topologique pour classifier les conversations et développons une théorie formelle des types conversationnels dans le cadre de Type Theory with Records.
En revanche, nous testerons l'hypothèse que la variation entre des distributions des énoncés non phrastiques peut servir à structurer l'espace des types conversationnels.
Cette recherche porte sur la réalisation linguistique de l'explication en anglais contemporain.
À la suite d'un travail définitoire de la notion sont identifiés deux types d'explication : la clarification et l'explication causale.
L'étude adopte une approche énonciative tout en offrant des considérations d'ordre pragmatique.
L'analyse linguistique se fonde sur un corpus de textes didactiques et examine plus particulièrement le rôle des connecteurs dans le discours explicatif.
Enfin, ce travail vise également la formalisation de quelques relations discursives à l'œuvre dans l'explication et l'écriture de règles linguistiques en vue de la reconnaissance automatique de ces relations.
Cette thèse s'inscrit dans un mouvement de reconnaissance de l'importance accrue de l'institution judiciaire, et de questionnement actuel sur la légitimité démocratique du juge.
Dans ce cadre, elle enquête sur le rôle, dans la fonction et la pratique judiciaire, de l'opinion publique, largement considérée comme un élément de légitimité démocratique.
Pour obtenir un éclairage plus complet sur cette question, une approche comparative est adoptée et appliquée à l'œuvre protectrice d'une cour nationale constitutionnelle et d'une cour internationale dans le domaine des droits et des libertés : la Cour suprême des États-Unis et la Cour européenne des droits de l'Homme.
Le raisonnement suivi est le suivant.
Au niveau théorique, il s'agit de clarifier le concept protéiforme d' « opinion publique » et d'établir les différentes sources de la légitimité judiciaire, afin de déterminer si l'opinion publique peut en faire partie.
On se penche enfin sur la substance des décisions de justice, qui révèlent la manière dont les juges conçoivent le rôle de l'opinion publique dans la démocratie et dans l'évolution judiciaire des droits et libertés.
L'étude de la substance des décisions se concentre d'une part sur la relation entre opinion publique et démocratie dans la protection de la liberté d'expression, et d'autre part sur le rôle de l'opinion publique dans l'évolution des droits des personnes homosexuelles.
Nous proposons dans ce travail de thèse une étude des constructions en parce que à l'oral, à l'interface entre linguistique de corpus et linguistique théorique.
Le corpus analysé est composé de conversations enregistrées dans le cadre de quatre enquêtes du projet PFC (Phonologie du Français Contemporain).
La première phase de l'annotation permet la caractérisation syntaxique desdites constructions (Debaisieux, 1994).
Nous proposons pour ce faire une nouvelle relation de type Explication permettant l'annotation des parce que en lien avec la modalité d'énoncé.
Des éléments pour l'identification des termes mis en relation par parce que sont donnés, qui montrent l'intérêt d'une double analyse du corpus, en syntaxe et en discours.
En Interaction Homme-Machine, la qualité est une utopie : malgré toutes les précautions prises en conception, il existe toujours des utilisateurs et des situations d'usage pour lesquels l'Interface Homme-Machine (IHM) est imparfaite.
Cette thèse explore l'auto-explication des IHM pour améliorer la qualité perçue par les utilisateurs.
L'approche s'inscrit dans une Ingénierie Dirigée par les Modèles.
Elle consiste à embarquer à l'exécution les modèles de conception pour dynamiquement augmenter l'IHM d'un ensemble de questions et de réponses.
Les questions peuvent être relatives à l'utilisation de l'IHM (par exemple, "A quoi sert ce bouton ? ", "Pourquoi telle action n'est pas possible ?) et à sa justification (par exemple, "Pourquoi les items ne sont-ils pas rangés par ordre alphabétique ?").
Cette thèse propose une infrastructure logicielle UsiExplain basée sur les méta-modèles UsiXML.
L'évaluation sur un cas d'étude d'achat de voitures montre que l'approche est pertinente pour les questions d'utilisation de l'IHM.
L'augmentation constante du nombre de documents disponibles et des moyens d'accès transforme les pratiques de recherche d'information.
Depuis quelques années, de plus en plus de plateformes de recherche d'information à destination des chercheurs ou du grand public font leur apparition sur la toile.
Auparavant, la principale problématique des chercheurs était de savoir si une information existait.
Aujourd'hui, il est plutôt question de savoir comment accéder à une information pertinente.
Pour résoudre ce problème, deux leviers d'action seront étudiés dans cette thèse.
Nous pensons qu'il est avant tout important d'identifier l'usage qui est fait des principaux moyens d'accès à l'information.
Être capable d'interpréter le comportement des utilisateurs est une étape nécessaire pour d'abord identifier ce que ces derniers comprennent des systèmes de recherche, et ensuite ce qui doit être approfondi.
En effet, la plupart de ces systèmes agissent comme des boîtes noires qui masquent les différents processus sous-jacents.
Pourquoi le moteur de recherche me renvoie-t-il ces résultats ?
Pourquoi ce document est-il plus pertinent qu'un autre ?
Ces questions apparemment banales sont pourtant essentielles à une recherche d'information critique.
Nous pensons que les utilisateurs ont le droit et le devoir de s'interroger sur la pertinence des outils informatiques mis à leur disposition.
Pour les aider dans cette tâche, nous avons développé une plateforme de recherche d'information en ligne à double usage.
Elle peut tout d'abord être utilisée pour l'observation et la compréhension du comportement des utilisateurs.
De plus, elle peut aussi être utilisée comme support pédagogique, pour mettre en évidence les différents biais de recherche auxquels les utilisateurs sont confrontés.
Dans le même temps, ces outils doivent être améliorés.
Nous prenons dans cette thèse l'exemple de la qualité des documents qui a un impact certain sur leur accessibilité.
La quantité de documents disponibles ne cessant d'augmenter, les opérateurs humains sont de moins en moins capables de les corriger manuellement et de s'assurer de leur qualité.
Il est donc nécessaire de mettre en place de nouvelles stratégies pour améliorer le fonctionnement des systèmes de recherche.
Nous proposons dans cette thèse une méthode pour automatiquement identifier et corriger certaines erreurs générées par les processus automatiques d'extraction d'information (en particulier l'OCR).
Le présent travail introduit en phonétique la décomposition atomique du signal, appelée aussi Matching Pursuit, traite les fichiers d'atomes par compression sans perte et enfin mesure la distance des fichiers comprimés par des algorithmes de Kolmogorov.
L'étalonnage est basé sur une première analyse classique de la coarticulation de séquences sonores VCV et CV, (ou V ∈ {[i] [u] [a]} et C ∈ {[t] [d] [s] [δ]}∪{[tʕ] [dʕ] [sʕ [δʕ]}, extraites d'un corpus issu de quatre régions arabophones.
L'équation de locus de CV vs CʕV, permet de différencier les variétés de langue.
La deuxième analyse applique un algorithme de décomposition atomique adaptative ou Matching Pursuit sur des séquences VCV et VCʕV du même corpus.
Les séquences atomiques représentant VCV et VCʕV sont ensuite compressées sans perte et la distance entre elles est recherchée par des algorithmes de Kolmogorov.
La classification des productions phonétiques et des régions arabophones obtenue est équivalente à celle de la première méthode.
Ce travail montre l'intérêt de l'introduction de Matching Pursuit en phonétique, la grande robustesse des algorithmes utilisés et suggère d'importantes possibilités d'automatisation des processus mis en oeuvre, tout en ouvrant de nouvelles directions d'investigation
Ce travail de recherche est conjointement effectué dans le cadre d'une cotutelle entre deux universités : en France l'Université Jean Monnet de Saint-Etienne, laboratoire Hubert Curien sous la supervision de Mme Frédérique Laforest, M. Christophe Gravier et M. Julien Subercaze, et au Maroc l'Université Mohamed V de Rabat, équipe LeRMA sous la supervision de Mme Rachida Ajhoun et Mme Mounia Abik.
Les connaissances et les apprentissages sont des préoccupations majeures dans la société d'aujourd'hui.
Les technologies de l'apprentissage humain visent à promouvoir, stimuler, soutenir et valider le processus d'apprentissage.
Notre approche explore les opportunités soulevées en faisant coopérer le Web Social et le Web sémantique pour le e-learning.
Plus précisément, nous travaillons sur l'enrichissement des profils des apprenants en fonction de leurs activités sur le Web Social.
Le Web social peut être une source d'information très importante à explorer, car il implique les utilisateurs dans le monde de l'information et leur donne la possibilité de participer à la construction et à la diffusion de connaissances.
Nous nous focalisons sur le suivi des différents types de contributions, dans les activités de collaboration spontanée des apprenants sur les réseaux sociaux.
Le profil de l'apprenant est non seulement basé sur la connaissance extraite de ses activités sur le système de e-learning, mais aussi de ses nombreuses activités sur les réseaux sociaux.
En particulier, nous proposons une méthodologie pour exploiter les hashtags contenus dans les écrits des utilisateurs pour la génération automatique des intérêts des apprenants dans le but d'enrichir leurs profils.
Cependant les hashtags nécessitent un certain traitement avant d'être source de connaissances sur les intérêts des utilisateurs.
Nous avons défini une méthode pour identifier la sémantique de hashtags et les relations sémantiques entre les significations des différents hashtags.
Par ailleurs, nous avons défini le concept de Folksionary, comme un dictionnaire de hashtags qui pour chaque hashtag regroupe ses définitions en unités de sens.
Les hashtags enrichis en sémantique sont donc utilisés pour nourrir le profil de l'apprenant de manière à personnaliser les recommandations sur le matériel d'apprentissage.
L'objectif est de construire une représentation sémantique des activités et des intérêts des apprenants sur les réseaux sociaux afin d'enrichir leurs profils.
Nous présentons également notre approche générale de recommandation multidimensionnelle dans un environnement d'e-learning.
Notre implémentation s'est focalisée sur la recommandation personnalisée
Les travaux présentés de cette thèse visent à proposer des outils numériques et théoriques pour la résolution de problèmes inverses en imagerie.
Une approche plébiscitée pour estimer un opérateur de dégradation consiste à observer un échantillon contenant des sources ponctuelles (microbilles en microscopie, étoiles en astronomie).
Une telle acquisition fournit une mesure de la réponse impulsionnelle de l'opérateur en plusieurs points du champ de vue.
Le traitement de cette observation requiert des outils robustes pouvant utiliser rapidement les données rencontrées en pratique.
Nous proposons une boîte à outils qui estime un opérateur de dégradation à partir d'une image contenant des sources ponctuelles.
L'opérateur estimé à la propriété qu'en tout point du champ de vue, sa réponse impulsionnelle s'exprime comme une combinaison linéaire de fonctions élémentaires.
Cela permet d'estimer des opérateurs invariants (convolutions) et variants (développement en convolution-produit) spatialement.
Une spécificité importante de cette boîte à outils est son caractère automatique : seul un nombre réduit de paramètres facilement accessibles permettent de couvrir une grande majorité des cas pratiques.
La taille de la source ponctuelle (e.g. bille), le fond et le bruit sont également pris en compte dans l'estimation.
Cet outil se présente sous la forme d'un module appelé PSF-Estimator pour le logiciel Fiji, et repose sur une implémentation parallélisée en C++.
En réalité, les opérateurs modélisant un système optique varient d'une expérience à une autre, ce qui, dans l'idéal, nécessite une calibration du système avant chaque acquisition.
Pour pallier à cela, nous proposons de représenter un système optique non pas par un unique opérateur de dégradation, mais par un sous-espace d'opérateurs.
Cet ensemble doit permettre de représenter chaque opérateur généré par un microscope.
Nous introduisons une méthode d'estimation d'un tel sous-espace à partir d'une collection d'opérateurs de faible rang (comme ceux estimés par la boîte à outils PSF-Estimator).
Nous montrons que sous des hypothèses raisonnables, ce sous-espace est de faible dimension et est constitué d'éléments de faible rang.
Dans un second temps, nous appliquons ce procédé en microscopie sur de grands champs de vue et avec des opérateurs variant spatialement.
Cette mise en œuvre est possible grâce à l'utilisation de méthodes complémentaires pour traiter des images réelles (e.g.
L'objectif de cette de thèse est d'étudier à la fois des solutions matérielles et logicielles pour améliorer les conditions de travail des sapeurs-pompiers.
Il s'agit de développer un système intelligent basé sur l'internet des objets pour surveiller l'état de santé des pompiers et aider à les localiser lors des interventions.
Dans la première partie de la thèse, nous avons étudié et proposé plusieurs approches permettant de réduire la consommation d'énergie du système afin de maximiser sa durée de vie.
La première approche présente un modèle de prédiction basé sur la corrélation temporelle entre les mesures collectées par le même capteur.
Il permet de réduire la quantité de données collectées et transmises au centre de contrôle.
Ce modèle est exécuté à la fois par le capteur et le centre et qui s'auto-adapte en fonction de l'écart constaté entre les mesures réelles collectées et les mesures prédites.
Une deuxième version de cette approche a été étudiée pour prendre en considération la perte de message et la synchronisation entre le capteur et le centre de contrôle.
D'un autre côté et pour réduire davantage la consommation d'énergie, nous avons couplé l'approche de prédiction avec un algorithme de collecte de données adaptatif permettant de réduire l'activité du capteur et le taux d'échantillonnage.
Toutes ces approches ont été testées via des simulations et de l'implémentation réelle.
Les résultats obtenus montrent l'efficacité de ces approches en termes de réduction de la consommation d'énergie tout en gardant l'intégrité de données.
La deuxième partie de cette thèse est dédiée au traitement des données issues des interventions des sapeurs-pompiers.
Nous avons étudié plusieurs méthodes de clustérisation permettant un prétraitement de données avant l'extraction des connaissances.
D'un autre côté, nous avons appliqué des méthodes d'apprentissage profond sur un grand ensemble de données concernant 200.000 interventions qui ont eu lieu pendant une période de 6 ans dans le département du Doubs, en France.
Le but de cette partie était de prédire le nombre d'interventions futures en fonction de variables explicatives externes, pour aider les pompiers à bien gérer leurs ressources.
Le sujet de la thèse est l'extraction et la segmentation des vêtements à partir d'images en utilisant des techniques de la vision par ordinateur, de l'apprentissage par ordinateur et de la description d'image, pour la recommandation de manière non intrusive aux utilisateurs des produits similaires provenant d'une base de données de vente.
Nous proposons tout d'abord un extracteur d'objets dédié à la segmentation de la robe en combinant les informations locales avec un apprentissage préalable.
Un détecteur de personne localises des sites dans l'image qui est probable de contenir l'objet.
Ensuite, un processus d'apprentissage intra-image en deux étapes est est développé pour séparer les pixels de l'objet de fond.
L'objet est finalement segmenté en utilisant un algorithme de contour actif qui prend en compte la segmentation précédente et injecte des connaissances spécifiques sur la courbure locale dans la fonction énergie.
Nous proposons ensuite un nouveau framework pour l'extraction des vêtements généraux en utilisant une procédure d'ajustement globale et locale à trois étapes.
Un ensemble de modèles initialises un processus d'extraction d'objet par un alignement global du modèle, suivi d'une recherche locale en minimisant une mesure de l'inadéquation par rapport aux limites potentielles dans le voisinage.
Les résultats fournis par chaque modèle sont agrégés, mesuré par un critère d'ajustement globale, pour choisir la segmentation finale.
De plus, nous introduisons une nouvelle base de données RichPicture, constituée de 1000 images pour l'extraction de vêtements à partir d'images de mode.
Les méthodes sont validées sur la base de données publiques et se comparent favorablement aux autres méthodes selon toutes les mesures de performance considérées.
Les données de classement, c.à. d. des listes ordonnées d'objets, apparaissent naturellement dans une grande variété de situations, notamment lorsque les données proviennent d'activités humaines (bulletins de vote d'élections, enquêtes d'opinion, résultats de compétitions) ou dans des applications modernes du traitement de données (moteurs de recherche, systèmes de recommendation).
La conception d'algorithmes d'apprentissage automatique, adaptés à ces données, est donc cruciale.
Cependant, en raison de l'absence de structure vectorielle de l'espace des classements et de sa cardinalité explosive lorsque le nombre d'objets augmente, la plupart des méthodes classiques issues des statistiques et de l'analyse multivariée ne peuvent être appliquées directement.
Par conséquent, la grande majorité de la littérature repose sur des modèles paramétriques.
Dans cette thèse, nous proposons une théorie et des méthodes non paramétriques pour traiter les données de classement.
Notre analyse repose fortement sur deux astuces principales.
La première est l'utilisation poussée de la distance du tau de Kendall, qui décompose les classements en comparaisons par paires.
Cela nous permet d'analyser les distributions sur les classements à travers leurs marginales par paires et à travers une hypothèse spécifique appelée transitivité, qui empêche les cycles dans les préférences de se produire.
La seconde est l'utilisation des fonctions de représentation adaptées aux données de classements, envoyant ces dernières dans un espace vectoriel.
Trois problèmes différents, non supervisés et supervisés, ont été abordés dans ce contexte : l'agrégation de classement, la réduction de dimensionnalité et la prévision de classements avec variables explicatives.
La première partie de cette thèse se concentre sur le problème de l'agrégation de classements, dont l'objectif est de résumer un ensemble de données de classement par un classement consensus.
Dans cette thèse, nous avons étudié la complexité de ce problème de deux manières.
Premièrement, nous avons proposé une méthode pour borner la distance du tau de Kendall entre tout candidat pour le consensus (généralement le résultat d'une procédure efficace) et un consensus de Kemeny, sur tout ensemble de données.
Nous avons ensuite inscrit le problème d'agrégation de classements dans un cadre statistique rigoureux en le reformulant en termes de distributions sur les classements, et en évaluant la capacité de généralisation de consensus de Kemeny empiriques.
La deuxième partie de cette théorie est consacrée à des problèmes d'apprentissage automatique, qui se révèlent être étroitement liés à l'agrégation de classement.
Le premier est la réduction de la dimensionnalité pour les données de classement, pour lequel nous proposons une approche de transport optimal, pour approximer une distribution sur les classements par une distribution montrant un certain type de parcimonie.
Le second est le problème de la prévision des classements avec variables explicatives, pour lesquelles nous avons étudié plusieurs méthodes.
Notre première proposition est d'adapter des méthodes constantes par morceaux à ce problème, qui partitionnent l'espace des variables explicatives en régions et assignent à chaque région un label (un consensus).
Notre deuxième proposition est une approche de prédiction structurée, reposant sur des fonctions de représentations, aux avantages théoriques et computationnels, pour les données de classements.
La connaissance des positions et des mouvements des articulateurs (lèvres, palais, langue...) du conduit vocal lors de la phonation est un enjeu crucial pour l'étude de la parole.
Nous décrivons un ensemble de protocoles et de méthodes pour obtenir et fusionner automatiquement un important volume de données échographiques (imageant en 2D la dynamique de la langue), stéréoscopiques (imageant en 3D la dynamique des lèvres), de capteurs électromagnétiques (capturant des points 3D de la langue et du visage), et d'Imagerie par Résonance Magnétique (IRM) pour acquérir en 3D l'ensemble des articulateurs en position statique.
Nos contributions concernent plus particulièrement la synchronisation temporelle, le recalage spatial des données et l'extraction automatique des formes à partir des données (suivi de la langue dans les images échographiques).
Ces travaux permettent l'obtention de données bien fondées pour la mise en place et l'étude de modèles articulatoires pour des applications en parole.
De nombreux algorithmes d'Apprentissage automatique ont été proposés afin de résoudre les différentes tâches pouvant être extraites des problèmes de prédiction issus d'un contexte réel.
Pour résoudre les différentes tâches pouvant être extraites, la plupart des algorithmes d'Apprentissage automatique se basent d'une manière ou d'une autre sur des relations liant les instances.
Les relations entre paires d'instances peuvent être définies en calculant une distance entre les représentations vectorielles des instances.
En se basant sur la représentation vectorielle des données, aucune des distances parmi celles communément utilisées n'est assurée d'être représentative de la tâche à résoudre.
Dans ce document, nous étudions l'intérêt d'adapter la représentation vectorielle des données à la distance utilisée pour une meilleure résolution de la tâche.
Nous nous concentrons plus précisément sur l'algorithme existant résolvant une tâche de classification en se basant sur un graphe.
Nous décrivons d'abord un algorithme apprenant une projection des données dans un espace de représentation permettant une résolution, basée sur un graphe, optimale de la classification.
En projetant les données dans un espace de représentation dans lequel une distance préalablement définie est représentative de la tâche, nous pouvons surpasser la représentation vectorielle des données lors de la résolution de la tâche.
Une analyse théorique de l'algorithme décrit est développée afin de définir les conditions assurant une classification optimale.
Un ensemble d'expériences nous permet finalement d'évaluer l'intérêt de l'approche introduite et de nuancer l'analyse théorique.
Les éditions critiques numériques sont des ressources patrimoniales annotées, sous une forme numérique.
De telles éditions prennent la forme d'une transcription des ressources originales, augmentées d'un apparat critique, c'est-à-dire, la forme de données structurées.
Dans un contexte collaboratif, a structure de ces données est définie explicitement par un schéma, document interprétable qui contraint la manière dont les éditeurs vont pouvoir annoter les ressources primaires et va de ce fait garantir une certaine homogénéité dans le respect de la politique éditoriale.
Les projets d'édition critique numérique font classiquement face à deux problèmes techniques.
Le premier a à voir avec l'expressivité des langages d'annotation, qui empêchent l'expression de certaines informations utiles.
La seconde tient au fait que, par expérience, les schémas qui sous-tendent une édition critique vont être amenés à évoluer au cours de la réalisation de cette édition ;
cependant, modifier le schéma implique qu'il faille mettre à jour l'intégralité des données structurées validées par ce schéma, ce qui est habituellement effectué à la main par les éditeurs, au moyen de scripts ad-hoc – si les éditeurs, faute de moyens ou de temps, ne renoncent pas à faire évoluer la structure de données.
Dans ce travail de thèse, nous définissons les fondements théoriques pour l'établissement d'un système éditorial dédié à l'édition critique numérique.
Nous définissons les eAG, un modèle d'annotation déporté basé sur un formalisme de graphes cycliques, autorisant a plus grande expressivité.
Nous définissons un mécanisme de schéma innovant, SeAG, permettant la validation à la volée des eAG au cours de leur manufacture.
Nous définissons également une syntaxe de balisage présentant des similarités avec les langages d'annotation classiques comme XML, tout en préservant l'expressivité des eAG.
Enfin, nous proposons une algèbre bidirectionnelle pour les eAG de telle sorte que, si un SeAG S est transformé en un SeAG S', alors tout eAG I validé par S est traduit de manière semi-automatique sous la forme d'un eAG I', validé par S', et tel que toute mise à jour de I (respectivement I') soit propagé, de manière semi-automatique, sur I'(resp. I).
La conception des systèmes de contrôle-commande souffre souvent des problèmes de communication et d'interprétation des spécifications entre les différents intervenants provenant souvent de domaines techniques très variés.
Afin de cadrer la conception de ces systèmes, plusieurs démarches ont été proposées dans la littérature.
Parmi elles, la démarche dite mixte (ascendante/descendante), qui voit la conception réalisée en deux phases.
Dans la première phase (ascendante), un modèle du système est défini à partir d'un ensemble de composants standardisés.
Ce modèle subit, dans la deuxième phase (descendante), plusieurs raffinages et transformations pour obtenir des modèles plus concrets (codes,applicatifs, etc.).
Afin de garantir la qualité des systèmes conçus par cette démarche, nous proposons dans cette thèse, deux approches de vérification formelle basées sur le Model-Checking.
La première approche porte sur la vérification des composants standardisés et permet la vérification d'une chaîne de contrôle-commande élémentaire complète.
La deuxième approche consiste en la vérification des modèles d'architecture (PandID) utilisés pour la génération des programmes de contrôle-commande.
Cette dernière est basée sur la définition d'un style architectural en Alloy pour la norme ANSI/ISA-5.1.
Pour supporter les deux approches, deux flots de vérification formelle semi-automatisés basés sur les concepts de l'IDM ont été proposés.
L'intégration des méthodes formelles dans un contexte industriel est facilitée, ainsi, par la génération automatique des modèles formels à partir des modèles de conception maîtrisés par les concepteurs métiers.
Nos deux approches ont été validées sur un cas industriel concret concernant un système de gestion de fluide embarqué dans un navire.
Dans ce contexte, les requêtes sont résolues par la composition de plusieurs services DaaS. Définir la sémantique des services cloud
DaaS est la première étape vers l'automatisation de leur composition.
Une approche intéressante pour définir la sémantique des services DaaS est de les décrire comme étant des vues sémantiques à travers une ontologie de domaine.
Cependant, la définition de ces vues sémantiques ne peut pas être toujours faite avec certitude, surtout lorsque les données retournées par un service sont trop complexes.
Dans cette thèse, nous proposons une approche probabiliste pour représenter les services DaaS à sémantique incertaine.
Dans notre approche, un service DaaS dont la sémantique est incertaine est décrit par plusieurs vues sémantiques possibles, chacune avec une probabilité.
En se basant sur nos modèles probabilistes, nous étudions le problème de l'interprétation d'une composition existante impliquant des services à sémantique incertaine.
Nous étudions aussi le problème de la réécriture de requêtes à travers les services DaaS incertains, et nous proposons des algorithmes efficaces permettant de calculer les différentes compositions possibles ainsi que leurs probabilités.
Nous menons une série d'expérimentation pour évaluer la performance de nos différents algorithmes de composition.
Les résultats obtenus montrent l'efficacité et la scalabilité de nos solutions proposées
Cette thèse s'inscrit dans le domaine du traitement automatique des langues et concerne l'analyse sémantique de la structure du discours.
Nous nous attachons plus particulièrement au problème de l'analyse thématique, qui vise l'étude de la structure des textes selon des critères relatifs à la répartition de leur contenu informationnel.
Cette tâche revêt une importance capitale dans la perspective de l'accès assisté à l'information, qui constitue notre principale visée applicative.
Le concept même de "thème" étant à la fois complexe et assez rarement considéré en tant qu'objet d'étude dans le domaine de la recherche d'information, la première partie du mémoire est consacrée à une vaste étude bibliographique autour des notions de thème, de topique, de sujet ou encore d'à propos, tant en linguistique qu'en sciences de l'information ou en traitement des langues.
Nous en dégageons les lignes de force qui fondent notre approche du thème comme objet discursif, sémantique et structuré.
Nous proposons sur cette base différents modèles et procédés s'attachant d'abord au traitement sémantique des documents géographiques, puis à l'analyse automatique des cadres de discours spatio-temporels au sens de Michel Charolles.
Nous généralisons ces travaux en introduisant les notions de thème discursif composite et d'axe sémantique.
Nous terminons en présentant LinguaStream, environnement d'expérimentation intégré que nous avons conçu pour faciliter l'élaboration de modèles linguistiques opérationnels, et qui nous conduit à proposer des principes méthodologiques originaux.
Notre projet de recherche vise à comprendre comment les organisations peuvent s'adapter à leur environnement et le faire évoluer en tirant partie de la richesse d'
Ils montrent par ailleurs que l'objectif du transfert d'informations entre les chargés d'études et le marketing opérationnel, ainsi que le degré de contrôle sur l'interprétation des informations par le marketing opérationnel influence la coordination des activités qui composent l'agilité.
La prolifération des réseaux sociaux et des données à caractère personnel apporte de nombreuses possibilités de développement de nouvelles applications.
Au même temps, la disponibilité de grandes quantités de données à caractère personnel soulève des problèmes de confidentialité et de sécurité.
Dans cette thèse, nous développons des méthodes pour identifier les différents comptes d'un utilisateur dans des réseaux sociaux.
Nous étudions d'abord comment nous pouvons exploiter les profils publics maintenus par les utilisateurs pour corréler leurs comptes.
Nous identifions quatre propriétés importantes-la disponibilité, la cohérence, la non-impersonabilite, et la discriminabilité (ACID)-pour évaluer la qualité de différents attributs pour corréler des comptes.
On peut corréler un grand nombre de comptes parce-que les utilisateurs maintiennent les mêmes noms et d'autres informations personnelles à travers des différents réseaux sociaux.
Pourtant, il reste difficile d'obtenir une précision suffisant pour utiliser les corrélations dans la pratique à cause de la grandeur de réseaux sociaux réels.
Nous développons des schémas qui obtiennent des faible taux d'erreur même lorsqu'elles sont appliquées dans les réseaux avec des millions d'utilisateurs.
Ensuite, nous montrons que nous pouvons corréler les comptes d'utilisateurs même si nous exploitons que leur activité sur un les réseaux sociaux.
Ça sa démontre que, même si les utilisateurs maintient des profils distincts nous pouvons toutefois corréler leurs comptes.
Enfin, nous montrons que, en identifiant les comptes qui correspondent à la même personne à l'intérieur d'un réseau social, nous pouvons détecter des imitateurs.
L'analyse de la littérature académique sur l'influence de la couleur débouche sur la proposition d'un cadre conceptuel des effets de la couleur de fond dans le contexte des avis de consommateurs en ligne (chapitre 2).
La phase empirique de cette recherche s'articule autour de trois études expérimentales.
Une première s'attache à examiner l'influence d'une couleur de fond de valence négative (rouge) pour un avis de consommateur en ligne négatif sur les évaluations des consommateurs exposés (chapitre 3).
Une seconde étude expérimentale porte sur le rôle de la valence perçue de la couleur comme un mécanisme d'influence de la couleur de fond d'un avis de consommateur en ligne (chapitre 4).
La recherche d'une plus grande validité écologique débouche sur une troisième étude expérimentale.
Les participants sont alors exposés à plusieurs avis présentés simultanément sur une page type d'avis de consommateurs en ligne (chapitre 5).
Cette recherche présente un certain nombre d'apports, et n'est pas dépourvue de limites qui ouvrent d'importantes voies de recherches additionnelles (conclusion).
Cette thèse aborde la conception d'un Système de Gestion Énergétique (EMS), prenant en compte les contraintes de trafic, pour un véhicule hybride électrique.
Actuellement, les EMS sont habituellement classé en deux catégories ceux proposant une architecture en temps réel cherchant un optimum local, et ceux qui recherchent un optimum global, plus coûteux en temps de calcul et donc plus approprié à un usage hors ligne.
Cette thèse repose sur le fait que la consommation énergétique peut être modélisée précisément à l'aide de distributions de probabilité sur la vitesse et l'accélération.
Dans le but de réduire la taille des données, une classification est proposé, basé sur la distance de Wasserstein, les barycentres des classes pouvant être calculés grâce aux itérations de Sinkhorn ou la méthode du Gradient Stochastique Alterné.
Cette modélisation trafic a permis à une optimisation hors ligne de déterminer le contrôle optimal (le couple du moteur électrique) qui minimise la consommation de carburant du véhicule hybride sur un segment routier.
Les réseaux de neurones ont obtenu ces dernières années des succès notables dans des domaines tels que la vision par ordinateur ou bien le traitement du langage naturel.
Ceux-ci bénéficient des grandes quantités de données annotées.
Toutefois, régulariser ces neurones quand les données annotées sont rares reste un problème ouvert, la plupart des approches existantes n'étant pas satisfaisantes.
Le but de cette thèse est donc de développer des algorithmes d'optimisation et des stratégies de régularisation adaptés aux réseaux de neurones profonds, avec un accent mis sur les régimes où les données annotées sont plus rares.
Les pistes possibles comprennent les normes spectrales et les normes d'espaces à noyau reproduisant.
L'architecture joue également un rôle important dans la complexité de l'entraînement d'un réseau de neurones.
Ce problème présente une structure particulière qu'il convient d'exploiter lors de la phase d'optimisation.
En particulier, le théorème de Shapley-Folkman montre que ce type de problème présente un faible écart de dualité dans certaines configurations.
Plusieurs ressources publiées sur le Web de données sont dotées de références spatiales qui décrivent leur localisation géographique.
Ces références spatiales sont un moyen favori pour interconnecter et visualiser les ressources sur le Web de données.
Cependant, les hétérogénéités des niveaux de détail et de modélisations géométriques entre les sources de données constituent un défi majeur pour l'utilisation de la comparaison des références spatiales comme critère pour l'interconnexion des ressources.
Ce défi est amplifié par la nature ouverte et collaborative des sources de données du Web qui engendre des hétérogénéités géométriques internes aux sources de données.
En outre, les applications de visualisation cartographique des ressources géoréférencées du Web de données ne fournissent pas une visualisation lisible à toutes les échelles.
Dans cette thèse, nous proposons un vocabulaire pour formaliser les connaissances sur les caractéristiques de chaque géométrie dans un jeu de données.
Nous proposons également une approche semi-automatique basée sur un référentiel topographique pour acquérir ces connaissances.
Nous proposons de mettre en oeuvre ces connaissances dans une approche d'adaptation dynamique du paramétrage de la comparaison des géométries dans un processus d'interconnexion.
Nous proposons une approche complémentaire s'appuyant sur un référentiel topographique pour la détection des liens de cardinalité n : m.
Nous proposons finalement des applications qui s'appuient sur des données topographiques de référence et leurs liens avec les ressources géoréférencées du Web pour offrir une visualisation cartographique multiéchelle lisible et conviviale
Cette thèse porte sur l'utilisation d'une langue contrôlée pour les spécifications des besoins du logiciel en thaï.
L'étude décrit les ambiguïtés syntaxiques et sémantiques ainsi que les problèmes rencontrés dans les spécifications des besoins du logiciel en thaï.
Ce travail explique également la nature de la langue thaïe.
Le modèle de la langue contrôlée pour les spécifications des besoins du logiciel en thaï, proposé dans cette étude, comprend trois composantes : l'analyse lexicale, l'analyse syntaxique et l'analyse sémantique.
Pour l'analyse syntaxique, une syntaxe contrôlée est conçue en utilisant la forme du Backus-Naur (BNF).
Quant à l'analyse lexicale, nous créons une ressource lexicale sous forme de langage XML pour stocker tous les mots classés selon leur domaine.
Les mots reçus de la ressource XML sont corrects d'un point de vue conceptuel mais ne sont pas pertinents d'un point de vue sémantique.
Pour résoudre ce problème, nous faisons alors usage de matrices booléennes pour aligner les phrases sémantiquement.
Ainsi les phrases produites par le modèle seront syntaxiquement et sémantiquement correctes.
Après avoir créé le modèle, nous avons construit un logiciel pour tester son efficacité.
Il est ainsi évalué par quatreméthodes d'évaluation : 1. le test de fonctionnement syntaxique pour vérifier la syntaxe de la phrase ; 2. le test defonctionnement sémantique pour tester la sémantique de la phrase ; 3. le test d'acceptation en terme de satisfaction desutilisateurs avec le logiciel ; et 4. le test d'acceptation en terme d'acception des données de sortie.
Des résultats positifs montrent que : 1.les phrases produites par le modèle proposé sont syntaxiquement correctes ; 2. les phrases produites par le modèle proposé sont sémantiquement correctes ; 3. les utilisateurs sont satisfaits et acceptent lelogiciel ; et 4. les utilisateurs acceptent et comprennent les phrases produites par ce modèle.
Dans cette thèse, nous présentons notre travail sur le développement d'algorithmes de traitement automatique des langues (TAL) pour aider les lecteurs et les auteurs d'articles scientifiques (biomédicaux) à détecter le spin (présentation inadéquate des résultats de recherche).
Notre algorithme se concentre sur le spin dans les résumés d'articles rapportant des essais contrôlés randomisés.
Nous avons étudié le phénomène de ” spin ” du point de vue linguistique pour créer une description de ses caractéristiques textuelles.
Nous avons annoté des corpus pour les tâches principales de notre chaîne de traitement pour la détection de spin : extraction des résultats — en anglais ” outcomes ” — déclarés (primaires) et rapportés, évaluation de la similarité sémantique des paires de résultats d'essais et extraction des relations entre les résultats rapportés et leurs niveaux de signification statistique.
En outre, nous avons annoté deux corpus plus petits pour identifier les déclarations de similarité des traitements et les comparaisons intra-groupe.
Nous avons développé et testé un nombre d'algorithmes d'apprentissage automatique et d'algorithmes basés sur des règles pour les tâches principales de la détection de spin (extraction des résultats, évaluation de la similarité des résultats et extraction de la relation résultat-signification statistique).
La meilleure performance a été obtenues par une approche d'apprentissage profond qui consiste à adapter les représentations linguistiques pré-apprises spécifiques à un domaine (modèles de BioBERT et SciBERT) à nos tâches.
Cette approche a été mise en oeuvre dans notre système prototype de détection de spin, appelé DeSpin, dont le code source est librement accessible sur un serveur public.
Notre prototype inclut d'autres algorithmes importants, tels que l'analyse de structure de texte (identification du résumé d'un article,identification de sections dans le résumé), la détection de déclarations de similarité de traitements et de comparaisons intra-groupe, l'extraction de données de registres d'essais.
L'identification des sections des résumés est effectuée avec une approche d'apprentissage profond utilisant le modèle BioBERT, tandis que les autres tâches sont effectuées à l'aide d'une approche basée sur des règles.
Notre système prototype a une interface simple d'annotation et de visualisation.
Avec l'émergence et la généralisation des systèmes d'intelligence artificielle, comprendre le comportement des agents artificiels, ou robots intelligents, devient essentiel pour garantir une collaboration fluide entre l'homme et ces agents.
De récentes études dans le domaine l'intelligence artificielle explicable, particulièrement sur les modèles utilisant des objectifs, ont confirmé qu'expliquer le comportement d'un agent à un humain favorise la compréhensibilité de l'agent par ce dernier et augmente son acceptabilité.
Cependant, fournir des informations trop nombreuses ou inutiles peut également semer la confusion chez les utilisateurs humains et provoquer des malentendus.
Pour ces raisons, la parcimonie des explications a été présentée comme l'une des principales caractéristiques facilitant une interaction réussie entre l'homme et l'agent.
Une explication parcimonieuse est définie comme l'explication la plus simple et décrivant la situation de manière adéquate.
Si la parcimonie des explications fait l'objet d'une attention croissante dans la littérature, la plupart des travaux ne sont réalisés que de manière conceptuelle.
Pour fournir des explications parcimonieuses, HAExA s'appuie d'abord sur la génération d'explications normales et contrastées, et ensuite sur leur mise à jour et leur filtrage avant de les communiquer à l'humain.
Nous validons nos propositions en concevant et menant des études empiriques d'interaction homme-machine utilisant la simulation orientée-agent.
Nos études reposent sur des mesures bien établies pour estimer la compréhension et la satisfaction des explications fournies par HAExA.
Les résultats sont analysés et validés à l'aide de tests statistiques paramétriques et non paramétriques.
Pour cela, nous introduisons au Chapitre 3, trois nouveaux modèles prenant en compte les dépendances entre les thèmes relatifs à chaque document pour deux documents successifs.
Le deuxième modèle utilise les copules, outil générique servant à modéliser les dépendances entre variables aléatoires.
La famille de copules utilisée est la famille des copules Archimédiens et plus précisément la famille des copules de Franck qui vérifient de bonnes propriétés (symétrie, associativité) et qui sont donc adaptés à la modélisation de variables échangeables.
Nos expériences numériques, réalisées sur cinq collections standard, mettent en évidence les performances de notre approche, par rapport aux approches existantes dans la littérature comme les dynamic topic models, le temporal LDA et les Evolving Hierarchical Processes, et ceci à la fois sur le plan de la perplexité et en terme de performances lorsqu'on cherche à détecter des thèmes similaires dans des flux de documents.
Notre approche, comparée aux autres, se révèle être capable de modéliser un plus grand nombre de situations allant d'une dépendance forte entre les documents à une totale indépendance.
Par ailleurs, l'hypothèse d'échangeabilité sous jacente à tous les topics models du type du LDA amène souvent à estimer des thèmes différents pour des mots relevant pourtant du même segment de phrase ce qui n'est pas cohérent.
Dans le Chapitre 4, nous introduisons le copulaLDA (copLDA), qui généralise le LDA en intégrant la structure du texte dans le modèle of the text et de relaxer l'hypothèse d'indépendance conditionnelle.
Nous montrons de manièreempirique l'efficacité du modèle copLDA pour effectuer à la fois des tâches de natureintrinsèque et extrinsèque sur différents corpus accessibles publiquement.
Pour compléter le modèle précédent (copLDA), le chapitre 5 présente un modèle de type LDA qui génére des segments dont les thèmes sont cohérents à l'intérieur de chaque document en faisant de manière simultanée la segmentation des documents et l'affectation des thèmes à chaque mot.
La cohérence entre les différents thèmes internes à chaque groupe de mots est assurée grâce aux copules qui relient les thèmes entre eux.
De plus ce modèle s'appuie tout à la fois sur des distributions spécifiques pour les thèmes reliés à chaque document et à chaque groupe de mots, ceci permettant de capturer les différents degrés de granularité.
Nous montrons que le modèle proposé généralise naturellement plusieurs modèles de type LDA qui ont été introduits pour des tâches similaires.
Par ailleurs nos expériences, effectuées sur six bases de données différentes mettent en évidence les performances de notre modèle mesurée de différentes manières : à l'aide de la perplexité, de la Pointwise Mutual Information Normalisée, qui capture la cohérence entre les thèmes et la mesure Micro F1 measure utilisée en classification de texte.
Dans cette thèse, nous proposons une nouvelle approche de l'apprentissage profond pour la classification des flux de données de grande dimension.
Au cours des dernières années, les réseaux de neurones sont devenus la référence dans diverses applications d'apprentissage automatique.
Cependant, la plupart des méthodes basées sur les réseaux de neurones sont conçues pour résoudre des problèmes d'apprentissage statique.
Effectuer un apprentissage profond en ligne est une tâche difficile.
La principale difficulté est que les classificateurs basés sur les réseaux de neurones reposent généralement sur l'hypothèse que la séquence des lots de données utilisées pendant l'entraînement est stationnaire ; ou en d'autres termes, que la distribution des classes de données est la même pour tous les lots (hypothèse i.i.d.).
Lorsque cette hypothèse ne tient pas les réseaux de neurones ont tendance à oublier les concepts temporairement indisponibles dans le flux.
Dans la littérature scientifique, ce phénomène est généralement appelé oubli catastrophique.
Les approches que nous proposons ont comme objectif de garantir la nature i.i.d. de chaque lot qui provient du flux et de compenser l'absence de données historiques.
Pour ce faire, nous entrainons des modèles génératifs et pseudo-génératifs capable de produire des échantillons synthétiques à partir des classes absentes ou mal représentées dans le flux, et complètent les lots du flux avec ces échantillons.
Nous testons nos approches dans un scénario d'apprentissage incrémental et dans un type spécifique de l'apprentissage continu.
Nos approches effectuent une classification sur des flux de données dynamiques avec une précision proche des résultats obtenus dans la configuration de classification statique où toutes les données sont disponibles pour la durée de l'apprentissage.
En outre, nous démontrons la capacité de nos méthodes à s'adapter à des classes de données invisibles et à de nouvelles instances de catégories de données déjà connues, tout en évitant d'oublier les connaissances précédemment acquises.
Les Véhicules Autonomes (VA) sont des systèmes émergents et considérés comme une pierre angulaire de la mobilité du futur.
Leur conception est à l'origine de nombreux efforts de recherche universitaires et industrielles.
L'industrialisation des VAs est un moyen pour les acteurs de la mobilité de renforcer le positionnement futur.
Les VAs fonctionnent en interagissant avec leur contexte opérationnel (CO) et doivent être adaptés à celui-ci.
Ce travail de recherche vise à soutenir les activités d'architecture des Véhicules Autonomes pour aboutir à des architectures adaptées à leurs contextes opérationnels.
Une ontologie du CO pour Véhicules Autonomes est proposée pour soutenir l'identification et la définition de scénarios dans la phase initiale de conception, suivant une approche de conception basée sur les scénarios.
En utilisant cette ontologie, une méthode de conception de l'architecture logique des VAs basée sur l'OC est proposée.
La prise en compte du CO dans les activités de conception d'architecture des VAs est renforcée par une deuxième méthode visant à évaluer l'impact du changement du CO sur l'architecture durant la phase de conception.
Les contributions proposées sont validées par des études de cas industriels sur la conception d'architectures AV tenant en compte du CO et de son évolution.
L'établissement de la similarité entre séries temporelles est au cœur de nombreuses tâches d'analyse de données.
Les mesures permettant d'établir des similitudes entre les séries temporelles sont spécifiques en ce sens qu'elles doivent pouvoir prendre en compte les différences entre les valeurs constituant la série, ainsi que les distorsions selon l'axe du temps.
La mesure de similarité la plus répandue est la mesure Dynamic Time Warping (DTW).
Cependant, son calcul est coûteux et son application à des séries temporelles nombreuses et/ou très longues est difficile en pratique.
Il s'agit de plonger les séries temporelles dans un espace euclidien construit de telle manière que les distances entre les séries selon la métrique DTW s'y trouvent préservées.
Ce manuscrit apporte des contributions majeures : (1) il explique comment les shapelets préservant la DTW peuvent être utilisées dans le contexte spécifique de la recherche de séries temporelles similaires ; (2) il propose des stratégies de sélection de ces shapelets pour faire face à l'échelle, c'est-à-dire pour traiter une collection extrêmement vaste de séries temporelles ; (3) il explique en détail comment gérer les séries temporelles univariées et multivariées, couvrant ainsi tout le spectre des problèmes de recherches et facilitant la moise au point d'applications très diverses.
Le coeur de la contribution présentée dans ce manuscrit permet de compenser facilement la complexité du processus de plongement par un jeu sur la précision de la recherche.
Des expérimentations utilisant les jeux de données UCR et UEA démontrent l'amélioration considérable des performances par rapport aux techniques de pointe.
Les systèmes intégrant des traitements venant du traitement automatique des langues reposent souvent sur des lexiques et des grammaires, parfois indirectement sur des corpus.
A cause de la quantité et de la complexité des informations qu'elles contiennent, ces ressources linguistiques deviennent facilement une source d'incohérence.
Dans cette thèse, nous explorons les moyens d'améliorer la gestion des nombreuses ressources linguistiques d'un moteur de recherche industriel en dix-neuf langues qui fait appel à une analyse textuelle élaborée.
Nous proposons une méthode pour formaliser l'architecture linguistique des traitements linguistiques et des ressources utilisées par ceux-ci.
Cette formalisation explicite la façon dont les connaissances contenues dans les ressources sont exploitées.
Grâce à elle, nous pouvons construire des outils de gestion qui respectent l'architecture du système.
L'environnement ainsi mis en place se concentre sur la mise à jour et l'acquisition des ressources linguistiques, leur exploitation étant figée par des contraintes industrielles.
Nous proposons une contribution empirique aux tentatives récentes d'unification des sciences cognitives et des sciences sociales.
La Théorie de l'Attraction Culturelle (CAT) propose de s'atteler à des questions interdisciplinaires en utilisant une ontologie commune faite de représentations.
D'après la CAT, malgré des transformations au niveau micro, la distribution globale des représentations peut rester stable grâce à des attracteurs culturels.
Nous présentons deux études de cas sur de courts énoncés écrits.
La première examine les changements que des citations subissent lorsqu'elles sont copiées en ligne.
En combinant psycholinguistique et fouille de données, nous montrons que les substitutions de mots sont cohérentes avec l'hypothèse des attracteurs culturels, et avec les effets connus de variables lexicales.
La deuxième étude étend ces résultats, et utilise une expérience web permettant de récolter des chaînes de transmission de qualité et en grande quantité.
En étendant un algorithme bioinformatique, nous décomposons les transformations en des opérations plus simples, et proposons un premier modèle descriptif du processus qui relie les connaissances psycholinguistiques sur la transformation de phrases aux tendances de haut niveau identifiées dans la littérature sur l'évolution culturelle.
Enfin, nous montrons que la compréhension de l'évolution de telles représentations nécessite une théorie du sens des énoncés, une tâche pour laquelle nous explorons les approches empiriques possibles.
De nombreuses applications de flux de données ont vu le jour au cours des dernières années.
Lorsque l'environnement évolue, il est nécessaire de s'appuyer sur un apprentissage en ligne pouvant s'adapter aux conditions changeantes, alias dérives de concept.
L'adaptation aux dérives de concept implique d'oublier une partie ou la totalité des connaissances acquises lorsque le concept change, tout en accumulant des connaissances sur le concept sous-jacent supposé stationnaire.
Les méthodes d'ensemble ont été parmi les approches les plus réussies.
Cependant, la gestion de l'ensemble qui détermine les informations à oublier n'a pas été complètement étudiée jusqu'ici.
Notre travail montre l'importance de la stratégie de l'oubli en comparant plusieurs approches.
Les résultats ainsi obtenus nous amènent à proposer une nouvelle méthode d'ensemble avec une stratégie d'oubli conçue pour s'adapter aux dérives de concept.
Des évaluations empiriques montrent que notre méthode se compare favorablement aux systèmes adaptatifs de l'état de l'art.
Dans ce travail, nous allons plus loin en introduisant un mécanisme d'anticipation capable de détecter des états pertinents de l'environnement, de reconnaître les contextes récurrents et d'anticiper les changements de concept susceptibles.
Par conséquent, la méthode que nous proposons traite à la fois le défi d'optimiser le dilemme stabilité-plasticité, l'anticipation et la reconnaissance des futurs concepts.
Ceci est accompli grâce à une méthode d'ensemble qui contrôle un comité d'apprenants.
Cette thèse étudie l'intégration de repères phonétiques dans la reconnaissance automatique de la parole (RAP) continue à grand vocabulaire.
Les repères sont des événements à temps discret indiquant la présence d'événements phonétiques dans le signal de parole.
Le but est de développer des détecteurs de repères qui sont motivés par la connaissance phonétique afin de modéliser quelques événements phonétiques plus précisément.
La thèse présente deux approches de détection de repères, qui utilisent l'information extraite par segments et étudie deux méthodes différentes pour intégrer les repères dans le décodage, qui sont un élagage basé sur les repères et une approche reposant sur les combinaisons pondérées.
Alors que les deux approches de détection de repères présentées améliorent les performance de reconnaissance de la parole comparées à l'approche de référence, elles ne surpassent pas les prédictions phonétiques standards par trame.
Ces résultats indiquant que la RAP guidée par des repères nécessite de l'information phonétique très hétérogène pour être efficace, la thèse présente une troisième méthode d'intégration conçue pour intégrer un nombre arbitraire de flux de repères hétérogènes et asynchrones dans la RAP.
Les résultats indiquent que cette méthode est en effet en mesure d'améliorer le système de référence, pourvu que les repères fournissent de l'information complémentaire aux modèles acoustiques standards.
Les industriels et les opérateurs des flottes de systèmes cyber-physiques (CPS) sont soumis à de fortes exigences exprimées en termes de disponibilité, fiabilité des produits et des services fournis lors de l'exploitation de ces flottes dans des environnements dynamiques.
Ces attentes incitent les industriels, et notamment dans le secteur du transport, à développer des mécanismes efficaces de planification réactive des opérations de maintenance au niveau de la flotte.
Dans cette thèse, un système multi-agent (SMA) pour la planification réactive de la maintenance d'une flotte de CPS est proposé.
Ce SMA est construit en utilisant la méthode de conception ANEMONA et a pour objectif d'optimiser la planification de la maintenance au niveau flotte afin de répondre aux exigences spécifiées.
Les expériences réalisées au cours de ces travaux démontrent la capacité de ce SMA à planifier la maintenance de la flotte de manière efficace (c'est-à-dire satisfaire les exigences de disponibilité et de fiabilité de la flotte dans un environnement statique) et de manière réactive (c'est-à-dire être capable d'adapter/de modifier les décisions de planification de la maintenance à la suite des perturbations).
L'efficacité de ce modèle SMA est validée par un modèle mathématique et sa réactivité est testée par simulation de perturbations.
Une application dans le domaine ferroviaire au sein de Bombardier Transport France est proposée.
Le SMA est intégré à un système d'aide à la décision dénommé « MainFleet » .
Le développement de MainFleet est en cours.
Ce travail porte sur la segmentation jointe d'un ensemble d'images dans un cadre bayésien.
Le modèle proposé combine le processus de Dirichlet hiérarchique (HDP) et le champ de Potts.
Ainsi, pour un groupe d'images, chacune est divisée en régions homogènes et les régions similaires entre images sont regroupées en classes.
D'une part, grâce au HDP, il n'est pas nécessaire de définir a priori le nombre de régions par image et le nombre de classes, communes ou non.
D'autre part, le champ de Potts assure une homogénéité spatiale.
Les lois a priori et a posteriori en découlant sont complexes rendant impossible le calcul analytique d'estimateurs.
Un algorithme de Gibbs est alors proposé pour générer des échantillons de la loi a posteriori.
De plus,un algorithme de Swendsen-Wang généralisé est développé pour une meilleure exploration dela loi a posteriori.
Enfin, un algorithme de Monte Carlo séquentiel a été défini pour l'estimation des hyperparamètres du modèle.
Le choix de la meilleure partition se fait par minimisation d'un critère indépendant de la numérotation.
Ces méthodes ont été évaluées sur des images-test et sur des images naturelles.
Les performances de l'algorithme sont évaluées via des métriques connues en statistiques mais peu utilisées en segmentation d'image.
Comment les mots sont-ils reconnus ?
Comment avons-nous accès à la signification des mots ?
Ces questions ont été explorées dans des études sur l'accès lexical et la reconnaissance des mots durant le demi-siècle dernier dans les domaines de la psycho-, neuro- et de la linguistique.
Le traitement morphologique est un niveau essentiel de traitement pour l'extraction d'information lors de la reconnaissance de mots.
A un extrême, les modèles de pleine-entrée proposent le stockage du mot entier dans la mémoire et un traitement morphologique post-lexical paradigmatiques ;
à l'autre extrême, les modèles décompositionnels proposent une décomposition pré-lexicale et une activation morphologique basée sur des règles ;
entre les deux, les modèles à double-mécanismes postulent deux voies pour la reconnaissance des mots, une route associative avec les mots entiers et une route combinatoire basée sur des règles.
Dans la présente thèse, le traitement morphologique des verbes fléchis en français a été étudié en modalité visuelle dans cinq études.
L'étude 1 a recherché à mettre à jour l'organisation du lexique mental en utilisant les effets de fréquences de surface et les effets de fréquences cumulée ;
l'étude 2 a exploré l'impact des différents processus de formation du radical verbal ;
l'étude 3 a étudié les opérations morphologiques au travers des suffixes flexionnels ;
l'étude 4 a testé le traitement morphologique verbal pour des locuteurs de français comme L2 ;
et l'étude 5 a exploré les violations morphologiques verbales via des mesures électro-encéphalographiques.
Globalement les résultats suggèrent que tous les verbes français fléchis sont traités par un mécanisme unique avec décomposition morphologique pré-lexicale pour l'accès lexical et la reconnaissance des mots.
Il est proposé un traitement différent pour les morphèmes lexicaux et fonctionnels.
Les mots sont décomposés en morphèmes atomiques, les représentations morphologiques sont activées dans le lexique mental, et les constituants de mots sont recombinés pour la vérification de mot
La maîtrise de la qualité de la production est un objectif particulièrement important pour la croissance des industries.
Contrôler la qualité d'un produit nécessite de la mesurer.
Le contrôle de cent pourcent des produits est un objectif important pour dépasser les limites du contrôle par prélèvement, dans le cas de défauts liés à des causes exceptionnelles.
Cependant, les contraintes industrielles ont limité le déploiement de la mesure des caractéristiques des produits directement au sein des lignes de production.
Le déploiement du contrôle visuel humain est limité par sa durée incompatible avec la durée du cycle des productions à haute cadence, par son coût et par sa variabilité.
L'intégration de systèmes de vision informatique présente un coût qui les réservent aux productions à hautes valeurs ajoutées.
De plus, le contrôle automatique de la qualité de l'aspect des produits reste une thématique de recherche ouverte.
Notre travail a pour objectifs de répondre à ces contraintes, dans le cadre du procédé d'injection
Nous proposons un système de contrôle qui est non invasif pour le procédé de production.
Les pièces sont contrôlées dès la sortie de la presse à injecter.
Nous étudierons l'apport de l'imagerie non-conventionnelle.
La thermographie d'une pièce moulée chaude permet d'obtenir une information sur sa géométrie, qui est complémentaire de l'imagerie conventionnelle.
De plus, les cahiers des charges des produits présentent de plus en plus d'exigences tant sur les géométries complexes que sur l'aspect.
Cependant, les caractéristiques d'aspect sont difficiles à formaliser.
Pour automatiser le contrôle d'aspect, il est nécessaire de modéliser la notion de qualité d'une pièce.
Afin d'exploiter les mesures réalisées sur les pièces chaudes, notre approche utilise des méthodes d'apprentissage statistique.
Ainsi, l'expert humain qui connait la notion de qualité d'une pièce transmet son savoir au système, par l'annotation d'un jeu de données d'apprentissage.
Notre système de contrôle apprend alors une métrique de la qualité d'une pièce, à partir des données brutes issues capteurs.
Nous avons privilégier une approche par réseaux de convolution profonds (textit{Deep Learning}) afin d'obtenir les meilleurs performances en justesse de discrimination des pièces conformes.
La faible quantité d'échantillons annotés disponible dans notre contexte industrielle nous ont amenée à utiliser des méthodes d'apprentissage par transfert de domaine.
Enfin, afin de répondre à l'ensemble des contraintes, nous avons réalisé l'intégration verticale d'une prototype de dispositif de mesure des pièces et de la solution logicielle de traitement par apprentissage statistique.
Le dispositif intègre l'imagerie thermique, polarimétrique, l'éclairage et le système de traitement embarqué nécessaire à l'envoi des données sur un serveur d'analyse distant.
Deux cas d'applications permettent d'évaluer les performances et la viabilité de la solution proposée
Cette thèse comprend trois essais dont l'objectif est d'étudier les déterminants et les implications de la qualité de l'information narrative des entreprises cotées.
Afin d'appréhender la qualité de cette information nous recourons à des techniques de traitement automatique de langage naturel qui permettent de construire des indices de lisibilité des rapports annuels.
Le premier essai étudie l'effet de la complexité textuelle des rapports annuels sur la liquidité des titres.
L'utilisation d'un échantillon d'entreprises françaises cotées en bourse sur la période 2002-2013 montre l'existence d'une relation positive entre le degré de lisibilité des rapports annuels et la liquidité des titres.
Ces résultats suggèrent que la complexité textuelle de l'information narrative affecte les investisseurs sur le marché des actions.
Le deuxième essai étudie l'effet de la lisibilité des rapports annuels sur le coût des fonds propres des entreprises.
Le troisième essai examine l'effet des pratiques de réduction d'impôt des entreprises sur la lisibilité de leurs divulgations financières.
La littérature mobilisant la théorie d'agence montre que ces pratiques de réduction d'impôt créent un cadre permettant aux dirigeants d'extraire des bénéfices privés aux dépens des autres parties prenantes.
En utilisant un échantillon d'entreprises américaines cotées en bourse pour la période 1995-2012, nous constatons que les entreprises qui s'engagent dans des politiques de diminution d'impôt publient des rapports annuels moins lisibles et plus ambigus.
En s'appuyant sur un corpus constitué de six-cents articles de recherche et comptabilisant plus de cinq-millions de mots, l'objectif est double.
Il s'agit d'abord de comprendre les mécanismes et procédés par lesquels les SHS se construisent leur appareillage terminologique pour ensuite dégager les profils combinatoires des termes ainsi que leur fonctionnement interactif contextuel.
La démarche s'inscrivant dans le cadre de la linguistique de corpus et celle du traitement automatique des langues naturelles (TALN), l'approche se veut donc sémasiologique, combinant socioterminologie et lexicométrie.
L'analyse du comportement de ces items en contexte permet en effet de relever les glissements sémantiques et leur relation.
Nous partons du postulat que le terme, bien que souvent présenté comme monosémique et stable dans le rapport biunivoque qu'il entretient avec le concept qu'il dénomme, semble être sujet à des variations dénominatives et est, de ce fait, révélateur de stratégies d'énonciation qui se manifestent dans sa lexicogenèse.
La disponibilité de grandes masses de textes sous forme informatique rend possible aujourd'hui la mise en œuvre de méthodes qui construisent des représentations du sens des mots par exploration automatique exhaustive de leur usage dans ces textes.
Ces représentations sont au cœur de nombreuses applications de traitement automatique des langues, de l'extraction d'information à la traduction automatique en passant par la recherche de réponses à des questions.
Cependant, dans les domaines spécialisés, deux facteurs rendent ces méthodes plus difficilement applicables.
D'une part, les corpus spécialisés à un domaine sont nécessairement de taille moins grande que les corpus non restreints à un domaine, alors que la taille ces corpus employés est un facteur clé dans la qualité des représentations construites.
D'autre part, les termes complexes (formés de plusieurs mots) ont une importance particulière dans les domaines spécialisés, alors que les méthodes standard sont conçues pour traiter des termes simples (formés d'un mot).
Le projet ADDICTE vise à surmonter ces difficultés en donnant une meilleure représentation des termes complexes, en enrichissant l'analyse des textes d'un domaine par des ressources additionnelles (terminologies du domaine et textes hors domaine), et en mettant au point des représentations qui peuvent tirer parti de ces informations supplémentaires.
Dans ce contexte, la thèse proposée porte spécifiquement sur la conception et le test de méthodes visant à faire bénéficier de ressources externes la construction de représentations distributionnelles.
Cette thèse s'intègre dans le cadre du traitement automatique du langage naturel.
La problématique du résumé automatique de documents arabes qui a été abordée, dans cette thèse, s'est cristallisée autour de deux points.
Le premier point concerne les critères utilisés pour décider du contenu essentiel à extraire.
Le deuxième point se focalise sur les moyens qui permettent d'exprimer le contenu essentiel extrait sous la forme d'un texte ciblant les besoins potentiels d'un utilisateur.
Afin de montrer la faisabilité de notre approche, nous avons développé le système "L.A.E", basé sur une approche hybride qui combine une analyse symbolique avec un traitement numérique.
Les résultats d'évaluation de ce système sont encourageants et prouvent la performance de l'approche hybride proposée.
Ces résultats, ont montré, en premier lieu, l'applicabilité de l'approche dans le contexte de documents sans restriction quant à leur thème (Éducation, Sport, Science, Politique, Reportage, etc.), leur contenu et leur volume.
Ils ont aussi montré l'importance de l'apprentissage dans la phase de classement et sélection des phrases forment l'extrait final.
Cette thèse s'inscrit dans la problématique de l'extraction de sens à partir de textes et flux textuels, produits dans notre cas lors de processus collaboratifs.
Plus précisément, nous nous intéressons aux courriels de travail et aux documents textuels objets de collaboration, avec une première application aux documents éducatifs.
La motivation de cet intérêt est d'aider les utilisateurs à accéder plus rapidement aux informations utiles ; nous cherchons donc à les repérer dans les textes.
Ainsi, nous nous intéressons aux tâches dans les courriels, et aux fragments de documents éducatifs qui concernent les thèmes de leurs intérêts.
Deux corpus, un de courriels et un de documents éducatifs, principalement en français, ont été constitués.
Cela était indispensable, car il n'y a pratiquement pas de travaux antérieurs sur ce type de données en français.
Notre première contribution théorique est une modélisation générique de la structure de ces données.
Nous l'utilisons pour spécifier le traitement formel des documents, prérequis au traitement sémantique.
Nous démontrons la difficulté du problème de segmentation, normalisation et structuration de documents en différents formats source, et présentons l'outil SEGNORM, première contribution logicielle de cette thèse.
SEGNORM segmente et normalise les documents (en texte brut ou balisé), récursivement et en unités de taille paramétrable.
Dans le cas des courriels, il segmente les messages contenant des messages cités en messages individuels, en conservant l'information du chaînage entre les fragments entremêlés.
Il analyse également les métadonnées des messages pour reconstruire les fils de discussions, et retrouve dans les citations les messages dont on ne possède pas le fichier source.
Nous abordons ensuite le traitement sémantique de ces documents.
Nous proposons une modélisation (ontologique) de la notion de tâche, puis décrivons l'annotation d'un corpus de plusieurs centaines de messages issus du contexte professionnel de VISEO et du GETALP.
Nous présentons alors la deuxième contribution logicielle de cette thèse, un outil de repérage de tâches et d'extraction de leurs attributs (contraintes temporelles, assignataires, etc.).
Cet outil, basé sur une combinaison d'une approche experte et d'apprentissage automatique, est évalué selon des critères classiques de précision, rappel et F-mesure, ainsi que selon la qualité d'usage.
Enfin, nous présentons nos travaux sur la plate-forme MACAU-CHAMILO, troisième contribution logicielle, qui aide à l'apprentissage par (1) structuration de documents pédagogiques selon deux ontologies (forme et contenu), (2) accès multilingue à du contenu initialement monolingue.
Il s'agit donc de nouveau de structuration selon les deux axes, forme et sens.
(1) L'ontologie des formes permet d'annoter les fragments des documents par des concepts comme théorème, preuve, exemple, par des niveaux de difficulté et d'abstraction, et par des relations comme élaboration_de, illustration_de.
L'ontologie de domaine modélise les objets formels de l'informatique, et plus précisément les notions de complexité calculatoire.
Cela permet de suggérer aux utilisateurs des fragments utiles pour la compréhension de notions d'informatique perçues comme abstraites ou difficiles.
(2) L'aspect relatif à l'accès multilingue a été motivé par le constat que nos universités accueillent un grand nombre d'étudiants étrangers, qui ont souvent du mal à comprendre nos cours à cause de la barrière linguistique.
Nous avons proposé une approche pour multilingualiser du contenu pédagogique avec l'aide d'étudiants étrangers, par post-édition en ligne de pré-traductions automatiques, puis, si besoin, amélioration incrémentale de ces post-éditions.
(Nos expériences ont montré que des versions multilingues de documents peuvent être produites rapidement et sans coût.)
Ce travail a abouti à un corpus de plus de 500 pages standard (250 mots/page) de contenu pédagogique post-édité vers le chinois.
Nous présentons, dans ce manuscrit, un dispositif informatique d'aide à la formation des futurs enseignants de FLE en Colombie.
Il prend ses sources dans la linguistique textuelle et cherche à améliorer le niveau linguistique des étudiants universitaires actuellement en formation.
Pour ce faire, le dispositif est fondé sur un corpus textuel spécifiquement annoté et étiqueté grâce aux outils de traitement automatique de langues (TAL) et à des annotations manuelles en format XML.
Ceci permet de développer des activités à visée formative, en tenant compte des besoins exprimés par les publics cibles (enseignants-formateurs et leurs étudiants en formation).
Comme nous l'exposons tout au long de cette thèse, l'élaboration d'un système comme le nôtre est le produit de la mise en œuvre de connaissances et de compétences issues de plusieurs disciplines et/ou domaines  : didactique des langues, ingénierie pédagogique, linguistique générale, linguistique textuelle, linguistique de corpus, TAL et ALAO.
Il se veut, principalement, un dispositif pédagogique pour la formation des étudiants en FLE dans le contexte de l'éducation supérieure en Colombie, un outil pensé en fonction des besoins et des objectifs de cet apprentissage.
L'originalité de notre système repose sur le type de public choisi, le modèle didactique de formation mis en œuvre et la spécificité du corpus utilisé.
À notre connaissance, il s'agit d'un des premiers systèmes d'ALAO fondé sur la linguistique textuelle s'adressant à la formation des futurs enseignants de FLE dans un contexte exolingue.
Internet propose aujourd'hui aux utilisateurs de services en ligne de commenter, d'éditer et de partager leurs points de vue sur différents sujets de discussion.
Ce type de contenu est maintenant devenu la ressource principale pour les analyses d'opinions sur Internet.
Néanmoins, à cause des abréviations, du bruit, des fautes d'orthographe et toutes autres sortes de problèmes, les outils de traitements automatiques des langues, y compris les reconnaisseurs d'entités nommées et les étiqueteurs automatiques morphosyntaxiques, ont des performances plus faibles que sur les textes bien-formés (Ritter et al., 2011).
Cette thèse a pour objet la reconnaissance d'entités nommées sur les contenus générés par les utilisateurs sur Internet.
Nous avons établi un corpus d'évaluation avec des textes multi-sources et multi-domaines.
Ensuite, nous avons développé un modèle de champs conditionnels aléatoires, entrainé sur un corpus annoté provenant des contenus générés par les utilisateurs.
Dans le but d'améliorer les résultats de la reconnaissance d'entités nommées, nous avons d'abord développé un étiqueteur morpho-syntaxique sur les contenus générés par les utilisateurs et nous avons utilisé les étiquettesprédites comme un attribut du modèle des champs conditionnels aléatoire.
Enfin, pour transformer les contenus générés par les utilisateurs en textes bien-formés, nous avons développé un modèle de normalisation lexicale basé sur des réseaux de neurones pour proposer une forme correcte pour les mots non-standard.
L'accès à l'information textuelle est devenu une composante essentielle de nombreux systèmes d'information.
Le paradigme de la lecture automatique a été récemment proposé.
Son objectif est de formaliser le processus de lecture comme un apprentissage supervisé de 'bout-en-bout'où l'objectif d'un modèle est de répondre à une question sur un texte donné.
Actuellement, les modèles proposés nécessitent une grande quantité de données et il a été montré qu'ils se révèlent particulièrement sensibles au bruit.
L'objectif de ce projet est d'étudier comment un processus d'apprentissage adversarial permettrais d'appréhender ces limitations.
Différentes composantes des systèmes de traduction automatique statistique sont considérées comme des problèmes d'optimisations.
En effet, l'apprentissage du modèle de traduction, le décodage et l'optimisation des poids de la fonction log-linéaire sont trois importants problèmes d'optimisation.
Savoir définir les bons algorithmes pour les résoudre est l'une des tâches les plus importantes afin de mettre en place un système de traduction performant.
Plusieurs algorithmes d'optimisation sont proposés pour traiter les problèmes d'optimisation du décodeur.
Ils sont combinés pour résoudre, d'une part, le problème de décodage qui produit une traduction dans la langue cible d'une phrase source, d'autre part, le problème d'optimisation des poids des scores combinés dans la fonction log-linéaire pour d'évaluation des hypothèses de traduction au cours du décodage.
Le système de traduction statistique de référence est basé sur un algorithme de recherche en faisceau pour le décodage, et un algorithme de recherche linéaire pour l'optimisation des poids associés aux scores.
Nous proposons un nouveau système de traduction avec un décodeur entièrement basé sur les algorithmes génétiques.
Les algorithmes génétiques sont des algorithmes d'optimisation bio-inspirés qui simulent le processus de l'évolution naturelle des espèces.
Ils permettent de manipuler un ensemble de solutions à travers plusieurs itérations pour converger vers des solutions optimales.
Ce travail, nous permet d'étudier l'efficacité des algorithmes génétiques pour la traduction automatique statistique.
Ce travail, nous a permis de proposer un nouveau système de traduction automatique statistique ayant un décodeur entièrement basé sur des algorithmes génétiques
La sécurité de l'information repose sur la bonne interaction entre différents niveaux d'abstraction : les composants matériels, systèmes d'exploitation, algorithmes, et réseaux de communication.
Cependant, protéger ces éléments a un coût ; ainsi de nombreux appareils sont laissés sans bonne couverture.
Cette thèse s'intéresse à ces différents aspects, du point de vue de la sécurité et de la cryptographie.
Nous décrivons ainsi de nouveaux algorithmes cryptographiques (tels que des raffinements du chiffrement de Naccache–Stern), de nouveaux protocoles (dont un algorithme d'identification distribuée à divulgation nulle de connaissance), des algorithmes améliorés (dont un nouveau code correcteur et un algorithme efficace de multiplication d'entiers),ainsi que plusieurs contributions à visée systémique relevant de la sécurité de l'information et à l'intrusion.
En outre, plusieurs de ces contributions s'attachent à l'amélioration des performances des constructions existantes ou introduites dans cette thèse.
Elle consiste à identifier certains objets textuels tels que les noms de personne, d' organisation et de lieu.
Le travail de cette thèse se concentre sur la tâche de reconnaissance des entités nommées pour la modalité orale.
Dans un premier temps, nous étudions les spécificités de la reconnaissance des entités nommées en aval du système de reconnaissance automatique de la parole.
Nous présentons une méthode pour la reconnaissance des entités nommées dans les transcription de la parole en adoptant une taxonomie hiérarchique et compositionnelle.
Nous mesurons l'impact des différents phénomènes spécifiques à la parole sur la qualité de reconnaissance des entités nommées.
Dans un second temps, nous proposons d'étudier le couplage étroit entre la tâche de transcription de la parole et la tâche de reconnaissance des entités nommées.
Dans ce but, nous détournons les fonctionnalités de base d'un système de transcription de la parole pour le transformer en un système de reconnaissance des entités nommées.
Ainsi, en mobilisant les connaissances propres au traitement de la parole dans le cadre de la tâche liée à la reconnaissance des entités nommées, nous assurons une plus grande synergie entre ces deux tâches.
Nous menons différents types d'expérimentations afin d'optimiser et d'évaluer notre approche.
Dans cette thèse, nous avons utilisé les données de l'IRM du conduit vocal pour étudier la production de la parole.
La première partie consiste en l'étude de l'impact que le vélum, l'épiglotte et la position de la tête a sur la phonation de cinq voyelles françaises.
Des simulations acoustiques ont été utilisées pour comparer les formants des cas étudiés avec la référence afin de mesurer leur impact.
Par conséquent, la deuxième partie présente quelques algorithmes que l'on peut utiliser pour améliorer les données de production de la parole.
Plusieurs transformations d'images ont été combinées afin de générer des estimations des formes du conduit vocal qui sont plus informatives que les originales.
À ce stade, nous avons envisagé, outre l'amélioration des données de production de la parole, de créer un modèle de référence générique qui pourrait fournir des informations améliorées non pas pour un sujet spécifique, mais globalement pour la parole.
Enfin, la dernière partie de la thèse, fait référence à une sélection de questions ouvertes du domaine qui restent encore sans réponse, quelques pistes intéressantes que l'on peut développer à partir de cette thèse et quelques approches potentielles qui pourraient être envisager afin de répondre à ces questions.
Le but de cette these consiste en l'extraction d'un echantillon de textes pour les modeles markoviens.
Pour cela, nous avons utilise la stratification.
Nous avons developpe un logiciel "echantillonnage multivarie" qui extrait un echantillon d'un corpus stratifie categorise et ambigu, minimisant une "perte d'information" selon un certain sens. Les resultats obtenus sont tres satisfaisants puisque nous avons ameliore le nombre de levees d'ambiguites morphologiques.
Cette probabilite n'est pas unique quelque soit la topologie consideree.
Grace aux isometries de la norme li, nous avons demontre qu'il est impossible d'obtenir une solution unique a ce probleme.
L'obtention d'une solution unique contraint les representation du "vrai" et du "faux" a se confondre.
Il faut egalement distinguer le "vrai" associe a une formule logique d'un "evenement certain" de la theorie des probabilites.
Finalement, nous avons propose un nouveau modele de markov capable de tenir compte du contexte associe a une categorie morphologique.
Nous avons travaillé sur un formalisme appelé grammaires d'arbres adjoints (tag) qui appartient a la famille des grammaires d'unification mais avec la particularité d'être basé sur des structures arborescente.
L'intérêt théorique est double : d'un point de vue linguistique, les tag élargissent la notion de localité et permettent de redéfinir les interactions multiples entre lexique, syntaxe et sémantique ;
nous avons ajouté une contrainte de lexicalisation qui les rend a même de formaliser les hypothèses qui sous-tendent les lexiques-grammaires de M. GROSS et peuvent contribuer à les préciser.
D'un point de vue informatique, les tags sont un peu plus puissants que les grammaires hors contexte mais gardent des propriétés informatiques intéressantes (temps de traitement, décidabilité).
En particulier, nous remettons en cause la notion de groupe verbal.
Nous comparons à chaque fois les représentations retenues en tag avec celles utilisées dans d'autres formalismes (gpsg, lfg, grammaires en chaine).
Nous présentons enfin la stratégie d'analyse spécifique aux grammaires lexicalisées et l'état actuel de l'implémentation de notre analyseur.
La majorité des systèmes embarqués récents sont basées sur des architectures massivement parallèles MPSoC, d'où la nécessité de développer des applications parallèles embarquées.
La conception et le développement d'une application parallèle embarquée devient de plus en plus difficile notamment pour les architectures multiprocesseurs hétérogènes ayant différents types de contraintes de communication et de conception tels que le coût du matériel, la puissance et la rapidité.
Cela est particulièrement important pour les systèmes embarqués de type MPSoC, où les applications doivent fonctionner correctement sur de nombreux cœurs.
En outre, la performance d'une application ne s'améliore pas forcément lorsque l'application tourne sur un nombre de cœurs encore plus grand.
La performance d'une application peut être limitée en raison de multiples goulot d'étranglement notamment la contention sur des ressources partagées telles que les caches et la mémoire.
Cela devient contraignant etune perte de temps pour un développeur de faire un profilage de l'application parallèle embarquée et d'identifier des goulots d'étranglement dans le code source qui diminuent la performance de l'application.
Pour surmonter ces problèmes, dans cette thèse, nous proposons trois méthodes automatiques qui détectent les instructions du code source qui ont conduit à une diminution de performance due à la contention et à l'évolutivité des processeurs sur une puce.
Les méthodes sont basées sur des techniques de fouille de données exploitant des gigaoctets de traces d'exécution de bas niveau produites par les platesformes MPSoC.
Nos approches de profilage permettent de quantifier et de localiser automatiquement les goulots d'étranglement dans le code source afin d'aider les développeurs à optimiserleurs applications parallèles embarquées.
Nous avons effectué plusieurs expériences sur plusieurs applications parallèles embarquées.
Nos expériences montrent la précision des techniques proposées, en quantifiant et localisant avec précision les hotspots dans le code source.
La thèse porte sur le développement d'architectures neuronales profondes permettant d'analyser des contenus textuels ou visuels, ou la combinaison des deux.
Réseaux récurrents pour la compréhension de la parole : différentes architectures de réseaux sont comparées pour cette tâche sur leurs facultés à modéliser les observations ainsi que les dépendances sur les étiquettes à prédire.
L'approche été étudiée principalement en structuration de collections de vidéos, dons le cadre d'évaluations internationales où l'approche proposée s'est imposée comme l'état de l'art.
Les réseaux sociaux et les communautés en ligne sont devenus des acteurs majeurs dans le domaine du tourisme.
Les informations échangées sur ces communautés influencent de plus en plus le comportement du consommateur.
Dans le même temps, le nombre des utilisateurs des téléphones mobiles est aussi en pleine croissance.
Les mobiles proposent à leurs utilisateurs différents services.
Les réseaux sociaux associés à la technologie mobile pourront ainsi constituer une source d'information pour le touriste et être ainsi considérés comme un facteur clé qui influence son comportement durant son voyage.
Notre propos serait ainsi d'étudier le comportement du touriste dans le cadre de l'utilisation d'une application mobile qui se base sur le partage des avis et des retours d'
Ce travail a pour objet l'adaptation des toponymes thaïlandais en français dans un corpus de quatre guides touristiques francophones.
Les analyses linguistiques et traductologiques montrent que les toponymes thaïlandais sont bien intégrés en français aux différents niveaux de leur adaptation.
Ils sont d'abord romanisés par divers systèmes, parfois avec la francisation graphématique.
Au niveau sémantico-référentiel, leur valeur fondamentale est locative mais dans certains contextes, ils peuvent subir une interprétation métonymique et métaphorique.
Ainsi le transfert sémantique est possible par les divers procédés traductologiques.
Avec la traduction libre, l'auteur peut modifier la traduction de la dénomination d'origine ou créer une nouvelle forme dénominative en présentant la caractérisation dominante du référent.
Ces stratégies soulignent une fonction pragmatique spécifique du guide touristique : permettre au lecteur d'identifier des lieux qui lui sont inconnus en suscitant son intérêt pour une langue-culture étrangère.
Cette recherche vise à comprendre comment le bouche-à-oreille électronique visuel émis par un youtubeur influence le sentiment et l'intention d'achat des consommateurs.
Nous étudions l'impact des caractéristiques du bouche-à-oreille et l'effet de la contagion sociale.
Les résultats principaux sont : 1.
Le statut professionnel du youtubeur, l'existence de liens commerciaux entre marques et youtubeurs et la valence de la revue ont un impact sur le sentiment exprimé et par conséquent l'intention d'achat.2.
Il existe une contagion sociale, à la fois émotionnelle et comportementale au sein de l'audience d'un bouche-à-oreille électronique et visuel qui impacte son efficacité.3.
Le nombre d'abonnés d'un individu (caractéristique individuelle du commentateur) impacte sa sensibilité à la contagion sociale.
Le volume de données disponible croît de manière très importante et ouvre d'importants défis pour les exploiter.
Les domaines scientifiques du Web sémantique et des ontologies sont alors une réponse pour aider à traiter les données de manière efficace.
Le domaine que nous considérons dans nos travaux est celui des Interventions
Pour ce faire, il est essentiel de disposer d'une classification évolutive et consensuelle effectuée au niveau international pour les spécialistes.
Dans ce domaine, le développement d'une ontologie est crucial pour faciliter les recherches bibliographiques et mettre en place des bonnes pratiques.
La construction manuelle de l'ontologie est en effet fastidieuse et longue.
En particulier, la collecte des termes liés au domaine des INM nécessite beaucoup d'efforts et de temps tant le champ du vocabulaire est large.
Ainsi le terme INM lui-même est parfois remplacé par d'autres (médecines alternatives, médecines douces, etc).
Ainsi, un outil de visualisation doit être proposé pour les experts des INM.
Des contributions sont proposées dans cette thèse sur ces deux sujets (construction du vocabulaire et visualisation).
Deux approches sont présentées pour la construction, l'une reposant sur la connaissance experte et l'autre sur un corpus.
Une mesure de similarité est introduite et évaluée.
Pour la visualisation, notre proposition repose sur l'utilisation de cartes conceptuelles.
Un outil a été implémenté, permettant de transformer les ontologies décrites en OWL pour les visualiser.
En prenant en compte le contexte industriel, nous avons centré notre recherche sur certains problèmes, informatiques et lexicographiques.
Maurel, D., 2006]. Nous montrons aussi comment l'étendre pour répondre à de nouveaux besoins, comme ceux du projet INNOVALANGUES.
Enfin, nous avons créé un "intergiciel de lemmatisation", LEXTOH, qui permet d'appeler plusieurs analyseurs morphologiques ou lemmatiseurs, puis de fusionner et filtrer leurs résultats.
Combiné à un nouvel outil de création de dictionnaires, CREATDICO, LEXTOH permet de construire à la volée un "mini-dictionnaire" correspondant à une phrase ou à un paragraphe d'un texte en cours de "post-édition" en ligne sous IMAG/SECTRA, ce qui réalise la fonctionnalité d'aide lexicale proactive prévue dans [Huynh, C.-P., 2010]. On pourra aussi l'utiliser pour créer des corpus parallèles "factorisés" pour construire des systèmes de TA en MOSES.
Grâce aux progrès impressionnants qui ont été réalisés dans la transcription du langage parlé, il est de plus en plus possible d'exploiter les données transcrites pour des tâches qui requièrent la compréhension de ce que l'on dit dans une conversation.
Le travail présenté dans cette thèse, réalisé dans le cadre d'un projet consacré au développement d'un assistant de réunion, contribue aux efforts en cours pour apprendre aux machines à comprendre les dialogues des réunions multipartites.
Nous nous sommes concentrés sur le défi de générer automatiquement les résumés abstractifs de réunion.
Nous présentons tout d'abord nos résultats sur le Résumé Abstractif de Réunion (RAR), qui consiste à prendre une transcription de réunion comme entrée et à produire un résumé abstractif comme sortie.
Nous introduisons une approche entièrement non-supervisée pour cette tâche, basée sur la compression multi-phrases et la maximisation sous-modulaire budgétisée.
Nous tirons également parti des progrès récents en vecteurs de mots et dégénérescence de graphes appliqués au TAL, afin de prendre en compte les connaissances sémantiques extérieures et de concevoir de nouvelles mesures de diversité et d'informativité.
Ensuite, nous discutons de notre travail sur la Classification en Actes de Dialogue (CAD), dont le but est d'attribuer à chaque énoncé d'un discours une étiquette qui représente son intention communicative.
La CAD produit des annotations qui sont utiles pour une grande variété de tâches, y compris le RAR.
Nous proposons une couche neuronale modifiée de Champ Aléatoire Conditionnel (CAC) qui prend en compte non seulement la séquence des énoncés dans un discours, mais aussi les informations sur les locuteurs et en particulier, s'il y a eu un changement de locuteur d'un énoncé à l'autre.
Nous proposons une nouvelle approche de la DCA dans laquelle nous introduisons d'abord un encodeur neuronal contextuel d'énoncé qui comporte trois types de mécanismes d'auto-attention, puis nous l'entraînons en utilisant les méta-architectures siamoise et triplette basées sur l'énergie.
Nous proposons en outre une méthode d'échantillonnage générale qui permet à l'architecture triplette de capturer des motifs subtils (p. ex., des groupes qui se chevauchent et s'emboîtent).
La thèse a pour objectif d'étudier la causalité à partir de son expression discursive dans des textes français.
Cette étude linguistique est effectuée dans la perspective du traitement automatique des langues.
Ce travail s'intègre dans un projet de filtrage sémantique des textes orienté vers la production de synthèses et de résumes (projet SAFIR : système automatique de filtrage d'informations pour le résumé).
Mais sa portée s'étend à l'acquisition des connaissances à partir de textes.
Notre premier objectif consiste à répertorier les différents procédés linguistiques employés par les auteurs pour communiquer des relations causales.
Nous utilisons une méthode informatique originale d'exploration contextuelle qui ne se base pas sur une "représentation profonde" du texte, mais sur une identification automatique de marqueurs considérés comme étant pertinents.
Nous proposons une carte de 1500 marqueurs (verbes, locutions, adverbes...) qui sont des indices automatiquement identifiables des relations causales prises en charge par l'énonciateur ou par des tiers.
Nous adoptons le formalisme des classes d'objets pour décrire la métonymie en français.
L'objectif de cette description est double : rendre compte des régularités du fonctionnement de ce mécanisme, et constituer des bases lexicales dédiées au traitement automatique des langues.
Il est postulé que le cadre minimal d'analyse des unités lexicales est la phrase simple définie en termes de prédicats et d'arguments.
Partant de ce principe, la métonymie est analysée comme un mécanisme sémantique signalé par un transfert de prédicats entre des classes de mots corrélés à des prédicats appropriés.
Quand il porte sur des noms élémentaires, le transfert opère une recatégorisation sémantique et engendre de nouveaux emplois.
La catégorisation des mots en arguments et prédicats nous permet de proposer une classification tripartite des métonymies.
Les métonymies de type métonymique et celles qui sont de type argumental mettent en relation des noms élémentaires, tandis que les métonymies de type prédicatif opèrent une recatégorisation structurelle.
Dans ce dernier cas, nous distinguons d'abord une double actualisation (prédicative et argumentale) des unités lexicales polysémiques, avant de faire état d'un transfert de prédicats qui rend compte de la recatégorisation structurelle.
Transfert et actualisation sont les deux principes organisateurs qui mettent en évidence le caractère systématique de la métonymie.
Le principal objectif de cette thèse vise à estimer de manière automatique la qualité de la traduction de langue parlée (Spoken Language Translation ou SLT), appelée estimation de confiance (Confidence Estimation ou CE).
En raison de multiples facteurs, la sortie de SLT, ayant une qualité insatisfaisante, pourrait causer différents problèmes pour les utilisateurs finaux.
Par conséquent, il est utile de savoir combien de confiance les tokens corrects pourraient être trouvés au sein de l'hypothèse.
Dans le cadre de cette thèse, nous avons proposé un boîte à outils, qui consiste en un framework personnalisable, flexible et en une plate-forme portative, pour l'estimation de confiance au niveau de mots (Word-level Confidence Estimation ou WCE) de SLT.
Cette tâche, relativement nouvelle, est définie et formalisée comme un problème d'étiquetage séquentiel dans lequel chaque mot, dans l'hypothèse de SLT, est annoté comme bon ou mauvais selon un ensemble des traits importants.
Nous proposons plusieurs outils servant d'estimer la confiance des mots (WCE) en fonction de notre évaluation automatique de la qualité de la transcription (ASR), de la qualité de la traduction (MT), ou des deux (combiner ASR et MT).
Ce travail de recherche est réalisable parce que nous avons construit un corpus spécifique, qui contient 6.7k des énoncés pour lesquels un quintuplet est normalisé comme suit : (1) sortie d'ASR, (2) transcription en verbatim, (3) traduction textuelle, (4) traduction vocale et (5) post-édition de la traduction.
La conclusion de nos multiples expérimentations, utilisant les traits conjoints entre ASR et MT pour WCE, est que les traits de MT demeurent les plus influents, tandis que les traits de ASR peuvent apporter des informations intéressantes complémentaires.
En deuxième lieu, nous proposons deux méthodes pour distinguer des erreurs susceptibles d'ASR et de celles de MT, dans lesquelles chaque mot, dans l'hypothèse de SLT, est annoté comme good (bon), asr_error (concernant les erreurs d'ASR) ou mt_error (concernant les erreurs de MT).
Nous contribuons donc à l'estimation de confiance au niveau de mots (WCE) pour SLT par trouver la source des erreurs au sein des systèmes de SLT.
En troisième lieu, nous proposons une nouvelle métrique, intitulée Word Error Rate with Embeddings (WER-E), qui est exploitée afin de rendre cette tâche possible.
Cette approche génère de meilleures hypothèses de SLT lors de l'optimisation de l'hypothèse de N-meilleure hypothèses avec WER-E.
En somme, nos stratégies proposées pour l'estimation de la confiance se révèlent un impact positif sur plusieurs applications pour SLT.
Les outils robustes d'estimation de la qualité pour SLT peuvent être utilisés dans le but de re-calculer des graphes de la traduction de parole ou dans le but de fournir des retours d'information aux utilisateurs dans la traduction vocale interactive ou des scénarios de parole aux textes assistés par ordinateur.
Mots-clés : Estimation de la qualité, Estimation de confiance au niveau de mots (WCE), Traduction de langue parlée (SLT), traits joints, Sélection des traits.
Les publications présentées couvrent un champ relativement vaste de la linguistique appliquée, depuis la didactique des langues étrangères et l'allemand enseigné en tant que langue étrangère jusqu'à la traduction automatique, en passant par des questions de syntaxe d'ordre général concernant la linguistique computationnelle dans son ensemble et les problèmes concernant les systèmes d'information (indexation automatique).
Dans ce travail, il y a une minutieuse réévaluation des systèmes existants et une élaboration d'une toute nouvelle approche concernant les problèmes fondamentaux.
Dans une série d'articles rédigés en Portugais brésilien, il est question de l'analyse automatique de textes portugais.
Cette œuvre contient plusieurs aspects : une formalisation détaillée de la morphologie et de la syntaxe, les problèmes d'application concernant l'indexation automatique, les systèmes d'information pour le parlement et la traduction automatique.
Depuis 1985, la traduction automatique occupe une place prépondérante.
Certains problèmes spéciaux ont été analysés avec des exemples concrets et finalement des systèmes concrets ont été mis à l'épreuve.
Cela a abouti à des conclusions qui vont fortement influencer le développement ultérieur de la partie "software" en EUROTRA.
L'événement est un concept central dans plusieurs tâches du Traitement Automatique des Langues, en dépit de l'absence d'une définition unifiée de ce que recouvre cette notion.
La création de ces schémas requiert une connaissance experte et est donc longue, coûteuse et difficile à étendre à un large ensemble de domaines de spécialité.
En parallèle de ces travaux, la quantité de données produites par les individus et les organisations a crû de manière exponentielle, ouvrant des perspectives applicatives inédites.
Pour ce faire, nous suivons une approche ascendante divisée en trois grandes étapes.
Dans la première étape, nous groupons ensemble les nombreuses mentions textuelles relatant la même réalisation d'un événement, identifiée dans le temps et l'espace et appelée instance.
La deuxième étape vise à s'abstraire des caractéristiques spatio-temporelles de chaque instance pour les grouper en grands types d'événements.
Enfin, la dernière étape de cette contribution vise à extraire les éléments caractéristiques de chaque type d'événement induit afin d'en proposer une représentation synthétique assimilable à un schéma d'événement.
La plateforme Weblab est un environnement de définition et d'exécution de chaines de traitements média
Avant le début de cette thèse, aucun outil n'existait pour l'analyse et l'amélioration de la qualité de workflows WebLab.
La problématique principale de la thèse repose sur le fonctionnement dit boite noire des services WebLab.
L'approche choisie est non-intrusive : nous complétons la définition du workflow WebLab par des règles de provenance et de propagation de qualité.
Les règles de provenance génèrent des liens de dépendance dit grains-fins entre les données et les services après l'exécution d'une chaine de traitements WebLab.
Les règles de propagation de qualité profitent des liens inférés précédemment pour raisonner sur l'influence de la qualité d'une donnée utilisée par un service sur la qualité d'une donnée produite...
Ce travail de thèse a pour objectif de proposer plusieurs méthodes d'identification non-supervisées des personnes présentes dans les flux télévisés à l'aide des noms écrits à l'écran.
Comme l'utilisation de modèles biométriques pour reconnaître les personnes présentes dans de larges collections de vidéos est une solution peu viable sans connaissance a priori des personnes à identifier, plusieurs méthodes de l'état de l'art proposent d'employer d'autres sources d'informations pour obtenir le nom des personnes présentes.
Ces méthodes utilisent principalement les noms prononcés comme source de noms.
Cependant, on ne peut avoir qu'une faible confiance dans cette source en raison des erreurs de transcription ou de détection des noms et aussi à cause de la difficulté de savoir à qui fait référence un nom prononcé.
Les noms écrits à l'écran dans les émissions de télévision ont été peu utilisés en raison de la difficulté à extraire ces noms dans des vidéos de mauvaise qualité.
Toutefois, ces dernières années ont vu l'amélioration de la qualité des vidéos et de l'incrustation des textes à l'écran.
Nous avons donc ré-évalué, dans cette thèse, l'utilisation de cette source de noms.
Nous avons d'abord développé LOOV (pour Lig Overlaid OCR in Vidéo), un outil d'extraction des textes sur-imprimés à l'image dans les vidéos.
Nous obtenons avec cet outil un taux d'erreur en caractères très faible.
Ce qui nous permet d'avoir une confiance importante dans cette source de noms.
Nous avons ensuite comparé les noms écrits et les noms prononcés dans leurs capacités à fournir le nom des personnes présentes dans les émissions de télévisions.
Il en est ressorti que deux fois plus de personnes sont nommables par les noms écrits que par les noms prononcés extraits automatiquement.
Un autre point important à noter est que l'association entre un nom et une personne est intrinsèquement plus simple pour les noms écrits que pour les noms prononcés.
Cette très bonne source de noms nous a donc permis de développer plusieurs méthodes de nommage non-supervisé des personnes présentes dans les émissions de télévision.
Nous avons commencé par des méthodes de nommage tardives où les noms sont propagés sur des clusters de locuteurs.
Ces méthodes remettent plus ou moins en cause les choix fait lors du processus de regroupement des tours de parole en clusters de locuteurs.
Nous avons ensuite proposé deux méthodes (le nommage intégré et le nommage précoce) qui intègrent de plus en plus l'information issue des noms écrits pendant le processus de regroupement.
Pour identifier les personnes visibles, nous avons adapté la méthode de nommage précoce pour des clusters de visages.
Enfin, nous avons aussi montré que cette méthode fonctionne aussi pour nommer des clusters multi-modaux voix-visage.
Avec cette dernière méthode, qui nomme au cours d'un unique processus les tours de paroles et les visages, nous obtenons des résultats comparables aux meilleurs systèmes ayant concouru durant la première campagne d'évaluation REPERE
La métabolomique permet une étude à large échelle du profil métabolique d'un individu, représentatif de son état physiologique.
La comparaison de ces profils conduit à l'identification de métabolites caractéristiques d'une condition donnée.
La métabolomique présente un potentiel considérable pour le diagnostic, mais également pour la compréhension des mécanismes associés aux maladies et l'identification de cibles thérapeutiques.
Cependant, ces dernières applications nécessitent d'inclure ces métabolites caractéristiques dans un contexte plus large, décrivant l'ensemble des connaissances relatives au métabolisme, afin de formuler des hypothèses sur les mécanismes impliqués.
Cette mise en contexte peut être réalisée à l'aide des réseaux métaboliques, qui modélisent l'ensemble des transformations biochimiques opérables par un organisme.
L'une des limites de cette approche est que la métabolomique ne permet pas à ce jour de mesurer l'ensemble des métabolites, et ainsi d'offrir une vue complète du métabolome.
Les travaux présentés dans cette thèse proposent une méthode pour pallier ces limitations, en suggérant des métabolites pertinents pouvant aider à la reconstruction de scénarios mécanistiques.
Cette méthode est inspirée des systèmes de recommandations utilisés dans le cadre d'activités en ligne, notamment la suggestion d'individus d'intérêt sur les réseaux sociaux numériques.
La méthode a été appliquée à la signature métabolique de patients atteints d'encéphalopathie hépatique.
Elle a permis de mettre en avant des métabolites pertinents dont le lien avec la maladie est appuyé par la littérature scientifique, et a conduit à une meilleure compréhension des mécanismes sous-jacents et à la proposition de scénarios alternatifs.
Cette thèse de Traitement Automatique des Langues a pour objectif l'annotation automatique en rôles sémantiques du français en domaine spécifique.
Cette tâche désambiguïse le sens des prédicats d'un texte et annote les syntagmes liés avec des rôles sémantiques tels qu'Agent, Patient ou Destination.
Elle aide de nombreuses applications dans les domaines où des corpus annotés existent, mais est difficile à utiliser quand ce n'est pas le cas.
Nous avons d'abord évalué sur le corpus FrameNet une méthode existante d'annotation basée uniquement sur VerbNet et donc indépendante du domaine considéré.
Pour utiliser cette méthode en français, nous traduisons deux ressources lexicales anglaises.
Nous commençons par la base de données lexicales WordNet.
Nous traduisons ensuite le lexique VerbNet dans lequel les verbes sont regroupés sémantiquement grâce à leurs traits syntaxiques.
La traduction, VerbNet, a été obtenue en réutilisant deux lexiques verbaux du français (le Lexique-Grammaire et Les Verbes Français) puis en modifiant manuellement l'ensemble des informations obtenues.
Enfin, une fois ces briques en place, nous évaluons la faisabilité de l'annotation en rôles sémantiques en anglais et en français dans trois domaines spécifiques.
Nous évaluons quels sont les avantages et inconvénients de se baser sur VerbNet et VerbuNet pour annoter ces domaines, avant d'indiquer nos perspectives pour poursuivre ces travaux.
La recherche de réponses précises à des questions formulées en langue naturelle renouvelle le champ de la recherche d'information.
De nombreux travaux ont eu lieu sur la recherche de réponses à des questions factuelles en domaine ouvert.
Moins de travaux ont porté sur la recherche de réponses en domaine de spécialité, en particulier dans le domaine médical ou biomédical.
Plusieurs conditions différentes sont rencontrées en domaine de spécialité comme les lexiques et terminologies spécialisés, les types particuliers de questions, entités et relations du domaine ou les caractéristiques des documents ciblés.
Dans une première partie, nous étudions les méthodes permettant d'analyser sémantiquement les questions posées par l'utilisateur ainsi que les textes utilisés pour trouver les réponses.
Dans une seconde partie, nous étudions l'apport des technologies du web sémantique pour la portabilité et l'expressivité des systèmes de questions-réponses.
Enfin, nous présentons notre système de questions-réponses, appelé MEANS, qui utilise à la fois des techniques de TAL, des connaissances du domaine et les technologies du web sémantique pour répondre automatiquement aux questions médicales.
Les flux de contenus audiovisuels peuvent être représentés sous forme de séquences d'événements (par exemple, des suites d'émissions, de scènes, etc.).
Dans le contexte d'une chaîne TV, la programmation des émissions suit une cohérence définie par cette même chaîne, mais peut également être influencée par les programmations des chaînes concurrentes.
Dans de telles conditions,les séquences d'événements des flux parallèles pourraient ainsi fournir des connaissances supplémentaires sur les événements d'un flux considéré.
La modélisation de séquences est un sujet classique qui a été largement étudié, notamment dans le domaine de l'apprentissage automatique.
Les réseaux de neurones récurrents de type Long Short-Term Memory (LSTM) ont notamment fait leur preuve dans de nombreuses applications incluant le traitement de ce type de données.
Néanmoins,ces approches sont conçues pour traiter uniquement une seule séquence d'entrée à la fois.
Notre contribution dans le cadre de cette thèse consiste à élaborer des approches capables d'intégrer conjointement des données séquentielles provenant de plusieurs flux parallèles.
Le contexte applicatif de ce travail de thèse, réalisé en collaboration avec le Laboratoire Informatique d'Avignon et l'entreprise EDD, consiste en une tâche de prédiction du genre d'une émission télévisée.
Cette prédiction peut s'appuyer sur les historiques de genres des émissions précédentes de la même chaîne mais également sur les historiques appartenant à des chaînes parallèles.
Nous proposons une taxonomie de genres adaptée à de tels traitements automatiques ainsi qu'un corpus de données contenant les historiques parallèles pour 4 chaînes françaises.
Deux méthodes originales sont proposées dans ce manuscrit, permettant d'intégrer les séquences des flux parallèles.
La première, à savoir, l'architecture des LSTM parallèles(PLSTM) consiste en une extension du modèle LSTM.
Les PLSTM traitent simultanément chaque séquence dans une couche récurrente indépendante et somment les sorties de chacune de ces couches pour produire la sortie finale.
Pour ce qui est de la seconde proposition, dénommée MSE-SVM, elle permet de tirer profit des avantages des méthodes LSTM et SVM.
D'abord, des vecteurs de caractéristiques latentes sont générés indépendamment, pour chaque flux en entrée, en prenant en sortie l'événement à prédire dans le flux principal.
Ces nouvelles représentations sont ensuite fusionnées et données en entrée à un algorithme SVM.
Les approches PLSTM et MSE-SVM ont prouvé leur efficacité dans l'intégration des séquences parallèles en surpassant respectivement les modèles LSTM et SVM prenant uniquement en compte les séquences du flux principal.
Les deux approches proposées parviennent bien à tirer profit des informations contenues dans les longues séquences.
En revanche, elles ont des difficultés à traiter des séquences courtes.
Cependant, le problème rencontré avec les séquences courtes est plus prononcé pour le cas de l'approche MSE-SVM.
Nous proposons enfin d'étendre cette approche en permettant d'intégrer des informations supplémentaires sur les événements des séquences en entrée (par exemple, le jour de la semaine des émissions de l'historique).
Cette extension, dénommée AMSE-SVM améliore remarquablement la performance pour les séquences courtes sans les baisser lorsque des séquences longues sont présentées.
Dans cette thèse, nous nous intéressons particulièrement aux données issues de l'imagerie par résonance magnétique fonctionnelle (IRMf), que nous étudions dans un cadre d'apprentissage statistique.
Tout d'abord, nous considérons les données d'IRMf de repos, que nous traitons grâce à des méthodes de factorisation de matrices.
Notre méthode principale introduit une réduction aléatoire de la dimension des données dans une boucle d'apprentissage en ligne.
L'algorithme proposé converge plus de 10 fois plus vite que les meilleures méthodes existantes, pour différentes configurations et sur plusieurs jeux de données.
Nous effectuons une vaste validation expérimentale de notre approche de sous-échantillonnage aléatoire.
Nous proposons une étude théorique des propriétés de convergence de notre algorithme.
Dans un second temps, nous nous intéressons aux données d'IRMf d'activation.
Nous démontrons comment agréger différents études acquises suivant des protocoles distincts afin d'apprendre des modèles joints de décodage plus justes et interprétables.
Cela suscite un transfert d'information entre les études.
En conséquence, notre modèle multi-étude est plus performant que les modèles de décodage appris sur chaque étude séparément.
Notre approche identifie une représentation universellement pertinente de l'activité cérébrale, supportée par un petit nombre de réseaux optimisés pour l'identification de tâches.
Les langages contrôlés sont des langages artificiellement définis utilisant un sous-ensemble du vocabulaire, des formes morphologiques, des constructions syntaxiques d'une langue naturelle tout en en éliminant la polysémie.
En quelque sorte, ils constituent le pont entre les langages formels et les langues naturelles.
De ce fait, ils remplissent la fonction de communication du médium texte tout en étant rigoureux et analysables par la machine sans ambiguïté.
En particulier, ils peuvent être utilisés pour faciliter l'alimentation de bases de connaissances, dans le cadre d'une interface homme-machine.
Le Service Hydrographique et Océanographique de la Marine (SHOM) publie depuis 1971 les Instructions nautiques, des recueils de renseignements généraux, nautiques et réglementaires, destinés aux navigateurs.
Ces ouvrages complètent les cartes marines.
D'autre part, l'Organisation Hydrographique Internationale (OHI) a publié des normes spécifiant l'échange de données liées à la navigation et notamment un modèle universel de données hydrographiques (norme S-100, janvier 2010).
Cette thèse se propose d'étudier l'utilisation d'un langage contrôlé pour représenter des connaissances contenues dans les Instructions nautiques, dans le but de servir de pivot entre la rédaction du texte par l'opérateur dédié, la production de l'ouvrage imprimé ou en ligne, et l'interaction avec des bases de connaissances et des outils d'aide à la navigation.
En particulier on étudiera l'interaction entre le langage contrôlé des Instructions nautiques et les cartes électroniques correspondantes.
Plus généralement, cette thèse se pose la question de l'évolution d'un langage contrôlé et des ontologies sous-jacentes dans le cadre d'une application comme les Instructions nautiques, qui ont la particularité d'avoir des aspects rigides (données numériques, cartes électroniques, législation) et des aspects nécessitant une certaine flexibilité (rédaction du texte par des opérateurs humains, imprévisibilité du type de connaissance à inclure par l'évolution des usages et des besoins des navigants).
De manière similaire aux ontologies dynamiques que l'on rencontre dans certains domaines de connaissance, on définit ici un langage contrôlé dynamique.
Bien que créé pour le domaine de la navigation maritime, les mécanismes du langage contrôlé présentés dans cette thèse ont le potentiel pour être adaptés à d'autres domaines utilisant des corpus multimodaux.
Enfin, les perspectives d'évolution pour un langage contrôlé hybride sont importantes puisqu'elles peuvent exploiter les différents avantages des modes en présence (par exemple, une exploitation de l'aspect visuel pour une extension 3D).
Les techniques d'apprentissage semi-supervisé basées sur des graphes (G-SSL) permettent d'exploiter des données étiquetées et non étiquetées pour construire de meilleurs classifiers.
Malgré de nombreuses réussites, leur performances peuvent encore être améliorées, en particulier dans des situations ou` les graphes ont une faible séparabilité de classes ou quand le nombres de sujets supervisés par l'expert est déséquilibrés.
Pour aborder ces limitations on introduit une nouvelle méthode pour G-SSL, appelee Lγ - PageRank, qui constitue la principal contribution de cette these.
Il s'agit d'une g´en´eralisation de l'algorithme PageRank ´a partir de l'utilisation de puissances positives γ de la matrice Laplacienne du graphe.
L'étude théorique de Lγ - PageRank montre que (i) pour γ &lt;1, cela correspond à une extension de l'algorithme PageRank aux processus de vol de Lévy : où les marcheurs aléatoires peuvent désormais relier, en un seul saut, des nœuds distants du graphe ; et (ii) pour γ &gt; 1, la classification est effectué sur des graphes signés : où les noeuds appartenant à une même classe ont plus de chances de partager des liens positifs, tandis que les noeuds de classes différentes ont plus de chances d'être connectés avec des arêtes négatifs.
Nous montrons l'existence d'une puissance optimale γ qui maximise la performance de classification, pour laquelle une méthode d'estimation automatique est conçue et évaluée.
Des expériences sur plusieurs jeux de données montrent que les marcheurs aléatoires de vols de Lévy peuvent améliorer la détection des classes ayant des structures locales complexes, tandis que les graphes signés permet d'améliorer considérablement la séparabilité des données et de surpasser le problème des données étiquetées non équilibrées.
Dans un second temps, nous étudions des implémentations efficaces de Lγ - PageRank.
Nous proposons des extensions de Power Iteration et Gauss-Southwell pour Lγ - PageRank, qui sont des algorithmes initialement conçues pour calculer efficacement la solution de la méthode PageRank standard.
Ensuite, les versions dynamiques de ces algorithmes sont également étendues à Lγ - PageRank, permettant de mettre `a jour la solution de Lγ - PageRank en complexité sub-linéaire lorsque le graphe évolue ou que de nouvelles données arrivent.
Pour terminer, nous appliquons Lγ - PageRank dans le contexte du routage Internet.
Nous abordons le problème de l'identification des systèmes autonomes (AS) pour des arêtes inter-AS `a partir du réseau d'adresses IP et des registres publics des AS.
Des expériences sur des mesures traceroute d'Internet montrent que Lγ -PageRank peut résoudre cette tâche sans erreurs, même lorsqu'il n'y a pas d'exemples étiquetés par l'expert pour la totalité des classes.
Le but est qu'ils s'expriment en langage naturel et puissent dialoguer avec des interlocuteurs humains.
Pour développer un ACA, il faut d'abord comprendre que des aspects tels que personnalité, les émotions et leur apparence sont extrêmement importants.
Le travail qui est présenté dans cette thèse a pour objectif d'augmenter l'acceptabilité et la crédibilité des agents au moyen de la personnalité, considérée comme une notion centrale à l'interaction ACA-humain.
On propose un modèle qui dote l'ACA de facettes de personnalité et de buts de communication « cachés » et qui module ainsi ses actions conversationnelles.
L'intégration et l'analyse des différentes bases de données liées à la même question de recherche, par exemple la corrélation entre phénotypes et génotypes, sont essentielles pour découvrir de nouvelles connaissances.
Elles fournissent un vocabulaire commun pour les humains, et des définitions d'entités formelles pour les machines.
Un grand nombre d'ontologies et de terminologies biomédicales a été développé pour représenter et annoter les différentes bases de données existantes.
Cependant, celles qui sont représentées avec différentes ontologies qui se chevauchent, c'est à dire qui ont des parties communes, ne sont pas interopérables.
Il est donc crucial d'établir des correspondances entre les différentes ontologies utilisées, ce qui est un domaine de recherche actif connu sous le nom d'alignement d'ontologies.
Les premières méthodes d'alignement d'ontologies exploitaient principalement le contenu lexical et structurel des ontologies à aligner.
Ces méthodes sont moins efficaces lorsque les ontologies à aligner sont fortement hétérogènes lexicalement, c'est à dire lorsque des concepts équivalents sont décrits avec des labels différents.
Pour pallier à ce problème, la communauté d'alignement d'ontologies s'est tournée vers l'utilisation de ressources de connaissance externes en tant que pont sémantique entre les ontologies à aligner.
Cette approche soulève plusieurs nouvelles questions de recherche, notamment : (1) la sélection des ressources de connaissance à utiliser, (2) l'exploitation des ressources sélectionnées pour améliorer le résultat d'alignement.
Plusieurs travaux de recherche ont traité ces problèmes conjointement ou séparément.
Dans notre thèse, nous avons fait une revue systématique et une comparaison des méthodes proposées dans la littérature.
Les ontologies, autres que celles à aligner, sont les ressources de connaissance externes (Background Knowledge : BK) les plus utilisées.
Les travaux apparentés sélectionnent souvent un ensemble d'ontologies complètes en tant que BK même si, seuls des fragments des ontologies sélectionnées sont réellement efficaces pour découvrir de nouvelles correspondances.
Nous proposons une nouvelle approche qui sélectionne et construit une ressource de connaissance à partir d'un ensemble d'ontologies.
Afin de faire face à ce problème, nous proposons deux méthodes pour sélectionner les correspondances les plus pertinentes parmi les candidates qui se basent sur : (1) un ensemble de règles et (2) l'apprentissage automatique supervisé.
Nous avons expérimenté et évalué notre approche dans le domaine biomédical, grâce à la profusion de ressources de connaissances en biomédecine (ontologies, terminologies et alignements existants).
Nous avons effectué des expériences intensives sur deux benchmarks de référence de la campagne d'évaluation de l'alignement d'ontologie (OAEI).
La rénovation des bâtiments de logements est aujourd'hui un axe majeur de la lutte pour la réduction des consommations énergétiques et, plus généralement, du réchauffement climatique.
Cependant, si la situation semble simple sur le plan technique, il existe encore de nombreux freins (économiques, sociaux, culturels) qui ralentissent la rénovation du parc existant.
Ces freins nécessitent d'aborder une rénovation sous un angle global, grâce notamment au travail d'un facilitateur, capable d'établir une relation de confiance avec les habitants.
Cette approche globale est encore plus importante lorsqu'il s'agit de rénover les copropriétés.
En effet, dans ces ensembles immobiliers, la prise de décision est rendue complexe par le nombre d'acteurs et leur organisation juridique spécifique.
Pour permettre aux copropriétés de voter des travaux de rénovation, un travail d'accompagnement long et couteux est nécessaire.
Pour permettre de rendre ce travail d'accompagnement le plus efficace possible, nous avons conçu un outil d'aide à la décision, utilisable par le facilitateur, qui permet de réaliser un diagnostic pluridisciplinaire de la copropriété et de préconiser des solutions d'accompagnements sur mesure.
Nous avons notamment défini une liste de critères influents sur la décision d'une copropriété d'entreprendre des travaux de rénovations, et nous les avons compilés dans un outil d'évaluation.
La copropriété est ainsi étudiée selon toutes ses caractéristiques (qualité d'usage, possibilités techniques, potentiel économique, profil sociologiques des propriétaires, qualité du quartier, état des dynamiques collectives) et des prescriptions spécifiques sont faites en fonction de l'évaluation de chaque critère.
Si les premiers résultats sont encourageants, de nombreuses perspectives d'améliorations s'offrent encore à nous pour massifier l'utilisation de cet outil et rendre le diagnostic encore plus précis.
De nombreuses méthodes existent pour résoudre des problèmes d'optimisation multicritère, et il n'est pas aisé de choisir une méthode suffisamment adaptée à un problème multicritère donné.
En effet, après le choix d'une méthode multicritère, différents paramètres (e.g. poids, fonctions d'utilité, etc.) doivent être déterminés, soit pour trouver la solution optimale (meilleur compromis) ou pour classer l'ensemble des solutions faisables (alternatives).
Justement, vue cette difficulté pour fixer les paramètres, les méthodes d'élicitation sont utilisées pour aider le décideur dans cette tâche de fixation des paramètres.
Par ailleurs, nous supposons que nous disposons d'un ensemble de solutions plausibles, et nous faisons aussi l'hypothèse de la disponibilité au préalable, des informations préférentielles obtenues après une interaction avec le décideur.
Dans la première contribution de ce travail, nous tirons profit d'une mesure statistique simple et rapidement calculable, à savoir, le coefficient de corrélation ρ de Spearman, afin de développer une approche gloutonne (approchée), et deux approches exactes basées sur la programmation par contraintes (PPC) et la programmation linéaire en nombres entiers (PLNE).
Ces méthodes sont ensuite utilisées pour éliciter automatiquement les paramètres appropriés de la méthode multicritère basée sur l'ordre lexicographique.
Nous proposons aussi des modèles d'élicitation des paramètres d'autres méthodes multicritère, telles que la méthode MinLeximax issue de la théorie du choix social et du partage équitable, la méthode de la somme pondérée et les opérateurs OWA.
Caractériser la qualité d'une odeur est une tâche complexe qui consiste à identifier un ensemble de descripteurs qui synthétise au mieux la sensation olfactive au cours de séances d'analyse sensorielle.
Généralement, cette caractérisation est une liste de descripteurs extraite d'un vocabulaire imposé par les industriels d'un domaine pour leurs analyses sensorielles.
Ces analyses représentent un coût significatif pour les industriels chaque année.
En effet, ces approches dites orientées reposent sur l'apprentissage de vocabulaires, limitent singulièrement les descripteurs pour un public non initié et nécessitent de couteuses phases d'apprentissage.
Si cette caractérisation devait être confiée à des évaluateurs naïfs, le nombre de participants pourrait être significativement augmenté tout en réduisant le cout des analyses sensorielles.
Malheureusement, chaque description libre n'est alors plus associée à un ensemble de descripteurs non ambigus, mais à un simple sac de mots en langage naturel (LN).
Deux problématiques sont alors rattachées à la caractérisation d'odeurs.
Ainsi, la première partie de notre travail se focalise sur la définition et l'évaluation de modèles qui peuvent être utilisés pour résumer un ensemble de mots en un ensemble de descripteurs désambiguïsés.
Parmi les différentes stratégies envisagées dans cette contribution, nous proposons de comparer des approches hybrides exploitant à la fois des bases de connaissances et des plongements lexicaux définis à partir de grands corpus de textes.
Nos résultats illustrent le bénéfice substantiel à utiliser conjointement représentation symbolique et plongement lexical.
Nous définissons ensuite de manière formelle le processus de synthèse d'un ensemble de concepts et nous proposons un modèle qui s'apparente à une forme d'intelligence humaine pour évaluer les résumés alternatifs au regard d'un objectif de synthèse donné.
L'approche non orientée que nous proposons dans ce manuscrit apparait ainsi comme l'automatisation cognitive des tâches confiées aux opérateurs des séances d'analyse sensorielle.
Elle ouvre des perspectives intéressantes pour développer des analyses sensorielles à grande échelle sur de grands panels d'évaluateurs lorsque l'on essaie notamment de caractériser les nuisances olfactives autour d'un site industriel.
L'apprentissage profond a permis des avancées significatives dans le domaine de la traduction automatique.
La traduction automatique neuronale (NMT) s'appuie sur l'entrainement de réseaux de neurones avec un grand nombre de paramètres sur une grand quantité de données parallèles pour apprendre à traduire d'une langue à une autre.
Un facteur primordial dans le succès des systèmes NMT est la capacité de concevoir des architectures puissantes et efficaces.
À cette fin, nous introduisons Pervasive Attention, un modèle basé sur des convolutions bidimensionnelles qui encodent conjointement les séquences source et cible avec des interactions qui sont omniprésentes dans le réseau neuronal.
Pour améliorer l'efficacité des systèmes NMT, nous étudions la traduction automatique simultanée où la source est lue de manière incrémentielle et le décodeur est alimenté en contextes partiels afin que le modèle puisse alterner entre lecture et écriture.
Nous abordons également l'efficacité computationnelle des modèles NMT et affirmons qu'ajouter plus de couches à un réseau de neurones n'est pas requis pour tous les cas.
Nous concevons des décodeurs Transformer qui peuvent émettre des prédictions à tout moment dotés de mécanismes d'arrêt adaptatifs pour allouer des ressources en fonction de la complexité de l'instance.
Le traitement automatique de la parole est un domaine qui englobe un grand nombre de travaux : de la reconnaissance automatique du locuteur à la détection des entités nommées en passant par la transcription en mots du signal audio.
Toutes ces informations peuvent être exploitées par des techniques d'indexation automatique qui vont permettre d'indexer de grandes collections de documents.
Les travaux présentés dans cette thèse s'intéressent à l'indexation automatique de locuteurs dans des documents audio en français.
Plus précisément nous cherchons à identifier les différentes interventions d'un locuteur ainsi qu'à les nommer par leur prénom et leur nom.
Ce processus est connu sous le nom d'identification nommée du locuteur (INL).
La particularité de ces travaux réside dans l'utilisation conjointe du signal audio et de sa transcription en mots pour nommer les locuteurs d'un document.
Le prénom et le nom de chacun des locuteurs est extrait du document lui même (de sa transcription enrichie plus exactement), avant d'être affecté à un des locuteurs du document.
Nous commençons par rappeler le contexte et les précédents travaux réalisés sur l'INL avant de présenter Milesin, le système développé lors de cette thèse.
L'apport de ces travaux réside tout d'abord dans l'utilisation d'un détecteur automatique d'entités nommées (LIA_NE) pour extraire les couples prénom / nom de la transcription.
Ensuite, ils s'appuient sur la théorie des fonctions de croyance pour réaliser l'affectation aux locuteurs du document et prennent ainsi en compte les différents conflits qui peuvent apparaître.
Pour finir, un algorithme optimal d'affectation est proposé.
Ce système obtient un taux d'erreur compris entre 12 et 20 % sur des transcriptions de référence (réalisées manuellement) en fonction du corpus utilisé.
Nous présentons ensuite les avancées réalisées et les limites mises en avant par ces travaux.
Nous proposons notamment une première étude de l'impact de l'utilisation de transcriptions entièrement automatiques sur Milesin.
C'est pourquoi de nombreux projets d'aide aux personnes âgées : techniques, universitaires et commerciaux ont vu le jour ces dernières années.
Ce travail de thèse a été eﬀectué sous convention Cifre, conjointement entre l'entreprise KRG Corporate et le laboratoire BMBI (Biomécanique et Bio-ingénierie) de l'UTC (Université de technologie de Compiègne).
Elle a pour objet de proposer un capteur de reconnaissance de sons et des activités de la vie courante, dans le but d'étoﬀer et d'améliorer le système de télé-assistance déjà commercialisé par la société.
Plusieurs méthodes de reconnaissance de parole ou de reconnaissance du locuteur ont déjà été éprouvées dans le domaine de la reconnaissance de sons, entre autres les techniques : GMM (Modèle de mélange gaussien–Gaussian Mixture Model), SVM-GSL (Machine à vecteurs de support, GMM-super-vecteur à noyau linéaire – Support vector machine GMM Supervector Linear kernel) et HMM (Modèle de Markov caché – Hidden Markov Model).
De la même manière, nous nous sommes proposés d'utiliser les i
Puis nous avons élargi notre spectre, et utilisé l'apprentissage profond (Deep Learning) qui donne actuellement de très bon résultats en classification tous domaines confondus.
Les méthodes précédemment évoquées ont également été testées en conditions bruités puis réelles.
Au début de cette thèse, aucun corpus annoté syntaxiquement (treebank) n'était disponible pour le serbe.
Or, les treebanks annotés manuellement sont une condition sine qua non du développement (entraînement et évaluation) d'outils statistiques dédiés à l'annotation syntaxique automatique (parsers).
L'existence des parsers performants permet à son tour l'annotation syntaxique de corpus plus larges, qui peuvent ensuite alimenter des recherches en linguistique théorique.
Afin de combler cette lacune, nous avons constitué un ensemble de ressources pour le traitement automatique du serbe.
Il s'agit en premier lieu du treebank ParCoTrain-Synt, qui contient 101 000 tokens annotés en morphosyntaxe, en lemmes et en syntaxe de dépendances.
Nous avons également confectionné le lexique ParCoLex, doté de 7 millions d'entrées provenant de 157 000 lemmes différents.
En exploitant ces deux ressources, nous avons développé des modèles pour le parsing, pour l'étiquetage et pour la lemmatisation.
Toutes les ressources citées sont librement diffusées à l'adresse suivante : https : //github.com/aleksandra-miletic/serbian-nlp-resources.
Les ressources constituées ont également été exploitées dans le cadre de deux études linguistiques, montrant ainsi que le corpus ParCoTrain-Synt ouvre la porte aux études empiriques basées sur des analyses quantitatives dans le domaine de la linguistique serbe.
L'étude de l'extraction de lexiques bilingues à partir de corpus comparables a été souvent circonscrite aux mots simples.
Les méthodes classiques ne peuvent gérer les expressions complexes que si elles sont de longueur identique, tandis que les méthodes de plongements de mots modélisent les expressions comme une seule unité.
Ces dernières nécessitent beaucoup de données, et ne peuvent pas gérer les expressions hors vocabulaire.
Dans cette thèse, nous nous intéressons à la modélisation d'expressions de longueur variable par co-occurrences et par les méthodes neuronales état de l'art.
Nous étudions aussi l'apprentissage de représentation d'expressions supervisé et non-supervisé.
Nous proposons deux contributions majeures.
Notre système améliore significativement l'alignement bilingue des expressions de longueurs différentes.
À partir d'un corpus vidéo de conversation spontanée en anglais, notre travail de thèse s'attache à déterminer si plusieurs types syntaxiques de constructions subordonnées expriment le même degré d'intégration à leur environnement co-textuel, d'une perspective multimodale.
La littérature syntaxique décrit les subordonnées comme des formes dépendantes, qui spécifient ou élaborent le contenu d'une autre proposition.
En montrant que les constructions sous étude n'expriment pas une dépendance uniforme à leur environnement selon la façon dont les locuteurs utilisent les modalités prosodique et gestuelle pour exprimer plus ou moins de démarcation, les résultats en production comme en perception suggèrent d'une part que les appositives sont produites avec davantage de rupture que les autres types syntaxiques, et d'autre part que la création d'une rupture s'appuie majoritairement sur des moyens davantage prosodiques que gestuels.
Cette thèse présente la réalisation d'un système de compréhension de parole continue sur une architecture informatique modeste.
Elle concerne la compréhension de parole continue dans le cadre d'applications précises et des langages artificiels.
Le système utilise dans un premier temps, la technique de reconnaissance analytique donnant comme résultat un treillis phonétique de la phrase prononcée.
Un outil d'aide de génération de données linguistiques a été ajouté au système, qui permet à l'utilisateur de déterminer la syntaxe de son langage d » application, les mots étant introduits sous leur forme graphémique.
Les graphes sont omniprésents dans de nombreux domaines de recherche, allant de la biologie à la sociologie.
Un graphe est une structure mathématique très simple constituée d'un ensemble d'éléments, appelés nœuds, reliés entre eux par des liens, appelés arêtes.
Le partitionnement ou clustering de graphe est un problème central en analyse de graphe dont l'objectif est d'identifier des groupes de nœuds densément interconnectés et peu connectés avec le reste du graphe.
Ces groupes de nœuds, appelés clusters, sont fondamentaux pour une compréhension fine de la structure des graphes.
Il n'existe pas de définition universelle de ce qu'est un bon cluster, et différentes approches peuvent s'avérer mieux adaptées dans différentes situations.
Alors que les méthodes classiques s'attachent à trouver des partitions des nœuds de graphe, c'est-à-dire à colorer ces nœuds de manière à ce qu'un nœud donné n'ait qu'une et une seule couleur, des approches plus élaborées se révèlent nécessaires pour modéliser la structure complexe des graphes que l'on rencontre en situation réelle.
En particulier, dans de nombreux cas, il est nécessaire de considérer qu'un nœud donné peut appartenir à plus d'un cluster.
Par ailleurs, de nombreux systèmes que l'on rencontre en pratique présentent une structure multi-échelle pour laquelle il est nécessaire de partir à la recherche de hiérarchies de clusters plutôt que d'effectuer un partitionnement à plat.
De plus, les graphes que l'on rencontre en pratique évoluent souvent avec le temps et sont trop massifs pour être traités en un seul lot.
Pour ces raisons, il est souvent nécessaire d'adopter des approches dites de streaming qui traitent les arêtes au fil de l'eau.
Enfin, dans de nombreuses applications, traiter des graphes entiers n'est pas nécessaire ou est trop coûteux, et il est plus approprié de retrouver des clusters locaux dans un voisinage de nœuds d'intérêt plutôt que de colorer tous les nœuds.
Dans ce travail, nous étudions des approches alternatives de partitionnement de graphe et mettons au point de nouveaux algorithmes afin de résoudre les différents problèmes évoqués ci-dessus.
La présente thèse est dédiée à l'étude de la préposition iz dans la langue russe moderne à travers les constructions qui la contiennent et qui s'organisent en un réseau sémantique complexe.
Bien que les prépositions aient toujours été au cœur des recherches linguistiques, iz n'a cependant pas reçu de description très détaillée de sa sémantique ni de son fonctionnement même s'il existe quelques travaux qui lui sont consacrés.
Le Chapitre II présente une analyse détaillée des emplois de iz au sein d'une construction, considérée comme une unité sémantico-syntaxique.
Cette analyse détaillée permet de mettre en évidence les paramètres qui rapprochent les différents emplois de iz en établissant les liens entre ses différents emplois et en les organisant en un réseau de significations bien structuré.
Ce réseau de significations constitue le profil sémantique de iz à proprement parler.
Le Chapitre III est un chapitre synthétique qui reprend les principales idées développées dans le Chapitre II et les confronte par le biais d'une étude contrastive aux emplois des prépositions ot et s qui apparaissent dans des contextes proches et, par conséquent, y entrent en concurrence avec iz.
Cette analyse contrastive permet d'identifier les particularités propres aux emplois de chacune des trois prépositions et de cerner la manière dont elles départagent les domaines d'emplois.
En outre, une place particulière est accordée à l'analyse quantitative des combinatoires des prépositions iz, ot et s avec les verbes à préfixe.
Les résultats obtenus confirment grandement l'hypothèse de la corrélation entre les prépositions et les préfixes homonymes (ot et ot-, s et s-) ou « synonymes » (iz et vy-).
Enfin, les thèses développées tout au long du travail sont utilisées pour expliquer les cas d'emplois erronés impliquant la préposition iz dans les travaux d'apprenants de russe (francophones et anglophones principalement).
Grâce à cette dimension comparative entre les langues à structure différente, l'analyse effectuée met en lumière le fait que certaines constructions avec iz (notamment, « appartenance » , « repérage d'une entité dans un groupe d'entités » ) ont des paramètres particuliers qui ne sont pas présents dans des constructions « équivalentes » en français et en anglais.
La conclusion générale expose l'ensemble des résultats et données obtenus, tout comme l'Annexe qui regroupe les principales données quantitatives issues des recherches sur les prépositions étudiées principalement dans le Corpus National de la Langue Russe (ruscorpora.ru).
Les résultats obtenus peuvent aussi bien contribuer aux études linguistiques sur les prépositions et aux entrées lexicographiques, qu'aux recherches portant sur l'acquisition du russe par des adultes non-russophones ou encore sur la didactique du russe langue étrangère.
Dans ce manuscrit, nous étudions la diffusion d'information dans les réseaux sociaux en ligne.
Des sites comme Facebook ou Twitter sont en effet devenus aujourd'hui des media d'information à part entière, sur lesquels les utilisateurs échangent de grandes quantités de données.
La plupart des modèles existant pour expliquer ce phénomène de diffusion sont des modèles génératifs, basés sur des hypothèses fortes concernant la structure et la dynamique temporelle de la diffusion d'information.
Nous considérerons dans ce manuscrit le problème de la prédiction de diffusion dans le cas où le graphe social est inconnu, et où seules les actions des utilisateurs peuvent être observées.
- Nous proposons, dans un premier temps, une méthode d'apprentissage du modèle independent cascade consistant à ne pas prendre en compte la dimension temporelle de la diffusion.
Des résultats expérimentaux obtenus sur des données réelles montrent que cette approche permet d'obtenir un modèle plus performant et plus robuste.
- Nous proposons ensuite plusieurs méthodes de prédiction de diffusion reposant sur des technique d'apprentissage de représentations.
Celles-ci nous permettent de définir des modèles plus compacts, et plus robustes à la parcimonie des données.
- Enfin, nous terminons en appliquant une approche similaire au problème de détection de source, consistant à retrouver l'utilisateur ayant lancé une rumeur sur un réseau social.
Cette thèse en informatique présente un travail de modélisation cognitive computationnelle, à partir de données de mouvements oculaires lors de tâches de recherche d'information dans des textes.
Parce que le temps est souvent une contrainte, les textes ne sont souvent pas entièrement lus
Plus précisément, nous avons analysé les mouvements des yeux dans deux tâches de recherche d'information consistant à lire un paragraphe et à décider rapidement i) s'il est associé à un but donné et ii) s'il est plus associé à un but donné qu'un autre paragraphe traité auparavant.
Un modèle est proposé pour chacune de ces situations.
Nos simulations sont réalisées au niveau des fixations et des saccades oculaires.
En particulier, nous prédisons le moment auquel les participants décident d'abandonner la lecture du paragraphe parce qu'ils ont suffisamment d'information pour prendre leur décision.
Les modèles font ces prédictions par rapport aux mots qui sont susceptibles d'être traités avant que le paragraphe soit abandonné.
Nous avons suivi une approche statistique paramétrique dans la construction de nos modèles.
Ils sont basés sur un classifieur bayésien.
Nous proposons un seuil linéaire bi-dimensionnel pour rendre compte de la décision d'arrêter de lire un paragraphe, utilisant le Rang de la fixation et i) la similarité sémantique (Cos) entre le paragraphe et le but ainsi que ii) la différence de similarité sémantique (Gap) entre chaque paragraphe et le but.
Pour chacun des modèles, les performances montrent que nous sommes capables de reproduire en moyenne le comportement des participants face aux tâches de recherche d'information étudiées durant cette thèse.
Cette thèse comprend deux parties principales : 1) la conception et la passation d'expériences psychophysiques pour acquérir des données de mouvements oculaires et 2) le développement et le test de modèles cognitifs computationnels.
Alors que les sources multimédias sont massivement disponibles en ligne, aider les utilisateurs à comprendre la grande quantité d'information générée est devenu un problème majeur.
Une façon de procéder consiste à résumer le contenu multimédia, générant ainsi des versions abrégées et informatives des sources.
Cette thèse aborde le sujet du résumé automatique (texte et parole) dans un contexte multilingue.
Le résumé multimédia basé sur le texte utilise des transcriptions pour produire des résumés qui peuvent être présentés sous forme textuelle ou dans leur format d'origine.
La transcription des sources multimédia peut être effectuée manuellement ou automatiquement par un système de Reconnaissance automatique de la parole (RAP).
Les transcriptions peuvent différer de la langue écrite car la source étant parlée.
De plus, ces transcriptions manquent d'informations syntaxiques.
Par exemple, les majuscules et les signes de ponctuation sont absents, ce qu'implique des phrases inexistantes.
Enfin, nous étendons ARTEX, un résumeur textuel extractif état de l'art, pour traiter de documents en arabe standard en adaptant ses modules de prétraitement.
Les résumés peuvent être présentés sous une forme textuelle ou dans leur format multimédia original en alignant les US sélectionnées.
En ce qui concerne le résumé multimédia basée sur l'audio, nous introduisons une méthode extractive qui représente l'informativité de la source à partir de ses caractéristiques audio pour sélectionner les segments les plus pertinents pour le résumé.
Pendant la phase d'entraînement, notre méthode utilise les transcriptions des documents audio pour créer un modèle informatif qui établit une correspondance entre un ensemble de caractéristiques audio et une mesure de divergence.
Dans notre système, les transcriptions ne sont plus nécessaires pour résumer des nouveaux documents audio.
Les résultats obtenus sur un schéma multi-évaluation montrent que notre approche génère des résumés compréhensibles et informatifs.
Nous avons étudié également les mesures d'évaluation et nous avons développé la méthode Window-based
Nous explorons également la possibilité de mesurer la qualité des transcriptions automatiques en fonction de leur informativité.
De plus, nous étudions dans quelle mesure le résumé automatique peut compenser les problèmes posés au cours de la transcription.
Enfin, nous étudions comment les mesures d'évaluation d'informativité peuvent être étendues pour l'évaluation de l'intérêt des passages textuels.
Il est donc important de prendre en compte ces représentations ou vues des données.
Ce problème d'apprentissage automatique est appelé apprentissage multivue.
Il est utile dans de nombreux domaines d'applications, par exemple en imagerie médicale, nous pouvons représenter le cerveau humains via des IRM, t-fMRI, EEG, etc.
Dans cette cette thèse, nous nous concentrons sur l'apprentissage multivue supervisé, où l'apprentissage multivue est une combinaison de différents modèles de classifications ou de vues.
Par conséquent, selon notre point de vue, il est intéressant d'aborder la question de l'apprentissage à vues multiples dans le cadre PAC-Bayésien.
C'est un outil issu de la théorie de l'apprentissage statistique étudiant les modèles s'exprimant comme des votes de majorité.
Un des avantages est qu'elle permet de prendre en considération le compromis entre précision et diversité des votants, au cœur des problématiques liées à l'apprentissage multivue.
La première contribution de cette thèse étend la théorie PAC-Bayésienne classique (avec une seule vue) à l'apprentissage multivue (avec au moins deux vues).
Pour ce faire, nous définissons une hiérarchie de votants à deux niveaux : les classifieurs spécifiques à la vue et les vues elles-mêmes.
Sur la base de cette stratégie, nous avons dérivé des bornes en généralisation PAC-Bayésiennes (probabilistes et non-probabilistes) pour l'apprentissage multivue.
Le premier algorithme appelé PB-MVBoost est un algorithme itératif qui apprend les poids sur les vues en contrôlant le compromis entre la précision et la diversité des vues.
Le second est une approche de fusion tardive où les prédictions des classifieurs spécifiques aux vues sont combinées via l'algorithme PAC-Bayésien CqBoost proposé par Roy et al.
Enfin, nous montrons que la minimisation des erreurs pour le vote de majorité multivue est équivalente à la minimisation de divergences de Bregman.
De ce constat, nous proposons un algorithme appelé MωMvC2 pour apprendre un vote de majorité multivue.
Aujourd'hui, nous recueillons des données de différentes sources, en ligne et hors ligne.
Habituellement, ces données se réfèrent principalement à des actes déjà commis et sont généralement analysées a posteriori.
Étant donné que l'alerte précoce conduit à une action rapide (et, espérons-le, plus efficace), nous suggérons dans ce travail d'essayer d'identifier les signaux d'alerte précoce des actions ou activités qui se produiront à l'avenir.
Un signal faible est une preuve passée ou actuelle avec des interprétations ambiguës pouvant être corrélée à d'autres événements et tendances plus importants, présents ou futurs.
Les signaux faibles sont des indices peu clairs qui peuvent nous avertir d'autres modèles émergents.
Le problème clé est de détecter ces signaux pertinents cachés dans la masse de données disponibles.
Avant d'être sûr que l'attention des analystes humains est nécessaire, il est important de détecter, mais également de vérifier et de valider ces signaux au moyen d'une série de méthodes différentes.
Une fois identifié et validé, un signal faible devient un signe précoce qu'il convient de surveiller attentivement.
Dans ce travail, nous souhaitons établir des méthodes d'identification des signaux faibles au sein d'ensembles de données volumineux, tels que des ensembles de données provenant de médias sociaux ou d'autres sources accessibles au public.
Des algorithmes basés sur des méthodes d'apprentissage statistique ou automatique ou d'intelligence artificielle, combinés à des descriptions sémantiques formelles des événements qui nous intéressent, nous permettraient d'identifier les signaux d'alerte précoce et de les transformer en indications d'actes futurs.
De plus, les méthodes liées à la détection d'entités nommées nous permettraient de lier ces signaux d'alerte aux entités participantes (par exemple, des personnes, des lieux, etc.) et d'utiliser les descriptions d'entités pour améliorer et élargir la description des événements possibles.
Le travail sera validé en appliquant ses résultats aux données réelles fournies par PJGN, partenaire de ce travail.
Identifier des références à des événements et résoudre les informations autour de ces événements est un problème important pour PJGN et le travail sera directement applicable.
Cette thèse s'inscrit dans le cadre de l'analyse d'
Notre étude se concentre sur l'utilisation d'une approche neuronale pour améliorer la détection de polarité, en exploitant la puissance des embeddings.
En effet, ceux-ci se sont révélés un atout fondamental dans différentes tâches de traitement automatique des langues naturelles (TALN).
Notre contribution dans le cadre de cette thèse porte plusieurs axes.
Nous commençons, d'abord, par une étude préliminaire des différentes ressources d'embeddings de mots pré-entraînés existants en langue arabe.
Ces embeddings considèrent les mots comme étant des unités séparées par des espaces afin de capturer, dans l'espace de projection, des similarités sémantiques et syntaxiques.
Les phénomènes comme l'agglutination et la richesse morphologique de l'arabe sont alors pris en compte.
Ces embeddings spécifiques ont été utilisés, seuls et combinés, comme entrée à deux réseaux neuronaux (l'un convolutif et l'autre récurrent) apportant une amélioration des performances dans la détection de polarité sur un corpus de revues.
Nous proposons une analyse poussée des embeddings proposées.
Dans une évaluation intrinsèque, nous proposons un nouveau protocole introduisant la notion de la stabilité de polarités (sentiment stability) dans l'espace d'embeddings.
Puis, nous proposons une analyse qualitative extrinsèque de nos embeddings en utilisant des méthodes de projection et de visualisation.
La combinaison du traitement sémantique des connaissances (Semantic Processing of Knowledge) et de la modélisation des étapes de raisonnement (Modeling Steps of Reasoning), utilisés dans le domaine clinique, offrent des possibilités intéressantes, nécessaires aussi, pour l'élaboration des ontologies médicales, utiles à l'exercice de cette profession.
Dans ce cadre, l'interrogation de banques de données médicales multiples, comme MEDLINE, PubMed… constitue un outil précieux mais insuffisant car elle ne permet pas d'acquérir des connaissances facilement utilisables lors d'une démarche clinique.
En effet, l'abondance de citations inappropriées constitue du bruit et requiert un tri fastidieux, incompatible avec une pratique efficace de la médecine.
Dans un processus itératif, l'objectif est de construire, de façon aussi automatisée possible, des bases de connaissances médicales réutilisables, fondées sur des ontologies et, dans cette thèse, nous développons une série d'outils d'acquisition de connaissances qui combinent des opérateurs d'analyse linguistique et de modélisation de la clinique, fondés sur une typologie des connaissances mises en œuvre, et sur une implémentation des différents modes de raisonnement employés.
La connaissance ne se résume pas à des informations issues de bases de données ; elle s'organise grâce à des opérateurs cognitifs de raisonnement qui permettent de la rendre opérationnelle dans le contexte intéressant le praticien.
Un système multi-agents d'aide à la décision clinique (SMAAD) permettra la coopération et l'intégration des différents modules entrant dans l'élaboration d'une ontologie médicale et les sources de données sont les banques médicales, comme MEDLINE, et des citations extraites par PubMed ; les concepts et le vocabulaire proviennent de l'Unified Medical Language System (UMLS).
Concernant le champ des bases de connaissances produites, la recherche concerne l'ensemble de la démarche clinique : le diagnostic, le pronostic, le traitement, le suivi thérapeutique de différentes pathologies, dans un domaine médical donné.
Sur l'ensemble, nous avons travaillé les aspects logiques liés aux opérateurs cognitifs de raisonnement utilisés et nous avons organisé la coopération et l'intégration des connaissances exploitées durant les différentes étapes du processus clinique (diagnostic, pronostic, traitement, suivi thérapeutique).
Cette intégration s'appuie sur un SMAAD : système multi-agent d'aide à la décision.
Le but de ce travail de thèse est de rendre sincère un Agent Conversationnel Animé (ACA) pour, d'une part, améliorer sa crédibilité du point de vue de l'humain, et d'autre part contribuer à le rendre acceptable dans une relation privilégiée compagnon artificiel - humain.
Les états mentaux portés par les ACM sont formalisés en logique : la volonté de représenter des états mentaux issus de raisonnements complexes (basés d'une part sur le raisonnement contrefactuel et d'autre part sur les normes et les buts de l'agent), dont l'expression se fait avant tout par le langage (Oatley 1987), a amené à mettre en place le modèle BIGRE (Beliefs, Ideals, Goals, Responsibility, Emotions).
Ce modèle, basé sur une logique de type BDI (Belief, Desire, Intention), permet de représenter également des émotions que nous appelons complexes, telles que la réjouissance, la gratitude ou le regret.
Le LCM est implémenté dans l'ACA Greta, ce qui permet une évaluation de ce langage en termes de crédibilité et de sincérité perçues par l'humain.
La deuxième partie de ce travail porte sur les capacités de raisonnement de l'ACA : dans le but de permettre à l'agent de raisonner dans le dialogue, c'est-à-dire mettre à jour ses états mentaux et ses émotions et sélectionner son intention communicative, un moteur de raisonnement a été mis en place.
Ce moteur de raisonnement est basé sur le cycle de comportement BDI - Perception, Décision, Action - et les opérateurs du modèle BIGRE, permettant ainsi la manipulation d'états mentaux issus de raisonnements complexes (dont les émotions complexes).
Les ACM qui composent notre langage sont intégrés dans le moteur, et sont utilisés pour atteindre l'intention communicative de l'ACA : par exemple, si l'agent a l'intention d'exprimer sa gratitude, il construit un plan pour satisfaire son intention, formé des ACM remercier ou féliciter, selon le degré de l'émotion.
Un type d'intention communicative, déclenché par des règles d'obligation du discours, participe à la régulation locale du dialogue.
L'ACA étant de plus affectif, sa sincérité l'amène à exprimer toutes ses émotions.
La généricité de ce moteur de raisonnement permet de l'implémenter dans l'ACA Greta (où il est en lien avec le LCM) et dans l'agent MARC.
L'expression multimodale des ACM avec l'agent MARC a été rendue possible par l'intégration des checks de Scherer dans le moteur de raisonnement que nous avons adapté au contexte du dialogue.
Une évaluation du moteur de raisonnement avec l'agent MARC montre que les états mentaux déduits par le moteur sont appropriés à la situation, et que leur expression (l'expression de la sincérité de l'agent) est également appropriée.
L'Intelligence Artificielle est présente dans tous les aspects de notre vie à l'ère du Big Data.
Elle a entraîné des changements révolutionnaires dans divers secteurs, dont le commerce électronique et la finance.
Dans cette thèse, nous présentons quatre applications de l'IA qui améliorent les biens et services existants, permettent l'automatisation et augmentent considérablement l'efficacité de nombreuses tâches dans les deux domaines.
Tout d'abord, nous améliorons le service de recherche de produits offert par la plupart des sites de commerce électronique en utilisant un nouveau système de pondération des termes pour mieux évaluer l'importance des termes dans une requête de recherche.
Ensuite, nous construisons un modèle prédictif sur les ventes quotidiennes en utilisant une approche de prévision des séries temporelles et tirons parti des résultats prévus pour classer les résultats de recherche de produits afin de maximiser les revenus d'une entreprise.
Ensuite, nous proposons la difficulté de la classification des produits en ligne et analysons les solutions gagnantes, consistant en des algorithmes de classification à la pointe de la technologie, sur notre ensemble de données réelles.
Nous effectuons une étude approfondie sur chaque titre de l'indice S&amp;P 500 en utilisant quatre algorithmes de classification à la pointe de la technologie et nous publions des résultats très prometteurs
Les produits et réseaux électriques sont soumis à un environnement réglementaire et normatif fractionné selon la géographie, le niveau de tension et le secteur d'application.
L'importance croissante des nouvelles technologies associées à la transition énergétique (énergies renouvelables, véhicule électrique, stockage et optimisation de l'énergie) entraîne une accélération de la production de référentiels normatifs et réglementaires.
Dans ce contexte, la conception de nouveaux produits, systèmes et installations, requiert une veille permanente sur les textes de référence à prendre en compte, l'évolution du vocabulaire et des concepts employés et, évidemment, les contraintes qu'il est nécessaire de respecter.
L'objectif de la thèse proposée est d'extraire de collections documentaires les règles et contraintes de conception à appliquer en les traduisant dans un langage formel actionnable (c'est-à-dire compréhensible et utilisable par les experts du domaine) pour l'aide à la conception.
Ceci inclut la formalisation des connaissances du domaine, l'analyse et la compréhension des textes, la construction de synthèse des règles extraites, ainsi que la détection de conflits et doublons
Étant donnée la masse toujours croissante de texte publié, la compréhension automatique des langues naturelles est à présent l'un des principaux enjeux de l'intelligence artificielle.
En langue naturelle, les faits exprimés dans le texte ne sont pas nécessairement tous explicites : le lecteur humain infère les éléments manquants grâce à ses compétences linguistiques, ses connaissances de sens commun ou sur un domaine spécifique, et son expérience.
Les systèmes de Traitement Automatique des Langues (TAL) ne possèdent naturellement pas ces capacités.
Incapables de combler les défauts d'information du texte, ils ne peuvent donc pas le comprendre vraiment.
Cette thèse porte sur ce problème et présente notre travail sur la résolution d'inférences pour la compréhension automatique de texte.
Une inférence textuelle est définie comme une relation entre deux fragments de texte : un humain lisant le premier peut raisonnablement inférer que le second est vrai.
Beaucoup de tâches de TAL évaluent plus ou moins directement la capacité des systèmes à reconnaître l'inférence textuelle.
Au sein de cette multiplicité de l'évaluation, les inférences elles-mêmes présentent une grande variété de types.
Nous nous interrogeons sur les inférences en TAL d'un point de vue théorique et présentons deux contributions répondant à ces niveaux de diversité : une tâche abstraite contextualisée qui englobe les tâches d'inférence du TAL, et une taxonomie hiérarchique des inférences textuelles en fonction de leur difficulté.
La reconnaissance automatique d'inférence textuelle repose aujourd'hui presque toujours sur un modèle d'apprentissage, entraîné à l'usage de traits linguistiques variés sur un jeu d'inférences textuelles étiquetées.
Cependant, les données spécifiques aux phénomènes d'inférence complexes ne sont pour le moment pas assez abondantes pour espérer apprendre automatiquement la connaissance du monde et le raisonnement de sens commun nécessaires.
Les systèmes actuels se concentrent plutôt sur l'apprentissage d'alignements entre les mots de phrases reliées sémantiquement, souvent en utilisant leur structure syntaxique.
Pour étendre leur connaissance du monde, ils incluent des connaissances tirées de ressources externes, ce qui améliore souvent les performances.
Mais cette connaissance est souvent ajoutée par dessus les fonctionnalités existantes, et rarement bien intégrée à la structure de la phrase.
Nos principales contributions dans cette thèse répondent au problème précédent.
En partant de l'hypothèse qu'un lexique plus simple devrait rendre plus facile la comparaison du sens de deux phrases, nous décrivons une méthode de récupération de passage fondée sur une expansion lexicale structurée et un dictionnaire de simplifications.
Cette hypothèse est testée à nouveau dans une de nos contributions sur la reconnaissance d'implication textuelle : des paraphrases syntaxiques sont extraites du dictionnaire et appliquées récursivement sur la première phrase pour la transformer en la seconde.
Nous présentons ensuite une méthode d'apprentissage par noyaux de réécriture de phrases, avec une notion de types permettant d'encoder des connaissances lexico-sémantiques.
Cette approche est efficace sur trois tâches : la reconnaissance de paraphrases, d'implication textuelle, et le question
Des tests de compréhension sont utilisés pour son évaluation, sous la forme de questions à choix multiples sur des textes courts, qui permettent de tester la résolution d'inférences en contexte.
Notre système est fondé sur un algorithme efficace d'édition d'arbres, et les traits extraits des séquences d'édition sont utilisés pour construire deux classifieurs pour la validation et l'invalidation des choix de réponses.
Cette approche a obtenu la deuxième place du challenge "Entrance Exams" à CLEF 2015.
Nous proposons une approche pour détecter les sujets, les communautés d'intérêt non disjointes, l'expertise, les tendances et les activités dans des sites où le contenu est généré par les utilisateurs et en particulier dans des forums de questions-réponses tels que StackOverFlow.
Nous décrivons d'abord QASM (Questions &amp; Réponses dans des médias sociaux), un système basé sur l'analyse de réseaux sociaux pour gérer les deux principales ressources d'un site de questions-réponses : les utilisateurs et le contenu.
Nous présentons également le vocabulaire QASM utilisé pour formaliser à la fois le niveau d'intérêt et l'expertise des utilisateurs.
Nous proposons ensuite une approche efficace pour détecter les communautés d'intérêts.
Elle repose sur une autre méthode pour enrichir les questions avec un tag plus général en cas de besoin.
Nous comparons trois méthodes de détection sur un jeu de données extrait du site populaire StackOverflow.
Notre méthode basée sur le se révèle être beaucoup plus simple et plus rapide, tout en préservant la qualité de la détection.
Nous proposons en complément une méthode pour générer automatiquement un label pour un sujet détecté en analysant le sens et les liens de ses mots-clefs.
Nous menons alors une étude pour comparer différents algorithmes pour générer ce label.
Enfin, nous étendons notre modèle de graphes probabilistes pour modéliser conjointement les sujets, l'expertise, les activités et les tendances.
Nous le validons sur des données du monde réel pour confirmer l'efficacité de notre modèle intégrant les comportements des utilisateurs et la dynamique des sujets.
Nous sommes amenés chaque jour à prendre un nombre important de décisions : quel nouveau livre lire ? Quel film regarder ce soir ou où aller ce week-end ?
De plus en plus, nous utilisons les ressources en ligne pour nous aider à prendre des décisions.
Comme la prise de décision est assistée par le domaine en ligne, l'utilisation de systèmes de recommandation est devenue essentielle dans la vie quotidienne.
Dans le même temps, les réseaux sociaux sont devenus une partie indispensable de ce processus ; partout dans le monde on les utilise quotidiennement pour récupérer des données de personne et de sources d'information en qui on a confiance.
Quand les internautes passent du temps sur les réseaux sociaux, ils laissent de précieuses informations sur eux-mêmes.
Comme le domaine de la recommandation est un domaine qui a assisté à des changements de grande ampleur attribuable à des réseaux sociaux, il y a un intérêt évident pour les systèmes de recommandation sociale.
Cependant, dans la littérature de ce domaine, nous avons constaté que de nombreux systèmes de recommandation sociale ont été évalués en utilisant des réseaux sociaux spécialisés comme Epinions, Flixter et d'autres types des réseaux sociaux de recommandation, qui tendent à être composées d'utilisateurs, d'articles, de notes et de relations.
Les enceintes intelligentes offrent la possibilité d'interagir avec les systèmes informatiques de la maison.
Elles permettent d'émettre un éventail de requêtes sur des sujets divers et représentent les premières interfaces vocales disponibles couramment dans les environnements domestiques.
La compréhension des commandes vocales concerne des énoncés courts ayant une syntaxe simple, dans le domaine des habitats intelligents destinés à favoriser le maintien à domicile des personnes âgées.
Ils les assistent dans leur vie quotidienne, améliorant ainsi leur qualité de vie, mais peuvent aussi leur porter assistance en situations de détresse.
La conception de ces habitats se concentre surtout sur les aspects de la sécurité et du confort, ciblant fréquemment sur la détection de l'activité humaine.
, en particulier pour des langues autres que l'anglais, alorsqu'ils sont essentiels pour développer les systèmes de communication entre l'habitat et ses habitants.
La disponibilité de tels corpus, pourrait contribuer au développement d'une génération d'enceintes intelligentes qui soient capables d'extraire des commandes vocales plus complexes.
Pour contourner une telle contrainte, une partie de notre travail consiste à développer un générateur de corpus, produisant des commandes vocales spécifiques au domaine domotique, automatiquement annotées d'étiquettes d'intentions et de concepts.
Un système de compréhension de la parole (SLU - Spoken Language Understanding) est nécessaire afin d'extraire les intentions et les concepts des commandes vocales avant de les fournir au module de prise de décision en charge de l'exécution des commandes.
Comme plusieurs études l'ont montré, l'enchaînement entre RAP et NLU dans une approche séquentielle de SLU cumule les erreurs.
À cette fin, nous élaborons d'abord une approche SLU séquentielle comme approche de référence, dans laquelle une méthode classique de RAP génère des transcriptions qui sont transmises au module NLU, avant de poursuivre par le développement d'un module de SLU de bout en bout.
Ces deux systèmes de SLU sont évalués sur un corpus enregistré spécifiquement au domaine de la domotique.
Nous étudions si l'information prosodique, à laquelle la SLU de bout en bout a accès, contribue à augmenter les performances.
Nous comparons aussi la robustesse des deux approches lorsqu'elles sont confrontées à un style de parole aux niveaux sémantiques et syntaxiques plus varié.
Cette étude est menée dans le cadre du projet VocADom financé par l'appel à projets génériques de l'ANR.
En raison de ses enjeux sociétaux, économiques et culturels, l'intelligence artificielle (dénotée IA) est aujourd'hui un sujet d'actualité très populaire.
L'un de ses principaux objectifs est de développer des systèmes qui facilitent la vie quotidienne de l'homme, par le biais d'applications telles que les robots domestiques, les robots industriels, les véhicules autonomes et bien plus encore.
La montée en popularité de l'IA est fortement due à l'émergence d'outils basés sur des réseaux de neurones profonds qui permettent d'apprendre simultanément, la représentation des données (qui était traditionnellement conçue à la main), et la tâche à résoudre (qui était traditionnellement apprise à l'aide de modèles d'apprentissage automatique).
Ceci résulte de la conjonction des avancées théoriques, de la capacité de calcul croissante ainsi que de la disponibilité de nombreuses données annotées.
Celle-ci consiste `a l'apprentissage d'un modèle qui une fois appris résoud une certaine tâche, et est généralement composée de deux sous-modules, l'un représentant la donnée (nommé ”représentation”) et l'autre prenant des décisions (nommé ”résolution de tâche”).
Alors que la spécialisation a été largement explorée par la communauté de l'apprentissage profond, seules quelques tentatives implicites ont été réalisée vers la seconde catégorie, à savoir, l'universalité.
Lorsqu'il s'agit la construction de ressources lexico-sémantiques multilingues, la première chose qui vient à l'esprit, et la nécessité que les ressources à alignées partagent le même format de données et la même représentations (interopérabilité représentationnelle).
Avec l'apparition de standard tels que LMF et leur adaptation au web sémantique pour la production de ressources lexico- sémantiques multilingues en tant que données lexicales liées ouvertes (Ontolex), l'interopérabilité représentationnelle n'est plus un verrou majeur.
Cependant, en ce qui concerne l'interopérabilité des alignements multilingues, le choix et la construction du pivot interlingue est l'un des obstacles principaux.
L'utilisation d'une pivot à acceptions interlingues, solution proposée il y a déjà plus de 20 ans, pourrait être viable.
Néanmoins, leur construction manuelle est trop ardue du fait du manque d'experts parlant assez de langues et leur construction automatique pose problème du fait de l'absence d'une formalisation et d'une caractérisation axiomatique permettant de garantir leur propriétés.
Nous proposons dans cette thèse de d'abord formaliser l'architecture à pivot interlingue par acceptions, en développant une axiomatisation garantissant leurs propriétés.
Nous proposons ensuite des algorithmes de construction initiale automatique en utilisant les propriétés combinatoires du graphe des alignements bilingues, mais aussi des algorithmes de mise à jour garantissant l'interopérabilité dynamique.
Dans un deuxième temps, nous étudions de manière plus pratique sur DBNary, un extraction périodique de Wiktionary dans de nombreuses éditions de langues, afin de cerner les contraintes pratiques à l'application des algorithmes proposés.
Cette thèse examine les convergences et divergences le long de l'histoire de la documentation muséale dans trois galeries nationales en Angleterre, au Canada et aux États-Unis.
La lutte continue afin d'intégrer véritablement la technologie dans la documentation d'art trahit l'héritage difficile entre ces deux modèles opposés.
Tout au long des trajectoires uniques à chacune des institutions étudiées, peu de preuves montrent que les technologies optiques ou numériques ont eu des répercussions importantes sur les méthodologies ou les philosophies de la documentation des œuvres d'art.
Au contraire, on observe que la documentation de forme numérique repose toujours sur une approche minimale de recueil de données, sur un groupe restreint de personnes habilitées à les collecter et sur un accès limité à ces données.
Cette recherche renforce l'argumentation pour une redéfinition de la documentation des œuvres d'art pour repenser ses stratégies et ses philosophies directrices, pour poser un nouveau regard sur la recherche dans les collections et pour élargir l'intégration des technologies numériques dans ces processus.
Cette étude est une analyse dialectométrique des parlers berbères de Kabylie.
Le présent travail inclut un échantillon de 168 parlers kabyles répartis sur tout le territoire kabylophone.
Le corpus analysé compte 130 entrées (lexèmes et syntagmes) recueillies dans chacune des variétés prises en compte.
Nous avons opté pour la méthode Levenshtein afin de calculer la distance entre les variantes.
Nous avons choisi l'algorithme de Ward's Method pour regrouper les variétés.
Nous avons testé trois méthodes pour calculer la distance entre les sons : la méthode binaire, la distance d'Euclide et la distance de Manhattan.
L'analyse des résultats nous a permis de montrer le continuum dialectal en Kabylie et de classifier les parlers kabyles en cinq zones infradialectales principales.
L'activité scientifique de cette thèse consiste à :
Comprendre la nature des relations d'anaphore et de coréférence dans les messages issus de la communication électronique médiée, avec une priorité donnée aux e-mails, de sorte à mener vers des méthodes capables de tisser des liens sémantiques entre différentes phrases et différents messages.
Etudier plus précisément les anaphores événementielles (Danlos, 2004) et la nature des relations entre les événements.
Il s'agit d'un verrou scientifique majeur pour la compréhension du langage naturel avec une retombée directe sur la suite du développement de la solution Prevyo de l'entreprise d'accueil Emvista.
L'avancée scientifique et l'appropriation des technologies en résultant permettront à Emvista de se positionner comme un acteur important de la compréhension du langage naturel.
Au cours des dernières années, le secteur touristique a été caractérisé par toute une série de changements fondamentaux.
Notre mémoire de recherche se situe à l'intersection de la terminologie thématique, de la linguistique de corpus et du traitement automatique des langues.
Dans le premier chapitre du travail que nous allons présenter, nous chercherons à introduire aux domaines d'études théoriques sur lesquels notre recherche s'appuie.
Premièrement, on traitera de la linguistique de corpus et on examinera les différentes catégories de corpus existantes.
On mettra l'accent sur deux notions fondamentales dans la conception de l'outil corpus en général et dans la création de notre corpus en particulier : représentativité et contexte.
Avant tout, on fournira les indications théoriques et pragmatiques nécessaires pour réaliser un corpus trilingue en langue de spécialité : la collecte des textes, l'homogénéisation des échantillons textuels repérés et l'annotation.
Au cours de ce chapitre, nous présenterons Alinea, l'instrument qu'on a utilisé pour l'alignement de textes recueillis et pour la consultation simultanée des traductions trilingues.
Dans le troisième et dernier chapitre, on passera à l'interrogation du corpus créé.
Sur la base d'un terme pris comme exemple, le terme ville, on lancera la recherche dans le CTT.
Ensuite, on analysera les collocations les plus usitées contenant le mot ville.
Pour conclure, l'objectif général de notre étude sera d'explorer la chaîne de gestion terminologique à travers la création d'un glossaire trilingue dans le domaine du tourisme.
Les différences entre conditions d'apprentissage et conditions de test peuvent considérablement dégrader la qualité des transcriptions produites par un système de reconnaissance automatique de la parole (RAP).
L'adaptation est un moyen efficace pour réduire l'inadéquation entre les modèles du système et les données liées à un locuteur ou un canal acoustique particulier.
Il existe deux types dominants de modèles acoustiques utilisés en RAP : les modèles de mélanges gaussiens (GMM) et les réseaux de neurones profonds (DNN).
L'approche par modèles de Markov cachés (HMM) combinés à des GMM (GMM-HMM) a été l'une des techniques les plus utilisées dans les systèmes de RAP pendant de nombreuses décennies.
Plusieurs techniques d'adaptation ont été développées pour ce type de modèles.
L'objectif principal de cette thèse est de développer une méthode de transfert efficace des algorithmes d'adaptation des modèles GMM aux modèles DNN.
La technique proposée fournit un cadre général pour le transfert des algorithmes d'adaptation développés pour les GMM à l'adaptation des DNN.
Elle est étudiée pour différents systèmes de RAP à l'état de l'art et s'avère efficace par rapport à d'autres techniques d'adaptation au locuteur, ainsi que complémentaire.
Associer une orientation temporelle au sens des mots pour capter l'information temporelle en langue est une tâche relativement directe pour les humains utilisant leurs connaissances sur le monde.
Une base de connaissances lexicales associant automatiquement cette orientation au sens des mots serait de fait cruciale pour les tâches automatiques visant à interpréter la temporalité dans les textes.
Dans cette recherche, nous présentons une ontologie temporelle, TempoWordNet, où les synsets de WordNet sont enrichis avec une information sur leur temporalité intrinsèque : atemporel, passé, présent et futur.
TempoWordNet est évalué de manière intrinsèque et extrinsèque, une ressource fiable devant à la fois contenir un étiquetage temporel de haute qualité et améliorer les performances de certaines tâches externes.
Les deux types d'évaluations montrent la qualité et l'intérêt de la ressource.
Pour compléter nos travaux, nous étudions aussi comment une application de recherche telle un moteur de recherche peut tirer parti de cette ressource.
Le retour des utilisateurs de TempoWordNet a encouragé à améliorer encore la ressource.
Nous terminons donc en proposant une nouvelle stratégie de construction permettant d'améliorer de manière conséquente TempoWordNet.
Après s'être interrogée sur la notion de sens, cette thèse se fixe pour objectif de dégager un certain nombre d'éléments linguistiques allant dans le sens d'une caractérisation abstraite à même de rendre compte des différents emplois du verbe anglais want.
Il est proposé que ce verbe conserve aujourd'hui quelque chose de son emploi diachronique initial en tant que « verbe de manque » à partir d'une première opération de localisation par laquelle le sujet grammatical est mis en relation avec un objet ou une propriété non-immédiatement disponible pour lui dans la situation d'énonciation.
Cette mise en relation particulière accompagne un processus de subjectivisation culminant dans une tendance forte à la modalisation de l'énoncé en want, en lien avec des effets déontiques et épistémiques.
Il semble intéressant de le traiter comme un domaine notionnel organisé autour de p, lequel sert de repère permettant d'envisager une seconde opération de localisation qui autorise le repérage d'une relation permettant d'atteindre p.
Ainsi, la distinction désir / volonté est pensable en termes de degrés dans les déterminations que l'on peut construire à partir du manque.
Ce travail met donc en relation les constructions syntaxiques possibles du second argument de want et ces déterminations.
Cette hétérogénéité des sources de connaissances pose le problème de l'utilisation secondaire des données, et en particulier de l'exploitation de données hétérogènes dans le cadre de la médecine personnalisée ou translationnelle.
En effet, les données à utiliser peuvent être codées par des sources de connaissances décrivant la même notion clinique de manière différente ou décrivant des notions distinctes mais complémentaires.
Pour répondre au besoin d'utilisation conjointe des sources de connaissances encodant les données de santé, nous avons étudié trois processus permettant de répondre aux conflits sémantiques (difficultés résultant de leur mise en relation) : (1)
Dans un premier travail, nous avons aligné la terminologie d'interface du laboratoire d'analyses du CHU de Bordeaux à la LOINC.
Deux étapes principales ont été mises en place : (i) le prétraitement des libellés de la terminologie locale qui comportaient des troncatures et des abréviations, ce qui a permis de réduire les risques de survenue de conflits de nomenclature, (ii) le filtrage basé sur la structure de la LOINC afin de résoudre les différents conflits de confusion.
Ainsi, les médicaments dans RxNorm ont été décrits en OWL grâce à leurs éléments définitionnels (substance, unité de mesure, dose, etc.).
Nous avons ensuite fusionné cette représentation de RxNorm à la structure de la SNOMED CT, résultant en une nouvelle source de connaissances.
Notre méthode a résolu des conflits de nomenclature mais s'est confrontée à certains conflits de confusion et d'échelle, ce qui a mis en évidence le besoin d'améliorer RxNorm et SNOMED CT.Finalement, nous avons réalisé une intégration sémantiquement enrichie de la CIM10 et de la CIMO3 en utilisant la SNOMED CT comme support.
La CIM10 décrivant des diagnostics et la CIMO3 décrivant cette notion suivant deux axes différents (celui des lésions histologiques et celui des localisations anatomiques), nous avons utilisé la structure de la SNOMED CT pour retrouver des relations transversales entre les concepts de la CIM10 et de la CIMO3 (résolution de conflits ouverts).
Au cours du processus, la structure de la SNOMED CT a également été utilisée pour supprimer les mappings erronés (conflits de nomenclature et de confusion) et désambiguïser les cas de mappings multiples (conflits d'échelle).
Les récentes avancées en matière d'intelligence artificielle ont vu une adoption limitée dans la communauté des auteurs de revues systématiques, et une grande partie du processus d'élaboration des revues systématiques est toujours manuelle, longue et coûteuse.
Les auteurs de revues systématiques rencontrent des défis tout au long du processus d'élaboration d'une revue.
Il est long et difficile de chercher, d'extraire, de collecter des données, de rédiger des manuscrits et d'effectuer des analyses statistiques.
L'automatisation de la sélection d'articles a été proposé comme un moyen de réduire la charge de travail, mais son adoption a été limitée en raison de différents facteurs,notamment l'investissement important de prise en main, le manque d'accompagnement et les décalages par rapport au flux de travail.
Il est nécessaire de mieux harmoniser les méthodes actuelles avec les besoins de la communauté des revues systématiques.
Les études sur l'exactitude des tests diagnostiques sont rarement indexées de façon à pouvoir être facilement retrouvées dans les bases de données bibliographiques.
Les requêtes de recherche méthodologique visant à repérer les études diagnostiques ont donc tendance à être peu précises, et leur utilisation dans les études méthodiques est déconseillée.
Par conséquent, il est particulièrement nécessaire d'avoir recours à d'autres méthodes pour réduire la charge de travail dans les études méthodiques sur l'exactitude des tests diagnostiques.
Dans la présente thèse, nous avons examiné l'hypothèse selon laquelle les méthodes d'automatisation peuvent offrir un moyen efficace de rendre le processus d'élaboration des revues systématique plus rapide et moins coûteux, à condition de pouvoir cerner et surmonter les obstacles à leur adoption.
Les travaux réalisés montrent que les méthodes automatisées offrent un potentiel de diminution des coûts tout en améliorant la transparence et la reproductibilité du processus.
Cette étude consiste en une analyse comparée des unités syntaxiques (les syntagmes) et des constructions fondamentales du fiançais et du persan, en ayant un regard sur les Langues Contrôlées (LC) et les cas problématiques et ambigus pour la traduction.
Après un passage sur l'histoire de ces langues et une brève présentation du système d 'écriture et phonétique du persan,les classes de mots et leurs classifications traditionnelle et moderne sont comparées.
Ensuite, les structures des syntagmes déterminant, nominal, adjectival, prépositionnel, adverbial et verbal et la nature de leurs composants, ainsi que les constructions fondamentales de la phrase de base dans ces deux langues sont analysées.
Tout au long du parcours, en faisant quelques tests de traduction avec des étudiants persanophones, certains cas problématiques pour la traduction sont repérés et traités pour une langue contrôlée français-persan éventuelle.
Dans la synthèse finale, sont rassemblées, les structures syntagmatiques et certaines instructions pour élaborer une LC concernant les langues française et persane.
Le marché de l'aviation fait face aujourd'hui à une croissance rapide des technologies innovantes.
Les drones cargo, les taxis drones, les dirigeables, les ballons stratosphériques, pour n'en citer que quelques-uns, pourraient faire partie de la prochaine génération de transport aérien.
Dans le même temps, les Petites et Moyennes Entreprises (PMEs) s'impliquent de plus en plus dans la conception et le développement de nouvelles formes de système aéroporté, passant du rôle traditionnel de fournisseur à celui de concepteur et intégrateur.
Cette situation modifie considérablement la portée de la responsabilité des PMEs.
En tant qu'intégrateurs, elles deviennent responsables de la certification des composants et du processus de fabrication, domaine dans lequel elles n'ont encore que peu d'expérience.
La certification, qui requiert une connaissance très spécifique des réglementations, des normes et standards, demeure un processus obligatoire et une activité critique pour les entreprises de l'industrie aéronautique.
C'est aussi un défi majeur pour les PMEs qui doivent assumer cette responsabilité de certification avec des moyens limités.
Dans cette thèse, deux besoins majeurs sont identifiés : le soutien méthodologique n'est pas facilement disponible pour les PMEs ; et les exigences de certification ne sont pas facilement compréhensibles et adaptables à chaque situation.
Nous examinons donc des voies alternatives pour réduire la complexité de la situation des PMEs.
L'objectif est de fournir un soutien afin qu'elles puissent être plus efficaces pour comprendre et intégrer les règles, les législations et les lignes directrices à leurs processus internes de manière plus simple.
Cette thèse propose ainsi une approche méthodologique pour soutenir ces organisations.
Développée en étroite collaboration avec une PME française, l'approche est composée d'un ensemble de modèles (métamodèle, modèles structurels et comportementaux) couverts par un mécanisme de gouvernance.
Cependant, cette mise en commun de ressources, données et savoir-faire implique de nouvelles exigences en termes de sécurité.
En effet, le manque de confiance dans les structures du Cloud est souvent vu comme un frein au développement de tels services.
L'objectif de cette thèse est d'étudier les concepts d'orchestration de services, de confiance et de gestion des risques dans le contexte du Cloud.
La contribution principale est un framework permettant de déployer des processus métiers dans un environnement Cloud, en limitant les risques de sécurité liés à ce contexte.
La contribution peut être séparée en trois partie distinctes qui prennent la forme d'une méthode, d'un modèle et d'un framework.
La méthode catégorise des techniques pour transformer un processus métier existant en un modèle sensibilisé (ou averti) qui prend en compte les risques de sécurité spécifiques aux environnements Cloud.
Le modèle formalise les relations et les responsabilités entre les différents acteurs du Cloud.
Ce qui permet d'identifier les différentes informations requises pour évaluer et quantifier les risques de sécurité des environnements Cloud.
Le framework est une approche complète de décomposition de processus en fragments qui peuvent être automatiquement déployés sur plusieurs Clouds.
Ce framework intègre également un algorithme de sélection qui combine les information de sécurité avec d'autres critère de qualité de service pour générer des configuration optimisées.
Finalement, les travaux sont implémentés pour démontrer la validité de l'approche.
Le framework est implémenté dans un outil.
Le modèle d'évaluation des risques de sécurité Cloud est également appliqué dans un contexte de contrôle d'accès.
La dernière partie présente les résultats de l'implémentation de nos travaux sur un cas d'utilisation réel.
Le contexte de cette thèse s'inscrit dans le domaine de la fouille de textes médicaux.
L'intérêt central concerne l'extraction des informations sur les interactions entre l'alimentation et les médicaments.
Les aliments peuvent en effet avoir des interactions avec les médicaments et cela peut mener à des conséquences malheureuses pour le patient.
Ces interactions sont très peu étudiées.
Il est ainsi nécessaire de proposer des méthodes de fouille de texte permettant d'analyser des articles scientifiques d'en extraire les informations relatives aux interactions entre aliment et médicament.
On s'appuiera également sur les informations proposées par les ressources terminologiques et de données liées existantes.
Notre thèse se donne comme objectif le traitement de l'adjectif polysémique applicable au domaine du traitement automatique des langues(TAL) et plus particulièrement à la traduction automatique (TA) (français-coréen).
La variabilité du sens des lexiques boursiers génère plusieurs interprétations sémantiques différentes et donc plusieurs traductions.
Pour résoudre ce genre de problèmes dans le cadre du TAL, nous proposons un modèle de désambiguïsation en mettant en œuvre des informations co-textuelles et plus particulièrement le nom.
Ce dernier constitue un syntagme nominal en se combinant avec un adjectif et permet de lever l'ambiguïté sémantique de l'adjectif et déterminer son sens adéquat.
Les indices co-textuels sur lesquels nous nous appuyons sont les suivants : l'ordre syntaxique de l'adjectif, la propriété du nom et la classe sémantique à laquelle le nom appartient.
Notre travail consiste à attribuer à chacun des adjectifs polysémiques sélectionnés les informations co-textuelles ci-dessus pour que la machine puisse trouver leur sens correct.
La méthodologie proposée est généralisable à d'autres parties du discours, nous l'avons nous-même appliquée au verbe et à son co-texte, le nom
L'utilisation répandue des smartphones dans notre vie quotidienne et l'essor technologique que connait le monde aujourd'hui - offrant l'accès mobile à très haut débit - ont exponentiellement augmenté la demande sur les services de vidéo streaming mobile, ce qui justifie la tendance à explorer de nouvelles approches pour la distribution des contenus média.
Afin d'assurer une qualité de streaming constante et acceptable, la majorité des approches proposent aujourd'hui d'adapter la distribution des flux média au contexte de l'utilisateur.
Pour assurer une bonne QoE, les solutions de vidéo streaming mobile exigent la connaissance au préalable du contexte de l'utilisateur, comme par exemple la capacité de son lien physique ou la disponibilité de sa bande passante.
L'acquisition de telles informations contextuelles est devenue possible aujourd'hui grâce à l'utilisation des capteurs sans fils dans les appareils mobiles et à l'existence de plusieurs applications intelligentes dédiées, le principe étant majoritairement d'exploiter la forte corrélation entre le contexte de l'utilisateur et sa position géographique.
En outre, plusieurs études menées sur les modèles de mobilité des usagers ont exhibé une quasi-régularité spatio-temporelle dans leurs trajets quotidiens, soit en prenant les transports publics ou en allant vers des endroits fréquemment visités.
Couplés avec les cartes radio, ces études permettent une haute précision dans la prédiction du contexte de l'utilisateur le long de son trajet.
Dans cette thèse, nous nous intéressons à analyser l'impact de l'adaptation du service vidéo streaming au contexte de l'utilisateur sur sa QoE finale.
Nous commençons par proposer CAMS (Context Aware Mode Switching), un mécanisme d'allocation de ressources qui dépend du contexte et qui s'applique à la distribution du vidéo streaming réel (non-adaptatif), pour assurer le minimum d'interruptions de vidéo.
CAMS est conçu pour être déployé dans une topologie de réseau spécifique avec un modèle de mobilité particulier.
Par la suite, nous explorons l'impact de la connaissance à l'avance du débit futur de l'utilisateur sur l'adaptation de la qualité de sa vidéo et sur le coût de sa transmission dans un contexte de streaming adaptatif.
Nous proposons NEWCAST (aNticipating qoE With threshold sCheme And aScending biTrate levels), un algorithme proactif pour l'ajustement du coût et l'adaptation de la qualité sous réserve d'une prédiction parfaite du débit.
Nous étendons cette étude, dans un deuxième temps, pour le cas où la prédiction du débit est imparfaite. Nous proposons, donc, d'autres algorithmes adaptatifs en nous inspirant de l'approche de NEWCAST.
Pour étudier la faisabilité de ces algorithmes sur le plan pratique, nous menons quelques expérimentations dans un environnement émulé à l'aide du lecteur média DASH-IF-Reference.
Finalement, nous explorons l'idée de coupler la connaissance parfaite du débit futur de l'utilisateur avec l'usage d'un mécanisme d'apprentissage automatique, pour améliorer la QoE dans un contexte de streaming adaptatif.
Nous proposons, donc, un système à boucle fermée, basé sur le retour des utilisateurs, pour apprendre progressivement leurs préférences et pour optimiser adéquatement la transmission des futures vidéos.
Ce système est particulièrement conçu pour être utilisé dans des populations hétérogènes avec des profils de QoE différents et inconnus à l'avance.
Cette thèse propose un modèle conventionnel de la structuration d'une tâche collaborative humain-machine pour concevoir un système interactif assistant un humain sur la réalisation de tâches complexes.
Plus spécifiquement, nous nous focalisons sur des tâches dont la résolution est hautement opportuniste et ne peut pas être planifiée.
Nous introduisons un modèle de représentation de la tâche s'appuyant sur les jeux de dialogue, des motifs d'interaction dialogique permettant de décrire la structure de l'interaction à l'aide d'enchaînements d'actes de dialogue conventionnellement acceptables.
Nous organisons ces motifs dialogiques en états, des structures décrivant les comportements attendus de la part de chaque interlocuteur au cours des différentes sous-tâches de la tâche collaborative.
Ces états regroupent les motifs dialogiques en un ensemble cohérent vis-à-vis de la sous-tâche à laquelle ils sont associés et les enrichissent avec des règles localement cohérentes.
Celles-ci permettent de décrire les effets de la réalisation d'un motif dialogique par les interlocuteurs dans un contexte particulier de la tâche.
Les états permettent au système qui les utilise de lier un énoncé de l'humain à l'état actuel du dialogue et de la tâche et d'initier des comportements cohérents avec l'interaction et constructifs pour la tâche.
Les décisions prises par le système sont basées sur la notion de maturité, une valeur associée à chaque état représentant la capacité du système à prendre des initiatives dans cet état.
Le modèle décisionnel du système est conçu pour être résilient et laisser un maximum de liberté à l'utilisateur.
Ce modèle est implémenté dans CoCoA, un système qui collabore avec un humain pour l'assister lors de la réalisation d'une tâche complexe.
À partir de l'étude d'un corpus de dialogues humain-humain sur une tâche de recherche d'information collaborative médicale, nous décrivons la tâche à l'aide de notre modèle.
Ce cas d'utilisation est implémenté à l'aide de CoCoA et une évaluation est réalisée en simulant le comportement d'un utilisateur ayant un besoin d'information et en faisant varier ses actions à partir de comportements identifiés dans le corpus.
Cette thèse propose une analyse morphosyntaxique, prosodique et conversationnelle des marqueurs discursifs dans le débat présidentiel et dans le talkshow politique télévisuel.
Les marqueurs discursifs, en corrélation avec la gestualité et l'intonation, participent considérablement à la construction de l'échange entre les différents participants au débat.
Pour l'analyse, l'approche proposée par Haselow (2017) a été adoptée car elle émane de la prise en compte de toutes les autres approches citées plus haut tout en y ajoutant les théories cognitives.
Sur le plan de l'organisation de l'interaction pendant le débat présidentiel ou les talkshows,les marqueurs discursifs, accompagnés de la gestualité et de l'intonation, jouent un rôle important dans la gestion des tours de parole et dans l'organisation séquentielle des actions.
Sur le plan énonciatif, ils interviennent, accompagnés des gestes manuels et du regard, dans l'expression de l'emphase, de la reformulation et de l'opposition.
Ils permettent également aux candidats d'attirer l'attention de leurs interlocuteurs dans le but d'introduire une justification, un changement d'optique, un discours rapporté, ou dans l'optique d'interrompre ces derniers, ou tout simplement pour rassurer les potentiels électeurs.
Ils permettent aussi aux candidats d'anticiper ou de prendre en compte le point de vue de leur interlocuteurs (passifs ou actifs), en s'inscrivant ou en se positionnant dans une logique d'acceptation ou de rupture avec ces derniers.
Vidéo d'interprétation et de compréhension est l'un des objectifs de recherche à long terme dans la vision par ordinateur.
Toutefois, afin de déployer des systèmes de reconnaissance visuelle à grande échelle dans la pratique, il devient important d'aborder l'évolutivité des techniques.
L'objectif principal est cette thèse est de développer des méthodes évolutives pour l'analyse de contenu vidéo (par exemple pour le classement ou la classification).
Restorer la faculté de parler chez des personnes paralysées et aphasiques pourrait être envisagée via l'utilisation d'une interface cerveau
L'objectif de cette thèse était de développer trois aspects nécessaires à la mise au point d'une telle preuve de concept.
Premièrement, un synthétiseur permettant de produire en temps-réel de la parole intelligible et controlé par un nombre raisonable de paramètres est nécessaire.
Nous avons donc développé un synthétiseur produisant de la parole intelligible à partir de données articulatoires.
Dans un premier temps, nous avons enregistré un large corpus de données articulatoire et acoustiques synchrones chez un locuteur.
Ensuite, nous avons utilisé des techniques d'apprentissage automatique, en particulier des réseaux de neurones profonds, pour construire un modèle permettant de convertir des données articulatoires en parole.
Ce synthétisuer a été construit pour fonctionner en temps réel.
Enfin, comme première étape vers un contrôle neuronal de ce synthétiseur, nous avons testé qu'il pouvait être contrôlé en temps réel par plusieurs locuteurs, pour produire de la parole inetlligible à partir de leurs mouvements articulatoires dans un paradigme de boucle fermée.
Deuxièmement, nous avons étudié le décodage de la parole et de ses propriétés articulatoires à partir d'activités neuronales essentiellement enregistrées dans le cortex moteur de la parole.
Nous avons construit un outil permettant de localiser les aires corticales actives, en ligne pendant des chirurgies éveillées à l'hôpital de Grenoble, et nous avons testé ce système chez deux patients atteints d'un cancer du cerveau.
Les résultats ont montré que le cortex moteur exhibe une activité spécifique pendant la production de parole dans les bandes beta et gamma du signal, y compris lors de l'imagination de la parole.
Les données enregistrées ont ensuite pu être analysées pour décoder l'intention de parler du sujet (réelle ou imaginée), ainsi que la vibration des cordes vocales et les trajectoires des articulateurs principaux du conduit vocal significativement au dessus du niveau de la chance.
Enfin, nous nous sommes intéressés aux questions éthiques qui accompagnent le développement et l'usage des interfaces cerveau
Les documents papiers sont à la base de nos connaissances et renferment une myriade d'informations dont certaines sont très précieuses pour notre société.
Dans un but de préservation et afin de les rendre plus accessibles, de nombreux projets de numérisation visent à convertir ce type de documents en textes numérisés, notamment en utilisant des logiciels de reconnaissance optique de caractères (OCR).
Ainsi, une œuvre sera numérisée page par page, ce qui ne correspond généralement qu'à une organisation physique et pas à l'intention rédactionnelle des auteurs.
Un second verrou du processus de numérisation, qui en est également le plus important, correspond aux performances des moteurs d'OCR.
En effet, celles-ci sont substantiellement réduites pour les documents patrimoniaux qui ont généralement subis des dégradations.
Les erreurs d'OCR que cela induit ont un impact non négligeable sur la performance des outils de recherches et sur les systèmes de traitement du langage naturel puisqu'il faut par exemple apparier des besoins bien écrits à des textes mal reconnus.
Cette thèse a pour objectif de faciliter l'accès aux documents historiques numérisés en étudiant les problèmes précédemment mentionnés.
En vue de faciliter l'accès aux documents historiques, plusieurs approches sont proposées, visant à reconstruire les structures logiques des ouvrages et à améliorer la qualité des textes numérisés par OCR.
Nos expériences ont démontré que cette approche surpasse l'état de l'art.
La contribution majeure de cette thèse fournit, quant à elle, des méthodes pour la détection et la correction des erreurs d'OCR.
Les caractéristiques communes et divergentes entre les erreurs d'OCR et celles des utilisateurs sont clarifiées pour mieux concevoir les traitements post-OCR.
Normalement, un système de post-traitement détecte et rectifie les erreurs résiduelles.
Toutefois, il peut être préférable de gérer ces erreurs séparément grâce à des applications qui permettent de filtrer, d'étiqueter, ou de traiter sélectivement de telles données.
Les résultats montrent que les performances de nos méthodes sont comparables à plusieurs méthodes de référence sur des jeux de données en anglais utilisés lors des deux premières éditions de la compétition sur la correction des textes post-OCR organisée durant les conférence ICDAR en 2017 et 2019.
Les médias sociaux ont changé notre manière de communiquer entre individus, au sein des organisations et des communautés.
La disponibilité de ces données sociales ouvre de nouvelles opportunités pour comprendre et influencer le comportement des utilisateurs.
De ce fait, la fouille des médias sociaux connait un intérêt croissant dans divers milieux scientifiques et économiques.
Dans cette thèse, nous nous intéressons spécifiquement aux utilisateurs de ces réseaux et cherchons à les caractériser selon deux axes : (i) leur expertise et leur réputation et (ii) les sentiments qu'ils expriment.
De manière classique, les données sociales sont souvent fouillées selon leur structure en réseau.
Cependant, le contenu textuel des messages échangés peut faire émerger des connaissances complémentaires qui ne peuvent être connues via la seule analyse de la structure.
Jusqu'à récemment, la majorité des travaux concernant l'analyse du contenu textuel était proposée pour l'Anglais.
L'originalité de cette thèse est de développer des méthodes et des ressources basées sur le contenu pour la fouille des réseaux sociaux pour la langue Française.
Dans le premier axe, nous proposons d'abord d'identifier l'expertise des utilisateurs.
Pour cela, nous avons utilisé des forums qui recrutent des experts en santé pour apprendre des modèles de classification qui servent à identifier les messages postés par les experts dans n'importe quel autre forum.
Nous démontrons que les modèles appris sur des forums appropriés peuvent être utilisés efficacement sur d'autres forums.
Puis, dans un second temps, nous nous intéressons à la réputation des utilisateurs dans ces forums.
L'idée est de rechercher les expressions de confiance et de méfiance exprimées dans les messages, de rechercher les destinataires de ces messages et d'utiliser ces informations pour en déduire la réputation des utilisateurs.
Nous proposons une nouvelle mesure de réputation qui permet de pondérer le score de chaque réponse selon la réputation de son auteur.
Des évaluations automatiques et manuelles ont démontré l'efficacité de l'approche.
Dans le deuxième axe, nous nous sommes focalisés sur l'extraction de sentiments (polarité et émotion).
Pour cela, dans un premier temps, nous avons commencé par construire un lexique de sentiments et d'émotions pour le Français que nous appelons FEEL (French Expanded Emotion Lexicon).
Ce lexique est construit de manière semi-automatique en traduisant et en étendant son homologue Anglais NRC EmoLex.
Nous avons ensuite comparé FEEL avec les lexiques Français de la littérature sur des benchmarks de référence.
Les résultats ont montré que FEEL permet d'améliorer la classification des textes Français selon leurs polarités et émotions.
Dans un deuxième temps, nous avons proposé d'évaluer de manière assez exhaustive différentes méthodes et ressources pour la classification de sentiments en Français.
Les expérimentations menées ont permis de déterminer les caractéristiques utiles dans la classification de sentiments pour différents types de textes.
Les systèmes appris se sont montrés particulièrement efficaces sur des benchmarks de référence.
De manière générale, ces travaux ont ouvert des perspectives prometteuses sur diverses tâches d'analyse des réseaux sociaux pour la langue française incluant : (i) combiner plusieurs sources pour transférer la connaissance sur les utilisateurs des réseaux sociaux;
Les méthodes à noyaux sont connues pour être efficaces pour l'analyse d'objets complexes en les plongeant implicitement dans un espace de caractéristiques (feature
Ce travail propose une méthode d'estimation de la pré-image pour rendre interprétable les méthodes d'analyse de séries temporelles à base de noyaux.
Dans la première étape, une fonction de déformation temporelle, supervisée par des contraintes de distances, est définie pour plonger les séries dans un espace métrique où des analyses pratiques peuvent être menées.
Dans la deuxième étape, l'estimation de la pré-image des séries temporelles est obtenue par l'apprentissage d'une transformation linéaire (ou non linéaire) assurant une isométrie locale entre le nouvel espace métrique des séries et l'espace des caractéristiques.
La méthode proposée est comparée aux méthodes de l'état de l'art au travers de trois tâches principales requérant l'estimation de la pré-image : 1) le centrage des séries temporelles, 2) la reconstruction et le débruitage des séries temporelles et 3) l'apprentissage de représentations pour des séries temporelles.
Ensuite, tout en introduisant les jeux de données qui ont aussi bien servi lors des études empiriques que motivé les choix de modélisation, nous essayons de donner des informations intéressantes sur l'état du marché des couvertures de défaillance peu connu du grand public sinon pour son rôle lors de la crise financière mondiale de 2007-2008.
De nouvelles distances entre séries temporelles financières prenant mieux en compte leur nature stochastique et pouvant être mis à profit dans les méthodes de partitionnement automatique existantes sont proposées.
Nous étudions empiriquement leur impact sur les résultats.
Les résultats de ces études peuvent être consultés sur www.datagrapple.com.
Les compétences linguistiques sont fondamentales à l'exercice du métier de traducteur.
La connaissance de la culture d'origine et d'arrivée est elle aussi d'une importance primordiale pour transmettre la richesse d'un ouvrage.
La question de la place du culturel dans la traduction est abordée par les chercheurs en traductologie depuis une trentaine d'années, et les critères définis sont de plus en plus précis.
Cependant, le couple spécifique que constitue le polonais vers le français n'est encore que peu étudié.
Nous nous interrogeons donc sur l'expression des connaissances culturelles et contextuelles dans la traduction lors du passage du polonais vers le français.
Pour ce faire, nous nous appuyons sur un corpus issu majoritairement des littératures de grande diffusion : notre questionnement porte sur la place de la culture, tant individuelle que nationale, quand l'objectif premier du texte est autre que culturel, ainsi que sur l'importance apportée à sa traduction.
Notre réflexion s'appuie sur les oeuvres d'un grand auteur de fantasy, Andrzej Sapkowski, Polonais à la renommée mondiale,et aux traductions ver s le français de ses oeuvres.
Nos observations sont par ailleurs complétées par une analyse de la même problématique dans le polar et le roman historique.
La question que nous posons est la suivante : dans des textes polonais dont l'objectif premier n'est pas la transmission de la culture, mais le divertissement du public, quels types de connaissances culturelles et contextuelles sont présents, quelle place occupent-ils et comment sont-ils transmis en français ?
Nous nous demanderons également quelle est l'incidence s'ils sont occultés.
Afin de répondre à ces interrogations, nous proposons une classification des références culturelles et contextuelles qui correspondent à une typologie possible du culturème dans la traduction polono-française, applicable à d'autres littératures que celles de grande diffusion.
Pour chacune des catégories et sous-catégories distinguées, nous analysons des exemples précis d'application dans notre corpus.
Ceci nous permet de mettre à jour les pratiques les plus courantes dans la traduction des littératures de grande diffusion, et peut-être d'envisager d'autres approches.
Cette etude, portant sur l'analyse litteraire assistee par ordinateur, se situe au carrefour de trois differentes disciplines : la litterature, la linguistique et l'informatique.
Presque tous les internautes sont susceptibles d'être victimes de clickbait, supposant à tort qu'il s'agit d'informations légitimes.
Un type important de clickbait se présente sous la forme de spam et de publicités qui sont utilisés pour rediriger les utilisateurs vers des sites web.
Un autre type de "clickbait" est conçu pour faire la une des journaux et rediriger les lecteurs vers leurs sites en ligne, mais ces nouvelles sensationnelles peuvent être trompeuses.
Dans cette thèse, on propose deux approches innovantes pour explorer le clickbait généré par les médias d'information dans les médias sociaux.
La recherche d'images basée sur le contenu visuel est un domaine très actif de la vision par ordinateur, car le nombre de bases d'images disponibles ne cesse d'augmenter.
L'objectif de ce type d'approche est de retourner les images les plus proches d'une requête donnée en terme de contenu visuel.
Notre travail s'inscrit dans un contexte applicatif spécifique qui consiste à indexer des petites bases d'images expertes sur lesquelles nous n'avons aucune connaissance a priori.
L'une de nos contributions pour palier ce problème consiste à choisir un ensemble de descripteurs visuels et de les placer en compétition directe.
Nous utilisons deux stratégies pour combiner ces caractéristiques : la première, est pyschovisuelle, et la seconde, est statistique.
Dans ce contexte, nous proposons une approche adaptative non supervisée, basée sur les sacs de mots et phrases visuels, dont le principe est de sélectionner les caractéristiques pertinentes pour chaque point d'intérêt dans le but de renforcer la représentation de l'image.
Les tests effectués montrent l'intérêt d'utiliser ce type de méthodes malgré la domination des méthodes basées réseaux de neurones convolutifs dans la littérature.
Nous proposons également une étude, ainsi que les résultats de nos premiers tests concernant le renforcement de la recherche en utilisant des méthodes semi-interactives basées sur l'expertise de l'utilisateur.
Les progrès des technologies informatiques et l'augmentation continue des capacités de stockage ont permis de disposer de masses de données de trés grandes tailles et de grandes dimensions.
Le volume et la nature même des données font qu'il est de plus en plus nécessaire de développer de nouvelles méthodes capables de traiter, résumer et d'extraire l'information contenue dans de tels types de données.
D'un point de vue extraction des connaissances, la compréhension de la structure des grandes masses de données est d'une importance capitale dans l'apprentissage artificiel et la fouille de données.
En outre, contrairement à l'apprentissage supervisé, l'apprentissage non supervisé peut fournir des outils pour l'analyse de ces ensembles de données en absence de groupes (classes).
Dans cette thèse, nous nous concentrons sur des méthodes fondamentales en apprentissage non supervisé notamment les méthodes de réduction de la dimension, de classification simple (clustering) et de classification croisée (co-clustering).
Notre contribution majeure est la proposition d'une nouvelle manière de traiter simultanément la classification et la réduction de dimension.
L'idée principale s'appuie sur une fonction objective qui peut être décomposée en deux termes, le premier correspond à la réduction de la dimension des données, tandis que le second correspond à l'objectif du clustering et celui du co-clustering.
Nous avons en outre proposé des versions régularisées de nos approches basées sur la régularisation du Laplacien afin de mieux préserver la structure géométrique des données.
Enfin, nous avons aussi étudié comment intégrer des contraintes dans les Laplaciens utilisés pour la régularisation à la fois dans l'espace des objets et l'espace des variables.
De cette façon, nous montrons comment des connaissances a priori peuvent contribuer à l'amélioration de la qualité du co-clustering.
Cette thèse de doctorat aborde les problématiques de l'estimation de confiance pour la traduction automatique, et de la traduction automatique statistique de la parole spontanée à grand vocabulaire.
Je propose une évaluation des performances des différentes méthodes évoquées, présente les résultats obtenus lors d'une campagne d'évaluation internationale et propose une application à la post-édition par des experts de documents traduits automatiquement.
J'aborde ensuite le problème de la traduction automatique de la parole.
Après avoir passé en revue les spécificités du medium oral et les défis particuliers qu'il soulève, je propose des méthodes originales pour y répondre, utilisant notamment les réseaux de confusion phonétiques, les mesures de confiances et des techniques de segmentation de la parole.
Le déploiement massif de ressources d'énergie renouvelable distribuée (RED) représente une opportunité majeur pour atteindre les objectifs de réduction des émissions de carbone en Europe, mais aussi dans le monde entier.
Visant à mobiliser des capitaux publics et privés, plusieurs plans ont été développés pour placer les clients finaux au cœur de la transition énergétique, dans l'espoir d'accélérer l'adoption de l'énergie verte en augmentant son attractivité et sa rentabilité.
Pour ce faire, nous concevons un cadre permettant de concevoir et de comparer divers paradigmes « d'investissements partagés et d'échanges monétisés locaux de l'énergie » , dont le potentiel de « gains » se traduit par une incitation forte à leur mise en œuvre.
Dans le cadre d'échanges monétisés locaux d'énergie, nous étudions les interactions entre prosommateurs (consommateurs avec capacité de production et éventuellement de stockage) situés dans le même réseau Basse Tension, éventuellement derrière le même départ.
Dans nos systèmes, ces prosommateurs seront toujours connectés au réseau électrique principal et ils auront la possibilité, comme ils le font aujourd'hui, d'acheter et de vendre à un opérateur de services de distribution d'électricité, suivant une politique tarifaire connue à l'avance (un taux forfaitaire ou un temps d'utilisation, pour exemple).
Dans la première partie de la thèse, nous étudions des modèles concurrentiels dans lesquels les prosommateurs vendent leur surplus à leurs voisins via un marché local d'énergie.
Nous analysons différents types de marchés et donc différentes stratégies que les acteurs pourraient utiliser pour participer à ces marchés, ainsi que leur impact sur le réseau électrique et sur le gestionnaire du réseau de distribution.
Dans la deuxième partie de la thèse, nous explorons les incitations qui peuvent être mises en œuvre par la coopération.
À cet égard, nous utilisons la théorie des jeux coopératifs pour modéliser l'investissement partagé dans l'acquisition de dispositifs de stockage énergie et de panneaux photovoltaïques (PV) par un groupe de prosommateurs.
Cette thèse, effectuée dans le cadre d'un contrat CIFRE avec la société OctopusMind, est centrée sur le développement d'un outillage informatique dédié et optimisé pour l'assistance à l'exploitation de la base d'appels d'offres, dans une finalité de veille stratégique.
Elle contient plus de deux millions de documents traduits dans 24 langues publiées durant les 9 dernières années.
Le deuxième chapitre concerne une étude sur les questions de vectorisation de mots, phrases et documents susceptibles de capturer au mieux la sémantique selon différentes échelles.
Nous avons proposé deux approches : la première est basée sur une combinaison entre word2vec et LSA.
La deuxième est basée sur une architecture neuronale originale basée sur des réseaux d'attention convolutionnels à deux niveaux.
Ces vectorisations sont exploitées à titre de validation sur des tâches de classification et de clustering de textes.
La fin de ce chapitre concerne la mise en production dans l'environnement logiciel d'OctopusMind des différentes solutions, notamment l'extraction d'informations, le système de recommandation, ainsi que la combinaison de ces différents modules pour résoudre des problèmes plus complexes
L'opérationnalisation des terminologies à des fins de traitement de l'information requière une représentation computationnelle du système conceptuel.
La théorie du concept sur laquelle se fonde la Terminologie ISO ne permet pas aujourd'hui une telle représentation informatique [Roche TKE 2012].
Même si les normes ISO sur la Terminologie n'ont pas pour objectif l'opérationnalisation de terminologies mais la communication entre humains, elles doivent pouvoir – elles le précisent – servir à la modélisation des informations et des données [ISO 704 : 2009].
Les résultats de disciplines telles que l'ingénierie des connaissances ont permis de mettre en évidence la nécessité de disposer d'une théorie du concept qui puisse donner lieu à une représentation informatique.
Dans ce cadre, les ontologies, issues de l'ingénierie des connaissances, constituent une des perspectives les plus intéressantes pour la modélisation du système conceptuel d'une terminologie [Roche 2015, Handbook of Terminology].
Cette thèse présente de nouveaux outils informatiques pour quantifier les déformations et le mouvement de structures anatomiques à partir d'images médicales dans le cadre d'une grande variété d'applications cliniques.
Des outils génériques de recalage déformable sont présentés qui permettent l'analyse de la déformation de tissus anatomiques pour améliorer le diagnostic, le pronostic et la thérapie.
Ces outils combinent des méthodes avancées d'analyse d'images médicales avec des méthodes d'apprentissage automatique performantes.
Dans un premier temps, nous nous concentrons sur les problèmes de recalages inter-sujets difficiles.
Dans un second temps, nous développons un modèle de déformation difféomorphe qui permet un recalage multi-échelle précis et une analyse de déformation en apprenant une représentation de faible dimension des déformations intra-sujet.
La méthode non supervisée utilise un modèle de variable latente sous la forme d'un autoencodeur variationnel conditionnel (CVAE) pour apprendre une représentation probabiliste des déformations qui est utile pour la simulation, la classification et la comparaison des déformations.
Troisièmement, nous proposons un modèle de mouvement probabiliste dérivé de séquences d'images d'organes en mouvement.
Ce modèle génératif décrit le mouvement dans un espace latent structuré, la matrice de mouvement, qui permet le suivi cohérent des structures ainsi que l'analyse du mouvement.
Ainsi cette approche permet la simulation et l'interpolation de modèles de mouvement réalistes conduisant à une acquisition et une augmentation des données plus rapides.
Enfin, nous démontrons l'intérêt des outils développés dans une application clinique où le modèle de mouvement est utilisé pour le pronostic de maladies et la planification de thérapies.
Il est démontré que le risque de survie des patients souffrant d'insuffisance cardiaque peut être prédit à partir de la matrice de mouvement discriminant avec une précision supérieure par rapport aux facteurs de risque classiques dérivés de l'image.
La recherche d'experts consiste en l'identification d'un ensemble d'individus que l'on considère comme experts d'une thématique particulière.
Il s'agit d'une problématique essentielle dans le milieu académique.
En effet, il est constamment nécessaire d'identifier des chercheurs appropriés lors de la constitution de comités de lecture ou d'évaluation de projets de recherche, par exemple.
De même, il est particulièrement utile d'identifier automatiquement les experts d'un domaine de recherche à partir de la littérature scientifique.
Nous proposons une approche de découverte et d'enrichissement de connaissances basée sur une annotation sémantique des articles scientifiques, sur leur représentation sous forme de réseaux de collaboration scientifique et leur fouille à l'aide d'une méthode d'abstraction de graphe.
Cette méthode permet de se focaliser sur les zones denses des réseaux et de découvrir des experts et leurs expertises associées à l'aide de contraintes de connectivité.
Ces dernières permettent de prendre en compte une validation par les pairs, matérialisée par la densité des relations de collaboration scientifique que les individus entretiennent entre eux.
Nous expérimentons notre approche sur un corpus de publications scientifiques, proposons une méthode d'évaluation originale de nos résultats et comparons nos performances aux méthodes de recherche d'experts implémentées dans le cadre d'évaluation LT ExpertFinder.
Nous obtenons des performances supérieures à l'état de l'art et découvrons que les indicateurs d'expertise les plus déterminants sont la rédaction d'articles fortement cités mais également la capacité à citer la littérature scientifique appropriée.
Le succès de l'apprentissage profond de ces dernières années dépend en grande partie de grands jeux de données annotées, extrêmement couteux à collecter et disponible en quantité limitée.
Les algorithmes d'apprentissage auto-supervisés sont devenus populaires ces 10 dernières années dans les communautés de la vision artificielle, l'apprentissage machine, et du traitement du langage naturel.
Ils bénéficient des avancées en apprentissage supervisé mais nécessite beaucoup moins d'intervention humaine, car la supervision provient des données elles-mêmes (par exemple, prédire les frames manquantes d'une vidéo) sans annotation manuelle supplémentaire.
Une alternative prometteuse est de considérer une famille de techniques qui mesurent les contradictions entre entrées prédites et réelles en utilisant une fonction paramétrée par une variable latente, régularisée pour éviter les solutions triviales.
En particulier, le modèle implicite correspondant permet de réaliser des prédictions multi-modales qui reflètent les aspects non déterministes du monde réel.
Cette thèse développera de nouvelles techniques adaptant ce cadre général à la vision artificielle et les appliquera à des tâches telles que l'apprentissage auto-supervisé de fonctions de photocohérence pour la stéréo multi-vues et de modèles d'objets pour la reconnaissance visuelle.
Alors que les méthodes de reconnaissance visuelle sont de plus en plus évoluées, la communauté scientifique s'intéresse désormais à des systèmes aux capacités de raisonnement plus poussées.
Dans cette thèse, nous nous intéressons au Visual Question Answering (VQA), qui consiste en la conception de systèmes capables de répondre à une question portant sur une image.
Ce problème difficile est habituellement abordé par des techniques d'apprentissage profond.
Dans la première partie de cette thèse, nous développons des stratégies de fusion multimodales permettant de modéliser des interactions entre les représentations d'image et de question.
Nous explorons des techniques de fusion bilinéaire, et assurons l'expressivité et la simplicité des modèles en utilisant des techniques de factorisation tensorielle.
Dans la seconde partie, on s'intéresse au raisonnement visuel qui encapsule ces fusions.
Après avoir présenté les schémas classiques d'attention visuelle, nous proposons une architecture plus avancée qui considère les objets ainsi que leurs relations mutuelles.
Tous les modèles sont expérimentalement évalués sur des jeux de données standards et obtiennent des résultats compétitifs avec ceux de la littérature.
L'évaluation de requêtes sur des données probabilistes(probabilistic query evaluation, ou PQE) est généralement très coûteuse en ressources et ce même à requête fixée.
Bien que certaines restrictions sur les requêtes et les données aient été proposées pour en diminuer la complexité, les résultats existants ne s'appliquent pas à la complexité combinée, c'est-à-dire quand la requête n'est pas fixe.
Ma thèse s'intéresse à la question de déterminer pour quelles requêtes et données l'évaluation probabiliste est faisable en complexité combinée.
La première contribution de cette thèse est d'étudier PQE pour des requêtes conjonctives sur des schémas d'arité 2.
Nous imposons que les requêtes et les données aient la forme d'arbres et montrons l'importance de diverses caractéristiques telles que la présence d'étiquettes sur les arêtes, les bifurcations ou la connectivité.
Les restrictions imposées dans ce cadre sont assez sévères, mais la deuxième contribution de cette thèse montre que si l'on est prêts à augmenter la complexité en la requête, alors il devient possible d'évaluer un langage de requête plus expressif sur des données plus générales.
Plus précisément, nous montrons que l'évaluation probabiliste d'un fragment particulier de Datalog sur des données de largeur d'arbre bornée peut s'effectuer en temps linéaire en les données et doublement exponentiel en la requête.
Ce résultat est prouvé en utilisant des techniques d'automates d'arbres et de compilation de connaissances.
La troisième contribution de ce travail est de montrer les limites de certaines de ces techniques, en prouvant des bornes inférieures générales sur la taille de formalismes de représentation utilisés en compilation de connaissances et en théorie des automates.
L'enjeu de la normalisation du reporting RSE occupe une place croissante dans la communication des entreprises à l'égard de leurs parties prenantes.
La norme GRI constitue, à l'heure actuelle, le référentiel de reporting RSE le plus utilisé à l'échelle internationale pour la rédaction des rapports RSE.
Le corpus G3C constitué est représentatif du genre discursif «
De nombreuses données médicales au format électronique sont produites dans les établissements de santé au moment du soin.
Ces données sont riches en information et présentent un intérêt majeur pour la recherche médicale.
Il est cependant très difficile pour un chercheur de réutiliser les données collectées durant le soin.
En 2015, le CHU de Bordeaux a mis en place un entrepôt de données clinique basé sur la solution i2b2 pour faciliter la réutilisation des données.
Même intégrées dans un entrepôt, de nombreux verrous restent encore à résoudre comme la présence de données textuelles et une hétérogénéité sémantique des différentes sources de données.
L'architecture système cherche à se distinguer de son domaine d'origine, l'ingénierie système, en devenant un domaine émergent.
Loin d'être reconnue en tant que science ou discipline à proprement parler, sa pratique est de plus en plus répandue de nos jours.
Cependant, cette pratique reste encore peu formalisée et peu enseignée, faute d'un corpus de connaissances, de techniques ou de démarches établi et accessible.
Notre thèse contribue à combler ce manque en proposant un paradigme de la conception architecturale des systèmes artificiels complexes.
Ce dernier est construit en se basant sur des paradigmes existants, en les combinant, puis en les complétant.
Il vise à doter l'architecte de systèmes artificiels complexes d'un cadre opérant, voire performatif.
Il se traduit par une structuration de la démarche de conception en quatre niveaux.
Un niveau dit archétypal condense les grands principes de toute démarche de conception architecturale de systèmes artificiels complexes.
Ces principes sont dérivés de diverses démarches déjà appliquées, principalement à la conception de systèmes ou de produits, mais également à la conception architecturale de bâtiments.
Cette vision impacte directement la nature des artéfacts sur lesquels il travaille.
Nous proposons ensuite d'agréger ces artéfacts en des modèles, reflétant soit sa perception du présent, soit son élaboration des futurs, évoluant suivant des processus identifiés.
Un niveau dit particulier a pour objectif de permettre la narration d'une conception particulière.
Nous proposons pour cela une notation de la conception.
De nombreux efforts ont été faits ces dernières années pour faciliter la gestion et la représentation des entités culturelles.
Toutefois, il existe encore un grand nombre de systèmes souvent isolés et encore utilisés dans les institutions culturelles reposant sur des modèles non sémantiques qui rendent difficile la validation et l'enrichissement des données.
Cette thèse a pour but de proposer de nouvelles solutions pour améliorer la représentation et l'enrichissement sémantique de données culturelles en utilisant les principes du Web Sémantique.
Toutefois, la qualité d'une telle transformation est cruciale et c'est pourquoi des améliorations doivent être faites au niveau de la configuration et de l'évaluation d'un tel processus.
Cependant, l'agrégation d'informations depuis des sources hétérogènes implique des étapes d'alignement à la fois au niveau du schéma et au niveau des entités
Les collections de manuscrits sur feuilles de palmier sont devenues une partie intégrante de la culture et de la vie des peuples de l'Asie du Sud-Est.
Avec l'augmentation des projets de numérisation des documents patrimoniaux à travers le monde, les collections de manuscrits sur feuilles de palmier ont finalement attiré l'attention des chercheurs en analyse d'images de documents (AID).
Les travaux de recherche menés dans le cadre de cette thèse ont porté sur les manuscrits d'Indonésie, et en particulier sur les manuscrits de Bali.
Nos travaux visent à proposer des méthodes d'analyse pour les manuscrits sur feuilles de palmier.
En effet, ces collections offrent de nouveaux défis car elles utilisent, d'une part, un support spécifique : les feuilles de palmier, et d'autre part, un langage et un script qui n'ont jamais été analysés auparavant.
Ces systèmes rendront ces manuscrits plus accessibles, lisibles et compréhensibles à un public plus large ainsi que pour les chercheurs et les étudiants du monde entier.
Cette thèse a permis de développer un système d'AID pour les images de documents sur feuilles de palmier, comprenant plusieurs tâches de traitement d'images : numérisation du document, construction de la vérité terrain, binarisation, segmentation des lignes de texte et des glyphes, la reconnaissance des glyphes et des mots, translittération et l'indexation de document.
Nous avons également développé un système de reconnaissance des glyphes et un système de translittération automatique des manuscrits balinais.
Cette thèse propose un schéma complet de reconnaissance de glyphes spatialement catégorisé pour la translittération des manuscrits balinais sur feuilles de palmier.
Le schéma proposé comprend six tâches : la segmentation de lignes de texte et de glyphes, un processus de classification de glyphes, la détection de la position spatiale pour la catégorisation des glyphes, une reconnaissance globale et catégorisée des glyphes, la sélection des glyphes et la translittération basée sur des règles phonologiques.
La translittération automatique de l'écriture balinaise nécessite de mettre en œuvre des mécanismes de représentation des connaissances et des règles phonologiques.
Dans cette thèse, j'étudie la manière dont les ressources lexicales basées sur l'organisation de la connaissance lexicale dans des classes qui partagent des propriétés communes (syntactiques, sémantiques, etc.) permettent le traitement automatique de la langue naturelle et en particulier la reconnaissance symbolique d'implications textuelles.
Tout d'abord, je présente une approche robuste et à large couverture sur la reconnaissance de paraphrases verbales lexico-structurelle basée sur la classification de verbes anglais par Levin (1993).
Puis, je montre qu'en étendant le cadre proposé par Levin pour traiter les modèles d'inférence généraux, on obtient une classification d'adjectifs anglais qui, comparée à des approches antérieures, propose une caractérisation sémantique à grain plus fin de leurs propriétés déductives.
De plus, je développe un cadre sémantique compositionnel pour assigner à des adjectifs une représentation sémantique sur la base d'une approche ontologiquement variée (Hobbs, 1985) et qui permet ainsi l'inférence de premier ordre pour tous les types d'adjectifs, y compris les adjectifs extensionnels.
Enfin, je présente un corpus de test pour l'inférence basée sur les adjectifs que j'ai développée comme ressource pour l'évaluation de systèmes de traitement automatique de l'inférence de la langue naturelle.
Avec l'augmentation exponentielle de nombre d'images disponibles sur Internet, le besoin en outils efficaces d'indexation et de recherche d'images est devenu important.
Dans cette thèse, nous nous baserons sur le contenu visuel des images comme source principale d'informations pour leur représentation.
Dans un premier temps, nous améliorons l'approche des sacs de mots visuels en caractérisant la constitution spatio-colorimétrique d'une image par le biais d'un mélange de n Gaussiennes dans l'espace de caractéristiques.
Cela permet de proposer un nouveau descripteur de contour qui joue un rôle complémentaire avec le descripteur SURF.
Dans un deuxième temps, nous introduisons un nouveau modèle probabiliste basé sur les catégories : le modèle MSSA (Multilayer Semantic Significance Analysis ou Analyse multi-niveaux de la pertinence sémantique) dans le but d'étudier la sémantique des mots visuels construits.
Ce modèle permet de construire des mots visuels sémantiquement cohérents (SSVW - Semantically Significant Visual Word).
Enfin, nous proposons un nouveau schéma de pondération spatiale ainsi qu'un classifieur multi-classes basé sur un vote.
Nos résultats expérimentaux extensifs démontrent que la représentation visuelle proposée permet d'atteindre de meilleures performances comparativement aux représentations traditionnelles utilisées dans le domaine de la recherche, la classification et de la reconnaissance d'objets.
Notre thèse se situe dans le contexte du projet ANR GEONTO qui porte sur la constitution, l'alignement, la comparaison et l'exploitation d'ontologies géographiques hétérogènes.
Dans ce contexte, notre objectif est d'extraire automatiquement des termes topographiques à partir des récits de voyage afin d'enrichir une ontologie géographique initialement conçue par l'IGN.
La méthode proposée permet de repérer et d'extraire des termes à connotation topographiques contenus dans un texte.
Notre méthode est basée sur le repérage automatique de certaines relations linguistiques afin d'annoter ces termes.
Sa mise en œuvre s'appuie sur le principe des relations n-aires et passe par l'utilisation de méthodes ou de techniques de TAL (Traitement Automatique de la Langue).
Il s'agit de relations n-aires entre les termes à extraire et d'autres éléments du textes qui peuvent être repérés à l'aide de ressources externes prédéfinies, telles que des lexiques spécifiques : les verbes de récit de voyage (verbes de déplacement, verbes de perceptions, et verbes topographiques), les pré-positions (prépositions de lieu, adverbes, adjectifs), les noms toponymiques, des thésaurus génériques, des ontologies de domaine (ici l'ontologie géographique initialement conçue par l'IGN).
Le point fort de notre approche est que la méthode proposée permet d'extraire non seulement des termes rattachés directement aux noms toponymiques mais également dans des structures de phrase où d'autres termes s'intercalent.
L'expérimentation sur un corpus comportant 12 récits de voyage (2419 pages, fournit par la médiathèque de Pau) a montré que notre méthode est robuste.
En résultat, elle a permis d'extraire 2173 termes distincts dont 1191 termes valides, soit une précision de 0,55.
Cela démontre que l'utilisation des relations proposées est plus efficace que celle des couples (termes, nom toponymique)(qui donne 733 termes distincts valides avec une précision de 0,38).
Notre méthode peut également être utilisée pour d'autres applications telles que la reconnaissance des entités nommées géographiques, l'indexation spatiale des documents textuels.
L'intégration des technologies numériques dans l'éducation (TICE) est souvent décrite comme difficile.
Parce que leur déploiement est généralisé et leur utilisation rendue obligatoire par l'institution, les environnements numériques de travail (ENT) constituent un outil à part dans le champ des TICE.
Ce travail propose d'investiguer comment cet outil a intégré la pensée professionnelle enseignante à partir d'une approche reposant sur la théorie des représentations sociales.
Nous proposons par une approche lexicométrique d'identifier les thématiques transversales à ces corpus et celles qui sont spécifiques de chacun de ces types de discours.
Le second temps de ce travail repose sur une enquête par questionnaire menée auprès de 625 enseignants du secondaire de l'académie de Toulouse.
À partir de tests d'évocations hiérarchisées, nous étudions les contenus des représentations professionnelles de quatre objets : l'ENT, le métier d'enseignant, la notion d'information et la notion de communication.
L'analyse de la structure des réponses, sur chacun de ces objets puis dans les relations entre les objets, met notamment en évidence la particularité de leur lecture par les enseignants au regard du discours social identifié dans la partie précédente.
Le système complexe formé par ces quatre objets permet d'observer que les représentations du métier d'enseignant et de l'ENT ont peu de contenus communs, mais qu'elles partagent toutes les deux de nombreux éléments avec les représentations de la communication et de l'information.
Ces résultats dessinent également deux grandes tendances dans la façon de décrire les objets de représentation investigués : une partie de la population semble systématiquement privilégier des éléments fonctionnels pour décrire chacun des objets alors qu'une autre partie de la population associe préférentiellement des éléments évaluatifs.
Une carte marine est un type de carte utilisé pour décrire la morphologie du fond marin et du littoral adjacent.
Un de ses principaux objectifs est de garantir la sécurité de la navigation maritime.
En conséquence, la construction d'une carte marine est contrainte par des règles très précises.
Le cartographe doit choisir et mettre en évidence les formes du relief sous-marin en fonction de leur intérêt pour la navigation.
Au sein d'un processus automatisé, le système doit être en mesure d'identifier et de classifier ces formes de relief à partir d'un modèle de terrain.
Un relief sous-marin est une individuation subjective d'une partie du fond océanique.
La reconnaissance de la morphologie du fond sous-marin est une tâche difficile, car les définitions des formes de relief reposent généralement sur une description qualitative et floue.
Obtenir la reconnaissance automatique des formes de relief nécessite donc une définition formelle des propriétés des reliefs et de leur modélisation.
Cette terminologie a été utilisée ici comme point de départ pour la classification automatique des formes de relief sous-marines d'un modèle numérique de terrain.
Afin d'intégrer les connaissances sur le relief sous-marin et sa représentation sur une carte nautique, cette recherche vise à définir des ontologies du relief sous-marin et des cartes marines.
Les ontologies sont ensuite utilisées à des fins de généralisation de carte marine.
Dans la première partie de la recherche, une ontologie est définie afin d'organiser la connaissance géographique et cartographique pour la représentation du relief sous-marin et la généralisation des cartes marines.
Tout d'abord, une ontologie de domaine du relief sous-marin présente les différents concepts de formes de relief sous-marines avec leurs propriétés géométriques et topologiques.
Cette ontologie est requise pour la classification des formes de relief.
Deuxièmement, une ontologie de représentation est présentée, qui décrit la façon dont les entités bathymétriques sont représentées sur la carte.
Troisièmement, une ontologie du processus de généralisation définit les contraintes et les opérations usitées pour la généralisation de carte marine.
Dans la deuxième partie de la recherche, un processus de généralisation fondé sur l'ontologie est conçu en s'appuyant sur un système multi-agents (SMA).
Quatre types d'agents (isobathe, sonde, forme de relief et groupe de formes de relief) sont définis pour gérer les objets cartographiques sur la carte.
Un modèle de base de données a été généré à partir de l'ontologie.
Le système proposé classe automatiquement les formes de relief sous-marines extraites à partir de la bathymétrie, et évalue les contraintes cartographiques.
Ensuite, les conflits de distance et de superficie sont évalués dans le SMA et des plans de généralisation sont proposés au cartographe.
L'objectif de cette thèse est de proposer un système de reconnaissance automatique des émotions (RAE) par analyse de la voix pour une application dans un contexte pédagogique d'orchestration de classe.
Ce système s'appuie sur l'extraction de nouvelles caractéristiques, par démodulation en amplitude et en fréquence, de la voix ; considérée comme un signal multi-composantes modulé en amplitude et en fréquence (AM-FM), non-stationnaire et issue d'un système non-linéaire.
Cette démodulation est basée sur l'utilisation conjointe de la décomposition en modes empiriques (EMD) et de l'opérateur d'énergie de Teager-Kaiser (TKEO).
Dans ce système, le modèle discret (ou catégoriel) a été retenu pour représenter les six émotions de base (la tristesse, la colère, la joie, le dégoût, la peur et la surprise) et l'émotion dite neutre.
La reconnaissance automatique a été optimisée par la recherche de la meilleure combinaison de caractéristiques, la sélection des plus pertinentes et par comparaison de différentes approches de classification.
Deux bases de données émotionnelles de référence, en allemand et en espagnol, ont servi à entrainer et évaluer ce système.
Une nouvelle base de données en Français, plus appropriée pour le contexte pédagogique a été construite, testée et validée.
L'évaluation de la gravité et la surveillance des maladies pulmonaires chroniques représentent deux challenges importants pour la prise en charge des patients et l'évaluation des traitements.
La surveillance repose principalement sur les données fonctionnelles respiratoires mais l'évaluation morphologique reste un point essentiel pour le diagnostic et l'évaluation de sévérité.
Une approche simple par seuillage adaptatif et une méthode plus sophistiquée de radiomique sont évaluées Dans la seconde partie, nous évaluons une méthode d'apprentissage profond pour contourer automatiquement l'atteinte fibrosante de la sclérodermie en scanner.
Dans la dernière partie, nous démontrons que l'étude de la déformation pulmonaire en IRM entre inspiration et expiration peut être utilisée pour repérer les régions pulmonaires en transformation fibreuse, moins déformables au cours de la respiration, et qu'en scanner, l'évaluation de la déformation entre des examens successifs de suivi peut diagnostiquer l'aggravation fibreuse chez les patients sclérodermiques.
Dans ce travail, nous nous intéressons aux mouvements de sourcils dans l'interaction en face- à-face en tant que ressource utilisée par les participants pour signaler un problème de compréhension.
Notre objectif est de décrire la séquence interactionnelle dans laquelle apparait ce mouvement et les trajectoires interactionnelles dont use le locuteur pour résoudre le trouble de la compréhension qui a surgi.
Nous nous appuyons sur trois corpus présentant différents degrés de spontanéité et diverses activités interactionnelles.
Après avoir identifié et annoté les mouvements de sourcils (haussement et froncement) et les séquences présentant un problème de compréhension (Weigand, 1999 ; Antaki, 2012), nous avons mené une analyse séquentielle sur une soixantaine d'extraits.
Cette recherche met en lumière le rôle des mouvements de sourcils en tant que ressource qui participe à rendre mutuellement reconnaissable un problème de compréhension, mais également en tant qu'indice de désalignement et de réalignement lors de ces dîtes séquences.
Ce travail s'inscrit dans le cadre du traitement automatique des langues naturelles, et s'interesse au probleme de la coherence dans la representation des connaissances issues de textes.
Deux resultats ont ete degages : soit la negation est source de coherence dans le sens ou elle est definie comme operation au sein des domaines &lt;&lt; notionnels &gt;&gt; introduits par le discours.
La structure de &lt;&lt; domaine &gt;&gt; facilite l'interpretation des enonces negatifs parce qu'elle permet de les exprimer de facon positive.
Soit, la negation est source d'incoherence et elle peut etre un facteur de construction d'un nouvel espace ou monde.
Nous avons propose une formalisation des deux types de negations dans le formalisme oriente-objet, dont les fondements theoriques sont les systemes logiques de lesniewski.
La notion de domaine a ete introduite dans les deux univers (intensionnel et extensionnel) du modele, en termes du calcul des noms et de la mereologie.
Cette formalisation peut nous permettre de rendre compte divers mecanismes d'inferences de la negation.
La majorité des méthodes de classification de textes utilisent le paradigme du sac de mots pour représenter les textes.
Pourtant cette technique pose différents problèmes sémantiques : certains mots sont polysémiques, d'autres peuvent être des synonymes et être malgré tout différenciés, d'autres encore sont liés sémantiquement sans que cela soit pris en compte et enfin, certains mots perdent leur sens s'ils sont extraits de leur groupe nominal.
Pour pallier ces problèmes, certaines méthodes ne représentent plus les textes par des mots mais par des concepts extraits d'une ontologie de domaine, intégrant ainsi la notion de sens au modèle.
Afin d'améliorer les performances de ces modèles, plusieurs méthodes ont été proposées pour enrichir les caractéristiques des textes à l'aide de nouveaux concepts extraits de bases de connaissances.
A l'aide de l'algorithme du classifieur naïf Bayésien, j'ai testé et comparé mes contributions sur le corpus de textes labéllisés Ohsumed et l'ontologie de domaine Disease Ontology.
Les résultats satisfaisants m'ont amené à analyser plus précisément le rôle des relations sémantiques dans l'enrichissement des modèles.
Ces nouveaux travaux ont été le sujet d'une seconde expérience où il est question d'évaluer les apports des relations hiérarchiques d'hyperonymie et d'hyponymie.
Face à cette évolution technologique vertigineuse, l'utilisation des dispositifs de l'Internet des Objets (IdO), les capteurs, et les réseaux sociaux, d'énormes flux de données IdO sont générées quotidiennement de différentes applications pourront être transformées en connaissances à travers l'apprentissage automatique.
En pratique, de multiples problèmes se posent afin d'extraire des connaissances utiles de ces flux qui doivent être gérés et traités efficacement.
Dans ce contexte, cette thèse vise à améliorer les performances (en termes de mémoire et de temps) des algorithmes de l'apprentissage supervisé, principalement la classification à partir de flux de données en évolution.
En plus de leur nature infinie, la dimensionnalité élevée et croissante de ces flux données dans certains domaines rendent la tâche de classification plus difficile.
La deuxième partie de la thèse détaille nos contributions en classification pour les flux de données.
Il s'agit de nouvelles approches basées sur les techniques de réduction de données visant à réduire les ressources de calcul des classificateurs actuels, presque sans perte en précision.
Pour traiter les flux de données de haute dimension efficacement, nous incorporons une étape de prétraitement qui consiste à réduire la dimension de chaque donnée (dès son arrivée) de manière incrémentale avant de passer à l'apprentissage.
Dans ce contexte, nous présentons plusieurs approches basées sur : Bayesien naïf amélioré par les résumés minimalistes et hashing trick, k-NN qui utilise compressed sensing et UMAP, et l'utilisation d'ensembles d'apprentissage également.
Ces dernières années, des méthodes issues de l'optimisation combinatoire ont été appliquées avec succès pour résoudre des problèmes algorithmiques difficiles en Traitement Automatique des Langues (TAL).
Nous suivons cette méthodologie dans le cadre de l'analyse syntaxique avec des Grammaires d'Arbres Adjoints Lexicalisés;
Plus précisément, un problème d'analyse est d'abord réduit à un problème de sélection de sous-graphe.
Ensuite nous formulons ce dernier sous forme de Programme Linéaire en Nombres Entiers.
Beaucoup d'algorithmes ont été proposés pour ces formulations.
Nous nous concentrons sur la Relaxation Lagrangienne qui a reçu beaucoup d'attention de la part de la communauté du TAL.
Les systèmes de fusion d'informations sont principalement composés, d'outils mathématiques permettant de réaliser la représentation et la combinaison des données.
L'objectif de ces systèmes peut s'exprimer comme un problème de décision sur la vérité ou la vraisemblance d'une proposition étant donné une ou plusieurs informations issues de différentes sources.
Les systèmes de fusion cherchent à exploiter au mieux les propriétés des sources de données en tenant compte de l'imperfection de l'information (imprécis, incomplet, ambigu, incertain, etc.) ainsi que l'aspect redondant, complémentaire et conflictuel des informations.
Le système de fusion concerné par cette thèse a la capacité d'intégrer dans ses traitements de la connaissance experte.
On le nomme système de fusion coopératif.
Puisque ce système cherche à intégrer pleinement les experts dans son fonctionnement, il est important de mettre à disposition des utilisateurs des informations aidant à mieux comprendre la fusion réalisée.
Une des grandes problématiques liées à ces systèmes de fusion d'informations porte sur l'évaluation de leurs performances.
L'évaluation doit permettre d'améliorer la qualité de la fusion, d'améliorer l'interaction expert/système et d'aider à mieux ajuster les paramètres du système.
En général, l'évaluation de tels systèmes est réalisée en fin de chaîne par une évaluation globale du résultat.
Mais, celle-ci ne permet pas de savoir précisément l'endroit de la chaîne qui nécessite une intervention.
Une autre difficulté réside dans le fait qu'une vérité terrain complète sur le résultat n'est pas toujours disponible, ce qui complique la tâche d'évaluation de performances de ce type de systèmes.
Le contexte applicatif de ce travail est l'interprétation d'images tridimensionnelles (images tomographiques, images sismiques, images synthétiques,...).
Dans ce contexte une évaluation locale des systèmes de fusion d'informations, a été mise en place.
L'approche a montré son intérêt dans l'ajustement efficace des paramètres et dans la coopération avec l'expert.
De nos jours, il est très fréquent de représenter un système en termes de relations entre objets.
Parmi les applications les plus courantes de telles données relationnelles, se situent les systèmes de recommandation (RS), qui traitent généralement des relations entre utilisateurs et items à recommander.
Les modèles relationnels probabilistes (PRM) sont un bon choix pour la modélisation des dépendances probabilistes entre ces objets.
Une tendance croissante dans les systèmes de recommandation est de rajouter une dimension spatiale à ces objets, que ce soient les utilisateurs, ou les items.
Cette thèse porte sur l'intersection peu explorée de trois domaines connexes - modèles probabilistes relationnels (et comment apprendre les dépendances probabilistes entre attributs d'une base de données relationnelles), les données spatiales et les systèmes de recommandation.
La première contribution de cette thèse porte sur le chevauchement des PRM et des systèmes de recommandation.
Nous avons proposé un modèle de recommandation à base de PRM capable de faire des recommandations à partir des requêtes des utilisateurs, mais sans profils d'utilisateurs, traitant ainsi le problème du démarrage à froid.
Notre deuxième contribution aborde le problème de l'intégration de l'information spatiale dans un PRM.
L'extraction automatique des intuitions et la construction de modèles computationnels à partir de connaissances sur des systèmes complexes repose largement sur le choix d'une représentation appropriée.
Ce travail s'efforce de construire un cadre adapté pour la représentation de connaissances fragmentées sur des systèmes complexes et sa curation semi-automatisé.
Un système de représentation des connaissances basé sur des hiérarchies de graphes liés à l'aide d'homomorphismes est proposé.
Les graphes individuels représentent des fragments de connaissances distincts et les homomorphismes permettent de relier ces fragments.
Nous nous concentrons sur la conception de mécanismes mathématiques, basés sur des approches algébriques de la réécriture de graphes, pour la transformation de graphes individuels dans des hiérarchies qui maintient des relations cohérentes entre eux.
De tels mécanismes fournissent une piste d'audit transparente, ainsi qu'une infrastructure pour maintenir plusieurs versions des connaissances.
La théorie développée est appliquée à la conception des schémas pour les bases de données orientée graphe qui fournissent des capacités de co-évolution schémas-données.
Ensuite, cette théorie est utilisée dans la construction du cadre KAMI, qui permet la curation des connaissances sur la signalisation dans les cellules.
KAMI propose des mécanismes pour une agrégation semi-automatisée de faits individuels sur les interactions protéine-protéine en corpus de connaissances, la réutilisation de ces connaissances pour l'instanciation de modèles de signalisation dans différents contextes cellulaires et la génération de modèles exécutables basés sur des règles.
Les graphes sont présents dans de nombreux domaines de recherche, que ce soit pour représenter des molécules, des réseaux sociaux ou des réseaux de transport.
Il est composé de nœuds reliés entre eux par des liens, appelés arêtes.
Récemment, les techniques d'apprentissage profond ont prouvé leur efficacité dans de nombreux domaines tels que le traitement de texte ou l'analyse d'images.
Ce constat a motivé de nombreux travaux de recherche visant à généraliser les techniques d'apprentissage profond à l'analyse de graphes.
Ainsi des algorithmes se basant notamment sur des réseaux de neurones et des convolutions ont été développés afin de répondre à des problématiques de classification de nœuds et de graphes.
Ces représentations à différentes échelles, encodent des informations hierarchiques sur le graphe.
En ce basant sur ces vectorialisations de graphes, nous proposons de nouvelles architectures afin de répondre à des tâches de classification de nœuds et classification de graphes.
Pour la recherche en santé publique, réutiliser les bases médicoadministratives est pertinent et ouvre de nouvelles perspectives
Cette thèse porte sur l'utilisation conjointe de bases de données médicoadministratives et de connaissances biomédicales pour l'étude des trajectoires de soin.
Cela recouvre à la fois (1) l'exploration et l'identification des trajectoires de soins pertinentes dans des flux volumineux au moyen de requêtes et (2) l'analyse des trajectoires retenues.
Les technologies du Web Sémantique et les ontologies du Web des données ont permis d'explorer efficacement les données médicoadministratives, en identifiant dans des trajectoires de soins des interactions, ou encore des contre-indications.
Nous avons également développé le package R queryMed afin de rendre plus accessible les ontologies médicales aux chercheurs en santé publique.
Après avoir permis d'identifier les trajectoires intéressantes, les connaissances relatives aux nomenclatures médicales de ces bases de données ont permis d'enrichir des méthodes d'analyse de trajectoires de soins pour mieux prendre en compte leurs complexités.
Cela s'est notamment traduit par l'intégration de similarités sémantiques entre concepts médicaux.
Les technologies du Web Sémantique ont également été utilisées pour explorer les résultats obtenus.
Il s'agit d'étudier le discours onusien sur la crise nucléaire iranienne pendant les dix années entre 2005 et 2015.
Une étude basée sur l'Analyse de Discours, est menée sur un corpus clos et prédéfini afin de discerner les différents procédés linguistiques et discursifs qui commandent le discours.
Il s'agit également d'appréhender les enjeux et les origines juridiques et politiques de cette crise diplomatique.
Notre défi majeur est ainsi de saisir le discours sous ses multiples dimensions, linguistiques, discursives, politiques et juridiques.
Telles sont les interrogations auxquelles nous répondons dans ce travail.
L'appréhension des incidences linguistiques et discussives se réalise à la lumière des données politiques et juridiques qui constituent un cadre interprétatif à l'analyse.
L'objectif est d'identifier la construction d'une identité onusienne à travers des notions de valeurs, par des mécanismes discursifs.
La quantité de contenu généré par l'utilisateur sur le Web croît à un rythme rapide.
Une grande partie de ce contenu est constituée des opinions et avis sur des produits et services.
Vu leur impact, ces avis sont un facteur important dans les décisions concernant l'achat de ces produits ou services.
Les utilisateurs ont tendance à faire confiance aux autres utilisateurs, surtout s'ils peuvent se comparer à ceux qui ont écrit les avis, ou, en d'autres termes, ils sont confiants de partager certaines caractéristiques.
Par exemple, les familles préféreront voyager dans les endroits qui ont été recommandés par d'autres familles.
Nous supposons que les avis qui contiennent des expériences vécues sont plus précieuses, puisque les expériences donnent aux avis un aspect plus subjective, permettant aux lecteurs de se projeter dans le contexte de l'écrivain.
En prenant en compte cette hypothèse, dans cette thèse, nous visons à identifier, extraire et représenter les expériences vécues rapportées dans les avis des utilisateurs en hybridant les techniques d'extraction des connaissances et de traitement du langage naturel,afin d'accélérer le processus décisionnel.
Pour cela, nous avons défini opérationnellement une expérience vécue d'un utilisateur comme un événement mentionné dans un avis, où l'auteur est présent parmi les participants.
Cette définition considère que les événements mentionnés dans le texte sont les éléments les plus importants dans les expériences vécues : toutes les expériences vécues sont basées sur des événements, qui sont clairement définis dans le temps et l'espace.
Par conséquent, nous proposons une approche permettant d'extraire les événements à partir des avis des utilisateurs, qui constituent la base d'un système permettant d'identifier et extraire les expériences vécues.
Pour l'approche d'extraction d'événements, nous avons transformé les avis des utilisateur sen leurs représentations sémantiques en utilisant des techniques de machine reading.
Nous avons effectué une analyse sémantique profonde des avis et détecté les cadres linguistiques les plus appropriés capturant des relations complexes exprimées dans les avis.
Le système d'extraction des expériences vécues repose sur trois étapes.
La première étape opère un filtrage des avis, basé sur les événements, permettant d'identifier les avis qui peuvent contenir des expériences vécues.
La deuxième étape consiste à extraire les événements pertinents avec leurs participants.
Afin de tester notre hypothèse, nous avons effectué quelques expériences pour vérifier si les expériences vécues peuvent être considérées comme des motivations pour les notes attribuées par les utilisateurs dans le système de notation.
Par conséquent, nous avons utilisé les expériences vécues comme des caractéristiques dans un système de classification, en comparant avec les notes associées avec des avis dans un ensemble de données extraites et annotées manuellement de Tripadvisor.
Les résultats montrent que les expériences vécues sont corrélées avec les notes.
Cette thèse fournit des contributions intéressantes dans le domaine de l'analyse d'opinion.
Tout d'abord, l'application avec succès de machine reading afin d'identifier les expériences vécues.
Ensuite, La confirmation que les expériences vécues sont liées aux notations.
Enfin, l'ensemble de données produit pour tester notre hypothèse constitue également une contribution importante de la thèse.
Bien que les réseaux de neurones soient à présent utilisés dans la quasi-totalité des composants d'un système de reconnaissance de la parole, du modèle acoustique au modèle de langue, l'entrée de ces systèmes reste une représentation analytique et fixée de la parole dans le domaine temps-fréquence, telle que les mel-filterbanks.
Cela se distingue de la vision par ordinateur, un domaine où les réseaux de neurones prennent en entrée les pixels bruts.
Les mel-filterbanks sont le produit d'une connaissance précieuse et documentée du système auditif humain, ainsi que du traitement du signal, et sont utilisées dans les systèmes de reconnaissance de la parole les plus en pointe, systèmes qui rivalisent désormais avec les humains dans certaines conditions.
Cependant, les mel-filterbanks, comme toute représentation fixée, sont fondamentalement limitées par le fait qu'elles ne soient pas affinées par apprentissage pour la tâche considérée.
Nous formulons l'hypothèse qu'apprendre ces représentations de bas niveau de la parole, conjontement avec le modèle, permettrait de faire avancer davantage l'état de l'art.
Nous explorons tout d'abord des approches d'apprentissage faiblement supervisé et montrons que nous pouvons entraîner un unique réseau de neurones à séparer l'information phonétique de celle du locuteur à partir de descripteurs spectraux ou du signal brut et que ces représentations se transfèrent à travers les langues.
De plus, apprendre à partir du signal brut produit des représentations du locuteur significativement meilleures que celles d'un modèle entraîné sur des mel-filterbanks.
Ces résultats encourageants nous mènent par la suite à développer une alternative aux mel-filterbanks qui peut être entraînée à partir des données.
Dans la seconde partie de cette thèse, nous proposons les Time-Domain filterbanks, une architecture neuronale légère prenant en entrée la forme d'onde, dont on peut initialiser les poids pour répliquer les mel-filterbanks et qui peut, par la suite, être entraînée par rétro-propagation avec le reste du réseau de neurones.
Au cours d'expériences systématiques et approfondies, nous montrons que les Time-Domain filterbanks surclassent systématiquement les melfilterbanks, et peuvent être intégrées dans le premier système de reconnaissance de la parole purement convolutif et entraîné à partir du signal brut, qui constitue actuellement un nouvel état de l'art.
Les descripteurs fixes étant également utilisés pour des tâches de classification non-linguistique, pour lesquelles elles sont d'autant moins optimales, nous entraînons un système de détection de dysarthrie à partir du signal brut, qui surclasse significativement un système équivalent entraîné sur des mel-filterbanks ou sur des descripteurs de bas niveau.
Enfin, nous concluons cette thèse en expliquant en quoi nos contributions s'inscrivent dans une transition plus large vers des systèmes de compréhension du son qui pourront être appris de bout en bout.
Le travail présenté dans ce mémoire vise à proposer des solutions aux problèmes d'entreposage des données textuelles.
L'intérêt porté à ce type de données est motivé par le fait qu'elles ne peuvent être intégrées et entreposées par l'application de simples techniques employées dans les systèmes décisionnels actuels.
Elle couvre les principales phases d'un processus classique d'entreposage des données et utilise de nouvelles méthodes adaptées aux données textuelles.
Dans ces travaux de thèse, nous nous sommes focalisés sur les deux premières phases qui sont l'intégration des données textuelles et leur modélisation multidimensionnelle.
Pour mettre en place une solution d'intégration de ce type de données, nous avons eu recours aux techniques de recherche d'information (RI) et du traitement automatique du langage naturel (TALN).
Pour cela, nous avons conçu un processus d'ETL (Extract-Transform-Load) adapté aux données textuelles.
Il s'agit d'un framework d'intégration, nommé ETL-Text, qui permet de déployer différentes tâches d'extraction, de filtrage et de transformation des données textuelles originelles sous une forme leur permettant d'être entreposées.
Certaines de ces tâches sont réalisées dans une approche, baptisée RICSH (Recherche d'information contextuelle par segmentation thématique de documents), de prétraitement et de recherche de données textuelles.
Celui-ci étend le modèle en constellation classique pour prendre en charge la représentation des textes dans un environnement multidimensionnel.
Dans TWM, il est défini une dimension sémantique conçue pour structurer les thèmes des documents et pour hiérarchiser les concepts sémantiques.
Pour cela, TWM est adossé à une source sémantique externe, Wikipédia, en l'occurrence, pour traiter la partie sémantique du modèle.
De plus, nous avons développé WikiCat, un outil pour alimenter la dimension sémantique de TWM avec des descripteurs sémantiques issus de Wikipédia.
Ces deux dernières contributions complètent le framework ETL-Text pour constituer le dispositif d'entreposage des données textuelles.
Pour valider nos différentes contributions, nous avons réalisé, en plus des travaux d'implémentation, une étude expérimentale pour chacune de nos propositions.
Face au phénomène des données massives, nous avons développé dans le cadre d'une étude de cas des algorithmes de parallélisation des traitements en utilisant le paradigme MapReduce que nous avons testés dans l'environnement Hadoop.
L'objectif principal de cette thèse est de proposer un framework complet pour une découverte, modélisation et reconnaissance automatiques des activités humaines dans les vidéos.
Afin de modéliser et de reconnaître des activités dans des vidéos à long terme, nous proposons aussi un framework qui combine des informations perceptuelles globales et locales issues de la scène, et qui construit, en conséquence, des modèles d'activités hiérarchiques.
Dans la première catégorie du framework, un classificateur supervisé basé sur le vecteur de Fisher est formé et les étiquettes sémantiques prédites sont intégrées dans les modèles hiérarchiques construits.
Dans la seconde catégorie, pour avoir un framework complètement non supervisé, plutôt que d'incorporer les étiquettes sémantiques, les codes visuels formés sont stockés dans les modèles.
Nous évaluons les frameworks sur deux ensembles de données réalistes sur les activités de la vie quotidienne enregistrées auprés des patients dans un environnement hospitalier.
Pour modéliser des mouvements fins du corps humain, nous proposons quatre différents frameworks de reconnaissance de gestes où chaque framework accepte une ou une combinaison de différentes modalités de données en entrée.
Nous évaluons les frameworks développés dans le contexte du test de diagnostic médical, appelé Praxis.
Nous proposons un nouveau défi dans la reconnaissance gestuelle qui consiste à obtenir une opinion objective sur les performances correctes et incorrectes de gestes très similaires.
Les expériences montrent l'efficacité de notre approche basée sur l'apprentissage en profondeur dans la reconnaissance des gestes et les tâches d'évaluation de la performance.
La présente thèse a pour but d'exploiter le potentiel des données spatiales et temporelles présentes dans les textes des articles scientifiques, afin de proposer une ontologie d'informations spatio-temporelles contenues dans les textes scientifiques dans une perspective du Web Sémantique.
Ces nouvelles métadonnées pourront servir comme support pour la production de nouvelles représentations sous forme graphique ou sous forme de synthèses textuelles, avec des applications en cartographie, en analyse chronologique et visualisations.
Ce projet répond au besoin d'exploitation des informations des corpus scientifiques à grande échelle par des analyses et traitement sémantique en plein texte des articles scientifiques.
Le principal sujet de cette thèse CIFRE, en partenariat avec le cabinet de conseil en IT Retail OneTeam, est le Natural Language Processing (NLP).
Sa spécificité est la conception et la mise en œuvre d'un ChatBot à usage interne.
Plus précisément, ses cas d'utilisation seront d'aider à la relation entre les clients de OneTeam et son service client en ligne, d'aider à gérer les tickets techniques concernant les difficultés des clients avec le logiciel de vente au détail.
Un ChatBot est une interface NLP entre un humain et une machine.
Il est censé comprendre un sous-ensemble limité de langage naturel pertinent pour le contexte de l'application, fournir une d'interaction sous forme de dialogue et être capable de traduire les demandes humaines dans un langage soit semi-formel (à communiquer à un responsable technique), ou un langage formel (par exemple une requête de recherche), ou un contexte pragmatique (par exemple implémenter un ensemble d'actions tel que lancer un logiciel ou exécuter une requête SQL), ou une combinaison de ceux-ci.
Dans le cadre de cette thèse, le ChatBot doit comprendre une description informelle d'un problème technique côté client lié à l'une des applications des suites logicielles de OneTeam, et fournir une interface conditionnelle basée sur un choix booléen : soit la description correspond à un problème existant dans les tickets fermés stockés dans la base de données, ou non.
Dans le premier cas, le ChatBot devrait construire la description pragmatique formelle nécessaire pour résoudre le problème client à portée de main.
Dans le deuxième cas, le ChatBot doit traduire la description informelle en un ticket ouvert semi-formel à écrire dans la base de données des tickets.
Bien que la conception et le déploiement de ChatBots ne soient pas en soi nouveau, le système global requis par OneTeam présente des fonctionnalités qui offre du challenge :
1. la description informelle en entrée peut être une transcription d'un message téléphonique, ce qui rend le texte beaucoup plus entaché d'erreurs, de signes de ponctuation et donc plus difficile à comprendre ;
2. le langage limité à comprendre comprend plusieurs acronymes qui peuvent être mal orthographiés dans la description informelle, ainsi que des phrases non grammaticales ;
3. dans le cas où le ChatBot doit engendrer directement la solution, il n'y a aucune marge d'erreur dans la compréhension et la bonne traduction formelle pragmatique.
Aborder ces caractéristiques nécessitera des recherches scientifiques entre l'algorithmique et la linguistique computationnelle.
Un autre défi est que le texte peut être en français ou en anglais, le français étant de loin plus probable.
Bien que cette fonctionnalité en elle-même ne soit pas nécessairement originale, il existe une quantité considérablement plus limitée de ressources logicielles NLP disponibles en français par rapport à l'anglais.
Cela nécessitera sans aucun doute une conception et une mise en œuvre supplémentaires des tâches de NLP de bas niveau.
Ce travail porte sur les représentations de l'adolescence dans le cinéma indépendant contemporain.
Il s'agit d'examiner la mise en images et en discours de ce phénomène social dans six films issus d'Europe et d'Amérique et sortis en salle entre 2009 et 2014.
En nous inscrivant en Sciences de l'Information et de la Communication, nous articulons une approche sémiotique et pragmatique afin de rendre compte des aspects plastiques, esthétiques et communicationnels des films du corpus.
Notre analyse poétique et socio-politique explore simultanément le langage cinématographique, les conditions de production et l'horizon d'attente auxquels renvoient les œuvres.
La première partie rend compte de la construction du cadre analytique et de la démarche analytique qui articule adolescence, cinéma et représentation.
La deuxième partie de la thèse saisit les formes et les effets de sens des films.
Le corps y est analysé comme le fondement de la représentation de l'adolescence, un objet sémiotique, social et politique.
La troisième partie de la recherche s'intéresse à l'émergence d'images des corps adolescents et de discours susceptibles d'engendrer un regard sur l'adolescence cinématographique en tant que construction symbolique et sociale.
La thèse conclut que ces films en tant que discours sur l'univers adolescent, invitent à une interprétation délibérative sur le monde social, historique et culturel qui est convoqué dans les films.
Les lignes de produits logiciels (LdPs) permettent la dérivation d'une famille de produits basés sur une gestion de la variabilité.
Les LdPs utilisent des configurations de caractéristiques afin de satisfaire les besoins de chaque client et, de même, permettre une réutilisation systématique en utilisant des assets réutilisables.
L'approche capitalisant sur des variantes des produits existants est appelé une approche extractive pour l'adoption de LdPs.
Il est également nécessaire de localiser les éléments associés à ces caractéristiques.
Les contraintes entre ces caractéristiques doivent être identifiées afin de garantir la sélection de configurations valides.
Cette thèse présente BUT4Reuse (Bottom-Up Technologies for Reuse), un framework unifié, générique et extensible pour l'adoption extractive de LdPs.
Une attention particulière est accordée à des scénarios de développement dirigée par les modèles.
Nous nous concentrons aussi sur l'analyse des techniques en proposant un benchmark pour la localisation de caractéristiques et une technique d'identification de familles de variantes.
Nous présentons des paradigmes de visualisation pour accompagner les experts du domaine dans le nommage de caractéristiques et aider à la découverte de contraintes.
Finalement, nous étudions l'exploitation des variantes pour l'analyse de la LdP après sa création.
Nous présentons une approche pour trouver des variantes pertinentes guidée par des évaluations des utilisateurs finaux.
Notre thèse propose l'analyse de l'information selon la théorie de la Grammaire Cognitive (Langacker 1987, 1991).
En fournissant des exemples du français, del 'anglais et du grec moderne, nous avons recherché les unités pertinentes pour unfiltrage de l'information.
Une fonction est définie comme une opération impliquant un morphème dépendant, p. ex. un verbe ou un suffixe, et un morphème autonome, p. ex. un nom ;
certaines fonctions sont primaires, dans le sens qu'elles sontpsychologiquement plus saillantes ;
il s'agit des morphèmes dépendants verbaux ou déverbaux, qui profilent différemment une même scène processuelle.
Afin d'en arriver àun filtrage automatique de l'information au niveau phrastique, il faut d'abord préparerdes dictionnaires de flexion et de dérivation déverbale.
Nous faisons les premiers pas en analysant exhaustivement la flexion verbale de l'anglais, du français et du grec moderne et en constituant des dictionnaires électroniques de toutes les formes fléchies verbales.
Ce travail s'inscrit dans le cadre des recherches en conception de systemes d'information textuelle effectuees dans le groupe sydo-lyon.
Une indexation d'un corpus homogene de textes francais est construite a partir de la description des syntagmes nominaux extraits des textes et des liens syntaxiques entre ces syntagmes nominaux.
L'anaphore peut etre un de ces liens syntaxiques entre syntagmes nominaux, et de plus, elle est une relation discursive qui contribue a representer le contenu d'un texte dans le sens ou elle met en valeur certains syntagmes nominaux (themes).
Le but principal de ce travail est d'identifier et de formaliser, en vue d'un traitement automatique, les principes de resolution d'anaphores et d'etudier les rapports entre l'anaphore et la coreference.
Nous avons etabli une typologie des unites anaphoriques et des unites antecedentes, suivie d'une proposition pour reconnaitre automatiquement ces unites dans un texte.
Enfin, un essai de mise en oeuvre automatique en prolog de la resolution des pronoms clitiques a ete realise.
En conclusion, ce travail montre l'interet d'une analyse linguistique approfondie pour la recherche en conception de systeme d'information textuelle.
Les systèmes de question-réponse renvoient une réponse précise à une question formulée en langue naturelle.
Les systèmes de question-réponse actuels, ainsi que les campagnes d'évaluation les évaluant, font en général l'hypothèse qu'une seule réponse est attendue pour une question.
Or nous avons constaté que, souvent, ce n'était pas le cas, surtout quand on cherche les réponses sur le Web et non dans une collection finie de documents.
Nous nous sommes donc intéressés au traitement des questions attendant plusieurs réponses à travers un système de question-réponse sur le Web en français.
Pour cela, nous avons développé le système Citron capable d'extraire des réponses multiples différentes à des questions factuelles en domaine ouvert, ainsi que de repérer et d'extraire le critère variant (date, lieu) source de la multiplicité des réponses.
Nous avons montré grâce à notre étude de différents corpus que les réponses à de telles questions se trouvaient souvent dans des tableaux ou des listes mais que ces structures sont difficilement analysables automatiquement sans prétraitement.
C'est pourquoi, nous avons également développé l'outil Kitten qui permet d'extraire le contenu des documents HTML sous forme de texte et aussi de repérer, analyser et formater ces structures.
Enfin, nous avons réalisé deux expériences avec des utilisateurs.
La première expérience évaluait Citron et les êtres humains sur la tâche d'extraction de réponse multiples : les résultats ont montré que Citron était plus rapide que les êtres humains et que l'écart entre la qualité des réponses de Citron et celle des utilisateurs était raisonnable.
La seconde expérience a évalué la satisfaction des utilisateurs concernant la présentation de réponses multiples : les résultats ont montré que les utilisateurs préféraient la présentation de Citron agrégeant les réponses et y ajoutant un critère variant (lorsqu'il existe) par rapport à la présentation utilisée lors des campagnes d'évaluation.
MATLAB est un environnement informatique doté d'un langage de programmation simple et d'une vaste bibliothèque de fonctions couramment utilisées en science et ingénierie (CSE) pour le prototypage rapide.
Cependant, certaines caractéristiques de son environnement, comme son langage dynamique ou son style de programmation interactif, affectent la rapidité d'exécution des programmes.
Les approches actuelles d'amélioration des programmes MATLAB traduisent le code dans des langages statiques plus rapides comme C ou Fortran, ou bien appliquent systématiquement des transformations de code au programme MATLAB sans considérer leur impact sur les performances.
Dans cette thèse, nous comblons cette lacune en développant des techniques d'analyse et de transformation de code des programmes MATLAB afin d'augmenter leur performance.
Plus précisément, nous analysons et modélisons le comportement d'un environnement MATLAB black-box uniquement en mesurant l'exécution caractéristique des programmes sur CPU.
À partir des données obtenues, nous formalisons un modèle statique qui prédit le type et l'ordonnancement des instructions programmées lors de l'exécution par le compilateur Just-In-Time (JIT).
Ce modèle nous permet de proposer plusieurs transformations de code qui améliorent les performances des programmes MATLAB en influençant la façon dont le compilateur JIT génère le code machine.
Les résultats obtenus démontrent les avantages pratiques de la méthodologie présentée.
Avec le développement de la robotique cognitive, le besoin d'outils avancés pour représenter, manipuler, raisonner sur les connaissances acquises par un robot a clairement été mis en avant.
Mais stocker et manipuler des connaissances requiert tout d'abord d'éclaircir ce que l'on nomme connaissance pour un robot, et comment celle-ci peut-elle être représentée de manière intelligible pour une machine.
Dans un second temps, nous présentons en profondeur ORO, une instanciation particulière d'un système de représentation et manipulation des connaissances, conçu et implémenté durant la préparation de cette thèse.
Nous détaillons le fonctionnement interne du système, ainsi que son intégration dans plusieurs architectures robotiques complètes.
Un éclairage particulier est donné sur la modélisation de la prise de perspective dans le contexte de l'interaction, et de son interprétation en terme de théorie de l'esprit.
La troisième partie de l'étude porte sur une application importante des systèmes de représentation des connaissances dans ce contexte de l'interaction homme-robot : le traitement du dialogue situé.
Nous concluons cette thèse sur un certain nombre de considérations sur la viabilité et l'importance d'une gestion explicite des connaissances des agents, ainsi que par une réflexion sur les éléments encore manquant pour réaliser le programme d'une robotique “de niveau humain”
Le cadre de nos recherches est la diffusion d'informations en Langue des Signes Française via un signeur virtuel, par combinaison de segments d'énoncés préenregistrés.
Notre étude porte sur une proposition de modèle de coarticulation pour ce système de diffusion.
Nous détaillons les différents aspects de la création et de l'annotation de ce corpus, et de l'analyse de ces annotations.
Des calculs statistiques quantitatifs et qualitatifs nous permettent de proposer un modèle de coarticulation, basé sur des relâchements et des tensions de configurations des mains.
Nous proposons et mettons en œuvre une méthodologie d'évaluation de notre modèle.
Enfin nous proposons des perspectives autour des utilisations potentielles de ce modèle pour des recherches en traitement d'image et en animation de personnages 3d s'exprimant en langue des signes française.
Quand les robots doivent affronter le monde réel, ils doivent s'adapter à diverses situations imprévues en acquérant de nouvelles compétences le plus rapidement possible.
Les algorithmes d'apprentissage par renforcement (par exemple, l'apprentissage par renforcement profond) pourraient permettre d'apprendre de telles compétences, mais les algorithmes actuels nécessitent un temps d'interaction trop important.
Notre objectif principal est de combiner des connaissances acquises sur un simulateur avec les expériences réelles du robot afin d'obtenir un apprentissage et une adaptation rapides.
Dans notre première contribution, nous proposons un nouvel algorithme de recherche de politiques basé sur un modèle, appelé Multi-DEX, qui (1) est capable de trouver des politiques dans des scénarios aux récompenses rares, (2) n'impose aucune contrainte sur le type de politique ou le type de fonction de récompense et (3) est aussi efficace en termes de données que l'algorithme de recherche de politiques de l'état de l'art dans des scénarios de récompenses non rares.
Dans notre troisième contribution, nous présentons un algorithme de méta-apprentissage basé sur les gradients appelé FAMLE.
FAMLE permet d'entraîner le modèle dynamique du robot à partir de données simulées afin que le modèle puisse être adapté rapidement à diverses situations invisibles grâce aux observations du monde réel.
En utilisant FAMLE pour améliorer un modèle pour la commande prédictive, nous montrons que notre approche surpasse plusieurs algorithmes d'apprentissage basés ou non sur un modèle, et résout les tâches données en moins de temps d'interaction que les algorithmes avec lesquels nous l'avons comparé.
Avec l'avancée du Web Sémantique et des initiatives Open Linked Data, une grande quantité de documents RDF sont disponibles sur Internet.
L'objectif est de rendre ces données lisibles pour les humains et les machines, en adoptant des formats spéciaux et en les connectant à l'aide des IRIs (International Resource Identifier), qui sont des abstractions de ressources réelles du monde.
Cependant, les études sur l'anonymisation dans le contexte des documents RDF sont très limitées.
Ces études sont les travaux initiaux de protection des individus sur des documents RDF, puisqu'ils montrent les approches pratiques d'anonymisation pour des scénarios simples comme l'utilisation d'opérations de généralisation et d'opérations de suppression basées sur des hiérarchies.
Cependant, pour des scénarios complexes, où une diversité de données est présentée, les approches d'anonymisations existantes n'assurent pas une confidentialité suffisante.
Ainsi, dans ce contexte, nous proposons une approche d'anonymisation, qui analyse les voisins en fonction des connaissances antérieures, centrée sur la confidentialité des entités représentées comme des nœuds dans les documents RDF.
Notre approche de l'anonymisation est capable de fournir une meilleure confidentialité, car elle prend en compte la condition de la diversité de l'environnement ainsi que les voisins (nœuds et arêtes) des entités d'intérêts.
En outre, un processus d'anonymisation automatique est assuré par l'utilisation d'opérations d'anonymisations associées aux types de données.
Les buts de cette thèse sont doubles, et concernent les circuits métaboliques synthétiques, qui permettent de détecter des composants chimiques par transmission de signal et de faire du calcul en utilisant des enzymes.
La première partie a consisté à développer des outils d'apprentissage actif et par renforcement pour améliorer la conception de circuits métaboliques et optimiser la biodétection et la bioproduction.
Pour atteindre cet objectif, un nouvel algorithme (RetroPath3.0) fondé sur une recherche arborescente de Monte Carlo guidée par similarité est présenté.
Cet algorithme, combiné à des règles de réaction apprises sur des données et des niveaux différents de promiscuité enzymatique, permet de focaliser l'exploration sur les composés et les chemins les plus prometteurs en bio-rétrosynthèse.
La deuxième partie a consisté à développer des méthodes d'analyse, pour générer des connaissances à partir de données biologiques, et modéliser les réponses de biocapteurs.
Dans un premier temps, l'effet du nombre de copies de plasmides sur la sensibilité d'un biocapteur utilisant un facteur de transcription a été modélisé.
Ensuite, en utilisant des systèmes acellulaires qui permettent un meilleur contrôle des variables expérimentales comme la concentration d'ADN, l'utilisation des ressources a été modélisée pour assurer que notre compréhension actuelle des phénomènes sous-jacents est suffisante pour rendre compte du comportement du circuit, en utilisant des modèles empiriques ou mécanistiques.
Dans l'ensemble, cette thèse présente des outils de conception et d'analyse pour les circuits métaboliques synthétiques.
Ces outils ont été utilisés pour développer une nouvelle méthode permettant d'effectuer des calculs en biologie synthétique.
Le développement de systèmes d'analyse discursive automatique des documents est un enjeu actuel majeur en Traitement Automatique des Langues.
La difficulté principale correspond à l'étape d'identification des relations (comme Explication, Contraste...) liant les segments constituant le document.
Dans cette thèse, nous utilisons des données brutes pour améliorer des systèmes d'identification automatique des relations implicites.
Nous proposons d'abord d'utiliser les connecteurs pour annoter automatiquement de nouvelles données.
Nous rapportons des améliorations sur le corpus anglais du Penn Discourse Treebank et montrons notamment que cette méthode permet de limiter le recours à des ressources riches, disponibles seulement pour peu de langues.
Les réseaux de neurones sont devenus un algorithme phare de l'intelligence artificielle, avec des succès majeurs dans la résolution de tâches complexes telles que la reconnaissance d'images et le jeu.
Les ordinateurs et les cartes graphiques consomment malheureusement une quantité très élevée d'énergie lorsqu'ils exécutent des réseaux de neurones.
Par exemple, l'entraînement d'un modèle de traitement du langage naturel de pointe sur un superordinateur moderne consomme 1000 kW.h [1], soit l'énergie consommée par un cerveau humain pour l'ensemble de ses tâches sur une durée de six ans.
Dans le cerveau, les neurones - qui peuvent être considérés comme effectuant le calcul - ont un accès direct à la mémoire, portée par les synapses.
L'électronique actuelle, sur laquelle reposent les GPU et les CPU utilisés en IA, sépare intrinsèquement la mémoire et le calcul dans des unités physiques distinctes, entre lesquelles les données doivent être transportées.
Ce "goulot d'étranglement de von Neuman" est un problème pour les algorithmes d'intelligence artificielle qui nécessitent la lecture de quantités considérables de données à chaque étape, l'exécution d'opérations avancées sur ces données, puis la réécriture des résultats en mémoire [2]-[4].
Ce processus ralentit le calcul et augmente considérablement la consommation d'énergie pour l'apprentissage et l'inférence.
Le paradigme général du calcul neuromorphique consiste donc à s'inspirer de la topologie du cerveau pour construire des circuits composés de neurones physiques interconnectés par des synapses physiques qui mettent en œuvre la mémoire in-situ, de manière non volatile, ce qui réduit considérablement la nécessité de déplacer les données dans le circuit et permet d'énormes gains en termes de vitesse et d'efficacité énergétique.
L'un des défis majeurs du calcul neuromorphique est de parvenir à former les réseaux sur puce de manière efficace.
La mise en œuvre d'un algorithme d'entraînement traditionnel tel que la rétropropagation du gradient nécessite des circuits de grande surface et gourmands en énergie pour stocker les activations, calculer les gradients, stocker les gradients, puis modifier séquentiellement chaque synapse physique du réseau.
C'est un obstacle immense au développement d'une IA intégrée capable d'apprendre.
C'est pourquoi les puces d'IA développées aujourd'hui dans les universités et l'industrie visent principalement l'inférence avec un apprentissage hors puce [5]-[7], ou mettent en œuvre des algorithmes d'entraînement dont les performances sont loin des méthodes de descente de gradient les plus modernes [8].
Dans ce contexte, les travaux de pionniers de l'IA, tels que Geoffrey Hinton [9] et Yoshua Bengio [10]-[12], visant à comprendre comment la descente de gradient peut être effectuée dans le cerveau, donnent des indications inspirantes pour atteindre le même résultat dans les systèmes neuromorphiques.
Des algorithmes tels que la propagation de l'équilibre, proposée pour la première fois par Scellier et Bengio [12], et la propagation de l'éligibilité (E-Prop) de Wolfgang Maas [13] montrent que, dans les réseaux de neurones de pointe, les gradients peuvent être transportés à travers le réseau directement par l'activité des neurones.
Le C2N et le CNRS/Thales, en collaboration avec l'équipe de Yoshua Bengio au Mila, ont montré que les gradients calculés par la propagation de l'équilibre sont égaux à ceux dérivés par la rétropropagation du gradient dans le temps, et ont souligné le potentiel de ce résultat pour la formation de systèmes neuromorphiques avec des performances de pointe dans une publication dans NeurIPS l'année dernière (acceptée comme présentation orale) [14].
L'objectif ultime de la thèse est de fabriquer des circuits neuromorphiques qui apprennent localement et de manière autonome grâce à des algorithmes tels que la propagation de l'équilibre, qui codent des gradients dans l'activité impulsionnelle des neurones, et peuvent être directement appliqués aux synapses qu'ils connectent.
À cette fin, le/la doctorant.e devra d'abord développer, coder et tester par des simulations de nouveaux algorithmes pour entraîner des réseaux de neurones à impulsions récurrents, inspirés de la propagation de l'équilibre mais adaptés au matériel neuromorphique.
Il/elle réalisera ensuite ces réseaux avec des nanocomposants, par des expériences à l'échelle du laboratoire, puis dans des systèmes plus importants.
Les éléments de base qui seront utilisés pour développer ce matériel sont des composants électroniques à l'échelle nanométrique appelés dispositifs à commutation résistive ou memristors (abréviation de 'memory resistors') [15].
Ils sont constitués d'un oxyde isolant pris en sandwich entre deux électrodes métalliques (Fig. 1(a-b).
Ces dispositifs à l'échelle nanométrique sont très prometteurs pour l'informatique neuromorphique car ils peuvent imiter à la fois les synapses et les neurones avec une très faible énergie, et ont de très petites dimensions.
Ils présentent des comportements différents selon les matériaux utilisés pour l'oxyde et les électrodes.
Ils peuvent être passifs et présenter une commutation de résistance non-volatile (par exemple, avec des hétérostructures Pt/TaOx/Pt, Fig. 1(d)), ce qui est utile pour les caractéristiques synaptiques (mémoire et plasticité à plusieurs niveaux).
Ils peuvent également être actifs et présenter une commutation volatile par résistance différentielle négative (par exemple, avec des hétérostructures Pt/NbOx/Pt), ce qui peut générer des caractéristiques neuronales oscillantes (comme des impulsions et des trains d'impulsions, Fig. 1(c)).
Le C2N et le CNRS/Thales ont une expertise de longue date dans la fabrication et l'étude de ces nanodispositifs [3], [16]-[20].
Pour l'instant, les études ont surtout porté sur les circuits qui utilisent ces dispositifs soit comme synapses, soit comme neurones, mais pas les deux à la fois.
Il a été démontré que les synapses mémorielles peuvent apprendre grâce à l'apprentissage bio-inspiré appelé Spike Timing Dependent Plasticity [3], [21].
Il a aussi été prouvé que les neurones mémoriels peuvent imiter de nombreuses caractéristiques des neurones comme leur réponse impulsionnelle et les trains d'impulsions [15].
Le but de cette thèse est d'assembler ces composants individuels pour former des réseaux de neurones à impulsions capables d'apprendre de manière autonome grâce à des algorithmes de pointe reproduisant la retro-propagation du gradient *in materio*.
Cette thèse de recherche aborde les champs de la créativité, de l'innovation et du processus créatif collaboratif.
Il est nécessaire de construire de nouveaux systèmes techniques et innovants qui peuvent imiter le comportement humain précieux consistant à visualiser et à donner vie à leurs idées.
Cette approche fait appel à la créativité et implique, en tant que processus, de nombreux concepts tels que la découverte, la création, la sociabilité, le raffinement et la communication.
L'objectif de cette recherche est de concevoir des outils informatiques et technologiques pour soutenir la créativité au cours des phases de création dans un atelier de créativité.
Pour opérationnaliser cet outil, il y a plusieurs objectifs à atteindre pendant le temps requis pour cette recherche.
Le contexte de la recherche est un atelier de créativité « 48H » , mobilisant des étudiants, des professeurs, des industriels, des experts travaillant avec des idées au moyen d'une combinatoire exploratoire, de techniques de transformation et de méthodes de création collaborative.
Pour construire un outil pour ce genre d'événement créatif, nous avons besoin d'étudier comment prendre en compte le modèle humain de partage des connaissances et comment un système proposé peut développer une approche sémantique pour comparer les cartes d'idées dans un atelier de créativité de 48 heures.
Les principaux objectifs de cette recherche sont de comprendre le partage des connaissances lors d'un atelier de créativité et d'étudier l'organisation humaine dans un atelier de créativité et son processus général, en particulier.
Nous étudions l'événement 48 Heures de créativité (48H) comme un processus créatif et innovant où l'idée est le produit final.
Cet événement a lieu à Nancy, en France, à l'Université de Lorraine (Laboratoire ERPI).
Nous concevons un système intelligent basé en multi-agents (MAS) également pour développer et annoter le système.
Nous proposons une approche sémantique pour gérer les idées et le système de design global.
Les implications pratiques de notre modèle permettent d'utiliser la créativité, les méthodes créatives collaboratives et les nouvelles méthodologies technologiques modernes, comme de nouveaux outils conçus pour construire des systèmes créatifs et innovants.
Quelques domaines et modèles pertinents participent à cette recherche tels que le système multi-agent (MAS), le web sémantique, l'ontologie, le modèle organisationnel pour mettre en évidence les connaissances et le modèle organisationnel de réutilisation des connaissances KROM.
Avec l'augmentation massive du contenu vidéo sur Internet et au-delà, la compréhension automatique du contenu visuel pourrait avoir un impact sur de nombreux domaines d'application différents tels que la robotique, la santé, la recherche de contenu ou le filtrage.
Le but de cette thèse est de fournir des contributions méthodologiques en vision par ordinateur et apprentissage statistique pour la compréhension automatique du contenu des vidéos.
Nous mettons l'accent sur les problèmes de la reconnaissance de l'action humaine à grain fin et du raisonnement visuel à partir des interactions entre objets.
Dans la première partie de ce manuscrit, nous abordons le problème de la reconnaissance fine de l'action humaine.
Nous introduisons deux différents mécanismes d'attention, entrainés sur le contenu visuel à partir de la pose humaine articulée.
Une première méthode est capable de porter automatiquement l'attention sur des points pré-sélectionnés importants de la vidéo, conditionnés sur des caractéristiques apprises extraites de la pose humaine articulée.
Nous montrons qu'un tel mécanisme améliore les performances sur la tâche finale et fournit un bon moyen de visualiser les parties les plus discriminantes du contenu visuel.
Une deuxième méthode va au-delà de la reconnaissance de l'action humaine basée sur la pose.
Nous développons une méthode capable d'identifier automatiquement un nuage de points caractéristiques non structurés pour une vidéo à l'aide d'informations contextuelles.
De plus, nous introduisons un système distribué entrainé pour agréger les caractéristiques de manière récurrente et prendre des décisions de manière distribuée.
Nous démontrons que nous pouvons obtenir de meilleures performances que celles illustrées précédemment, sans utiliser d'informations de pose articulée au moment de l'inférence.
Dans la deuxième partie de cette thèse, nous étudions les représentations vidéo d'un point de vue objet.
Étant donné un ensemble de personnes et d'objets détectés dans la scène, nous développons une méthode qui a appris à déduire les interactions importantes des objets à travers l'espace et le temps en utilisant uniquement l'annotation au niveau vidéo.
Cela permet d'identifier une interaction inter-objet importante pour une action donnée ainsi que le biais potentiel d'un ensemble de données.
Enfin, dans une troisième partie, nous allons au-delà de la tâche de classification et d'apprentissage supervisé à partir de contenus visuels, en abordant la causalité à travers les interactions, et en particulier le problème de l'apprentissage contrefactuel.
Nous introduisons une nouvelle base de données, à savoir CoPhy, où, après avoir regardé une vidéo, la tâche consiste à prédire le résultat après avoir modifié la phase initiale de la vidéo.
Nous développons une méthode basée sur des interactions au niveau des objets capables d'inférer les propriétés des objets sans supervision ainsi que les emplacements futurs des objets après l'intervention.
Cette etude sur la classification et l'analyse syntaxique des phrases figees s'inscrit dans la perspective de l'elaboration d'un lexique-grammaire du grec moderne,en d'autres termes un dictionnaire syntaxique de la langue.
Le cadre syntaxique theorique est celui de la grammaire transformationnelle de z. S. Harris et de m. Gross.
Les phrases figees se definissent par le fait qu'un ou plusieurs de leurs actants sont lexicalement invariables par rapport au verbe.
Les phrases figees, s'analysent, dans la majorite des cas, de facon syntaxiquement correcte et les regles qu'elles subissent sont les regles de la syntaxe des phrases libres.
Ceci est valable aussi bien pour leurs parties libres que pour leurs parties figees.
Notre classification est donc basee sur le fait que le nombre et la position des parties libres et figees est variable.
Quant aux eventuelles applications pratiques nous pensons aux differents traitements automatiques du langage ainsi qu'a l'enseignement des langues et a la traduction.
La reconnaissance des opinions d'un locuteur dans une interaction orale est une étape cruciale pour améliorer la communication entre un humain et un agent virtuel.
Dans cette thèse, nous nous situons dans une problématique de traitement automatique de la parole (TAP) sur les phénomènes d'opinions dans des interactions orales spontanées naturelles.
L'analyse d'opinion est une tâche peu souvent abordée en TAP qui se concentrait jusqu'à peu sur les émotions à l'aide du contenu vocal et non verbal.
De plus, la plupart des systèmes récents existants n'utilisent pas le contexte interactionnel afin d'analyser les opinions du locuteur.
Dans cette thèse, nous nous penchons sur ces sujet.
Nous nous situons dans le cadre de la détection automatique en utilisant des modèles d'apprentissage statistiques.
Après une étude sur la modélisation de la dynamique de l'opinion par un modèle à états latents à l'intérieur d'un monologue, nous étudions la manière d'intégrer le contexte interactionnel dialogique, et enfin d'intégrer l'audio au texte avec différents types de fusion.
Nous avons travaillé sur une base de données de Vlogs au niveau d'un sentiment global, puis sur une base de données d'interactions dyadiques multimodales composée de conversations ouvertes, au niveau du tour de parole et de la paire de tours de parole.
Pour finir, nous avons fait annoté une base de données en opinion car les base de données existantes n'étaient pas satisfaisantes vis-à-vis de la tâche abordée, et ne permettaient pas une comparaison claire avec d'autres systèmes à l'état de l'art.
A l'aube du changement important porté par l'avènement des méthodes neuronales, nous étudions différents types de représentations : les anciennes représentations construites à la main, rigides mais précises, et les nouvelles représentations apprises de manière statistique, générales et sémantiques.
Nous étudions différentes segmentations permettant de prendre en compte le caractère asynchrone de la multi-modalité.
Dernièrement, nous utilisons un modèle d'apprentissage à états latents qui peut s'adapter à une base de données de taille restreinte, pour la tâche atypique qu'est l'analyse d'opinion, et nous montrons qu'il permet à la fois une adaptation des descripteurs du domaine écrit au domaine oral, et servir de couche d'attention via son pouvoir de clusterisation.
La fusion multimodale complexe n'étant pas bien gérée par le classifieur utilisé, et l'audio étant moins impactant sur l'opinion que le texte, nous étudions différentes méthodes de sélection de paramètres pour résoudre ces problèmes.
Leur qualité joue par conséquent un rôle déterminant dans le succès du système final.
Des études ont montré que la majeure partie des changements que subit un SI concerne des manques ou des défaillances liés aux fonctionnalités attendues.
Sachant que la définition de ses fonctionnalités incombe à la phase de l'analyse et conception dont les MC constituent les livrables, il apparaît indispensable pour une méthode de conception de veiller à la qualité des MC qu'elle produit.
Notre approche vise les problèmes liés à la qualité de la modélisation conceptuelle en proposant une solution intégrée au processus de développement qui à l'avantage d'être complète puisqu'elle adresse à la fois la mesure de la qualité ainsi que son amélioration.
La proposition couvre les aspects suivants :
En effet, un des manques constaté dans le domaine de la qualité des MC est l'absence de consensus sur les concepts et leurs définitions.
Un pattern de qualité sert à aider un concepteur de SI dans l'identification des critères de qualité applicables à sa spécification, puis de le guider progressivement dans la mesure de la qualité ainsi que dans son amélioration.
Sachant que la plupart des approches existantes s'intéresse à la mesure de la qualité et néglige les moyens de la corriger.
iii. Formulation d'une méthode orientée qualité incluant à la fois des concepts, des guides et des techniques permettant de définir les concepts de qualité souhaités, leur mesure et l'amélioration de la qualité des MC.
iv. Développement d'un prototype "CM-Quality".
Il existe dans le sud ouest algérien plusieurs variétés de berbère.
Certaines d'entre elles sont situées dans la région dite du Sud-Oranais et peuvent être cataloguées comme des langues en danger.
Nous avons donc entrepris de décrire ces variétés avant qu'elles ne disparaissent.
Cela a été mené à bien en réalisant plusieurs enquêtes de terrain.
Par ailleurs, ce travail de documentation linguistique et de conservation du patrimoine culturel n'est qu'un des aspects de cette thèse.
Nous avons eu recours aux méthodes en usage en Sciences de l'Information Géographique (SIG) et en Sciences Des Données (SDD) pour mener une étude dialectologique.
Grâce aux SIG, nous avons réalisé une étude géolinguistique qui nous a permis de visualiser sur des cartes linguistiques la distribution de la variation linguistique de certaines consonnes.
À partir de ces données, nous avons discuté de la réalité phonologique de ces consonnes simples et géminées.
Dans le prolongement, une étude dialectométrique a été effectuée en nous basant sur des méthodes de partitions des données.
Nous avons utilisé les méthodes d'Apprentissage Non Supervisé (PHA, k-moyenne, MDS,...) et les méthodes d'Apprentissage Supervisé (CART) connues en SDD.
Puis, nous avons entrepris une analyse phonétique fondée sur une étude acoustique des rhotiques alvéolaires : [ɾ], [r], [ɾˤ] et [rˤ].
Ces unités phoniques se distinguent par leur temporalité et leur réalisation articulatoire.
Ainsi, les spectrogrammes nous ont permis d'examiner la distribution de ces sons.
Les discours normés que produisent les institutions sont concurrencés par les discours informels ou faiblement formalisés issus du web social.
La démocratisation de la prise de parole redistribue l'autorité en matière de connaissance et modifie les processus de construction des savoirs.
Ces discours spontanés sont accessibles par tous et dans des volumes exponentiels, ce qui offre aux sciences humaines et sociales de nouvelles possibilités d'exploration.
Pourtant elles manquent encore de méthodologies pour appréhender ces données complexes et encore peu décrites.
L'objectif de la thèse est de montrer dans quelle mesure les discours du web social peuvent compléter les discours institutionnels.
Nous y développons une méthodologie de collecte et d'analyse adaptée aux spécificités des discours natifs du numérique (massivité, anonymat, volatilité, caractéristiques structurelles, etc.).
Ce terrain applicatif recouvre plusieurs enjeux de société : sanitaire et social, évolutions des moeurs, concurrence des discours.
L'étude est complétée par l'analyse d'un corpus comparable de langue française, relevant des mêmes thématique, genre et discours que le corpus vietnamien, de manière à mettre en évidence les spécificités de contextes socioculturels distincts.
L'objectif de ce projet est d'exploiter le flux audio en profondeur pour enrichir de façon automatique les scripts et sous-titres de séries télévisées et de films, en y ajoutant automatiquement les noms et positions des personnages.
locuteur A : 'Nice to meet you, I am Leonard, and this is Sheldon. We live across the hall.'
locuteur B : 'Oh. Hi. I'm Penny.'
locuteur A : 'Sheldon, what the hell are you doing ?'
locuteur C : 'I am not quite sure yet. Do you know where Howard lives ?'
En lisant ces deux conversations, un humain peut facilement déterminer que 'locuteur A' s'appele 'Leonard', 'locuteur B' est 'Penny' et 'locuteur C' est 'Sheldon'.
L'objectif de ce projet est de combiner des approches de traitement automatique de la langue naturelle et de la parole pour arriver à ce résultat automatiquement.
De manière simple, il peut être présenté ainsi : imaginez que vous êtes dans un labyrinthe, dont vous connaissez toutes les routes menant à chacune des portes de sortie.
Pour cela, il vous indiquera la direction à prendre à chaque intersection.
Malheureusement, cet homme ne parle pas votre langue, et les mots qu'il utilise pour dire ``droite'' ou ``gauche'' vous sont inconnus.
Est-il possible de trouver le trésor et de comprendre l'association entre les mots du vieil homme et leurs significations ?
Ce problème, bien qu'en apparence abstrait, est relié à des problématiques concrètes dans le domaine de l'interaction homme-machine.
Remplaçons le vieil homme par un utilisateur souhaitant guider un robot vers une sortie spécifique du labyrinthe.
Ce robot ne sait pas en avance quelle est la bonne sortie mais il sait où se trouvent chacune des portes et comment s'y rendre.
Imaginons maintenant que ce robot ne comprenne pas a priori le langage de l'humain ; en effet, il est très difficile de construire un robot à même de comprendre parfaitement chaque langue, accent et préférence de chacun.
Il faudra alors que le robot apprenne l'association entre les mots de l'utilisateur et leur sens, tout en réalisant la tâche que l'humain lui indique (i.e.trouver la bonne porte).
En effet, le résoudre reviendrait à créer des interfaces ne nécessitant pas de phase de calibration car la machine pourrait s'adapter,automatiquement et pendant l'interaction, à différentes personnes qui ne parlent pas la même langue ou qui n'utilisent pas les mêmes mots pour dire la même chose.
Cela veut aussi dire qu'il serait facile de considérer d'autres modalités d'interaction (par exemple des gestes, des expressions faciales ou des ondes cérébrales).
Dans cette thèse, nous présentons une solution à ce problème.
Nous appliquons nos algorithmes à deux exemples typiques de l'interaction homme robot et de l'interaction cerveau machine : une tâche d'organisation d'une série d'objets selon les préférences de l'utilisateur qui guide le robot par la voix, et une tâche de déplacement sur une grille guidé par les signaux cérébraux de l'utilisateur.
Ces dernières expériences ont été faites avec des utilisateurs réels.
L'objectif principal de cette thèse est de proposer des techniques d'apprentissage de représentation avares en données étiquetées : apprentissage non-supervisé, supervision distante, ou encore apprentissage actif sont autant de pistes qui pourront être étudiées.
L'apprentissage non-supervisé est le cas extrême où les données non-étiquetées sont disponibles en abondance, mais aucune donnée étiquetée n'est disponible.
L'appellation 'auto-supervisé'(self-supervised, en anglais) est de plus en plus utilisée pour décrire ce type d'approches car il s'agit généralement d'approches visant à reconstruire les données originales à partir d'une version altérée (BERT [Devlin et al., 2018], auto-encodeurs, etc.).
On parle de supervision distante quand des étiquettes (parfois imparfaites) sont effectivement disponibles, mais pas pour la tâche visée.
Par exemple, pour la tâche de vérification du locuteur, il est possible d'utiliser un système de transcription de la parole automatique pour obtenir des étiquettes phonétiques du signal de parole.
Cette information orthogonale à la tâche principale doit permettre de démêler des signaux contradictoires pour améliorer la représentation des locuteurs [Zeghidour et al., 2016]).
L'apprentissage actif est un modèle d'apprentissage semi-supervisé où un oracle (généralement, un humain) intervient au cours du processus.
Plus précisément, à partir de données non-étiquetés, l'algorithme d'apprentissage détermine quelles données doivent être annotées par l'oracle pour obtenir les meilleures performances à moindre coût [Lowell et al., 2018, Drugman et al., 2019, Feyisetan et al., 2019, Settles and Craven, 2008, Duong et al., 2018, Kholghi et al., 2016,Settles, 2009].
Avec l'évolution des systèmes d'information des établissements de santé, les praticiens sont tenus de saisir de plus en plus d'informations de manière numérique à travers différents logiciels spécialisés.
La prise en main de ces logiciels impliquent toujours une étape de formation et d'adaptation nécessite l'accès immédiat à un poste de travail, ce qui prend du temps sur leur mission de soin.
Par ailleurs, ces contraintes modifient grandement les habitudes de travail des médecins habitués à saisir leurs ordonnances directement sur une page blanche ou dans un autre logiciel.
Le but de ce travail de thèse est de libérer les prescripteurs d'un certain nombre de ces contraintes, et de leur proposer un outil leur permettant de se rapprocher le plus possible d'une construction la plus *naturelle* et *culturelle* possible de l'ordonnance, dans un langage quasi-naturel.
Pour répondre à ce problème, nous nous plaçons dans un contexte d'interaction vocale sur smartphone qui permet de répondre à deux problèmes observés : l'interaction non naturelle avec les logiciel d'aide à la prescription et le manque d'accès immédiat à un système informatique connecté.
Les systèmes de recommandation essayent de déduire les intérêts de leurs utilisateurs afin de leurs suggérer des items pertinents.
Ces systèmes offrent ainsi aux utilisateurs un service utile car ils filtrent automatiquement les informations non-pertinentes, ce qui évite le problème de surcharge d'information qui est courant de nos jours.
C'est pourquoi les systèmes de recommandation sont aujourd'hui populaires, si ce n'est omniprésents dans certains domaines tels que le World Wide Web.
Cependant, les intérêts d'un individu sont des données personnelles et privées, comme par exemple son orientation politique ou religieuse.
Les systèmes de recommandation recueillent donc des données privées et leur utilisation répandue nécessite des mécanismes de protection de la vie privée.
Dans cette thèse, nous étudions la protection de la confidentialité des intérêts des utilisateurs des systèmes de recommandation appelés systèmes de filtrage collaboratif (FC).
Notre première contribution est Hide &amp; Share, un nouveau mécanisme de similarité, respectueux de la vie privée, pour la calcul décentralisé de graphes de K-Plus-Proches-Voisins (KPPV).
C'est un mécanisme léger, conçu pour les systèmes de FC fondés sur les utilisateurs et décentralisés (ou pair-à-pair), qui se basent sur les graphes de KPPV pour fournir des recommandations.
Notre seconde contribution s'applique aussi aux systèmes de FC fondés sur les utilisateurs, mais est indépendante de leur architecture.
Cette contribution est double : nous évaluons d'abord l'impact d'une attaque active dite « Sybil » sur la confidentialité du profil d'intérêts d'un utilisateur cible, puis nous proposons une contre-mesure.
Celle-ci est 2-step, une nouvelle mesure de similarité qui combine une bonne précision, permettant ensuite de faire de bonnes recommandations, avec une bonne résistance à l'attaque Sybil en question.
Le Center for Data Science de l'Université Paris-Saclay a déployé une plateforme compatible avec le Linked Data en 2016.
Or, les chercheurs rencontrent face à ces technologies de nombreuses difficultés. Pour surmonter celles-ci, une approche et une plateforme appelée LinkedWiki, ont été conçues et expérimentées au-dessus du cloud de l'université (IAAS) pour permettre la création d'environnements virtuels de recherche (VRE) modulaires et compatibles avec le Linked Data.
Nous avons ainsi pu proposer aux chercheurs une solution pour découvrir, produire et réutiliser les données de la recherche disponibles au sein du Linked Open Data, c'est-à-dire du système global d'information en train d'émerger à l'échelle du Web.
Cette expérience nous a permis de montrer que l'utilisation opérationnelle du Linked Data au sein d'une université est parfaitement envisageable avec cette approche.
Cependant, certains problèmes persistent, comme (i) le respect des protocoles du Linked Data et (ii) le manque d'outils adaptés pour interroger le Linked Open Data avec SPARQL.
Nous proposons des solutions à ces deux problèmes.
Afin de pouvoir vérifier le respect d'un protocole SPARQL au sein du Linked Data d'une université, nous avons créé l'indicateur SPARQL Score qui évalue la conformité des services SPARQL avant leur déploiement dans le système d'information de l'université.
De plus, pour aider les chercheurs à interroger le LOD, nous avons implémenté le démonstrateur SPARQLets-Finder qui démontre qu'il est possible de faciliter la conception de requêtes SPARQL à l'aide d'outils d'autocomplétion sans connaissance préalable des schémas RDF au sein du LOD.
Parmi les désignations qui fleurissent dans la presse économique sous l'appellation attractive de « new business models » , deux ont pour point commun de s'appuyer sur une logique de partage de biens : l' « économie de (la) fonctionnalité » et l ' « économie collaborative » .
Elles articulent l'exploitation d'innovations technologiques récentes avec l'évolution de pratiques sociales.
Ces approches entendent tirer profit d'une transformation contemporaine des modes de consommation, caractérisée par une désacralisation du rôle accordé aux biens matériels.
Notre axe de recherche questionne la construction et la signification de plusieurs modèles socio-économiques a priori tournés vers un développement durable.
Bien que la multiplication des désignations sème le trouble dans leurs définitions, chacune d'entre elles peut être reliée à des réseaux d'acteurs distincts.
Si les expressions « économie de fonctionnalité » et « économie de la fonctionnalité » ne se distinguent que par un déterminant, elles renvoient à deux approches en tension.
De même, alors que le terme « économie collaborative » évoque au moment de son émergence la bannière « peer-to-peer » , il se diffuse rapidement pour qualifier une forme de capitalisme connexionniste.
Le déploiement des modèles étudiés permet de capter certaines transformations des représentations contemporaines.
Le relatif succès des modèles est fonction de la correspondance des idéaux qui y sont attachés avec les faits socio-économiques éprouvés par les acteurs.
Évolution des formes de travail, modification des contours de
Ce travail de recherche s'inscrit en sciences de l'information et de la communication et s'intéresse à l'émotion dans la recherche d'information à travers l'exemple des forums de santé.
Le succès de ces dispositifs résulte d'une motivation informationnelle et émotionnelle des participants qui peuvent accéder à des témoignages, des informations ponctuelles ou encore des informations médicales filtrées par le vécu du malade (ou du proche de malade) qui s'exprime.
Les messages sont donc souvent empreints d'émotion.
La problématique s'attache aux évolutions de l'activité d'information et notamment au rôle que peuvent jouer les marqueurs d'émotion dans la structuration des informations mais également lors de leur évaluation.
Une première analyse vise à mettre en évidence l'organisation des messages et des indices d'émotion grâce à une analyse de corpus (de fils de discussion provenant de différents forums de santé).
Une seconde enquête s'attache à l'analyse de données recueillies pendant une phase d'entretiens et d'expérimentations sur l'utilisation des forums de santé et sur la manière dont les participants évaluent les informations.
Les résultats montrent que les informations médicales sont très présentes et majoritairement entremêlées d'indices d'émotion de peur.
Toutefois, la joie est l'émotion la plus présente dans l'ensemble du corpus.
Enfin, si les marqueurs d'émotion sont des critères d'évaluation, il apparaît que les informations médicales sont également des indices d'évaluation et non les informations évaluées.
Cette thèse aborde le problème de l'apprentissage avec des fonctions de perte nonmodulaires.
Pour les problèmes de prédiction, où plusieurs sorties sont prédites simultanément, l'affichage du résultat comme un ensemble commun de prédiction est essentiel afin de mieux incorporer les circonstances du monde réel.
Dans la minimisation du risque empirique, nous visons à réduire au minimum une somme empirique sur les pertes encourues sur l'échantillon fini avec une certaine perte fonction qui pénalise sur la prévision compte tenu de la réalité du terrain.
Dans cette thèse, nous proposons des méthodes analytiques et algorithmiquement efficaces pour traiter les fonctions de perte non-modulaires.
D'abord, nous avons introduit une méthode pour les fonctions de perte supermodulaires, qui est basé sur la méthode d'orientation alternée des multiplicateurs, qui ne dépend que de deux problémes individuels pour la fonction de perte et pour l'infèrence.
Deuxièmement, nous proposons une nouvelle fonction de substitution pour les fonctions de perte submodulaires, la Lovász hinge, qui conduit à une compléxité en O(p log p) avec O(p) oracle pour la fonction de perte pour calculer un gradient ou méthode de coupe.
Enfin, nous introduisons un opérateur de fonction de substitution convexe pour des fonctions de perte nonmodulaire, qui fournit pour la première fois une solution facile pour les pertes qui ne sont ni supermodular ni submodular.
Cet opérateur est basé sur une décomposition canonique submodulaire-supermodulaire.
Le travail que nous présentons dans cette thèse s'inscrit dans une problématique générale d'étude et de conception des MOOCs (Massive Open Online Courses).
Elle s'intéresse plus particulièrement à l'étude didactique d'un MOOC d'algorithmique conçu au profit des étudiants de premier cycle de l'université d'Hassan Premier au Maroc.
Ce travail se situe dans une approche compréhensive et vise plus précisément à comprendre le processus d'élaboration du contenu d'algorithmique véhiculé par le dispositif MOOC et la manière dont les étudiants le construisent au sein et en relation avec ce dispositif.
En considérant le MOOC comme un dispositif didactique, deux approches didactique et épistémologique des activités d'apprentissage en algorithmique ont été articulées.
La notion de performance didactique est mobilisée pour examiner les stratégies d'apprentissage adoptées par les étudiants.
En recourant aux forums de discussion et en mobilisant un questionnaire et des entretiens semi-directifs, les discours des étudiants ont été analysés en vue de caractériser les contenus construits, les performances didactiques et les difficultés rencontrées.
La caractérisation de la conception du MOOC met en évidence deux étapes :
1) l'identification des concepts incontournables en algorithmique : variable, instructions de base, conditions, boucles et leur organisation en unités d'apprentissage
2) la scénarisation pédagogique décrivant les tâches d'apprentissage des unités pédagogiques et leur organisation ; le cours est par ailleurs adapté à la massification des audiences notamment en diminuant la charge horaire des semaines du MOOC et en exigeant peu de prérequis.
L'analyse des pratiques identifiées montre que les étudiants ont construit deux types de contenus, d'une part, des savoirs conceptuels (condition et boucle) et d'autre part des savoirs procéduraux (démarche de résolution d'un problème, exécution d'un algorithme) ; que les étudiants ont manifesté plus de performances didactiques cognitives et techniques que sociales pour construire le contenu.
Plus particulièrement, la démarche de construction du contenu consiste : 1) en des stratégies cognitives d'élaboration telle que la mise en lien du contenu avec les connaissances antérieures et d'organisation à savoir l'utilisation des organigrammes pour construire pas à pas le savoir algorithmique ; 2) en des stratégies techniques en termes de mobilisations des vidéos du MOOC.
Les résultats montrent également que même si les étudiants se sont avérés particulièrement performants dans l'analyse des problèmes (détermination des objets d'entrées et sorties), certaines difficultés subsistent, tels que le passage de la phase d'analyse d'un problème à celle d'élaboration de l'algorithme.
Ce travail de thèse a l'ambition de proposer aux concepteurs pédagogiques des MOOCs des principes utiles pour l'élaboration d'un contenu, d'une part, et ouvrir une voie de recherche en didactiques, sur les dispositifs MOOCs, qui tient compte de la spécificité des contenus véhiculés.
Notre thèse se situe dans le domaine interdisciplinaire de la stylistique computationnelle, à savoir l'application des méthodes statistiques et computationnelles à l'étude du style littéraire.
Historiquement, la plupart des travaux effectués en stylistique computationnelle se sont concentrés sur les aspects lexicaux.
Dans notre thèse, l'accent est mis sur l'aspect syntaxique du style qui est beaucoup plus difficile à analyser étant donné sa nature abstraite.
Comme contribution principale, dans cette thèse, nous travaillons sur une approche à l'étude stylistique computationnelle de textes classiques de littérature française d'un point de vue herméneutique, où découvrir des traits linguistiques intéressants se fait sans aucune connaissance préalable.
Suivant la ligne de pensée herméneutique, nous proposons un processus de découverte de connaissances pour la caractérisation stylistique accentué sur la dimension syntaxique du style et permettant d'extraire des motifs pertinents à partir d'un texte donné.
Ce processus proposé consiste en deux étapes principales, une étape d'extraction de motifs séquentiels suivi de l'application de certaines mesures d'intérêt.
En particulier, l'extraction de tous les motifs syntaxiques possibles d'une longueur donnée est proposée comme un moyen particulièrement utile pour extraire des caractéristiques intéressantes dans un scénario exploratoire.
Nous proposons, évaluons et présentons des résultats sur les trois mesures d'intérêt proposées, basée chacune sur un raisonnement théorique linguistique et statistique différent.
Cette thèse est une contribution à la description du kmhmouʔ – langue à tradition orale parlée au Laos.
Elle présente les caractéristiques générales de cette langue peu décrite dans la région de l'Asie du Sud-Est comme le montre notre référence bibliographique.
Le kmhmouʔ, langue isolante, est dépourvue de marqueurs grammaticaux morphologiques.
Les mots du kmhmouʔ sont pour la plupart pluricatégoriels et plurifonctionnels : la distinction verbo-nominale se fonde essentiellement sur des critères combinatoires.
La description grammaticale s'appuie sur des données spontanées recueillies lors d'un travail de terrain dans les villages kmhmouʔ et à la radio nationale lao pour l'émission en kmhmouʔ.
Le travail de terrain a été conduit en kmhmouʔ, et l'analyse et l'interprétation du corpus ont bénéficié du fait que l'auteur est locuteur natif.
Cette thèse, outre qu'elle rend disponible pour la première fois les données et la grammaire du dialecte du kmhmouʔ de l'Est, ouvre des pistes de réflexion particulièrement intéressantes pour les travaux en typologie des langues isolantes et de la transcatégorialité, ainsi que la réflexion sur le rôle du contact de langue dans la grammaticalisation.
Au cours de ces dernières années, la sécurité des Systèmes d'Information (SI) est devenue une préoccupation importante, qui doit être prise en compte dans toutes les phases du développement du SI, y compris dans la phase initiale de l'ingénierie des exigences (IE).
Prendre en considération la sécurité durant les premières phases du développement des SI permet aux développeurs d'envisager les menaces, leurs conséquences et les contre-mesures avant qu'un système soit mis en place.
Les exigences de sécurité sont connues pour être "les plus difficiles des types d'exigences", et potentiellement celles qui causent le plus de risque si elles ne sont pas correctes.
De plus, les ingénieurs en exigences ne sont pas principalement intéressés à, ou formés sur la sécurité.
Leur connaissance tacite de la sécurité et leur connaissance primitive sur le domaine pour lequel ils élucident des exigences de sécurité rendent les exigences de sécurité résultantes pauvres et trop génériques.
Cette thèse explore l'approche de l'élucidation des exigences fondée sur la réutilisation de connaissances explicites.
Tout d'abord, la thèse propose une étude cartographique systématique et exhaustive de la littérature sur la réutilisation des connaissances dans l'ingénierie des exigences de sécurité identifiant les différentes formes de connaissances.
Suivi par un examen et une classification des ontologies de sécurité comme étant la principale forme de réutilisation.
Dans la deuxième partie, AMAN-DA est présentée.
AMAN-DA est la méthode développée dans cette thèse.
Elle permet l'élucidation des exigences de sécurité d'un système d'information spécifique à un domaine particulier en réutilisant des connaissances encapsulées dans des ontologies de domaine et de sécurité.
En outre, la thèse présente les différents éléments d'AMAN-DA : (I) une ontologie de sécurité noyau, (II) une ontologie de domaine multi-niveau, (iii) des modèles syntaxique de buts et d'exigences de sécurité, (IV) un ensemble de règles et de mécanismes nécessaires d'explorer et de réutiliser la connaissance encapsulée dans les ontologies et de produire des spécifications d'exigences de sécurité.
La dernière partie rapporte l'évaluation de la méthode.
AMAN-DA a été implémenté dans un prototype d'outil.
Sa faisabilité a été évaluée et appliquée dans les études de cas de trois domaines différents (maritimes, applications web, et de vente).
La facilité d'utilisation et l'utilisabilité de la méthode et de son outil ont également été évaluées dans une expérience contrôlée.
L'expérience a révélé que la méthode est bénéfique pour l'élucidation des exigences de sécurité spécifiques aux domaines, et l'outil convivial et facile à utiliser.
Le développement et la multiplication des systèmes et plateformes informatiques pour accéder à de l'information ne fait que s'accentuer depuis une trentaine d'années.
Le grand volume d'information disponible a soulevé de nombreux défis scientifiques dans des domaines tel que la recherche d'information.
Pour accéder à des documents regroupés dans un corpus numérique, il faut être en mesure d'exprimer son besoin en information, souvent sous la forme d'une requête, d'y associer les documents pertinents et de les présenter de la meilleure manière possible aux utilisateurs.
La recherche documentaire dans un corpus documentaire thématique présentant un haut niveau de technicité dans la discipline concernée s'apparente à un processus de navigation guidé par un besoin d'information d'un utilisateur.
Cette navigation nécessite l'usage d'outils classiques de recherche d'information pour sélectionner des documents pertinents en fonction d'une requête, mais ils doivent être complétés par des mécanismes de personnalisation et d'adaptation capable de faire évoluer la représentation du besoin en fonction des spécificités d'un utilisateur, de sa navigation en cours ou du corpus considéré.
En effet, l'accès aux documents d'un corpus numérique soulève des problèmes liés à la recherche d'information, à la visualisation des résultats d'une requête et à la navigation entre les documents.
Le processus de recherche d'information nécessite des améliorations et surtout l'intégration de l'utilisateur comme facteur principal à prendre en compte dans la recherche de satisfaction de son besoin informationnel.
Nous considérons plusieurs approches pour aider un utilisateur dans sa recherche de documents.
Une première assistance porte sur la reformulation de requêtes en visant un public d'utilisateurs peu familier avec les termes techniques du domaine et en difficulté pour exprimer sous la forme d'une requête leur besoin.
La deuxième approche que nous proposons consiste à ne pas considérer l'utilisateur isolément mais à le rapprocher de ceux ayant exprimé des recherches similaires pour retrouver les documents qu'ils avaient jugés pertinents.
Enfin, nous incluons des travaux issus du domaine de la recommandation afin de mieux cerner le besoin informationnel de l'utilisateur et l'aider à trouver plus facilement ce qu'il cherche en lui recommandant des ressources documentaires.
Nous proposons dans cette thèse de traiter cette diversité d'influence par un système multi-agent interagissant par un environnement partagé représentant la navigation des utilisateurs, de manière à pouvoir adapter le système en utilisant l'une ou l'autre des techniques d'assistance proposées en fonction de l'expertise de l'utilisateur.
Ce travail a été appliqué à une recherche documentaire dans un corpus numérique de documents juridiques.
Les robots sont de plus en plus utilisés dans un cadre social.
Il ne suffit plusde partager l'espace avec des humains, mais aussi d'interagir avec eux.
Dans ce cadre, il est attendu du robot qu'il comprenne un certain nombre de signaux ambiguës, verbaux et visuels, nécessaires à une interaction humaine.
En particulier, on peut extraire beaucoup d'information, à la fois sur l'état d'esprit des personnes et sur la dynamique de groupe à l'œuvre, en connaissant qui ou quoi chaque personne regarde.
On parle de la Cible d'attention visuelle, désignée par l'acronyme anglais VFOA.
Dans cette thèse, nous nous intéressons aux données perçues par un robot humanoı̈de qui participe activement à une in-teraction sociale, et à leur utilisation pour deviner ce que chaque personne regarde.
D'une part, le robot doit “regarder les gens”, à savoir orienter sa tête (et donc la caméra) pour obtenir des images des personnes présentes.
Nous présentons une méthode originale d'apprentissage par renforcement pour contrôler la direction du regard d'un robot.
Cette méthode utilise des réseaux de neurones récurrents.
Le robot s'entraı̂ne en autonomie à déplacer sa tête enfonction des données visuelles et auditives.
Il atteint une stratégie efficace, qui lui permet de cibler des groupes de personnes dans un environnement évolutif.
D'autre part, les images du robot peuvent être utilisée pour estimer les VFOAs au cours du temps.
Pour chaque visage visible, nous calculons laposture 3D de la tête (position et orientation dans l'espace) car très fortement corrélée avec la direction du regard.
Nous l'utilisons dans deux applications.
Premièrement, nous remarquons que les gens peuvent regarder des objets qui ne sont pas visible depuis le point de vue du robot.
Sous l'hypothèse que les dits objets soient regardés au moins une partie du temps, nous souhaitons estimer leurs positions exclusivement à partir de la direction du regard des personnes visibles.
Nous utilisons une représentation sous forme de carte de chaleur.
Nous avons élaboré et entraı̂né plusieurs réseaux de convolutions afin d'estimer la régression entre une séquence de postures des têtes, et les positions des objets.
Nous présentons alors un modèle probabiliste, suggéré par des résultats en psychophysique, afin de modéliser la relation entre les postures des têtes, les positions des objets, la direction du regard et les VFOAs.
La formulation utilise un modèle markovien à dynamiques multiples.
En appliquant une approches bayésienne, nous obtenons un algorithme pour calculer les VFOAs au fur et à mesure, et une méthode pour estimer les paramètres du modèle.
Nos contributions reposent sur la possibilité d'utiliser des données, afin d'exploiter des approches d'apprentissage automatique.
Toutes nos méthodes sont validées sur des jeu de données disponibles publiquement.
De plus, la génération de scénarios synthétiques permet d'agrandir à volonté la quantité de données disponibles ; les méthodes pour simuler ces données sont explicitement détaillée.
Acquérir l'adjectif épithète pose deux problèmes majeurs en français.
D'abord, l'adjectif dénote une propriété à propos d'un nom, les enfants doivent donc pouvoir concevoir un objet comme un tout et comme un ensemble de propriétés pour manier un SN avec épithète.
De plus, bien que les locuteurs connaissent cette possibilité, ils optent plutôt pour un placement fixe en usage.
Ces faits nous ont amenée à nous demander si l'input permet à l'enfant de se construire la notion d'adjectif épithète sans avoir recours à des connaissances langagières innées.
Pour y répondre, nous proposons une étude comparant les usages de trois enfants à ceux de leur famille à deux temps de leur acquisition (T1 : 3 ; 8, T2 : 4 ; 6).
Nous étudions quatre aspects de l'usage de l'épithète (lexique, placement, combinaison avec d'autres modifieurs ou un dépendant adjectival) et nous confrontons l'adjectif aux autres modifieurs nominaux.
Ces phénomènes montrent tous la même évolution.
À T1, les enfants emploient la construction la plus fréquente des adultes, avec un fort degré de spécificité lexicale.
À T2, d'autres constructions émergent selon leur ordre de fréquence chez les adultes.
Le lexique de la construction de T1 s'est en outre élargi dans le champ de la classe sémantique des usages de T1.
Les enfants montrent ainsi une sensibilité aux informations quantitatives et une abstraction graduelle des structures par analogie sémantique, qui plaident pour une construction progressive de la notion d'adjectif épithète à partir de l'input.
En conséquence, une grande partie des événements quotidiens a vocation à être numérisée.
Dans ce cadre, le Web contient des descriptions de divers événements du monde réel et provenant du monde entier.
L'ampleur de ces événements peut varier, allant de ceux pertinents uniquement localement à ceux qui retiennent l'attention du monde entier.
La presse et les médias sociaux permettent d'atteindre une diffusion presque mondiale.
L'ensemble de toutes ces données décrivant des événements sociétaux potentiellement complexes ouvre la porte à de nombreuses possibilités de recherche pour analyser et mieux comprendre l'état de notre société.
Dans cette thèse, nous étudions diverses tâches d'analyse de l'impact des événements sociétaux.
Plus précisément, nous abordons trois facettes dans le contexte des événements et du Web, à savoir la diffusion d'événements dans des communautés de langues étrangères, la classification automatisée des contenus Web et l'évaluation et la visualisation de la viralité de l'actualité.
Nous émettons l'hypothèse que les entités nommées associées à un événement ou à un contenu Web contiennent des informations sémantiques précieuses, qui peuvent être exploitées pour créer des modèles de prédiction précis.
À l'aide de nombreuses études, nous avons montré que l'élévation du contenu Web au niveau des entités saisissait leur essence essentielle et offrait ainsi une variété d'avantages pour obtenir de meilleures performances dans diverses tâches.
Nous exposons de nouvelles découvertes sur des tâches disparates afin de réaliser notre objectif global en matière d'analyse de l'impact des événements sociétaux.
L'apprentissage machine est l'étude de la conception d'algorithmes qui apprennent à partir des données d'apprentissage pour réaliser une tâche spécifique.
Le modèle résultant est ensuite utilisé pour prédire de nouveaux points de données (invisibles) sans aucune aide extérieure.
Ces données peuvent prendre de nombreuses formes telles que des images (matrice de pixels), des signaux (sons,...), des transactions (âge, montant, commerçant,...), des journaux (temps, alertes,...).
Les ensembles de données peuvent être définis pour traiter une tâche spécifique telle que la reconnaissance d'objets, l'identification vocale, la détection d'anomalies, etc.
Dans ces tâches, la connaissance des résultats escomptés encourage une approche d'apprentissage supervisé où chaque donnée observée est assignée à une étiquette qui définit ce que devraient être les prédictions du modèle.
Par exemple, dans la reconnaissance d'objets, une image pourrait être associée à l'étiquette "voiture" qui suggère que l'algorithme d'apprentissage doit apprendre qu'une voiture est contenue dans cette image, quelque part.
Cela contraste avec l'apprentissage non supervisé où la tâche à accomplir n'a pas d'étiquettes explicites.
Par exemple, un sujet populaire dans l'apprentissage non supervisé est de découvrir les structures sous-jacentes contenues dans les données visuelles (images) telles que les formes géométriques des objets, les lignes, la profondeur, avant d'apprendre une tâche spécifique.
Ce type d'apprentissage est évidemment beaucoup plus difficile car il peut y avoir un nombre infini de concepts à saisir dans les données.
Dans cette thèse, nous nous concentrons sur un scénario spécifique du cadre d'apprentissage supervisé : 1) l'étiquette d'intérêt est sous-représentée (p. ex. anomalies) et 2) l'ensemble de données augmente avec le temps à mesure que nous recevons des données d'événements réels (p. ex. transactions par carte de crédit).
En fait, ces deux problèmes sont très fréquents dans le domaine industriel dans lequel cette thèse se déroule.
Avec le développement de la robotique grand public apparaît une nouvelle forme de télécommunication  : la robotique de téléprésence.
Le principe consiste à représenter une personne à distance par l'intermédiaire d'un robot mobile, dont elle peut contrôler librement les déplacements.
L'objectif n'est pas simplement de lui permettre de communiquer à distance, mais de lui donner une présence physique et sociale, que le téléphone ou la visioconférence ne suffisent pas à transmettre.
Dans ce contexte, il est particulièrement important de parvenir à transmettre au mieux le «  toucher social  » du pilote du robot  : c'est-à-dire lui permettre d'échanger avec ses interlocuteurs un vaste ensemble de signaux socio-affectifs, qui sont les vecteurs du lien social.
En particulier, cette thèse s'intéresse à un élément fondamental du toucher social et fortement impacté par la téléprésence  : la portée vocale, à travers laquelle un locuteur contrôle qui peut l'entendre, et s'adapte en permanence aux conditions acoustiques de l'environnement.
À travers une première étude, nous nous intéresserons au lien entre toucher vocal et proxémie, en nous demandant si la manière dont un auditeur perçoit à l'aveugle un interlocuteur dans l'espace peut être influencée par les socio-affects produits par celui-ci.
Ensuite, nous montrerons que la portée vocale peut-être affectée par effet Lombard en cas de téléprésence ubiquïte  : le pilote, qui perçoit à la fois son environnement local, et l'environnement du robot, s'adapte au niveau de bruit ambiant, même lorsque ce bruit n'est pas perçu par ses interlocuteurs.
Enfin, nous présenterons notre participation à un projet Arts et Sciences  : le spectacle Aporia, au cours duquel un acteur unique, aidé d'un logiciel de transformation vocale, incarne plusieurs personnages.
Un grand nombre de méthodes de mesure ont ainsi été mises au point afin de la mesurer, mais demeurent immatures, voire contradictoires entre elles.
C'est pourquoi un travail mérite d'être mené afin d'en augmenter la validité et la fiabilité.
Ce travail de thèse s'est attaché donc à utiliser plusieurs de ces méthodes, de les combiner et les articuler selon différentes techniques afin d'améliorer la qualité de la mesure.
Nous nous sommes appuyés pour cela sur un large spectre d'indicateurs, d'ordre physiologiques, comportementaux et auto rapportés et de deux stratégies de triangulation en particulier : multi-facettes et multi-mesures.
Enfin, ces méthodes ont testées dans des cas d'application réels et selon une complexification croissante des procédures et traitements statistiques.
Cela a donné lieu à trois études distinctes.
La première a consisté à évaluer la pertinence d'un algorithme de recommandation de films face à son concurrent en utilisant une stratégie d'évaluation multi-facettes.
Une deuxième étude a été élaborée afin de tester la pertinence de modèles d'évaluation multi-mesures, en évaluant l'utilisabilité de sites universitaires grâce à un logiciel de test utilisateur à distance (Evalyzer) et la combinaison multimodale de divers indicateurs d'utilisabilité.
Enfin, une dernière étude a été réalisée afin de valider un protocole de mesure d'immersion multi-mesure (questionnaire, expression faciale, conductance de la peau, rythme cardiaque, comportement oculaire).
Ces trois études nous ont permis d'évaluer la pertinence d'un certain nombre de mesures (d'utilisabilité et d'expérience utilisateur), la valeur ajoutée de certaines de leurs combinaisons, ainsi qu'un retour critique sur la procédure de validation multi-facettes utilisée dans cette thèse
De nos jours, il existe de nombreuses bases de données géographiques (BDG) couvrant le même territoire.
Les données géographiques sont modélisées différemment (par exemple une rivière peut être modélisée par une ligne ou bien par une surface), elles sont destinées à répondre à plusieurs applications (visualisation, analyse) et elles sont créées suivant des modes d'acquisition divers (sources, processus).
Tous ces facteurs créent une indépendance entre les BDG, qui pose certains problèmes à la fois aux producteurs et aux utilisateurs.
Ainsi, une solution est d'expliciter les relations entre les divers objets des bases de données, c'est-à-dire de mettre en correspondance des objets homologues représentant la même réalité.
La complexité du processus d'appariement fait que les approches existantes varient en fonction des besoins auxquels l'appariement répond, et dépendent des types de données à apparier (points, lignes ou surfaces) et du niveau de détail.
Nous avons remarqué que la plupart des approches sont basées sur la géométrie et les relations topologiques des objets géographiques et très peu sont celles qui prennent en compte l'information descriptive des objets géographiques.
De plus, pour la plupart des approches, les critères sont enchaînés et les connaissances sont à l'intérieur du processus.
Suite à cette analyse, nous proposons une approche d'appariement de données qui est guidée par des connaissances et qui prend en compte tous les critères simultanément en exploitant à la fois la géométrie, l'information descriptive et les relations entre eux.
Afin de formaliser les connaissances et de modéliser leurs imperfections (imprécision, incertitude et incomplétude), nous avons utilisé la théorie des fonctions de croyance [Shafer, 1976].
Notre approche d'appariement de données est composée de cinq étapes : après une sélection des candidats, nous initialisons les masses de croyance en analysant chaque candidat indépendamment des autres au moyen des différentes connaissances exprimées par divers critères d'appariement.
Ensuite, nous fusionnons les critères d'appariement et les candidats.
Enfin, une décision est prise.
Nous avons testé notre approche sur des données réelles ayant des niveaux de détail différents représentant le relief (données ponctuelles) et les réseaux routiers (données linéaires)
Les travaux de cette thèse portent sur la modélisation des émotions pour la synthèse audiovisuelle expressive de la parole à partir du texte.
Aujourd'hui, les résultats des systèmes de synthèse de la parole à partir du texte sont de bonne qualité, toutefois la synthèse audiovisuelle reste encore une problématique ouverte et la synthèse expressive l'est encore d'avantage.
Nous proposons dans le cadre de cette thèse une méthode de modélisation des émotions malléable et flexible, permettant de mélanger les émotions comme on mélange les teintes sur une palette de couleurs.
Dans une première partie, nous présentons et étudions deux corpus expressifs que nous avons construits.
La stratégie d'acquisition ainsi que le contenu expressif de ces corpus sont analysés pour valider leur utilisation à des fins de synthèse audiovisuelle de la parole.
Dans une seconde partie, nous proposons deux architectures neuronales pour la synthèse de la parole.
Nous avons utilisé ces deux architectures pour modéliser trois aspects de la parole : 1) les durées des sons, 2) la modalité acoustique et 3) la modalité visuelle.
Dans un premier temps, nous avons adopté une architecture entièrement connectée.
Cette dernière nous a permis d'étudier le comportement des réseaux de neurones face à différents descripteurs contextuels et linguistiques.
Nous avons aussi pu analyser, via des mesures objectives, la capacité du réseau à modéliser les émotions.
La deuxième architecture neuronale proposée est celle d'un auto-encodeur variationnel.
Cette architecture est capable d'apprendre une représentation latente des émotions sans utiliser les étiquettes des émotions.
Après analyse de l'espace latent des émotions, nous avons proposé une procédure de structuration de ce dernier pour pouvoir passer d'une représentation par catégorie vers une représentation continue des émotions.
Nous avons pu valider, via des expériences perceptives, la capacité de notre système à générer des émotions, des nuances d'émotions et des mélanges d'émotions, et cela pour la synthèse audiovisuelle expressive de la parole à partir du texte.
De nos jours, les informations liées au déplacement et à la mobilité dans un réseau de transport représentent sans aucun doute un potentiel important.
Ces travaux visent à mettre en œuvre un Système d'Information de Service d'Aide à la Mobilité Urbaine (SISAMU).
Le SISAMU doit pouvoir procéder par des processus de décomposition des requêtes simultanées en un ensemble de tâches indépendantes.
Chaque tâche correspond à un service qui peut être proposé par plusieurs fournisseurs d'information en concurrence, avec différents coûts, temps de réponse et formats.
L'aspect dynamique, distribué et ouvert du problème, nous a conduits à adopter une modélisation multi-agent pour assurer au système une évolution continue et une flexibilité pragmatique.
Pour ce faire, nous avons proposé d'automatiser la modélisation des services en utilisant la notion d'ontologie.
Notre SISAMU prend en considération les éventuelles perturbations sur le RETM.
Ansi, nous avons créé un protocole de négociation entre les agents.
Le protocole de négociation proposé qui utilise l'ontologie de la cartographie se base sur un système de gestion des connaissances pour soutenir l'hétérogénéité sémantique.
Nous avons détaillé l'Algorithme de Reconstruction Dynamique des Chemins des Agents (ARDyCA) qui est basé sur l'approche de l'ontologie cartographique.
Finalement, les résultats présentés dans cette thèse justifient l'utilisation de l'ontologie flexible et son rôle dans le processus de négociation
Cette thèse vise à comprendre le risque de fuite d'informations personnelles sur un réseau social.
Nous étudions les violations potentielles de la vie privée, concevons des attaques, prouvons leur faisabilité et analysons leur précision.
Cette approche nous aide à identifier l'origine des menaces et constitue un premier pas vers la conception de contre-mesures efficaces.
Nous avons d'abord introduit une mesure de sensibilité des sujets à travers une enquête par questionnaire.
Puis, nous avons conçu des attaques de divulgation (avec certitude) des liens d'amitié et des liens d'appartenance aux groupes sur “Facebook”.
Ces attaques permettent de découvrir le réseau local d'une cible en utilisant uniquement des requêtes légitimes.
Nous avons également conçu une technique d'échantillonnage pour collecter rapidement des données utiles autour d'une cible.
Les données collectées sont ensuite représentées par des graphes et utilisées pour effectuer des inférences d'attributs (avec incertitude).
Pour augmenter la précision des attaques, nous avons conçu des algorithmes de nettoyage.
Ces algorithmes quantifient la corrélation entre les sujets, sélectionnent les plus pertinents et permettent de gérer la rareté (sparsity) des données.
Enfin, nous avons utilisé un réseau de neurones pour classer les données et déduire les valeurs secrètes d'un attribut sensible d'une cible donnée avec une précision élevée mesurée par AUC sur des données réelles.
Les algorithmes proposés dans ce travail sont inclus dans un système appelé SONSAI qui aide les utilisateurs finaux à contrôler la collecte d'informations sur leur vie privée
Lors du développement de logiciels, la maintenance et l'évolution constituent une partie importante du cycle de vie du développement représentant 80% du coût et des efforts globaux.
Au cours de la maintenance, il arrive que les développeurs aient copier-coller des fragments de code source afin de les réutiliser.
Une telle pratique, apparemment inoffensive, est plus fréquente qu'on ne le pense.
Communément appelés « clones » dans la littérature, ces doublons de code source sont un sujet bien connu et étudié en génie logiciel.
Dans cette thèse, nous visons à mettre en lumière les pratiques du copier-coller sur les artefacts logiciels.
En particulier, nous avons choisi de concentrer nos contributions sur deux types d'artefacts logiciels : Documentation d'API et fichiers de compilation (c.-à-d. Dockerfiles).
Pour les deux contributions, nous suivons une méthodologie d'étude empirique commune.
Tout d'abord, nous montrons que les documentations d'API et les fichiers de construction de logiciels (c.-à-d. Dockerfiles) sont confrontés à des problèmes de doublons et que de tels doublons sont fréquents.
Deuxièmement, nous identifions les raisons derrière l'existence de ces doublons.
Enfin, nous montrons que les deux artefacts logiciels manquent de mécanismes de réutilisation pour faire face aux doublons, et que certains développeurs ont même recours à des outils ad-hoc pour les gérer.
Ce document propose d'apprendre le comportement d'un système à partir d'un ensemble de dialogues annotés.
Le système apprend un comportement optimal via l'apprentissage par renforcement.
Nous montrons qu'il n'est pas nécessaire de définir une représentation de l'espace d'état ni une fonction de récompense.
En effet, ces deux paramètres peuvent être appris à partir du corpus de dialogues annotés.
Nous montrons qu'il est possible pour un développeur de systèmes de dialogue d'optimiser la gestion du dialogue en définissant seulement la logique du dialogue ainsi qu'un critère à maximiser (par exemple, la satisfaction utilisateur).
La première étape de la méthodologie que nous proposons consiste à prendre en compte un certain nombre de paramètres de dialogue afin de construire une représentation de l'espace d'état permettant d'optimiser le critère spécifié par le développeur.
Par exemple, si le critère choisi est la satisfaction utilisateur, il est alors important d'inclure dans la représentation des paramètres tels que la durée du dialogue et le score de confiance de la reconnaissance vocale.
L'espace d'état est modélisé par une mémoire sparse distribuée.
Notre modèle, Genetic Sparse Distributed Memory for Reinforcement Learning (GSDMRL), permet de prendre en compte de nombreux paramètres de dialogue et de sélectionner ceux qui sont importants pour l'apprentissage par évolution génétique.
L'espace d'état résultant ainsi que le comportement appris par le système sont aisément interprétables.
Ces deux méthodes interprètent le critère à optimiser comme étant la récompense globale pour chaque dialogue.
Nous comparons ces deux fonctions sur un ensemble de dialogues simulés et nous montrons que l'apprentissage est plus rapide avec ces fonctions qu'en utilisant directement le critère comme récompense finale.
Nous avons développé un système de dialogue dédié à la prise de rendez-vous et nous avons collecté un corpus de dialogues annotés avec ce système.
Ce corpus permet d'illustrer la capacité de mise à l'échelle de la représentation de l'espace d'état GSDMRL et constitue un bon exemple de système industriel sur lequel la méthodologie que nous proposons pourrait être appliquée
Au vu de l'émergence rapide des nouvelles technologies mobiles et la croissance des offres et besoins d'une société en mouvement en formation, les travaux se multiplient pour identifier de nouvelles plateformes d'apprentissage pertinentes afin d'améliorer et faciliter le processus d'apprentissage à distance.
La prochaine étape de l'apprentissage à distance est naturellement le port de l'apprentissage électronique vers les nouveaux systèmes mobiles.
Jusqu'à présent l'environnement d'apprentissage était soit défini par un cadre pédagogique soit imposé par le contenu d'apprentissage.
Maintenant, nous cherchons, à l'inverse, à adapter le cadre pédagogique et le contenu d'apprentissage au contexte de l'apprenant.
Une plateforme de conception est une solution totale qui permet à une équipe de conception de développer un système sur puce.
Une telle plateforme se compose d'un ensemble de bibliothèques et de circuits réutilisables (IPs), d'outils de CAO et de kits de conception en conformité avec les flots de conception et les méthodologies supportés.
Les spécifications de ce type de plateforme offrent un large éventail d'informations, depuis des paramètres de technologie, jusqu'aux informations sur les outils.
En outre, les développeurs de bibliothèque/IP ont des difficultés à obtenir les données nécessaires à partir ces spécifications en raison de leur informalité et complexité.
Dans cette thèse, nous proposons des méthodologies, des flots et des outils pour formaliser les spécifications d'une plateforme de conception et les traiter.
Cette description proposée vise à être utilisée comme une référence pour générer et valider les bibliothèques et les IPs.
De plus, nous présentons une méthode basée sur des références pour créer une spécification fiable en LDSpecX et des mots-clés basés sur des tâches pour en extraire les données efficacement.
A l'aide des solutions proposées, nous développons une plateforme de spécification.
Nous développons une bibliothèque de cellules standard en utilisant cette plateforme de spécification.
Nous montrons ainsi que notre approche permet de créer une spécification complète et cohérente avec une réduction considérable du temps.
Cette proposition comble également l'écart entre les spécifications et le système automatique existant pour le développement rapide de bibliothèques/IPs.
Tous les travaux de cette thèse ont été développés dans le contexte du Cherenkov Telescope Array (CTA), qui sera le principal observatoire de la prochaine génération pour l'astronomie gamma à très haute énergie au sol.
Le plan de ce travail est d'utiliser les GPU et le Cloud Computing afin d'accélérer les tâches de calcul exigeantes, en développant et en optimisant les pipelines d'analyse de données.
La thèse se compose de deux parties : la première est destinée à l'estimation des performances du CTA pour l'observation de phénomènes violents tels que ceux générant des sursauts de rayons gamma (GRB) et des ondes gravitationnelles, avec un premier travail effectué pour la création des modèles pour le premier CTA Data Challenge (DC1).
La deuxième partie de la thèse est liée au développement des pipelines pour la reconstruction des données de bas niveau provenant des simulations de Monte Carlo.
Dans le chapitre 1, je présente les détails du projet CTA, les télescopes et les performances du réseau, ainsi que les méthodes utilisées pour les dériver des simulations de Monte Carlo.
Plus de 500 AGNs ont été modélisées pour le DC1, qui a été important à la fois pour impliquer davantage de personnes dans l'analyse des données du CTA et pour calculer le temps d'observation nécessaire aux différents KSP.
Les simulations pour les papier du Consortium sur les ondes gravitationnelles et les sursauts gamma ont été créés avec le pipeline ctools_pipe (présenté au chapitre 4), mis en œuvre autour des bibliothèques ctools et gammalib.
Le pipeline est composé de deux parties : la tâche à exécuter (simulation de fond, création de modèle et la partie qui effectue la détection) et dans quel centre de calcul.
La deuxième partie de la thèse est axée sur le développement et l'optimisation des pipelines d'analyse à utiliser pour la reconstruction d'événements à partir de données brutes simulées et pour la visualisation des événements dans un espace 3D.
Ces analyses ont été réalisées à l'aide de ctapipe, un framework pour le prototypage des algorithmes de traitement de données de bas niveau pour CTA.
La structure de la bibliothèque est présentée dans le chapitre 5, avec un accent particulier sur les méthodes de reconstruction qui sont mises en œuvre dans ctapipe, y compris le système ImPACT.
Cette méthode utilise un modèle d'images créé à partir des simulations de Monte Carlo et une "seed" de la méthode de reconstruction standard pour s'adapter entre les modèles afin de trouver une meilleure estimation des paramètres de la gerbe atmosphérique.
Le profilage temporel et les stratégies adoptées pour optimiser le pipeline ImPACT sont présentés au chapitre 6.
L'implémentation d'un pipeline pour l'analyse de l'observation du Large Size Telescope en mode monoscopique et son implémentation GPU avec PyTorch est également présentée.
ctapipe a également été utilisé et développé pour estimer les performances du CTA lors d'observations en mode "pointage divergent", dans lequel les directions de pointage sont légèrement différentes par rapport au mode de pointage parallèle, de sorte que l'hyper champ de vision final de tous les télescopes est plus grand par rapport au mode de pointage parallèle.
Les résolutions angulaires et énergétiques ainsi que la sensibilité sont moins bonnes dans ce scénario, mais le fait d'avoir un hyper field-of-view plus large peut être bénéfique pour d'autres sujets, comme la recherche de sources transitoires.
Les modifications du code de reconstruction introduites dans ctapipe et certains tracés de résolution angulaire pour les gammas de source ponctuelle simulés sont présentés au chapitre 7.Les résultats présentés dans cette thèse sont une démonstration de l'utilisation de techniques logicielles avancées en astrophysique de très haute énergie.
Les discrétisations adaptatives sont importantes dans les problèmes de flux compressible/incompressible puisqu'il est souvent nécessaire de résoudre desdétails sur plusieurs niveaux, en permettant de modéliser de grandes régions d'espace en utilisant un nombre réduit de degrés de liberté (et en réduisant le temps de calcul).
Il existe une grande variété de méthodes de discrétisation adaptative, maisles grilles cartésiennes sont les plus efficaces, grâce à leurs stencils numériques simples et précis et à leurs performances parallèles supérieures.
Et telles performance et simplicité sont généralement obtenues en appliquant un schéma de différences finies pour la résolution des problèmes, mais cette approche de discrétisation ne présente pas, au contraire, un chemin facile d'adaptation.
Dans un schéma de volumes finis, en revanche, nous pouvons incorporer différent stypes de maillages, plus appropriées aux raffinements adaptatifs, en augmentant la complexité sur les stencils et en obtenant une plus grande flexibilité.
L'opérateur de Laplace est un élément essentiel des équations de Navier-Stokes,un modèle qui gouverne les écoulements de fluides, mais il se produit également dans des équations différentielles qui décrivent de nombreux autres phénomènes physiques, tels que les potentiels électriques et gravitationnels.
Il s'agit donc d'un opérateur différentiel très important, et toutes les études qui ont été effectuées sur celui-ci, prouvent sa pertinence.
Dans ce travail seront présentés des approches de différences finies et devolumes finis 2D pour résoudre l'opérateur laplacien, en appliquant des patchs de grilles superposées où un niveau plus fin est nécessaire, en laissant des maillages plus grossiers dans le reste du domaine de calcul.
Ces grilles superposées auront des formes quadrilatérales génériques.
Plus précisément, les sujets abordés seront les suivants : 1) introduction à la méthode des différences finies, méthode des volumes finis, partitionnement des domaines, approximation de la solution ; 2) récapitulatif des différents types de maillages pour représenter de façon discrète la géométrie impliquée dans un problème, avec un focussur la structure de données octree, présentant PABLO et PABLitO.
Le premier est une bibliothèque externe utilisée pour gérer la création de chaque grille, l'équilibrage de charge et les communications internes, tandis que la seconde est l'API Python de cette bibliothèque, écrite ad hoc pour le projet en cours ;
3) la présentation de l'algorithme utilisé pour communiquer les données entreles maillages (en ignorant chacune l'existence de l'autre) en utilisant lesintercommunicateurs MPI et la clarification de l'approche monolithique appliquéeà la construction finale de la matrice pour résoudre le système, en tenantcompte des blocs diagonaux, de restriction et de prolongement ;
4) la présentation de certains résultats ; conclusions, références.
Il est important de souligner que tout est fait sous Python comme framework de programmation, en utilisant Cython pour l'écriture de PABLitO, MPI4Py pour les communications entre grilles, PETSc4py pour les parties assemblage et résolution du système d'inconnues, NumPy pour les objets à mémoire continue.
Le choix de ce langage de programmation a été fait car Python, facile à apprendre et à comprendre, est aujourd'hui un concurrent significatif pour l'informatique numérique et l'écosystème HPC, grâce à son style épuré, ses packages, ses compilateurs et pourquoi pas ses versions optimisées pour des architectures spécifiques.
Les invasions biologiques font partie des changements globaux qui contribuent à la perte de biodiversité.
Les plantes invasives peuvent aussi provoquer des pertes économiques, notamment d'importants coûts pour leur contrôle.
Dans l'archipel des Mascareignes plusieurs programmes de gestion ont été mis en place pour contrôler les espèces invasives.
La perception du public sur les espèces invasives varie fortement entre les parties prenantes.
En raison d'opinions divergentes sur la gestion des invasions, les travaux de recherche et de mise en œuvre de programme de lutte ont récemment généré des conflits d'usage.
Ce travail de thèse a permis de conduire une étude pluridisciplinaire sur les dimensions socio-écologiques et économiques de la gestion de Rubus alceifolius, objet d'un programme de contrôle biologique à l'île de La Réunion.
Nous avons mené une analyse économique des différentes options de gestion de R. alceifolius et des coûts futurs de son invasion.
Nous avons évalué l'impact de la lutte biologique sur le rétablissement des espèces indigènes dans une aire protégée.
Le succès économique et écologique du programme de contrôle biologique de R. alceifolius a été démontré dans les habitats d'altitude &lt; 800 m.
Afin de comprendre la raison des conflits entre les parties prenantes nous avons parallèlement développé un travail de recherche socio-anthropologique.
Nous avons pu mettre en évidence des faiblesses dans le processus de prise de décision et de mise en œuvre collective de ce programme de lutte.
Nous abordons d'abord le problème de la représentation sous forme d'un graphe de l'image et de ses applications en reconnaissance des formes, en mettant l'accent sur les applications de recherche d'images basées sur le contenu (CBIR).
Les images utilisées dans cette thèse sont des images de bandes dessinées, qui possèdent des spécificités qui sont des freins pour les méthodes de recherche d'information par le contenu utilisées dans la littérature.
Nous proposons ainsi une représentation qui permet d'obtenir des graphes stables et qui conserve des informations structurelles de haut niveau pour les objets d'intérêt dans les images de bandes dessinées.
Ensuite, nous étendons le problème d'indexation et d'appariement aux structures de graphes représentant les images d'une bande dessinée et nous l'appliquons au problème de la recherche d'information.
Un album de bandes dessinées est ainsi transformé en une base de graphes, chaque graphe correspondant à la description d'une seule case.
La stratégie utilisée pour retrouver un objet ou un personnage donné, consiste donc à rechercher des motifs fréquents (ou des sous-structures fréquentes) dans cette base de graphes.
Il apparait donc un écart sémantique entre le graphe et le contenu de l'image de bande dessinée.
Nous étudions des données de nature diverse sous forme de flux, en particulier :
Base de données,
Réseaux sociaux,
Données de texte.
Pour une base de données qui suit un schéma relationnel, un schéma d'analyse OLAP (Online Analytical Processing) définit une des tables de la base de données comme une table d'analyse.
Nous supposons que les tuples de la table d'analyse arrivent sous forme d'un flux.
Nous étudions l'approximation des requêtes OLAP, en échantillonnant de manière non uniforme les tuples du flux sans stocker les données d'analyse et donnons un modèle de préférence dans ce cadre.
Dans le cas du réseau social Twitter, nous observons un flux de tweets qui contiennent un tag donné et le transformons en un flux d'arêtes d'un graphe.
Nous souhaitons étudier l'existence des grands clusters dans le graphe ainsi obtenu.
Nous proposons une méthode d'échantillonnage uniforme qui va associer au graphe un sous-graphe aléatoire et étudions les composantes géantes de ce sous-graphe aléatoire comme témoin des grands clusters du graphe d'origine.
Pour un flux de texte, nous considérons les paires de mots dans une phrase lemmatisée comme des arêtes d'un graphe où les nœuds sont les mots.
Nous transformons le flux de texte en flux d'arêtes.
Nous échantillonnons les arêtes proportionnellement à la similarité Word2vec des mots.
Nous analysons ensuite les composantes géantes.
Nous étendons les vecteurs Word2vec en prenant en compte la morphologie d'une langue, en particulier la structure des préfixes et des suffixes d'un mot.
Cette étude s'appuie sur l'enseignement de la lecture-compréhension en français pour un public universitaire débutant en FLE au Brésil.
En effet, malgré la diffusion de cet enseignement parmi les étudiants en Amérique latine et sa constante réélaboration afin de rendre compte de nouvelles situations d'apprentissage, il présente des lacunes importantes.
Ces lacunes se trouvent autour d'un point névralgique pour une grande partie des lecteurs : le déficit lexical.
Un déficit qui n'est pas lié qu'à un simple manque de vocabulaire, mais qui passe aussi par un savoir-apprendre les mots.
Cette situation est mise à l'épreuve dans cette thèse par deux recherches expérimentales.
Une première recherche à caractère plutôt prospectif vise à repérer le rôle du dictionnaire, et particulièrement du dictionnaire bilingue lors de la lecture et l'effet de son usage sur la construction du sens.
Une deuxième recherche met en rapport l'emploi de deux outils lexicographiques, un dictionnaire bilingue et un dictionnaire pédagogique destiné à des apprenants de FLE.
Ces recherches nous donnent des pistes essentielles pour l'intégration de l'étude lexicale à ce genre d'enseignement et sont complétées par l'analyse des dictionnaires disponibles pour ce public particulier.
Ce parcours nous aide à tracer les principes méthodologiques pour l'élaboration d'un dictionnaire pédagogique d'appui à la lecture et à l'acquisition lexicale fondés sur la lexicographie fonctionnelle.
Notre but dans cette thèse est de construire un système qui réponde à une question en langue naturelle (NL) en représentant sa sémantique comme une forme logique (LF) et ensuite en calculant une réponse en exécutant cette LF sur une base de connaissances.
La partie centrale d'un tel système est l'analyseur sémantique qui transforme les questions en formes logiques.
Notre objectif est de construire des analyseurs sémantiques performants en apprenant à partir de paires (NL, LF).
Nous proposons de combiner des réseaux neuronaux récurrents (RNN) avec des connaissances préalables symboliques exprimées à travers des grammaires hors-contexte (CFGs) et des automates.
En intégrant des CFGs contrôlant la validité des LFs dans les processus d'apprentissage et d'inférence des RNNs, nous garantissons que les formes logiques générées sont bien formées ; en intégrant, par le biais d'automates pondérés, des connaissances préalables sur la présence de certaines entités dans la LF, nous améliorons encore la performance de nos modèles.
Expérimentalement, nous montrons que notre approche permet d'obtenir de meilleures performances que les analyseurs sémantiques qui n'utilisent pas de réseaux neuronaux, ainsi que les analyseurs à base de RNNs qui ne sont pas informés par de telles connaissances préalables.
La classification automatique des messages courts est de plus en plus employée de nos jours dans diverses applications telles que l'analyse des sentiments ou la détection des « spams » .
Nous présentons dans cette thèse deux nouvelles approches visant à améliorer la classification de ce type de message.
Notre première approche est nommée « forêts sémantiques » .
Dans le but d'améliorer la qualité des messages, cette approche les enrichit à partir d'une source externe construite au préalable.
Puis, pour apprendre un modèle de classification, contrairement à ce qui est traditionnellement utilisé, nous proposons un nouvel algorithme d'apprentissage qui tient compte de la sémantique dans le processus d'induction des forêts aléatoires.
Notre deuxième contribution est nommée « IGLM » (Interactive Generic Learning Method).
C'est une méthode interactive qui met récursivement à jour les forêts en tenant compte des nouvelles données arrivant au cours du temps, et de l'expertise de l'utilisateur qui corrige les erreurs de classification.
L'ensemble de ce mécanisme est renforcé par l'utilisation d'une méthode d'abstraction permettant d'améliorer la qualité des messages.
Les différentes expérimentations menées en utilisant ces deux méthodes ont permis de montrer leur efficacité.
Enfin, la dernière partie de la thèse est consacrée à une étude complète et argumentée de ces deux prenant en compte des critères variés tels que l'accuracy, la rapidité, etc.
La littérature recense de nombreux traits linguistiques faisant partie de ces idées associées à un genre ou l'autre tels que la tournure interrogative (tag question) ou la déférence par exemple.
De toutes les caractéristiques linguistiques genrées, celle qui a probablement été le plus débattue est celle concernant l'utilisation des jurons.
A cause d'une interaction complexe entre pression et pouvoir social, la vulgarité a traditionnellement été associée à l'idée de masculinité avant tout.
Utiliser des jurons est souvent considéré comme étant l'affirmation linguistique d'une forme de pouvoir social.
Par conséquent, l'association intrinsèque de la vulgarité comme forme de pouvoir à un genre ou l'autre pourrait conduire à l'association d'autres caractéristiques sociales aux questions de masculinité ou de féminité, qu'elles soient fondées ou non.
Certaines études ont démontré que, contrairement aux idées longtemps répandues, les femmes n'utilisent pas la vulgarité moins souvent que les hommes, pas plus qu'elles n'utilisent un registre linguistique fondamentalement différent.
Certaines ont même prédit que l'utilisation de jurons dits « forts » (i.e. « strong swear words » ) chez les femmes augmenterait dans certains contextes, et en particulier sur les réseaux sociaux (Thelwall, 2008) ; ceci s'appliquerait particulièrement aux jeunes générations de femmes.
En d'autres termes, l'utilisation de certains jurons chez ces jeunes générations de femmes deviendrait à terme plus fréquente que celle des hommes du même âge.
Par conséquent, la question suivante se pose : les prévisions faites par Thelwall en 2008 se sont-elles réalisées près de dix ans plus tard, dans une société où les médias sociaux n'ont jamais eu autant d'importance dans notre vie quotidienne ?
Cette étude est basée sur un corpus composé d'un peu plus de dix-huit millions de tweets représentatifs de près de 739 000 utilisateurs.
Le corpus a été constitué à partir de tweets provenant du Royaume Unis, émis par des utilisateurs masculins et féminins, et appartenant à différentes tranches d'âge.
Une méthodologie et des outils d'analyse issus de la linguistique dite de corpus ont été utilisés pour mener à bien ce projet et tenter de répondre aux problématiques soulevées précédemment.
Aussi, en raison du manque d'informations démographiques directement associées aux profils Twitter (e.g. le genre et l'âge des utilisateurs), il fût nécessaire de recourir à des techniques issues de la programmation informatique afin d'inférer le genre et l'âge de ces personnes.
Par exemple, plusieurs millions de photos sont partagées quotidiennement sur les réseaux sociaux.
Les méthodes d'interprétation d'images vise à faciliter l'accès à ces données visuelles, d'une manière sémantiquement compréhensible.
Dans ce manuscrit, nous définissons certains buts détaillés qui sont intéressants pour les taches d'interprétation d'images, telles que la classification ou la recherche d'images, que nous considérons dans les trois chapitres principaux.
Tout d'abord, nous visons l'exploitation de la nature multimodale de nombreuses bases de données, pour lesquelles les documents sont composés d'images et de descriptions textuelles.
Dans ce but, nous définissons des similarités entre le contenu visuel d'un document, et la description textuelle d'un autre document.
Ces similarités sont calculées en deux étapes, tout d'abord nous trouvons les voisins visuellement similaires dans la base multimodale, puis nous utilisons les descriptions textuelles de ces voisins afin de définir une similarité avec la description textuelle de n'importe quel document.
Ensuite, nous présentons une série de modèles structurés pour la classification d'images, qui encodent explicitement les interactions binaires entre les étiquettes (ou labels).
Un scenario interactif comme celui-ci offre un compromis intéressant entre la précision, et l'effort d'annotation manuelle requis.
Nous explorons les modèles structurés pour la classification multi-étiquette d'images, pour la classification d'image basée sur les attributs, et pour l'optimisation de certaines mesures de rang spécifiques.
Enfin, nous explorons les classifieurs par k plus proches voisins, et les classifieurs par plus proche moyenne, pour la classification d'images à grande échelle.
Nous proposons des méthodes d'apprentissage de métrique efficaces pour améliorer les performances de classification, et appliquons ces méthodes à une base de plus d'un million d'images d'apprentissage, et d'un millier de classes.
Comme les deux méthodes de classification permettent d'incorporer des classes non vues pendant l'apprentissage à un coût presque nul, nous avons également étudié leur performance pour la généralisation.
Nous montrons que la classification par plus proche moyenne généralise à partir d'un millier de classes, sur dix mille classes à un coût négligeable, et les performances obtenus sont comparables à l'état de l'art.
Entailment vise à capturer les principaux besoins d'inférence sémantique dans les applications de Traitement du Langage Naturel.
Depuis 2005, dans la Textual Entailment reconnaissance tâche (RTE), les systèmes sont appelés à juger automatiquement si le sens d'une portion de texte, le texte - T, implique le sens d'un autre texte, l'hypothèse - H
Cette thèse nous nous intéressons au cas particulier de l'implication, l'implication de généralité.
Pour nous, il y a différents types d'implication, nous introduisons le paradigme de l'implication textuelle en généralité, qui peut être définie comme l'implication d'une peine spécifique pour une phrase plus générale, dans ce contexte, le texte T implication Hypothèse H, car H est plus générale que T.
Nous proposons des méthodes sans surveillance indépendante de la langue de reconnaissance de l'implication textuelle par la généralité, pour cela, nous présentons une mesure asymétrique informatif appelée Asymmetric simplifié InfoSimba, que nous combinons avec différentes mesures d'association asymétriques à reconnaître le cas spécifique de l'implication textuelle par la généralité.
Cette thèse, nous introduisons un nouveau concept d'implication, les implications de généralité, en conséquence, le nouveau concept d'implications de la reconnaissance par la généralité, une nouvelle orientation de la recherche en Traitement du Langage Naturel.
Cette thèse de doctorat traite de l'inférence variationnelle et de la robustesse en statistique et en machine learning.
Plus précisément, elle se concentre sur les propriétés statistiques des approximations variationnelles et sur la conception d'algorithmes efficaces pour les calculer de manière séquentielle, et étudie les estimateurs basés sur le Maximum Mean Discrepancy comme règles d'apprentissage qui sont robustes à la mauvaise spécification du modèle.
Ces dernières années, l'inférence variationnelle a été largement étudiée du point de vue computationnel, cependant, la littérature n'a accordé que peu d'attention à ses propriétés théoriques jusqu'à très récemment.
Dans cette thèse, nous étudions la consistence des approximations variationnelles dans divers modèles statistiques et les conditions qui assurent leur consistence.
En particulier, nous abordons le cas des modèles de mélange et des réseaux de neurones profonds.
Nous justifions également d'un point de vue théorique l'utilisation de la stratégie de maximisation de l'ELBO, un critère numérique qui est largement utilisé dans la communauté VB pour la sélection de modèle et dont l'efficacité a déjà été confirmée en pratique.
En outre, l'inférence Bayésienne offre un cadre d'apprentissage en ligne attrayant pour analyser des données séquentielles, et offre des garanties de généralisation qui restent valables même en cas de mauvaise spécification des modèles et en présence d'adversaires.
Malheureusement, l'inférence Bayésienne exacte est rarement tractable en pratique et des méthodes d'approximation sont généralement employées, mais ces méthodes préservent-elles les propriétés de généralisation de l'inférence Bayésienne ?
Dans cette thèse, nous montrons que c'est effectivement le cas pour certains algorithmes d'inférence variationnelle (VI).
Nous proposons de nouveaux algorithmes tempérés en ligne et nous en déduisons des bornes de généralisation.
Notre résultat théorique repose sur la convexité de l'objectif variationnel, mais nous soutenons que notre résultat devrait être plus général et présentons des preuves empiriques à l'appui.
Notre travail donne des justifications théoriques en faveur des algorithmes en ligne qui s'appuient sur des méthodes Bayésiennes approchées.
Une autre question d'intérêt majeur en statistique qui est abordée dans cette thèse est la conception d'une procédure d'estimation universelle.
Cette question est d'un intérêt majeur, notamment parce qu'elle conduit à des estimateurs robustes, un thème d'actualité en statistique et en machine learning.
Nous abordons le problème de l'estimation universelle en utilisant un estimateur de minimisation de distance basé sur la Maximum Mean Discrepancy.
Nous montrons que l'estimateur est robuste à la fois à la dépendance et à la présence de valeurs aberrantes dans le jeu de données.
Nous mettons également en évidence les liens qui peuvent exister avec les estimateurs de minimisation de distance utilisant la distance L2.
Enfin, nous présentons une étude théorique de l'algorithme de descente de gradient stochastique utilisé pour calculer l'estimateur, et nous étayons nos conclusions par des simulations numériques.
Nous proposons également une version Bayésienne de notre estimateur, que nous étudions à la fois d'un point de vue théorique et d'un point de vue computationnel.
Cette thèse propose d'examiner les façons dont l'expérience de visite peut être transformée dans le contexte d'une culture numérique qui touche les publics réels et les publics potentiels du musée.
On s'interroge plus précisément sur les processus d'appropriation et de partage qu'entraine la pratique muséale à l'époque de la culture numérique.
Pour explorer cet objet de recherche, nous construisons un appareil méthodologique interdisciplinaire
Il permet d'élaborer un cadre analytique susceptible de saisir des pratiques qui se partagent et se combinent entre un espace numérique et un espace physique.
De la sorte, nous proposons des protocoles de recherches hybrides et exploratoires qui offrent la possibilité de saisir, par leur association, des pratiques multiples de l'expérience de visite dans la triple temporalité de l'avant, du pendant et de l'après visite du musée.
À travers cette thèse nous mettons en évidence la façon dont le numérique intervient dans les pratiques de l'expérience de visite comme un objet technique spécifique et un système d'échange avec et dans lequel le visiteur et le visiteur potentiel reconfigurent leur relation à l'institution muséale, à sa collection, aux expositions et aux autres visiteurs et visiteurs potentiels.
Cette étude est une nouvelle adaptation de la théorie linguistique systémique, créée en 1987 par Sylviane CARDEY, dans laquelle nous proposons un système de règles basé sur les faits et les définitions linguistiques et présenté sous forme de modèle qui prend en considération les principes de la modélisation mathématique.
L'objectif de cette thèse est de démontrer aux apprenants de la langue étrangère le mécanisme de son fonctionnement, notamment celui de la grammaire, par des méthodes adéquates et des programmes informatiques soigneusement travaillés pour un style d'apprentissage cohérent.
Notre propos porte donc sur une analyse précise et détaillée concernant les règles de la grammaire, ce qui nous permet d'obtenir une matière didactique claire et exhaustive avec laquelle l'apprenant peut étudier le sujet dans sa globalité, en identifiant ses différentes règles et la corrélation entres elles et entre ses domaines.
Ces derniers vont par la suite pouvoir être utilisés dans un cadre pratique, celui de l'enseignement de la grammaire.
Nous avons également dans ce but construit un site accessible aux enseignants susceptibles de l'utiliser pour leurs cours.
Dans un second temps, après avoir traité la partie théorique, nous avons appliqué la théorie dans le deuxième chapitre sur la conjugaison des verbes arabes.
Le corpus que nous avons choisi pour cet objectif est constitué par les 127 verbes modèles du Bescherelle arabe.
Dans ce chapitre, nous avons utilisé notre système de règles pour décrire le système de la conjugaison arabe.
L'application a également permis de mettre en évidence certains défauts que l'on retrouve dans divers manuels de grammaire linguistiques ou autres
La mesure des différents phénomènes terrestres et l'échange d'informations ont permis l'émergence d'un type de données appelé série temporelle.
Celle-ci se caractérise par un grand nombre de points la composant et surtout par des interactions entre ces points.
En outre, une série temporelle est dite multivariée lorsque plusieurs mesures sont captées à chaque instant de temps.
Bien que l'analyse des séries temporelles univariées, une mesure par instant, soit très développée, l'analyse des séries multivariées reste un challenge ouvert.
Or les méthodes mises à disposition, aujourd'hui, pour la classification supervisée de ces séries, ne permettent pas de répondre de manière satisfaisante à cette problématique en plus d'une gestion rapide et efficace des données.
Cette approche emploie donc un nouvel outil, qui n'a jamais été utilisé dans le domaine de la classification de séries temporelles multivariées, qui est le M-histogramme pour répondre à cette question.
Un M-histogramme est à la base une méthode de visualisation sur M axes de la fonction de densité sous-jacente à un échantillon de données.
Son utilisation ici permet de produire une nouvelle représentation de nos données afin de mettre en évidence les interactions entre dimensions.
Cette recherche de liens entre dimensions correspond aussi tout particulièrement à un sous-domaine d'apprentissage, appelé l'apprentissage multi-vues.
Où une vue est une extraction de plusieurs dimensions d'un ensemble de données, de même nature ou type.
L'objectif est alors d'exploiter le lien entre ces dimensions afin de mieux classifier les dites données, au travers d'un modèle ensembliste permettant d'agréger les prédictions émises à partir de chaque vue.
Dans cette thèse, nous proposons donc une méthode multi-vues ensembliste de M-histogrammes afin de classifier les Séries Temporelles Multivariées (STM).
Cela signifie que plusieurs M-histogrammes sont créés à partir de plusieurs vues des STM exploitées.
Une prédiction est ensuite réalisée grâce à chaque M-histogramme.
Enfin ces prédictions sont ensuite agrégées afin de produire une prédiction finale.
Cette thèse a pour objectif de démontrer l'existence d'une quatrième vague féministe ayant émergé en France au début des années 2010.
Le concept de vague est défini comme un cycle de mobilisation féministe qui peut être constaté par la conjonction de trois critères interdépendants et empiriquement testables.
Le premier critère requiert d'observer une hausse du traitement médiatique de la cause des femmes sur une période donnée.
D'un point de vue qualitatif, ce traitement, bien qu'hétérogène, doit être globalement favorable à l'égalité entre les femmes et les hommes.
Le deuxième critère implique de constater une transformation des idées et / ou pratiques des mouvements féministes.
Ces transformations sont le signe d'une adaptation réussie des mouvements aux évolutions sociales et techniques, elles prouvent qu'ils ont été capables de rendre leur discours audible dans un espace-tempsdonné – ce dont atteste l'existence du premier critère.
Enfin, le troisième critère est celui du renouvellement générationnel : il a pour intérêt essentiel d'historiciser l'analyse en confrontant l'émergence d'un cycle de mobilisation à l'évolution des grammaires mobilisatrices et des conditions produisant des incitations à la mobilisation.
Cette recherche s'inscrit dans le domaine de la didactique du FLE et s'intéresse à l'intégration de la pratique théâtrale dans l'enseignement-apprentissage de l'expression orale en FLE, en s'appuyant sur l'utilisation du téléphone portable et des réseaux sociaux.
Elle interroge la potentialité du jeu théâtral pour le développement et l'amélioration des compétences d'expression orale des apprenants, avec une focalisation sur la prononciation.
Dans le cadre d'une recherche action à caractère ethnographique s'appuyant sur l'approche par les tâches, un dispositif hybride d'apprentissage alliant séances en présentiel et activités à distance a été mis en place en faveur d'étudiants marocains inscrits en Licence d'Etudes Françaises.
En vue de mesurer l'impact du dispositif sur le développement des compétences langagières à l'oral, nous avons analysé la précision phonologique et la fluidité des productions orales des apprenants par le biais d'un pré-test et d'un post test, et lors de l'observation directe des séances (analyse d'enregistrements audio) dans lesquelles les apprenants s'entraînaient à répéter des extraits de scène d'une pièce de théâtre afin d'en présenter une mise en scène à l'issue de la formation.
Les résultats obtenus ont fait ressortir des acquisitions langagières avec une nette amélioration de la précision phonologique pour l'ensemble des apprenants, mais un gain en fluidité pour seulement une partie des apprenants.
Dans l'ensemble ils ont évalué positivement le dispositif et en sont globalement satisfaits.
Cependant, ils n'ont pas tous donné la même importance au travail en groupe que suscite le projet de mise en scène d'une pièce de théâtre, tout comme ils n'ont pas tous montré la même implication.
Le domaine du traitement automatique de la parole regroupe un très grand nombre de tâches parmi lesquelles on trouve la reconnaissance de la parole, l'identification de la langue ou l'identification du locuteur.
Ce domaine de recherche fait l'objet d'études depuis le milieu du vingtième siècle mais la dernière rupture technologique marquante est relativement récente et date du début des années 2010.
C'est en effet à ce moment qu'apparaissent des systèmes hybrides utilisant des réseaux de neurones profonds (DNN) qui améliorent très notablement l'état de l'art.
Dans cette thèse, nous nous intéressons tout particulièrement aux RNN à mémoire court-terme persistante (Long Short Term Memory (LSTM) qui permettent de s'affranchir d'un certain nombre de difficultés rencontrées avec des RNN standards.
Nous augmentons ce modèle et nous proposons des processus d'optimisation permettant d'améliorer les performances obtenues en segmentation parole/non-parole et en identification de la langue.
Cette recherche s'intéresse à la possibilité d'établir des points de comparaison entre le prélinguistique et le linguistique dans la période des premiers mots.
Le constat d'un flou régnant autour des notions de mot et de proto-mot nous a fait considérer différentes approches : historique, épistémologique et expérimentale.
L'apport de l'approche historique est essentiel pour cerner la problématique et considérer la façon dont parler est envisagé par une société et une époque.
Cettepartie nous permet de mettre en avant deux éléments : la question de l'émergence de la parole implique la notion de représentation sociale, et l'émergence de la parole, aujourd'hui, se situe durant la période des premiers mots.
L'analyse de cette période nous conduit à une partie épistémologique permettant de définir le type d'unités caractéristiques de cette période : les proto-mots et les mots.
Une fois nos unités identifiées, nous effectuons une analyse longitudinale de quatre enfants, de un an à deux ans.
Nous avons d'abord identifié un phénomène de substitution des mots aux proto-mots.
Ensuite, nous avons observé les deux éléments communs à ces productions : la prosodie et la phonologie.
Il découle de notre analyse que la prosodie fournit un cadre commun assurant la transition entre les proto-mots et les mots, et que la phonologie est le domaine où s'observent les différences : les mots sont le lieu du développement des structures phonologiques complexes, contrairement aux proto-mots.
Nous avons pu considérer que parler, c'était privilégier les motscomme support de communication verbale, par rapport aux proto-mots, et que cette particularité est l'objet du développement phonologique.
Alors que Twitter évolue vers un outil omniprésent de diffusion de l'information, la compréhension des tweets en langues étrangères devient un problème important et difficile.
En raison de la nature intrinsèquement à commutation de code, discrète et bruitée des tweets, la traduction automatique (MT) à l'état de l'art n'est pas une option viable (Farzindar &amp; Inkpen, 2015).
En effet, au moins pour le hindi et le japonais, nous observons que le pourcentage de tweets « compréhensibles » passe de 80% pour les locuteurs natifs à moins de 30% pour les lecteurs monolingues cible (anglais ou français) utilisant Google Translate.
Notre hypothèse de départ est qu'il devrait être possible de créer des outils génériques, permettant aux étrangers de comprendre au moins 70% des « tweets locaux » , en utilisant une interface polyvalente de « lecture active » (LA, AR en anglais) tout en déterminant simultanément le pourcentage de tweets compréhensibles en-dessous duquel un tel système serait jugé inutile par les utilisateurs prévus.
Nous avons donc spécifié un « SUFT » (système d'aide à la compréhension des tweets étrangers) générique, et mis en œuvre SUFT-1, un système interactif à mise en page multiple basé sur la LA, et facilement configurable en ajoutant des dictionnaires, des modules morphologiques et des plugins de TA.
Il est capable d'accéder à plusieurs dictionnaires pour chaque langue source et fournit une interface d'évaluation.
Pour les évaluations, nous introduisons une mesure liée à la tâche induisant un coût négligeable, et une méthodologie visant à permettre une « évaluation continue sur des données ouvertes » , par opposition aux mesures classiques basées sur des jeux de test liés à des ensembles d'apprentissage fermés.
Nous proposons de combiner le taux de compréhensibilité et le temps de décision de compréhensibilité comme une mesure de qualité à deux volets, subjectif et objectif, et de vérifier expérimentalement qu'une présentation de type lecture active, basée sur un dictionnaire, peut effectivement aider à comprendre les tweets mieux que les systèmes de TA disponibles.
En plus de rassembler diverses ressources lexicales, nous avons construit une grande ressource de "formes de mots" apparaissant dans les tweets indiens, avec leurs analyses morphologiques (à savoir 163221 formes de mots hindi dérivées de 68788 lemmes et 72312 formes de mots marathi dérivées de 6026 lemmes) pour créer un analyseur morphologique multilingue spécialisé pour les tweets, capable de gérer des tweets à commutation de code, de calculer des traits unifiés, et de présenter un tweet en lui attachant un graphe de LA à partir duquel des lecteurs étrangers peuvent extraire intuitivement une signification plausible, s'il y en a une.
Les systèmes informatiques impliquant la détection d'anomalies émergent aussi bien dans le domaine de la recherche que dans l'industrie.
Ainsi, des domaines aussi variés que la médecine (identification de tumeurs malignes), la finance (détection de transactions frauduleuses), les technologies de l'information (détection d'intrusion réseau) et l'environnement (détection de situation de pollution) sont largement impactés.
L'apprentissage automatique propose un ensemble puissant d'approches qui peuvent aider à résoudre ces cas d'utilisation de manière efficace.
Il implique également plusieurs experts qui travailleront ensemble pour trouver les bonnes approches.
De plus, les possibilités ouvertes aujourd'hui par le monde de la sémantique montrent qu'il est possible de tirer parti des technologies du web afin de raisonner intelligemment sur les données brutes pour en extraire de l'information à forte valeur ajoutée.
L'absence de systèmes combinant les approches numériques d'apprentissage automatique et les techniques sémantiques du web des données constitue la motivation principale derrière les différents travaux proposés dans cette thèse.
Enfin, les anomalies détectées ne signifient pas nécessairement des situations de réalité anormales.
En effet, la présence d'informations externes pourrait aider à la prise de décision en contextualisant l'environnement dans sa globalité.
Exploiter le domaine spatial et les réseaux sociaux permet de construire des contextes enrichis sur les données des capteurs.
Ces contextes spatio-temporels deviennent ainsi une partie intégrante de la détection des anomalies et doivent être traités en utilisant une approche Big Data.
Son originalité tient dans sa capacité à raisonner intelligemment sur des données brutes afin d'inférer des informations implicites à partir d'informations explicites et d'aider dans la prise de décision.
Cette plateforme a été développée dans le cadre d'un projet FUI dont le principal cas d'usage est la détection d'anomalies dans un réseau d'eau potable.
RAMSSES :
Système hybride d'apprentissage automatique dont l'originalité est de combiner des approches numériques avancées ainsi que des techniques sémantiques éprouvées.
Système intelligent de "scrapping web" permettant la contextualisation des singularités liées à l'Internet des Objets en exploitant aussi bien des informations spatiales que le web des données
La prise en compte des enjeux environnementaux est un sujet très présent dans notre société actuelle.
Dans le domaine du développement de produits, l'Eco-conception est une démarche qui permet la considération de ces enjeux en proposant de réduire les impacts environnementaux des produits tout au long de leur cycle de vie.
La phase d'utilisation du cycle de vie est une étape cruciale puisque le mode d'utilisation des produits peut avoir des conséquences non négligeables sur leur performance environnementale.
Nous proposons dans cette thèse d'enrichir la compréhension de cette phase d'utilisation en mettant en évidence les composantes Kansei qui jouent un rôle dans l'interaction de l'utilisateur avec le produit et qui peuvent être intégrées en amont de la conception de produits à faible impact environnemental.
Notre démarche permet de mieux renseigner la phase d'utilisation, contribuant ainsi à la maitrise de la performance environnementale des produits.
Nous démontrons à travers notre recherche que l'utilisateur peut être défini, non seulement par des informations basiques que nous retrouvons communément dans la Conception Centrée Utilisateur, mais également à partir d'informations subjectives apportées par la dimension Kansei.
A travers nos expérimentations, nous mettons en application deux attributs EcoKansei correspondants aux valeurs et aux émotions afin d'illustrer la modélisation utilisateur pour la conception de produits à faible impact environnemental.
L'approche que nous proposons vient en complément au projet EcoUse qui vise à développer une méthodologie d'Eco-conception centrée utilisateur.
Les apports de notre démarche sont multiples.
Du point de vue de la recherche, une mise en relation de l'Eco-conception et du Kansei, qui sont à la base complètement déconnectées entraine un enrichissement mutuel entre ces deux approches.
Du point de vue industriel, une démarche d'Eco-conception appuyée par les études Kansei doit permettre la conception de nouveaux produits qui auront l'avantage d'être à la fois mieux acceptés par les utilisateurs car en adéquation avec leur sensibilité environnementale et en même temps moins impactants pour l'environnement.
Toutes les sources envisageables seront potentiellement mises à contribution en plus des données de l'AMF, par exemple, les informations publiques (agences de presses spécialisées ou non, réseaux sociaux, WEB...) et les sources d'informations professionnelles, comme par exemple les « carnets d'ordres » (transactions boursières spécifiées en langue naturelle).
L'identification s'accompagnera de l'appréciation qualitative du risque considéré, d'un classement par ordre d'importance des documents justifiant l'alerte ainsi que de la génération d'une synthèse des informations extraites justifiant l'alerte, sous des formes qui restent à définir (résumés automatiques, des bases de faits, des graphes...) en fonction des spécificités des systèmes (hypercubes de données, interface de veille) alimentés par l'algorithme de détection.
La conception de l'algorithme prendra en compte le fait qu'il devra être opérationnel dans un contexte « Big Data » nécessitant un maintien des performances d'utilisabilité lors du passage à l'échelle.
La fonction de l'algorithme objet de la thèse étant de fournir aux experts qui établissent le diagnostic du risque et/ou infraction aux réglements, une pré-analyse (détection de signaux faibles [Sidhom11]) avec tous les éléments d'information justificatifs et complémentaires.
L'accessibilité numérique joue un rôle décisif pour l'éducation, l'inclusion sociale et l'autonomie des individus souffrant d'une déficience.
Dans ces travaux, nous nous sommes intéressés à une composante universelle des documents numériques : la mise en forme des textes.
L'utilisation de couleurs, polices et dispositions de texte peut paraître anodin, mais il se trouve qu'au-delà de l'esthétique du texte, la mise en forme a non seulement du sens, mais elle permet aux lecteurs d'optimiser leur activité de lecture.
Par exemple des couleurs et une police particulière peuvent suffire à nous indiquer un titre, qui va permettre au lecteur de se représenter globalement le contenu.
Ces travaux visaient donc à rendre accessible la signification de la mise en forme aux déficients visuels, afin qu'ils puissent accéder aux mêmes informations que les lecteurs voyants mais aussi bénéficier des mêmes optimisations quand ils accèdent aux documents à l'aide de voix de synthèse.
Dans le contexte actuel, l'Intelligence Artificielle (IA) est largement répandue et s'applique à de nombreux domaines tels que les transports, la médecine et les véhicules autonomes.
Parmi les algorithmes d'IA, on retrouve principalement les réseaux de neurones, qui peuvent être répartis en deux familles : d'une part, les Réseaux de Neurones Impulsionnels (SNNs) qui sont issus du domaine des neurosciences ; d'autre part, les Réseaux de Neurones Analogiques (ANNs) qui sont issus du domaine de l'apprentissage machine.
Les ANNs connaissent un succès inédit grâce à des résultats inégalés dans de nombreux secteurs tels que la classification d'images et la reconnaissance d'objets.
Cependant, leur déploiement nécessite des capacités de calcul considérables et ne conviennent pas à des systèmes très contraints.
Afin de pallier ces limites, de nombreux chercheurs s'intéressent à un calcul bio-inspiré, qui serait la parfaite alternative aux calculateurs conventionnels basés sur l'architecture de Von Neumann.
Ce paradigme répond aux exigences de performance de calcul, mais pas aux exigences d'efficacité énergétique.
Il faut donc concevoir des circuits matériels neuromorphiques adaptés aux calculs parallèles et distribués.
Dans ce contexte, nous avons établi un certain nombre de critères en termes de précision et de coût matériel pour différencier les SNNs et ANNs.
Dans le cas de topologies simples, nous avons montré que les SNNs sont plus efficaces en termes de coût matériel que les ANNs, et ce, avec des précisions de prédiction quasiment similaires.
Ainsi, dans ce travail, notre objectif est de concevoir une architecture neuromorphique basée sur les SNNs.
Dans un contexte d'efficacité énergétique, nous avons réalisé une étude approfondie sur divers paradigmes de codage neuronal utilisés avec les SNNs.
Par ailleurs, nous avons proposé de nouvelles versions dérivées du codage fréquentiel, visant à se rapprocher de l'activité produite avec le codage temporel, qui se caractérise par un nombre réduit d'impulsions (spikes) se propageant dans le SNN.
En faisant cela, nous sommes en mesure de réduire le nombre de spikes, ce qui se traduit par un SNN avec moins d'événements à traiter, et ainsi, réduire la consommation énergétique sous-jacente.
Pour cela, deux techniques nouvelles ont été proposées : "First Spike", qui se caractérise par l'utilisation d'un seul spike au maximum par donnée ; "Spike Select", qui permet de réguler et de minimiser l'activité globale du SNN.Dans la partie d'exploration RTL, nous avons comparé de manière quantitative un certain nombre d'architectures de SNN avec différents niveaux de parallélisme et multiplexage de calculs.
En effet, le codage "Spike Select" engendre une régulation de la distribution des spikes, avec la majorité générée dans la première couche et peu d'entre eux propagés dans les couches profondes.
Nous avons constaté que cette distribution bénéficie d'une architecture hybride comportant une première couche parallèle et les autres multiplexées.
Par conséquent, la combinaison du "Spike Select" et de l'architecture hybride serait une solution efficace, avec un compromis efficace entre coût matériel, consommation et latence.
Enfin, en se basant sur les choix architecturaux et neuronaux issus de l'exploration précédente, nous avons élaboré une architecture évènementielle dédiée aux SNNs mais suffisamment programmable pour supporter différents types et tailles de réseaux de neurones.
L'architecture supporte les couches les plus utilisées : convolution, pooling et entièrement connectées.
En utilisant cette architecture, nous serons bientôt en mesure de comparer les ANNs et les SNNs sur des applications réalistes et enfin conclure sur l'utilisation des SNNs pour l'IA embarquée.
Le présent travail étudie une situation de communication nouvelle : la communication via l'e-mail.
Notre étude s'est centrée plus précisément sur les mails envoyés par des clients vers l'entreprise et ce dans le domaine du tourisme aérien (notion de e-crm).
Pour mener nos analyses linguistiques, nous avons constitué un important corpus de messages récoltés sur des forums Internet et traitant de voyages.
Notre but est d'automatiser la gestion, la catégorisation et la thématisation des mails.
Nous avons donc rassemblé un ensemble de traits lexicaux, syntaxiques, morpho-syntaxiques et sémantiques spécifiques à la notion de déplacement, de toponymie et propres au sous-langage du tourisme aérien.
Nous montrons également comment une analyse linguistique des informations spatiales et indissociable d'un traitement des éléments temporels de la phrase.
En outre, nous choisissons d'analyser les informations de type émotionnel contenues dans les messages.
Dans la dernière partie de notre travail, nous replaçons notre étude dans un système de veille appliqué aux mails.
Nous montrons comment les techniques à base de statistiques sont limitées dès qu'il s'agit de traiter des énoncés linguistiquement complexes tels que les nôtres.
Notre approche est hybride : à base de mots clés, dictionnaires de synonymes, scripts sur le modèle de SCHANK et ABELSON, mais surtout à base de modélisation des connaissances.
Finalement, nous proposons un traitement de haute qualité des connaissances et donnons quelques exemples d'informatisation de notre système grâce à XML, PROLOG et PERL
La vidéosurveillance est d'une grande valeur pour la sécurité publique.
En tant que l'un des plus importantes applications de vidéosurveillance, la ré-identification de personnes est définie comme le problème de l'identification d'individus dans des images captées par différentes caméras de surveillance à champs non-recouvrants.
Dans la première approche, nous utilisons les attributs des piétons tels que genre, accessoires et vêtements.
Nous fusionnons ensuite ces deux branches pour la ré-identification.
Deuxièmement, nous proposons un CNN prenant en compte différentes orientations du corps humain.
Comme troisième contribution de cette thèse, nous proposons une nouvelle fonction de coût basée sur une liste d'exemples.
Elle introduit une pondération basée sur le désordre du classement et permet d'optimiser directement les mesures d'évaluation.
Enfin, pour un groupe de personnes, nous proposons d'extraire une représentation de caractéristiques visuelles invariante à la position d'un individu dans une image de group.
Cette prise en compte de contexte de groupe réduit ainsi l'ambigüité de ré-identification.
Pour chacune de ces quatre contributions, nous avons effectué de nombreuses expériences sur les différentes bases de données publiques pour montrer l'efficacité des approches proposées.
Il est important d'évaluer régulièrement les produits de l'innovation technologique afin d'estimer le niveau de maturité atteint par les technologies et d'étudier les cadres applicatifs dans lesquels elles pourront être exploitées.
Pendant longtemps, les différentes briques technologiques issues du TAL étaient développées séparément.
Le nouveau défi en terme d'évaluation est alors de pouvoir évaluer les différents modules (ou briques) tout en prenant en compte le contexte applicatif.
Nous y décrivons les tâche de RAP et de REN proposées dans les campagnes d'évaluation ainsi que les protocoles mis en place pour leurs évaluation.
Nous y discutons également les limites des approches d'évaluations modulaires et nous y exposons les mesures alternatives proposées dans la littératures.
En deuxième partie, nous décrivons la tâche de détection, classification et décomposition d'entités nommées étudiée et nous proposons une nouvelle métriques ETER (Entity Tree Error Rate) permettant de prendre en compte les spécificité de cette tâche et le contexte applicatif lors de l'évaluation.
ETER permet également de supprimer les biais observés avec les métriques existantes.
ATENE consiste à comparer les probabilités de présence d'entités sur les transcriptions de référence et d'hypothèse plutôt qu'une comparaison directe des graphèmes.
Elle est composée de deux mesures élémentaires.
Cette thèse présente une méthode générique de reconnaissance automatique des émotions à partir d'un système bimodal basé sur les expressions faciales et les signaux physiologiques.
Cette approche de traitement des données conduit à une extraction d'information de meilleure qualité et plus fiable que celle obtenue à partir d'une seule modalité.
L'algorithme de reconnaissance des expressions faciales qui est proposé, s'appuie sur la variation de distances des muscles faciaux par rapport à l'état neutre et sur une classification par les séparateurs à vastes marges (SVM).
La reconnaissance des émotions à partir des signaux physiologiques est, quant à elle, basée sur la classification des paramètres statistiques par le même classifieur.
Afin d'avoir un système de reconnaissance plus fiable, nous avons combiné les expressions faciales et les signaux physiologiques.
La combinaison directe de telles informations n'est pas triviale étant donné les différences de caractéristiques (fréquence, amplitude de variation, dimensionnalité).
Pour y remédier, nous avons fusionné les informations selon différents niveaux d'application.
Au niveau de la fusion des caractéristiques, nous avons testé l'approche par l'information mutuelle pour la sélection des plus pertinentes et l'analyse en composantes principales pour la réduction de leur dimensionnalité.
Au niveau de la fusion de décisions, nous avons implémenté une méthode basée sur le processus de vote et une autre basée sur les réseaux Bayésien dynamiques.
Les meilleurs résultats ont été obtenus avec la fusion des caractéristiques en se basant sur l'Analyse en Composantes Principales.
Ces méthodes ont été testées sur une base de données conçue dans notre laboratoire à partir de sujets sains et de l'inducteur par images IAPS.
Une étape d'auto évaluation a été demandée à tous les sujets dans le but d'améliorer l'annotation des images d'induction utilisées.
Les résultats ainsi obtenus mettent en lumière leurs bonnes performances et notamment la variabilité entre les individus et la variabilité de l'état émotionnel durant plusieurs jours
Une définition très générale de la structure musicale consiste à considérer tout ce qui distingue la musique d'un bruit aléatoire comme faisant partie de sa structure.
Dans cette thèse, nous nous intéressons à l'aspect macroscopique de cette structure, en particulier la décomposition de passages musicaux en unités autonomes (typiquement, des sections) et à leur caractérisation en termes de groupements d'entités élémentaires conjointement compressibles.
Un postulat de ce travail est d'établir un lien entre l'inférence de structure musicale et les concepts de complexité et d'entropie issus de la théorie de l'information.
Nous travaillons ainsi à partir de l'hypothèse que les segments structurels peuvent être inférés par des schémas de compression de données.
Dans une première partie, nous considérons les grammaires à dérivation unique (GDU), conçues à l'origine pour la découverte de structures répétitives dans les séquences biologiques (Gallé, 2011), dont nous explorons l'utilisation pour modéliser les séquences musicales.
Cette approche permet de compresser les séquences en s'appuyant sur leurs statistiques d'apparition, leur organisation hiérarchique étant modélisée sous forme arborescente.
Nous développons plusieurs adaptations de cette méthode pour modéliser des répétitions inexactes et nous présentons l'étude de plusieurs critères visant à régulariser les solutions obtenues.
La seconde partie de cette thèse développe et explore une approche novatrice d'inférence de structure musicale basée sur l'optimisation d'un critère de compression tensorielle.
Celui-ci vise à compresser l'information musicale sur plusieurs échelles simultanément en exploitant les relations de similarité, les progressions logiques et les systèmes d'analogie présents dans les segments musicaux.
La méthode proposée est introduite d'un point de vue formel, puis présentée comme un schéma de compression s'appuyant sur une extension multi-échelle du modèle Système &amp; Contraste (Bimbot et al., 2012) à des patrons tensoriels hypercubiques.
Nous généralisons de surcroît l'approche à d'autres patrons tensoriels, irréguliers, afin de rendre compte de la grande variété d'organisations structurelles des segments musicaux.
Les méthodes étudiées dans cette thèse sont expérimentées sur une tâche de segmentation structurelle de données symboliques correspondant à des séquences d'accords issues de morceaux de musique pop (RWC-Pop).
Les méthodes sont évaluées et comparées sur plusieurs types de séquences d'accords, et les résultats établissent l'attractivité des approches par critère de complexité pour l'analyse de structure et la recherche d'informations musicales, les meilleures variantes fournissant des performances de l'ordre de 70% de F-mesure.
Extraire de l'information de données langagières est un sujet de plus en plus d'actualité compte tenu de la quantité toujours croissante d'information qui doit être régulièrement traitée et analysée, et nous assistons depuis les années 90 à l'essor des recherches sur des données de parole également.
La parole pose des problèmes supplémentaires par rapport à l'écrit, notamment du fait de la présence de phénomènes propres à l'oral (hésitations, reprises, corrections) mais aussi parce que les données orales sont traitées par un système de reconnaissance automatique de la parole qui génère potentiellement des erreurs.
Ainsi, extraire de l'information de données audio implique d'extraire de l'information tout en tenant compte du « bruit » intrinsèque à l'oral ou généré par le système de reconnaissance de la parole.
Il ne peut donc s'agir d'une simple application de méthodes qui ont fait leurs preuves sur de l'écrit.
L'utilisation de techniques adaptées au traitement des données issues de l'oral et prenant en compte à la fois leurs spécificités liées au signal de parole et à la transcription –manuelle comme automatique – de ce dernier représente un thème de recherche en plein développement et qui soulève de nouveaux défis scientifiques.
Ces défis sont liés à la gestion de la variabilité dans la parole et des modes d'expressions spontanés.
Par ailleurs, l'analyse robuste de conversations téléphoniques a également fait l'objet d'un certain nombre de travaux dans la continuité desquels s'inscrivent ces travaux de thèse.
Cette thèse porte plus spécifiquement sur l'analyse des disfluences et de leur réalisation dans des données conversationnelles issues des centres d'appels EDF, à partir du signal de parole et des transcriptions manuelle et automatique de ce dernier.
Ce travail convoque différents domaines, de l'analyse robuste de données issues de la parole à l'analyse et la gestion des aspects liés à l'expression orale.
L'objectif de la thèse est de proposer des méthodes adaptées à ces données, qui permettent d'améliorer les analyses de fouille de texte réalisées sur les transcriptions (traitement des disfluences).
Pour répondre à ces problématiques, nous avons analysé finement le comportement de phénomènes caractéristiques de l'oral spontané (disfluences) dans des données orales conversationnelles issues de centres d'appels EDF, et nous avons mis au point une méthode automatique pour leur détection, en utilisant des indices linguistiques, acoustico-prosodiques,discursifs et para-linguistiques.
Les apports de cette thèse s'articulent donc selon trois axes de recherche.
Premièrement, nous proposons une caractérisation des conversations en centres d'appels du point de vue de l'orals pontané et des phénomènes qui le caractérisent.
Deuxièmement, nous avons mis au point (i) une chaîne d'enrichissement et de traitement des données orales effective sur plusieurs plans d'analyse (linguistique, prosodique, discursif, para-linguistique) ; (ii) un système de détection automatique des disfluences d'édition adapté aux données orales conversationnelles, utilisant le signal et les transcriptions (manuelles ou automatiques).
Troisièmement, d'un point de vue « ressource » , nous avons produit un corpus de transcriptions automatiques de conversations issues de centres d'appels annoté en disfluences d'édition (méthode semi-automatique).
Système de recommandation personnalisé pour les visites touristiques
L'imagerie de résonance magnétique de diffusion (dMRI) est une technique très sensible pour la tractographie des fibres de substance blanche et la caractérisation de l'intégrité et de la connectivité axonale.
A travers la mesure des mouvements des molécules d'eau dans les trois dimensions de l'espace, il est possible de reconstruire des cartes paramétriques reflétant l'organisation tissulaire.
Parmi ces cartes, la fraction d'anisotropie (FA) et les diffusivités axiale (λa), radiale (λr) et moyenne (MD) ont été largement utilisés pour caractériser les pathologies du système nerveux central.
L'emploi de ces cartes paramétriques a permis de mettre en évidence la survenue d'altérations micro structurelles de la substance blanche (SB) et de la substance grise (SG) chez les patients atteints d'une sclérose en plaques (SEP).
Cependant, il reste à déterminer l'origine de ces altérations qui peuvent résulter de processus globaux comme la cascade inflammatoire et les mécanismes neurodégénératifs ou de processus plus localisés comme la démyélinisation et l'inflammation.
De plus, ces processus pathologiques peuvent survenir le long de faisceaux de SB afférents ou efférents, conduisant à une dégénérescence antero- ou rétrograde.
Ainsi, pour une meilleure compréhension des processus pathologiques et de leur progression dans l'espace et dans le temps, une caractérisation fine et précise des faisceaux de SB est nécessaire.
En couplant l'information spatiale de la tractographie des fibres aux cartes paramétriques de diffusion, obtenues grâce à un protocole d'acquisitions longitudinal, les profils des faisceaux de SB peuvent être modélisés et analysés.
Une telle analyse des faisceaux de SB peut être effectuée grâce à différentes méthodes, partiellement ou totalement non-supervisées.
Dans la première partie de ce travail, nous dressons l'état de l'art des études déjà présentes dans la littérature. Cet état de l'art se focalisera sur les études montrant les effets de la SEP sur les faisceaux de SB grâce à l'emploi de l'imagerie de tenseur de diffusion.
Dans la seconde partie de ce travail, nous introduisons deux nouvelles méthodes,“string-based”, l'une semi-supervisée et l'autre non-supervisée, pour extraire les faisceaux de SB.
Nous montrons comment ces algorithmes permettent d'améliorer l'extraction de faisceaux spécifiques comparé aux approches déjà présentes dans la littérature.
De plus, dans un second chapitre, nous montrons une extension de la méthode proposée par le couplage du formalisme “string-based” aux informations spatiales des faisceaux de SB.
Dans la troisième et dernière partie de ce travail, nous décrivons trois algorithmes automatiques permettant l'analyse des changements longitudinaux le long des faisceaux de SB chez des patients atteints d'une SEP.
Ces méthodes sont basées respectivement sur un modèle de mélange Gaussien, la factorisation de matrices non-négatives et la factorisation de tenseurs non-négatifs.
De plus, pour valider nos méthodes, nous introduisons un nouveau modèle pour simuler des changements longitudinaux réels, base sur une fonction de probabilité Gaussienne généralisée.
Des hautes performances ont été obtenues avec ces algorithmes dans la détection de changements longitudinaux d'amplitude faible le long des faisceaux de SB chez des patients atteints de SEP.
En conclusion, nous avons proposé dans ce travail des nouveaux algorithmes non supervisés pour une analyse précise des faisceaux de SB, permettant une meilleure caractérisation des altérations pathologiques survenant chez les patients atteints de SEP
Les événements distribués, se déroulant sur plusieurs jours et/ou sur plusieurs lieux, tels que les conventions, festivals ou croisières, sont de plus en plus populaires ces dernières années et attirant des milliers de participants.
Les programmes de ces événements sont généralement très denses, avec un grand nombre d'activités se déroulant en parallèle.
Les systèmes de recommandation peuvent constituer une solution privilégiée dans ce genre d'environnement.
De nombreux travaux en recommandation se sont concentrés sur la recommandation personnalisée d'objets spatiaux (points d'intérêts immuables dans le temps ou événements éphémères) indépendants les uns des autres.
Ensuite, nous proposons ANASTASIA, une approche de recommandation personnalisée de séquences d'activités lors des événements distribués.
Notre approche est basée sur trois composants clés : (1) l'estimation de l'intérêt d'un utilisateur pour une activité, prenant en compte différentes influences, (2) l'intégration de motifs comportementaux d'utilisateurs basés sur leurs historiques d'activités et (3) la construction d'un planning ou séquence d'activités prenant en compte les contraintes spatio-temporelles de l'utilisateur et des activités.
Nous explorons ainsi des méthodes issus de l'apprentissage de séquences et de l'optimisation discrète pour résoudre le problème.
Enfin, nous démontrons le manque de jeu de données librement accessibles pour l'évaluation des algorithmes de recommandation d'événements et de séquences d'événements.
Nous pallions à ce problème en proposant deux jeux de données, librement accessibles, que nous avons construits au cours de la thèse : Fantasy_db et DEvIR.
Les interactions sociales se trouvent au cœur des activités économiques.
Pourtant en sciences économiques, elles ne sont traitées que d'une manière limitée en se concentrant uniquement aux rapports de qu'elles entretiennent avec le marché (Mankiw and Reis, 2002).
Le rôle que jouent les interactions sociales vis-à-vis des comportements des agents, ainsi que la formation de leurs attentes sont souvent négligé.
Cette négligence reste d'actualité malgré que les premières contributions dans la littérature économique les ont depuis longtemps déjà identifiées comme étant de déterminants importants pour la prise des décisions des agents économiques, comme par exemple Sherif (l936), Hyman (1942), Asch (1951), Jahoda (1959) ou Merlon (1968).
En revanche, dans les études de consommation (une spécialité au croisement entre les sciences économiques, de la sociologie et de la psychologie), les interactions sociales (influences sociales) sont con­sidérées comme les "... déterminants dominants[...] du comportement de l'individu... " (Burnkrant and Cousineau, 1975).
Le but de cette thèse est de construire un pont entre les interactions sociales et leur influence sur la formation des anticipations et le comportement des agents.
Cette thèse s'inscrit dans les domaines des systèmes Question Réponse en domaine restreint, la recherche d'information ainsi que TALN.
Les systèmes de Question Réponse (QR) ont pour objectif de retrouver un fragment pertinent d'un document qui pourrait être considéré comme la meilleure réponse concise possible à une question de l'utilisateur.
Le but de cette thèse est de proposer une approche de localisation de réponses dans des masses de données complexes et évolutives décrites ci-dessous.
De nos jours, dans de nombreux domaines d'application, les systèmes informatiques sont instrumentés pour produire des rapports d'événements survenant, dans un format de données textuelles généralement appelé fichiers log.
Les fichiers logs représentent la source principale d'informations sur l'état des systèmes, des produits, ou encore les causes de problèmes qui peuvent survenir.
Les fichiers logs peuvent également inclure des données sur les paramètres critiques, les sorties de capteurs, ou une combinaison de ceux-ci.
Ces fichiers sont également utilisés lors des différentes étapes du développement de logiciels, principalement dans l'objectif de débogage et le profilage.
Bien que le processus de génération de fichiers logs est assez simple et direct, l'analyse de fichiers logs pourrait être une tâche difficile qui exige d'énormes ressources de calcul, de temps et de procédures sophistiquées [Valdman, 2004].
En effet, il existe de nombreux types de fichiers logs générés dans certains domaines d'application qui ne sont pas systématiquement exploités d'une manière efficace en raison de leurs caractéristiques particulières.
Dans cette thèse, nous nous concentrerons sur un type des fichiers logs générés par des systèmes EDA (Electronic Design Automation).
Ces fichiers logs contiennent des informations sur la configuration et la conception des Circuits Intégrés (CI) ainsi que les tests de vérification effectués sur eux.
Ces informations, très peu exploitées actuellement, sont particulièrement attractives et intéressantes pour la gestion de conception, la surveillance et surtout la vérification de la qualité de conception.
Cependant, la complexité de ces données textuelles complexes, c.-à-d. des fichiers logs générés par des outils de conception de CI, rend difficile l'exploitation de ces connaissances.
Plusieurs aspects de ces fichiers logs ont été moins soulignés dans les méthodes de TALN et Extraction d'Information (EI).
Le problème est accentué lorsque nous devons également prendre leurs structures évolutives et leur vocabulaire spécifique en compte.
Dans ce contexte, un défi clé est de fournir des approches qui prennent les spécificités des fichiers logs en compte tout en considérant les enjeux qui sont spécifiques aux systèmes QR dans des domaines restreints.
Ainsi, les contributions de cette thèse consistent brièvement en :
Au sein de cette approche, nous proposons un type original de descripteur qui permet de modéliser la structure textuelle et le layout des documents textuels.
> Proposer une approche de la localisation de réponse (recherche de passages) dans les fichiers logs.
Afin d'améliorer la performance de recherche de passage ainsi que surmonter certains problématiques dûs aux caractéristiques des fichiers logs, nous proposons une approches d'enrichissement de requêtes.
Cela dit, nous proposons également une nouvelle fonction originale de pondération (scoring), appelée TRQ (Term Relatedness to Query) qui a pour objectif de donner un poids élevé aux termes qui ont une probabilité importante de faire partie des passages pertinents.
Cette approche est également adaptée et évaluée dans les domaines généraux.
> Etudier l'utilisation des connaissances morpho-syntaxiques au sein de nos approches.
A cette fin, nous nous sommes intéressés à l'extraction de la terminologie dans les fichiers logs.
Ainsi, nous proposons la méthode Exterlog, adaptée aux spécificités des logs, qui permet d'extraire des termes selon des patrons syntaxiques.
Afin d'évaluer les termes extraits et en choisir les plus pertinents, nous proposons un protocole de validation automatique des termes qui utilise une mesure fondée sur le Web associée à des mesures statistiques, tout en prenant en compte le contexte spécialisé des logs.
Cette thèse examine la construction du sens et l'influence translangagière dans la représentation de la signification lexicale et du sens discursif.
Nous effectuons cette étude à travers des discours définitionnels et des significations telles qu'elles sont proposées par des locuteurs amenés à exprimer leurs savoirs métalinguistiques sémantiques dans le cadre d'une recherche expérimentale.
Le concept culturel de travail est analysé en adoptant le cadre théorique de la Sémantique des Possibles Argumentatifs de Galatanu.
Une étude comparative du français et de l'anglais a permis de déterminer les ressemblances, ainsi que les divergences entre les représentations de la signification lexicale et du sens discursif des mots « travail » et « work » proposées par quatre groupes de locuteurs.
Chaque groupe de locuteurs ayant son propre profil langagier, spécifiquement deux groupes de locuteurs monolingues (francophone et anglophone), un groupe constitué de locuteurs de langue maternelle française qui parlent l'anglais, et un groupe de locuteurs de langue maternelle anglaise qui parlent le français, présente l'opportunité de non seulement comparer les variations entre les représentations de la signification lexicale et du sens discursif des mots « travail » et « work » , mais aussi de constater l'influence potentielle d'une compétence dans une deuxième langue sur ces représentations.
Dans un deuxième temps, cette thèse comporte une analyse de la représentation de l'insulte de loser aux États-Unis.
Inspiré par la présence d'un élément liant le concept de work et ladite insulte, le choix de présenter loser rend possible une vision de la mobilisation discursive des valeurs sociales portées par les mots work et loser.
Les bases de connaissances sont des ensembles de faits, souvent sur des sujets encyclopédiques.
Elles sont souvent utilisées pour la reconnaissance d'entités nommées, la recherche structurée, la réponse automatique à des questions, etc.
Ces bases de connaissances doivent être maintenues, ce qui est une tâche cruciale mais coûteuse.
Le sujet de cette thèse est la maintenance automatique de bases de connaissances à l'aide de contraintes.
La première contribution de cette thèse est à propos de la découverte automatique de contraintes.
Elle améliore les approches classiques d'apprentissage de règles en utilisant des méta-informations de complétude des données.
Elle montre que que ces informations permettent d'améliorer de manière significative la qualité des règles trouvées.
La seconde contribution est la création d'une base de connaissance, YAGO 4, qui assure le respect d'une série de contraintes en supprimant les faits qui n'y correspondent pas.
La troisième contribution est une méthode pour corriger automatiquement les violations de contraintes.
Cette méthode utilise l'historique des modifications de la base de connaissance afin de proposer des corrections, ceci à partir de la manière avec laquelle les utilisateurs de la base de connaissance ont déjà corrigé des violations similaires.
L'Intelligence Artificielle (IA) et les Interfaces Homme-Machine (IHM) sont deux champs de recherche avec relativement peu de travaux communs.
Les spécialistes en IHM conçoivent habituellement les interfaces utilisateurs directement à partir d'observations et de mesures sur les interactions humaines, optimisant manuellement l'interface pour qu'elle corresponde au mieux aux attentes des utilisateurs.
Ce processus est difficile à optimiser : l'ergonomie, l'intuitivité et la facilité d'utilisation sont autant de propriétés clé d'une interface utilisateur (IU) trop complexes pour être simplement modélisées à partir de données d'interaction.
Ce constat restreint drastiquement les utilisations potentielles de l'apprentissage automatique dans ce processus de conception.
A l'heure actuelle, l'apprentissage automatique dans les IHMs se cantonne majoritairement à la reconnaissance de gestes et à l'automatisation d'affichage, par exemple à des fins publicitaires ou pour suggérer une sélection.
L'apprentissage automatique peut également être utilisé pour optimiser une interface utilisateur existante, mais il ne participe pour l'instant pas à concevoir de nouvelles façons d'intéragir.
Notre objectif avec cette thèse est de proposer grâce à l'apprentissage automatique de nouvelles stratégies pour améliorer le processus de conception et les propriétés des IUs.
Notre but est de définir de nouvelles IUs intelligentes – comprendre précises, intuitives et adaptatives – requérant un minimum d'interventions manuelles.
Nous proposons une nouvelle approche à la conception d'IU : plutôt que l'utilisateur s'adapte à l'interface, nous cherchons à ce que l'utilisateur et l'interface s'adaptent mutuellement l'un à l'autre.
Le but est d'une part de réduire le biais humain dans la conception de protocoles d'interactions, et d'autre part de construire des interfaces co-adaptatives capables de correspondre d'avantage aux préférences individuelles des utilisateurs.
Pour ce faire, nous allons mettre à contribution les différents outils disponibles en apprentissage automatique afin d'apprendre automatiquement des comportements, des représentations et des prises de décision.
Nous expérimenterons sur les interfaces tactiles pour deux raisons majeures : celles-ci sont largement utilisées et fournissent des problèmes facilement interprétables.
La première partie de notre travail se focalisera sur le traitement des données tactiles et l'utilisation d'apprentissage supervisé pour la construction de classifieurs précis de gestes tactiles.
La seconde partie détaillera comment l'apprentissage par renforcement peut être utilisé pour modéliser et apprendre des protocoles d'interaction en utilisant des gestes utilisateur.
Enfin, nous combinerons ces modèles d'apprentissage par renforcement avec de l'apprentissage non supervisé pour définir une méthode de conception de nouveaux protocoles d'interaction ne nécessitant pas de données d'utilisation réelles.
La prolifération des données numériques a permis aux communautés de scientifiques et de praticiens de créer de nouvelles technologies basées sur les données pour mieux connaître les utilisateurs finaux et en particulier leur comportement. L'objectif est alors de fournir de meilleurs services et un meilleur support aux personnes dans leur expérience numérique.
La majorité de ces technologies créées pour analyser le comportement humain utilisent très souvent des données de logs générées passivement au cours de l'interaction homme-machine.
Une particularité de ces traces comportementales est qu'elles sont enregistrées et stockées selon une structure clairement définie.
En revanche, les traces générées de manière proactive sont très peu structurées et représentent la grande majorité des données numériques existantes.
À ce jour, malgré la prédominance des données textuelles et la pertinence des connaissances comportementales dans de nombreux domaines, les textes numériques sont encore insuffisamment étudiés en tant que traces du comportement humain pour révéler automatiquement des connaissances détaillées sur le comportement.
Plusieurs contributions originales sont faites.
Il y est menée la seule revue systématique existante à ce jour sur la modélisation automatique des conversations asynchrones avec des actes de langage.
Une méthode automatique, indépendante du corpus, pour annoter les énoncées de communication asynchrone avec la taxonomie des intentions de discours proposée, est conçue sur la base d'un apprentissage automatique supervisé.
Pour cela, deux corpus "ground-truth" validés sont créés et trois groupes de caractéristiques (discours, contenu et conversation) sont conçus pour être utilisés par les classificateurs.
En particulier, certaines des caractéristiques du discours sont nouvelles et définies en considérant des moyens linguistiques pour exprimer des intentions de discours,sans s'appuyer sur le contenu explicite du corpus, le domaine ou les spécificités des types de communication asynchrones.
Une méthode automatique basée sur la fouille de processus est conçue pour générer des modèles de processus d'intentions de discours interdépendantes à partir de tours de parole, annotés avec plusieurs labels par phrase.
Comme la fouille de processus repose sur des logs d'événements structurés et bien définis, un algorithme est proposé pour produire de tels logs d'événements à partir de conversations.
Par ailleurs, d'autres solutions pour transformer les conversations annotées avec plusieurs labels par phrase en logs d'événements, ainsi que l'impact des différentes décisions sur les modèles comportementaux en sortie sont analysées afin d'alimenter de futures recherches.
Nous présentons une solution pour la création d'un corpus numérisé en utilisant une approche crowdsourcing pour annoter des bandes dessinées (BD).
Les encodages XML qui en résultent aident également les chercheurs, les éditeurs, les bibliothécaires et les conservateurs de collections BDs.
Pour atteindre notre objectif de recueil de données, nous développons un moteur de crowdsourcing en ligne pour annoter les BDs.
Les tâches sont conçues pour reproduire l'expérience de lecture des pages des BDs, en demandant aux participants d'identifier et d'annoter les éléments structurels (cases, splash-pages) et de contenu (personnages, lieux, événements, onomatopées, objets, lignes de mouvement) des BDs.
Les bibliothécaires et les conservateurs de collections des BDs disposeront d'un contenu structuré qui pourrait permettre la création d'artefacts spécifiques, tels que des dictionnaires de BDs, des indices de recherche ou des dictionnaires d'onomatopée.
Du point de vue de l'édition, les standards actuels pour les BDs numériques se chargent exclusivement de la couche de présentation (c'est-à-dire, rendre tout simplement une publication sur l'écran d'un dispositif numérique).
Mais la nature artistique de la BD et le grand potentiel des BDs numériques nous permettent d'aller au-delà de la simple présentation du contenu.
À cet égard, nous contribuons avec des améliorations aux standards sémantiques (CBML) et de présentation (EPUB).
Les mécanismes de compréhension chez l'être humain sont par essence multimodaux.
Comprendre le monde qui l'entoure revient chez l'être humain à fusionner l'information issue de l'ensemble de ses récepteurs sensoriels.
Le but de cette thèse est de proposer des traitements joints s'appliquant principalement au texte et à l'image pour le traitement de documents multimodaux à travers deux études : l'une portant sur la fusion multimodale pour la reconnaissance du rôle du locuteur dans des émissions télévisuelles, l'autre portant sur la complémentarité des modalités pour une tâche d'analyse linguistique sur des corpus d'images avec légendes.
Pour la première étude nous nous intéressons à l'analyse de documents audiovisuels provenant de chaînes d'information télévisuelle.
Nous proposons une approche utilisant des réseaux de neurones profonds pour la création d'une représentation jointe multimodale pour les représentations et la fusion des modalités.
Dans la seconde partie de cette thèse nous nous intéressons aux approches permettant d'utiliser plusieurs sources d'informations multimodales pour une tâche monomodale de traitement automatique du langage, afin d'étudier leur complémentarité.
Nous proposons un système complet de correction de rattachements prépositionnels utilisant de l'information visuelle, entraîné sur un corpus multimodal d'images avec légendes.
Nos perspectives sont éducatives : créer des exercices de grammaire pour le français.
La paraphrase est une opération de reformulation.
Nos travaux tendent à attester que les modèles séquence vers séquence ne sont pas de simples répétiteurs mais peuvent apprendre la syntaxe.
Nous avons montré, en combinant divers modèles, que la représentation de l'information sous de multiples formes (en utilisant de la donnée formelle (RDF), couplée à du texte pour l'étendre ou le réduire, ou encore seulement du texte) permet d'exploiter un corpus sous différents angles, augmentant la diversité des sorties, exploitant les leviers syntaxiques mis en place.
Nous nous sommes penchée sur un problème récurrent, celui de la qualité des données, et avons obtenu des paraphrases avec une haute adéquation syntaxique (jusqu'à 98% de couverture de la demande) et un très bon niveau linguistique.
Nous obtenons jusqu'à 83.97 points de BLEU*, 78.41 de plus que la moyenne de nos lignes de base, sans levier syntaxique.
Ce taux indique un meilleur contrôle des sorties, pourtant variées et de bonne qualité en l'absence de levier.
Le passage à du texte en français était aussi pour nous un impératif.
Travailler depuis du texte brut, en automatisant les procédures, nous a permis de créer un corpus de plus de 450 000 couples représentations/phrases, grâce auquel nous avons appris à générer des textes massivement corrects (92% sur la validation qualitative).
Anonymiser ce qui n'est pas fonctionnel a participé notablement à la qualité des résultats (68.31 de BLEU, soit +3.96 par rapport à la ligne de base, qui était la génération depuis des données non anonymisées).
La représentation formelle de l'information dans un cadre linguistique particulier à une langue est une tâche ardue.
Cette thèse offre des pistes de méthodes pour automatiser cette opération.
Par ailleurs, nous n'avons pu traiter que des phrases relativement courtes.
L'utilisation de modèles neuronaux plus récents permettrait sans doute d'améliorer les résultats.
Enfin, l'usage de traits adéquats en sortie permettrait des vérifications poussées.
*BLEU (Papineni et al., 2002) : qualité d'un texte sur une échelle de 0 (pire) à 100 (meilleur)
Un nombre incalculable de documents est imprimé, numérisé, faxé, photographié chaque jour.
Ces documents sont hybrides : ils existent sous forme papier et numérique.
De plus les documents numériques peuvent être consultés et modifiés simultanément dans de nombreux endroits.
Avec la disponibilité des logiciels d'édition d'image, il est devenu très facile de modifier ou de falsifier un document.
Cela crée un besoin croissant pour un système d'authentification capable de traiter ces documents hybrides.
D'autres solutions reposent sur une vérification visuelle et offrent seulement
Afin de surmonter tous ces problèmes, nous proposons de créer un algorithme de hachage sémantique pour les images de documents.
Cet algorithme de hachage devrait fournir une signature compacte pour toutes les informations visuellement significatives contenues dans le document.
Ce condensé permettra la création de systèmes de sécurité hybrides pour sécuriser tout le document.
Ceci peut être réalisé grâce à des algorithmes d'analyse du document.
Après avoir défini le contexte de l'étude et ce qu'est un algorithme stable, nous nous sommes attachés à produire des algorithmes stables pour la description de la mise en page, la segmentation d'un document, la reconnaissance de caractères et la description des zones graphiques.
L'objectif de la thèse sera de développer des modèles liant modèles de Markov et réseaux de neurones et d'en étudier des applications, notamment pour les systèmes de Questions-Réponses, pour ensuite les mettre en place sur des agents conversationnels.
Les algorithmes les plus utilisés pour les systèmes de Questions-Réponses aujourd'hui reposent sur des modèles de Réseaux de Neurones.
Ceux-ci peuvent s'interpréter comme des modèles probabilistes cachés, dont le même problème peut être abordés par des modèles de Markov cachés.
Ceux-ci ont, depuis une quinzaine d'année, été étendu en modèles de Markov « couples » et « triplets » , permettant d'obtenir des résultats parfois spectaculaires.
Le doctorant s'inspirera alors des démarches amenant à la création des modèles de Markov couples et triplets à partir du modèle de Markov cachés pour proposer des modèles de réseaux de neurones « couples » et « triplets » , et des modèles hybrides entre réseau de neurones et modèles de Markov.
Il en recherchera des applications, notamment pour les systèmes de Questions-Réponse, et observera les apports de ces nouveaux modèles sur les réseaux de neurones classiques.
La création d'un centre d'appel d'urgence pour les sourds et malentendants a nécessité la conception et la mise en place d'un dispositif sociotechnique ad hoc respectant la diversité de leurs pratiques de communication.
Ancrée dans ce projet technique mené afin de rendre accessible une institution organisée autour de l'interaction téléphonique, cette recherche fut l'occasion de conduire une réflexion croisée sur les dispositifs techniques de communication appréhendés comme outil de travail et outil d'accessibilité.
À partir de matériaux issus d'une enquête ethnographique réalisée dans les centres d'appel d'urgence que nous faisons dialoguer avec des traces de l'activité de conception, nous interrogeons, dans un même élan, le rôle que tiennent ces dispositifs dans l'écologie des situations ainsi que la manière dont ils définissent en actes la dynamique contemporaine de prise en compte environnementale du handicap.
Au travers de ce questionnement deux grandes thématiques sont explorées : le travail en situation de communication médiatisée et le processus reformulation des pratiques engagé par la conception et la mise en service d'un dispositif technique d'accessibilité.
Détaillant finement les multiples cours d'action en nous attachant notamment à rematérialiser les pratiques interactionnelles, nous démontrons en quoi les dispositifs techniques de communication participent d'une armature invisible qui fait système et contribue à organiser et pérenniser les pratiques ; armature que nous appréhendons à travers la notion d'infrastructure communicationnelle.
Ainsi, nous proposons cette dernière notion comme outil conceptuel pour appréhender l'introduction de nouveaux dispositifs dans les organisations et pour accompagner une réflexion sur l'accessibilité-en-pratiques.
Les systèmes de traitement automatique des langues reposent souvent sur l'idée que le langage est compositionnel, c'est-à-dire que le sens d'une entité linguistique peut être déduite à partir du sens de ses parties.
Cette supposition ne s'avère pas vraie dans le cas des expressions polylexicales (EPLs).
Par exemple, une "poule mouillée" n'est ni une poule, ni nécessairement mouillée.
Les techniques pour déduire le sens des mots en fonction de leur distribution dans le texte ont obtenu de bons résultats sur plusieurs tâches, en particulier depuis l'apparition des word embeddings.
Cependant, la représentation des EPLs reste toujours un problème non résolu.
En particulier, on ne sait pas comment prédire avec précision, à partir des corpus, si une EPL donnée doit être traitée comme une unité indivisible (p.ex. "carton plein") ou comme une combinaison du sens de ses parties (p.ex. "eau potable").
Cette thèse propose un cadre méthodologique pour la prédiction de compositionnalité d'EPLs fondé sur des représentations de la sémantique distributionnelle, que nous instancions à partir d'une variété de paramètres.
Nous présenterons une évaluation complète de l'impact de ces paramètres sur trois nouveaux ensembles de données modélisant la compositionnalité d'EPLs, en anglais, français et portugais.
Finalement, nous présenterons une évaluation extrinsèque des niveaux de compositionnalité prédits par le modèle dans le contexte d'un système d'identification d'EPLs.
Les résultats suggèrent que le choix spécifique de modèle distributionnel et de paramètres de corpus peut produire des prédictions de compositionnalité qui sont comparables à celles présentées dans l'état de l'art.
En préparation à l'utilisation d'infrastructures distribuées pour accélérer le calcul, cette étude vise à explorer la possibilité d'exécuter l'algorithme de Frank-Wolfe dans un réseau en étoile avec le modèle BSP (Bulk Synchronous Parallel) et à étudier son efficacité théorique et empirique.
Concernant l'aspect théorique, cette étude revisite le taux de convergence déterministe de Frank-Wolfe et l'étend à des cas non déterministes.
En particulier, il montre qu'avec le sous-problème linéaire résolu de manière appropriée, Frank-Wolfe peut atteindre un taux de convergence sous-linéaire à la fois en espérance et avec une probabilité élevée.
Cette contribution pose la fondation théorique de l'utilisation de la méthode de la puissance itérée ou de l'algorithme de Lanczos pour résoudre le sous-problème linéaire de Frank-Wolfe associé à la minimisation de la norme de trace.
Concernant l'aspect algorithmique, dans le cadre de BSP, cette étude propose et analyse quatre stratégies pour le sous-problème linéaire ainsi que des méthodes pour la recherche linéaire.
En outre, remarquant la propriété de mise à jour de rang-1 de Frank-Wolfe, il met à jour le gradient de manière récursive, avec une représentation dense ou de rang faible, au lieu de le recalculer de manière répétée à partir de zéro.
Toutes ces conceptions sont génériques et s'appliquent à toutes les infrastructures distribuées compatibles avec le modèle BSP.
Concernant l'aspect empirique, cette étude teste les conceptions algorithmiques proposées dans un cluster Apache SPARK.
Selon les résultats des expériences, pour le sous-problème linéaire, la centralisation des gradients ou la moyenne des vecteurs singuliers est suffisante dans le cas de faible dimension, alors que la méthode de la puissance itérée distribuée, avec aussi peu qu'une ou deux itérations par époque, excelle dans le cas de grande dimension.
La librairie Python développée pour les expériences est modulaire, extensible et prête à être déployée dans un contexte industriel.
Cette étude a rempli sa fonction de preuve de concept.
Suivant le chemin qu'il met en place, des solveurs peuvent être implémentés pour différentes infrastructures, parmi lesquelles des clusters GPU, pour résoudre des problèmes pratiques dans des contextes spécifiques.
En outre, ses excellentes performances dans le jeu de données ImageNet le rendent prometteur pour l'apprentissage en profondeur.
Un mapping d'ontologies est un ensemble de correspondances.
Chaque correspondance relie des artefacts, typiquement concepts et propriétés, d'une ontologie avec ceux d'une autre ontologie.
Le mapping entre ontologies a suscité beaucoup d'intérêt durant ces dernières années.
En effet, le mapping d'ontologies est largement utilisé pour mettre en oeuvre de l'interopérabilité et intégration (transformation de données, réponse à la requête, composition de web service) dans les applications, et également dans la création de nouvelles ontologies.
D'une part, vérifier l'exactitude (logique) d'un mapping est devenu un prérequis fondamentale à son utilisation.
D'autre part, pour deux ontologies données, plusieurs mappings peuvent être établis, obtenus par différentes méthodes d'alignement, ou définis manuellement.
L'utilisation de plusieurs mappings entre deux ontologies dans une seule application ou pour synthétiser un seul mapping tirant profit de ces plusieurs mappings, peut générer des erreurs dans l'application ou dans le mapping synthétisé car ces plusieurs mappings peuvent être contradictoires.
Dans les deux situations décrites ci-dessus, l'exactitude, la non-contradiction et autres propriétés sont généralement exprimées de façon formelle et vérifiées dans le contexte des ontologies formelles (par exemple, lorsque les ontologies sont représentées en logique)
La vérification de ces propriétés est généralement effectuée à l'aide d'un seul formalisme, exigeant d'une part que les ontologies soient représentées par ce seul formalisme et, d'autre part, qu'une représentation formelle des mappings soit fournie, complétée par des notions formalisant les propriétés recherchées.
Cependant, il existe une multitude de formalismes hétérogènes pour exprimer les ontologies, allant des plus informels (par exemple, du texte contrôlé, des modèles en UML) aux formels (par exemple, des logiques de description ou des catégories).
Ceci implique que pour appliquer les approches existantes, les ontologies hétérogènes doivent être traduites (ou juste transformées, si l'ontologie source est exprimée de façon informelle ou si la traduction complète pour maintenir l'équivalence n'est pas possible) dans un seul formalisme commun et les mappings sont reformulés à chaque fois : seulement à l'issu de ce processus, les propriétés recherchées peuvent être établies.
Même si cela est possible, ce processus peut produire à la fois des mappings corrects et incorrects vis-à-vis de ces propriétés, en fonction de la traduction (transformation) opérée.
En effet, les propriétés recherchées dépendent du formalisme employé pour exprimer les ontologies et les mappings.
Dans ce contexte, les ontologies sont représentées comme treillis, et les mappings sont reformulés comme fonctions entre ces treillis.
Les treillis sont des structures naturelles pour la représentation directe d'ontologies sans obligation de traduire ou transformer les formalismes dans lesquels les ontologies sont exprimées à l'origine.
Cette reformulation unifiée a permis d'introduire une nouvelle notion de mappings compatibles et incompatibles.
Il est ensuite formellement démontré que cette nouvelle notion couvre plusieurs parmi les propriétés recherchées de mappings, mentionnées dans l'état de l'art.
L'utilisation directe de mappings compatibles et incompatibles est démontrée par l'application à des mappings d'ontologies de haut niveau.
La notion de mappings compatibles et incompatibles est aussi appliquée sur des ontologies de domaine, mettant en évidence comment les mappings incompatibles génèrent des résultats incorrects pour la fusion d'ontologies.
Plusieurs découvertes récentes, provenant notamment des neurosciences cognitives, ont eu des retombées dans les sciences humaines et plus particulièrement dans le champ de la linguistique.
Elles ont suscité un renouveau de l'intérêt pour la thématique de l'iconicité phonologique, qui s'intéresse à l'ensemble des phénomènes de similarité entre signifiant et signifié à l'intérieur d'une langue.
Une multitude d'études a alors vu le jour, attestant l'existence de phénomènes phonosymboliques dans les langues du monde.
Malgré cet essor considérable, les contenus de ces travaux, principalement rédigés en anglais, demeurent à ce jour assez méconnu du public francophone, encore relativement ancré dans la tradition structuraliste.
Cette thèse présente tout d'abord une synthèse des travaux internationaux menés dans le champ de l'iconicité phonologique en vue d'en retenir les principaux acquis.
Sur ces bases, elle apporte de nouvelles preuves empiriques de l'existence de phénomènes iconiques dans un corpus de verbes monosyllabiques français à travers deux méthodes.
Les systèmes de traduction automatique (TA), qui génèrent automatiquement la phrase de la langue cible pour chaque entrée de la langue source, ont obtenu plusieurs réalisations convaincantes pendant les dernières décennies et deviennent les aides linguistiques efficaces pour la communauté entière dans un monde globalisé.
Néanmoins, en raison de différents facteurs, sa qualité en général est encore loin de la perfection, constituant le désir des utilisateurs de savoir le niveau de confiance qu'ils peuvent mettre sur une traduction spécifique.
La construction d'une méthode qui est capable d'indiquer des bonnes parties ainsi que d'identifier des erreurs de la traduction est absolument une bénéfice pour non seulement les utilisateurs, mais aussi les traducteurs, post-éditeurs, et les systèmes de TA eux-mêmes.
Nous appelons cette méthode les mesures de confiance (MC).
Cette thèse se porte principalement sur les méthodes des MC au niveau des mots (MCM).
Le système de MCM assigne à chaque mot de la phrase cible un étiquette de qualité.
Aujourd'hui, les MCM jouent un rôle croissant dans nombreux aspects de TA.
Tout d'abord, elles aident les post-éditeurs d'identifier rapidement les erreurs dans la traduction et donc d'améliorer leur productivité de travail.
De plus, elles informent les lecteurs des portions qui ne sont pas fiables pour éviter leur malentendu sur le contenu de la phrase.
Troisièmement, elles sélectionnent la meilleure traduction parmi les sorties de plusieurs systèmes de TA.
Finalement, et ce qui n'est pas le moins important, les scores MCM peuvent aider à perfectionner la qualité de TA via certains scénarios : ré-ordonnance des listes N-best, ré-décodage du graphique de la recherche, etc.
Dans cette thèse, nous visons à renforcer et optimiser notre système de MCM, puis à l'exploiter pour améliorer TA ainsi que les mesures de confiance au niveau des phrases (MCP).
Comparer avec les approches précédentes, nos nouvelles contributions étalent sur les points principaux comme suivants.
Tout d'abord, nous intégrons différents types des paramètres : ceux qui sont extraits du système TA, avec des caractéristiques lexicales, syntaxiques et sémantiques pour construire le système MCM de base.
L'application de différents méthodes d'apprentissage nous permet d'identifier la meilleure (méthode : "Champs conditionnels aléatoires") qui convient le plus nos donnés.
En suite, l'efficacité de touts les paramètres est plus profond examinée en utilisant un algorithme heuristique de sélection des paramètres.
Troisièmement, nous exploitons l'algorithme Boosting comme notre méthode d'apprentissage afin de renforcer la contribution des sous-ensembles des paramètres dominants du système MCM, et en conséquence d'améliorer la capacité de prédiction du système MCM.
En outre, nous enquérons les contributions des MCM vers l'amélioration de la qualité de TA via différents scénarios.
Dans le re-ordonnance des liste N-best, nous synthétisons les scores à partir des sorties du système MCM et puis les intégrons avec les autres scores du décodeur afin de recalculer la valeur de la fonction objective, qui nous permet d'obtenir un mieux candidat.
D'ailleurs, dans le ré-décodage du graphique de la recherche, nous appliquons des scores de MCM directement aux noeuds contenant chaque mot pour mettre à jour leurs coûts.
Finalement, les scores de MCM sont aussi utilisés pour renforcer les performances des systèmes de MCP.
Au total, notre travail apporte une image perspicace et multidimensionnelle sur des MCM et leurs impacts positifs sur différents secteurs de la TA.
Les résultats très prometteurs ouvrent une grande avenue où MCM peuvent exprimer leur rôle, comme : MCM pour la reconnaissance automatique de la parole (RAP), pour la sélection parmi plusieurs systèmes de TA, et pour les systèmes de TA auto-apprentissage.
Les avancées récentes en matière d'animation ont permis le déploiement de personnages virtuels à des fins diverses et variées.
Cependant, la génération d'animations pour ces personnages dépend de la description lexicale des signes, modèle linguistique dépendant du système de génération.
Les signes décrits par ces modèles sont généralement des réalisations parfaites et géométriques menant à des mouvements robotiques et peu naturels de la part du signeur.
Cette thèse s'intéresse à l'addition d'informations anatomiques au squelette de contrôle du personnage virtuel de manière à le faire signer de manière plus humaine et réaliste.
Ces informations supplémentaires sont regroupées sous l'appellation de "modèle anatomique" et sont divisées en cinq contributions principales : une nouvelle représentation informatique du squelette, une étude anthropométrique sur la main, l'unification de dépendances articulatoires, un nouveau modèle de complexe carpo-métacarpien permettant l'opposition aisée du pouce et enfin un modèle calculant le confort d'une posture.
Ces apports sont intégrés à une plateforme de génération au moyen de techniques adaptées aux contraintes imposées par les modèles linguistiques.
Les travaux sont conclus par une évaluation du système ainsi qu'une réflexion sur les travaux futurs pouvant être élaborés à partir de cette thèse.
En linguistique informatique, la relation entre langues différentes est souvent étudiée via des techniques d'alignement automatique.
De tels alignements peuvent être établis à plusieurs niveaux structurels.
En particulier, les alignements debi-textes aux niveaux phrastiques et sous-phrastiques constituent des sources importantes d'information dans pour diverses applications du Traitement Automatique du Language Naturel (TALN) moderne, la Traduction Automatique étant un exemple proéminent.
Cependant, le calcul effectif des alignements de bi-textes peut être une tâche compliquée.
Les divergences entre les langues sont multiples,de la structure de discours aux constructions morphologiques.
Les alignements automatiques contiennent, majoritairement, des erreurs nuisant aux performances des applications.
Dans cette situation, deux pistes de recherche émergent. La première est de continuer à améliorer les techniques d'alignement. La deuxième vise à développer des mesures de confiance fiables qui permettent aux applications de sélectionner les alignements selon leurs besoins.
Les techniques d'alignement et l'estimation de confiance peuvent tous les deux bénéficier d'alignements manuels.
Des alignements manuels peuvent jouer un rôle de supervision pour entraîner des modèles, et celui des données d'évaluation.
Pourtant, la création des telles données est elle-même une question importante, en particulier au niveau sous-phrastique, où les correspondances multilingues peuvent être implicites et difficiles à capturer.
Cette thèse étudie des moyens pour acquérir des alignements de bi-textes utiles, aux niveaux phrastiques et sous-phrastiques.
Le chapitre 1 fournit une description de nos motivations,la portée et l'organisation du travail, et introduit quelques repères terminologiques et les principales notations.
L'état-de-l'art des techniques d'alignement est revu dans la Partie I.
Les chapitres 2 et 3 décrivent les méthodes respectivement pour l'alignement des phrases et des mots.
Le chapitre 4 présente les bases de données d'alignement manuel,et discute de la création d'alignements de référence.
Le reste de la thèse, la Partie II,présente nos contributions à l'alignement de bi-textes, en étudiant trois aspects.
Le chapitre 5 présente notre contribution à la collection d'alignements de référence.
Pour l'alignement des phrases, nous collectons les annotations d'un genre spécifique de textes : les bi-textes littéraires.
Nous proposons aussi un schéma d'annotation de confiance.
Pour l'alignement sous-phrastique,nous annotons les liens entre mots isolés avec une nouvelle catégorisation, et concevons une approche innovante de segmentation itérative pour faciliter l'annotation des liens entre groupes de mots.
Toutes les données collectées sont disponibles en ligne.
L'amélioration des méthodes d'alignement reste un sujet important de la recherche.
Nous prêtons une attention particulière à l'alignement phrastique, qui est souvent le point de départ de l'alignement de bi-textes.
Le chapitre 6 présente notre contribution.
En commençant par évaluer les outils d'alignement d'état-de-l'art et par analyser leurs modèles et résultats,nous proposons deux nouvelles méthodes pour l'alignement phrastique, qui obtiennent des performances d'état-de-l'art sur un jeu de données difficile.
L'autre sujet important d'étude est l'estimation de confiance.
Dans le chapitre 7, nous proposons des mesures de confiance pour les alignements phrastique et sous-phrastique.
Les expériences montrent que l'estimation de confiance des liens d'alignement reste un défi remarquable. Il sera très utile de poursuivre cette étude pour renforcer les mesures de confiance pour l'alignement de bi-textes.
Enfin, notons que les contributions apportées dans cette thèse sont employées dans une application réelle : le développement d'une liseuse qui vise à faciliter la lecture des livres électroniques multilingues.
Cette étude est dédiée à un problème d'exploration de données dans les médias sociaux : la prédiction d'activité.
Dans ce problème nous essayons de prédire l'activité associée à une thématique pour un horizon temporel restreint.
Dans ce problème des contenus générés par différents utilisateurs, n'ayant pas de lien entre eux, contribuent à l'activité d'une même thématique.
Trois définitions de la prédiction d'activité sont proposées.
Premièrement la prédiction de la magnitude d'activité, un problème de régression qui vise à prédire l'activité exacte d'une thématique.
Ces trois problèmes sont étudiés avec les méthodes de l'état de l'art en apprentissage automatique.
Les descripteurs proposés pour ces études sont définis en utilisant le cadre d'analyse générique.
Ainsi il est facile d'adapter ces descripteurs à différent média sociaux.
Plus de 500 millions de contenus générés par les utilisateurs ont été capturé.
Une méthode de validation croisée est proposée afin de ne pas introduire de biais expérimental lié au temps.
De plus, une méthode d'extraction non-supervisée des candidats au buzz est proposée.
En effet, les changements abrupts de popularité sont rares et l'ensemble d'entraˆınement est très déséquilibré.
Les problèmes de prédiction de l'activité sont étudiés dans deux configurations expérimentales différentes.
La première configuration expérimentale porte sur l'ensemble des données collectées dans les deux médias sociaux, et sur les trois langues observées.
La seconde configuration expérimentale porte exclusivement sur Twitter.
Cette seconde configuration expérimentale vise à améliorer la reproductibilité de nos expériences.
Pour ce faire, nous nous concentrons sur un sous-ensemble des thématiques non ambigu¨es en Anglais.
Ce travail de thèse traite de la découverte de connaissances à partir de Séries Temporelles de Champs de Déplacements (STCD) obtenues par imagerie satellitaire.
De telles séries occupent aujourd'hui une place centrale dans l'étude et la surveillance de phénomènes naturels tels que les tremblements de terre, les éruptions volcaniques ou bien encore le déplacement des glaciers.
En effet, ces séries sont riches d'informations à la fois spatiales et temporelles et peuvent aujourd'hui être produites régulièrement à moindre coût grâce à des programmes spatiaux tels que le programme européen Copernicus et ses satellites phares Sentinel.
Nos propositions s'appuient sur l'extraction de motifs Séquentiels Fréquents Groupés (SFG).
Néanmoins, ils ne permettent pas d'utiliser les indices de confiance intrinsèques aux STCD et la méthode de swap randomisation employée pour sélectionner les motifs les plus prometteurs ne tient pas compte de leurs complémentarités spatiotemporelles, chaque motif étant évalué individuellement.
Notre contribution est ainsi double.
Une première proposition vise tout d'abord à associer une mesure de fiabilité à chaque motif en utilisant les indices de confiance.
Cette mesure permet de sélectionner les motifs portés par des données qui sont en moyenne suffisamment fiables.
Nous proposons un algorithme correspondant pour réaliser les extractions sous contrainte de fiabilité.
Celui-ci s'appuie notamment sur une recherche efficace des occurrences les plus fiables par programmation dynamique et sur un élagage de l'espace de recherche grâce à une stratégie de push partiel, ce qui permet de considérer des STCD conséquentes.
Une deuxième contribution visant à sélectionner les motifs les plus prometteurs est également présentée.
Celle-ci, basée sur un critère informationnel, permet de prendre en compte à la fois les indices de confiance et la façon dont les motifs se complètent spatialement et temporellement.
Pour ce faire, les indices de confiance sont interprétés comme des probabilités, et les STCD comme des bases de données probabilistes dont les distributions ne sont que partielles.
Le gain informationnel associé à un motif est alors défini en fonction de la capacité de ses occurrences à compléter/affiner les distributions caractérisant les données.
Sur cette base, une heuristique est proposée afin de sélectionner des motifs informatifs et complémentaires.
Cette méthode permet de fournir un ensemble de motifs faiblement redondants et donc plus faciles à interpréter que ceux fournis par swap randomisation.
Elle a été implémentée au sein d'un prototype dédié.
Outre le fait d'être construites à partir de données et de techniques de télédétection différentes, ces séries se différencient drastiquement en termes d'indices de confiance, la série couvrant le massif du Mont-Blanc se situant à des niveaux de confiance très faibles.
Pour les deux STCD, les méthodes proposées ont été mises en œuvre dans des conditions standards au niveau consommation de ressources (temps, espace), et les connaissances des experts sur les zones étudiées ont été confirmées et complétées.
Les Agents Conversationnels
Animés sont des personnages virtuels dont la fonction principale est d'interagir avec l'utilisateur.
Les utilisateurs, conscient d'interagir avec une machine, sont tout de même capable d'analyser et d'identifier des comportements sociaux à travers les signaux émis par les agents.
La recherche en ACA s'est longtemps intéressée aux mécanismes de reproduction et de reconnaissance des émotions au sein de ces personnages virtuels et maintenant l'intérêt se porte sur la capacité d'exprimer différentes attitudes sociales.
Ces attitudes reflètent un style comportemental et s'expriment à travers différentes modalités du corps comme les expressions faciales, les regards ou les gestes par exemple.
Nous avons proposé un modèle permettant à un agent de produire différents comportements non-verbaux traduisant l'expression d'attitudes sociales dans une conversation.
L'ensemble des comportements générés par notre modèle permettent à un groupe d'agents animés par celui-ci de simuler une conversation, sans tenir compte du contenu verbal.
Deux évaluations du modèle ont été conduites, l'une sur Internet et l'autre dans un environnement de réalité virtuelle, afin de vérifier que les attitudes étaient bien reconnues.
Dans cette thèse, nous nous intéressons à la manière dont les images SAR peuvent être utilisées pour étudier la végétation.
La végétation est au coeur de la vie humaine en fournissant à la fois des ressources alimentaires, financières et en participant à la régulation du climat.
Traditionnellement, la végétation est classée en trois catégories : les champs, les prairies irriguées et les forêts.
Nous utiliserons ces trois catégories dans notre étude.
L'objectif de la première partie est de fournir une meilleure compréhension du potentiel des images radar Sentinel-1 (bande C) pour cartographier l'occupation du sol à l'aide des techniques d'apprentissage en profondeur.
Nous avons obtenu de bons résultats avec la "F-Measure/Accuracy" supérieure à 86% et le meilleur coefficient Kappa de plus de 0,82.
Nous avons constaté que les résultats des deux classificateurs basés sur les réseaux neuronaux récurrents profonds (RNN) dépassaient clairement les approches classiques de Machine Learning.
Dans la seconde partie, l'objectif est d'étudier la capacité des images radar multitemporelles pour l'estimation de la hauteur du riz et de la biomasse sèche à l'aide des données Sentinel-1.
Pour ce faire, nous avons utilisé les données de Sentinel-1 en appliquant des techniques classiques d'apprentissage de "Machine Learning" (MLR, SVR et RF) pour estimer la hauteur du riz et la biomasse sèche.
Ces résultats indiquent que les données radar Sentinel-1 pourraient être exploitées pour la récupération de la biomasse et pourraient être utilisées pour des tâches opérationnelles.
Enfin, la réduction des émissions de carbone dues à la déforestation nécessite un aperçu de la façon dont la forêt de biomasse est mesurée et distribuée.
Nous utilisons des observations du radar satellitaire ALOS/PALSAR (résolution de 25 m) et des données optiques du capteur Landsat (résolution de 30 m) pour estimer les stocks de biomasse forestière à Madagascar, pour les années 2007-2010.
Le signal radar et la biomasse in situ étaient fortement corrélés (R² =0,71) et l'erreur quadratique moyenne était de 30% (pour la biomasse allant de 0 à 500 t/ha).
Le signal radar (données SAR en bande L) combiné avec les données optiques semblent être une approche prometteuse pour cartographier la biomasse forestière (et donc du carbone) à de larges échelles géographiques.
Notre travail est présenté en trois parties indépendantes.
Tout d'abord, nous proposons trois heuristiques d'apprentissage actif pour les réseaux de neurones profonds : Nous mettons à l'échelle le `query by committee', qui agrège la décision de sélectionner ou non une donnée par le vote d'un comité.
Pour se faire nous formons le comité à l'aide de différents masques de dropout.
Un autre travail se base sur la distance des exemples à la marge.
Nous proposons d'utiliser les exemples adversaires comme une approximation de la dite distance.
Nous démontrons également des bornes de convergence de notre méthode dans le cas de réseaux linéaires.
Notre méthode sélectionne les données qui minimisent l'énergie libre variationnelle.
Dans un second temps, nous nous sommes concentrés sur la distance de Wasserstein.
Nous projetons les distributions dans un espace où la distance euclidienne mimique la distance de Wasserstein.
Également, nous démontrons les propriétés sous-modulaires des prototypes de Wasserstein et comment les appliquer à l'apprentissage actif.
Enfin, nous proposons de nouveaux outils de visualisation pour expliquer les prédictions d'un CNN sur du langage naturel.
Deuxièmement, nous profitons des algorithmes de déconvolution des CNNs afin de présenter une nouvelle perspective sur l'analyse d'un texte.
Le but de cette thèse est de caractériser ce qui se passe à la jonction entre le système linguistique et le système conceptuel de notre esprit.
Nous défendons la thèse selon laquelle le système linguistique de notre esprit encode des règles permettant de manipuler directement des représentations mentales dotées d'un contenu sémantique.
Après avoir examiné les propositions des formalistes du début du XXe siècle (Frege, Russell), des sémanticiens formels (Montague, Lewis) et de la pragmatique de Grice, nous développons notre propre théorie.
Nous testons alors notre théorie en l'appliquant au problème des implicatures enchâssées et terminons en signalant ses applications possibles en TALN, à la clarification de l'hypothèse du langage de la pensée et à la distinction sémantique/pragmatique.
Objectif de cette thèse est le développent de méthodes de planification pour la résolution de tâches jointes homme-robot dans des espaces publiques.
La thèse décrit l'état de l'art sur la coopération homme-robot dans la robotique de service, et sur les modèles de planification.
La thèse introduit une structure hiérarchique qui sépare l'aspect coopératif d'une activité jointe de la tâche en soi.
L'approche a été appliquée dans un scénario réel, un robot guide dans un centre commercial.
La thèse présente les expériences effectuées pour mesurer la qualité de l'approche proposée, ainsi que les expériences avec le robot réel.
Cette thèse vise à établir une théorie sémantique des textes appelée herméneutique formelle qui applique des méthodes mathématiques rigoureuses dans l'étude des processus d'interprétation des textes en langue naturelle, dits admissibles, que nous dirons écrits « avec bonne volonté » en tant que messages destinés à la compréhension.
Dans le paradigme phonocentrique de lecture, une langue est décrite dans une catégorie Logos dite des espaces textuels.
Un genre particulier des textes y définit une sous-catégorie pleine des schémas formels discursifs.
Définies pour un texte X donné, la catégorie Schl(X) des faisceaux des significations fragmentaires, dite de Schleiermacher, sert à formaliser un principe compositionnel généralisé de Frege, et la catégorie Context(X) des espaces étalés des significations contextuelles sert à formaliser un principe contextuel généralisé de Frege.
Établie par le foncteur de sections et le foncteur de germes, une équivalence de catégories Schl(X) ?Context(X), dite dualité de Frege, donne lieu à une représentation fonctionnelle des significations fragmentaires, ce qui permet de décrire le processus de la compréhension d'un texte.
Nous considérons comme universaux linguistiques la connexité et la T0–séparabilité de Kolmogoroff de la topologie phonocentrique sous-jacente à un texte.
Les bases de données lexicales jouent un grand rôle dans le TAL, mais, elles nécessitent un développement et un enrichissement permanents via l'exploitation des ressources libres du web sémantique, entre autres, l'encyclopédie Wikipédia, DBpedia, Geonames et Yago2.
Prolexbase, comporte à ce jour dix langues, trois parmi elles sont bien couvertes : le francais, l'anglais et le polonais.
Il a été conçu manuellement et une première tentative semi-automatique a été réalisée par le projet ProlexFeeder (Savary et al. 2013).
L'objectif de notre travail était d'élaborer un outil de mise à jour et d'extension automatiques de ce lexique, et l'ajout de la langue arabe.
Un système automatique a également été mis en place pour calculer via la Wikipédia l'indice de notoriété des entrées de Prolexbase ;
cet indice dépend de la langue et participe, d'une part, à la construction d'un module de Prolexbase pour la langue arabe et, d'autre part, à la révision de la notoriété présente pour les autres langues de la base.
Un graphe est un ensemble de noeuds, ensemble de liens reliant des paires de noeuds.
Avec la quantité accumulée de données collectées, il existe un intérêt croissant pour la compréhension des structures et du comportement de très grands graphes.
Néanmoins, l'augmentation rapide de la taille des grands graphes rend l'étude de tous les graphes de moins en moins efficace.
Ainsi, il existe une demande impérieuse pour des méthodes plus efficaces pour étudier de grands graphes sans nécessiter la connaissance de tous les graphes.
Une méthode prometteuse pour comprendre le comportement de grands graphes consiste à exploiter des propriétés spécifiques de structures locales, telles que la taille des grappes ou la présence locale d'un motif spécifique, c'est-à-dire un graphe donné (généralement petit).
Un exemple classique de la théorie des graphes (cas avérés de la conjecture d'Erdos-Hajnal) est que, si un graphe de grande taille ne contient pas de motif spécifique, il doit alors avoir un ensemble de noeuds liés par paires ou non liés, de taille exponentiellement plus grande que prévue.
Cette thèse abordera certains aspects de deux questions fondamentales de la théorie des graphes concernant la présence, en abondance ou à peine, d'un motif donné dans un grand graphe :
- Le grand graphe peut-il être partitionné en copies du motif ?
- Le grand graphe contient-il une copie du motif ?
Nous discuterons de certaines des conjectures les plus connues de la théorie des graphes sur ce sujet : les conjectures de Tutte sur les flots dans les graphes et la conjecture d'Erdos-Hajnal mentionnée ci-dessus, et présenterons des preuves pour plusieurs conjectures connexes-y compris la conjecture de Barát-Thomassen, une conjecture de Haggkvist et Krissell, un cas particulier de la conjecture de Jaeger-Linial-Payan-Tarsi, une conjecture de Berger et al, et une autre d'Albouker et al.
Image sur le web : analyse de la dynamique des images sur le Web 2.0.
Chaque jour, des millions d'individus publient leurs avis sur le Web 2.0 (réseaux sociaux, blogs, etc.).
Ces commentaires portent sur des sujets aussi variés que l'actualité, la politique, les résultats sportifs, biens culturels, des objets de consommation, etc.
Cette idée porte a priori sur un sujet particulier et n'est valable que dans un contexte, à un instant donné.
Cette image perçue est par nature différente de celle que l'entité souhaitait initialement diffuser (par exemple via une campagne de communication).
De plus, dans la réalité, il existe au final plusieurs images qui cohabitent en parallèle sur le réseau, chacune propre à une communauté et toutes évoluant différemment au fil du temps (imaginons comment serait perçu dans chaque camp le rapprochement de deux hommes politiques de bords opposés).
Enfin, en plus des polémiques volontairement provoquées par le comportement de certaines entités en vue d'attirer l'attention sur elles (pensons aux tenues ou déclarations choquantes), il arrive également que la diffusion d'une image dépasse le cadre qui la régissait et même parfois se retourne contre l'entité (par exemple, « le mariage pour tous » devenu « la manif pour tous » ).
Les opinions exprimées constituent alors autant d'indices permettant de comprendre la logique de construction et d'évolution de ces images.
Ce travail d'analyse est jusqu'à présent confié à des spécialistes de l'e-communication qui monnaient leur subjectivité.
Ces derniers ne peuvent considérer qu'un volume restreint d'information et ne sont que rarement d'accord entre eux.
Dans cette thèse, nous proposons d'utiliser différentes méthodes automatiques, statistiques, supervisées et d'une faible complexité permettant d'analyser et représenter l'image de marque d'entité à partir de contenus textuels les mentionnant.
Plus spécifiquement, nous cherchons à identifier les contenus(ainsi que leurs auteurs) qui sont les plus préjudiciables à l'image de marque d'une entité.
Nous introduisons un processus d'optimisation automatique de ces méthodes automatiques permettant d'enrichir les données en utilisant un retour de pertinence simulé (sans qu'aucune action de la part de l'entité concernée ne soit nécessaire).
Nous comparer également plusieurs approches de contextualisation de messages courts à partir de méthodes de recherche d'information et de résumé automatique.
Nous tirons également parti d'algorithmes de modélisation(tels que la Régression des moindres carrés partiels), dans le cadre d'une modélisation conceptuelle de l'image de marque, pour améliorer nos systèmes automatiques de catégorisation de documents textuels.
Les thiopurines sont des médicaments cytotoxiques et immunosuppresseurs largement prescrits, notamment dans les maladies inflammatoires chroniques de l'intestin (MICI).
La variabilité interindividuelle de la réponse à ces médicaments rend nécessaire leur optimisation thérapeutique.
Ce travail de thèse a d'une part, analysé les relations entre activité TPMT et concentrations des métabolites thiopuriniques, et d'autre part, recherché des facteurs associés à la résistance aux thiopurines.
De plus, une étude clinique rétrospective dans les MICI pédiatriques a permis d'identifier des facteurs associés à la lymphopénie observée sous thiopurines.
Enfin, à partir d'un modèle in vitro fondé sur des lignées cellulaires lymphoblastoïdes (LCL) sélectionnées, nous avons établi une signature transcriptomique, incluant 32 gènes, prédictive de la résistance aux thiopurines.
Une analyse fonctionnelle bioinformatique a abouti à l'identification de voies métaboliques liées à la protéine p53 et au cycle cellulaire, ainsi que des mécanismes moléculaires associés à la résistance aux thiopurines.
En conclusion, ce travail de thèse, qui a exploré la variabilité de réponse aux thiopurines et tout particulièrement la résistance à ces médicaments, propose des hypothèses pour l'individualisation et l'optimisation thérapeutique des thiopurines.
En effet, bien qu'il existe plusieurs dizaines de modèles de la saillance visuelle, à la fois en termes de contraste et de cognition, il n'existe pas de modèle hybridant les deux mécanismes d'attention : l'aspect visuel et l'aspect cognitif.
Pour créer un tel modèle, nous avons exploré les approches existantes dans le domaine de l'attention visuelle, ainsi que plusieurs approches et paradigmes relevant des domaines connexes (tels que la reconnaissance d'objets, apprentissage artificiel, classification, etc.).Une architecture fonctionnelle d'un système d'attention visuelle hybride, combinant des principes et des mécanismes issus de l'attention visuelle humaine avec des méthodes calculatoires et algorithmiques, a été mise en œuvre, expliquée et détaillée.
Les études menées ont conclu à la validation expérimentale des modèles proposés, confirmant la pertinence de l'approche proposée dans l'accroissement de l'autonomie des systèmes robotisés – et ceci dans un environnement réel
Les réseaux de neurones profonds ont obtenu des résultats impressionnants dans de nombreux domaines liés à l'IA, allant des jeux informatiques à la vision par ordinateur et au traitement du langage naturel, pour n'en citer que quelques-uns.
L'ingénierie d'architecture a permis au taux d'erreur sur ImageNet de passer de 16,4% (AlexNet) à 3,57% (ResNet-152).
La conception d'architectures prend du temps et est difficile.
Des méthodes automatiques ont été développées pour apprendre des architectures et, ces dernières années, elles ont utilisé une puissance de calcul massive pour atteindre des performances de pointe sur divers problèmes.
L'objectif de ce projet de thèse, en collaboration entre Paris Dauphine et Facebook AI Research Paris, comprend :
Contribuer à l'état de l'art en concevant et en développant de nouvelles méthodes d'optimisation sans dérivé pour la recherche d'architecture.
Trouvez de nouvelles architectures offrant une meilleure précision ou des compromis intéressants entre la taille, la vitesse de calcul et les performances.
Généraliser l'architecture pour apprendre des architectures plus efficaces pour l'apprentissage par renforcement, l'adaptation de domaine ou pour gérer les problèmes survenant lorsque la distribution des données de test est différente de celle des données du train (décalage covariable).
Dans un environnement collaboratif de développement de systèmes complexes, plusieurs entreprises doivent échanger un nombre important de modèles hétérogènes et d'exigences.
Durant les phases du cycle de vie du système, ces artefacts, reliés les uns aux autres et issus de différents outils de modélisations, évoluent constamment.
Dans un tel environnement, il est crucial de gérer l'impact des différents changements se produisant dans les différents espaces de conception.
La traçabilité répond à ce besoin.
Toutefois, établir des liens entre des exigences et des modèles en ingénierie des systèmes complexes suppose de faire face à une volumétrie importante des artefacts.
Par exemple, pour une spécification d'un véhicule autonome comprenant 3 000 exigences et 400 éléments de modèles, il faudrait en théorie vérifier de l'ordre d'un million de liens potentiels.
Bien que plusieurs approches aient été proposées pour l'identification des liens de traçabilité, le processus de validation des liens est toujours chronophage et générateur d'erreurs.
Cette approche fournit ainsi une mesure quantitative de confiance sur chaque lien candidat.
Même si l'information est abondante dans le monde, l'information structurée, prête à être utilisée est rare.
Ce travail propose l'Extraction d'Information (EI) comme une approche efficace pour la production de l'information structurée, utilisable sur la biologie, en présentant une tâche complète d'EI sur un organisme modèle, Arabidopsis thaliana.
Un système d'EI se charge d'extraire les parties de texte les plus significatives et d'identifier leurs relations sémantiques.
En collaboration avec des experts biologistes sur la plante A. Thaliana un modèle de connaissance a été conçu.
Son objectif est de formaliser la connaissance nécessaire pour bien décrire le domaine du développement de la graine.
Ce modèle contient toutes les entités et relations les connectant qui sont essentielles et peut être directement utilisé par des algorithmes.
En parallèle ce modèle a été testé et appliqué sur un ensemble d'articles scientifiques du domaine, le corpus nécessaire pour l'entraînement de l'apprentissage automatique.
Les experts ont annoté le texte en utilisant les entités et relations du modèle.
Le modèle et le corpus annoté sont les premiers proposés pour le développement de la graine, et parmi les rares pour A. Thaliana, malgré son importance biologique.
Ce modèle réconcilie les besoins d'avoir un modèle assez complexe pour bien décrirele domaine, et d'avoir assez de généralité pour pouvoir utiliser des méthodes d'apprentissage automatique.
Une approche d'extraction de relations (AlvisRE) a également été élaborée et développée.
Une fois les entités reconnues, l'extracteur de relations cherche à détecter les cas où le texte mentionne une relation entre elles, et identifier précisément de quel type de relation du modèle il s'agit.
L'approche AlvisRE est basée sur la similarité textuelle et utilise à la fois des informations lexiques,syntactiques et sémantiques.
Dans les expériences réalisées, AlvisRE donne des résultats qui sont équivalents et parfois supérieurs à l'état de l'art.
En plus, AlvisRE a l'avantage de la modularité et adaptabilité en utilisant des informations sémantiques produites automatiquement.
Ce dernier caractéristique permet d'attendre des performances équivalentes dans d'autres domaines.
Cependant, cette classe de mots reste à l'heure actuelle la moins exploitée : les règles de la flexion ne décrivent pas tous les cas de figures de la conjugaison macédonienne et leur approche s'effectue de manière trop synthétique pour être opérationnelle dans une optique didactique.
Pour toutes ces raisons, le but de cette thèse est d'explorer un grand nombre de verbes fléchis afin de déceler des modèles stables de conjugaison ouvrant de nouvelles pistes pour l'apprentissage du système verbal du macédonien.
La Théorie du Contrôle par Supervision (TCS) est l'un des paradigmes formels les plus importants pour le développement de contrôleurs dans le cadre de l'étude des systèmes à événements discrets (SED).
Le grand nombre de contributions scientifiques montre que la TCS suscite un intérêt académique considérable et il a été prouvé que cette théorie était applicable dans divers domaines industriels tels que les systèmes de fabrication, les systèmes embarqués ou les systèmes de transport.
La conséquence immédiate est que l'étape de vérification n'a plus lieu d'être, ce qui élimine aussi tous les cycles de reconception et de vérification nécessaires à la mise au point du système.
Cependant, cette théorie souffre d'un manque d'intégration dans un processus global de conception dans lequel les exigences initiales sur le système à concevoir sont utilisées pour définir d'une part les exigences sur ses sous-systèmes et d'autre part les contrôleurs locaux associés.
L'ingénierie dirigée par les modèles (IDM) fournit les solutions permettant de gérer les limitations de SCT.
L'objectif de cette étude est de proposer un nouveau cadre pour la conception des contrôleurs, qui intègre à la fois TCS et IDM afin de combler les lacunes du paradigme formel et du processus d'ingénierie.
Dans le cadre proposé, différents diagrammes SysML sont utilisés en tant que modèles complémentaires présentant les vues indispensables du système à étudier dans le processus de modélisation globale.
De plus, afin de maintenir la cohérence entre les modèles SysML et les modèles formels, des méthodes de modélisation et de vérification formelles sont proposées.
Une étude de cas présentée à la fin de la thèse montre que le cadre proposé, qui fournit un processus de développement global allant de l'analyse des besoins à la mise en œuvre du contrôleur, peut parfaitement répondre aux besoins de la pratique de l'ingénierie.
Le texte est l'une des sources d'informations les plus répandues et les plus persistantes.
L'analyse de contenu du texte se réfère à des méthodes d'étude et de récupération d'informations à partir de documents.
Aujourd'hui, avec une quantité de texte disponible en ligne toujours croissante l'analyse de contenu du texte revêt une grande importance parce qu' elle permet une variété d'applications.
À cette fin, les méthodes d'apprentissage de la représentation sans supervision telles que les modèles thématiques et les word embeddings constituent des outils importants.
L'objectif de cette dissertation est d'étudier et de relever des défis dans ce domaine.
Dans la première partie de la thèse, nous nous concentrons sur les modèles thématiques et plus précisément sur la manière d'incorporer des informations antérieures sur la structure du texte à ces modèles.
Les modèles de sujets sont basés sur le principe du sac-de-mots et, par conséquent, les mots sont échangeables.
Bien que cette hypothèse profite les calculs des probabilités conditionnelles, cela entraîne une perte d'information.
Pour éviter cette limitation, nous proposons deux mécanismes qui étendent les modèles de sujets en intégrant leur connaissance de la structure du texte.
Nous supposons que les documents sont répartis dans des segments de texte cohérents.
Le premier mécanisme attribue le même sujet aux mots d'un segment.
La seconde, capitalise sur les propriétés de copulas, un outil principalement utilisé dans les domaines de l'économie et de la gestion des risques, qui sert à modéliser les distributions communes de densité de probabilité des variables aléatoires tout en n'accédant qu'à leurs marginaux.
En règle générale, une collection de documents pour ces modèles se présente sous la forme de paires de documents comparables.
Les documents d'une paire sont écrits dans différentes langues et sont thématiquement similaires.
À moins de traductions, les documents d'une paire sont semblables dans une certaine mesure seulement.
Pendant ce temps, les modèles de sujets représentatifs supposent que les documents ont des distributions thématiques identiques, ce qui constitue une hypothèse forte et limitante.
Pour le surmonter, nous proposons de nouveaux modèles thématiques bilingues qui intègrent la notion de similitude interlingue des documents qui constituent les paires dans leurs processus générateurs et d'inférence.
La dernière partie de la thèse porte sur l'utilisation d'embeddings de mots et de réseaux de neurones pour trois applications d'exploration de texte.
Tout d'abord, nous abordons la classification du document polylinguistique où nous soutenons que les traductions d'un document peuvent être utilisées pour enrichir sa représentation.
À l'aide d'un codeur automatique pour obtenir ces représentations de documents robustes, nous démontrons des améliorations dans la tâche de classification de documents multi-classes.
Deuxièmement, nous explorons la classification des tweets à plusieurs tâches en soutenant que, en formant conjointement des systèmes de classification utilisant des tâches corrélées, on peut améliorer la performance obtenue.
À cette fin, nous montrons comment réaliser des performances de pointe sur une tâche de classification du sentiment en utilisant des réseaux neuronaux récurrents.
La troisième application que nous explorons est la récupération d'informations entre langues.
Compte tenu d'un document écrit dans une langue, la tâche consiste à récupérer les documents les plus similaires à partir d'un ensemble de documents écrits dans une autre langue.
Dans cette ligne de recherche, nous montrons qu'en adaptant le problème du transport pour la tâche d'estimation des distances documentaires, on peut obtenir des améliorations importantes.
Dans cette thèse, nous considérons le problème d'évaluation de situations dans le cadre plus spécifique des situations de crise.
Nous proposons un système d'évaluation de situations automatisé (le système ASAAP) cherchant à pallier ces deux problèmes.
Nous y présentons une modélisation dynamique de l'environnement permettant une analyse des variables d'états utiles à l'évaluation de la situation afin de réduire le champ des possibles et maximiser le gain d'informations.
Dans le cadre des situations de crise, nous proposons également une application du système à l'évaluation de menace.
Cette contribution permet d'analyser et définir la menace à laquelle sont exposées différentes zones tout en définissant la stratégie de l'ennemi.
Enfin, nous proposons une approche préliminaire sur l'optimisation de l'utilisation des capteurs pour couvrir les variables d'états utiles dans l'optique de maximisation du gain d'informations.
Nous avons généré des scénarios inspirés de situations réelles afin d'évaluer nos approches dans les domaines maritime et militaire.
Les résultats du système ASAAP montrent une amélioration de l'évaluation de situation en terme de complexité ainsi que la capacité de décrire la stratégie de l'ennemi dans un temps raisonnable.
Cette thèse se concentre sur les méthodes d'apprentissage pour la transcription automatique de la batterie.
Elles sont basées sur un algorithme de transcription utilisant une méthode de décomposition non-négative, la NMD.
Cette thèse soulève deux principales problématiques : l'adaptation des méthodes au signal analysé et l'utilisation de l'apprentissage profond.
La prise en compte des informations du signal analysé dans le modèle peut être réalisée par leur introduction durant les étapes de décomposition.
Une première approche est de reformuler l'étape de décomposition dans un contexte probabiliste pour faciliter l'introduction d'informations a posteriori avec des méthodes comme la SI-PLCA et la NMD statistique.
Une deuxième approche est d'implémenter directement dans la NMD une stratégie d'adaptation : l'application de filtres modelables aux motifs pour modéliser les conditions d'enregistrement ou l'adaptation des motifs appris directement au signal en appliquant de fortes contraintes pour conserver leur signification physique.
La deuxième approche porte sur la sélection des segments de signaux à analyser.
Les résultats obtenus étant très intéressants, le détecteur est entraîné à ne détecter qu'un seul instrument permettant la réalisation de la transcription des trois principaux instruments de batterie avec trois CNN.
Finalement, l'utilisation d'un CNN multi-sorties est étudiée pour transcrire la partie de batterie avec un seul réseau.
Depuis leur apparition, les emojis ont une popularité grandissante dans les systèmes de communication.
Ces petites images pouvant représenter une idée, un concept ou une émotion, se retrouvent disponibles aux utilisateurs dans de nombreux contextes logiciels : messagerie, courriel, forums et autres réseaux sociaux.
Leur usage, en hausse constante, a entraîné l'apparition récurrente de nouveaux emojis.
Le parcours de bibliothèques d'emojis ou l'utilisation de moteur de recherche intégré n'est plus suffisant pour permettre à l'utilisateur de maximiser leur utilisation ;
une recommandation d'emojis adaptée est nécessaire.
Pour cela nous présentons nous travaux de recherche axés sur le thème de la recommandation d'emojis.
Ces travaux ont pour objectifs de créer un système de recommandation automatique d'emojis adapté à un contexte conversationnel informel et privé.
Ce système doit améliorer l'expérience utilisateur et la qualité de la communication, et prendre en compte d'éventuels nouveaux emojis créés.
Dans le cadre de cette thèse, nous contribuons tout d'abord en montrant les limites d'usage réel d'une prédiction d'emojis ainsi que la nécessité de prédire des notions plus générales.
Nous vérifions également si l'usage réel des emojis représentant une expression faciale d'émotion correspond à l'existant théorique.
Enfin, nous abordons les pistes d'évaluation d'un tel système par l'insuffisance des métriques, et l'importance d'une interface utilisateur dédiée.
La maladie d'Alzheimer (MA) est la première cause de démence dans le monde, touchant plus de 20 millions de personnes.
Son diagnostic précoce est essentiel pour assurer une prise en charge adéquate des patients ainsi que pour développer et tester de nouveaux traitements.
La MA est une maladie complexe qui nécessite différentes mesures pour être caractérisée : tests cognitifs et cliniques, neuroimagerie, notamment l'imagerie par résonance magnétique (IRM) et la tomographie par émission de positons (TEP), génotypage, etc.
Il y a un intérêt à explorer les capacités discriminatoires et prédictives à un stade précoce de ces différents marqueurs, qui reflètent différents aspects de la maladie et peuvent apporter des informations complémentaires.
L'objectif de cette thèse de doctorat était d'évaluer le potentiel et d'intégrer différentes modalités à l'aide de méthodes d'apprentissage statistique, afin de classifier automatiquement les patients atteints de la MA et de prédire l'évolution de la maladie dès ses premiers stades.
Plus précisément, nous visions à progresser vers une future application de ces approches à la pratique clinique.
La thèse comprend trois études principales.
La première porte sur le diagnostic différentiel entre différentes formes de démence à partir des données IRM.
Cette étude a été réalisée à l'aide de données de routine clinique, ce qui a permis d'obtenir un scénario d'évaluation plus réaliste.
La seconde propose un nouveau cadre pour l'évaluation reproductible des algorithmes de classification de la MA à partir des données IRM et TEP.
En effet, bien que de nombreuses approches aient été proposées dans la littérature pour la classification de la MA, elles sont difficiles à comparer et à reproduire.
Alzheimer chez les patients atteints de troubles cognitifs légers par l'intégration de données multimodales, notamment l'IRM, la TEP, des évaluations cliniques et cognitives, et le génotypage.
En particulier, nous avons systématiquement évalué la valeur ajoutée de la neuroimagerie par rapport aux seules données cliniques/cognitives.
Comme la neuroimagerie est plus coûteuse et moins répandue, il est important de justifier son utilisation dans les algorithmes de classification.
La présente thèse recherche une approche sémantique pour l'extraction et l'analyse du discours idéologique au sein de documents textuels sous forme électronique.
Cette approche intègre une méthode qualitative d'analyse de textes issue des sciences sociales (analyse critique du discours) avec une méthode quantitative de raisonnement et extraction d'information à base d'ontologies de domaine et de traitement semi-automatique du langage naturel.
L'application centrale du projet de thèse vise à étudier le discours marxiste comme représenté par la collection thématique Archive Internet des Marxistes (http : //www.marxists.org) contenant près de 15000 textes.
L'objectif de l'application est l'acquisition de schémas émergents, qui pourront contribuer à la classification par point de vue idéologique de textes inconnus.
Toutefois, leur mise en œuvre au sein de laboratoires physiques requiert des infrastructures souvent coûteuses pour les institutions de formation qui peuvent ainsi difficilement faire face à la forte augmentation du nombre d'étudiants.
Dans ce contexte, les laboratoires virtuels et distants (VRL) représentent une alternative pour assurer le passage à l'échelle des activités pratiques à moindre coût.
De nombreux travaux de recherche ont émergé au cours de la dernière décennie en se focalisant sur les problématiques techniques et technologiques induites par ces nouveaux usages, telles que la fédération, la standardisation, ou la mutualisation des ressources de laboratoires.
Cependant, les récentes revues de littérature du domaine mettent en exergue la nécessité de se préoccuper davantage des facettes pédagogiques liées à ces environnements informatiques innovants dédiés à l'apprentissage pratique.
Dans cet objectif, nos travaux exploitent les traces issues des activités réalisées par les apprenants lors de sessions d'apprentissage pratique pour mettre en œuvre les théories socio-constructivistes qui sont au cœur de l'apprentissage exploratoire, et ainsi favoriser l'engagement et le processus de réflexion des étudiants.
À partir de la littérature traitant des relations sociales entre apprenants, nous identifions dans un premier temps un ensemble de critères pour la conception de systèmes d'apprentissage pratique engageants.
En effet, Lab4CE repose sur les Learning Analytics pour supporter différentes formes d'apprentissage telles que la collaboration, la coopération ou l'entraide entre pairs, mais également pour fournir des outils d'awareness et de réflexion visant à promouvoir l'apprentissage en profondeur pendant et après les activités pratiques.
De plus, ces expérimentations soulignent l'existence d'une corrélation significative entre l'engagement des étudiants dans la plateforme et les stratégies d'apprentissage qu'ils mettent en œuvre d'une part, et leur performance académique d'autre part.
Ces premiers résultats nous permettent d'affirmer que les théories socio-constructivistes sont un levier à l'engagement et à la réflexion dans les VRL.
Ils nous invitent à confronter notre approche à d'autres modalités d'apprentissage, mais aussi à intégrer de nouvelles sources d'informations pour approfondir nos analyses du comportement et ainsi renforcer nos contributions à une meilleure prise en compte de l'apprentissage pratique dans les EIAH.
Les études linguistiques les plus récentes admettent que les noms propres sont souvent traduits, ou adaptés, par opposition aux anciennes théories sur la non-traduisibilité des noms propres.
Ce qui distingue les toponymes du reste de l'onomastique, ce sont les implications politiques, sociologiques et historiques qui concernent une bonne partie des noms géographiques.
Dire « Breslau » pour la ville polonaise « Wrocław » peut avoir une connotation négative selon le contexte.
Et pourtant, nous rencontrons très fréquemment cette forme sur Internet.
De plus, on observe avec la mondialisation la multiplication de nombreuses versions du même toponyme.
Le travail présenté ici est composé de trois parties.
La première partie présente de manière brève les théories concernant les noms propres, mais adaptées aux toponymes, et la deuxième décrit leurs fonctions, ainsi que leur statut linguistique par rapport à la normalisation internationale.
Le concept de la toponymie synchronique-contrastive et les méthodes d'analyse des toponymes selon cette approche sont introduits dans la partie 2.
La partie trois est une analyse du corpus qui a pour le but d'observer les structures et l'intégration des toponymes polonais dans la langue française ainsi que leur usage populaire dans les publications courantes (sur Internet, dans les brochures touristiques, dans les applications et réseaux sociaux, etc.), qui diffère de l'usage officiel, censé être politiquement correct.
Avec l'avènement du « big data » , de nombreuses répercussions ont eu lieu dans tous les domaines de la technologie de l'information, préconisant des solutions innovantes remportant le meilleur compromis entre coûts et précision.
Nous abordons dans cette thèse deux problématiques principales : dans un premier temps, le problème du partitionnement des graphes est abordé d'une perspective « big data » , où les graphes massifs sont partitionnés en streaming.
Nous étudions et proposons plusieurs modèles de partitionnement en streaming et nous évaluons leurs performances autant sur le plan théorique qu'empirique.
Dans un second temps, nous nous intéressons au requêtage des graphes distribués/partitionnés.
Cette thèse étudie la production des nabol, des pratiques narratives de la communauté linguistique nisvaie, située dans le sud-est de Malekula, au Vanuatu.
S'appuyant sur la demande de locuteurs nisvais afin que des ressources langagières soient produites pour l'école locale, un corpus de textes oraux a été constitué pour montrer que les nabol sont produites en fonction de la situation d'énonciation et d'enjeux sociaux locaux liés à la classe d'âge de l'orateur.
Le corpus de textes oraux annotés résulte de séjours de recherche réalisés entre 2011 et 2015,totalisant 14 mois de terrain au sein de la communauté nisvaie.
Les nabol sont étudiées d'une part, à l'aide des concepts issus de la linguistique textuelle afin de décrire les procédés discursifs employés par les locuteurs nisvais.
À partir de ces procédés, il a été possible de comparer l'organisation des nabol et de mettre en évidence des variations significatives en fonction de la situation d'énonciation.
D'autre part, l'observation participante et des entretiens dirigés ont permis d'identifier des enjeux sociaux que les locuteurs nisvais associent à leurs pratiques narratives.
L'emploi des noms propres de personnages ou de lieux lors de la narration répond à un régime de vérité.
En fonction de sa classe d'âge, l'orateur doit nommer ou non les personnages qui prennent part à l'intrigue au risque de se faire critiquer par ses pairs.
Les pratiques et normes issues du programme de documentation des langues à travers le monde et du traitement automatique des langues ont fourni des outils pour élaborer des ressources langagières pertinentes à l'étude des narrations et à son utilisation par la communauté nisvaie.
Deux ressources papier sont jointes en annexe, un lexique bilingue nisvai-français et un recueil bilingue des textes du corpus, conçues pour les locuteurs nisvais et leur école francophone.
De plus, deux ressources accessibles en ligne, une interface de lecture-écoute des textes et une interface de consultation des annotations ont été développées pour communiquer les travaux aux chercheurs travaillant sur des pratiques narratives orales ou les langues du Vanuatu.
D'autre part, nous avons construit notre propre gant de données et nous l'utilisons pour confirmer expérimentalement que la solution de reconnaissance gestuelle permet le contrôle temps réel d'un robot en mobilité.
Enfin, nous montrons la flexibilité de notre technique en ce sens qu'elle permet de contrôler non seulement des robots, mais aussi des systèmes de natures différentes.
L'Internet des objets, ou IdO (en anglais Internet of Things, ou IoT) conduit à un changement de paradigme du secteur de la logistique.
L'avènement de l'IoT a modifié l'écosystème de la gestion des services logistiques.
Les fournisseurs de services logistiques utilisent aujourd'hui des technologies de capteurs telles que le GPS ou la télémétrie pour collecter des données en temps réel pendant la livraison.
La collecte en temps réel des données permet aux fournisseurs de services de suivre et de gérer efficacement leur processus d'expédition.
Le principal avantage de la collecte de données en temps réel est qu'il permet aux fournisseurs de services logistiques d'agir de manière proactive pour éviter des conséquences telles que des retards de livraison dus à des événements imprévus ou inconnus.
Les données provenant de ces sources externes enrichissent l'ensemble de données et apportent une valeur ajoutée à l'analyse.
De plus, leur collecte en temps réel permet d'utiliser les données pour une analyse en temps réel et de prévenir des résultats inattendus (tels que le délai de livraison, par exemple) au moment de l'exécution.
Cependant, les données collectées sont brutes et doivent être traitées pour une analyse efficace.
La collecte et le traitement des données en temps réel constituent un énorme défi.
La raison principale est que les données proviennent de sources hétérogènes avec une vitesse énorme.
La grande vitesse et la variété des données entraînent des défis pour effectuer des opérations de traitement complexes telles que le nettoyage, le filtrage, le traitement de données incorrectes, etc.
La diversité des données - structurées, semi-structurées et non structurées - favorise les défis dans le traitement des données à la fois en mode batch et en temps réel.
Parce que, différentes techniques peuvent nécessiter des opérations sur différents types de données.
Une structure technique permettant de traiter des données hétérogènes est très difficile et n'est pas disponible actuellement.
Par conséquent, pour exploiter le Big Data dans les processus de services logistiques, une solution efficace pour la collecte et le traitement des données en temps réel et en mode batch est essentielle.
Dans cette thèse, nous avons développé et expérimenté deux méthodes pour le traitement des données : SANA et IBRIDIA.
SANA est basée sur un classificateur multinomial Naïve Bayes, tandis qu'IBRIDIA s'appuie sur l'algorithme de classification hiérarchique (CLH) de Johnson, qui est une technologie hybride permettant la collecte et le traitement de données par lots et en temps réel.
SANA est une solution de service qui traite les données non structurées.
Cette méthode sert de système polyvalent pour extraire les événements pertinents, y compris le contexte (tel que le lieu, l'emplacement, l'heure, etc.).
En outre, il peut être utilisé pour effectuer une analyse de texte sur les événements ciblés.
IBRIDIA a été conçu pour traiter des données inconnues provenant de sources externes et les regrouper en temps réel afin d'acquérir une connaissance / compréhension des données permettant d'extraire des événements pouvant entraîner un retard de livraison.
Selon nos expériences, ces deux approches montrent une capacité unique à traiter des données logistiques
Dans le but d'exploiter au mieux les grandes masses de données hétérogènes produites par les instruments scientifiques modernes de l'astrophysique, les scientifiques ont développé le concept d'Observatoire Virtuel (OV).
Il s'agit d'une architecture orientée services, qui a pour objectif de faciliter l'identification et l'interopérabilité des données astrophysiques.
Malgré le développement et les avancées permises par l'OV dans l'exploitation de ces données, certains objectifs sont partiellement atteints notamment l'interopérabilité, la sélection de services et l'identification de services connexes, etc.
Par ailleurs, l'ergonomie des outils à la disposition de l'utilisateur final reste perfectible.
De même l'utilisation actuelle des ressources de l'OV, s'appuyant sur des compétences humaines, gagnerait à être automatisée.
Les services de données astrophysiques n'étant pas tous inscrits dans l'OV, il serait aussi souhaitable pour permettre une utilisation plus large de ces outils, qu'ils s'appuient également sur des services disponibles en-dehors de l'OV.
En vue d'automatiser l'utilisation des ressources en ligne, les sciences de l'information travaillent depuis 2001 à l'élaboration du Web sémantique.
Cette évolution apporte au Web des capacités de raisonnement automatiques, basées sur des algorithmes utilisant une nouvelle forme de description des contenus.
Cette nouvelle forme de description sémantique se trouve exprimée dans des représentations informatiques appelées ontologies.
Malheureusement, les méthodes actuelles d'élaboration du Web sémantique ne sont pas complètement compatibles avec les services OV qui utilisent des modèles de données, des formats et des protocoles d'accès aux services qui s'éloignent de ceux rencontrés habituellement dans les sciences de l'information.
Dans ce contexte, cette thèse décrit une méthodologie générique de composition de services sans état, basée sur la description des services par une ontologie dont la définition est proposée dans ce document.
Cette ontologie représente aussi bien des services Web que des services non accessibles par le Web.
Elle prend en compte certaines spécificités qui peuvent être rencontrées dans les infrastructures de services préexistantes.
La population de cette ontologie, par des services éventuellement éloignés des standards utilisés habituellement dans les sciences de l'information, est aussi traitée.
La méthodologie a été appliquée avec succès dans le cadre de l'astrophysique, et a permis de développer une application Web permettant la composition automatique de services utilisable par un public non averti.
Les outils de traduction assistée par ordinateur et de gestion terminologique sont le plus souvent utilisés pour répondre au besoin de la gestion de l'écrit multilingue et monolingue.
En effet, ils facilitent l'accès aux termes techniques et aux expressions liés à des domaines de spécialité, et indispensables à tout processus de communication.
La compréhension de ces expressions techniques peut être potentialisée au moyen de leur « contextualisation » .
Néanmoins, avoir accès à un terme ou à sa traduction ne suffit pas, encore faut-il être capable de l'employer correctement et d'en appréhender le sens exact.
Cette contextualisation a donc lieu à deux niveaux : dans les textes et dans la terminologie.
Au niveau textuel, l'utilisateur doit avoir accès à des informations concernant l'usage des termes, à savoir des contextes riches en connaissances linguistiques.
Au niveau terminologique, il doit avoir accès aux relations sémantiques ou conceptuelles entre termes afin de mieux en saisir le sens, à savoir des contextes riches en connaissances conceptuelles.
Nous avons poursuivi nos travaux dans un cadre bilingue et plus particulièrement en phase de révision du processus de traduction spécialisée.
Nous avons proposé une méthodologie d'élaboration d'un concordancier bilingue fournissant des CRC alignés à partir de corpus comparables spécialisés.
Les évaluations menées montrent que les CRC proposés sont utiles malgré la difficulté de l'exercice étudié.
Les réseaux sociaux numériques ont pris une place prépondérante dans l'espace informationnel, et sont souvent utilisés pour la publicité, le suivi de réputation, la propagande et même la manipulation, que ce soit par des individus, des entreprises ou des états.
Alors que la quantité d'information rend difficile son exploitation par des humains, le besoin reste entier d'analyser un réseau social numérique : il faut dégager des tendances à partir des messages postés dont notamment les opinions échangées, qualifier les comportements des utilisateurs, et identifier les structures sociales émergentes.
Pour résoudre ce problème, nous proposons un système d'analyse en trois niveaux.
Tout d'abord, l'analyse du message vise à en déterminer l'opinion.
Nous appliquons ce système d'analyse sur deux corpus provenant de deux médias sociaux différents : le premier est constitué de messages publiés sur Twitter, représentant toutes les activités réalisées par 5 000 comptes liés entre eux sur une longue période.
Le second provient d'un réseau social basé sur TOR, nommé Galaxy2.
Nous évaluons la pertinence de notre système sur ces deux jeux de données, montrant la complémentarité des outils de caractérisation des comptes utilisateurs (influence, comportement, rôle) et des communautés de comptes (force d'interaction, cohésion thématique), qui enrichissent l'exploitation du graphe social par les éléments issus des contenus textuels échangés.
La méthodologie est fondée sur trois piliers : la définition, la recherche / analyse et l'innovation.
La définition exhaustive de la fonction principale du système industriel cible le champ de recherche et permet la récupération de mots clés initiaux grâce à une analyse approfondie de l'existant.
La recherche itérative des brevets se base sur la décomposition fonctionnelle et sur l'analyse physique.
L'analyse intègre la décomposition fonctionnelle énergétique pour déceler les énergies, les flux fonctionnels transmis et les phénomènes physiques impliqués dans le processus de conversion énergétique afin de sélectionner des effets physiques potentiellement pertinents.
Pour délimiter le champ d'exploration nous formulons des requêtes de recherche à partir d'une base de données de mots clés constituée par des mots clés initiaux, des mots clés physiques et des mots clés technologiques.
Une matrice des découvertes basée sur les croisements entre ces mots clés permet le classement des brevets pertinents.
La recherche des opportunités d'innovation exploite la matrice des découvertes pour déceler les tendances évolutives suivies par les inventions.
Les opportunités sont déduites à partir de l'analyse des cellules non pourvues de la matrice des découvertes, de l'analyse par tendances d'évolution et du changement de concept par la substitution du convertisseur énergétique.
Nous proposons des tendances d'évolution construites à partir de lois d'évolution de la théorie TRIZ, d'heuristiques de conception et de règles de l'art de l'ingénieur.
Un cas d'application concernant l'étude d'évolution et la proposition de nouveaux systèmes de séparation de mélanges bi-phasiques en offshore profond met en valeur la méthode.
L'ouvrage unifie les paradigmes du darwinisme universel, de la psycholinguistique développementale et de la linguistique computationnelle afin de fournir un récit nouveau d'ontogénie de structures linguistiques chez les agents humains ou bien artificiels.
La thèse est précédée d'un volume supplémentaire intitulé « Fondements conceptuels » qui présente une théorie de l'évolution intra-mentale qui pose l'hypothèse que l'ontogénie d'un esprit individuel peut être interprétée et même simulée comme un processus impliquant la réplication, la variation et la sélection des structures qui encodent de l'information.
La dissertation elle-même présente quatre simulations distinctes abordant quatre problèmes distincts.
La 0ème simulation illustre comment l'optimisation évolutionnist pourrait conduire à la découverte d'idées utiles concernant l'énigme cryptologique connue sous le nom de Voynich Manuscript.
La première simulation montre comment la théorie des prototypes, les architectures symboliques vectorielles et l'optimisation évolutionniste peuvent être mutuellement combinées afin de produire une nouvelle méthode d'apprentissage automatique supervisée.
La deuxième simulation utilise une approche similaire pour démontrer que l'optimisation évolutive peut découvrir des constellations minimalistes et légères de marqueurs de partie de discours.
La dernière simulation vise le « saint graal » de la linguistique computationnelle, c'est-à-dire le problème de « l'induction grammaticale » et montre que le problème peut être potentiellement résolu en utilisant une stratégie évolutive capable de combler l'écart entre le domaine subsymbolique et le domaine symbolique.
L'internet et les nouvelles formes de média de communication, d'information, et de divertissement ont entraîné une croissance massive de la quantité des données numériques.
Le traitement et l'interprétation automatique de ces données permettent de créer des bases de connaissances, de rendre les recherches plus efficaces et d'effectuer des recherches sur les médias sociaux.
Les travaux de recherche sur le traitement automatique du langage naturel concernent la conception et le développement d'algorithmes, qui permettent aux ordinateurs de traiter automatiquement le langage naturel dans les textes, les contenus audio, les images ou les vidéos, pour des tâches spécifiques.
De par la complexité du langage humain, le traitement du langage naturel sous forme textuelle peut être divisé en 4 niveaux : la morphologie, la syntaxe, la sémantique et la pragmatique.
Les technologies actuelles du traitement du langage naturel ont eu de grands succès sur les tâches liées auxdeux premiers niveaux, ce qui a permis la commercialisation de beaucoup d'applications comme les moteurs de recherche.
Cependant, les moteurs de recherches avancés (structurels) nécessitent une interprétation du langage plus avancée.
L'extraction d'information consiste à extraire des informations structurelles à partir des ressources non annotées ou semi-annotées, afin de permettre des recherches avancées et la création automatique des bases de connaissances.
Cette thèse étudie le problème d'extraction d'information dans le domaine spécifique de l'extraction des événements biomédicaux.
Nous proposons une solution efficace, qui fait un compromis entre deux types principaux de méthodes proposées dans la littérature.
Cette solution arrive à un bon équilibre entre la performance et la rapidité, ce qui la rend utilisable pour traiter des données à grande échelle.
Elle a des performances compétitives face aux meilleurs modèles existant avec une complexité en temps de calcul beaucoup plus faible.
Lors la conception de ce modèle, nous étudions également les effets des différents classifieurs qui sont souvent proposés pour la résolution des problèmes de classification multi-classe.
Nous testons également deux méthodes permettant d'intégrer des représentations vectorielles des mots appris par apprentissage profond (deep learning).
Même si les classifieurs différents et l'intégration des vecteurs de mots n'améliorent pas grandement la performance, nous pensons que ces directions de recherche ont du potentiel et sont prometteuses pour améliorer l'extraction d'information.
Elles servent de cadre au développement de deux outils qui illustrent la mise en œuvre de ces contraintes, associées à l'adaptation de systèmes de production artistique automatique, notamment basés sur des algorithmes de colonie de fourmis artificielles.
Le premier programme est un instrument de musique virtuel permettant au plus grand nombre de jouer de la musique, et fournissant un accompagnement automatique.
Le second est un atelier de dessin où des outils basés sur des méthodes génératives offrent un résultat complexe à partir d'actions très simples.
Cette thèse détaille le développement de ces deux programmes ainsi que leur évaluation, sur le terrain, à la rencontre d'utilisateurs réels.
Cette thèse vise à étudier le lien entre institutions, genre et politique.
Elle cherche à répondre à trois questions : les institutions peuvent-elles défaire les normes de genre ?
Le premier chapitre de cette thèse vise à étudier le rôle des institutions dans la création des normes de genre.
La norme étudiée est celle selon laquelle une femme doit gagner moins que son mari.
En utilisant, la division de l'Allemagne comme une expérience naturelle, nous montrons que les institutions égalitaires est-allemandes ont défait le genre.
Après la réunification, une femme est-allemande peut gagner plus que son mari sans augmenter son nombre d'heures de travail domestique, risquer de divorcer ou de se retirer du marché du travail.
A l'opposé, en Allemagne de l'Ouest, ces comportements sont toujours observables.
Le deuxième chapitre étudie si les institutions seraient plus égalitaires avec des femmes à leur tête.
En particulier, nous cherchons à déterminer si les femmes politiciennes ont les mêmes priorités que leurs collègues masculins.
Le contexte étudié est celui du Parlement Français durant la période 2001-2017.
En combinant des méthodes d'analyse de texte avec des variations exogènes dans le sexe des politiciens, ce chapitre montre que, relativement à leurs collègues masculins, les femmes politiciennes à l'Assemblée Nationale défendent plus les intérêts des femmes dans la population.
Le thème où les différences sexuées d'activité parlementaire sont les plus marquées est précisément celui de l'égalité femmes-hommes, suivi des thématiques liées à l'enfance et à la santé.
Nous montrons que ces différences proviennent de l'intérêt individuel des législateurs.
Enfin, nous répliquons ces résultats au Sénat en exploitant l'introduction d'une réforme qui a imposé la parité.
Le troisième chapitre s'intéresse aux raisons derrière la sous-représentation des femmes dans les positions de pouvoir.
Il cherche à déterminer si dans un contexte où les politiciens sont majoritairement des hommes, la "prime aux sortants" lors d'élections réduit le nombre de femmes élues.
Nous montrons que contrairement à ce qu'on peut s'attendre, lorsque les politiciens ne sont pas éligibles à leur réélection, la part de femmes élus n'augmente pas.
C'est parce qu'il est plus difficile pour une femme de remplacer une femme que de remplacer un homme.
Les systèmes digitaux jouent un rôle croissant dans le bon fonctionnement de notre société.
Au delà de la grande diversité de leur domaines d'utilisations, on confie aujourd'hui des tâches importantes à des algorithmes.
Déjà largement utilisés dans des domaines aussi délicat que le transport, la chirurgie ou l'économie, il est aujourd'hui de plus en plus question de faire de la place aux systèmes digitaux dans les domaines sociaux et politiques : vote électronique, algorithmes de sélection, profilage électoral dots.
Pour les tâches confiées à des algorithmes, la responsabilité est déplacées de l'exécutant vers les concepteurs, développeurs et testeurs de ces algorithmes.
Il incombe aussi aux chercheurs qui étudient ces algorithmes de proposer des techniques de vérifications fiable qui pourront être utilisées à tous les niveaux : conception, développement et test.
Les méthodes de vérifications formelles donnent des outils mathématiques pour prévenir des erreurs à chaque niveaux.
Parmi elle, le diagnostic d'erreur consiste en la création d'un diagnostiqueur basé sur un modèle formel du système à vérifier.
Le diagnostiqueur est exécuté en parallèle du système qu'il doit surveiller et prévient un contrôleur si il détecte un comportement dangereux du système.
Pour les systèmes modélisés par des automates temporisés, il n'est pas toujours possible de construire un diagnostiqueur sous la forme d'un autre automate temporisé.
En effet les automates temporisés, introduits par cite{AD94} dans les années 90 et largement étudiés et utilisés depuis pour modéliser des systèmes avec contraintes temporelles,ne sont pas déterminisable.
Une machine plus puissante qu'un automate temporisé peut cependant être utilisée pour construire le diagnostiqueur d'un automate temporisé comme le montre cite{Tripakis02}.
L'aboutissement de ce travail de thèse est la construction automatique d'un diagnostiqueur pour les automates temporisés à une horloge.
Ce diagnostiqueur, dans le même esprit que celui de cite{Tripakis02}, est une machine plus puissante qu'un automate temporisé.
La partie~I du manuscrit introduit un cadre formel pour ce type de machine et plus généralement pour la modélisation et la déterminisation de systèmes quantitatifs.
Y est introduit le modèle des automates sur structures temporisés, qui apporte un nouveau point de vue sur la manière de modéliser les systèmes avec variables quantitatives.
La partie~II étudie le problème de la déterminisation des automates sur structures temporises, et plus spécifiquement celui des automates temporisés qui peuvent se traduire dans ce cadre nouveau cadre formel.
La partie~III montre comment utiliser les automates sur structure temporisés pour construire de manière générique un diagnostiqueur pour les automate temporisés à une horloge.
Cette technique est implémentée dans un outils, DOTA, et comparée à la machine construite par cite{Tripakis02}.
Le "Big Data''et les grands systèmes d'apprentissage sont omniprésents dans les problèmes d'apprentissage automatique aujourd'hui.
Contrairement à l'apprentissage de petite dimension, les algorithmes d'apprentissage en grande dimension sont sujets à divers phénomènes contre-intuitifs et se comportent de manière très différente des intuitions de petite dimension sur lesquelles ils sont construits.
Cependant, en supposant que la dimension et le nombre des données sont à la fois grands et comparables, la théorie des matrices aléatoires (RMT) fournit une approche systématique pour évaluer le comportement statistique de ces grands systèmes d'apprentissage, lorsqu'ils sont appliqués à des données de grande dimension.
L'objectif principal de cette thèse est de proposer un schéma d'analyse basé sur la RMT, pour une grande famille de systèmes d'apprentissage automatique : d'évaluer leurs performances, de mieux les comprendre et finalement les améliorer, afin de mieux gérer les problèmes de grandes dimensions aujourd'hui.
Précisément, nous commençons par exploiter la connexion entre les grandes matrices à noyau, les projection aléatoires non-linéaires et les réseaux de neurones aléatoires simples.
En considérant que les données sont tirées indépendamment d'un modèle de mélange gaussien, nous fournissons une caractérisation précise des performances de ces systèmes d'apprentissage en grande dimension, exprimée en fonction des statistiques de données, de la dimensionnalité et, surtout, des hyper-paramètres du problème.
Lorsque des algorithmes d'apprentissage plus complexes sont considérés, ce schéma d'analyse peut être étendu pour accéder à de systèmes d'apprentissage qui sont définis (implicitement) par des problèmes d'optimisation convexes, lorsque des points optimaux sont atteints.
Pour trouver ces points, des méthodes d'optimisation telles que la descente de gradient sont régulièrement utilisées.
À cet égard, dans le but d'avoir une meilleur compréhension théorique des mécanismes internes de ces méthodes d'optimisation et, en particulier, leur impact sur le modèle d'apprentissage, nous évaluons aussi la dynamique de descente de gradient dans les problèmes d'optimisation convexes et non convexes.
Ces études préliminaires fournissent une première compréhension quantitative des algorithmes d'apprentissage pour le traitement de données en grandes dimensions, ce qui permet de proposer de meilleurs critères de conception pour les grands systèmes d'apprentissage et, par conséquent, d'avoir un gain de performance remarquable lorsqu'il est appliqué à des jeux de données réels.
Les émotions et leurs expressions par des agents virtuels sont deux enjeux importants pour les interfaces homme-machine affectives à venir.
En effet, les évolutions récentes des travaux en psychologie des émotions, ainsi que la progression des techniques de l'informatique graphique, permettent aujourd'hui d'animer des personnages virtuels réalistes et capables d'exprimer leurs émotions via de nombreuses modalités.
Si plusieurs systèmes d'agents virtuels existent, ils restent encore limités par la diversité des modèles d'émotions utilisés, par leur niveau de réalisme, et par leurs capacités d'interaction temps réel.
Dans nos recherches, nous nous intéressons aux agents virtuels capables d'exprimer des émotions via leurs expressions faciales en situation d'interaction avec l'utilisateur.
Nos travaux posent de nombreuses questions scientifiques et ouvrent sur les problématiques suivantes : Comment modéliser les émotions en informatique en se basant sur les différentes approches des émotions en psychologie ?
Quel niveau de réalisme visuel de l'agent est nécessaire pour permettre une bonne expressivité émotionnelle ?
Comment permettre l'interaction temps réel avec un agent virtuel ?
Comment évaluer l'impact des émotions exprimées par l'agent virtuel sur l'utilisateur ?
A partir de ces problématiques, nous avons axé nos travaux sur la modélisation informatique des émotions et sur leurs expressions faciales par un personnage virtuel réaliste.
En effet, les expressions faciales sont une modalité privilégiée de la communication émotionnelle.
Notre objectif principal est de contribuer l'amélioration de l'interaction entre l'utilisateur et un agent virtuel expressif.
Nos études ont donc pour objectif de mettre en lumière les avantages et les inconvénients des différentes approches des émotions ainsi que des méthodes graphiques étudiées.
Nous avons travaillé selon deux axes de recherches complémentaires.
D'une part, nous avons exploré différentes approches des émotions (catégorielle, dimensionnelle, cognitive, et sociale).
Pour chacune de ces approches, nous proposons un modèle informatique et une méthode d'animation faciale temps réel associée.
Notre second axe de recherche porte sur l'apport du réalisme visuel et du niveau de détail graphique à l'expressivité de l'agent.
Cet axe est complémentaire au premier, car un plus grand niveau de détail visuel pourrait permettre de mieux refléter la complexité du modèle émotionnel informatique utilisé.
Les travaux que nous avons effectués selon ces deux axes ont été évalués par des études perceptives menées sur des utilisateurs.
La combinaison de ces deux axes de recherche est rare dans les systèmes d'agents virtuels expressifs existants.
L'ensemble des logiciels que nous avons conçus forme notre plateforme d'agents virtuels MARC (Multimodal Affective and
MARC a été utilisée dans des applications de natures diverses : jeu, intelligence ambiante, réalité virtuelle, applications thérapeutiques, performances artistiques,
La neuroimagerie permet d'étudier les liens entre la structure et le fonctionnement du cerveau.
Des milliers d'études de neuroimagerie sont publiées chaque année.
Il est difficile d'exploiter cette grande quantité de résultats.
En effet, chaque étude manque de puissance statistique et peut reporter beaucoup de faux positifs.
De plus, certains effets sont spécifiques à un protocole expérimental et difficile à reproduire.
Les méta-analyses rassemblent plusieurs études pour identifier les associations entre structures anatomiques et processus cognitifs qui sont établies de manière consistante dans la littérature.
Les méthodes classiques de méta-analyse commencent par constituer un échantillon d'études focalisées sur un même processus mental ou une même maladie.
Ensuite, un test statistique permet de délimiter les régions cérébrales dans lesquelles le nombre d'observations reportées est significatif.
Dans cette thèse, nous introduisons une nouvelle forme de méta-analyse, qui s'attache à construire des prédictions plutôt qu'à tester des hypothèses.
Nous introduisons des modèles statistiques qui prédisent la distribution spatiale des observations neurologiques à partir de la description textuelle d'une expérience, d'un processus cognitif ou d'une maladie cérébrale.
Notre approche est basée sur l'apprentissage statistique supervisé qui fournit un cadre classique pour évaluer et comparer les modèles.
Nous construisons le plus grand jeu de données d'études de neuroimagerie et de coordonnées stéréotaxiques existant, qui rassemble plus de 13 000 publications.
Cette thèse introduit des méthodes adaptées à la méta-analyse prédictive.
Cette approche est complémentaire de la méta-analyse standard, et aide à interpréter les résultats d'études de neuroimagerie ainsi qu'à formuler des hypothèses ou des a priori statistiques.
Face au volume croissant de données audio et multimédia, les technologies liées à l'indexation de données et à l'analyse de contenu ont suscité beaucoup d'intérêt dans la communauté scientifique.
D'importants progrès ont été réalisés dans le domaine ces dernières années principalement menés par les évaluations internationales du NIST.
En effet, dans un premier temps, nous montrons qu'après avoir introduit une nouvelle composante de purification des clusters dans l'approche top-down, nous obtenons des performances comparables à celles de l'approche bottom-up.
De plus, en étudiant en détails les deux types d'approches nous montrons que celles-ci se comportent différemment face à la discrimination des locuteurs et la robustesse face à la composante lexicale.
Ces différences sont alors exploitées au travers d'un nouveau système combinant les deux approches.
Enfin, nous présentons une nouvelle technologie capable de limiter l'influence de la composante lexicale, source potentielle d'artefacts dans le regroupement et la segmentation en locuteurs.
Avec le développement du commerce électronique, les clients ont publié de nombreux commentaires de produit sur Internet.
Ces données sont précieuses pour les concepteurs de produit, car les informations concernant les besoins de client sont identifiables.
L'objectif de cette étude est de développer une approche d'analyse automatique des commentaires utilisateurs permettant d'obtenir des informations utiles au concepteur pour guider l'amélioration et l'innovation des produits.
L'approche proposée contient deux étapes : structuration des données et analyse des données.
Dans la structuration des données, l'auteur propose d'abord une ontologie pour organiser les mots et les expressions concernant les besoins de client décrient dans les commentaires.
Ensuite, une méthode de traitement du langage naturelle basée des règles linguistiques est proposé pour structurer automatiquement les textes de commentaires dans l'ontologie proposée.
Dans l'analyse des données, deux méthodes sont proposées pour obtenir des idées d'innovation et des visions sur le changement de préférence d'utilisateur avec le temps.
Dans ces deux méthodes, les modèles et les méthodes traditionnelles comme affordance base design, l'analyse conjointe, et le Kano model sont étudié et appliqué d'une façon innovante.
Pour évaluer la praticabilité de l'approche proposée dans la réalité, les commentaires de client de liseuse numérique Kindle sont analysés. Des pistes d'innovation et des stratégies pour améliorer le produit sont identifiés et construites.
L'objectif principal de l'ingénierie des systèmes est la création d'un ensemble de produits et des services de haute qualité qui permettent l'accomplissement de tâches pour répondre aux besoins des clients.
Un projet typique d'ingénierie des systèmes peut être divisé en trois phases : la définition, le développement et le déploiement.
La phase de définition comprend les activités de capture des exigences et de leur raffinement.
À la fin de la phase de définition du système, nous avons toutes les exigences fonctionnelles et non-fonctionnelles du système.
L'un des résultats de la phase de développement est le modèle de travail initiale du système.
La phase de déploiement se compose des activités liées à (1) l'évaluation opérationnelle du système, à (2) l'utilisation du système et à (3) son entretien.
Dans un cycle de vie du projet, il y a de nombreuses questions qui doivent être traitées au cours des différentes phases pour finalement livrer un produit.
Nous avons proposé une solution aux problèmes liés à l'ingénierie des exigences et aux techniques de la détection, de la gestion et de la résolution des conflits entre les parties prenantes.
Cette thèse est basée sur les dernières avancées dans les pratiques industrielles et de recherche dans le domaine de l'ingénierie de conception du système.
L'objectif de ce travail de thèse est de proposer une méthodologie de conception novatrice et globale en tenant compte de l'environnement multidisciplinaire et de multiples intervenants.
Nous avons proposé un langage de modélisation des exigences basé sur les techniques GORE.
Nous avons proposé quelques outils pour réduire l'ambiguïté des exigences tels l'utilisation de phrases négatives et de tests à l'aide de négation lorsqu'il s'agit de traiter certaines exigences difficiles à comprendre avec les techniques classiques.
Nous avons également proposé des techniques de gestion des exigences pour mieux assurer leur traçabilité.
En utilisant la même technique de pondération de critères, une méthode de décision multicritères et multi participants est proposée pour divers problèmes de décision survenant pendant le cycle de vie du projet d'ingénierie systèmes.
Enfin, une approche globale de l'ingénierie des systèmes est proposée pour intégrer toutes les contributions faites précédemment et est illustrée sur une étude de cas concernant un projet réel avec la présentation d'un outil SysEngLab que nous avons développé pour mettre en oeuvre la majorité des méthodes et des techniques proposées au cours de thèse.
Dans le cadre de la rééducation orthophonique des troubles de la parole associés à un mauvais positionnement de la langue, il peut être utile au patient et à l'orthophoniste de visualiser la position et les mouvements de cet articulateur naturellement très peu visible.
L'imagerie échographique peut pallier ce manque, comme en témoignent de nombreuses études de cas menées depuis plusieurs années dans les pays anglo-saxons.
Appuyés par de nombreux travaux sur les liens entre production et perception de la parole, ces études font l'hypothèse que ce retour articulatoire visuel faciliterait la rééducation du patient.
Lors des séances orthophoniques, le patient semble, en effet, mieux appréhender les déplacements de sa langue, malgré la difficulté d'interprétation sous-jacente de l'image échographique liée au bruit inhérent à l'image et à l'absence de vision des autres articulateurs.
Nous développons dans cette thèse le concept d'échographie linguale augmentée.
Nous proposons deux approches afin d'améliorer l'image échographique brute, et présentons une première application clinique de ce dispositif.
La première approche porte sur le suivi du contour de la langue sur des images échographiques.
Nous proposons une méthode basée sur une modélisation par apprentissage supervisé des relations entre l'intensité de l'ensemble des pixels de l'image et les coordonnées du contour de langue.
Une étape de réduction de la dimension des images et des contours par analyse en composantes principales est suivie d'une étape de modélisation par réseaux de neurones.
Nous déclinons des implémentations mono-locuteur et multi-locuteur de cette approche dont les performances sont évaluées en fonction de la quantité de contours manuellement annotés (données d'apprentissage).
Nous obtenons pour des modèles mono-locuteur une erreur de 1,29 mm avec seulement 80 images, performance meilleure que celle de la méthode de référence EdgeTrak utilisant les contours actifs.
Nous construisons tout d'abord un modèle d'association entre les images échographiques et les paramètres de contrôle de la langue acquis sur ce locuteur de référence.
Nous adaptons ensuite ce modèle à de nouveaux locuteurs dits locuteurs source.
Nous comparons cette approche avec une régression directe par GMR entre données du locuteur source et paramètre de contrôle de la tête parlante.
Nous montrons que l'approche par C-GMR réalise le meilleur compromis entre quantité de données d'adaptation d'une part, et qualité de la prédiction d'autre part.
Enfin, nous évaluons la capacité de généralisation de l'approche C-GMR et montrons que l'information a priori sur le locuteur de référence exploitée par ce modèle permet de généraliser à des configurations articulatoires du locuteur source non vues pendant la phase d'adaptation.
Enfin, nous présentons les premiers résultats d'une application clinique de l'échographie augmentée à une population de patients ayant subi une ablation du plancher de la bouche ou d'une partie de la langue.
Les premiers résultats montrent une amélioration des performances des patients, notamment sur le placement de la langue.
Avec la prolifération rapide des plateformes de données qui récoltent des données relatives à plusieurs domaines tels que les données de gouvernements, d'éducation, d'environnement ou les données de notations de produits, plus de données sont disponibles en ligne.
Ceci représente une opportunité sans égal pour étudier le comportement des individus et les interactions entre eux.
Sur le plan politique, le fait de pouvoir interroger des ensembles de données de votes peut fournir des informations intéressantes pour les journalistes et les analystes politiques.
En particulier, ce type de données peut être exploité pour l'investigation des sujet exceptionnellement conflictuels ou consensuels.
Considérons des données décrivant les sessions de votes dans le parlement Européen (PE).
Ces données offrent la possibilité d'étudier les accords et désaccords de sous-groupes cohérents, en particulier pour mettre en évidence des comportements inattendus.
Par exemple, il est attendu que sur la majorité des sessions, les députés votent selon la ligne politique de leurs partis politiques respectifs.
Cependant, lorsque les sujets sont plutôt d'intérêt d'un pays particulier dans l'Europe, des coalitions peuvent se former ou se dissoudre.
À titre d'exemple, quand une procédure législative concernant la pêche est proposée devant les MPE dans l'hémicycle, les MPE des nations insulaires du Royaume-Uni peuvent voter en accord sans être influencés par la différence entre les lignes politiques de leurs alliances respectives, cela peut suggérer un accord exceptionnel comparé à la polarisation observée habituellement.
Dans cette thèse, nous nous intéressons à ce type de motifs décrivant des (dés)accords exceptionnels, pas uniquement sur les données de votes mais également sur des données similaires appelées données comportementales.
La première permet la découverte de (dés)accords exceptionnels entre groupes tandis que la seconde permet de mettre en évidence les comportements exceptionnels qui peuvent au sein d'un même groupe.
Dans l'esprit d'évaluer la capacité des deux méthodes à réaliser cet objectif, nous évaluons les performances quantitatives et qualitatives sur plusieurs jeux de données réelles.
De plus, nous motivons l'utilisation des méthodes proposées dans le contexte du journalisme computationnel.
La dématérialisation des données de santé a permis depuis plusieurs années de constituer un véritable gisement de données provenant de tous les domaines de la santé.
Ces données ont pour caractéristiques d'être très hétérogènes et d'être produites à différentes échelles et dans différents domaines.
Leur réutilisation dans le cadre de la recherche clinique, de la santé publique ou encore de la prise en charge des patients implique de développer des approches adaptées reposant sur les méthodes issues de la science des données.
L'objectif de cette thèse est d'évaluer au travers de trois cas d'usage, quels sont les enjeux actuels ainsi que la place des data sciences pour l'exploitation des données massives en santé.
La démarche utilisée pour répondre à cet objectif consiste dans une première partie à exposer les caractéristiques des données massives en santé et les aspects techniques liés à leur réutilisation.
La seconde partie expose les aspects organisationnels permettant l'exploitation et le partage des données massives en santé.
La troisième partie décrit les grandes approches méthodologiques en science des données appliquées actuellement au domaine de la santé.
Enfin, la quatrième partie illustre au travers de trois exemples l'apport de ces méthodes dans les champs suivant : la surveillance syndromique, la pharmacovigilance et la recherche clinique.
Nous discutons enfin les limites et enjeux de la science des données dans le cadre de la réutilisation des données massives en santé.
Le Web sémantique propose un ensemble de standards et d'outils pour la formalisation et l'interopérabilité de connaissances partagées sur le Web, sous la forme d'ontologies.
Les travaux de cette thèse concernent dans un premier temps une méthode fondée sur les structures de patrons, une extension de l'analyse formelle de concepts pour la découverte de co-occurences de événements indésirables médicamenteux dans des données patients.
Cette méthode utilise une ontologie de phénotypes et une ontologie de médicaments pour permettre la comparaison de ces événements complexes, et la découverte d'associations à différents niveaux de généralisation, par exemple, au niveau de médicaments ou de classes de médicaments.
Dans un second temps, on utilisera une méthode numérique fondée sur des mesures de similarité sémantique pour la classification de déficiences intellectuelles génétiques.
On étudiera deux mesures de similarité utilisant des méthodes de calcul différentes, que l'on utilisera avec différentes combinaisons d'ontologies phénotypiques et géniques.
En particulier, on quantifiera l'influence que les différentes connaissances de domaine ont sur la capacité de classification de ces mesures, et comment ces connaissances peuvent coopérer au sein de telles méthodes numériques.
Une troisième étude utilise les données ouvertes liées ou LOD du Web sémantique et les ontologies associées dans le but de caractériser des gènes responsables de déficiences intellectuelles.
L'ensemble des contributions de cette thèse montre qu'il est possible de faire coopérer avantageusement une ou plusieurs ontologies dans divers processus de fouille de données
Cette thèse porte sur la co-construction de la signification en interaction et les manifestations des processus interprétatifs des participants.
En s'intéressant au processus d'explicitation, c'est-à-dire le processus par lequel un contenu informationnel devient explicite dans la conversation, elle propose une étude pluridimensionnelle de la séquence conversationnelle en jeu dans ce processus.
La co-construction de la signification est ici abordée comme relevant d'une transformation informationnelle et de l'inférence.
Nos analyses ont porté sur un corpus de français parlé en interaction, en contexte de repas et apéritifs entre amis.
Ainsi, nous proposons de parcourir cettepratique selon trois axes d'analyse : (a) une analyse séquentielle, s'intéressant au déploiement de la séquence d'explicitation et des éléments la composant, (b) une analyse reposant sur une modélisation de la gestion informationnelle dans cetteséquence, et (c) une analyse des formats linguistiques employés pour l'exhibition du processus inférentiel.
Un des enjeux de ce travail l'élaboration d'un modèle conversationnaliste pour la gestion informationnelle et son application à l'analyse desdonnées de langue parlée en interaction.
Notre recherche s'inscrit à la fois dans le domaine des Recherches sur l'acquisition des langues et celui de la Didactique des langues étrangères, et poursuit un double objectif.
D'abord, nous avons décrit les influences translinguistiques, issues de l'espagnol L1 comme de l'anglais L2, dans le processus d'acquisition de certaines structures liées au groupe verbal en français L3, à savoir la construction des verbes avec des compléments nominaux, la sélection des prépositions introduisant éventuellement ces compléments et l'acquisition des pronoms compléments.
Pour cela, nous avons pris en compte les apports théoriques des recherches sur l'acquisition plurilingue mais également des études ayant dégagé des itinéraires de développement pour le français.
Nous nous sommes basée sur des productions écrites réalisées par des étudiants, débutants en français, de première année de la filière de Traduction/Interprétation de la Universidad de Concepción au Chili.
Le deuxième objectif de notre recherche correspond à une démarche de transposition didactique de résultats empiriques dans le but de mieux accompagner l'apprentissage du français dans notre contexte de formation.
Nos propositions didactiques concernent aussi bien la sélection et le séquençage des contenus grammaticaux, la démarche méthodologique d'enseignement de la grammaire, et la correction/évaluation de la compétence grammaticale des apprenants.
In fine, notre objectif didactique est de contribuer à la formation d'apprenants plurilingues conscients des processus à l'œuvre dans leur acquisition, et de les accompagner dans un usage réflexif de leurs connaissances linguistiques, notamment grammaticales.
L'intelligence artificielle (IA), avec les récents progrès de l'apprentissage statistique (ML), vise actuellement à révolutionner la manière dont la science expérimentale est conduite.
En physique, chimie, biologie, neurosciences ou médecine, les données sont désormais le moteur de nouvelles théories et de nouvelles hypothèses scientifiques.
L'apprentissage supervisé et les modèles prédictifs sont désormais utilisés pour évaluer si quelque chose est 'prévisible' :
La ML est maintenant utilisée en remplacement des tests statistiques d'hypothèses classiques.
Dans le domaine des soins de santé, on parle de médecine de précision, de patients virtuels dont la vision est telle que l'intelligence artificielle permettra d'avoir des prédictions individualisées à partir de données génomiques, physiologiques ou d'imagerie.
Après des percées pionnières dans le domaine de la vision par ordinateur, du traitement de la parole ou du langage naturel, la ML doit maintenant relever de nouveaux défis afin d'avoir un impact sur diverses disciplines scientifiques et en particulier sur les applications liées à la santé.
Lors de l'examen des applications médicales, des problèmes statistiques et informatiques apparaissent.
Le problème particulier étudié dans ce projet est lié à l'absence ou à la limitation de la supervision des algorithmes : les modèles prédictifs supervisés ont besoin de ce que l'on appelle des annotations ou des étiquettes pour être estimés et testés, et malheureusement trop peu d'applications médicales peuvent en fournir suffisamment.
L'approche envisagée sera basée sur l'apprentissage auto-supervisé et l'ICA non linéaire.
La multiplication de sources textuelles sur le Web offre un champ pour l'extraction de connaissances depuis des textes et à la création de bases de connaissances.
Dernièrement, de nombreux travaux dans ce domaine sont apparus ou se sont intensifiés.
De ce fait, il est nécessaire de faire collaborer des approches linguistiques, pour extraire certains concepts relatifs aux entités nommées, aspects temporels et spatiaux, à des méthodes issues des traitements sémantiques afin de faire ressortir la pertinence et la précision de l'information véhiculée.
Cependant, les imperfections liées au langage naturel doivent être gérées de manière efficace.
Enfin, pour présenter un intérêt à l'échelle du Web, les traitements linguistiques doivent être multisources et interlingue.
Cette thèse s'inscrit dans la globalité de cette problématique, c'est-à-dire que nos contributions couvrent aussi bien les aspects extraction et représentation de connaissances incertaines que la visualisation des graphes générés et leur interrogation.
L'analyse bioinformatique des données de transcriptomique a pour but d'identifier les gènes qui présentent des variations d'expression entre différentes situations, par exemple entre des échantillons de tissu sain et de tissu malade et de caractériser ces gènes à partir de leurs annotations fonctionnelles.
Dans ce travail de thèse, je propose quatre contributions pour la prise en compte des connaissances du domaine dans ces méthodes.
Tout d'abord je définis une nouvelle mesure de similarité sémantique et fonctionnelle (IntelliGO) entre les gènes, qui exploite au mieux les annotations fonctionnelles issues de l'ontologie GO ('Gene Ontology').
Je montre ensuite, grâce à une méthodologie d'évaluation rigoureuse, que la mesure IntelliGO est performante pour la classification fonctionnelle des gènes.
En troisième contribution je propose une approche différentielle avec affectation floue pour la construction de profils d'expression différentielle (PED).
Je définis alors un algorithme d'analyse de recouvrement entre classes fonctionnelles et ensemble des références, ici les PEDs, pour mettre en évidence des gènes ayant à la fois les mêmes variations d'expression et des annotations fonctionnelles similaires.
Cette méthode est appliquée à des données expérimentales produites à partir d'échantillons de tissus sains, de tumeur colo-rectale et de lignée cellulaire cancéreuse.
Finalement, la mesure de similarité IntelliGO est généralisée à d'autres vocabulaires structurés en graphe acyclique dirigé et enraciné (rDAG) comme l'est l'ontologie GO, avec un exemple d'application concernant la réduction sémantique d'attributs avant la fouille.
À partir d'une étude de cas construite autour de l'analyse des discours produits par les différents acteurs des champs institutionnels étudiés (le champ de l'hébergement touristique pour Airbnb, celui du transport public particulier de personnes pour Uber), nous cherchons à mettre en évidence les stratégies des plateformes pour se constituer une place dans l'espace social.
Nous mobilisons différentes grilles d'analyse pour comprendre les stratégies mises en œuvre par Airbnb et Uber : la dynamique des écosystèmes d'affaires des plateformes développée par la recherche en management stratégique, la grille de lecture du mégamarketing définie par Kotler en 1986, la théorie néo-institutionnelle et ses derniers développements concernant le travail institutionnel et la question de la légitimité développée en particulier par Suchman (1995).
Airbnb et Uber ont mobilisé chacune à leur manière les compétences de mégamarketing pour constituer leur écosystème d'affaires et leur système de légitimité, véritable support à leur conquête institutionnelle.
Ces différentes expressions de leurs stratégies s'incarnent aussi dans le processus de travail institutionnel, tourné vers la négociation pour Airbnb et vers la défiance pour Uber.
Les résultats du processus institutionnel ont des similarités entre les cas : constitution des systèmes de légitimité nécessaires à l'interprétation du rôle de ces deux plateformes, reconnaissance par la loi des activités permises par les plateformes et de leurs versants producteurs, ajustement des offres des professionnels établis.
Ce travail permet d'entrevoir un modèle de cycle de vie des plateformes, prenant en compte les dynamiques propres à ces formes organisationnelles ainsi que celles découlant du travail institutionnel et de leur quête de légitimité.
Ce projet vise à produire des outils experts afin de détecter automatiquement, voire de proposer, des (portions de) textes adaptées aux capacités de compréhension des enfants.
Ces outils s'appuieront sur l'analyse de l'expression linguistique de la temporalité et des émotions dans les textes pour enfants, en lien avec des résultats de travaux psycholinguistiques portant sur les étapes développementales de compréhension de ces deux dimensions.
Extraire de l'information utile de ces données nécessite d'importants prétraitements et des étapes de réduction de bruit.
La complexité de ces analyses rend les résultats très sensibles aux paramètres choisis.
Le temps de calcul requis augmente plus vite que linéairement : les jeux de données sont si importants qu'il ne tiennent plus dans le cache, et les architectures de calcul classiques deviennent inefficaces.
Pour réduire les temps de calcul, nous avons étudié le feature-grouping comme technique de réduction de dimension.
Pour ce faire, nous utilisons des méthodes de clustering.
Nous proposons un algorithme de clustering agglomératif en temps linéaire : Recursive Nearest Agglomeration (ReNA).
ReNA prévient la création de clusters énormes, qui constitue un défaut des méthodes agglomératives rapides existantes.
Nous démontrons empiriquement que cet algorithme de clustering engendre des modèles très précis et rapides, et permet d'analyser de grands jeux de données avec des ressources limitées.
En neuroimagerie, l'apprentissage statistique peut servir à étudier l'organisation cognitive du cerveau.
Des modèles prédictifs permettent d'identifier les régions du cerveau impliquées dans le traitement cognitif d'un stimulus externe.
L'entraînement de ces modèles est un problème de très grande dimension, et il est nécéssaire d'introduire un a priori pour obtenir un modèle satisfaisant.
Afin de pouvoir traiter de grands jeux de données et d'améliorer la stabilité des résultats, nous proposons de combiner le clustering et l'utilisation d'ensembles de modèles.
Nous évaluons la performance empirique de ce procédé à travers de nombreux jeux de données de neuroimagerie.
Enfin, nous montrons que l'utilisation d'ensembles de modèles améliore la stabilité des cartes de poids résultantes et réduit la variance du score de prédiction.
Nous proposons deux nouvelles approches pour les systèmes de recommandation et les réseaux.
Dans la première partie, nous donnons d'abord un aperçu sur les systèmes de recommandation avant de nous concentrer sur les approches de rang faible pour la complétion de matrice.
En nous appuyant sur une approche probabiliste, nous proposons de nouvelles fonctions de pénalité sur les valeurs singulières de la matrice de rang faible.
L'algorithme résultant est un algorithme à seuillage doux itératif qui adapte de manière itérative les coefficients de réduction associés aux valeurs singulières.
L'algorithme est simple à mettre en œuvre et peut s'adapter à de grandes matrices.
Nous fournissons des comparaisons numériques entre notre approche et de récentes alternatives montrant l'intérêt de l'approche proposée pour la complétion de matrice à rang faible.
Dans la deuxième partie, nous présentons d'abord quelques prérequis sur l'approche bayésienne non paramétrique et en particulier sur les mesures complètement aléatoires et leur extension multivariée, les mesures complètement aléatoires composées.
Nous proposons ensuite un nouveau modèle statistique pour les réseaux creux qui se structurent en communautés avec chevauchement.
Le modèle est basé sur la représentation du graphe comme un processus ponctuel échangeable, et généralise naturellement des modèles probabilistes existants à structure en blocs avec chevauchement au régime creux.
Notre construction s'appuie sur des vecteurs de mesures complètement aléatoires, et possède des paramètres interprétables, chaque nœud étant associé un vecteur représentant son niveau d'affiliation à certaines communautés latentes.
Nous développons des méthodes pour simuler cette classe de graphes aléatoires, ainsi que pour effectuer l'inférence a posteriori.
Nous montrons que l'approche proposée peut récupérer une structure interprétable à partir de deux réseaux du monde réel et peut gérer des graphes avec des milliers de nœuds et des dizaines de milliers de connections.
L'imagerie médicale est une ressource de données principale pour différents types d'applications.
Bien que les images concrétisent beaucoup d'informations sur le cas étudié, toutes les connaissances a priori du médecin restent implicites.
Elles jouent cependant un rôle très important dans l'interprétation et l'utilisation des images médicales.
Dans cette thèse, des connaissances anatomiques a priori sont intégrées dans deux applications médicales.
Nous proposons d'abord une chaîne de traitement automatique qui détecte, quantifie et localise des anévrismes dans un arbre vasculaire segmenté.
Des lignes de centre des vaisseaux sont extraites et permettent la détection et la quantification automatique des anévrismes.
Pour les localiser, une mise en correspondance est faite entre l'arbre vasculaire du patient et un arbre vasculaire sain.
Les connaissances a priori sont fournies sous la forme d'un graphe.
Nous proposons ensuite un nouvel algorithme pour cette tâche, qui profite de toutes les connaissances a priori disponibles dans l'ontologie.
Les hospitalisations potentiellement évitables (HPE) sont les admissions à l'hôpital qui auraient pu être évitées grâce à des traitements rapides et efficaces.
Les taux élevés de HPE sont associés à de nombreux facteurs.
D'un autre côté, ces hospitalisations évitables sont associées à un coût de plusieurs centaines de millions d'euros pour l'assurance maladie.
En d'autres termes, la réduction des HPE améliore non seulement la qualité de vie des patients, mais pourrait également faire économiser des coûts substantiels grâce aux traitements des patients.
Par conséquent, les autorités sanitaires sont très intéressées par des solutions améliorant les services de santé pour réduire les HPE.
Certaines études récentes en France ont suggéré que l'augmentation du nombre d'infirmières dans certaines zones géographiques pourrait conduire à une réduction des taux des HPE.
Dans notre approche, après avoir évalué certaines méthodes de régression courantes, nous avons étendu la machine à vecteurs de support pour la régression à l'information spatiale.
Cette approche nous permet de sélectionner non seulement les zones géographiques mais aussi le nombre d'infirmières à ajouter dans ces zones pour la plus forte réduction du nombre des HPE.
Concrètement, notre approche est appliquée en Occitanie, en France et les zones géographiques mentionnées ci-dessus sont les bassins de vie (BV).
D'un autre côté, la température extrême pourrait être un facteur potentiel associé à des taux élevés de HPE.
Par conséquent, une partie de nos travaux consiste à mesurer l'impact de la température extrême sur les HPE ainsi qu'à inclure ces données environnementales dans notre approche ci-dessus.
Dans nos travaux, nous avons utilisé les valeurs de température mesurées toutes les heures par des capteurs dans les stations météorologiques.
Cependant, ces valeurs sont parfois discontinues et nous avons besoin d'une méthode d'imputation pour ces valeurs manquantes.
Dans la littérature, deux approches les plus populaires traitant de cette étape de traitement exploitent soit la composante spatiale soit la composante temporelle des données de température.
Respectivement, ces approches sont des méthodes d'interpolation spatiale telles que la pondération de distance inverse (IDW) et des modèles de séries temporelles tels que la moyenne mobile intégrée autorégressive (ARIMA).
Les résultats montrent que par rapport aux méthodes IDW et ARIMA, notre approche fonctionne mieux à 100% et 99,8% (604 sur 605) stations météorologiques respectivement.
De plus, l'amélioration de la coordination entre les prestataires de soins de santé pourrait conduire à la réduction des HPE.
Dans le cas où les patients changeraient d'hôpital pour des traitements, afin d'assurer des traitements efficaces et de haute qualité, les médecins devraient avoir accès au dossier médical des patients des hôpitaux précédents.
Par conséquent, nous proposons une approche graphique pour résoudre ce problème.
En particulier, nous modélisons d'abord les flux de données des patients entre les hôpitaux sous forme de graphique pondéré non orienté dans lequel les nœuds et les bords présentent respectivement les hôpitaux et la quantité de flux de patients.
Ensuite, après avoir évalué deux méthodes de regroupement de graphes courantes, nous personnalisons celle qui convient le mieux à nos besoins.
Notre résultat fournit des informations intéressantes par rapport aux approches basées sur les limites administratives.
Cette recherche s'intéresse à l'acquisition du bilinguisme précoce simultané dans un contexte de mixité familiale français
Les recherches récentes dans ce domaine ont montré qu'un ensemble de facteurs tels que l'input parental, ainsi que les stratégies discursives familiales, peuvent expliquer comment l'enfant accède à la parole dans ce contexte (Döpke, 1998 ; Lanza, 1997, 2004 ; De Houwer, 2009 ; King et Fogle, 2013 entre autres).
L'enfant bilingue a été enregistrée en interaction spontanée et naturelle avec ses deux parents respectifs sur une durée de deux ans (2;00 à 4;00 ans).
Le corpus total est constitué de 68 heures d'enregistrement et l'échantillon analysé a été restreint à 28 heures de transcriptions.
Les résultats montrent que la fréquence de l'exposition à l'input et les pratiques langagières familiales ont un impact considérable sur les rapports émergents entre les deux langues à un âge précoce.
Il découle de notre analyse que l'enfant développe une forme dominante du bilinguisme et passe progressivement à l'usage harmonieux des deux langues vers 3 ans.
Ce passage est accompagné des changements du paysage sonore et de la fréquence de l'exposition à l'input en russe.
Un décalage dans l'apparition des catégories grammaticales a été noté : l'acquisition du français suit les modalités générales observées chez les enfants monolingues français, tandis que celle du russe connaît un décalage substantiel.
Les transferts interlangues au niveau du lexique, de la morphologie (les fillers) et de la syntaxe permettent de soutenir l'existence des compétences sous-jacentes communes.
Les principaux sujets étudiés dans cette thèse concernent le développement d'algorithmes stochastiques d'optimisation sous incertitude, l'étude de leurs propriétés théoriques et leurs applications.
On étudie leur convergence en utilisant des outils développés dans la théorie des processus de Markov : on utilise les propriétés du générateur infinitésimal et des inégalités fonctionnelles pour mesurer la distance entre leur distribution et une distribution cible.
La première partie est dédiée aux graphes quantiques, munis d'une mesure de probabilité sur l'ensemble des sommets.
Les graphes quantiques sont des versions continues de graphes pondérés non-orientés.
Le point de départ de cette thèse a été de trouver la moyenne de Fréchet de tels graphes.
La moyenne de Fréchet est une extension aux espaces métriques de la moyenne euclidienne et est définie comme étant le point qui minimise la somme des carrés des distances pondérées à tous les sommets.
Notre méthode est basée sur une formulation de Langevin d'un recuit simulé bruité et utilise une technique d'homogénéisation.
Dans le but d'établir la convergence en probabilité du processus, on étudie l'évolution de l'entropie relative de sa loi par rapport a une mesure de Gibbs bien choisie.
En utilisant des inégalités fonctionnelles (Poincaré et Sobolev) et le lemme de Gronwall, on montre ensuite que l'entropie relative tend vers zéro.
Notre méthode est testée sur des données réelles et nous proposons une méthode heuristique pour adapter l'algorithme à de très grands graphes, en utilisant un clustering préliminaire.
Dans le même cadre, on introduit une définition d'analyse en composantes principales pour un graphe quantique.
Ceci implique, une fois de plus, un problème d'optimisation stochastique, cette fois-ci sur l'espace des géodésiques du graphe.
Nous présentons un algorithme pour trouver la première composante principale et conjecturons la convergence du processus de Markov associé vers l'ensemble voulu.
Dans une deuxième partie, on propose une version modifiée de l'algorithme du recuit simulé pour résoudre un problème d'optimisation stochastique global sur un espace d'états fini.
On montre la convergence en probabilité de l'algorithme vers l'ensemble optimal, on donne la vitesse de convergence et un choix de paramètres optimisés pour assurer un nombre minimal d'évaluations pour une précision donnée et un intervalle de confiance proche de 1.
Ce travail est complété par un ensemble de simulations numériques qui illustrent la performance pratique de notre algorithme à la fois sur des fonctions tests et sur des données réelles issues de cas concrets.
Aligner des macromolécules telles que des protéines, des ADN et des ARN afin de révéler ou exploiter, leur homologie fonctionnelle est un défi classique en bioinformatique, qui offre de nombreuses applications, notamment dans la modélisation de structures et l'annotation des génomes.
Un certain nombre d'algorithmes et d'outils ont été proposés pour le problème d'alignement structure-séquence d'ARN.
Cependant, en ce qui concerne les ARN complexes, comportant des pseudo-noeuds, des interactions multiples et des paires de bases non canoniques, de tels outils sont rarement utilisés dans la pratique, en partie à cause de leurs grandes exigences de calcul, et de leur incapacité à supporter des types généraux de structures.
Récemment, Rinaudo et al. ont donné un algorithme paramétré général pour la comparaison structure-séquence d'ARN, qui est capable de prendre en entrée n'importe quel type de structures comportant des pseudo-noeuds.
L'algorithme paramétré est un algorithme de programmation dynamique basée sur la décomposition arborescente.
Afin de l'accélérer sans perte sensible de précision, nous avons introduit une approche de programmation dynamique par bandes.
De plus, trois algorithmes ont été développés pour obtenir des alignements sous-optimaux.
Tous ces algorithmes ont été implémentés dans un logiciel nommé LiCoRNA (aLignment of Complex RNAs).
Les performances de LiCoRNA ont été évaluées d'abord sur l'alignement des graines des familles de de la base de données RFAM qui comportent des pseudo-noeuds.
Comparé aux autres algorithmes de l'état de l'art, LiCoRNA obtient généralement des résultats équivalents ou meilleurs que ses concurrents.
Grâce à la grande précision démontrée par LiCoRNA, nous montrons que cet outil peut être utilisé pour améliorer les alignements de certaines familles de RFAM qui comportent des pseudo-noeuds.
Le cerveau biologique est composé d'un ensemble d'éléments qui évoluent depuis des millions d'années.
Les neurones et autres cellules forment un réseau complexe d'interactions duquel émerge l'intelligence.
C'est particulièrement le cas des réseaux neuronaux profonds modernes qui révolutionnent actuellement de nombreux domaines de recherche en informatique tel que la vision par ordinateur, la traduction automatique, le traitement du langage naturel et bien d'autres.
Cependant, les réseaux de neurones artificiels ne sont basés que sur un petit sous-ensemble de fonctionnalités biologiques du cerveau.
Ils se concentrent souvent sur les fonctions globales, homogènes et à un système complexe et localement hétérogène.
Dans cette thèse, nous avons d'examiner le cerveau biologique, des neurones simples aux réseaux capables d'apprendre.
Nous avons examiné individuellement la cellule neuronale, la formation des connexions entre les cellules et comment un réseau apprend au fil du temps.
Pour chaque composant, nous avons utilisé l'évolution artificielle pour trouver les principes de conception neuronale qui nous avons optimisés pour les réseaux neuronaux artificiels.
Notre objectif est d'améliorer la performance des réseaux de neurones artificiels par les moyens inspirés des neurosciences modernes.
Cependant, en évaluant les effets biologiques dans le contexte d'un agent virtuel, nous espérons également fournir des modèles de cerveau utiles aux biologistes.
Au fil des dernières années les robots ont fait partie de notre quotidien.
Les robots sont utilisés aussi pour l'organisation des produits dans les usines.
Un autre domaine de croissance est la robotique sociale.
Nous pouvons voir des études tel que des robots d'aide aux enfants autistes.
Il y a aussi des robots qui sont utilisés pour accueillir des personnes dans des hôtels ou dans centres commerciaux pour interagir avec les gens.
Ainsi, le robot doit comprendre le comportement des personnes.
Et, pour les robots mobiles, il faut savoir comment naviguer dans l'environnement humain.
En ce qui concerne les environnements humains, ce travail explore la navigation acceptable socialement des robots en direction de personnes.
Une personne est une entité qui doit être pris en compte sur la base des normes sociales que nous (en tant que personnes) utilisons tous les jours.
Dans cette thèse, nous explorons comment un robot s'approche d'une personne.
Celle-ci peut-être gênée si quelque chose ou quelqu'un envahit son espace personnel.
La personne se sentira aussi menacée si elle est approchée par derrière.
Ces normes sociales doivent être respectées par le robot.
C'est pour cela que nous modélisons le comportement du robot à travers des algorithmes d'apprentissage.
Nous faisons approcher (manuellement) un robot d'un personne plusieurs fois et le robot apprend à reproduire ce comportement.
Un autre travail de cette thèse est la compréhension d'un groupe de personnes.
Nous, en tant que humains, avons la capacité de le faire intuitivement.
Toutefois, un robot nécessite impérativement un modèle mathématique.
Enfin, nous abordons le sujet d'un robot qui s'approche d'un groupe de personnes.
Nous utilisons des démonstrations pour faire apprendre le robot.
Nous évaluons le bon déroulement du comportement du robot comme par exemple, en observant combien de fois le robot envahit l'espace personnel des personnes pendant la navigation.
Cette étude présente une nouvelle approche pour la génération automatique de résumés, un des principaux défis du Traitement de la Langue Naturelle.
Ce sujet, traité pendant un demi-siècle par la recherche, reste encore actuel car personne n'a encore réussi à créer automatiquement des résumés comparables, en qualité, avec ceux produits par des humains.
Dans le premier, les phrases sont triées de façon à ce que les meilleures conforment le résumé final.
Or, les phrases sélectionnées pour le résumé portent souvent des informations secondaires, une analyse plus fine s'avère nécessaire.
Nous proposons une méthode de compression automatique de phrases basée sur l'élimination des fragments à l'intérieur de celles-ci.
La méthode proposée est capable de générer des résumés corrects en espagnol.
Les résultats de cette étude soulèvent divers aspects intéressants vis-à-vis du résumé de textes par compression de phrases.
La recrudescence de contenus dans lesquels les clients expriment leurs opinions relativement à des produits de consommation a fait de l'analyse d'opinion un sujet d'intérêt pour la recherche en apprentissage automatique.
Cependant, prédire une opinion est un tâche difficile et parmi les modèles à disposition, peu sont capables de capturer la complexité de tels objets.
Les approches actuelles reposent sur la prédiction de représentations simplifiées d'expressions affectives.
Par exemple, il est possible de se restreindre à la reconnaissance de l'attribut de valence.
Cette thèse propose d'étudier le problème de la construction de modèles structurés capables de tirer parti des dépendances entre les différentes composantes des opinions.
Dans ce contexte, le choix d'un modèle d'opinion a des conséquences sur la complexité du problème d'apprentissage et sur les propriétés statistiques des fonctions de prédiction associées.
Nous étudions 2 problèmes classique de l'analyse d'opinion pour lesquels nous mettons en oeuvre des modèles à base de fonctions à noyau de sortie permettant d'illustrer le compromis précision
Un second aspect de cette thèse repose sur l'adaptation de méthodes issues de l'état de l'art à un jeu de données comportant des données d'opinion à la structure complexe.
Nous proposons une approche basée sur l'apprentissage profond pour prendre en contre jointement les différentes étiquettes du modèle d'opinions.
Une nouvelle architecture hiérarchique est introduite issue de la fusion de structures précédemment proposées en les étendant à un jeu de données multimodal.
Nous montrons que notre approche fournit des résultats compétitifs par rapport à des architectures traitant séparément les différentes représentations des opinions ce qui soulève des nouvelles questions concernant les stratégies optimales de traitement de données définies selon une hiérarchie.
Le verbe dar est une unité lexicale particulièrement riche en nuances significatives.
Parmi les études qui ont abordé cette unité, certaines font l'hypothèse d'une désémantisation lors de la 'grammaticalisation'du verbe.
Ce processus serait illustré notamment par l'existence du marqueur discursif dale ainsi que par les emplois de dar en tant que 'support'dans des constructions verbo-nominales plus ou moins lexicalisées ou dans périphrases verbales du type dar un golpe, dar que hablar.
L'hypothèse envisagée dans cette étude – menée dans le cadre théorique de la linguistique du signifiant, qui prône l'unicité du signe linguistique – est que le verbe dar exprime une notion d'existence sous-jacente à tous ses emplois discursifs.
Le signifié de langue de dar exprimerait l'accès à l'existence d'une entité b, pour le bénéficiaire c d'une opération conduite par une entité a.
Cette notion d'existence serait présente dans toutes les occurrences d'un lexème qui n'apparaît jamais vidé – ne serait-ce que partiellement – de son contenu lexical.
Les opérateurs de télécommunications se doivent de maîtriser et d'évaluer la qualité des services qu'ils offrent à leurs clients, dans un contexte en perpétuelle évolution.
Comme alternative rapide et à moindre coût aux évaluations fondées sur l'interrogation d'utilisateurs, des outils de mesure ont été développés, qui intègrent des modèles permettant de prédire la qualité perçue.
Cette thèse avait pour but de concevoir un outil de diagnostic de qualité vocale (applicable aux services de téléphonie), complémentaire à de tels modèles objectifs, afin d'obtenir des informations spécifiques sur la nature des défauts présents sur le signal audio et d'orienter vers des causes potentielles de ces défauts.
En partant de l'hypothèse que la qualité vocale est multidimensionnelle, nous avons fondé l'outil de diagnostic sur la modélisation des quatre dimensions identifiées dans la littérature : la Bruyance, représentative des bruits de fond, la Continuité, relative à la perception des discontinuités dans le signal, la Coloration, liée aux distorsions du spectre de la voix, et la Sonie, traduisant la perception du niveau sonore.
Chacune de ces dimensions est quantifiée à l'aide d'indicateurs de qualité issus de l'analyse du signal audio.
Notre démarche a consisté, dans un premier temps, à rechercher dans des modèles objectifs récents (notamment la norme P.863 de l'UIT-T) des indicateurs de qualité et à en développer d'autres pour caractériser parfaitement chaque dimension.
Finalement, pour chaque dimension, nous avons développé un module de classification automatique de défauts perçus en fonction de la nature du défaut identifié dans le signal, ainsi qu'un module supplémentaire estimant l'impact du défaut sur la qualité vocale.
L'outil proposé couvre les trois bandes audio (bande étroite, bande élargie et bande super-élargie) couramment utilisées dans les systèmes de télécommunications avec, toutefois, une priorité pour les signaux en bande super-élargie, plus représentatifs des contenus audio qu'on sera amené à rencontrer dans les futurs services de télécommunications.
Pour estimer la répartition de la puissance au sein d'un réacteur nucléaire, il est nécessaire de coupler des modélisations neutroniques et thermohydrauliques.
De telles simulations doivent disposer des valeurs sections efficaces homogénéisées à peu de groupes d'énergies qui décrivent les interactions entre les neutrons et la matière.
Cette thèse est consacrée à la modélisation des sections efficaces par des techniques académiques innovantes basées sur l'apprentissage machine.
La performance d'un modèle est principalement définie par le nombre de coefficients qui le caractérisent (c'est-à-dire l'espace mémoire nécessaire pour le stocker), la vitesse d'évaluation, la précision, la robustesse au bruit numérique, la complexité, etc.
Dans cette thèse, un assemblage standard de combustible UOX REP est analysé avec trois variables d'état : le burnup, la température du combustible et la concentration en bore.
Trois techniques d'approximation sont étudiées.
Les méthodes de noyaux, qui utilisent le cadre général d'apprentissage machine, sont capables de proposer, dans un espace vectoriel normalisé, une grande variété de modèles de régression ou de classification.
Les méthodes à noyaux peuvent reproduire différents espaces de fonctions en utilisant un support non structuré, qui est optimisé avec des techniques d'apprentissage actif.
Les approximations sont trouvées grâce à un processus d'optimisation convexe facilité par "l'astuce du noyau”.
Le caractère modulaire intrinsèque de la méthode facilite la séparation des phases de modélisation : sélection de l'espace de fonctions, application de routines numériques, et optimisation du support par apprentissage actif.
Les réseaux de neurones sont des méthodes d'approximation universelles capables d'approcher de façon arbitraire des fonctions continues sans formuler de relations explicites entre les variables.
Une fois formés avec des paramètres d'apprentissage adéquats, les réseaux à sorties multiples (intrinsèquement parallélisables) réduisent au minimum les besoins de stockage tout en offrant une vitesse d'évaluation élevée.
Les stratégies que nous proposons sont comparées entre elles et à l'interpolation multilinéaire sur une grille cartésienne qui est la méthode utilisée usuellement dans l'industrie.
L'ensemble des données, des outils, et des scripts développés sont disponibles librement sous licence MIT.
Depuis 2004, les médias sociaux en ligne ont connu une croissance considérable.
Ce développement rapide a eu des effets intéressants pour augmenter la connexionet l'échange d'informations entre les utilisateurs, mais certains effets négatifs sont également apparus, dont le nombre de faux comptes grandissant jour après jour.
Les sockpuppets sont les multiples faux comptes créés par un même utilisateur.
Ils sont à l'origine de plusieurs types de manipulations comme la création de faux comptes pour louer, défendre ou soutenir une personne ou une organisation, ou pour manipuler l'opinion publique.
Des expérimentations ont été réalisées sur l'étape d'adaptation en utilisant des données réelles extraites de Wikipédia anglais.
Afin de trouver le meilleur algorithme d'apprentissage automatique pour la phase de détection de sockpuppet, les résultats de six algorithmes d'apprentissage automatique sont comparés.
En outre, ils sont comparés à la littérature où les résultats de la comparaison montrent que notre proposition améliore la précision de la détection des sockpuppets.
De plus, les résultats de cinq algorithmes de détection de communauté sont comparés pour la phase de regroupement de Sockpuppet, afin de trouver le meilleur algorithme de détection de communauté qui sera utilisé en temps réel.
La communication affective est au cœur de nos interactions interpersonnelles.
Nous communiquons les émotions à travers de multiples canaux non verbaux.
Plusieurs travaux de recherche sur l'interaction homme-machine ont exploité ces modalités de communication afin de concevoir des systèmes permettant de reconnaître et d'afficher automatiquement des signaux affectifs.
Le toucher est la modalité la moins explorée dans ce domaine de recherche.
L'aspect intrusif des interfaces haptiques actuelles est l'un des principaux obstacles à leur utilisation dans la communication affective médiée.
En effet, l'utilisateur est physiquement connecté à des systèmes mécaniques pour recevoir la stimulation.
Cette configuration altère la transparence de l'interaction médiée et empêche la perception de certaines dimensions affectives comme la valence.
Sur la base de l'état de l'art des interfaces haptiques, nous avons proposé une stratégie de stimulation tactile basée sur l'utilisation d'un jet d'air mobile.
Cette technique permet de fournir une stimulation tactile non-intrusive sur des zones différentes du corps.
De plus, ce dispositif tactile permettrait une stimulation efficace de certains mécanorécepteurs qui jouent un rôle important dans les perceptions d'affects positifs.
Nous avons conduit une étude expérimentale pour comprendre les relations entre les caractéristiques physiques de la stimulation tactile par jet d'air et la perception affective des utilisateurs.
Les résultats mettent en évidence les effets principaux de l'intensité et de la vitesse du mouvement du jet d'air sur l'évaluation subjective mesurée dans l'espace affectif (à savoir, la valence, l'arousal et de la dominance).La communication des émotions est clairement multimodale.
Nous utilisons le toucher conjointement avec d'autres modalités afin de communiquer les différents messages affectifs.
C'est dans ce sens que nous avons conduit deux études expérimentales pour examiner la combinaison de la stimulation tactile par jet d'air avec les expressions faciales et vocales pour la perception de la valence.
Ces expérimentations ont été conduites dans un cadre théorique et expérimental appelé théorie de l'intégration de l'information.
Ce cadre permet de modéliser l'intégration de l'information issue de plusieurs sources en employant une algèbre cognitive.
Les résultats de nos travaux suggèrent que la stimulation tactile par jet d'air peut être utilisée pour transmettre des signaux affectifs dans le cadre des interactions homme-machine.
Les modèles perceptifs d'intégration bimodales peuvent être exploités pour construire des modèles computationnels permettant d'afficher des affects en combinant la stimulation tactile aux expressions faciales ou à la voix.
Les arguments de sécurité sont couramment utilisés pour montrer que des efforts suffisants ont été faits pour atteindre les objectifs de sécurité.
Ainsi, la sécurité du système est souvent justifiée par l'évaluation des arguments de sécurité.
L'évaluation de tels arguments repose généralement sur l'avis d'experts sans s'appuyer sur des outils ou des méthodes dédiés.
Ceci pose des questions sur la validité des résultats.
Dans cette thèse, une approche quantitative est proposée, basé sur la théorie de Dempster-Shafer (théorie D-S) pour évaluer notre confiance dans les arguments de sécurité.
Une application dans le domaine ferroviaire conduit à l'estimation des paramètres du cadre par une enquête auprès d'experts en sécurité.
Cette thèse propose une méthode et un système d'identification d'entités (personnes, lieux, organisations) mentionnées au sein des contenus textuels produits par l'Agence France Presse dans la perspective de l'enrichissement automatique de ces contenus.
Les différents domaines concernés par cette tâche ainsi que par l'objectif poursuivi par les acteurs de la publication numérique de contenus textuels sont abordés et mis en relation : Web Sémantique, Extraction d'Information et en particulier Reconnaissance d'Entités Nommées (REN), Annotation Sémantique, Liage d'Entités.
À l'issue de cette étude, le besoin industriel formulé par l'Agence France Presse fait l'objet des spécifications utiles au développement d'une réponse reposant sur des outils de Traitement Automatique du Langage.
L'approche adoptée pour l'identification des entités visées est ensuite décrite : nous proposons la conception d'un système prenant en charge l'étape de REN à l'aide de n'importe quel module existant, dont les résultats, éventuellement combinés à ceux d'autres modules, sont évalués par un module de Liage capable à la fois (i) d'aligner une mention donnée sur l'entité qu'elle dénote parmi un inventaire constitué au préalable, (ii) de repérer une dénotation ne présentant pas d'alignement dans cet inventaire et (iii) de remettre en cause la lecture dénotationnelle d'une mention (repérage des faux positifs).
Le système Nomos est développé à cette fin pour le traitement de données en français.
Sa conception donne également lieu à la construction et à l'utilisation de ressources ancrées dans le réseau des Linked Data ainsi que d'une base de connaissances riche sur les entités concernées.
Dans un contexte où l'industrie du semi-conducteur explore de nouvelles voies avec la diversification des produits et le paradigme de « More than Moore » , les délais de livraison et la précision de livraison sont des éléments clés pour la compétitivité d'entreprises de semi-conducteur et l'industrie 4.0 en général.
La première partie de cette thèse offre une étude approfondie de la variabilité : nous mettons d'abord en avant les conséquences de la variabilité pour mieux la définir, puis nous clarifions que la variabilité concerne les flux de production en introduisant la notion de variabilité des flux de production et en apportant des éléments de mesure associés, et nous clôturons cette première partie par l'étude des sources de variabilité à travers une étude bibliographique et des exemples industriels.
La seconde partie est dédiée à l'intégration de la variabilité dans les outils de gestion de production : nous montrons comment une partie des conséquences peut être mesurée et intégrée aux projections d'encours pour améliorer le contrôle et la prévisibilité de la production, proposons un nouvel outil ((the WIP Concurrent) pour mesurer plus précisément les performances des systèmes en environnement complexe, et mettons en avant des effets de dépendances prépondérants sur la variabilité des flux de production et pourtant jamais pris en compte dans les modèles.
La troisième et dernière partie de la thèse couvre les perspectives de réduction de la variabilité : en se basant sur les éléments présentés dans la thèse, nous proposons un plan pour réduire la variabilité des flux de production sur le court terme, et une direction pour la recherche à moyen et long terme.
Le doctorant prendra part à l'ANR JCJC MAOI (Analyse Multimodale des Opinions en Interactions) à Telecom-ParisTech.
Le principal défi consistera à intégrer le contexte d'interaction dans les méthodes de détection d'opinion par apprentissage automatique.
Dans le premier, nous utilisons le modèle autorégressif de la parole qui décrit les corrélations court et long termes (quasi-périodique) pour formuler un modèle d'état dépendant de paramètres inconnus.
EM-Kalman est ainsi utilisé pour estimer conjointement les sources et les paramètres.
Dans le deuxième, nous proposons une méthode fréquentielle pour le même modèle de la parole où les sources et les paramètres sont estimés séparément.
Les observations sont découpées à l'aide d'un fenêtrage bien conçu pour assurer une reconstruction parfaite des sources après.
Les paramètres (de l'enveloppe spectrale) sont estimés en maximisant le critère du GML exprimé avec la matrice de covariance paramétrée que nous modélisons plus correctement en tenant compte de l'effet du fenêtrage.
Le filtre de Wiener est utilisé pour estimer les sources.
Nous considérons le cas conjointement Gaussien (observations et variables cachées) et trois méthodes itératives d'estimation conjointe : MAP en alternance avec ML, biaisé même asymptotiquement pour les paramètres, EM qui converge asymptotiquement vers ML et VB que nous prouvons converger asymptotiquement vers la solution ML pour les paramètres déterministes.
La diversité linguistique est actuellement menacée : la moitié des langues connues dans le monde pourraient disparaître d'ici la fin du siècle.
Cette prise de conscience a inspiré de nombreuses initiatives dans le domaine de la linguistique documentaire au cours des deux dernières décennies, et 2019 a été proclamée Année internationale des langues autochtones par les Nations Unies, pour sensibiliser le public à cette question et encourager les initiatives de documentation et de préservation.
Néanmoins, ce travail est coûteux en temps, et le nombre de linguistes de terrain, limité.
Par conséquent, le domaine émergent de la documentation linguistique computationnelle (CLD) vise à favoriser le travail des linguistes à l'aide d'outils de traitement automatique.
Le projet Breaking the Unwritten Language Barrier (BULB), par exemple, constitue l'un des efforts qui définissent ce nouveau domaine, et réunit des linguistes et des informaticiens.
Cette thèse examine le problème particulier de la découverte de mots dans un flot non segmenté de caractères, ou de phonèmes, transcrits à partir du signal de parole dans un contexte de langues très peu dotées.
Il s'agit principalement d'une procédure de segmentation, qui peut également être couplée à une procédure d'alignement lorsqu'une traduction est disponible.
En utilisant deux corpus en langues bantoues correspondant à un scénario réaliste pour la linguistique documentaire, l'un en Mboshi (République du Congo) et l'autre en Myene (Gabon), nous comparons diverses méthodes monolingues et bilingues de découverte de mots sans supervision.
Nous montrons ensuite que l'utilisation de connaissances linguistiques expertes au sein du formalisme des Adaptor Grammars peut grandement améliorer les résultats de la segmentation, et nous indiquons également des façons d'utiliser ce formalisme comme outil de décision pour le linguiste.
Nous proposons aussi une variante tonale pour un algorithme de segmentation bayésien non-paramétrique, qui utilise un schéma de repli modifié pour capturer la structure tonale.
Pour tirer parti de la supervision faible d'une traduction, nous proposons et étendons, enfin, une méthode de segmentation neuronale basée sur l'attention, et améliorons significativement la performance d'une méthode bilingue existante.
Nous proposons de concevoir et développer un outil permettant d'analyser la diffusion d'information sur les services de réseaux sociaux en ligne grâce au traitement et à la visualisation de données.
Fruit d'une réflexion méthodologique, ce dispositif permet d'observer les relations entre les dimensions conversationnelles, sémantiques, temporelles et géographiques des actes de communication en ligne.
Courts messages se propageant rapidement sur la Toile selon des modèles encore mal connus, les mèmes Internet comptent parmi les contenus les plus prisés sur les plate-formes web.
Les mèmes Internet circulant sur le service de microblog chinois Sina Weibo articulent notamment discussions personnelles, débats sociétaux et vastes campagnes médiatiques.
Mobilisant des méthodes issues de l'analyse des réseaux et du traitement automatisé de la langue chinoise, nous procédons à l'analyse d'un vaste corpus de 200 millions de messages représentant l'activité sur Sina Weibo durant l'année 2012.
Néanmoins, le volume de calculs nécessaires pour obtenir des résultats fiables sur un large corpus nous amène à abandonner cette approche, montrant par là-même la complexité d'une définition intéressante de l'objet numérique composite mème.
Les résultats montrent que les usages majoritaires de Sina Weibo sont similaires à ceux des médias traditionnels (publicité, divertissement, loisirs...).
Néanmoins, nous écartons les hashtags comme représentants des mèmes Internet, artefacts d'usages commerciaux et stratégiques à la diffusion cadrée et planifiée.
L'organisation de ces informations selon un axe temporel dans un espace de visualisation interactif autorise une lecture détaillée de leur diffusion.
La plupart des systèmes de recommandation actuels se base sur des évaluations sous forme de notes (i.e., chiffre entre 0 et 5) pour conseiller un contenu (film, restaurant...) à un utilisateur.
Ce dernier a souvent la possibilité de commenter ce contenu sous forme de texte en plus de l'évaluer.
Il est difficile d'extraire de l'information d'un texte brut tandis qu'une simple note contient peu d'information sur le contenu et l'utilisateur.
Dans cette thèse, nous tentons de suggérer à l'utilisateur un texte lisible personnalisé pour l'aider à se faire rapidement une opinion à propos d'un contenu.
Plus spécifiquement, nous construisons d'abord un modèle thématique prédisant une description de film personnalisée à partir de commentaires textuels.
Nous évaluons notre modèle sur une base de données IMDB et illustrons ses performances à travers la comparaison de thèmes.
Nous étudions ensuite l'inférence de paramètres dans des modèles à variables latentes à grande échelle, incluant la plupart des modèles thématiques.
Nous proposons un traitement unifié de l'inférence en ligne pour les modèles à variables latentes à partir de familles exponentielles non-canoniques et faisons explicitement apparaître les liens existants entre plusieurs méthodes fréquentistes et Bayesiennes proposées auparavant.
Enfin, nous proposons une nouvelle classe de processus ponctuels déterminantaux (PPD) qui peut être manipulée pour l'inférence et l'apprentissage de paramètres en un temps potentiellement sous-linéaire en le nombre d'objets.
Cette classe, basée sur une factorisation spécifique de faible rang du noyau marginal, est particulièrement adaptée à une sous-classe de PPD continus et de PPD définis sur un nombre exponentiel d'objets.
Nous appliquons cette classe à la modélisation de documents textuels comme échantillons d'un PPD sur les phrases et proposons une formulation du maximum de vraisemblance conditionnel pour modéliser les proportions de thèmes, ce qui est rendu possible sans aucune approximation avec notre classe de PPD.
Nous présentons une application à la synthèse de documents avec un PPD sur 2 à la puissance 500 objets, où les résumés sont composés de phrases lisibles.
Des milliards de « choses » connectées à l'internet constituent les réseaux symbiotiques de périphériques de communication (par exemple, les téléphones, les tablettes, les ordinateurs portables), les appareils intelligents, les objets (par exemple, la maison intelligente, le réfrigérateur, etc.) et des réseaux de personnes comme les réseaux sociaux.
La notion de réseaux traditionnels se développe et, à l'avenir, elle ira au-delà, y compris plus d'entités et d'informations.
Ces réseaux et ces dispositifs détectent, surveillent et génèrent constamment une grande uantité de données sur tous les aspects de la vie humaine.
L'un des principaux défis dans ce domaine est que le réseau se compose de « choses » qui sont hétérogènes à bien des égards, les deux autres, c'est qu'ils changent au fil du temps, et il y a tellement d'entités dans le réseau qui sont essentielles pour identifier le lien entre eux.
Dans cette recherche, nous abordons ces problèmes en combinant la théorie et les algorithmes du traitement des événements avec les domaines d'apprentissage par machine.
Notre objectif est de proposer une solution possible pour mieux utiliser les informations générées par ces réseaux.
Cela aidera à créer des systèmes qui détectent et répondent rapidement aux situations qui se produisent dans la vie urbaine afin qu'une décision intelligente puisse être prise pour les citoyens, les organisations, les entreprises et les administrations municipales.
Les médias sociaux sont considérés comme une source d'information sur les situations et les faits liés aux utilisateurs et à leur environnement social.
Au début, nous abordons le problème de l'identification de l'opinion publique pour une période donnée (année, mois) afin de mieux comprendre la dynamique de la ville.
Le deuxième défi est de combiner les données du réseau avec diverses propriétés et caractéristiques en format commun qui faciliteront le partage des données entre les services.
Pour le résoudre, nous avons créé un modèle d'événement commun qui réduit la complexité de la représentation tout en conservant la quantité maximale d'informations.
Ce modèle comporte deux ajouts majeurs : la sémantiques et l'évolutivité.
La partie sémantique signifie que notre modèle est souligné avec une ontologie de niveau supérieur qui ajoute des capacités d'interopérabilité.
Bien que la partie d'évolutivité signifie que la structure du modèle proposé est flexible, ce qui ajoute des fonctionnalités d'extensibilité.
Nous avons validé ce modèle en utilisant des modèles d'événements complexes et des techniques d'analyse prédictive.
Pour faire face à l'environnement dynamique et aux changements inattendus, nous avons créé un modèle de réseau dynamique et résilient.
Il choisit toujours le modèle optimal pour les analyses et s'adapte automatiquement aux modifications en sélectionnant le meilleur modèle.
Nous avons utilisé une approche qualitative et quantitative pour une sélection évolutive de flux d'événements, qui réduit la solution pour l'analyse des liens, l'optimale et l'alternative du meilleur modèle.
A partir des décès certifiés électroniquement de 2012 à 2016 en France, la thèse vise à mettre en œuvre et évaluer les performances de méthodes de traitement automatique des langues, pour classer les causes médicales de décès disponibles en texte libre dans des regroupements syndromiques (RS) pertinents pour la surveillance réactive de la mortalité à visée d'alerte et d'évaluation d'impact sanitaire.
Près de 100 RS répondant aux objectifs ont été définis.
Deux méthodes de classification ont été développées : une méthode à base de règles linguistiques et une méthode par apprentissage supervisé (SVM).
Deux modèles SVM ont été développés utilisant différentes combinaisons de caractéristiques.
Le développement et l'évaluation des performances des méthodes se sont appuyés sur 4 500 certificats de décès annotés.
L'évaluation a porté dans un premier temps sur 7 RS, puis a été étendue à 60 autres RS.
Les évolutions mensuelles des RS attribués par les méthodes ont été comparées sur l'ensemble des décès de 2012 à 2016 (204 000 décès).
La méthode par règles et le modèle SVM incluant l'ensemble des caractéristiques ont obtenu des performances élevées (F-mesure≥0,95) pour la classification des causes dans 31 RS.
L'évolution temporelle de ces RS obtenus par les deux méthodes était comparable.
En moyenne, les causes de décès au sein d'un certificat sont classées dans 3,7 RS.
Une méthode de pondération équilibrée des RS a été proposée pour prendre en compte ces causes multiples lors de l'analyse en routine de la mortalité pour la surveillance à visée d'alerte et d'évaluation d'impact.
Ces résultats permettent d'améliorer la surveillance réactive actuellement fondée sur des données d'état-civil.
L'acquisition de connaissances relatives aux constructions verbales est une question importante pour le traitement automatique des langues, mais aussi pour la lexicographie qui vise à documenter les nouveaux usages linguistiques.
Cette tâche pose de nombreux enjeux, techniques et théoriques.
Dans le cadre de cette thèse, nous nous intéressons plus particulièrement à deux aspects fondamentaux de la description du verbe : la notion d'entrée lexicale et la distinction entre arguments et circonstants.
A la suite de précédentes études en traitement automatique des langues et en linguistique nous faisons l'hypothèse qu'il n'y a pas de distinction marquée entre homonymes et quasi-synonymes ; de même, nous posons qu'il existe un continuum entre arguments et circonstants.
Nous proposons une chaîne de traitement complète pour l'acquisition de schémas prédicatifs verbaux en japonais à partir d'un corpus non étiqueté de textes journalistiques.
Cette chaîne de traitement intègre la notion d'argumentalité au processus de création des entrées lexicales et met en œuvre une modélisation de ces deux continuums.
La ressource produite a fait l'objet d'une évaluation comparative qualitative, qui a permis de mettre en évidence la difficulté des ressources linguistiques à décrire de nouvelles données, plaidant par là même pour une lexicologie s'inscrivant dans le cadre épistémologique de la linguistique de corpus.
Malgré ses récents succès en vision assistée par ordinateur ou en traduction automatique, l'utilisation de l'apprentissage profond dans le secteur biomédical fait face à de nombreux challenges.
Parmi eux, nous comptons l'accès difficile à des données en quantité et qualité suffisantes, ainsi que le besoin en l'interopérabilité et en l'interprétabilité des modèles.
Dans cette thèse, nous nous intéressons à ces différentes problématiques à la lueur de la création de modèles prédisant la glycémie future de patients diabétiques.
De tels modèles permettraient aux patients d'anticiper les variations de leur glycémie au quotidien, les aidant ainsi à mieux la réguler afin d'éviter les états d'hypoglycémie et d'hyperglycémie.
Pour cela, nous utilisons trois ensembles de données.
Tandis que le premier a été récolté à l'occasion de cette thèse sur plusieurs patients diabétiques de type 2, les deux autres sont composés de patients diabétiques de type 1, à la fois réels et virtuels.
Dans l'ensemble des études, nous utilisons les données passées de glycémie, d'insuline et de glucides de chaque patient pour construire des modèles personnalisés prédisant la glycémie du patient 30 minutes dans le futur.
Dans un premier temps, nous faisons une analyse détaillée de l'état de l'art en construisant une base de résultats de référence open source de modèles prédictifs de glycémie.
Bien que prometteurs, nous mettons en évidence la difficulté qu'ont les modèles profonds à effectuer des prédictions qui soient à la fois précises et sans danger pour le patient.
Afin d'améliorer l'acceptabilité clinique des modèles, nous proposons d'intégrer des contraintes cliniques au sein de l'apprentissage des modèles.
Nous explorons son utilisation pratique à travers un algorithme permettant d'obtenir un modèle maximisant la précision des prédictions tout en respectant des contraintes cliniques fixées au préalable.
Puis, nous étudions la piste de l'apprentissage par transfert pour améliorer les performances des modèles prédictifs de glycémie.
Celui-ci permet de faciliter l'apprentissage des modèles personnalisés aux patients en réutilisant les connaissances apprises sur d'autres patients.
En particulier nous proposons le cadre de l'apprentissage par transfert multi-sources adverse.
Celui-ci permet de significativement améliorer les performances des modèles en permettant l'apprentissage de connaissances a priori qui sont plus générales, car agnostiques des patients sources du transfert.
Nous investiguons différents scénarios de transfert à travers l'utilisation de nos trois jeux de données.
Nous montrons qu'il est possible d'effectuer un transfert de connaissance à partir de données provenant de dispositifs expérimentaux différents, de patients de types de diabète différents, mais aussi à partir de patients virtuels.
Enfin, nous nous intéressons à l'amélioration de l'interprétabilité des modèles profonds à travers le principe d'attention.
En particulier, nous explorons l'utilisation d'un modèle profond et interprétable pour la prédiction de la glycémie.
Celui-ci implémente un double mécanisme d'attention lui permettant d'estimer la contribution de chaque variable en entrée au modèle à la prédiction finale.
Nous montrons empiriquement l'intérêt d'un tel modèle pour la prédiction de glycémie en analysant son comportement dans le calcul de ses prédictions.
Ce travail de thèse s'inscrit dans la thématique de l'identification automatique des langues.
Son objectif est de mettre en évidence des indices linguistiques susceptibles de permettre la distinction des idiomes issus du latin.
Les langues romanes ont bénéficié d'une longue tradition descriptive et représentent des langues officielles dans plusieurs pays du monde.
L'étude des approches taxinomistes consacrées aux idiomes néo-latins révèle une pertinence particulière de la classification typologique.
Les indices vocaliques fournissent des critères appropriés pour une division des langues en deux zones linguistiques, selon leurs complexités respectives.
Ces indices séparent l'espagnol et l'italien, langues à vocalisme prototypique du roumain, du français et du portugais, dont les systèmes vocaliques sont riches en oppositions supplémentaires.
Afin de tester une pertinence perceptive des critères typologiques, deux paradigmes expérimentaux ont été développés.
Une première série d'expériences, de type discrimination, a permis de délimiter le rôle des facteurs " langue maternelle " et " familiarité " des quatre populations participantes, dont deux de langue maternelle romane (français et Roumains) et deux de contrôle (Japonais, Américains).
Les résultats ont partiellement convergé vers un regroupement linguistique basé sur la proximité sonore des langues et analogue à la classification typologique fondée sur les spécificités vocaliques.
La seconde série d'expériences de type jugement de similarités effectuées par des sujets français et américains a confirmé ce regroupement.
Ainsi, les proximités sonores établies de manières perceptive entre les langues romanes permettent leur macro-discrimination en deux groupes principaux : italien, espagnol vs, roumain, français, portugais.
Dans le domaine de la chimie, il est intéressant de pouvoir estimer des propriétés physico-chimiques de molécules, notamment pour des applications industrielles.
Celles-ci sont difficiles à estimer par simulations physique, présentant une complexité temporelle prohibitive.
L'émergence des données (publiques ou privées) ouvre toutefois de nouvelles perspectives pour le traitement de ces problèmes par des méthodes statistiques et d'apprentissage automatique.
La principale difficulté réside dans la caractérisation des molécules : celles-ci s'apparentent davantage à un réseau d'atomes (autrement dit un graphe coloré) qu'à un vecteur.
Le but de cette thèse est de tirer parti des corpus publics pour apprendre les meilleures représentations possibles de ces structures, et de transférer cette connaissance globale vers des jeux de données plus restreints.
Nous nous inspirons pour ce faire de méthodes utilisées en traitement automatique des langages naturels.
Pour les mettre en œuvre, des travaux d'ordre plus théorique ont été nécessaires, notamment sur le problème d'isomorphisme de graphes.
Les résultats obtenus sur des tâches de classification/régression sont au moins compétitifs avec l'état de l'art, voire meilleurs, en particulier sur des jeux de données restreints, attestant des possibilités d'apprentissage par transfert sur ce domaine.
Comment concilier l'impératif d'assistance linguistique à toute personne ne parlant pas français et l'absence de ressources linguistiques standardisées pour traduire des combinaisons de langues génétiquement et culturellement distantes~ ?
C'est le problème posé par la traduction du hindi et de l'ourdou en France dans le contexte judiciaire.
Les systèmes judiciaires dont elles sont le moyen d'expression proviennent de l'héritage colonial britannique qui repose sur la common law.
Ce travail propose, à travers l'analyse d'un corpus de documents variés, de créer des ressources terminologiques et phraséologiques afin d'aider le traducteur-interprète à trouver des équivalences de traduction multilingues.
Dans un premier temps, nous abordons les différences entre les systèmes judiciaires et le statut des langues de travail dans les trois pays.
Enfin, nous proposons une méthode d'extraction des termes et d'alignement par sous-corpus afin de faire ressortir les équivalences terminologiques ou traductionnelles du genre judiciaire entre ces langues.
Ce travail, qui met en lumière les relations entre le texte, le contexte et les mots, fournit aux professionnels de la traduction et de l'interprétation des ressources attestées, adaptées au domaine de spécialité et contextualisées.
Cette thèse s'inscrit dans une étude sur l'élaboration d'une tête parlante.
Nous nous intéressons tout particulièrement à la prédiction du mouvement de coarticulation des lèvres et de la mâchoire.
Après avoir analysé les variations intra et interlocuteur des paramètres labiaux de deux corpora audiovisuels, nous avons conçu un algorithme de prédiction de la coarticulation basé sur des règles phonétiques et prenant en considération l'interaction entre les articulateurs.
Le principe de base est la concaténation de séquences élémentaires de type VC...CV qui ont été jugées pertinentes par notre algorithme de prédiction phonétique, et qui sont soit extraites du corpus, soit obtenues par complétion.
Afin d'estimer la qualité de notre synthèse, nous avons mesuré les différences entre les signaux réels et synthétisés sur l'ensemble des phrases du corpus et nous avons comparé notre solution avec l'algorithme de Cohen et Massaro.
Nous avons montré que notre synthèse est meilleure pour certaines séquences spécifiques de type VCCV où l'anticipation est plus complexe.
Les grandes variations de style de l'écriture et les difficultés de segmenter les mots cursifs sont les raisons principales pour laquelle la reconnaissance de mots cursive manuscrite pour être une tâche si difficile.
Un système de lecture des documents postaux indien basé sur le modèle stochastique basé d'un contexte sans segmentation est présenté.
L'originalité du travail réside sur une combinaison de caractéristiques conceptuelles à haut niveau avec les renseignements de pixel à basse altitude considérés par ancien modèle et une stratégie d'arrêt dans l'algorithme Viterbi.
Pendant que l'information de bas niveau peut être facilement extraite de la forme analysée, le pouvoir discriminatoire de telle information a des limites, car il décrit la forme avec moins de précision.
Pour cette raison, nous avons considéré dans le cadre d'une approche analytique, utilisant une segmentation implicite, d'implanter de la haute information an le réduisant à un niveau plus bas.
Cet enrichissement peut être perçu comme un poids au niveau de pixel, donnant une importance à chaque pixel analysé fondé sur leurs propriétés conceptuelles.
Le défi est de combiner les types différents des caractéristiques considérant une certaine dépendance entre eux.
Pour réduire le temps de décodage dans la recherche de Viterbi, un mécanisme de seuil cumulatif est proposé dans une représentation de vocabulaire plate.
Au lieu de l'utilisation d'une représentation de trie où les parties de préfixe communes sont partagées nous proposons un mécanisme de seuil dans le vocabulaire plat où basé juste sur une analyse de Viterbi partielle, nous pouvons élaguer un modèle et arrêtons le traitant plus.
Les seuils cumulatifs sont fondés sur les scores correspondants prémédités à chaque niveau de lettre, permettant une certaine dynamique et élasticité au modèle.
Comme nous sommes intéressés dans un système de reconnaissance d'adresses postaux complet, nous avons convergé aussi notre attention sur la reconnaissance des chiffres, proposant différent solutions neuronaux et stochastiques.
Pour augmenter la précision et la robustesse des classifieur, un stratégie de combinaison est aussi proposé.
Les programmes sont partout dans notre vie quotidienne : les ordinateurs et les téléphones, mais aussi les frigo, les avions et ainsi de suite.
L'acteur principal dans la création de ces programmes est humain les êtres.
Aussi minutie qu'ils peuvent être, les humains sont connus pour faire des erreurs involontaires sans leur conscience.
Toute la durée de leur tâche de développement, les développeurs doivent faire face continuellement leurs erreurs (ou leurs collègues).
Cette observation clé soulève la nécessité d'aider les développeurs dans leurs tâches de développement / maintenance.
Que ce soit pour des professionnels qui doivent prendre connaissance du contenu de documents en un temps limité ou pour un particulier désireux de se renseigner sur un sujet donné sans disposer du temps nécessaire pour lire l'intégralité des textes qui en traitent, le résumé est une aide contextuelle importante.
Avec l'augmentation de la masse documentaire disponible électroniquement, résumer des textes automatiquement est devenu un axe de recherche important dans le domaine du traitement automatique de la langue.
La présente thèse propose une méthode de résumé automatique multi-documents fondée sur une classification des phrases à résumer en classes sémantiques.
Cette classification nous permet d'identifier les phrases qui présentent des éléments d'informations similaires, et ainsi de supprimer efficacement toute redondance du résumé généré.
Cette méthode a été évaluée sur la tâche # résumé d'opinions issues de blogs # de la campagne d'évaluation TAC 2008 et la tâche # résumé incrémental de dépêches # des campagnes TAC 2008 et TAC 2009.
Les résultats obtenus sont satisfaisants, classant notre méthode dans le premier quart des participants.
Nous avons également proposé d'intégrer la structure des dépêches à notre système de résumé automatique afin d'améliorer la qualité des résumés qu'il génère.
Pour finir, notre méthode de résumé a fait l'objet d'une intégration à un système applicatif visant à aider un possesseur de corpus à visualiser les axes essentiels et à en retirer automatiquement les informations importantes.
Selon la représentation d'entrée, cette thèse étudie ces deux types : la génération de texte à partir de représentation de sens et à partir de texte.
En la première partie (Génération des phrases), nous étudions comment effectuer la réalisation de surface symbolique à l'aide d'une grammaire robuste et efficace.
Cette approche s'appuie sur une grammaire FB-LTAG et prend en entrée des arbres de dépendance peu profondes.
Afin nous proposons deux algorithmes de fouille d'erreur : le premier, un algorithme qui exploite les arbres de dépendance plutôt que des données séquentielles et le second, un algorithme qui structure la sortie de la fouille d'erreur au sein d'un arbre afin de représenter les erreurs de façon plus pertinente.
Nous montrons que nos réalisateurs combinés à ces algorithmes de fouille d'erreur améliorent leur couverture significativement.
En la seconde partie (Simplification des phrases), nous proposons l'utilisation d'une forme de représentations sémantiques (contre à approches basées la syntaxe ou SMT) afin d'améliorer la tâche de simplification de phrase.
Nous utilisons les structures de représentation du discours pour la représentation sémantique profonde.
Nous proposons alors deux méthodes de simplification de phrase : une première approche supervisée hybride qui combine une sémantique profonde à de la traduction automatique, et une seconde approche non-supervisée qui s'appuie sur un corpus comparable de Wikipedia
La sclérose en plaques (SEP) est une maladie chronique du système nerveux central, principale cause de handicap d'origine non traumatique chez l'adulte jeune.
Il se caractérise par de nombreux processus de démyélinisation inflammatoire qui provoquent une vaste gamme de symptômes, notamment des déficits cognitifs et invalidité irréversible.
L'imagerie par résonance magnétique (IRM) est aujourd'hui l'outil de référence pour le diagnostic de la maladie.
En particulier, de nouvelles approches basées sur la représentation d'images IRM utilisant la théorie des graphes ont été appliquées avec succès pour l'étude et la quantification des dommages à la substance blanche.
Grâce à leur capacité à analyser d'énormes quantités de données et à identifier les relations latentes, ce domaine de l'intelligence artificielle a connu un assez grand succès dans la communauté scientifique et s'applique désormais dans de nombreux contextes, notamment le diagnostic médical.
Dans ce manuscrit, nous présenterons les différentes techniques d'apprentissage profond développées dans ce travail concernant l'analyse des images biomédicales et, en particulier, pour la classification et la caractérisation des patients atteints de SEP.
En fait, la théorie des graphes est devenue un outil sensible pour la détection des altérations causées par les pathologies cérébrales, et peut être combinée à des techniques d'apprentissage automatique afin d'identifier les propriétés structurelles latentes utiles pour étudier la progression de la maladie.
La première partie de ce manuscrit est consacré à la description de l'état de l'art.
Cet état de l'art se focalisera sur les études montrant les effets de la SEP sur les faisceaux de SB grâce à l'emploi de l'imagerie de tenseur de diffusion.
Une description des principales techniques d'apprentissage profond sera également fournie, ainsi que des exemples d'applicabilité dans le contexte biomédical.
Dans la seconde partie, deux techniques d'apprentissage profond seront proposées, concernant la génération de nouvelles images IRM du cerveau humain et l'identification automatique du disque optique dans les images du fond oculaire.
Dans la troisième partie, nous présenterons les techniques d'apprentissage profond combinées à la théorie des graphiques que développée dans ce travail pour étudier la connectivité structurelle des patients atteints d'une SEP.
Enfin, nous explorerons des approches semi-supervisées et non supervisées pour réduire l'intervention humaine dans les processus de décision
Ces graphes peuvent ainsi servir de base à l'évaluation automatisée des risques, en s'appuyant sur l'identification et l'évaluation des actifs essentiels.
Cela permet de concevoir des contre-mesures proactives et réactives pour la réduction des risques et peut être utilisé pour la surveillance et le renforcement de la sécurité du réseau.
Cette thèse vise à appliquer une approche similaire dans les environnements Cloud, ce qui implique de prendre en compte les nouveaux défis posés par ces infrastructures modernes, la majorité des graphes d'attaque étant conçue pour une application dans des environnements traditionnels.
Les nouveaux scénarios d'attaque liés à la virtualisation, ainsi que les propriétés inhérentes du Cloud, à savoir l'élasticité et le caractère dynamique, sont quelques-uns des obstacles à franchir à cette fin.
Ainsi, pour atteindre cet objectif, un inventaire complet des vulnérabilités liées à la virtualisation a été effectué, permettant d'inclure cette nouvelle dimension dans les graphes d'attaque existants.
Par l'utilisation d'un modèle adapté à l'échelle du Cloud, nous avons pu tirer parti des technologies Cloud et SDN, dans le but de construire des graphes d'attaque et de les maintenir à jour.
Des algorithmes capables de faire face aux modifications fréquentes survenant dans les environnements virtualisés ont été conçus et testés à grande échelle sur une plateforme Cloud réelle afin d'évaluer les performances et confirmer la validité des méthodes proposées dans cette thèse pour permettre à l'administrateur de Cloud de disposer d'un graphe d'attaque à jour dans cet environnent.
L'analyse des images satellite et aériennes figure parmi les sujets fondamentaux du domaine de la télédétection.
Ces dernières années, les avancées technologiques ont permis d'augmenter la disponibilité à large échelle des images, en comprenant parfois de larges étendues de terre à haute résolution spatiale.
En plus des questions évidentes de complexité calculatoire qui en surgissent, un de plus importants défis est l'énorme variabilité des objets dans les différentes régions de la terre.
Pour aborder cela, il est nécessaire de concevoir des méthodes de classification qui dépassent l'analyse du spectre individuel de chaque pixel, en introduisant de l'information contextuelle de haut niveau.
Dans cette thèse, nous proposons d'abord une méthode pour la classification avec des contraintes de forme, basée sur l'optimisation d'une structure de subdivision hiérarchique des images.
Nous explorons ensuite l'utilisation des réseaux de neurones convolutionnels (CNN), qui nous permettent d'apprendre des descripteurs hiérarchiques profonds.
Nous étudions les CNN depuis de nombreux points de vue, ce qui nous permettra de les adapter à notre objectif.
Parmi les sujets abordés, nous proposons différentes solutions pour générer des cartes de classification à haute résolution et nous étudions aussi la récolte des données d'entrainement.
Nous avons également créé une base de données d'images aériennes sur des zones variées, pour évaluer la capacité de généralisation des CNN.
Finalement, nous proposons une méthode pour polygonaliser les cartes de classification issues des réseaux de neurones, afin de pouvoir les intégrer dans des systèmes d'information géographique.
Au long de la thèse, nous conduisons des expériences sur des images hyperspectrales, satellites et aériennes, toujours avec l'intention de proposer des méthodes applicables, généralisables et qui passent à l'échelle.
Cette thèse propose une description synchronique du système de détermination nominale du créole réunionnais, ainsi qu'une analyse de l'interprétation des syntagmes nominaux (SN).
Elle inclut des données nouvelles provenant d'une part d'un ensemble de corpus oraux et d'autre part de jugements de grammaticalité et de félicité.
Nous examinons la distribution des différents SN, le statut morphosyntaxique des éléments figurant en position pré- et postnominale, le système du nombre, ainsi que l'expression de la définitude en réunionnais.
Cette thèse propose une étude de l'alternance entre do it, this and do that dans leur emploi comme anaphores verbales (Verb Phrase anaphors, VPAs), où ils renvoient à une action saillante soit évoquée précédemment dans le discours, (la plus souvent via un SV) soit, par exophore, à une action saillante dans la situation discursive mais non évoquée explicitement dans le discours précédent.
Do it/this/that ont été peu étudiés dans la littérature par ailleurs conséquente sur l'anaphore and et en particulier l'ellipse du VP (VP ellipsis, VPE, par ex. Kim knows the answer and Pat does too).
En effet, on a longtemps considéré que ces trois constructions sont interchangeables entre elles ainsi qu'avec do so et l'ellipse, de sorte qu'un examen détaillé de leur propriétés discursives n'a pas été jugé utile.
Les exemples ci-dessous montrent que cette supposition est incorrecte : en (1), un exemple attesté tiré du BNC, do this/that/so pourraient être employés au lieu de do it, mais en (3), do that est nettement préféré.
S'agissant de l'ellipse, elle est peu naturelle en (1) et préfère un contexte comme celui en (2).
1.They've been rescuing companies for so long they do it automatically now, I expect. (AB9, ok : they do this/that/so automatically…) 2. They've been rescuing companies for so long that whenever they do, it's always a success. 3. He closes his eyes when he speaks and I don't trust anyone who does that. (AHF ; …anyone who #does this/#it/#so)
A partir d'un échantillon d'exemples annotés du British National corpus (BNC, Davies 2004-), notre étude examinera les facteurs qui entrent en jeu dans l'alternance entre do it/this/that.
Le choix entre les VPAs est déterminé entre autres par le registre, la présence d'un circonstant après l'anaphore, la mention ou non de l'antécédent avant la phrase antécédent, et dans une moindre mesure, la saillance de l'antécédent et la familiarité supposée qu'en a le destinataire.
Do it renvoie en général a des actions très saillantes qui sont ensuite décrites plus en détail par le biais d'un circonstant.
Do that est employé le plus souvent sans circonstant, et son usage présente parfois de grandes similarités avec l'ellipse.
L'apprentissage – principe vital de l'évolution – assure la transformation des données primaires captées par nos sens en connaissances utiles ou idées abstraites et générales, exploitables dans de nouvelles situations et contextes.
Les neurosciences cognitives montrent que les mécanismes de l'apprentissage reposent sur l'engagement cognitif (e.g. se questionner, évaluer ses erreurs), physique (e.g. manipuler, bouger) et social (e.g. débattre, collaborer).
L'apprenant construit ses connaissances par l'expérience, en explorant son environnement, en formulant des hypothèses et en expérimentant.
Apprendre est crucial dans un contexte où l'évolution exponentielle des technologies de l'information et de la communication change les objets, les pratiques et les usages.
Le développement de l'Internet des Objets (IdO) transforme les objets physiques du quotidien (e.g. ampoule, montre, voiture) en objets connectés (OC) pouvant collecter des données et agir sur l'environnement de l'usager.
L'apprentissage devient aussi bien biologique qu'artificiel et permet de créer des systèmes d'Intelligence artificielle (SIA) analysant de grands volumes de données pour automatiser des tâches et assister les individus.
Les technologies peuvent favoriser l'apprentissage, lorsque les possibilités techniques qu'elles offrent sont utilisées pour soutenir le processus de construction de connaissances.
Ainsi, cette thèse porte sur l'apprentissage dans le contexte de l'IdO et examine la manière dont les spécificités des OC peuvent s'articuler avec les mécanismes de l'apprentissage.
Afin d'identifier les caractéristiques de l'apprentissage dans le contexte de l'IdO, nous avons étudié les usages existants d'OC.
En s'appuyant sur l'état de l'art, nous avons proposé un outil conceptuel décrivant l'IdO au travers de quatre dimensions d'analyse : Données, Interfaces, Agents et Pervasivité.
Cet outil nous a permis d'identifier, de répertorier, de classer et, in fine, d'analyser les usages d'OC au service de l'apprentissage.
Dans le cadre ces usages, l'apprentissage est caractérisé par l'engagement physique, la contextualisation des savoirs et le rapprochement des activités pédagogiques avec la réalité.
En valorisant les résultats de ce premier travail, nous avons élaboré une approche pour mettre les spécificités des OC au service de l'apprentissage des sciences.
L'aspect abstrait et souvent contre-intuitif des savoirs scientifiques freine leur apprentissage, en partie car notre perception de la réalité est subjective et limitée par nos sens.
Or, les données collectées par les OC et analysées par des SIA apportent des informations sur l'environnement pouvant être utilisées pour étendre la perception humaine.
Ainsi, l'objectif de notre approche, traduite par le modèle Données-Représentations-Interactions (DRI), vise à exploiter les OC et les SIA pour faciliter l'observation de phénomènes physiques.
Selon le modèle DRI, l'apprenant interagit avec des représentations d'un phénomène physique générées à partir d'OC et de SIA.
En accord avec les mécanismes de l'apprentissage (e.g. constructivisme, rôle de l'expérience), l'apprenant est amené à faire des observations et des manipulations, à formuler des hypothèses et à les tester.
Afin d'évaluer les effets et les contraintes du modèle DRI, nous avons conçu les dispositifs LumIoT dédiés à l'apprentissage des grandeurs photométriques (e.g. flux lumineux, intensité lumineuse, éclairement).
Puis, nous avons conduit une expérimentation avec 17 étudiants du Master 1 Produits et Services Multimédia de l'Université de Franche-Comté (site de Montbéliard).
Les résultats de l'expérimentation montrent que les dispositifs LumIoT, basés sur le modèle DRI, ont facilité l'observation et la compréhension des grandeurs photométriques.
En rendant accessibles des savoirs abstraits, le modèle DRI ouvre la voie à des dispositifs d'apprentissage mettant les OC et les SIA au service de la médiation des savoirs.
Cette thèse fait suite à une étude récente, menée par quelques chercheurs de l'Université de Montpellier, dans le but de proposer à la communauté scientifique une procédure d'inversion capable d'estimer de manière non invasive la pression dans les artères cérébrales d'un patient.
Son premier objectif est, d'une part, d'examiner la précision et la robustesse de la procédure d'inversion proposée par ces chercheurs, en lien avec diverses sources d'incertitude liées aux modèles utilisés, aux hypothèses formulées et aux données cliniques du patient, et d'autre part, de fixer un critère d'arrêt pour l'algorithme basé sur le filtre de Kalman d'ensemble utilisé dans leur procédure d'inversion.
À cet effet, une analyse d'incertitude et plusieurs analyses de sensibilité sont effectuées.
Le second objectif est d'illustrer comment l'apprentissage machine, orienté réseaux de neurones convolutifs, peut être une très bonne alternative à la longue et coûteuse procédure mise en place par ces chercheurs pour l'estimation de la pression.
Une approche prenant en compte les incertitudes liées au traitement des images médicales du patient et aux hypothèses formulées sur les modèles utilisés, telles que les hypothèses liées aux conditions limites, aux paramètres physiques et physiologiques, est d'abord présentée pour quantifier les incertitudes sur les résultats de la procédure.
Les incertitudes liées à la segmentation des images sont modélisées à l'aide d'une distribution gaussienne et celles liées au choix des hypothèses de modélisation sont analysées en testant plusieurs scénarios de choix d'hypothèses possibles.
De cette démarche, il ressort que les incertitudes sur les résultats de la procédure sont du même ordre de grandeur que celles liées aux erreurs de segmentation.
Par ailleurs, cette analyse montre que les résultats de la procédure sont très sensibles aux hypothèses faites sur les conditions aux limites du modèle du flux sanguin.
En particulier, le choix des conditions limites symétriques de Windkessel pour le modèle s'avère être le plus approprié pour le cas du patient étudié.
Ensuite, une démarche permettant de classer les paramètres estimés à l'aide de la procédure par ordre d'importance et de fixer un critère d'arrêt pour l'algorithme utilisé dans cette procédure est proposée.
Les résultats de cette stratégie montrent, d'une part, que la plupart des résistances proximales sont les paramètres les plus importants du modèle pour l'estimation du débit sanguin dans les carotides internes et, d'autre part, que l'algorithme d'inversion peut être arrêté dès qu'un certain seuil de convergence raisonnable de ces paramètres les plus influents est atteint.
Enfin, une nouvelle plateforme numérique basée sur l'apprentissage machine permettant d'estimer la pression artérielle spécifique au patient dans les artères cérébrales beaucoup plus rapidement qu'avec la procédure d'inversion mais avec la même précision, est présentée.
L'application de cette plateforme aux données du patient utilisées dans la procédure d'inversion permet une estimation non invasive et en temps réel de la pression dans les artères cérébrales du patient cohérente avec l'estimation de la procédure d'inversion.
Contexte : Un des enjeux majeurs du traitement de pathologies neuro-psychatriques est le suivi des patients chroniques afin de mesurer les rechutes précoces mais également l'observance et l'adhérence des patients au traitement.
Un tel suivi est possible grâce à l'utilisation de dispositifs médicaux connectés (mesurant par exemple le poids, la tension artérielle ou l'activité physique) mais des informations cruciales sur le ressenti de la maladie sont complexes à mesurer et nécessitent des entretiens verbaux réguliers entre le médecin et le malade.
L'augmentation du nombre de patients à suivre engendre des files d'attente résultant souvent en des suivis épisodiques avec des consultations trop espacées.
Outre les entretiens cliniques, il est toutefois également possible de mesurer certains symptômes (ex : tristesse ou somnolence) avec différentes méthodes : en examinant les mouvements oculaires, avec des mesures électro-encéphalographiques, ou en observant les expressions verbales et/ou corporelles.
Grâce aux récentes avancées dans le domaine du traitement automatique de la parole, il est désormais possible de détecter des indices précis dans la voix permettant la caractérisation de l'état des patients (par exemple pour mesurer le degré de somnolence).
Cette méthode possède les avantages suivants : l'enregistrement de données vocales n'est pas invasif car il ne nécessite pas l'équipement de capteurs spécifiques, ni de calibration complexe.
Elle peut donc être mise en place dans des environnements variés, hors conditions de laboratoire et autorise ainsi un suivi plus régulier et moins contraignant des patients.
Nous souhaitons dans ce projet mettre en place une solution utilisant l'analyse automatique de la parole qui, couplée à des algorithmes d'intelligence artificielle, permettra de définir de nouveaux biomarqueurs utilisables pour un suivi personnalisé à domicile de la qualité de vie des patients.
Objectifs : Depuis 2016, nous avons envisagé la possibilité d'utiliser l'analyse vocale pour le suivi de patients souffrant de somnolence excessive.
Cette étude, effectuée en collaboration entre le laboratoire SANPSY USR 6231 et le LaBRI UMR 5800, est actuellement soutenue par la Région Nouvelle Aquitaine dans le cadre du projet IS-OSA.
Nous avons ainsi pu mettre en place un protocole d'enregistrement nous permettant d'être à ce jour les seules équipes disposant d'une base de données audio en français ainsi que des diagnostics cliniques associés à chaque patient.
Lors cette étude préliminaire, nous avons mis au point une méthode d'analyse utilisant un nombre réduit de paramètres interprétables tout en conservant un niveau de performance élevé.
Nous projetons la publication très prochaine de ces recherches dans une revue internationale.
Ces résultats prometteurs ouvrent la voie à des améliorations du système nécessaires pour lever les verrous scientifiques du projet :
1. Etudier la cohérence des différentes mesures cliniques de somnolence et leur lien éventuel avec différents paramètres vocaux.
2. Optimiser la recherche des paramètres vocaux pertinents et les lier avec des symptômes médicaux 3.
Augmenter la base de données afin de permettre la mise au point d'un système de classification performant utilisant l'intelligence artificielle via des réseaux de neurones profonds.
4. Intégrer le système au compagnon virtuel mis au point par SANPSY et le tester en conditions réelles au domicile des patients.
La méthode textométrique, en revanche, met en œuvre un ensemble de modèles statistiques permettant l'analyse des distributions de mots dans de vastes corpus, afin faire émerger les caractéristiques significatives des données textuelles.
Dans cette recherche, la textométrie, traditionnellement considérée comme étant incompatible avec la fouille par l'extraction, est substituée à cette dernière pour obtenir des informations sur des événements économiques dans le discours.
Plusieurs analyses textométriques (spécificités et cooccurrences) sont donc menées sur un corpus de flux de presse numérisé.
On constate que chacune des approches contribuent différemment au traitement des données textuelles, produisant toutes deux des analyses complémentaires.
À l'issue de la comparaison est exposé l'apport des deux méthodes de fouille pour la veille d'événements.
La découverte d'unités linguistiques élémentaires (phonèmes, mots) uniquement à partir d'enregistrements sonores est un problème non-résolu qui suscite un fort intérêt de la communauté du traitement automatique de la parole, comme en témoignent les nombreuses contributions récentes de l'état de l'art.
Durant cette thèse, nous nous sommes concentrés sur l'utilisation de réseaux de neurones pour répondre au problème.
Nous avons approché le problème en utilisant les réseaux de neurones de manière supervisée, faiblement supervisée et multilingue.
Nous avons ainsi développé des outils de segmentation automatique en phonèmes et de classification phonétique fondés sur des réseaux de neurones convolutifs.
L'outil de segmentation automatique a obtenu 79% de F-mesure sur le corpus de parole conversationnelle en anglais BUCKEYE.
Ce résultat est similaire à un annotateur humain d'après l'accord inter-annotateurs fourni par les créateurs du corpus.
De plus, il n'a pas besoin de beaucoup de données (environ une dizaine de minutes par locuteur et 5 locuteurs différents) pour être performant.
De plus, il est portable à d'autres langues (notamment pour des langues peu dotées telle que le xitsonga).
Le système de classification phonétique permet de fixer les différents paramètres et hyperparamètres utiles pour un scénario non supervisé.
Dans le cadre non supervisé, les réseaux de neurones (Auto-Encodeurs) nous ont permis de générer de nouvelles représentations paramétriques, concentrant l'information de la trame d'entrée et ses trames voisines.
Nous avons étudié leur utilité pour la compression audio à partir du signal brut, pour laquelle ils se sont montrés efficaces (faible taux de RMS, même avec une compression de 99%).
Nous avons également réalisé une pré-étude novatrice sur une utilisation différente des réseaux de neurones, pour générer des vecteurs de paramètres non pas à partir des sorties des couches mais des valeurs des poids des couches.
Ces paramètres visent à imiter les coefficients de prédiction linéaire (Linear Predictive Coefficients, LPC).
Dans le contexte de la découverte non supervisée d'unités similaires à des phonèmes (dénommées pseudo-phones dans ce mémoire) et la génération de nouvelles représentations paramétriques phonétiquement discriminantes, nous avons couplé un réseau de neurones avec un outil de regroupement (k-means).
L'alternance itérative de ces deux outils a permis la génération de paramètres phonétiquement discriminants pour un même locuteur : de faibles taux d'erreur ABx intra-locuteur de 7,3% pour l'anglais, 8,5% pour le français et 8,4% pour le mandarin ont été obtenus.
Ces résultats permettent un gain absolu d'environ 4% par rapport à la baseline (paramètres classiques MFCC) et sont proches des meilleures approches actuelles (1% de plus que le vainqueur du Zero Ressource Speech Challenge 2017).
Les résultats inter-locuteurs varient entre 12% et 15% suivant la langue, contre 21% à 25% pour les MFCC.
Les travaux présentés dans cette thèse concernent la modélisation des aspects socioculturels et temporels pour permettre aux communautés sénégalaises departager et de co-construire leur connaissances socioculturelles.
En effet, avecla mondialisation la nouvelle génération sénégalaise a de moins en moins de connaissances sur son environnement socioculturel.
Ainsi, nous avons initié la mise en place d'une application en ligne pour permettre à nos concitoyens de partager et de co-construire leur patrimoine socioculturel.
Nos propositions s'appuient sur les technologies du Web social et du Web sémantique.
En effet, le Web social propose un cadre à tout utilisateur pour participer à la création de contenu.
Le Web sémantique rend accessible les ressources aux agents logiciels pour une meilleure recherche et partage d'informations.
La combinaison de ces deux technologies permet aux communautés sénégalaises de partager et de co-construire leur patrimoine culturel dans un cadre collaboratif et sémantique.
Nos contributions consistent à (i) proposer des ontologies pour annoter des ressources socioculturelles et (ii) proposer un cadre collaboratif aux communautés sénégalaises.
Les ontologies représentent le socle du Web sémantique et permettent de caractériser un domaine.
Ainsi, nous avons défini : 1) une ontologie socioculturelle reposant sur la théorie historicoculturelle de l'activité et 2) une ontologie temporelle.
Nous avons également défini les communautés de co-élaboration de connaissances culturelles et proposé un prototype qui intègre les différentes contributions.
Les corpus, collections de textes sélectionnés dans un objectif spécifique, occupent une place de plus en plus déterminante en Linguistique comme en Traitement Automatique des Langues (TAL).
Considérés à la fois comme source de connaissances sur l'usage authentique des langues, ou sur les entités que désignent des expressions linguistiques, ils sont notamment employés pour évaluer la performance d'applications de TAL.
Les critères qui prévalent à leur constitution ont un impact évident, mais encore délicat à caractériser, sur (i) les structures linguistiques majeures qu'ils renferment, (ii) les connaissances qui y sont véhiculées, et, (iii) la capacité de systèmes informatiques à accomplir une tâche donnée.
Ce mémoire étudie des méthodologies d'extraction automatique de relations sémantiques dans des corpus de textes écrits.
Un tel sujet invite à examiner en détail le contexte dans lequel une expression linguistique s'applique, à identifier les informations qui déterminent son sens, afin d'espérer relier des unités sémantiques.
Généralement, la modélisation du contexte est établie à partir de l'analyse de co-occurrence d'informations linguistiques issues de ressources ou obtenues par des systèmes de TAL.
Les intérêts et limites de ces informations sont évalués dans le cadre de la tâche d'extraction de relations sur des corpus de genre différent (article de presse, conte, biographie).
Les résultats obtenus permettent d'observer que pour atteindre une représentation sémantique satisfaisante ainsi que pour concevoir des systèmes robustes, ces informations ne suffisent pas.
Deux problèmes sont particulièrement étudiés.
D'une part, il semble indispensable d'ajouter des informations qui concernent le genre du texte.
Pour caractériser l'impact du genre sur les relations sémantiques, une méthode de classification automatique, reposant sur les restrictions sémantiques qui s'exercent dans le cadre de relations verbo-nominales, est proposée.
La méthode est expérimentée sur un corpus de conte et un corpus de presse.
D'autre part, la modélisation du contexte pose des problèmes qui relèvent de la variation discursive de surface.
Un texte ne met pas toujours bout à bout des expressions linguistiques en relation et il est parfois nécessaire de recourir à des algorithmes complexes pour détecter des relations à longue portée.
Pour répondre à ce problème de façon cohérente, une méthode de segmentation discursive, qui s'appuie sur des indices de structuration de surface apparaissant dans des corpus écrits, est proposée.
Elle ouvre le champ à la conception de grammaires qui permettent de raisonner sur des catégories d'ordre macro-syntaxique afin de structurer la représentation discursive d'une phrase.
Cette méthode est appliquée en amont d'une analyse syntaxique et l'amélioration des performances est évaluée.
Les solutions proposées à ces deux problèmes nous permettent d'aborder l'extraction d'information sous un angle particulier : le système implémenté est évalué sur une tâche de correction d'Entités Nommées dans le contexte d'application des Systèmes de Question-Réponse.
Ce besoin spécifique entraîne l'alignement de la définition d'une catégorie sur le type de réponse attendue par une question.
Dans le cadre d'une montée en version logicielle, d'une migration technique ou encore de l'implémentation de connecteurs afin d'assurer une communication avec des services tiers, il est crucial de pouvoir conserver l'ensemble des données clients d'une application, d'assurer leur cohérence et de les communiquer selon des structures (ou méta-modèles) hétérogènes non connus à l'avance.
Dans un tel contexte, il est donc important de savoir efficacement gérer la 'conversion'des données d'un méta-modèle à un autre.
Cette thèse s'intéresse à l'implémentation d'une transformation de modèle automatisée afin d'assurer une interopérabilité fédérée : il s'agit d'étudier l'inférence à la volée de règles permettant de convertir des données structurées selon un méta-modèle source afin de les adapter à un méta-modèle cible.
Pour cela, la thèse vise à étudier les techniques issues de plusieurs disciplines telles que la sémantique, l'apprentissage automatique et le traitement du langage naturel ou encore l'exploitation de base de données de type graphe via des algorithmes d'optimisation par exemple.
Cette thèse est financée par la société Forterro Sylob qui amènera en outre des cas d'étude réels.
Ce document rassemble les travaux effectués durant mes années de thèse.
Je commence par une présentation concise des résultats principaux, puis viennent trois parties relativement indépendantes.
Dans la première partie, je considère des problèmes d'inférence statistique sur un échantillon i.i.d. issu d'une loi inconnue à support dénombrable.
Le premier chapitre est consacré aux propriétés de concentration du profil de l'échantillon et de la masse manquante.
Il s'agit d'un travail commun avec Stéphane Boucheron et Mesrob Ohannessian.
Après avoir obtenu des bornes sur les variances, nous établissons des inégalités de concentration de type Bernstein, et exhibons un vaste domaine de lois pour lesquelles le facteur de variance dans ces inégalités est tendu.
Le deuxième chapitre présente un travail en cours avec Stéphane Boucheron et Elisabeth Gassiat, concernant le problème de la compression universelle adaptative d'un tel échantillon.
Nous établissons des bornes sur la redondance minimax des classes enveloppes, et construisons un code quasi-adaptatif sur la collection des classes définies par une enveloppe à variation régulière.
Dans la deuxième partie, je m'intéresse à des marches aléatoires sur des graphes aléatoires à degrés precrits.
Je présente d'abord un résultat obtenu avec Justin Salez, établissant le phénomène de cutoff pour la marche sans rebroussement.
Sous certaines hypothèses sur les degrés, nous déterminons précisément le temps de mélange, la fenêtre du cutoff, et montrons que le profil de la distance à l'équilibre converge vers la fonction de queue gaussienne.
Puis je m'intéresse à la comparaison des temps de mélange de la marche simple et de la marche sans rebroussement.
Enfin, la troisième partie est consacrée aux propriétés de concentration de tirages pondérés sans remise et correspond à un travail commun avec Yuval Peres et Justin Salez.
Pour prévoir la production, les systèmes de surveillance de la sécurité alimentaire doivent être renseignés par des données sur les surfaces cultivées et sur le rendement.
Ces données peuvent être estimées par les systèmes d'observations satellitaires à moyenne résolution spatiale, qui, par leur vision synoptique, constituent une source d'information particulièrement adéquate.
En Afrique de l'Ouest, l'estimation des surfaces cultivées par télédétection reste cependant problématique en raison d'un domaine cultivé fragmenté, d'une grande hétérogénéité spatiale due aux conditions environnementales et aux pratiques culturales, et de la synchronisation des phénologies des agrosystèmes et des écosystèmes liée au régime des précipitations.
Dans ce contexte, cette thèse présente, en trois volets, des développements méthodologiques originaux pour la caractérisation des systèmes agricoles d'Afrique de l'Ouest par télédétection.
Les méthodes ont été développées à partir de séries temporelles MODIS (250 m à 500 m de résolutionspatiale) acquises sur le Mali.
(i) La cartographie des surfaces cultivées a été réalisée à partir d'indices spectraux, spatiaux, texturaux et temporels dérivés des images.
Deux approches ont été appliquées : une approche de type ISODATA consécutive à une segmentation du territoire basée sur les images MODIS et une approche de fouille de données basée sur des « motifs séquentiels » .
Les produits cartographiques obtenus présentent une meilleure précision que les produits globaux « occupation du sol » existants (70% vs 50% en moyenne).
Cependant, une part importante des erreurs d'omission et de commission (de 20% à 40%) reste incompressible en raison de la fragmentation du domaine cultivé.
(ii) La cartographie des types de systèmes agricoles a nécessité un premier travail de typologie effectué à partir d'une BD d'enquêtes de terrain de l'Institut d'Economie Rurale de Bamako sur 100 villages.
Trois types de systèmes agricoles ont été déterminés à l'échelle du village : céréales dominantes (mil, sorgho), cultures intensives dominantes (maïs, coton) et mélange de sorgho et de coton.
La classification des systèmes agricoles à partir des indicateurs de télédétection précédemment cités a été produite par un algorithme de type Random Forest avec une précision globale de 60%.
Les résultats mettent en évidence une combinaison optimale d'indicateurs comprenant le NDVI ainsi que la texture pour la caractérisation des systèmes agricoles.
(iii) Enfin, pour le suivi des cultures, le produit phénologique MODIS a été testé et évalué à partir de variables phénologiques obtenues par simulations agro-météorologiques du modèle de plante SARRA-H.
Les résultats montrent que ce produit comporte des incohérences dues au fort ennuagement de début de saison des pluies.
Après suppression des données aberrantes, on montre que les dates de transition phénologique des surfaces cultivées issues de MODIS sont plus précoces de 20 jours comparées aux sorties du modèle de culture, en raison notamment de la nature mixte « agro-écosystème » des surfaces à l'échelle du pixel MODIS.
Les résultats de cette thèse permettent de dégager de nouvelles pistes de couplage entre télédétection, données de terrain et modélisation agro-météorologique en apportant une information continue dans le temps et dans l'espace sur la caractérisation du domaine cultivé au « Sahel » .
Si la parole est une faculté dont l'usage nous semble parfaitement naturel,il reste toutefois beaucoup à comprendre sur la nature des représentations et des processus cognitifs qui la gouvernent.
Au cœur de cette thèse se trouve la question des interactions entre perception et action dans la production et la perception de syllabes.
Nous adoptons le cadre rigoureux de la programmation bayésienne au sein duquel nous définissons mathématiquement le modèle COSMO (pour "Communicating Objects using Sensori-Motor Operations"), qui permet de formaliser les théories motrice, auditive et perceptuo-motrice de la communication parlée et de les étudier quantitativement.
Cette approche conduit à un premier résultat théorique fort : nous démontrons un théorème d'indistinguabilité d'après lequel, lorsque l'on pose certaines hypothèses de conditions idéales d'apprentissage, les théories auditive et motrice font des prédictions identiques pour des tâches de perception, et sont de ce fait indistinguables.
Cet algorithme d'apprentissage par imitation de ciblesacoustiques permet l'apprentissage de compétences motrices à partir d'entrées perceptives uniquement, avec la propriété remarquable de se focaliser sur les régions d'intérêt pour l'apprentissage.
Nous utilisons des syllabes synthétisées grâce au modèle de conduit vocal VLAM pour analyser les dynamiques d'évolution des modèles appris ainsi que leur robustesse aux dégradations.
Les données métagénomiques du microbiome humain constituent une nouvelle source de données pour améliorer le diagnostic et le pronostic des maladies humaines.
L'apprentissage automatique a obtenu de grandes réalisations sur d'importants problèmes de métagénomique liés au regroupement d'OTU, à l'assignation taxonomique, etc.
La contribution de cette thèse est multiple : 1) un cadre de sélection de caractéristiques pour approche pour prédire les maladies à l'aide de représentations d'images artificielles.
La première contribution, qui est une approche efficace de sélection de caractéristiques basée sur les capacités de visualisation de la carte auto-organisée, montre une précision de classification raisonnable par rapport aux méthodes de pointe.
La seconde approche vise à visualiser les données métagénomiques en utilisant une méthode simple de remplissage, ainsi que des approches d'apprentissage de réduction dimensionnelle.
La nouvelle représentation des données métagénomiques peut être considérée comme une image synthétique et utilisée comme un nouvel ensemble de données pour une méthode efficace d'apprentissage en profondeur.
Les résultats montrent que les méthodes proposées permettent d'atteindre des performances prédictives à la pointe de la technologie ou de les surpasser sur des benchmarks métagénomiques riches en public.
Les sites web de critiques en ligne aident les utilisateurs à décider quoi acheter ou quels hôtels choisir.
Ces plateformes permettent aux utilisateurs d'exprimer leurs opinions à l'aide d'évaluations numériques et de commentaires textuels.
Les notes numériques donnent une idée approximative du service.
D'autre part, les commentaires textuels donnent des détails complets, ce qui est fastidieux à lire.
Dans cette thèse, nous développons de nouvelles méthodes et algorithmes pour générer des résumés personnalisés de critiques de films, basés sur les aspects, pour un utilisateur donné.
Le premier problème que nous abordons consiste à extraire un ensemble de mots liés à un aspect des critiques de films.
Notre évaluation montre que notre méthode est capable d'extraire même des termes impopulaires qui représentent un aspect, tels que des termes composés ou des abréviations.
Nous étudions ensuite le problème de l'annotation des phrases avec des aspects et proposons une nouvelle méthode qui annote les phrases en se basant sur une similitude entre la signature d'aspect et les termes de la phrase.
Le troisième problème que nous abordons est la génération de résumés personnalisés, basés sur les aspects.
Nous proposons un algorithme d'optimisation pour maximiser la couverture des aspects qui intéressent l'utilisateur et la représentativité des phrases dans le résumé sous réserve de contraintes de longueur et de similarité.
Enfin, nous réalisons trois études d'utilisateur qui montrent que l'approche que nous proposons est plus performante que la méthode de pointe en matière de génération de résumés.
Le chapitre 1 sert d'introduction à la thèse, pose les problématiques et les méthodes, remet en perspective les enjeux et annonce le plan suivi.
Le chapitre 2 définit les principaux types de disfluences (cliniques et naturelles), résume les études principales conduites sur les disfluences, et présente les différents points de vue sur leur rôle dans le discours.
Le chapitre 3 dresse l'état de la question sur le statut des deux pauses pleines (fillers) um et uh et montre comment plusieurs études récentes accréditent l'idée d'une différence pragmatique, voire fonctionnelle, entre ces deux "fillers", qu'il convient donc d'envisager comme des marqueurs.
Le chapitre 5 caractérise les deux corpus étudiés, ATAROS et Switchboard (SWB), et établit leurs contributions.
Ce chapitre présente les méthodologies d'annotation des corpus, les deux versions de SWB, ainsi que la méthode suivie pour construire une interopérabilité de ces deux corpus pour l'analyse de um et uh.
Le chapitre 6 analyse la distribution et la durée des deux marqueurs dans SWB et ATAROS en fonction du genre des interlocuteurs, de l'authenticité de la conversation, et du nombre de conversations auxquelles les sujets participent.
Ce chapitre montre que um et uh ont des durées et des distributions différentes et indique que les marqueurs ne sont pas utilisés au hasard.
Le chapitre 7 se penche sur la production de um et uh dans SWB, et sur la perception des deux marqueurs en comparant les deux versions des transcriptions du corpus.
Les principaux résultats montrent que um et uh sont plus souvent oublis que d'autres mots fréquents tels que les mots fonctionnels, et que les transcripteurs de SWB font plus d'erreurs sur uh que sur um, suggérant que um joue un rôle discursif plus important que uh.
Le chapitre 8 interroge la relation entre la prise de position ("stance") d'une unité de parole et la présence et la position des marqueurs dans une phrase, et révèle que ces deux dimensions sont dépendantes.
Le chapitre 9 évalue la relation entre la prise de position d'une unité de parole et la réalisation acoustique de la voyelle des marqueurs, comparé à la même voyelle dans d'autres mots monosyllabiques.
Les résultats indiquent que les valeurs de "stance" affectent avec différents degrés la réalisation acoustique des marqueurs.
Le chapitre 10 incorpore les résultats des expériences précédentes dans plusieurs taches de classification qui testent les traits les plus importants pour prédire automatiquement les valeurs de "stance" en fonction des paramètres correspondants à um et uh (traits lexicaux, positionnels et acoustiques).
Les résultats aussi indiquent que différentes propriétés acoustiques améliorent les scores de prédictions.
Le chapitre 11 conclut la thèse en résumant les résultats des chapitres 6 à 10, en soulignant les impacts de cette recherche, et en indiquant les futures pistes de recherche.
Dans cette thèse, nous collectons sur le web deux types de connaissances.
Le premier porte sur le sens commun, i.e. des connaissances intuitives partagées par la plupart des gens comme " le ciel est bleu ".
Nous utilisons des logs de requêtes et des forums de questions-réponses pour extraire des faits essentiels grâce à des questions avec une forme particulière.
Ensuite, nous validons nos affirmations grâce à d'autres ressources comme Wikipedia, Google Books ou les tags d'images sur Flickr.
Finalement, nous groupons tous les signaux pour donner un score à chaque fait.
Nous obtenons une base de connaissance, QUASIMODO, qui, comparée à ses concurrents, montre une plus grande précision et collecte plus de faits essentiels.
Le deuxième type de connaissances qui nous intéresse sont les connaissances cachées, i.e. qui ne sont pas directement données par un fournisseur de données.
En effet, les services web donnent généralement un accès partiel à l'information.
Il faut donc combiner des méthodes d'accès pour obtenir plus de connaissances : c'est de la réécriture de requête.
Dans un premier scénario, nous étudions le cas où les fonctions ont la forme d'un chemin, la base de donnée est contrainte par des " dépendences d'inclusion unitaires " et les requêtes sont atomiques.
Nous montrons que le problème est alors décidable en temps polynomial.
Ensuite, nous retirons toutes les contraites et nous créons un nouvelle catégorie pertinente de plans : les " smart plans ".
Nous montrons qu'il est décidable de les trouver.
Cette thèse est consacrée à l'étude d'accents régionaux en français, à partir de deux grands corpus de parole face à face (PFC) et de parole téléphonique.
Nous avons étudié la perception humaine et les caractéristiques acoustiques de différentes variétés de français (français standard, français du sud de la France, d'Alsace, de Belgique et de Suisse) afin de les modéliser dans leurs aspects segmentaux (articulation des phonèmes) et prosodiques (accentuation et intonation).
Dans un premier temps, des tests perceptifs ont permis d'évaluer quels accents sont distingués par des auditeurs français.
Les résultats ont été analysés par des techniques de clustering et de scaling.
Dans un second temps, nous avons mesuré des paramètres acoustiques (formants, fréquence fondamentale, durée et intensité) en nous appuyant sur les frontières temporelles des segments phonémiques fournies par un système d'alignement standard, ce qui nous a également permis de dégager certains patrons prosodiques spécifiques.
Nous avons en outre introduit des variantes dans le dictionnaire de prononciation utilisé pour l'alignement, afin d'observer les variantes choisies par le système.
Ces deux méthodes ont permis de mettre en évidence un certain nombre de traits pertinents concernant la réalisation des voyelles nasales, l'antériorisation du /O/, le dévoisement des consonnes sonores et l'accentuation initiale.
Les réseaux de neurones profonds récents possèdent de nombreuses couches cachées ce qui augmente significativement le nombre total de paramètres.
L'apprentissage de ce genre de modèles nécessite donc un grand nombre d'exemples étiquetés, qui ne sont pas toujours disponibles en pratique.
Le sur-apprentissage est un des problèmes fondamentaux des réseaux de neurones, qui se produit lorsque le modèle apprend par coeur les données d'apprentissage, menant à des difficultés à généraliser sur de nouvelles données.
Le problème du sur-apprentissage des réseaux de neurones est le thème principal abordé dans cette thèse.
Dans la littérature, plusieurs solutions ont été proposées pour remédier à ce problème, tels que l'augmentation de données, l'arrêt prématuré de l'apprentissage ("early stopping"), ou encore des techniques plus spécifiques aux réseaux de neurones comme le "dropout" ou la "batch normalization".
Dans cette thèse, nous abordons le sur-apprentissage des réseaux de neurones profonds sous l'angle de l'apprentissage de représentations, en considérant l'apprentissage avec peu de données.
Pour aboutir à cet objectif, nous avons proposé trois différentes contributions.
La première contribution, présentée dans le chapitre 2, concerne les problèmes à sorties structurées dans lesquels les variables de sortie sont à grande dimension et sont généralement liées par des relations structurelles.
Notre proposition vise à exploiter ces relations structurelles en les apprenant de manière non-supervisée avec des autoencodeurs.
Notre approche a montré une accélération de l'apprentissage des réseaux et une amélioration de leur généralisation.
La deuxième contribution, présentée dans le chapitre 3, exploite la connaissance a priori sur les représentations à l'intérieur des couches cachées dans le cadre d'une tâche de classification.
Cet à priori est basé sur la simple idée que les exemples d'une même classe doivent avoir la même représentation interne.
Nous avons formalisé cet à priori sous la forme d'une pénalité que nous avons rajoutée à la fonction de perte.
Des expérimentations empiriques sur la base MNIST et ses variantes ont montré des améliorations dans la généralisation des réseaux de neurones, particulièrement dans le cas où peu de données d'apprentissage sont utilisées.
Notre troisième et dernière contribution, présentée dans le chapitre 4, montre l'intérêt du transfert d'apprentissage ("transfer learning") dans des applications dans lesquelles peu de données d'apprentissage sont disponibles.
Dans cette application, la tâche consiste à localiser la troisième vertèbre lombaire dans un examen de type scanner.
L'utilisation du transfert d'apprentissage ainsi que de prétraitements et de post traitements adaptés a permis d'obtenir des bons résultats, autorisant la mise en oeuvre du modèle en routine clinique.
Les systèmes de recommandation sont conçus dans une variété d'applications pour aider à la prise de décision.
Dans un environnement collaboratif, le système de recommandation peut guider la collaboration.
Les utilisateurs laissent des traces d'interaction lorsqu'ils collaborent sur une plateforme numérique.
Ces traces peuvent être analysées pour détecter les signaux forts et les signaux faibles d'une collaboration.
Cette thèse porte sur la mise en œuvre d'un système de recommandation exploitant les traces de collaboration dans un environnement informatique.
Les travaux réalisés ont été testés au sein de la plateforme web collaborative E-MEMORAe.
Ce travail vise à présenter des méthodes de construction des dictionnaires électroniques de séquences nominales figées du coréen et de leurs formes fléchies, et à justifier leur validité en appliquant notre dictionnaire dans les domaines appliqués de l'analyse automatique de textes coréens.
En vue de la reconnaissance des séquences nominales figées par dictionnaire, nous avons classé celles-ci en trois catégories selon les conventions typographiques : noms compacts (NC), noms figés à espacement facultatif (NFF) et noms figés à espacement obligatoire (NFO).
Puisque des formes fléchies des séquences nominales figées apparaissent dans les textes coréens, nous avons construit, d'une part, un dictionnaire électronique des NFF à 45000 entrées et d'autre part, un transducteur des séquences de postpositions nominales avec leur segmentation, et enfin fusionné ces deux ensembles de données à partir de codes flexionnels associés à chaque entrée et de la fonctionnalité de flexion d'INTEX.
Notre dictionnaire construit d'après ces méthodes a les principaux avantages suivants par rapport aux systèmes préexistants : 1)
Le dictionnaire des formes fléchies de NFF permet la reconnaissance automatique de toutes les variantes de NFF liées à l'espacement 2)
Le dictionnaire des formes fléchies de NFF permet la segmentation des formes fléchies des NFF en un NFF et une séquence de postpositions nominales 3)
Le dictionnaire des séquences de postpositions nominales sous forme de graphes permet leur segmentation en postpositions nominales 4)
Le dictionnaire des NFF sert à la segmentation des séquences nominales libres soudées 5)
Le dictionnaire des NFF peut être étendu en un dictionnaire bilingue pour la traduction automatique 6) Chaque entrée du dictionnaire de NFF comporte des codes utiles pour les applications dans le traitement automatique : codes indiquant un trait sémantique, le statut de nom prédicatif, le nom tête de chaque entrée, l'origine et la catégorie grammaticale
Nous avons traité dans cette étude de différents types de prédicats nominaux réciproques ou noms de réciprocité dans la perspective de décrire de manière explicite leurs propriétés syntactico-sémantiques.
Il s'agit de noms qui se trouvent dans la construction de type "Il y a Npréd entre A et B" mettant en relation au moins deux éléments (humains, concrets, idées, etc.), ces derniers étant obligatoires ;
ils peuvent permuter dans la phrase sans que cette opération provoque de changement de sens.
Notre description a comme objectif de contribuer à un objectif plus général qui consiste à établir une typologie sémantique d'une partie du lexique à savoir des noms prédicatifs et ce, dans le but d'un traitement automatique de la langue.
Ces dernières années ont témoigné du succès du projet Linked Open Data (LOD) et de la croissance du nombre de sources de données sémantiques disponibles sur le web.
Cependant, il y a encore beaucoup de données qui ne sont pas encore mises à disposition dans le LOD telles que les données sur demande, les données de capteurs etc.
Elles sont néanmoins fournies par des API des services Web.
L'intégration de ces données au LOD ou dans des applications de mashups apporterait une forte valeur ajoutée.
Cependant, chercher de tels services avec les outils de découverte de services existants nécessite une connaissance préalable des répertoires de services ainsi que des ontologies utilisées pour les décrire.
Dans cette thèse, nous proposons de nouvelles approches et des cadres logiciels pour la recherche de services web sémantiques avec une perspective d'intégration de données.
Premièrement, nous introduisons LIDSEARCH, un cadre applicatif piloté par SPARQL pour chercher des données et des services web sémantiques.
Afin d'atteindre ce but, nous utilisons des techniques de traitement automatique de la langue et d'appariement de textes basées sur le deep-learning pour mieux comprendre les descriptions des services.
La thèse aborde un sujet très nouveau à fort enjeu ; les premières publications sont des prépublications qui émanent de grandes compagnies.
Prédire des variables cibles structurées dans un cadre rigoureux et générique n'est pas l'option choisie actuellement par ces grands groupes.
Au delà des possibles applications que nous prévoyons en biologie computationnelle, de nombreuses autres instances de problèmes de prédiction structurée peuvent aussi être résolues : prédiction de profil client /utilisateur, prédiction d'activités et de comportement, émargeant ainsi sur les thèmes de la société numérique.
Dans cette thèse nous testons l'effet de différents types d'entraînements sur l'acquisition des monophtongues /æ, ʌ, ɑː, ɪ, et iː/ de l'anglais par des apprenants francophones adultes.
Des mesures comportementales sont réalisées avant et après entraînements afin d'évaluer les changements sur la perception et la production de ces voyelles.
De plus, une étude en potentiels évoqués a été menée afin de mettre en évidence des corrélats neuronaux consécutifs à l'entraînement du contraste /æ-ʌ/.
1) le premier, du type High Variability Phonetic Training (HVPT) comportait des tâches classiques d'identification et de discrimination avec feedback sur la justesse des réponses.
Cet entraînement était proposé à 16 participants constituant le groupe PE.
En perception, les résultats montrent que l'entraînement HVPT améliore significativement les performances des apprenants en identification et en discrimination comparativement au groupe C, alors que l'amélioration du groupe PR est limitée à l'identification.
Les performances en production ont été évaluées par le biais de deux méthodes :
1) une méthode « objective » basée sur l'analyse des paramètres acoustiques des voyelles.
2) une méthode « subjective » basée sur une tâche d'identification des voyelles produites réalisée par des juges natifs.
L'entraînement de type HVPT semble être le plus enclin à améliorer les performances en production des apprenants.
Cependant, il a été observé que l'amélioration en production était restreinte à l'intelligibilité telle qu'évaluée par des locuteurs natifs et que l'accent étranger mesuré par le biais de mesures acoustiques n'était pas significativement réduit dans les groupes expérimentaux comparés au groupe contrôle.
L'étude en électroencéphalographie n'a pas permis d'observer les corrélats attendus mais nous discutons les améliorations potentielles à apporter à notre protocole.
L'apprentissage faiblement supervisé cherche à réduire au minimum l'effort humain requis pour entrainer les modèles de l'état de l'art.
Toutefois, dans la pratique, les méthodes faiblement supervisées sont nettement moins efficaces que celles qui sont totalement supervisées.
Plus particulièrement, dans l'apprentissage profond, où les approches de vision par ordinateur sont les plus performantes, elles restent entièrement supervisées, ce qui limite leurs utilisations dans les applications du monde réel.
Cette thèse tente tout d'abord de combler le fossé entre les méthodes faiblement supervisées et entièrement supervisées en utilisant l'information de mouvement.
Puis étudie le problème de la segmentation des objets en mouvement, en proposant l'une des premières méthodes basées sur l'apprentissage pour cette tâche.
Le défi est de capturer de manières précises les bordures des objets et d'éviter les optimums locaux (ex : segmenter les parties les plus discriminantes).
Contrairement à la plupart des approches de l'état de l'art, qui reposent sur des images statiques, nous utilisons les données vidéo avec le mouvement de l'objet comme informations importantes.
Notre méthode utilise une approche de segmentation vidéo de l'état de l'art pour segmenter les objets en mouvement dans les vidéos.
Les masques d'objets approximatifs produits par cette méthode sont ensuite fusionnés avec le modèle de segmentation sémantique appris dans un EM-like framework, afin d'inférer pour les trames vidéo, des labels sémantiques au niveau des pixels.
Ainsi, au fur et à mesure que l'apprentissage progresse, la qualité des labels s'améliore automatiquement.
Nous intégrons ensuite cette architecture à notre approche basée sur l'apprentissage pour la segmentation de la vidéo afin d'obtenir un framework d'apprentissage complet pour l'apprentissage faiblement supervisé à partir de vidéos.
Dans la deuxième partie de la thèse, nous étudions la segmentation vidéo non supervisée, plus précisément comment segmenter tous les objets dans une vidéo qui se déplace indépendamment de la caméra.
De nombreux défis tels qu'un grand mouvement de la caméra, des inexactitudes dans l'estimation du flux optique et la discontinuité du mouvement, complexifient la tâche de segmentation.
Nous abordons le problème du mouvement de caméra en proposant une méthode basée sur l'apprentissage pour la segmentation du mouvement : un réseau de neurones convolutif qui prend le flux optique comme entrée et qui est entraîné pour segmenter les objets qui se déplacent indépendamment de la caméra.
Il est ensuite étendu avec un flux d'apparence et un module de mémoire visuelle pour améliorer la continuité temporelle.
Le flux d'apparence tire profit de l'information sémantique qui est complémentaire de l'information de mouvement.
Le module de mémoire visuelle est un paramètre clé de notre approche : il combine les sorties des flux de mouvement et d'apparence et agréger une représentation spatio-temporelle des objets en mouvement.
La segmentation finale est ensuite produite à partir de cette représentation agrégée.
L'approche résultante obtient des performances de l'état de l'art sur plusieurs jeux de données de référence, surpassant la méthode d'apprentissage en profondeur et heuristique simultanée.
La conception concourante de produits matériels centrée sur l'homme est basée sur une collaboration entre le concepteur mécanicien, l'ergonome et le designer industriel.
Cette collaboration souvent difficile peut être facilitée par l'utilisation d'objets intermédiaires de conception, tels que la Réalité Virtuelle (RV).
Néanmoins, bien que largement utilisée dans l'industrie, la RV souffre d'un déficit d'acceptation de la part des concepteurs de produits.
Dans le cadre de ces travaux, nous proposons d'utiliser la RV sous la forme d'outils immersifs d'assistance à la convergence multidisciplinaire développés selon une démarche anthropocentrée en fonction des besoins spécifiques à chaque projet de conception de produits.
Afin d'optimiser les délais de développement, nous proposons une méthodologie de conception d'applications immersive dédiée : la méthodologie ASAP (As Soon As Possible).
Une première série expérimentale a été conduite dans le cadre de contrats industriels d'études et de recherche afin de valider la faisabilité de la méthodologie et l'efficacité des outils développés.
Une deuxième série expérimentale a été effectuée sur plus de 50 sujets dans le cadre de projets, cette fois, pédagogiques qui ont nécessité le développement de 12 applications.
Elle a permis de valider quantitativement l'influence des outils immersifs sur l'efficacité perçue des phases de convergence interdisciplinaires ainsi que l'influence de l'approche proposée sur l'acceptation de la RV par les concepteurs de produits.
Ces travaux de thèse présentent une première approche qui, selon nous, permettra à terme, de faire évoluer l'usage de la RV vers une intégration plus forte au sein des processus de conception de produits avec, par exemple, une plus large utilisation des applications immersives de modélisation 3D, réelles sources d'innovation.
L'annotation manuelle de corpus est devenue un enjeu fondamental pour le Traitement Automatique des Langues (TAL).
En effet, les corpus annotés sont utilisés aussi bien pour créer que pour évaluer des outils de TAL.
Or, le processus d'annotation manuelle est encore mal connu et les outils proposés pour supporter ce processus souvent mal utilisés, ce qui ne permet pas de garantir le niveau de qualité de ces annotations.
Nous proposons dans cette thèse une vision unifiée de l'annotation manuelle de corpus pour le TAL.
Ce travail est le fruit de diverses expériences de gestion et de participation à des campagnes d'annotation, mais également de collaborations avec différents chercheur(e)s.
Nous proposons dans un premier temps une méthodologie globale pour la gestion de campagnes d'annotation manuelle de corpus qui repose sur deux piliers majeurs : une organisation des campagnes d'annotation qui met l'évaluation au coeur du processus et une grille d'analyse des dimensions de complexité d'une campagne d'annotation.
Un second volet de notre travail a concerné les outils du gestionnaire de campagne.
Nous avons pu évaluer l'influence exacte de la pré-annotation automatique sur la qualité et la rapidité de correction humaine, grâce à une série d'expériences menée sur l'annotation morpho-syntaxique de l'anglais.
Nous avons également apporté des solutions pratiques concernant l'évaluation de l'annotation manuelle, en donnant au gestionnaire les moyens de sélectionner les mesures les plus appropriées.
Enfin, nous avons mis au jour les processus en oeuvre et les outils nécessaires pour une campagne d'annotation et instancié ainsi la méthodologie que nous avons décrite.
La tâche de segmentation et de regroupement en locuteurs (speaker diarization) consiste à identifier "qui parle quand" dans un flux audio sans connaissance a priori du nombre de locuteurs ou de leur temps de parole respectifs.
Les systèmes de segmentation et de regroupement en locuteurs sont généralement construits en combinant quatre étapes principales.
Premièrement, les régions ne contenant pas de parole telles que les silences, la musique et le bruit sont supprimées par la détection d'activité vocale (VAD).
Ensuite, les régions de parole sont divisées en segments homogènes en locuteur par détection des changements de locuteurs, puis regroupées en fonction de l'identité du locuteur.
Enfin, les frontières des tours de parole et leurs étiquettes sont affinées avec une étape de re-segmentation.
Dans cette thèse, nous proposons d'aborder ces quatre étapes avec des approches fondées sur les réseaux de neurones.
Au stade du regroupement des régions de parole, nous proposons d'utiliser l'algorithme de propagation d'affinité à partir de plongements neuronaux de ces tours de parole dans l'espace vectoriel des locuteurs.
Des expériences sur un jeu de données télévisées montrent que le regroupement par propagation d'affinité est plus approprié que le regroupement hiérarchique agglomératif lorsqu'il est appliqué à des plongements neuronaux de locuteurs.
La segmentation basée sur les réseaux récurrents et la propagation d'affinité sont également combinées et optimisées conjointement pour former une chaîne de regroupement en locuteurs.
Comparé à un système dont les modules sont optimisés indépendamment, la nouvelle chaîne de traitements apporte une amélioration significative.
De plus, nous proposons d'améliorer l'estimation de la matrice de similarité par des réseaux neuronaux récurrents, puis d'appliquer un partitionnement spectral à partir de cette matrice de similarité améliorée.
Le système proposé atteint des performances à l'état de l'art sur la base de données de conversation téléphonique CALLHOME.
Pour mieux comprendre le comportement du système, une analyse basée sur une architecture de codeur-décodeur est proposée.
Sur des exemples synthétiques, nos systèmes apportent une amélioration significative par rapport aux méthodes de regroupement traditionnelles.
La présente recherche, qui s'inscrit à la croisée de la linguistique, de la didactique des langues et du traitement automatique des langues, se focalise sur la problématique d'un côté du classement des supports textuels utilisés dans le cadre de l'apprentissage du français langue étrangère (FLE) et de l'autre côté de l'analyse des divers critères nécessaires à ce classement.
Notre analyse se base principalement sur le critère lexical, déterminant dans le processus de la lecture.
Ainsi, nous nous sommes interrogé, à partir d'un échantillon de supports textuels, préalablement sélectionnés dans les manuels de FLE, si une telle classification est viable.
De ce fait, on a procédé à l'analyse de ces textes du point de vue de leur correspondance avec les niveaux établis par le Cadre européen commun de référence(CECR) (niveaux A1 à B2) et cela à l'aide des inventaires développés dans les Référentiels pour le français.
La première partie développe une réflexion sur l'interdisciplinarité, les supports textuels, les typologies des textes et les référentiels, en essayant de voir à quel point l'approche par le lexique se justifie afin de devenir le critère principal d'une telle classification.
La seconde partie nous amène à l'analyse de la correspondance des supports textuels avec les niveaux du CECR à partir du critère lexical, à l'aide d'outils informatiques, et cela dans la perspective d'étudier quelles sont les possibilités de pouvoir élaborer un corpus dynamique des supports textuels utilisable dans le cadre du FLE.
De nos jours, il existe de nombreuses applications liées à la vision et à l'audition visant à reproduire par des machines les capacités humaines.
Notre intérêt pour ce sujet vient du fait que ces problèmes sont principalement modélisés par la classification de signaux temporels.
En fait, nous nous sommes intéressés à deux cas distincts, la reconnaissance de la démarche humaine et la reconnaissance de signaux audio, (notamment environnementaux et musicaux).
Dans le cadre de la reconnaissance de la démarche, nous avons proposé une nouvelle méthode qui apprend et sélectionne automatiquement les parties dynamiques du corps humain.
Ceci permet de résoudre le problème des variations intra-classe de façon dynamique;
Nous proposons la prise en compte d'indices linguistiques et discursifs variés et faisant appel à des niveaux d'analyses différents.
L'obsolescence étant un phénomène non linguistique, notre hypothèse est qu'il faut considérer les indices linguistiques et discursifs en termes de combinaisons.
Un système d'apprentissage automatique est ensuite mis en place afin de faire émerger les configurations d'indices pertinentes dans les segments obsolescents caractérisés par les experts.
Nos objectifs sont remplis : nous proposons une description fine de l'obsolescence dans notre corpus de textes encyclopédiques et ainsi qu'un prototype logiciel d'aide à la mise à jour des textes.
Une double évaluation a été menée : par validation croisée sur le corpus d'apprentissage et par les experts sur un corpus de test.
Les radiologues utilisent au quotidien des solutions d'imagerie médicale pour le diagnostic.
L'amélioration de l'expérience utilisateur est toujours un axe majeur de l'effort continu visant à améliorer la qualité globale et l'ergonomie des produits logiciels.
Les applications de monitoring permettent en particulier d'enregistrer les actions successives effectuées par les utilisateurs dans l'interface du logiciel.
Ces interactions peuvent être représentées sous forme de séquences d'actions.
Sur la base de ces données, ce travail traite de deux sujets industriels : les pannes logicielles et l'ergonomie des logiciels.
Ces deux thèmes impliquent d'une part la compréhension des modes d'utilisation, et d'autre part le développement d'outils de prédiction permettant soit d'anticiper les pannes, soit d'adapter dynamiquement l'interface logicielle en fonction des besoins des utilisateurs.
Pour ce faire, nous proposons d'utiliser un test binomial afin de déterminer quel type de pattern est le plus approprié pour représenter les signatures de crash.
L'amélioration de l'expérience utilisateur par la personnalisation et l'adaptation des systèmes aux besoins spécifiques de l'utilisateur exige une très bonne connaissance de la façon dont les utilisateurs utilisent le logiciel.
Afin de mettre en évidence les tendances d'utilisation, nous proposons de regrouper les sessions similaires.
Nous comparons trois types de représentation de session dans différents algorithmes de clustering.
La deuxième contribution de cette thèse concerne le suivi dynamique de l'utilisation du logiciel.
Les deux méthodologies tirent parti de la structure récurrente des réseaux LSTM pour capturer les dépendances entre nos données séquentielles ainsi que leur capacité à traiter potentiellement différents types de représentations d'entrée pour les mêmes données.
Ce mémoire de thèse de doctorat présente, discute et propose des outils de fouille automatique de mégadonnées dans un contexte de classification supervisée musical.
L'application principale concerne la classification automatique des thèmes musicaux afin de générer des listes de lecture thématiques.
Le premier chapitre introduit les différents contextes et concepts autour des mégadonnées musicales et de leur consommation.
Le deuxième chapitre s'attelle à la description des bases de données musicales existantes dans le cadre d'expériences académiques d'analyse audio.
Ce chapitre introduit notamment les problématiques concernant la variété et les proportions inégales des thèmes contenus dans une base, qui demeurent complexes à prendre en compte dans une classification supervisée.
Le troisième chapitre explique l'importance de l'extraction et du développement de caractéristiques audio et musicales pertinentes afin de mieux décrire le contenu des éléments contenus dans ces bases de données.
Ce chapitre explique plusieurs phénomènes psychoacoustiques et utilise des techniques de traitement du signal sonore afin de calculer des caractéristiques audio.
De nouvelles méthodes d'agrégation de caractéristiques audio locales sont proposées afin d'améliorer la classification des morceaux.
Le quatrième chapitre décrit l'utilisation des caractéristiques musicales extraites afin de trier les morceaux par thèmes et donc de permettre les recommandations musicales et la génération automatique de listes de lecture thématiques homogènes.
Cette partie implique l'utilisation d'algorithmes d'apprentissage automatique afin de réaliser des tâches de classification musicale.
Les contributions de ce mémoire sont résumées dans le cinquième chapitre qui propose également des perspectives de recherche dans l'apprentissage automatique et l'extraction de caractéristiques audio multi-échelles.
Notre approche linguistique a été implémentée sur machine à l'aide d'un générateur de système experts "snark".
Fouille des opinion, une sous-discipline dans la recherche d'information (IR) et la linguistique computationnelle, fait référence aux techniques de calcul pour l'extraction, la classification, la compréhension et l'évaluation des opinions exprimées par diverses sources de nouvelles en ligne, social commentaires des médias, et tout autre contenu généré par l'utilisateur.
Il est également connu par de nombreux autres termes comme trouver l'opinion, la détection d'opinion, l'analyse des sentiments, la classification sentiment, de détection de polarité, etc.
Définition dans le contexte plus spécifique et plus simple, fouille des opinion est la tâche de récupération des opinions contre son besoin aussi exprimé par l'utilisateur sous la forme d'une requête.
Il y a de nombreux problèmes et défis liés à l'activité fouille des opinion.
Dans cette thèse, nous nous concentrons sur quelques problèmes d'analyse d'opinion.
Les fournisseurs de services géo-localisés (LBS) offrent des données textuelles et spatiales complémentaires, parfois incohérentes et imprécises, représentant les différents points d'intérêt (POI) sur un territoire donné.
De plus, chaque fournisseur utilise sa propre convention graphique pour représenter les POIs.
Le développement croissant des réseaux et en particulier l'Internet a considérablement développé l'écart entre les systèmes d'information hétérogènes.
En faisant une analyse sur les études de l'interopérabilité des systèmes d'information hétérogènes, nous découvrons que tous les travaux dans ce domaine tendent à la résolution des problèmes de l'hétérogénéité sémantique.
Le W3C (World Wide Web Consortium) propose des normes pour représenter la sémantique par l'ontologie.
L'ontologie est en train de devenir un support incontournable pour l'interopérabilité des systèmes d'information et en particulier dans la sémantique.
La structure de l'ontologie est une combinaison de concepts, propriétés et relations.
Cette combinaison est aussi appelée un graphe sémantique.
Les langages OWL (Ontology Web Language) et RDF (Resource Description Framework) sont les langages les plus importants du web sémantique, ils sont basés sur XML.
Le RDF est la première norme du W3C pour l'enrichissement des ressources sur le Web avec des descriptions détaillées et il augmente la facilité de traitement automatique des ressources Web.
Les descriptions peuvent être des caractéristiques des ressources, telles que l'auteur ou le contenu d'un site web.
Ces descriptions sont des métadonnées.
Enrichir le Web avec des métadonnées permet le développement de ce qu'on appelle le Web Sémantique.
Le RDF est aussi utilisé pour représenter les graphes sémantiques correspondant à une modélisation des connaissances spécifiques.
Les fichiers RDF sont généralement stockés dans une base de données relationnelle et manipulés en utilisant le langage SQL ou les langages dérivés comme SPARQL.
Malheureusement, cette solution, bien adaptée pour les petits graphes RDF n'est pas bien adaptée pour les grands graphes RDF.
Ces graphes évoluent rapidement et leur adaptation au changement peut faire apparaître des incohérences.
Conduire l'application des changements tout en maintenant la cohérence des graphes sémantiques est une tâche cruciale et coûteuse en termes de temps et de complexité.
Un processus automatisé est donc essentiel.
Pour ces graphes RDF de grande taille, nous suggérons une nouvelle façon en utilisant la vérification formelle « Le Model checking » .
Le Model checking est une technique de vérification qui explore tous les états possibles du système.
De cette manière, on peut montrer qu'un modèle d'un système donné satisfait une propriété donnée.
Cette thèse apporte une nouvelle méthode de vérification et d'interrogation de graphes sémantiques.
Nous proposons une approche nommé ScaleSem qui consiste à transformer les graphes sémantiques en graphes compréhensibles par le model checker (l'outil de vérification de la méthode Model checking).
Il est nécessaire d'avoir des outils logiciels permettant de réaliser la traduction d'un graphe décrit dans un formalisme vers le même graphe (ou une adaptation) décrit dans un autre formalisme
Le sujet des travaux concerne l'amélioration du comportement des machines dites \og intelligentes\fg, c'est-à-dire capables de s'adapter à leur environnement, même lorsque celui-ci évolue.
Un des domaines concerné est celui des interactions homme-machine.
La machine doit alors gérer différents types d'incertitude pour agir de façon appropriée.
D'abord, elle doit pouvoir prendre en compte les variations de comportements entre les utilisateurs et le fait que le comportement peut varier d'une utilisation à l'autre en fonction de l'habitude à interagir avec le système.
De plus, la machine doit s'adapter à l'utilisateur même si les moyens de communication entre lui et la machine sont bruités.
L'objectif est alors de gérer ces incertitudes pour exhiber un comportement cohérent.
Une manière habituelle pour gérer les incertitudes passe par l'introduction de modèles : modèles de l'utilisateur, de la tâche, ou encore de la décision.
Un inconvénient de cette méthode réside dans le fait qu'une connaissance experte liée au domaine concerné est nécessaire à la définition des modèles.
Si l'introduction d'une méthode d'apprentissage automatique, l'apprentissage par renforcement a permis d'éviter une modélisation de la décision \textit{ad hoc} au problème concerné, des connaissances expertes restent toutefois nécessaires.
La thèse défendue par ces travaux est que certaines contraintes liées à l'expertise humaine peuvent être relaxées tout en limitant la perte de généricité liée à l'introduction de modèles
Cette thèse s'intéresse particulièrement aux sites de vente en ligne et à leurs réseaux sociaux.
La propension des utilisateurs utiliser ces sites Web tels qu'eBay et Amazon est de plus en plus importante en raison de leur fiabilité.
Les consommateurs se réfèrent à ces sites Web pour leurs besoins et en deviennent clients.
L'un des défis à relever est de fournir les informations utiles pour aider les clients dans leurs achats.
Ainsi, une question sous-jacente à la thèse cherche à répondre est de savoir comment fournir une information complète aux clients afin de les aider dans leurs achats.
C'est important pour les sites d'achats en ligne car cela satisfait les clients par ces informations utiles.
Pour ce faire, les utilisateurs sont classés en fonction de deux scores : optimiste et pessimiste.
Dans la deuxième partie, une nouvelle méthodologie de propagation de l'opinion est présentée pour parvenir à un accord et maintenir la cohérence entre les utilisateurs, ce qui rend l'agrégation possible.
La propagation se fait en tenant compte des impacts des utilisateurs influents et des voisins.
Enfin, dans la troisième partie, l'agrégation des avis est proposée pour rassembler les avis existants et les présenter comme des informations utiles pour les clients concernant chaque produit du site de vente en ligne.
Pour ce faire, l'opérateur de calcul de la moyenne pondérée et les techniques floues sont utilisées.
La thèse présente un modèle d'opinion consensuelle dans les réseaux.
Les travaux peuvent s'appliquer à tout groupe qui a besoin de trouver un avis parmi les avis de ses membres.
Cette thèse en didactique des langues étrangères montre le potentiel des corpus pour l'apprentissage de l'allemand au collège.
Conformément aux objectifs poursuivis dans une recherche-action, elle renseigne sur l'évolution des acteurs impliqués dans un dispositif spécifique, en tenant compte de facteurs multiples, tels que le type d'input, les outils, les modes de travail - en présentiel et à distance - et la relation pédagogique.
J'ai observé les effets d'un scénario pédagogique, basé sur l'exploitation de corpus spécialisés pour la création de textes du domaine du tourisme.
J'ai analysé leurs discours et leurs actions face à l'input, et j'ai décrit l'évolution de certains aspects linguistiques et discursifs de leurs textes.
Le projet et les outils proposés ont soutenu le développement de modes de lecture qui ont favorisé la recherche ciblée d'informations dans l'input.
Le recyclage d'éléments repérés dans l'input a mené, pour une partie des apprenants, à l'obtention de productions écrites dont les caractéristiques linguistiques et discursives sont proches de celles attestées dans les corpus.
L'écriture intertextuelle a permis aux participants, à des degrés variables, de se décentrer et de progresser dans l'apprentissage de l'altérité.
L'observation de formes linguistiques à l'aide de lignes de concordances et l'intégration de collocations identifiées dans l'input ont contribué au développement du système linguistique, en particulier dans le domaine de la flexion des adjectifs.
La tâche de Segmentation et Regroupement en Locuteurs (SRL), telle que définie par le NIST, considère le traitement des enregistrements d'un corpus comme des problèmes indépendants.
Les enregistrements sont traités séparément, et le tauxd'erreur global sur le corpus correspond finalement à une moyenne pondérée.
Dans ce contexte, les locuteurs détectés par le système sont identifiés par des étiquettes anonymes propres à chaque enregistrement.
Un même locuteur qui interviendrait dans plusieurs enregistrements sera donc identifié par des étiquettes différentes selon les enregistrements.
Cette situation est pourtant très fréquente dans les émissions journalistiques d'information : les présentateurs, les journalistes et autres invités qui animent une émission interviennent généralement de manière récurrente.
En conséquence, la tâche de SRL a depuis peu été considérée dans un contexte plus large, où les locuteurs récurrents doivent être identifiés de manière unique dans tous les enregistrements qui composent un corpus.
Cette généralisation du problème de regroupement en locuteurs va de pair avec l'émergence du concept de collection, qui se réfère, dans le cadre de la SRL, à un ensemble d'enregistrements ayant une ou plusieurs caractéristiques communes.
Le travail proposé dans cette thèse concerne le regroupement en locuteurs sur des collections de documents audiovisuels volumineuses (plusieurs dizaines d'heures d'enregistrements).
L'objectif principal est de proposer (ou adapter) des approches de regroupement afin de traiter efficacement de gros volumes de données, tout en détectant les locuteurs récurrents.
L'efficacité des approches proposées est étudiée sous deux aspects : d'une part, la qualité des segmentations produites (en termes de taux d'erreur), et d'autre part, la durée nécessaire pour effectuer les traitements.
Nous proposons à cet effet deux architectures adaptées au regroupement en locuteurs sur des collections de documents.
Nous proposons une approche de simplification où le problème de regroupement est représenté par une graphe non-orienté.
La décompositionde ce graphe en composantes connexes permet de décomposer le problème de regroupement en un certain nombre de sous-problèmes indépendants.
Une chaîne de coréférences est l'ensemble des expressions linguistiques — ou mentions — qui font référence à une même entité ou un même objet du discours.
La tâche de reconnaissance des chaînes de coréférences consiste à détecter l'ensemble des mentions d'un document et à le partitionner en chaînes de coréférences.
Ces chaînes jouent un rôle central dans la cohérence des documents et des interactions et leur identification est un enjeu important pour de nombreuses autres tâches en traitement automatique du langage, comme l'extraction d'informations ou la traduction automatique.
Des systèmes automatiques de reconnaissance de chaînes de coréférence existent pour plusieurs langues, mais aucun pour le français ni pour une langue parlée.
Nous nous proposons dans cette thèse de combler ce manque par un système de reconnaissance automatique de chaînes de coréférences pour le français parlé.
À cette fin, nous proposons un système utilisant des réseaux de neurones artificiels et ne nécessitant pas de resources externes. Ce système est viable malgré le manque d'outils de prétraitements adaptés au français parlé et obtient des performances comparable à l'état de l'art.
Nous proposons également des voies d'amélioration de ce système, en y introduisant des connaissances issues de ressources et d'outils conçus pour le français écrit.
Enfin, nous proposons un nouveau format de représentation pour l'annotation des chaînes de coréférences dans des corpus de langues écrites et parlées et en nous en donnons une exmple en proposant un nouvelle version d'ANCOR — le premier corpus de français annoté en coréférence.
Dans une première étape, on explicite les enjeux théoriques d'une telle recherche.
Cela conduit à accorder une place centrale au concept d'invariant sémantique pour rendre compte de l'identité sémantique d'un nom (en langue) par-delà sa variation de sens (en contexte).
Dans une deuxième étape, on circonscrit l'objet empirique – les noms de parties du corps humain en français contemporain – tout en justifiant ce terrain d'étude.
La suite de la thèse est consacrée à l'investigation empirique proprement dite.
Il s'agit d'abord d'offrir une description générale du potentiel de variation sémantique des noms de parties du corps humain en français.
Enfin, quatre autres noms (artère, épaule, bouche et pied) font également l'objet d'une analyse spécifique.
Chacune de ces quatre études est l'occasion d'éprouver la pertinence du concept d'invariant sémantique pour rendre compte de la polysémie dans le domaine nominal.
Dans certains environnements sensibles, tels que le domaine de la santé, où les utilisateurs sont généralement de confiance et où des évènements particuliers peuvent se produire, comme les situations d'urgence, les contrôles de sécurité mis en place dans les systèmes d'information correspondants ne doivent pas bloquer certaines décisions et actions des utilisateurs.
Cela pourrait avoir des conséquences graves.
En revanche, il est important de pouvoir identifier et tracer ces actions et ces décisions afin de détecter d'éventuelles violations de la politique de sécurité mise en place et fixer les responsibilités.
Ces fonctionnalités sont assurées par le contrôle d'accès à posteriori qui se base un mécanisme de monitoring à partir des logs.
Dans la littérature, ce type de contrôle de sécurité a été divisé en trois étapes qui sont : le traitement des logs, l'analyse des logs, et l'imputabilité.
Dans cette thèse, nous couvrons ces trois domaines du contrôle d'accès à posteriori en apportant de nouvelles solutions, et nous introduisons des nouveaux aspects qui n'avaient pas été abordés auparavant.
L'analyse criminelle est une discipline d'appui aux enquêtes pratiquée au sein de la Gendarmerie Nationale.
Or, l'analyse criminelle s'appuie entre autres sur le concept d'entités pour formaliser son travail.
La présentation du contexte de recherche détaille la pratique de l'analyse criminelle ainsi que la constitution du dossier de procédure judiciaire en tant que corpustextuel.
Nous proposons ensuite des perspectives pour l'adaptation des méthodes de traitement automatique de la langue (TAL) et d'extraction d'information au cas d'étude, notamment la mise en parallèle des concepts d'entité en analyse criminelle et d'entité nommée en TAL.
Cette comparaison est réalisée sur les plans conceptuels et linguistiques.
Une première approche de détection des entités dans les auditions de témoins est présentée.
Enfin, le genre textuel étant un paramètre à prendre en compte lors de l'appli-cation de traitements automatiques à du texte, nous construisons une structuration du genre textuel « légal » en discours, genres et sous-genres par le biais d'une étude textométrique visant à caractériser différents types de textes (dont les auditions de témoins) produits par le domaine de la justice.
La vision par ordinateur est un domaine interdisciplinaire étudiant la manière dont les ordinateurs peuvent acquérir une compréhension de haut niveau à partir d'images ou de vidéos numériques.
En intelligence artificielle, et plus précisément en apprentissage automatique, domaine dans lequel se positionne cette thèse, la vision par ordinateur passe par l'extraction de caractéristiques présentes dans les images puis par la généralisation de concepts liés à ces caractéristiques.
Ce domaine de recherche est devenu très populaire ces dernières années, notamment grâce aux résultats des réseaux de neurones convolutifs à la base des méthodes dites d'apprentissage profond.
Aujourd'hui les réseaux de neurones permettent, entre autres, de reconnaître les différents objets présents dans une image, de générer des images très réalistes ou même de battre les champions au jeu de Go.
Leurs performances ne s'arrêtent d'ailleurs pas au domaine de l'image puisqu'ils sont aussi utilisés dans d'autres domaines tels que le traitement du langage naturel (par exemple en traduction automatique) ou la reconnaissance de son.
Dans cette thèse, nous étudions les réseaux de neurones convolutifs afin de développer des architectures et des fonctions de coûts spécialisées à des tâches aussi bien de bas niveau (la constance chromatique) que de haut niveau (la segmentation sémantique d'image).
En vision par ordinateur, l'approche principale consiste à estimer la couleur de l'illuminant puis à supprimer son impact sur la couleur perçue des objets.
Les expériences que nous avons menées montrent que notre méthode permet d'obtenir des performances compétitives avec l'état de l'art.
Néanmoins, notre architecture requiert une grande quantité de données d'entraînement.
Afin de corriger en parti ce problème et d'améliorer l'entraînement des réseaux de neurones, nous présentons plusieurs techniques d'augmentation artificielle de données.
Nous apportons également deux contributions sur une problématique de haut niveau : la segmentation sémantique d'image.
Cette tâche, qui consiste à attribuer une classe sémantique à chacun des pixels d'une image, constitue un défi en vision par ordinateur de par sa complexité.
D'une part, elle requiert de nombreux exemples d'entraînement dont les vérités terrains sont coûteuses à obtenir.
D'autre part, elle nécessite l'adaptation des réseaux de neurones convolutifs traditionnels afin d'obtenir une prédiction dite dense, c'
Pour résoudre la difficulté liée à l'acquisition de données d'entrainements, nous proposons une approche qui exploite simultanément plusieurs bases de données annotées avec différentes étiquettes.
Nous développons aussi une approche dites d'auto-contexte capturant d'avantage les corrélations existantes entre les étiquettes des différentes bases de données.
Finalement, nous présentons notre troisième contribution : une nouvelle architecture de réseau de neurones convolutifs appelée GridNet spécialisée pour la segmentation sémantique d'image.
Contrairement aux réseaux traditionnels, notre architecture est implémentée sous forme de grille 2D permettant à plusieurs flux interconnectés de fonctionner à différentes résolutions.
En outre, nous montrons empiriquement que notre architecture généralise de nombreux réseaux bien connus de l'état de l'art.
Nous terminons par une analyse des résultats empiriques obtenus avec notre architecture qui, bien qu'entraînée avec une initialisation aléatoire des poids, révèle de très bonnes performances, dépassant les approches populaires souvent pré-entraînés
Oedipe, le personnage de la tragedie de sophocle, resout l'enigme du sphinx "par sa seule intelligence".
Il est ici le point de depart d'une reflexion generale sur le statut linguistique des jeux de langage, dont la pratique est repandue a toutes les epoques et dans toutes les cultures.
L'intelligence d'oedipe se fonde sur une capacite a "calculer" l'interpretation de l'enigme en abandonnant un raisonnement inductif (par recurrence) pour adopter un raisonnement analogique.
Dans une seconde partie, on montre que le calcul du sens des messages plurivoques permet de proposer un modele d'analyse combinatoire qui est un outil de traitement automatique des langues (tal), capable d'aider au calcul des jeux de charades et a l'interpretation des definitions cryptees des mots croises.
Ce modele sert de pierre de touche a une analyse des structures semantiques sous-jacentes aux interpretations et montre quels sont les items lexicaux qui sont concernes par l'isotopie.
L'isotopie n'est en l'occurrence pas consideree comme une donnee du message mais comme un construit de l'interpretation.
L'ensemble de la demarche adopte donc le point de vue d'une semantique interpretative.
La troisieme partie prolonge la reflexion en inscrivant le traitement des messages enigmatiques dans la problematique du dialogue homme-machine (dhm) qui permet de traiter les ambiguites de certains enonces et est capable de comprendre des "messages etranges" a partir des propositions d'interpretation extrapolees du modele.
De proche en proche on analyse ainsi le calcul du recepteur des messages comme une activite qui consiste a analyser les traces graphematiques ou acoustiques.
La prise en compte des traces est une confrontation avec les attendus du systeme linguistique qui permet de proceder a une serie de decisions aboutissant a l'identification d'un point de vue coherent.
La decouverte de cette coherence et de ce point de vue sont compares a la demarche que l'on adopte dans la "lecture" d'une anamorphose (en peinture) ou quand on dechiffre les regles d'organisation des suites de cartes dans le jeu d'eleusis.
On retrouve une demarche analogue quand il s'agit d'interpreter la"scriptio continua" des inscriptions paleographiques, dont la technique sert de base a la fois a certaines experiences de production litteraire sous contrainte et au jeux des mots caches.
De nos jours, les médias sociaux ont largement affecté tous les aspects de la vie humaine.
Le changement le plus significatif dans le comportement des gens après l'émergence des réseaux sociaux en ligne (OSNs) est leur méthode de communication et sa portée.
Avoir plus de connexions sur les OSNs apporte plus d'attention et de visibilité aux gens, où cela s'appelle la popularité sur les médias sociaux.
Selon le type de réseau social, la popularité se mesure par le nombre d'adeptes, d'amis, de retweets, de goûts et toutes les autres mesures qui servaient à calculer l'engagement.
L'étude du comportement de popularité des utilisateurs et des contenus publiés sur les médias sociaux et la prédiction de leur statut futur sont des axes de recherche importants qui bénéficient à différentes applications telles que les systèmes de recommandation, les réseaux de diffusion de contenu, les campagnes publicitaires, la prévision des résultats des élections, etc.
Cette thèse porte sur l'analyse du comportement de popularité des utilisateurs d'OSN et de leurs messages publiés afin, d'une part, d'identifier les tendances de popularité des utilisateurs et des messages et, d'autre part, de prévoir leur popularité future et leur niveau d'engagement pour les messages publiés par les utilisateurs.
A cette fin, i) l'évolution de la popularité des utilisateurs de l'ONS est étudiée à l'aide d'un ensemble de données d'utilisateurs professionnels 8K Facebook collectées par un crawler avancé.
L'ensemble de données collectées comprend environ 38 millions d'instantanés des valeurs de popularité des utilisateurs et 64 millions de messages publiés sur une période de 4 ans.
Le regroupement des séquences temporelles des valeurs de popularité des utilisateurs a permis d'identifier des modèles d'évolution de popularité différents et intéressants.
Les grappes identifiées sont caractérisées par l'analyse du secteur d'activité des utilisateurs, appelé catégorie, leur niveau d'activité, ainsi que l'effet des événements externes.
Ensuite ii) la thèse porte sur la prédiction de l'engagement des utilisateurs sur les messages publiés par les utilisateurs sur les OSNs.
Un nouveau modèle de prédiction est proposé qui tire parti de l'information mutuelle par points (PMI) et prédit la réaction future des utilisateurs aux messages nouvellement publiés.
Enfin, iii) le modèle proposé est élargi pour tirer profit de l'apprentissage de la représentation et prévoir l'engagement futur des utilisateurs sur leurs postes respectifs.
L'approche de prédiction proposée extrait l'intégration de l'utilisateur de son historique de réaction au lieu d'utiliser les méthodes conventionnelles d'extraction de caractéristiques.
La performance du modèle proposé prouve qu'il surpasse les méthodes d'apprentissage conventionnelles disponibles dans la littérature.
Les modèles proposés dans cette thèse, non seulement déplacent les modèles de prédiction de réaction vers le haut pour exploiter les fonctions d'apprentissage de la représentation au lieu de celles qui sont faites à la main, mais pourraient également aider les nouvelles agences, les campagnes publicitaires, les fournisseurs de contenu dans les CDN et les systèmes de recommandation à tirer parti de résultats de prédiction plus précis afin d'améliorer leurs services aux utilisateurs
Mes travaux de thèse s'intéressent à l'utilisation de nouvelles technologies d'intelligence artificielle appliquées à la problématique de la classification automatique des séquences audios selon l'état émotionnel du client au cours d'une conversation avec un téléconseiller.
En 2016, l'idée est de se démarquer des prétraitements de données et modèles d'apprentissage automatique existant au sein du laboratoire, et de proposer un modèle qui soit le plus performant possible sur la base de données audios IEMOCAP.
Nous nous appuyons sur des travaux existants sur les modèles de réseaux de neurones profonds pour la reconnaissance de la parole, et nous étudions leur extension au cas de la reconnaissance des émotions dans la voix.
Nous nous intéressons ainsi à l'architecture neuronale bout-en-bout qui permet d'extraire de manière autonome les caractéristiques acoustiques du signal audio en vue de la tâche de classification à réaliser.
Pendant longtemps, le signal audio est prétraité avec des indices paralinguistiques dans le cadre d'une approche experte.
Nous choisissons une approche naïve pour le prétraitement des données qui ne fait pas appel à des connaissances paralinguistiques spécialisées afin de comparer avec l'approche experte.
Exploiter un réseau neuronal pour une tâche de prédiction précise implique de devoir s'interroger sur plusieurs aspects.
D'une part, il convient de choisir les meilleurs hyperparamètres possibles.
D'autre part, il faut minimiser les biais présents dans la base de données (non discrimination) en ajoutant des données par exemple et prendre en compte les caractéristiques de la base de données choisie.
Nous étudions ces aspects pour une architecture neuronale bout-en-bout qui associe des couches convolutives spécialisées dans le traitement de l'information visuelle, et des couches récurrentes spécialisées dans le traitement de l'information temporelle.
Nous proposons un modèle d'apprentissage supervisé profond compétitif avec l'état de l'art sur la base de données IEMOCAP et cela justifie son utilisation pour le reste des expérimentations.
Notre modèle est évalué sur deux bases de données audios anglophones proposées par la communauté scientifique : IEMOCAP et MSP-IMPROV.
Une première contribution est de montrer qu'avec un réseau neuronal profond, nous obtenons de hautes performances avec IEMOCAP et que les résultats sont prometteurs avec MSP-IMPROV.
Une autre contribution de cette thèse est une étude comparative des valeurs de sortie des couches du module convolutif et du module récurrent selon le prétraitement de la voix opéré en amont : spectrogrammes (approche naïve) ou indices paralinguistiques (approche experte).
À l'aide de la distance euclidienne, une mesure de proximité déterministe, nous analysons les données selon l'émotion qui leur est associée.
Nous tentons de comprendre les caractéristiques de l'information émotionnelle extraite de manière autonome par le réseau.
L'idée est de contribuer à une recherche centrée sur la compréhension des réseaux de neurones profonds utilisés en reconnaissance des émotions dans la voix et d'apporter plus de transparence et d'explicabilité à ces systèmes dont le mécanisme décisionnel est encore largement incompris.
En traitement automatique des langues, deux grandes approches sont utilisées : l'apprentissage automatique et la fouille de données.
Dans cette thèse, nous présentons trois contributions majeures : l'introduction des motifs delta libres,utilisés comme descripteurs de modèle statistiques;
Les approches formelles de représentation des signes des langues des signes sont majoritairement paramétriques et nous montrons en quoi celles-ci ne sont pas suffisantes dans l'optique d'une utilisation informatique.
Les plus fortes raisons sont le caractère ni nécessaire ni suffisant de l'ensemble de paramètres traditionnellement utilisé, leur nature fixe alors qu'un signe est dynamique et évolue au cours du temps, et le fait que les descriptions ne rendent pas compte de l'adaptabilité des signes décrits à différents contextes, pourtant à l'origine de leur réutilisabilité et de la force de concision des langues des signes.
Nous proposons Zebedee, un modèle de description en séquence d'unités temporelles décrivant chacune un ensemble de contraintes nécessaires et suffisantes, appliquées à un squelette.
L'espace de signation est vu comme un espace euclidien dans lequel toute construction géométrique annexe est possible.
Les dépendances entre éléments des descriptions ou sur des valeurs contextuelles sont non seulement possibles mais pertinentes, et reposent sur des considérations à la fois articulatoires, cognitives et sémantiques.
Nous donnons ensuite deux processus complémentaires d'évaluation : en informatique où nous discutons l'implantation de Zebedee dans une plateforme d'animation de signeur virtuel et son utilisation pour la diffusion d'informations en gare, et en linguistique où nous décrivons l'avantage d'une base de données et les nouvelles possibilités de requêtes offertes au linguiste.
En perspectives, nous citons plusieurs domaines informatiques où Zebedee sera utile et plusieurs questionnements linguistiques actuels auxquels il offre des pistes de réponse.
La détection sans fil a évolué depuis la découverte de la détection radar en 1886.
Cependant, pendant très longtemps, la détection sans fil a rarement été utilisée pour des applications centrées sur l'homme en raison de limitations techniques, d'impraticabilité ou de coût.
L'introduction des réseaux sans fil a suscité́ un nouvel intérêt pour le développement de nouveaux services de détection sans fil en raison de leur souplesse et de leur polyvalence.
L'intégration de ces fonctionnalités contribuerait à résoudre certains problèmes de société importants.
La localisation, la détection de mouvements et la surveillance des signes vitaux ont un grand potentiel pour promouvoir le vieillissement en bonne santé, la sécurité publique et le commerce.
La détection sans contact offre un degré de liberté appréciable, permettant de surveiller à distance les personnes âgées isolées sans entraver leur vie quotidienne.
Elle pourrait aider les services de sécurité publique à dénombrer les foules et à détecter les survivants à l'intérieur des bâtiments en cas d'urgence.
Les commerces de détail et les établissements publics tireraient parti d'une localisation active et passive pour offrir une expérience améliorée à leurs visiteurs et faciliter leurs efforts logistiques.
Tandis que d'autres travaux fournissent des solutions à granularité grossière pour résoudre de tels problèmes, nous utilisons les techniques de radar MIMO pour fournir un système d'estimation d'orientation précis pour lesinfrastructures Wi-Fi.
Plus précisément, nous analysons les informations de phase des signaux reçus sur le réseau d'antennes afin de calculer le cap d'un terminal Wi
Les solutions actuelles sont complexes, coûteuses ou consomment beaucoup d'énergie.
Pour résoudre ce problème, nous introduisons les fonctions MIMO dans les systèmes LoRa LPWAN afin de permettre une localisation précise avec des coûts de démarrage limités.
Nous activons l'estimation de l'angle d'arrivée en utilisant une deuxième antenne sur la passerelle LoRaWAN.
Nous prouvons également l'utilité de ces informations pour augmenter l'efficacité des communications sans fil.
Un troisième défi pour la localisation sans fil est l'inefficacité des approches actuelles basées sur un modèle en cas de conditions de non-visibilité et la rigidité des approches basées sur les données en cas de changements d'environnement de propagation.
Pour relever ce défi, nous proposons une nouvelle solution de localisation passive pilotée par les données afin de remédier aux limitations des techniques de localisation basées sur un modèle.
Le changement climatique en cours modifie les conditions environnementales et les espèces doivent s'adapter à ces nouvelles conditions, en restant sur place ou en se déplaçant conduisant alors à de nouvelles distributions.
Ce repositionnement revêt deux dimensions principales : (i) l'adaptabilité des espèces aux nouvelles conditions (changement de traits d'histoire de vie) liée à la résilience des populations et (ii) leur capacité à explorer de nouveaux habitats favorables.
Cette étude avait pour objectif l'élaboration d'un modèle dynamique mécaniste intégrant ces deux dimensions de manière à pouvoir évaluer, comprendre et prédire les possibilités de repositionnement des poissons migrateurs amphihalins européens face au changement climatique.
Pour accomplir leurs cycles de vie, les espèces migratrices amphihalines utilisent nécessairement des écosystèmes dulçaquicoles, estuariens et marins.
Ces cycles de vie particuliers leur confèrent un plus grand potentiel de repositionnement que les espèces dulçaquicoles.
Une base de données sur les traits de vie de ces espèces intégrant notamment ceux pouvant potentiellement être influencés par le changement climatique et ceux pouvant jouer un rôle dans le potentiel de dispersion des espèces a été construite pour l'ensemble des espèces amphihalines européens.
Une méthode d'analyse multicritère hiérarchique a été proposée pour définir un indice basé sur les traits de vies visant à caractériser le potentiel de repositionnement des espèces migratrices amphihalines.
Le modèle GR3D (Global Repositioning Dynamics for Diadromous fish Distribution) a ensuite été développé pour étudier de façon dynamique le repositionnement potentiel de ces espèces, à large échelle, dans un contexte scénarisé de changement climatique.
Il s'agit d'un modèle de simulation stochastique, individus centré, intégrant les principaux processus de dynamique de population d'un poisson migrateur amphihalin (reproduction, mortalité, croissance, migration de montaison avec dispersion, migration de dévalaison).
Un premier cas d'étude exploratoire simulant le repositionnement d'une population virtuelle de grande alose (Alosa alosa) de son bassin versant d'origine à un bassin versant voisin inhabité dans un contexte d'augmentation de la température a permis de réaliser une analyse de sensibilité globale du modèle GR3D à la fois aux paramètres incertains de dynamique de population et aux paramètres reliés à la structure de l'environnement.
Il a été mis en évidence une sensibilité particulière du modèle aux paramètres liés à la durée de vie et à la mortalité en mer ainsi qu'à la distance entre les deux bassins versants de l'environnement pour déterminer le succès de colonisation.
Enfin, l'utilisation du modèle GR3D sur un cas d'application réel a permis de commencer à évaluer l'évolution de la persistance de la grande alose à l'échelle de son aire de répartition (i.e. la façade atlantique) dans un contexte de changement climatique.
Les simulations du modèle GR3D devraient ainsi trouver à terme des applications pour la gestion et la conservation des espèces migratrices amphihalines.
Cette thèse introduit différentes méthodes de vérification (ou model-checking) sur des modèles de systèmes à pile.
En effet, les systèmes à pile (pushdown systems) modélisent naturellement les programmes séquentiels grâce à une pile infinie qui peut simuler la pile d'appel du logiciel.
La première partie de cette thèse se concentre sur la vérification sur des systèmes à pile de la logique HyperLTL, qui enrichit la logique temporelle LTL de quantificateurs universels et existentiels sur des variables de chemin.
Il a été prouvé que le problème de la vérification de la logique HyperLTL sur des systèmes d'états finis est décidable ;
nous montrons que ce problème est en revanche indécidable pour les systèmes à pile ainsi que pour la sous-classe des systèmes à pile visibles (visibly pushdown systems).
Nous introduisons donc des algorithmes d'approximation de ce problème, que nous appliquons ensuite à la vérification de politiques de sécurité.
Dans la seconde partie de cette thèse, dans la mesure où la représentation de la pile d'appel par les systèmes à pile est approximative, nous introduisons les systèmes à surpile (pushdown systems with an upper stack) ; dans ce modèle, les symboles retirés de la pile d'appel persistent dans la zone mémoire au dessus du pointeur de pile, et peuvent être plus tard écrasés par des appels sur la pile.
Nous montrons que les ensembles de successeurs post* et de prédécesseurs pre* d'un ensemble régulier de configurations ne sont pas réguliers pour ce modèle, mais que post* est toutefois contextuel (context-sensitive), et que l'on peut ainsi décider de l'accessibilité d'une configuration.
Enfin, dans le but d'analyser des programmes avec plusieurs fils d'exécution, nous introduisons le modèle des réseaux à piles dynamiques synchronisés (synchronized dynamic pushdown networks), que l'on peut voir comme un réseau de systèmes à pile capables d'effectuer des changements d'états synchronisés, de créer de nouveaux systèmes à piles, et d'effectuer des actions internes sur leur pile.
Nous appliquons ensuite cette méthode à un processus itératif de raffinement des abstractions.
La langue arabe est pauvre en ressources sémantiques électroniques.
Il y a bien la ressource Arabic WordNet, mais il est pauvre en mots et en relations.
Cette thèse porte sur l'enrichissement d'Arabic WordNet par des synsets (un synset est un ensemble de mots synonymes) à partir d'un corpus général de grande taille.
Pour validation, il a fallu créer un corpus de référence (il n'en existe pas en arabe pour ce domaine) à partir d'Arabic WordNet, puis comparer la méthode GraPaVec avec Word2Vec et Glove.
Le résultat montre que GraPaVec donne pour ce problème les meilleurs résultats avec une F-mesure supérieure de 25 % aux deux autres.
Les classes produites seront utilisées pour créer de nouveaux synsets intégrés à Arabic WordNet
Cette thèse porte sur l'utilisation d'ontologies et de bases de connaissances pour guider différentes étapes du processus d'extraction de connaissances à partir de bases de données (ECBD) et une application dans le domaine de la pharmacogénomique.
Cette approche a été implémentée grâce aux technologies du Web sémantique et conduit au peuplement d'une base de connaissances pharmacogénomique.
Le fait que les données à fouiller soient alors disponibles dans une base de connaissances entraîne de nouvelles potentialités pour le processus d'extraction de connaissances.
Je me suis d'abord intéressé au problème de la sélection des données les plus pertinentes à fouiller en montrant comment la base de connaissances peut être exploitée dans ce but.
Ensuite j'ai décrit et appliqué à la pharmacogénomique, une méthode qui permet l'extraction de connaissances directement à partir d'une base de connaissances.
Cette méthode appelée Analyse des Assertions de Rôles (ou AAR) permet d'utiliser des algorithmes de fouille de données sur un ensemble d'assertions de la base de connaissances pharmacogénomique et d'expliciter des connaissances nouvelles et pertinentes qui y étaient enfouies.
Avec l'essor du Web 2.0 et des technologies collaboratives qui y sont rattachées,le Web est aujourd'hui devenu une vaste plate-forme d'échanges entre internautes.
La majeure partie des sites Web sont actuellement soit dédiés aux interactions sociales de leurs utilisateurs, soit proposent des outils pour développer ces interactions.
Nos travaux portent sur la compréhension de ces échanges, ainsi que des structures communautaires qui en découlent, au moyen d'une approche sémantique.
Pour répondre aux besoins de compréhension propres aux analystes de site Web et autres gestionnaires de communautés, nous analysons ces structures communautaires pour en extraire des caractéristiques essentielles comme leurs centres thématiques et contributeurs centraux.
Notre analyse sémantique s'appuie notamment sur des ontologies légères de référence pour définir plusieurs nouvelles métriques,comme la centralité sémantique temporelle et la probabilité de propagation sémantique.
Nous employons une approche « en ligne » afin de suivre l'activité utilisateur en temps réel, au sein de notre outil d'analyse communautaire Web-Tribe.
Nous avons implémenté et testé nos méthodes sur des données extraites de systèmes réels de communication sociale sur le Web
Cette thèse se consacre aux enjeux de la polysémie verbale pour l'acquisition des langues secondes.
Nous nous intéressons plus particulièrement à la polysémie du verbe prendre.
Cette thèse poursuit deux objectifs complémentaires.
D'une part, notre premier objectif est de décrire la polysémie du verbe prendre au moyen d'une analyse en sémantique lexicale dans une approche cognitive.
D'autre part, notre second objectif est d'évaluer l'incidence de la polysémie du verbe prendre sur les connaissances qu'ont les apprenants de ce verbe et d'isoler les différentes acceptions de prendre mises au jour par notre analyse sémantique, qui s'avèrent problématiques pour les apprenants du français L2.
Par ailleurs, nous cherchons également à savoir si les problèmes liés à la polysémie chez les anglophones peuvent être liés à l'influence translangagière.
Afin de délimiter le corpus d'acceptions soumis à l'analyse sémantique, nous avons effectué une analyse syntaxique permettant de départager les différents emplois du verbe prendre : les constructions à verbe support, les locutions verbales et les acceptions prédicatives.
Chacune des parties du noyau de sens peut faire l'objet d'un fenêtrage de l'attention, qui nous permet de mettre au jour les différents sens du verbe.
Nous avons également émis des hypothèses sur les différences entre le verbe prendre et ses équivalents en anglais.
Pour atteindre le second objectif de notre travail, nous avons mené une expérimentation auprès de 191 apprenants du français langue seconde.
Tous les participants ont complété trois tâches expérimentales : un test de closure afin de mesurer leur niveau de compétence langagière en français, une tâche de production écrite et une tâche de jugement d'acceptabilité.
Les résultats des analyses de régression logistique et multinomiale montrent non seulement que l'analyse sémantique que nous avons proposée permet de prédire la connaissance des différentes acceptions du verbe par les apprenants du français L2, mais aussi que les apprenants anglophones et allophones ont un comportement différent par rapport aux types d'acceptions du verbe prendre, comportement que nous avons pu expliquer par l'influence translangagière chez les participants anglophones.
La physique statistique, développée à l'origine pour décrire les systèmes thermodynamiques, a joué pendant les dernières décennies un rôle central dans la modélisation d'un ensemble incroyablement vaste et hétérogène de différents phénomènes qui ont lieu par exemple dans des systèmes sociaux, économiques ou biologiques.
Un champ d'applications possibles aussi vaste a été trouvé aussi pour les réseaux, comme une grande variété de systèmes peut être décrite en termes d'éléments interconnectés.
Après une partie introductive sur les thèmes abordés ainsi que sur le rôle de la modélisation abstraite dans la science, dans ce manuscrit seront décrites les nouvelles perspectives auxquelles on peut arriver en approchant d'une façon physico-statistique trois problèmes d'intérêt dans la théorie des réseaux : comment une certaine quantité peut se répandre de façon optimale sur un graphique, comment explorer un réseau et comment le reconstruire à partir d'un jeu d'informations partielles.
Quelques remarques finales sur l'importance que ces thèmes préserveront dans les années à venir conclut le travail.
Cette thèse propose une analyse contrastive de la notion de défini telle qu'elle est exprimée dans le système de l'article en anglais et en arabe moderne standard.
Le récit, The Brook Kerith de l'écrivain irlandais George Moore a été choisi pour des raisons géo-historiques et littéraires : les événements racontés se déroulent en Terre Sainte à l'aube de l'ère chrétienne.
Les occurrences du syntagme nominal en anglais et en arabe analysées dans le premier chapitre permettent de dégager les convergences et les divergences des deux systèmes.
Les résultats sont soumis à une analyse quantitative et statistique.
Il en ressort que la valeur de l'article défini en anglais (“the”) et en arabe (“al”) correspondent dans 76% des emplois.
La ressemblance entre la valeur de l'article indéfini (“a / an”) en anglais et son équivalent en arabe s'élève à 96%.
Cependant, dans la mesure où l'arabe est une langue sans article indéfini, le fonctionnement de l'article zéro en anglais est sans équivalence ;
En dernière analyse, on constate une grande ressemblance entre les mécanismes cognitifs sous-jacents ;
les différences concernent les transformations sémiotiques de la structure profonde.
Cette thèse porte sur la détection d'événements dans des signaux issus de capteurs sols pour le suivi des personnes âgées.
Au vu des questions pratiques, il semble en effet que les capteurs de pression situés au sol soient de bons candidats pour les activités de suivi, notamment la détection de chute.
Ainsi, afin de concevoir un détecteur de chutes, nous proposons une approche basée sur les forêts aléatoires, tout en répondant aux contraintes matérielles à l'aide d'une procédure de sélection des variables.
Les performances sont améliorées à l'aide d'une méthode d'augmentation des données ainsi qu'à l'agrégation temporelle des réponses du modèle.
Nous les testons sur plusieurs ensembles de données, montrant ainsi des résultats encourageants pour la suite, et une implémentation Python est mise à disposition.
Enfin, motivés par la question du suivi des personnes âgées tout en traitant un signal unidimensionnel pour une grande zone, nous proposons de distinguer les personnes âgées des individus plus jeunes grâce à un modèle de réseau de neurones convolutifs et un apprentissage de dictionnaire.
Les signaux à traiter étant principalement constitués de marches, la première brique du modèle est entraînée pour se focaliser sur les pas dans les signaux, et la seconde partie du modèle est entraînée séparément sur la tâche finale.
Cette nouvelle approche de la classification de la marche permet de reconnaître avec efficacité les signaux issus de personnes âgées.
Les systèmes de recommandation visent à présélectionner et présenter en premier les informations susceptibles d'intéresser les utilisateurs.
Ceci a suscité l'attention du commerce électronique, où l'historique des achats des utilisateurs sont analysés pour prédire leurs intérêts futurs et pouvoir personnaliser les offres ou produits (appelés aussi items) qui leur sont proposés.
Dans ce cadre, les systèmes de recommandation exploitent les préférences des utilisateurs et les caractéristiques des produits et des utilisateurs pour prédire leurs préférences pour des futurs items.
Bien qu'ils aient démontré leur précision, ces systèmes font toujours face à de grands défis tant pour le monde académique que pour l'industrie : ces techniques traitent un grand volume de données qui exige une parallélisation des traitements, les données peuvent être également très hétérogènes, et les systèmes de recommandation souffrent du démarrage à froid, situation dans laquelle le système n'a pas (ou pas assez) d'informations sur (les nouveaux) utilisateurs/items pour proposer des recommandations précises.
La technique de factorisation matricielle a démontré une précision dans les prédictions et une simplicité de passage à l'échelle.
Cependant, cette approche a deux inconvénients : la complexité d'intégrer des données hétérogènes externes (telles que les caractéristiques des items) et le démarrage à froid pour un nouvel utilisateur.
Cette thèse présente quatre contributions au domaine des systèmes de recommandation : (1) nous proposons une implémentation d'un algorithme de recommandation de factorisation matricielle parallélisable pour assurer un meilleur passage à l'échelle, (2) nous améliorons la précision des recommandations en prenant en compte l'intérêt implicite des utilisateurs dans les attributs des items, (3) nous proposons une représentation compacte des caractéristiques des utilisateurs/items basée sur les filtres de bloom permettant de réduire la quantité de mémoire utile, (4) nous faisons face au démarrage à froid d'un nouvel utilisateur en utilisant des techniques d'apprentissage actif.
La phase d'expérimentation utilise le jeu de données MovieLens et la base de données IMDb publiquement disponibles, ce qui permet d'effectuer des comparaisons avec des techniques existantes dans l'état de l'art.
Ces expérimentations ont démontré la précision et l'efficacité de nos approches.
Dans cette thèse, nous présentons notre méthodologie de la connaissance interactive et itérative pour une extraction des textes-le système KESAM : Un outil pour l'extraction des connaissances et le Management de l'Annotation Sémantique.
Le KESAM est basé sur l'analyse formelle du concept pour l'extraction des connaissances à partir de ressources textuelles qui prend en charge l'interaction aux experts.
Dans le système KESAM, l'extraction des connaissances et l'annotation sémantique sont unifiées en un seul processus pour bénéficier à la fois l'extraction des connaissances et l'annotation sémantique.
Les annotations sémantiques sont utilisées pour formaliser la source de la connaissance dans les textes et garder la traçabilité entre le modèle de la connaissance et la source de la connaissance.
Le modèle de connaissance est, en revanche, utilisé afin d'améliorer les annotations sémantiques.
Le processus KESAM a été conçu pour préserver en permanence le lien entre les ressources (textes et annotations sémantiques) et le modèle de la connaissance.
Le noyau du processus est l'Analyse Formelle de Concepts (AFC) qui construit le modèle de la connaissance, i.e. le treillis de concepts, et assure le lien entre le modèle et les annotations des connaissances.
Afin d'obtenir le résultat du treillis aussi près que possible aux besoins des experts de ce domaine, nous introduisons un processus itératif qui permet une interaction des experts sur le treillis.
Les experts sont invités à évaluer et à affiner le réseau ; ils peuvent faire des changements dans le treillis jusqu'à ce qu'ils parviennent à un accord entre le modèle et leurs propres connaissances ou le besoin de l'application.
Grâce au lien entre le modèle des connaissances et des annotations sémantiques, le modèle de la connaissance et les annotations sémantiques peuvent co-évoluer afin d'améliorer leur qualité par rapport aux exigences des experts du domaine.
En outre, à l'aide de l'AFC de la construction des concepts avec les définitions des ensembles des objets et des ensembles d'attributs, le système KESAM est capable de prendre en compte les deux concepts atomiques et définis, à savoir les concepts qui sont définis par un ensemble des attributs.
Afin de combler l'écart possible entre le modèle de représentation basé sur un treillis de concept et le modèle de représentation d'un expert du domaine, nous présentons ensuite une méthode formelle pour l'intégration des connaissances d'expert en treillis des concepts d'une manière telle que nous pouvons maintenir la structure des concepts du treillis.
La connaissance d'expert est codée comme un ensemble de dépendance de l'attribut qui est aligné avec l'ensemble des implications fournies par le concept du treillis, ce qui conduit à des modifications dans le treillis d'origine.
La méthode permet également aux experts de garder une trace des changements qui se produisent dans le treillis d'origine et la version finale contrainte, et d'accéder à la façon dont les concepts dans la pratique sont liés à des concepts émis automatiquement à partir des données.
Nous pouvons construire les treillis contraints sans changer les données et fournir la trace des changements en utilisant des projections extensives sur treillis.
À partir d'un treillis d'origine, deux projections différentes produisent deux treillis contraints différents, et, par conséquent, l'écart entre le modèle de représentation basée sur un treillis de réflexion et le modèle de représentation d'un expert du domaine est rempli avec des projections
Les décès du cancer de la peau sont majoritairement des mélanomes malins.
Il est considéré comme l'un des plus dangereux cancer.
A ses débuts, les mélanomes malins sont traités avec des simples biopsies et sont complètement curable.
Pour cela, une détection précoce est la meilleure solution pour réduire ses conséquences désastreuses.
Imagerie médicale telle que la dermoscopie et les caméras à images standard sont les outils disponibles les plus adaptées pour diagnostiquer précocement les mélanomes.
Le diagnostic assisté par ordinateur (CAD) est développé dans le but d'accompagner les radiologistes dans la détection et le diagnostic.
Cependant, il y a un besoin d'améliorer la précision de la segmentation et de détection des lésions.
La deuxième tâche consiste à extraire des caractéristiques afin de discriminer les mélanomes.
Deux méthodes ont été développée, une se basant sur l'irrégularité des bords des lésions et l'autre par la fusion des caractéristiques texturales et structurelles.
Les résultats ont montrés de bonnes performances avec une précision de 86.54% et de 86.07%, respectivement.
L'apprentissage à distance métrique est une branche de l'apprentissage par re-présentation des algorithmes d'apprentissage automatique.
Nous résumons le développement et la situation actuelle de l'algorithme actuel d'apprentissage à distance métrique à partir des aspects de la base de données plate et de la base de données non plate.
Pour une série d'algorithmes basés sur la distance de Mahalanobis pour la base de données plate qui ne parvient pas à exploiter l'intersection de trois dimensions ou plus, nous proposons un algorithme d'apprentissage métrique basé sur la fonction sousmodulaire.
Pour le manque d'algorithmes d'apprentissage métrique pour les bases de données relationnelles dans des bases de données non plates, nous proposons LSCS (sélection de contraintes relationnelles de force relationnelle) pour la sélection de contraintes pour des algorithmes d'apprentissage métrique avec informations parallèles et MRML (Multi-Relation d'apprentissage métrique) qui somme la perte des contraintes relationnelles et les contraintes d'etiquetage.
Grâce aux expériences de conception et à la vérification sur la base de données réelle, les algorithmes proposés sont meilleurs que les algorithmes actuels.
L'enjeu de cette thèse est l'acquisition automatique de nouveaux sens lexicaux.
Nous définissons un modèle théorique sur l'émergence d'un nouveau sens pour une unité lexicale ayant déjà un sens codé.
Nous la modélisons à partir d'indices quantitatifs articulés à des principes issus de la sémantique textuelle.
Le sens codé est représentécomme un ensemble structuré de traits sémantiques.
Il est modulé en discours sous l'effet de récurrences d'autres traits.
La dynamique du sens est représentée à l'aide de descripteurs de granularité sémantique variable.
Ensuite, nous proposons des ressources et outils adaptés, relevant de la linguistique de corpus.
En pratique, le Trésor de la Langue Française informatisé fournit les sens codés.
Une plateforme transforme ses définitions en ensembles de traits sémantiques.
Trois corpus journalistiques des années 2000 servent de ressources textuelles.
Les outils mathématiques, essentiellementstatistiques, permettent de jouer sur la structure des ressources, d'extraire des unités saillantes et d'organiser l'information.
Enfin, nous établissons les grandes lignes d'une procédure pour allouer de façon semi-automatique un nouveau sens.
Elles sont étayées par des expériences illustratives.
Il s'appuie sur des jeux de contrastes multiples, permettant de nuancer l'informationsémantique.
L'évaluation de connaissances à travers un support de questions à choix multiples est une méthode fiable et largement utilisée, y compris dans des contextes officiels, comme pour l'examen du code de la route.
Cette méthode d'évaluation offre en effet de nombreux avantages, comme une égalité de notation entre les candidats, ou de façon plus pragmatique, une possibilité de correction automatique.
L'émergence des MOOCs, des cours dispensés sous un format numérique, a contribué à accroître ce besoin d'évaluation automatique.
Les travaux de cette thèse s'inscrivent ainsi dans ce contexte, en proposant une solution permettant de générer des questions thématiques, c'est à dire des questions centrées autour d'un thème prédéfini.
Les travaux présentés dans cette thèse utilisent des bases de connaissances comme sources de données pour générer automatiquement des questions à choix multiples thématiques.
Cette thèse présente ainsi une méthode basée sur les méta-données de Wikipedia permettant d'identifier et de trier les entités de bases de connaissances en fonction de thèmes prédéfinis.- Pour qu'une question soit intelligible, son énoncé doit être grammaticalement correct, et contenir suffisamment d'informations pour lever toute ambiguïté quand-à la bonne réponse.
Dans une dernière contribution, nous présentons la méthode utilisée pour sélectionner des distracteurs qui soient non seulement pertinents vis-à-vis de l'énoncé de la question, mais aussi de son contexte.
Le script est une structure qui décrit une séquence stéréotypée d'événements ou d'actions survenant dans notre vie quotidienne.
Les histoires utilisent des scripts, avec une ou plusieurs déviations intéressantes, qui nous permettent de mieux saisir les situations quotidiennes rapportées et les faits saillants du récit.
Ainsi, la notion de script est très utile dans de nombreuses applications d'intelligence ambiante telles que la surveillance de la santé et les services d'urgence.
La reconnaissance de l'activité humaine (HAR) a ainsi connue un essor important grâce notamment à des approches d'apprentissage automatique telles que le réseau neuronal ou le réseau bayésien.
Ces avancées ouvre des perspectives qui vont au delà de la simple reconnaissance d'activités.
Ce manuscrit défend la thèse selon laquelle ces données de capteurs portables peuvent être utilisées pour générer des récits articulés autour de scripts en utilisant l'apprentissage automatique.
Il ne s'agit pas d'une tâche triviale en raison du grand écart sémantique entre les informations brutes de capteurs et les abstractions de haut niveau présente dans les récits.
A notre connaissance, il n'existe toujours pas d'approche pour générer une histoire à partir de données de capteurs en utilisant l'apprentissage automatique, même si de nombreuses approches d'apprentissage automatique (réseaux de neurones convolutifs, réseaux de neurones profonds) ont été proposées pour la reconnaissance de l'activité humaine au cours des dernières années.
Deuxièmement, nous présentons un nouveau système permettant de générer automatiquement des scripts à partir de données d'activité humaine à l'aide de l'apprentissage profond.
Enfin, nous proposons une approche pour l'apprentissage de scripts à partir de textes en langage naturel capable d'exploiter l'information syntaxique et sémantique sur le contexte textuel des événements.
Cette approche permet l'apprentissage de l'ordonnancement d'événements à partir d'histoires décrivant des situations typiques de vie quotidienne.
L'objet de cette thèse est l'étude de la reconnaissance automatique de parole.
Ce document débute avec la description des traitements acoustiques les plus répandus en vue de reconnaître la parole.
Nous décrivons ensuite les diverses architectures qui ont été utilisées : comparaison dynamique de formes acoustiques, systèmes experts, réseaux neuro-mimétiques et modèles de Markov.
Puis ce document se divise en deux parties.
Pour cela, nous utilisons des automates qui modélisent le vocabulaire.
Celui-ci comporte les dix chiffres anglo-saxons, dont deux prononciations différentes pour le zéro.
Le corpus de parole TiDigits a été utilisé par d'autres laboratoires ce qui nous permet de comparer nos résultats.
La première étape est consacrée à la reconnaissance de mots isolés.
Puis nous présentons une méthode de segmentation de séquences de chiffres.
La fin de ce chapitre est consacrée à la reconnaissance de mots enchaînés et à une discussion sur les mérites et les faiblesses de notre approche.
La deuxième partie traite de l'utilisation d'un modèle de production qui pourrait être utilisé pour le reconnaissance de la parole.
Nous commençons par présenter les équations acoustiques régissant l'écoulement de l'air dans le conduit vocal et divers modèles articulatoires.
Ensuite nous justifions le choix du modèle articulatoire de Maeda.
Nous décrivons comment nous avons adapté le modèle à un locuteur masculin.
Puis nous présentons la méthode variationnelle utilisée pour retrouver les trajectoires des articulateurs en fonction de la parole prononcée.
Une dernière section présente les logiciels réalisés.
En conclusion, nous résumons les résultats obtenus et donnons quelques perspectives en vue de reconnaître la parole continue quel que soit le locuteur.
Les images satellites représentent de nos jours une source d'information incontournable.
Elles sont exploitées dans diverses applications, telles que : la gestion des risques, l'aménagent des territoires, la cartographie du sol ainsi qu'une multitude d'autre taches.
Nous proposons des méthodes d'analyse de STIS orientée objets, en opposition aux approches par pixel, qui exploitent des images satellites segmentées.
Nous identifions d'abord les profils d'évolution des objets de la série.
Ensuite, nous analysons ces profils en utilisant des méthodes d'apprentissage automatique.
Afin d'analyser les graphes d'évolution, nous avons proposé trois contributions.
La première contribution explore des STIS annuelles.
Elle permet d'analyser les graphes d'évolution en utilisant des algorithmes de clustering, afin de regrouper les entités spatio-temporelles évoluant similairement.
Nous explorons plusieurs sites d'étude qui sont décrits par des STIS pluri
Dans la troisième contribution, nous introduisons une méthode d'analyse semi-supervisée basée sur du clustering par contraintes.
Ces contraintes sont utilisées pour guider le processus de clustering et adapter le partitionnement aux besoins de l'utilisateur.
Nous avons évalué nos travaux sur diﬀérents sites d'étude.
Les résultats obtenus ont permis d'identifier des profils d'évolution types sur chaque site d'étude.
En outre, nous avons aussi identifié des évolutions caractéristiques communes à plusieurs sites.
Par ailleurs, la sélection de contraintes pour l'apprentissage semi-supervisé a permis d'identifier des entités profitables à l'algorithme de clustering.
Ainsi, les partitionnements obtenus en utilisant l'apprentissage non supervisé ont été améliorés et adaptés aux besoins de l'utilisateur.
La pharmacovigilance souffre d'une sous notification chronique des effets indésirables de la part des professionnels de santé.
La FDA (US Food and Drug Administration), l'EMA (European Medicines Agency), et d'autres agences sanitaires, suggèrent que les réseaux sociaux pourraient constituer une source de données supplémentaire pour la détection de signaux faibles de pharmacovigilance.
L'OMS (Organisation Mondiale de la Santé) a publié un rapport en 2003 exposant le problème que pose la non-observance au traitement sur le long terme et son caractère préjudiciable à l'efficacité des systèmes de santé au niveau mondial.
Les données nécessaires à la mise au point d'un système d'extraction d'informations de santé depuis les forums de patients sont mise à disposition par la société́ Kappa Santé.
La première approche proposée s'inscrit dans un contexte de détection de cas de pharmacovigilance à partir d'échanges entre patients sur des forums de santé.
Nous proposons un filtre basé sur le nombre de mots séparant le nom du médicament évoqué dans le message du terme considéré́ comme un potentiel effet indésirable.
Nous proposons une seconde approche basée sur les « topic models » afin de cibler les groupes de messages abordant les thèmes traitant de non-observance.
En terme de pharmacovigilance, le filtre gaussien proposé permet d'identifier 50.03% des faux positifs avec une précision de 95.8% et un rappel de 50%.
L'approche de détection de cas de non-observance permet l'identification de ces derniers avec une précision de 32.6% et un rappel de 98.5%.
La « Behavioral Strategy » suggère de lier ces erreurs à la psychologie des décideurs, et notamment à leurs biais cognitifs.
Toutefois, cette vision suppose de connecter le niveau d'analyse de l'individu et celui de l'organisation.
Nous proposons pour ce faire un niveau « méso » , la routine de choix stratégique (RCS), où interagissent la psychologie des décideurs et les décisions stratégiques.
Après avoir distingué trois types de RCS, nous formulons des hypothèses d'intervention sur celles-ci visant à prévenir les erreurs stratégiques.
Nous illustrons ces hypothèses par six cas pratiques, en testons certaines par une étude quantitative, et analysons les préférences qui conduisent les dirigeants à les adopter ou non.
Nous concluons en discutant les implications théoriques et pratiques de notre démarche.
Les récents développements des nouvelles technologies de l'information et de la communication font du Web une véritable mine d'information.
Cependant, les pages Web sont très peu structurées.
Par conséquent, il est difficile pour une machine de les traiter automatiquement pour en extraire des informations pertinentes pour une tâche ciblée.
C'est pourquoi les travaux de recherche s'inscrivant dans la thématique de l'Extraction d'Information dans les pages web sont en forte croissance.
Notre travail de thèse se situe à la croisée de ces deux thématiques.
Notre objectif principal est de concevoir et de mettre en œuvre des stratégies permettant de scruter le web pour extraire des Entités Nommées (EN) complexes (EN composées de plusieurs propriétés pouvant être du texte ou d'autres EN) de type entreprise ou de type événement, par exemple.
Nous proposons ensuite des services d'indexation et d'interrogation pour répondre à des besoins d'informations.
Ces travaux ont été réalisés au sein de l'équipe T2I du LIUPPA, et font suite à une commande de l'entreprise Cogniteev, dont le cœur de métier est centré sur l'analyse du contenu du Web.
Les problématiques visées sont, d'une part, l'extraction d'EN complexes sur le Web et, d'autre part, l'indexation et la recherche d'information intégrant ces EN complexes.
Notre première contribution porte sur l'extraction d'EN complexes dans des textes.
Pour cette contribution, nous prenons en compte plusieurs problèmes, notamment le contexte bruité caractérisant certaines propriétés (pour un événement par exemple, la page web correspondante peut contenir deux dates : la date de l'événement et celle de mise en vente des billets).
Pour ce problème en particulier, nous introduisons un module de détection de blocs qui permet de focaliser l'extraction des propriétés sur des blocs de texte pertinents.
Nos expérimentations montrent une nette amélioration des performances due à cette approche.
Nous nous sommes également intéressés à l'extraction des adresses, où la principale difficulté découle du fait qu'aucun standard ne se soit réellement imposé comme modèle de référence.
Nous proposons donc un modèle étendu et une approche d'extraction basée sur des patrons et des ressources libres.
Notre deuxième contribution porte sur le calcul de similarité entre EN complexes.
Dans l'état de l'art, ce calcul se fait généralement en deux étapes : (i) une première calcule les similarités entre propriétés et (ii) une deuxième agrège les scores obtenus pour le calcul de la similarité globale.
Notons que nos principales propositions se situent au niveau de la deuxième étape.
Ainsi, nous proposons trois techniques pour l'agrégation des scores intermédiaires.
Les deux premières sont basées sur la somme pondérée des scores intermédiaires (combinaison linéaire et régression logistique).
La troisième exploite les arbres de décisions pour agréger les scores intermédiaires.
Son originalité vient du fait qu'elle ne nécessite pas de passer par le calcul de scores de similarités intermédiaires.
Le système de COATIS met en oeuvre une méthode opérationnelle pour passer des textes à des représentations sémantiques impliquant la notion de causalité.
Les robots sous-marins peuvent aujourd'hui évoluer dans des environnements complexes difficilement accessibles à l'Homme pour des raisons de coût ou de sécurité.
Or, la complexité de ces milieux impose de doter le vecteur robotique d'une autonomie opérationnelle suffisante afin qu'il puisse mener sa mission à bien tout en préservant son intégrité.
Cela nécessite de développer des lois de commande répondant aux spécificités de l'application.
Ces lois de commande se basent sur des connaissances provenant de différentes disciplines scientifiques ce qui souligne l'interdisciplinarité inhérente à la robotique.
Une fois la loi de commande développée, il faut implémenter le contrôleur sur le robot sous forme de logiciel de contrôle basé sur une architecture logicielle temps-réel.
Or la conception actuelle des lois de commande, sous forme de blocs "monolithiques", rend difficile l'évolution d'une loi de commande d'une application à l'autre, l'intégration de connaissances provenant d'autres disciplines scientifiques que ne maitrisent pas forcément les automaticiens et pénalisent son implémentation sur des architectures logicielles qui nécessitent la modularité.
Cela permettra en outre une projection plus efficace sur l'architecture logicielle.
Nous proposons donc un nouveau formalisme de description des lois de commande selon une composition modulaire d'entités de base appelées Atomes et qui encapsulent les différents éléments de connaissance.
Pour cela, nous enrichissons nos Atomes de contraintes chargées de véhiculer les informations relatives à ces aspects temporels.
Nous proposons également une méthodologie basée sur notre formalisme afin de guider l'implémentation de nos stratégies de commande sur un Middleware temps-réel, dans notre cas le Middleware ContrACT développé au LIRMM.Nous illustrons notre approche par diverses fonctionnalités devant être mises en oeuvre lors de missions d'exploration de l'environnement aquatique et notamment pour l'évitement de parois lors de l'exploration d'un aquifère karstique.
À l'époque du démarrage des travaux présentés dans ce document (2003) l'annotation d'un génome était une tâche longue et fastidieuse.
Avec l'apparition des nouvelles technologies de séquençage, de nombreux outils ont été développés pour faciliter et accélérer ce processus.
Pour les meilleurs, l'annotation d'un génome automatique peut prendre moins de 3 minutes, reportant l'activité chronophage sur l'annotation manuelle.
Ainsi, de nombreux génomes sont déposés dans les banques de séquences tels quels sans annotation manuelle experte.
Il est donc rapidement apparu nécessaire de fournir aux annotateurs la possibilité d'accéder à des bases de données consolidées et spécifiques de leur domaine d'expertise.
Nous présentons dans ce document un outil modulaire d'annotation et de visualisation, GenoBrowser, que nous avons créé dans le cadre de nos travaux de recherche dans une équipe de microbiologie.
Celui-ci nous permet d'intégrer simplement de nouvelles fonctionnalités liées aux données de Omics générées dans l'équipe.
L'architecture de notre outil et la création d'une API (Application Programming Interface) spécifique nous ont permis de développer et de mettre à la disposition de la communauté scientifique deux bases de données (P2CS et P2TF) dédiées aux réseaux de régulation chez les bactéries, ainsi que le serveur web associé pour la prédiction de ces systèmes pour des génomes séquencés de novo.
Ce travail a permis de développer, au sein d'une équipe de recherche, un ensemble d'outils d'aide à l'expertise pour la recherche en génomique environnementale.
Il nous a permis de travailler sur la consolidation et la réutilisation de la quantité croissante de données de type Omics.
Les méthodes de compréhension de la parole visent à extraire des éléments de sens pertinents du signal parlé.
On distingue principalement deux catégories dans la compréhension du signal parlé : la compréhension de dialogues homme/machine et la compréhension de dialogues homme/homme.
En fonction du type de conversation, la structure des dialogues et les objectifs de compréhension varient.
Cependant, dans les deux cas, les systèmes automatiques reposent le plus souvent sur une étape de reconnaissance automatique de la parole pour réaliser une transcription textuelle du signal parlé.
Les systèmes de reconnaissance automatique de la parole, même les plus avancés, produisent dans des contextes acoustiques complexes des transcriptions erronées ou partiellement erronées.
Ces erreurs s'expliquent par la présence d'informations de natures et de fonction variées, telles que celles liées aux spécificités du locuteur ou encore l'environnement sonore.
Celles-ci peuvent avoir un impact négatif important pour la compréhension.
Dans un premier temps, les travaux de cette thèse montrent que l'utilisation d'autoencodeur profond permet de produire une représentation latente des transcriptions d'un plus haut niveau d'abstraction.
Cette représentation permet au système de compréhension de la parole d'être plus robuste aux erreurs de transcriptions automatiques.
Dans un second temps, nous proposons deux approches pour générer des représentations robustes en combinant plusieurs vues d'un même dialogue dans le but d'améliorer les performances du système la compréhension.
La seconde approche propose d'introduire une forme d'information de supervision dans les processus de débruitages par autoencodeur.
À l'aide d'une méthode qualitative et inductive, partant de jugements singuliers, ce travail laisse la parole au locuteur ordinaire et essaie de relever les différents sens ainsi que les diverses fonctions que peuvent avoir les termes vague/vage dans le langage courant.
Les algorithmes de machine learning sont construits pour apprendre, à partir de données, des modèles statistiques de décision ou de prédiction, sur un large panel de tâches.
Quand la distribution des donnée est complexe (e.g. grande dimension avec des interactions non-linéaires entre les variables observées), les hypothèses simplificatrices peuvent être contre-productives.
Il est alors nécessaire de trouver une représentation alternatives des données avant d'apprendre le modèle de décision.
Jusqu'à récemment, beaucoup de représentations standards étaient construites à la main par des experts.
Dans cette thèse, nous nous sommes intéressés à l'apprentissage de représentation de séries temporelles multivariées (STM) et de graphes.
STM et graphes sont des objets complexes qui ont des caractéristiques les rendant difficilement traitables par des algorithmes standards de machine learning.
Par exemple, ils peuvent avoir des tailles variables et ont des alignements non-triviaux, qui empêchent l'utilisation de métriques standards pour les comparer entre eux.
Il est alors nécessaire de trouver pour les échantillons observés (STM ou graphes) une représentation alternatives qui les rend comparables.
Les contributions de ma thèses sont un ensemble d'analyses, d'approches pratiques et de résultats théoriques présentant des nouvelles manières d'apprendre une représentation de STM et de graphes.
Deux méthodes de représentation de STM ont dédiées au suivi d'état caché de systèmes mécaniques.
La première propose une représentation basée "model-based" appelée Sequence-to-graph (Seq2Graph).
Une preuve d'identifiabilité et une extension à des problèmes d'analyse de survie rendent cette approche puissante pour le suivi d'état de système mécaniques.
Deux méthodes de représentation de graphes pour la classification sont aussi proposées.
Une second méthode propose une analyse théorique et pratique du spectre du Laplacien pour la classification de graphes.
Le cadre general de cette etude consiste en une description du syntagme nominal en portugais a partir d'un corpus de textes scientifiques et techniques (documents medicaux en portugais).
Il s'agit d'un travail qui, tout en etant une etude linguistique au sens classique du terme, vise a degager un ensemble de ressources linguistiques qui puissent etre utilisees par la suite dans le traitement automatique des langues (tal) notamment le traitement documentaire et la traduction par ordinateur.
Les langues des signes (LS) se sont développées naturellement au sein des communautés de Sourds.
Ne disposant pas de forme écrite, ce sont des langues orales, utilisant les canaux gestuel pour l'expression et visuel pour la réception.
Ces langues peu dotées ne font pas l'objet d'un large consensus au niveau de leur description linguistique.
Elles intègrent des signes lexicaux, c'est-à-dire des unités conventionnalisées du langage dont la forme est supposée arbitraire, mais aussi – et à la différence des langues vocales, si on ne considère pas la gestualité co-verbale – des structures iconiques, en utilisant l'espace pour organiser le discours.
Dans cette thèse, nous souhaitons montrer les limites de cette approche, en élargissant cette perspective pour envisager la reconnaissance d'éléments utilisés pour la construction du discours ou au sein de structures illustratives.
Pour ce faire, nous montrons l'intérêt et les limites des corpus de linguistes : la langue y est naturelle et les annotations parfois détaillées, mais pas toujours utilisables en données d'entrée de système d'apprentissage automatique, car pas nécessairement cohérentes.
Nous proposons alors la refonte d'un corpus de dialogue en langue des signes française, Dicta-Sign-LSF-v2, avec des annotations riches et cohérentes, suivant un schéma d'annotation partagé par de nombreux linguistes.
Nous proposons ensuite une redéfinition du problème de la reconnaissance automatique de LS, consistant en la reconnaissance de divers descripteurs linguistiques, plutôt que de se focaliser sur les signes lexicaux uniquement.
En parallèle, nous discutons de métriques de la performance adaptées.
Pour réaliser une première expérience de reconnaissance de descripteurs linguistiques non uniquement lexicaux, nous développons alors une représentation compacte et généralisable des signeurs dans les vidéos.
Celle-ci est en effet réalisée par un traitement parallèle des mains, du visage et du haut du corps, en utilisant des outils existants ainsi que des modèles que nous avons développés.
Un prétraitement permet alors de former un vecteur de caractéristiques pertinentes.
Par la suite, nous présentons une architecture adaptée et modulaire d'apprentissage automatique de descripteurs linguistiques, consistant en un réseau de neurones récurrent et convolutionnel.
Nous montrons enfin via une analyse quantitative et qualitative l'effectivité du modèle proposé, testé sur Dicta-Sign-LSF-v2.
L'étude des prédictions du modèle montre alors le bien-fondé de l'approche proposée, avec une performance tout à fait intéressante pour la reconnaissance continue de quatre descripteurs linguistiques, notamment au vu de l'incertitude relative aux annotations elles-mêmes.
La segmentation de ces dernières est en effet subjective, et la pertinence même des catégories utilisées n'est pas démontrée de manière forte.
Indirectement, le modèle proposé pourrait donc permettre de mesurer la validité de ces catégories.
Avec plusieurs pistes d'amélioration envisagées, notamment sur la représentation des signeurs et l'utilisation de corpus de taille supérieure, le bilan est très encourageant et ouvre la voie à une acception plus large de la reconnaissance continue de langue des signes.
Cette thèse concerne l'analyse automatique des SMS et l'extraction des informations qui y sont contenues.
Le point de départ de notre recherche est le constat que la plupart des messages courts, observés dans le corpus alpes4science, présentent des différences en comparaison avec le langage standard.
Les différences sont mises en évidence, d'une part, par la morphologie particulière des mots et, d'autre part, par les règles de syntaxe et de grammaire qui ne sont pas respectées lorsque l'émetteur considère que cela ne nuit pas à l'intelligibilité du message.
À cause des écarts par rapport à la langue standard, le traitement et l'analyse des messages bruités est toujours un défi pour les tâches du TAL.
Le résultat produit par ce modèle a été évalué, par la suite, pour la reconnaissance d'entités nommées au travers d'une série de tests appliqués à l'aide de trois autres systèmes.
Les résultats obtenus ont montré que les performances de ces systèmes de reconnaissance d'entités nommées présentent des améliorations significatives lorsqu'ils sont appliqués sur les SMS automatiquement normalisés en comparaison avec le corpus brut et manuellement transcrit.
Mots-clés : communication médiée par ordinateur, langage SMS, normalisation des SMS, extraction d'
Ces travaux de thèse portent sur la détection audio-visuelle de marqueurs affectifs (rire et sourire) et attentionnels de personnes âgées en interaction sociale avec un robot.
Pour comprendre efficacement et modéliser le comportement des personnes très âgées en présence d'un robot, des données pertinentes sont nécessaires.
J'ai participé à la collection d'un corpus de personnes âgées notamment pour l'enregistrement des données visuelles.
Le système utilisé pour contrôler le robot est un magicien d'Oz, plusieurs scénarios de conversation au quotidien ont été utilisés pour encourager les gens à coopérer avec le robot.
Ces scénarios ont été élaborés dans le cadre du projet ROMEO2 avec l'association Approche.
Nous avons décrit tout d'abord le corpus recueilli qui contient 27 sujets de 85 ans en moyenne pour une durée totale de 9 heures, les annotations et nous avons discuté des résultats obtenus à partir de l'analyse des annotations et de deux questionnaires.
Ma recherche se focalise ensuite sur la détection de l'attention et la détection de rire et de sourire.
Les motivations pour la détection de l'attention consistent à détecter quand le sujet ne s'adresse pas au robot et à adapter le comportement du robot à la situation.
Après avoir considéré les difficultés liées aux personnes âgées et les résultats d'analyse obtenus par l'étude des annotations du corpus, nous nous intéressons à la rotation de la tête au niveau de l'indice visuel et à l'énergie et la qualité de voix pour la détection du destinataire de la parole.
La détection de rire et sourire peut être utilisée pour l'étude sur le profil du locuteur et de ses émotions.
Mes intérêts se concentrent sur la détection de rire et sourire dans la modalité visuelle et la fusion des informations audio-visuelles afin d'améliorer la performance du système automatique.
Les expressions sont différentes des expressions actées ou posés à la fois en apparence et en temps de réaction.
La conception d'un système qui marche sur les données réalistes des personnes âgées est encore plus difficile à cause de plusieurs difficultés à envisager telles que le manque de données pour l'entrainement du modèle statistique, l'influence de la texture faciale et de la façon de sourire pour la détection visuelle, l'influence de la qualité vocale pour la détection auditive, la variété du temps de réaction, le niveau de compréhension auditive, la perte de la vue des personnes âgées, etc.
Les systèmes de détection de la rotation de la tête, de la détection de l'attention et de la détection de rire et sourire sont évalués sur le corpus ROMEO2 et partiellement évalués (détections visuelles) sur les corpus standard Pointing04 et GENKI-4K pour comparer avec les scores des méthodes de l'état de l'art.
Nous avons également trouvé une corrélation négative entre la performance de détection de rire et sourire et le nombre d'évènement de rire et sourire pour le système visuel et le système audio-visuel.
Ce phénomène peut être expliqué par le fait que les personnes âgées qui sont plus intéressées par l'expérimentation rient plus souvent et sont plus à l'aise donc avec des poses variées.
La variété des poses et le manque de données correspondantes amènent des difficultés pour la reconnaissance de rire et de sourire pour les systèmes statistiques.
Les expérimentations montrent que la rotation de la tête peut être efficacement utilisée pour détecter la perte de l'attention du sujet dans l'interaction avec le robot.
Au niveau de la détection de l'attention, le potentiel d'une méthode en cascade qui utilise les modalités d'une manière complémentaire est montré.
L'identification d'appareils photos a récemment fait l'objet d'une grande attention en raison de son apport en terme sécurité et juridique.
Établir l'origine d'un médias numériques, obtenus par un appareil d'imagerie est important à chaque fois que le contenu numériques est présente et utilise comme preuve devant un tribunal.
L'identification d'appareils photos consiste à déterminer la marque, le modèle, ou le dispositif qui a été utilisé pour prendre une image.
Notre première contribution pour l'identification du modèle d'appareil photo numérique est basée sur l'extraction de trois ensembles de caractéristiques puis l'utilisation d'apprentissage automatique.
Ces caractéristiques sont la matrice de cooccurrences, des corrélations inter-canaux mesurant la trace laissée par l'interpolation CFA, et les probabilités conditionnelles calculées dans le domaine JPEG.
Ces caractéristiques donnent des statistiques d'ordre élevées qui complètent et améliorent le taux d'identification.
Les expériences prouvent la force de notre proposition, car la précision obtenue est supérieure à celle des méthodes basées sur la corrélation.
La deuxième contribution est basée sur l'utilisation des CNNs.
Contrairement aux méthodes traditionnelles, les CNNs apprennent simultanément les caractéristiques et la classification.
Nous proposons d'ajouter une couche de pré-traitement (filtre passe-haut applique à l'image d'entrée) au CNN.
Le CNN obtenu donne de très bonnes performances pour une faible complexité d'apprentissage.
La méthode proposée donne des résultats équivalent à ceux obtenu par une approche en deux étapes (extraction de caractéristiques + SVM).
Par ailleurs nous avons également examines les CNNs : AlexNet et GoogleNet.
Tout buzz a un ou plusieurs points d'origine : les sources primaires.
L'information est ensuite relayée par des sources secondaires qui vont accélérer ou non la propagation en fonction de leur degré d'influence.
Tout au long du cycle de vie du buzz, le contenu sémantique est amené à évoluer.
La compréhension d'un buzz sur Internet passe ainsi par l'analyse de ce qui se dit et la qualification des émetteurs.
Nos travaux s'axeront donc autour de deux types d'analyses complémentaires : une analyse topologique des sources (théorie des graphes et des réseaux) et une analyse du contenu textuel (linguistique de corpus).
Le partitionnement consiste à rechercher une partition d'éléments, de sorte que les éléments d'un même cluster soient plus similaires que les éléments de différents clusters.
Les données proviennent de différentes sources et prennent des formes différentes.
L'un des défis consiste à concevoir un système capable de tirer parti des différentes sources de données.
Certaines contraintes peuvent être connues sur les données.
On peut savoir qu'un objet est d'un certain type ou que deux objets partagent le même type ou sont de types différents.
On peut également savoir qu'à l'échelle globale, les différents types d'objets apparaissent avec une fréquence connue.
Dans cette thèse, nous nous concentrons sur le partitionnement avec trois types de contraintes : les contraintes d'étiquettes, les contraintes de paires et les contraintes de lois de puissance.
Une contrainte d'étiquette spécifie dans quel cluster appartient un objet.
Les contraintes par paire spécifient que les paires d'objets doivent ou ne doivent pas partager le même cluster.
Enfin, la contrainte de loi de puissance est une contrainte globale qui spécifie que la distribution des tailles de cluster est soumise à une loi de puissance.
Nous voulons montrer que l'introduction de la semi-supervision aux algorithmes de clustering peut modifier et améliorer les solutions retournées par des algorithmes de clustering non supervisés.
Nous contribuons à cette question en proposant des algorithmes pour chaque type de contraintes.
Nos expériences sur les ensembles de données UCI et les jeux de données en langage naturel montrent la bonne performance de nos algorithmes et donnent des indications pour des travaux futurs prometteurs.
Les différents dialectes de la langue arabe (DA) présentent de grandes variations phonologiques, morphologiques, lexicales et syntaxiques par rapport à la langue Arabe Standard Moderne (MSA).
Jusqu'à récemment, ces dialectes n'étaient présents que sous leurs formes orales et la plupart des ressources existantes pour la langue arabe se limite à l'Arabe Standard (MSA), conduisant à une abondance d'outils pour le traitement automatique de cette variété.
Étant donné les différences significatives entre le MSA et les DA, les performances de ces outils s'écroulent lors du traitement des DA.
La présence de ce dernier d'une manière désordonnée dans le discours pose une sérieuse problématique pour le Traitement Automatique de Langue et fait de cet oral une langue peu dotée.
Toutefois, les ressources nécessaires pour modéliser cet oral sont quasiment inexistantes.
Ainsi, l'objectif de cette thèse consiste à pallier ce manque afin de construire un modèle de langage dédié à un système de reconnaissance automatique pour l'oral parlé dans les médias tunisiens.
Pour ce fait, nous décrivons dans cette thèse une méthodologie de création de ressources et nous l'évaluons par rapport à une tâche de modélisation de langage.
Les résultats obtenu sont encourageants.
Ce travail s'inscrit dans le domaine du contrôle performatif de la synthèse vocale, et plus particulièrement de la modification temps-réel de signaux de voix pré-enregistrés.
Dans un contexte où de tels systèmes n'étaient en mesure de modifier que des paramètres de hauteur, de durée et de qualité vocale, nos travaux étaient centrés sur la question de la modification performative du rythme de la voix.
Une grande partie de ce travail de thèse a été consacrée au développement de Vokinesis, un logiciel de modification performative de signaux de voix pré-enregistrés.
Il a été développé selon 4 objectifs : permettre le contrôle du rythme de la voix, avoir un système modulaire, utilisable en situation de concert ainsi que pour des applications de recherche.
Son développement a nécessité une réflexion sur la nature du rythme vocal et sur la façon dont il doit être contrôlé.
Il est alors apparu que l'unité rythmique inter-linguistique de base pour la production du rythme vocale est de l'ordre de la syllabe, mais que les règles de syllabification sont trop variables d'un langage à l'autre pour permettre de définir un motif rythmique inter-linguistique invariant.
Nous avons alors pu montrer que le séquencement précis et expressif du rythme vocal nécessite le contrôle de deux phases, qui assemblées forment un groupe rythmique : le noyau et la liaison rythmiques.
Nous avons mis en place plusieurs méthodes de contrôle rythmique que nous avons testées avec différentes interfaces de contrôle.
Une évaluation objective a permis de valider l'une de nos méthodes du point de vue de la précision du contrôle rythmique.
De nouvelles stratégies de contrôle de la hauteur et de paramètres de qualité vocale avec une tablette graphique ont été mises en place.
L'utilisation musicale de Vokinesis a été évaluée avec succès dans le cadre de représentations publiques du Chorus Digitalis, pour du chant de type variété ou musique contemporaine.
Les perspectives d'application sont multiples : études scientifiques (recherches en prosodie, en parole expressive, en neurosciences...), productions sonores et musicales, pédagogie des langues, thérapies vocales.
La présente recherche s'interroge sur la présumée dichotomie entre les alternances et les généralisations de surface dans le cadre théorique de la grammaire de constructions.
Plus précisément,l'objectif de cette thèse est ternaire.
Par l'analyse attentive d'une grande quantité de données, nous faisons une description détaillée de l'alternance causative en anglais (The fabric stretched vs. Joan stretched the fabric), nous proposons une méthode qui permet de mesurer la force d'alternance des verbes ainsi que la quantité de sens partagée entre les deux constructions, et, enfin, nous montrons que si l'on veut rendre compte des contraintes au niveau de la construction, l'on doit alors prendre en compte les généralisations de plus bas niveau, telles que les interactions entre le verbe et ses arguments dans le cadre de chaque construction.
Afin d'ajouter au débat entre alternance et généralisations de surface, nous proposons une analyse détaillée des deux constructions qui forment l'alternance causative en anglais : la construction intransitive non-causative d'une part et la construction transitive causative de l'autre.
Notre but est de mesurer la quantité de sens partagée par les deux constructions mais aussi démontrer en quoi ces deux constructions diffèrent.
Dans cette optique, nous prenons en compte trois éléments : construction, verbe et thème (i.e. l'entité sujette à l'évènement dénoté par le verbe).
Nous utilisons la sémantique distributionnelle pour la mesure des similarités sémantiques entre les divers thèmes employés avec chaque verbe dans chaque construction dans notre corpus.
Ce groupement sémantique met en lumière les différents sens verbaux employés avec chaque construction et nous permet d'établir des généralisations quant aux contraintes qui s'appliquent au thème dans chaque construction.
La quantité d'information disponible dans le domaine biomédical ne cesse d'augmenter.
Pour que cette information soit facilement utilisable par les experts d'un domaine, il est nécessaire de l'extraire et de la structurer.
Pour avoir des données structurées, il convient de détecter les relations existantes entre les entités dans les textes.
Nos recherches se sont focalisées sur la question de l'extraction de relations complexes représentant des résultats expérimentaux, et sur la détection et la catégorisation de relations binaires entre des entités biomédicales.
Nous nous sommes intéressée aux résultats expérimentaux présentés dans les articles scientifiques.
Ces résultats sont importants pour les experts en biologie, par exemple pour faire de la modélisation.
Dans le domaine de la physiologie rénale, une base de données a été créée pour centraliser ces résultats d'expérimentation, mais l'alimentation de la base est manuelle et de ce fait longue.
Nous proposons une solution pour extraire automatiquement des articles scientifiques les connaissances pertinentes pour la base de données, c'est-à-dire des résultats expérimentaux que nous représentons par une relation n-aire.
La méthode procède en deux étapes : extraction automatique des documents et proposition de celles-ci pour validation ou modification par l'expert via une interface.
Nous avons également proposé une méthode à base d'apprentissage automatique pour l'extraction et la classification de relations binaires en domaine de spécialité.
Nous nous sommes intéressée aux caractéristiques et variétés d'expressions des relations, et à la prise en compte de ces caractéristiques dans un système à base d'apprentissage.
Nous avons étudié la prise en compte de la structure syntaxique de la phrase et la simplification de phrases dirigée pour la tâche d'extraction de relations.
Nous avons en particulier développé une méthode de simplification à base d'apprentissage automatique, qui utilise en cascade plusieurs classifieurs.
Depuis le début de la décennie, les réseaux de neurones convolutifs profonds pour le traitement d'images ont démontré leur capacité à produire d'excellent résultats.
Pour cela, ces modèles transforment une image en une succession de représentations latentes.
Dans cette thèse, nous travaillerons à l'amélioration de la qualité de ces représentations latentes.
Dans un second temps, nous proposons de structurer l'information en deux sous-espaces latents complémentaires, résolvant un conflit entre l'invariance des représentations et la reconstruction.
La structuration en deux espaces permet ainsi de relâcher la contrainte posée par les architectures classiques, permettant ainsi d'obtenir de meilleurs résultats en classification semi-supervisé.
Enfin, nous nous intéressons au disentangling, c'est-à-dire la séparation de facteurs sémantiques indépendants.
Nous poursuivons nos travaux de structuration des espaces latent et utilisons des coûts adverses pour assurer une séparation efficace de l'information.
Cela permet d'améliorer la qualité des représentations ainsi que l'édition sémantique d'images.
L'étude probabiliste de sûreté (EPS) parasismique est l'une des méthodologies les plus utilisées pour évaluer et assurer la performance des infrastructures critiques, telles que les centrales nucléaires, sous excitations sismiques.
La thèse discute sur les aspects suivants :
(i) Construction de méta-modèles avec les réseaux de neurones pour construire les relations entre les intensités sismiques et les paramètres de demande des structures, afin d'accélérer l'analyse de fragilité.
L'incertitude liée à la substitution des modèles des éléments finis par les réseaux de neurones est étudiée.
(ii) Proposition d'une méthodologie bayésienne avec réseaux de neurones adaptatifs, afin de prendre en compte les différentes sources d'information, y compris les résultats des simulations numériques, les valeurs de référence fournies dansla littérature et les évaluations post-sismiques, dans le calcul de courbes de fragilité.
(iii) Calcul des lois d'atténuation avec les réseaux de neurones. Les incertitudes épistémiques des paramètres d'entrée de lois d'atténuation, tels que la magnitude et la vitesse moyenne des ondes de cisaillement de trente mètres, sont prises en compte dans la méthodologie développée.
(iv) Calcul du taux de défaillance annuel en combinant les résultats des analyses de fragilité et de l'aléa sismique.
Les courbes de fragilité sont déterminées parle réseau de neurones adaptatif, tandis que les courbes d'aléa sont obtenues à partir des lois d'atténuation construites avec les réseaux de neurones.
Les méthodologies proposées sont appliquées à plusieurs cas industriels, tels que le benchmark KARISMA et le modèle SMART.
Les relations paraphrastiques entre plusieurs ensembles de paraphrases peuvent se décrire en termes de suites de transformations textuelles.
Pour qu'il ait paraphrase, il faut qu'une substitution lexicale noyau se mette en route entrainant d'autres modifications syntaxiques, lexicales et morphologiques.
Après avoir décrit les mécanismes de paraphrasage récurrents, nous avons proposé deux formalisations.
La première est théorique et explique les différentes relations paraphrastiques entretenues par les paraphrases entre-elles.
La deuxième formalise les structures paraphrastiques sous-forme de prédicats-arguments.
Nous considérons cette dernière adaptée au traitement automatique de la paraphrase.
Nous avons à la suite implémenté un système d'extraction de structures paraphrastiques.
Il s'agit d'un système opérationnel appliqué à un volume de données relevant de notre domaine d'étude, et dont le but est de donner un exemple concret d'emploi possible de notre formalisation.
Au cours des deux dernières décennies les objets connectés ont révolutionné la traçabilité des phénomènes sociaux.
Les trajectoires sociales laissent aujourd'hui des traces numériques, qui peuvent être analysées pour obtenir une compréhension plus profonde des comportements collectifs.
L'essor de grands réseaux sociaux (comme Facebook, Twitter et plus généralement les réseaux de communication mobile) et d'infrastructures connectées (comme les réseaux de transports publiques et les plate-formes en ligne géolocalisées) ont permis la constitution de grands jeux de données temporelles.
Ces nouveaux jeux de données nous donnent l'occasion de développer de nouvelles méthodes pour analyser les dynamiques temporelles de et dans ces systèmes.
De nos jours, la pluralité des données nécessite d'adapter et combiner une pluralité de méthodes déjà existantes pour élargir la vision globale que l'on a de ces systèmes complexes.
Le but de cette thèse est d'explorer les dynamiques des systèmes sociaux au moyen de trois groupes d'outils : les réseaux complexes, la physique statistique et l'apprentissage automatique.
Le troisième chapitre montre la valeur ajoutée de l'utilisation de jeux de données temporelles.
Puis, nous analysons les résultats d'un algorithme d'apprentissage automatique non supervisé ayant pour but de classer les utilisateurs en fonction de leurs profils.
Le quatrième chapitre explore les différences entre une méthode globale et une méthode locale de détection de communautés temporelles sur des réseaux scientométriques.
Le dernier chapitre combine l'analyse de réseaux complexes et l'apprentissage automatique supervisé pour décrire et prédire l'impact de l'introduction de nouveaux commerces sur les commerces existants.
Nous explorons l'évolution temporelle de l'impact et montrons le bénéfice de l'utilisation de mesures de topologies de réseaux avec des algorithmes d'apprentissage automatique.
Le but de cette thèse est de construire un fragment de grammaire du français rendant compte de la syntaxe de l'infinitif dans le cadre des grammaires d'arbres polychromes (GAP).
La thèse se compose de deux parties.
La première concerne l'étude des problèmes que pose l'infinitif.
L'infinitif, comme un verbe, a des compléments et le constituant qu'il forme avec ses compléments peut être lui aussi un complément d'un verbe, d'un nom, d'un adjectif.
Ce que les grammaires traditionnelles traduisent en parlant d'une double nature : il possède à la fois des propriétés nominales et verbales.
D'où une difficulté spécifique pour faire entrer la syntaxe de l'infinitif dans un modèle formel qui puisse se prêter au traitement automatique.
Nous appelons constituant infinitif l'unité composée d'un verbe infinitif et de ses compléments.
Il est expliqué pourquoi ce terme est préféré à celui de « proposition subordonnée infinitive » .
La seconde partie aborde l'analyse syntaxique des constituants infinitifs en GAP et s'organise autour des contextes dans lesquels apparaît l'infinitif (un verbe, un nom, un adjectif).
Le choix d'une représentation en GAP permet de mettre à l'épreuve ce formalisme et de montrer l'intérêt qu'il y a de séparer les fonctions syntaxiques des catégories.
Ainsi peut-on rendre compte des cas où un constituant d'une catégorie non nominale occupe une position en général occupée par un nom.
Ce travail aura permis, en s'intéressant à la question de la syntaxe de l'infinitif, en partant d'une réinterrogation de certaines études en provenance des grammaires traditionnelles, de parvenir à l'enrichissement d'un formalisme élaboré dans la perspective du TAL.
La problématique traitée dans cette thèse vise à développer une stratégie efficace de détection et de classification des impacts en présence d'incertitudes de modélisation du robot et de son environnement et en utilisant un nombre minimal de capteurs, notamment en l'absence de capteur d'effort.
La première partie de la thèse porte sur la détection d'un impact pouvant avoir lieu à n'importe quel endroit du bras robotique et à n'importe quel moment de sa trajectoire.
Les méthodes de détection d'impacts sont généralement basées sur un modèle dynamique du système, ce qui les rend sujettes au compromis entre sensibilité de détection et robustesse aux incertitudes de modélisation.
A cet égard, une méthodologie quantitative a d'abord été mise au point pour rendre explicite la contribution des erreurs induites par les incertitudes de modèle.
Cette méthodologie a été appliquée à différentes stratégies de détection, basées soit sur une estimation directe du couple extérieur, soit sur l'utilisation d'observateurs de perturbation, dans le cas d'une modélisation parfaitement rigide ou à articulations flexibles.
Une comparaison du type et de la structure des erreurs qui en découlent et de leurs conséquences sur la détection d'impacts en a été déduite.
Dans une deuxième étape, de nouvelles stratégies de détection d'impacts ont été conçues : les effets dynamiques des impacts sont isolés en déterminant la marge d'erreur maximale due aux incertitudes de modèle à l'aide d'une approche stochastique.
Une fois l'impact détecté et afin de déclencher la réaction post-impact du robot la plus appropriée, la deuxième partie de la thèse aborde l'étape de classification.
En particulier, la distinction entre un contact intentionnel (l'opérateur interagit intentionnellement avec le robot, par exemple pour reconfigurer la tâche) et un contact non-désiré (un sujet humain heurte accidentellement le robot), ainsi que la localisation du contact sur le robot, est étudiée en utilisant des techniques d'apprentissage supervisé et plus spécifiquement des réseaux de neurones feedforward.
La généralisation à plusieurs sujet humains et à différentes trajectoires du robot a été étudiée.
Cette thèse de doctorat porte sur la croissance « exclusive » et la vulnérabilité qui caractérisent le développement du Vietnam et plus largement de l'Asie aujourd'hui.
Les chapitres traitent trois aspects importants de la vulnérabilité et du développement inclusif, à savoir : l'informalité (chapitre 1), le dilemme de l'éducation (chapitre 2) et l'emploi atypique (chapitre 3).
Le chapitre 2 se concentre sur la variation des rendements de l'enseignement supérieur dans la population vietnamienne en recourant à différents modèles d'estimation.
Le chapitre 3 est la première étude qui étudie systématiquement les écarts de salaire induits par le statut de travail temporaire dans les pays en développement d'Asie.
Dans l'ensemble, toute la thèse implique que le capital humain, l'emploi et le revenu sont des facettes interdépendantes du bien-être individuel et que certains phénomènes de développement doivent être analysés dans leur hétérogénéité.
La protection de l'environnement naturel constitue un enjeu déterminant pour le futur de l'humanité.
L'ESS, qui partage les principes du développement durable, est particulièrement bien placée pour mettre en œuvre des alternatives de développement plus écologiques.
La thèse appréhende les organisations de l'ESS sous l'angle de l'identité organisationnelle et s'intéresse d'une part à la communication environnementale, d'autre part aux actions concrètes.
L'étude de la communication environnementale a pour terrain le réseau social Twitter.
Elle s'appuie sur un programme codé en Python, et sur les techniques d'exploration automatique de texte.
Elle permet de mettre en évidence plusieurs stratégies rhétoriques.
Une seconde étude traite de sept cas, sur la base d'entretiens semi-directifs.
Elle met en lumière le rôle de l'engagement individuel mais aussi des logiques collectives dans l'action environnementale.
Ce travail apporte une contribution méthodologique en développant l'approche de l'exploration automatique de texte, peu utilisée dans les Sciences de Gestion.
Sur le plan théorique, la thèse introduit la dimension collective en tant qu'identité organisationnelle de l'ESS.
Nous adaptons ensuite un modèle d'action environnementale en identifiant un déterminant supplémentaire spécifique à ces organisations.
Finalement, la recherche invite l'ESS à remettre au centre les questions d'écologie, et donne des pistes pour soutenir les organisations dans une démarche environnementale.
Cependant, la plupart de ces travaux de recherches furent orientés sur des données statiques pour des utilisateurs experts.
Dernièrement, des évolutions technologique et sociétales ont eu pour effet de rendre les données de plus en plus dynamiques et accessibles pour une population plus diverse.
Par exemple des flux de données tels que les emails, les mises à jours de statuts sur les réseaux sociaux, les flux RSS, les systèmes de gestion de versions, et bien d'autres.
Ces nouveaux types de données sont utilisés par une population qui n'est pas forcément entraînée ou éduquée à utiliser des visualisations de données.
La plupart de ces personnes sont des utilisateurs occasionnels, d'autres utilisent très souvent ces données dans leurs travaux.
Dans les deux cas, il est probable que ces personnes n'aient pas reçu de formation formelle en visualisation de données.
Ces changements technologiques et sociétaux ont généré une multitude de nouveaux défis, car la plupart des techniques de visualisations sont conçues pour des experts et des bases de données statiques.
Peu d'études ont été conduites pour explorer ces défis.
Dans ce rapport de thèse, j'adresse la question suivante : « Peut-­on permettre à des utilisateurs non­-experts de créer leur propre visualisation et de contribuer à l'analyse de flux de données ? »
La première étape pour répondre à cette question est d'évaluer si des personnes non formées à la visualisation d'informations ou aux « data sciences » peuvent effectuer des tâches d'analyse de données dynamiques utiles, en utilisant un système de visualisation adapté pour supporter cette tâche.
Dans la première partie de cette dissertation, je présente différents scénarios et systèmes, qui permettent à des utilisateurs non­-experts (de 20 à 300 ou 2000 à 700 000 personnes) d'utiliser la visualisation d'informations pour analyser des données dynamiques.
Un autre problème important est le manque de principes génériques de design pour l'encodage visuel de visualisations d'informations dynamiques.
Dans cette dissertation, je conçois, définis, et explore un espace de design pour représenter des donnés dynamiques pour des utilisateurs non­-experts.
Cette espace de design est structuré par des jetons graphiques représentant des éléments de données qui permettent de construire dans le temps différentes visualisations, tant classiques que nouvelles.
Ce paradigme est inspiré par des théories établies en psychologie du développement, tout autant que par des pratiques passées et présentes de création de visualisation à partir d'objets tangibles.
Je décris tout d'abord les composants et processus de bases qui structurent ce paradigme.
Ensuite, j'utiliserai cette description pour étudier "si et comment" des utilisateur non­-experts sont capables de créer, discuter, et mettre à jour leurs propres visualisations.
Cette étude nous permettra de réviser notre modèle précédent et de fournir une première exploration des phénomènes relatifs à la création d'encodages visuels par des utilisateurs non­-experts sans logiciel.
En résumé, cette thèse contribue à la compréhension des visualisations dynamiques pour des utilisateurs non­-experts.
Les robots sont les futurs compagnons et équipiers de demain.
Cependant, nous sommes encore loin d'un vrai robot autonome, qui agirait de manière naturelle, efficace et sécurisée avec l'homme.
Afin de doter le robot de la capacité d'agir naturellement avec l'homme, il est important d'étudier dans un premier temps comment les hommes agissent entre eux.
Cette thèse commence donc par un état de l'art sur l'action conjointe en psychologie et philosophie avant d'aborder la mise en application des principes tirés de cette étude à l'action conjointe homme-robot.
Nous décrirons ensuite le module de supervision pour l'interaction homme-robot développé durant la thèse.
Une partie des travaux présentés dans cette thèse porte sur la gestion de ce que l'on appelle un plan partagé.
Ici un plan partagé est une séquence d'actions partiellement ordonnées à effectuer par l'homme et/ou le robot afin d'atteindre un but donné.
Dans un premier temps, nous présenterons comment le robot estime l'état des connaissances des hommes avec qui il collabore concernant le plan partagé (appelées états mentaux) et les prend en compte pendant l'exécution du plan.
Cela permet au robot de communiquer de manière pertinente sur les potentielles divergences entre ses croyances et celles des hommes.
Puis, dans un second temps, nous présenterons l'abstraction de ces plan partagés et le report de certaines décisions.
En effet, dans les précédents travaux, le robot prenait en avance toutes les décisions concernant le plan partagé (qui va effectuer quelle action, quels objets utiliser...) ce qui pouvait être contraignant et perçu comme non naturel par l'homme lors de l'exécution car cela pouvait lui imposer une solution par rapport à une autre.
Ces travaux vise à permettre au robot d'identifier quelles décisions peuvent être reportées à l'exécution et de gérer leur résolutions suivant le comportement de l'homme afin d'obtenir un comportement du robot plus fluide et naturel.
Le système complet de gestions des plan partagés à été évalué en simulation et en situation réelle lors d'une étude utilisateur.
Par la suite, nous présenterons nos travaux portant sur la communication non-verbale nécessaire lors de de l'action conjointe homme-robot.
Ces travaux sont ici focalisés sur l'usage de la tête du robot, cette dernière permettant de transmettre des informations concernant ce que fait le robot et ce qu'il comprend de ce que fait l'homme, ainsi que des signaux de coordination.
Finalement, il sera présenté comment coupler planification et apprentissage afin de permettre au robot d'être plus efficace lors de sa prise de décision.
L'idée, inspirée par des études de neurosciences, est de limiter l'utilisation de la planification (adaptée au contexte de l'interaction homme-robot mais coûteuse) en laissant la main au module d'apprentissage lorsque le robot se trouve en situation "connue".
Les premiers résultats obtenus démontrent sur le principe l'efficacité de la solution proposée.
Les services de microblogging (comme Twitter ou Sina Weibo) sont devenu ces dernières années des plateformes très importantes de partage d'information sur l'Internet.
L'analyse de la diffusion d'information dans les microblogs nécessite la collecte de donnée des microblogs, la modélisation de la diffusion d'information et l'application des modèles résultants.
Traiter les données massives issues des microblogs est un défi en soi.
Concevoir des algorithmes efficaces et sans biais afin d'échantillonner les microblogs est ainsi fondamental.
Ceci doit prendre en compte la complexité du phénomène de «  retweet  » qui dépend de la valeur éphémère de l'information, de la topologie du réseau de microblogging et des caractéristiques particulières des éditeurs et retweeteurs.
Deux modèles ont été traditionnellement appliqués à la diffusion d'information  : les cascades indépendantes et modèle à seuil linéaire.
Aucun de ces deux modèles n'est à même de décrire le processus du retweeting de façon correcte.
Il devient donc nécessaire de de caractériser la diffusion d'information.
De plus, une description complète de la relation entre la diffusion d'information dans les microblogs et de popularité des termes recherchés sur Internet serait utile.
Ces travaux de thèse présentent une analyse complète de la diffusion d'information dans les microblogs.
Les contributions ce cette thèse sont les suivantes  :
1) Il y'a deux technique d'échantillonnage sans biais pour les réseaux sociaux  : la marche aléatoire de Métropolis-Hastings (MHRW), et la méthode d'échantillonnage sans biais de graphes dirigés (USDSG).
Néanmoins ces deux méthodes peuvent aboutit à un taux important d'auto-échantillonnage quand elles sont appliquées à des microblogs.
Pour résoudre ce problème, j'ai modélisé l'échantillonnage d'un OSN par un processus de Markov et j'en ai déduit les conditions nécessaires et suffisantes d'un échantillonnage sans biais.
L'évaluation empirique montre que la moyenne des dégrées des nœuds échantillonnés est proche de la vérité terrain alors que pour MHRW et USDSG elle est 2 à 4 fois supérieure.
2) La seconde contribution de cette thèse vise les lacunes des modèles en cascades indépendantes et de seuils linéaires.
J'ai développé un modèle fondé sur les processus de Galton-Watson avec mort (GWK) qui prennent en compte tous les facteurs importants du processus de retweet.
Ce nouveau modèle est validé par une application sur des données issues de Twitter et de Weibo.
3) La troisième contribution est relative au développement d'un modèle économique du marché des acteurs actifs dans le domaine du marketing sur les mots clés dans les sites de recherches. J'ai développé des méthodes de gestion de portfolios de mots clés et montrés que ces portfolios permettent d'améliorer fortement les rendements sans augmenter le niveau de risque.
Ces dernières années, avec l'apparition des sites tels que Youtube, Dailymotion ou encore Blip TV, le nombre de vidéos disponibles sur Internet aconsidérablement augmenté.
Le volume des collections et leur absence de structure limite l'accès par le contenu à ces données.
Le résumé automatique est un moyen de produire des synthèses qui extraient l'essentiel des contenus et les présentent de façon aussi concise que possible.
Dans ce travail, nous nous intéressons aux méthodes de résumé vidéo par extraction, basées sur l'analyse du canal audio.
Nous traitons les différents verrous scientifiques liés à cet objectif : l'extraction des contenus, la structuration des documents, la définition et l'estimation des fonctions d'intérêts et des algorithmes de composition des résumés.
Sur chacun de ces aspects, nous faisons des propositions concrètes qui sont évaluées.
Sur l'extraction des contenus, nous présentons une méthode rapide de détection de termes.
La principale originalité de cette méthode est qu'elle repose sur la construction d'un détecteur en fonction des termes cherchés.
Nous montrons que cette stratégie d'auto-organisation du détecteur améliore la robustesse du système, qui dépasse sensiblement celle de l'approche classique basée sur la transcription automatique de la parole.
Nous présentons ensuite une méthode de filtrage qui repose sur les modèles à mixtures de Gaussiennes et l'analyse factorielle telle qu'elle a été utilisée récemment en identification du locuteur.
L'originalité de notre contribution tient à l'utilisation des décompositions par analyse factorielle pour l'estimation supervisée de filtres opérants dans le domaine cepstral.
Nous abordons ensuite les questions de structuration de collections de vidéos.
Nous montrons que l'utilisation de différents niveaux de représentation et de différentes sources d'informations permet de caractériser le style éditorial d'une vidéo en se basant principalement sur l'analyse de la source audio, alors que la plupart des travaux précédents suggéraient que l'essentiel de l'information relative au genre était contenue dans l'image.
Le troisième axe de ce travail concerne le résumé lui-même.
Dans le cadre du résumé automatique vidéo, nous essayons, dans un premier temps, de définir ce qu'est une vue synthétique.
S'agit-il de ce qui le caractérise globalement ou de ce qu'un utilisateur en retiendra (par exemple un moment émouvant, drôle....
Nous proposons ensuite un algorithme de recherche du résumé d'intérêt maximal qui dérive de celui introduit dans des travaux précédents, basé sur la programmation linéaire en nombres entiers.
Les ressources lexicographiques bilingues des expressions polylexicales (EPL) sont encore rares et elles le sont davantage pour le couple de langues français / arabe qui nous intéresse.
Les recherches concernant ce couple de langues existent peu.
Nous nous intéressons dans notre projet de recherche à extraire, à partir de corpus parallèle et comparable fr - ar spécialisés, des expressions polylexicales notamment à base verbale.
Ces constructions, de notre point de vue, constituent un aspect pertinent à étudier dans les textes spécialisés.
Le corpus constitué se rattachera au domaine médical, un domaine stratégique à l'heure actuelle où le monde entier fait face à une pandémie.
Nous profiterons donc de la disponibilité des articles scientifiques vulgarisés qui abondent en ce moment.
Dans notre étude nous adopterons une approche contrastive qui consiste à recenser et analyser les EPL et leurs équivalences traductionnelles, et ceci dans une approche corpus based ou basé sur corpus.
Notre objectif principal est d'approfondir les recherches sur ce couple de langues encore trop peu étudié dans ce domaine, et de fournir des ressources pour les traducteurs et les apprenants de la traduction spécialisée.
Il s'agira in fine d'exploiter les résultats dans le domaine de l'apprentissage de la traduction spécialisé assistée par ordinateur, en s'inspirant de l'approche d'Apprentissage guidée par les données, ou Data Driven Learning.
Toutes les étapes seront réalisées en exploitant les outils TAL appropriés à chaque langue et à chaque procédure.
Il en découle un découpage hiérarchique des différentes tâches à réaliser afin d'analyser un énoncé.
Les systèmes classiques du TAL reposent sur des analyseurs indépendants disposés en cascade au sein de chaînes de traitement (pipelines).
Cette approche présente un certain nombre de limitations : la dépendance des modèles à la sélection empirique des traits, le cumul des erreurs dans le pipeline et la sensibilité au changement de domaine.
Ces limitations peuvent conduire à des pertes de performances particulièrement importantes lorsqu'il existe un décalage entre les conditions d'apprentissage des modèles et celles d'utilisation.
Un tel décalage existe lors de l'analyse de transcriptions automatiques de parole spontanée comme par exemple les conversations téléphoniques enregistrées dans des centres d'appels.
Le tuffeau est un matériau de construction calcaire largement utilisé dans la vallée de la Loire pour les bâtiments historiques.
Le temps passant, il est confronté à des problèmes de pathologies, principalement liées à sa teneur en eau.
Pour évaluer cet indicateur, le radar à sauts de fréquence (SFR) a été retenu parmi les techniques d'évaluation non destructives.
Cette dernière peut être associée à une technique d'inversion « forme d'onde » dans le domaine fréquentiel (FWFI), technique performante pour obtenir la permittivité, directement en lien avec la teneur en eau.
L'inversion FWFI d'un signal SFR utilise, pour le calcul direct, un modèle analytique de propagation des ondes, basé sur les fonctions de Green d'un dipôle, et sur le modèle de dispersion de Jonscher pour la caractérisation EM du milieu.
Ce modèle direct est validé par une approche numérique utilisant le logiciel HFSS et par une approche expérimentale sur des blocs tests.
Le problème inverse, comprenant une fonction Objectif et un algorithme de minimisation de type génétique, est ensuite étudié pour obtenir la permittivité du milieu ausculté.
En parallèle, une base de données entre teneur en eau et permittivité du tuffeau est construite à l'aide de mesures EM en cellule sur des échantillons homogènes.
Enfin, cette méthode est validée sur plusieurs applications pratiques, comprenant l'acquisition d'informations géométriques (estimation d'un front d'eau) et la caractérisation du matériau (estimation du gradient d'eau et cartographie hydrique).
Cette thèse est constituée de deux volets, théorique et pratique.
Nous aborderons également le lien indéfectible existant entre la terminographie et la traduction spécialisée tant au niveau didactique que professionnel.
Dans un second temps, à l'aide des extracteurs de termes, nous entamerons l'opération d'extraction des termes à partir du corpus délimité.
Après avoir mené un tri des termes générés selon des critères linguistiques bien définis, ces termes feront l'objet d'une analyse lexicale et morphologique afin de souligner les moyens dont la langue arabe dispose pour la formation des termes ainsi que l'effet de son contact avec les autres langues.
De plus, nous mènerons une enquête de terrain, par le biais d'un questionnaire destiné aux traducteurs et interprètes de conférence, dans le but d'examiner leurs besoins urgents en matière de terminologie médicale et évaluer leur usage des outils de gestion terminologique disponibles.
Cette étude s'est donc fixée plusieurs finalités à savoir, répondre aux besoins de dénominations urgents dans le domaine médical et mettre à la disposition des traducteurs et interprètes de conférence un outil de gestion terminologique aisément accessible pour les assister dans leur travail exigeant la pertinence et la rapidité.
Elle tentera également à ouvrir de nouveaux horizons sur les méthodes d'enseignement de la traduction spécialisée en soulignant la contribution considérable de la terminotique dans l'acquisition des compétences professionnelles dans ce domaine.
L'étude se penchera également sur l'évaluation de l'utilisation des logiciels du TAL afin de dénoter leurs apports et limites dans l'extraction terminologique, ce qui pourrait guider les concepteurs pour améliorer la performance des outils développés.
Alors que l'efficacité et l'intérêt des dispositifs d'enseignement bilingue précoce ont été largement démontrés, peu d'études se sont penchées en détail sur les processus à l'oeuvre dans l'acquisition précoce d'une langue étrangère.
Cette étude vise à analyser le développement de la compétence en L2 chez de jeunes enfants, dans un contexte où l'école est la seule source d'exposition à celle-ci.
À partir d'un corpus recueilli dans des classes de maternelle et primaire de deux écoles franco-américaines en Californie, nous proposons d'examiner comment des enfants anglophones de 4 à 7 ans scolarisés dans un dispositif de type immersif construisent leur compétence en français.
En nous appuyant sur un cadre interactionniste d'acquisition de la L2, nous montrerons comment ce contexte spécifique impacte le déroulement de l'acquisition, en termes d'opportunités d'interaction et d'input adressé aux enfants, et examinerons dans quelle mesure il fournit un socle aux premières acquisitions en français.
Nous suggérons que l'idée selon laquelle commencer l'apprentissage d'une L2 le plus jeune possible serait garant d'une meilleure maîtrise de la langue n'est pas nécessairement valable dans ce contexte particulier d'acquisition, tant de multiples autres facteurs sont à prendre en compte (conscience linguistique et métalinguistique, stratégies d'interaction, influence de la L1).
Des pistes pédagogiques sont également proposées à partir de ces résultats, afin de fournir quelques orientations pour l'enseignement bilingue en maternelle.
Cette thèse propose une analyse sociophonétique synchronique et diachronique de l'anglais de Tyneside à partir de deux sous-corpus du Corpus Diachronique de l'Anglais de Tyneside (DECTE) datant des années 1970 et de 1990 (Corrigan, Buchstaller, Mearns, and Moisl, 2012).
Elle comporte deux grands volets : (1) une analyse de la variation inter et intra-locuteurs par le biais de transcriptions phonétiques des variantes linguistiques de FACE, GOAT, PRICE et MOUTH (Wells 1982) à l'aide d'une analyse factorielle multiple (AFM, Escofier 2008, Husson et al. 2011) (2) une étude acoustique des trajectoires formantiques de ces quatre ensembles lexicaux à l'aide de modèles mixtes additifs généralisés afin de vérifier la pertinence du codage (GAMMs, Wood 2015).
Pour ce premier volet, nous proposons un profilage sociolinguistique de 44 locuteurs de Gateshead et de Newcastle, à partir de données phonétiques transcrites dans les années 1970 lors de l'Enquête Linguistique de Tyneside (TLS, Strang 1968).
Bien que notre analyse porte sur la totalité des transcriptions du système phonétique des locuteurs, l'accent est davantage porté sur FACE, GOAT, PRICE and MOUTH.
Selon l'AFM suivi d'une classification, FACE est l'ensemble lexical le plus déterminant dans la catégorisation sociolinguistique des locuteurs.
La symétrie entre FACE et GOAT (Watt 1999), PRICE et MOUTH est plus nette chez les femmes : celles de la classe moyenne privilégient une diphtongue fermantes dans entre FACE et GOAT et une attaque de diphthongue ouverte pour PRICE et MOUTH, tandis que les femmes issues de classes plus populaires optent pour la monophtongue pan-régionale pour FACE et GOAT, avec une attaque davantage fermée et antérieure chez PRICE et MOUTH.
La monophtongue centrale de GOAT la variante privilégiée par des hommes à l'accent local moins marqué, ce qui entre en cohérence avec les résultats de Watt (1998) dans le sous-corpus des années 1990 du DECTE.
Le second volet analyse les trajectoires formantiques de FACE, GOAT et PRICE.
Le but premier de cette analyse est de vérifier la correspondance des transcriptions avec le contour formantique.
Les résultats confirment la pertinence du codage au niveau des liste de mots (TLS and PVC).
Les différences entre les deux variantes principales de PRICE ([aɪ] vs. [eɪ]) se révèlent être foncièrement différentes tant sur le plan de l'attaque, de la trajectoire et de la cible.
Les progrès des technologies de l'information, comme les technologies de la communication et la technologie des bases de données, nous permettent d'accéder à de vastes quantités d'informations et provoquent une augmentation exponentielle du nombre de documents disponibles en ligne.
Une extraction et une extraction efficaces deviennent de plus en plus difficiles.
La représentation de document vise à représenter l'entrée de document dans un vecteur de longueur fixe pour réduire la complexité des documents et les rendre plus faciles à manipuler.
Par conséquent, la représentation de documents joue un rôle important dans de nombreuses applications du monde réel, par exemple la recherche d'informations, le regroupement de textes et la classification.
La moyenne pondérée simple des vecteurs de mots est un moyen efficace de représenter des phrases.
Cependant, lorsque le document est long, il peut ne pas être efficace et perdre des distinctions fines.
La raison en est que lorsque le document est long, il est susceptible de contenir des mots provenant de nombreux sujets différents et, par conséquent, la création d'un seul vecteur ignorerait la structure thématique.
Pour surmonter la limitation mentionnée ci-dessus, nous voulons faire quelque chose d'un peu différent.
C'est-à-dire qu'au lieu d'obtenir des représentations par simple moyenne, nous pouvons représenter un document par un ensemble de vecteurs et nous pouvons obtenir une nouvelle représentation.
Cela s'avère plus raisonnable car l'ensemble de vecteurs peut couvrir différentes parties des documents.
Par ce moyen, la représentation peut avoir la capacité de couvrir différents sujets.
La partie centrale du modèle de représentation de document est l'encodeur de document hiérarchique.
Nous voulons proposer une méthode pour d'abord pré-former des codeurs de transformateur bidirectionnel hiérarchique au niveau du document sur des données non étiquetées.
En outre, les modèles de documents pré-entraînés peuvent être adaptés à différentes tâches (recherche d'informations, regroupement de texte et classification de texte) et collections via un réglage fin.
Dans ces travaux de thèse nous abordons l'expressivité de la parole lue avec un type de données particulier qui sont les livres audio.
Les livres audio sont des enregistrements audio d'œuvres littéraires fait par des professionnels (des acteurs, des chanteurs, des narrateurs professionnels) ou par des amateurs.
Ces enregistrements peuvent être destinés à un public particulier (aveugles ou personnes mal voyantes).
La disponibilité de ce genre de données en grande quantité avec une assez bonne qualité a attiré l'attention de la communauté scientifique en traitement automatique du langage et de la parole en général, ainsi que des chercheurs spécialisés dans la synthèse de parole expressive.
Pour explorer ce vaste champ d'investigation qui est l'expressivité, nous proposons dans cette thèse d'étudier trois entités élémentaires de l'expressivité qui sont véhiculées par les livres audio : l'émotion, les variations liées aux changements discursifs et les propriétés du locuteur.
Nous traitons ces patrons d'un point de vue prosodique.
L'information occupe désormais une place centrale dans notre vie quotidienne, elle est à la fois omniprésente et facile d'accès.
Pourtant, l'extraction de l'information à partir des données est un processus souvent inaccessible.
En effet, même si les méthodes de fouilles de données sont maintenant accessibles à tous, les résultats de ces fouilles sont souvent complexes à obtenir et à exploiter pour l'utilisateur.
La fouille de motifs combinée à l'utilisation de contraintes est une direction très prometteuse de la littérature pour à la fois améliorer l'efficience de la fouille et rendre ses résultats plus appréhendables par l'utilisateur.
Cependant, la combinaison de contraintes désirée par l'utilisateur est souvent problématique car, elle n'est pas toujours adaptable aux caractéristiques des données fouillées tel que le bruit.
Dans cette thèse, nous proposons deux nouvelles contraintes et un algorithme pour pallier ce problème.
La contrainte de robustesse permet de fouiller des données bruitées en conservant la valeur ajoutée de la contrainte de contiguïté.
La contrainte de clôture allégée améliore l'appréhendabilité de la fouille de motifs tout en étant plus résistante au bruit que la contrainte de clôture classique.
L'algorithme C3Ro est un algorithme générique de fouille de motifs séquentiels intégrant de nombreuses contraintes, notamment les deux nouvelles contraintes que nous avons introduites, afin de proposer à l'utilisateur la fouille la plus efficiente possible tout en réduisant au maximum la taille de l'ensemble des motifs extraits.
C3Ro rivalise avec les meilleurs algorithmes de fouille de motifs de la littérature en termes de temps d'exécution tout en consommant significativement moins de mémoire.
C3Ro a été expérimenté dans le cadre de l'extraction de compétences présentes dans les offres d'emploi sur le Web
Nombre des succès de l'apprentissage profond reposent sur la disponibilité de données massivement collectées et annotées, exploités par des algorithmes supervisés.
Ces annotations, cependant, peuvent s'avérer difficiles à obtenir.
La conception de méthodes peu gourmandes en annotations est ainsi un enjeu important, abordé dans des approches semi-supervisées ou faiblement supervisées.
Par ailleurs ont été récemment introduit les réseaux génératifs profonds, capable de manipuler des distributions complexes et à l'origine d'avancées majeures, en édition d'image et en adaptation de domaine par exemple.
Dans cette thèse, nous explorons comment ces outils nouveaux peuvent être exploités pour réduire les besoins en annotations.
En premier lieu, nous abordons la tâche de prédiction stochastique.
Il s'agit de concevoir des systèmes de prédiction structurée tenant compte de la diversité des réponses possibles.
Ensuite, nous étudions la décomposition en deux facteurs latents indépendants dans le cas où un seul facteur est annoté.
Nous proposons des modèles qui visent à retrouver des représentations latentes sémantiquement cohérentes de ces facteurs explicatifs.
Le premier modèle est appliqué en génération de données de capture de mouvements, le second, sur des données multi-vues.
Enfin, nous nous attaquons au problème, crucial en vision par ordinateur, de la segmentation d'image.
Nous proposons un modèle, inspiré des idées développées dans cette thèse, de segmentation d'objet entièrement non supervisé.
Cette thèse explore l'utilisation de données multi-échelles pour modéliser une représentation tridimensionnelle (3D) et générer un registre numérique complet d'un assemblage de fossiles contenant des hominines à partir de l'unité lithostratigraphique P à Kromdraai situé dans le " berceau de l'humanité " classé au patrimoine mondial par l'UNESCO (Province de Gauteng, Afrique du Sud).
Nous avons réalisé une analyse multi-scalaire du site, avec l'application de méthodes de photogrammétrie terrestre et aérienne.
Conformément aux principes et directives de la gestion du patrimoine archéologique mandatés par les agences internationales telles que l'UNESCO, nous présentons également un protocole de documentation du patrimoine.
Nous avons utilisé des technologies de capture de données 3D pour numériser le site de Kromdraai et ses éléments archéologiques découverts entre 2014 et 2018 lors des fouilles.
Cette recherche présente une technique originale développée pour la visualisation et la quantification des sédiments volumiques prélevés sur le site à chaque période de fouille par chaque fouilleur.
Les estimations de volume calculées à l'aide de la photogrammétrie 3D fournissent un contexte temporel et spatial des sédiments prélevés lors des fouilles successives, et permettent un repositionnement virtuel et plus précis des vestiges découverts ex situ.
De plus, nous avons mis en place une modélisation des métadonnées pour démontrer l'utilisation d'un système de gestion de base de données 4D pour la fusion, l'organisation et la diffusion de l'ensemble des données du site de Kromdraai et le partage de la propriété intellectuelle.
Nous introduisons également l'une des premières approches statistiques de la modélisation spatiale 3D dans un site Plio-Pléistocène porteurs d'hominines en en Afrique du Sud.
En mettant en œuvre des méthodes classiques de tests statistiques telles le partitionnement de données spatiales 3D, nous avons étudié les modèles de l'assemblage de fossiles dans l'unité P, ainsi qu'un échantillon de 810 spécimens catalogués entre 2014 et 2018.
Le regroupement de bovidés, de carnivores, d'homininés et de primates non humains a révélé un modèle de distribution spatiale non uniforme des fossiles in situ.
La quantité d'informations sur Internet aujourd'hui accable la plupart des utilisateurs.
La découverte d'informations pertinentes (p. Ex. Des nouvelles à lire ou des vidéos à regarder) prend du temps et est fastidieuse ; pourtant, elle fait partie du travail quotidien d'au moins 80% des employés en Amérique du Nord.
Plusieurs systèmes de filtrage d'informations pour le Web peuvent faciliter cette tâche pour les utilisateurs.
Les exemples se retrouvent dans des familles telles que les réseaux sociaux, les systèmes de notation sociale et les systèmes de bookmarking social.
Tous ces systèmes exigent que l'engagement de l'utilisateur fonctionne (par exemple, la soumission ou l'évaluation du contenu).
Ils fonctionnent bien dans une communauté Internet, mais souffrent dans le cas des petites communautés.
En effet, dans les petites communautés, l'apport des utilisateurs est plus rare.
Nous nous concentrons sur les communautés d'un endroit qui sont des communautés qui regroupent les gens qui vivent, travaillent ou étudient dans la même région.
Exemples de communautés d'un lieu : (i) les étudiants d'un campus, (ii) les personnes vivant dans un quartier ou (iii) les chercheurs travaillant sur le même site.
Anecdote nous savons que seulement 0,3% des travailleurs contribuent quotidiennement à leur réseau social d'entreprise.
Cette information montre qu'il ya un manque d'engagement des utilisateurs dans les communautés d'un endroit.
Cette thèse est consacrée à la modélisation et l'optimisation non convexe basées sur la programmation DC et DCA pour certaines classes de problèmes issus de deux domaines importants : le Data Mining et la Cryptologie.
Il s'agit des problèmes d'optimisation non convexe de très grande dimension pour lesquels la recherche des bonnes méthodes de résolution est toujours d'actualité.
Notre travail s'appuie principalement sur la programmation DC et DCA.
Cette démarche est motivée par la robustesse et la performance de la programmation DC et DCA, leur adaptation aux structures des problèmes traités et leur capacité de résoudre des problèmes de grande dimension.
La thèse est divisée en trois parties.
Dans la première partie intitulée Méthodologie nous présentons des outils théoriques servant des références aux autres.
Le premier chapitre concerne la programmation DC et DCA tandis que le deuxième porte sur les algorithmes génétiques.
Dans la deuxième partie nous développons la programmation DC et DCA pour la résolution de deux classes de problèmes en Data Mining.
Dans le chapitre quatre, nous considérons le modèle de la classification floue FCM et développons la programmation DC et DCA pour sa résolution.
Plusieurs formulations DC correspondants aux différentes décompositions DC sont proposées.
Notre travail en classification hiérarchique (chapitre cinq) est motivé par une de ses applications intéressante et très importantes, à savoir la communication multicast.
C'est un problème non convexe, non différentiable de très grande dimension pour lequel nous avons reformulé sous la forme des trois programmes DC différents et développé les DCA correspondants.
La troisième partie porte sur la Cryptologie.
Nous proposons une méthode de résolution des deux problèmes PP et PPP par DCA et une méthode de coupes dans le dernier chapitre
Cette thèse présente une réflexion sur les verbes à particule et les verbes prépositionnels en anglais.
La relation qui existe entre la syntaxe et la sémantique est une des pierres angulaires de cette étude.
Nous étudierons le rôle de la configuration syntaxique, de la structure argumentale et des connaissances extralinguistiques dans la construction du sens.
Les aspects théoriques abordés sont la catégorisation des particules et des prépositions, l'interaction du sémantisme des particules - prépositions et le sémantisme verbal, la structure interne des verbes à particule, et les raisons pour lesquelles les particules peuvent apparaître soit avant, soit après le complément régi par le verbe.
Chaque combinaison est analysée en contexte afin d'identifier les facteurs qui influencent l'interprétation sémantique finale de la combinaison.
Au cours de cette étude, nous identifierons toute une gamme de facteurs qui influencent l'interprétation sémantique finale des verbes à particule et des verbes prépositionnels anglais ainsi que leur interaction.
Dans un contexte de maintien et de développement de la compétitivité de l'entreprise, les services de veille alimentent cette dernière en informations susceptibles d'être utilisées comme objets de référence pour l'analyse de l'environnement et l'aide à la prise de décisions.
Le projet ALSEM vise la conception d'un système informatique assistant le veilleur dans ses travaux d'exploration et de combinaison des ressources informationnelles concernant l'environnement de l'entreprise.
L'objectif de ce projet est l'intégration au sein d'un même système de techniques offrant un cadre méthodologique de gestion de l'information efficace et plus adapté à une tâche spécifique et complexe comme la veille.
Notre travail se situe au confluent de trois disciplines : le traitement automatique de la langue, la sémantique et l'ingénierie des connaissances.
Il vise à l'élaboration de techniques d'extraction automatique de l'information contenue dans les textes via une analyse sémantique de ces derniers et de techniques d'interprétation, de modélisation de l'information extraite pour la rendre opérationnelle.
Le Web sémantique est la vision de la prochaine génération de Web proposé par Tim Berners
Les outils traditionnels d'interrogation et de raisonnement sur les données du Web sémantique sont conçus pour fonctionner dans un environnement centralisé.
A ce titre, les algorithmes de calcul traditionnels vont inévitablement rencontrer des problèmes de performances et des limitations de mémoire.
De gros volumes de données hétérogènes sont collectés à partir de différentes sources de données par différentes organisations.
Ces sources de données présentent souvent des divergences et des incertitudes dont la détection et la résolution sont rendues encore plus difficiles dans le big data.
Mes travaux de recherche présentent des approches et algorithmes pour une meilleure exploitation de données dans le contexte big data et du web sémantique.
Nous avons tout d'abord développé une approche de résolution des identités (Entity Resolution) avec des algorithmes d'inférence et d'un mécanisme de liaison lorsque la même entité est fournie dans plusieurs ressources RDF décrite avec différentes sémantiques et identifiants de ressources URI.
Nous avons également développé un moteur de réécriture de requêtes SPARQL basé le modèle MapReduce pour inférer les données implicites décrites intentionnellement par des règles d'inférence lors de l'évaluation de la requête.
L'approche de réécriture traitent également de la fermeture transitive et règles cycliques pour la prise en compte de langages de règles plus riches comme RDFS et OWL.
La deuxième contribution concerne le traitement d'incohérence dans le big data.
Nous étendons l'approche présentée dans la première contribution en tenant compte des incohérences dans les données.
La troisième contribution concerne le raisonnement et l'interrogation sur la grande quantité données RDF incertaines.
Nous proposons une approche basée sur MapReduce pour effectuer l'inférence de nouvelles données en présence d'incertitude.
Nous proposons un algorithme d'évaluation de requêtes sur de grandes quantités de données RDF probabilistes pour le calcul et l'estimation des probabilités des résultats.
Nous étudions le problème de détection de relations visuelles de la forme (sujet, prédicat, objet) dans les images, qui sont des entités intermédiaires entre les objets et les scènes visuelles complexes.
Cette thèse s'attaque à deux défis majeurs : (1) le problème d'annotations coûteuses pour l'entrainement de modèles fortement supervisés, (2) la variation d'apparence visuelle des relations.
Nous proposons un premier modèle de détection de relations visuelles faiblement supervisé, n'utilisant que des annotations au niveau de l'image, qui, étant donné des détecteurs d'objets pré-entrainés, atteint une précision proche de celle de modèles fortement supervisés.
Nous validons expérimentalement le bénéfice apporté par chacune de ces composantes sur des bases de données réelles.
Cette thèse porte sur l'établissement de similarités de données textuelles dans le domaine de la gestion de la relation client.
Elle se décline en deux parties : - l'analyse automatique de messages courts en réponse à des questionnaires de satisfaction ; - la recherche de produits à partir de l'énonciation de critères au sein d'une conversation écrite mettant en jeu un humain et un programme agent.
La première partie a pour objectif la production d'informations statistiques structurées extraites des réponses aux questions.
Les idées exprimées dans les réponses sont identifiées, organisées selon une taxonomie et quantifiées.
La seconde partie vise à transcrire les critères de recherche de produits en requêtes compréhensibles par un système de gestion de bases de données.
Les critères étudiés vont de critères relativement simples comme la matière du produit jusqu'à des critères plus complexes comme le prix ou la couleur.
Les deux parties se rejoignent sur la problématique d'établissement de similarités entre données textuelles par des techniques de TAL.
Les principales difficultés à surmonter sont liées aux caractéristiques des textes, rédigés en langage naturel, courts, et comportant fréquemment des fautes d'orthographe ou des négations.
L'établissement de similarités sémantiques entre mots (synonymie, antonymie, etc) et l'établissement de relations syntaxiques entre syntagmes (conjonction, opposition, etc) sont également des problématiques abordées.
Nous étudions également dans cette thèse des méthodes de regroupements et de classification automatique de textes afin d'analyser les réponses aux questionnaires de satisfaction.
Le sociologue Bourdieu définit le capital social comme : "L'ensemble des ressources actuelles ou potentielles qui sont liées à la possession d'un réseau durable de relations".
Sur Twitter, les abonnements, mentions et retweets créent un réseau de relations pour chaque utilisateur dont les ressources sont l'obtention d'informations pertinentes, la possibilité d'être lu, d'assouvir un besoin narcissique, de diffuser efficacement des messages.
Certains utilisateurs Twitter-appelés capitalistes sociaux-cherchent à maximiser leur nombre d'abonnements pour maximiser leur capital social.
Nous introduisons leurs techniques, basées sur l'échange d'abonnements et l'utilisation de hashtags dédiés.
Afin de mieux les étudier, nous détaillons tout d'abord une méthode pour détecter à l'échelle du réseau ces utilisateurs en se basant sur leurs abonnements et abonnés.
Puis, nous montrons avec un compte Twitter automatisé que ces techniques permettent de gagner efficacement des abonnés et de se faire beaucoup retweeter.
Nous établissons ensuite que ces dernières permettent également aux capitalistes sociaux d'occuper des positions qui leur accordent une bonne visibilité dans le réseau.
De plus, ces méthodes rendent ces utilisateurs influents aux yeux des principaux outils de mesure.
Nous mettons en place une méthode de classification supervisée pour détecter avec précision ces utilisateurs et ainsi produire un nouveau score d'influence.
Le cycle de l‟information, de la collecte à la dissémination, est central en intelligence économique.
Nos travaux ont pour sujet l‟étude de l‟impact que ce fameux web 2.0 a sur le cycle en question et nous proposons des méthodes et outils afin de tirer parti de ce nouveau paradigme, et ce, pour chaque étape du cycle
La thèse est préparée dans le cadre d'une convention de cotutelle sous la direction des Professeurs Jean-Hugues Chauchat (ERIC-Lyon2) et N.V. Charonova (Université Nationale Polytechnique de Kharkov en Ukraine).1.
Les résultats obtenus peuvent se résumer ainsi : Rétrospective des fondations théoriques sur la formalisation des connaissances et langue naturelle en tant que précurseurs de l'ingénierie des ontologies.
Actualisation de l'état de l'art sur les approches générales dans le domaine de l'apprentissage d'ontologie, et sur les méthodes d'extraction des termes et des relations sémantiques.
Une méthode d'apprentissage des patrons morphosyntaxiques et d'installation de taxonomies partielles de termes.
Une méthode de formation de classes sémantiques représentant les concepts et les relations pour le domaine de la sécurité radiologique.
Un cadre (famework) d'organisation des étapes de travaux menant à la construction de l'ontologie du domaine de la sécurité radiologique.3.
Implémentation des trois méthodes proposées et analyse des résultats obtenus.
La surveillance de la qualité des données qui proviennent des expériences de physique des hautes énergies est une tâche exigeante mais cruciale pour assurer que les analyses physiques sont basées en données de la meilleure qualité possible.
Lors de l'expérience Compact Muon Solenoid opérant au Grand collisionneur de hadrons du CERN, le paradigme actuel d'évaluation de la qualité des données est basé sur l'examen détaillé d'un grand nombre de tests statistiques.
Cependant, la complexité toujours croissante des détecteurs et le volume des données de surveillance appellent un changement de paradigme.
Ici, les techniques de Machine Learning promettent une percée.
Cette thèse traite du problème de l'automatisation applique à la surveillance de la qualité des données avec les méthodes de détection des anomalies d'
Les anomalies causées par un dysfonctionnement du détecteur sont difficiles à énumérer a priori et rares, ce qui limite la quantité de données étiquetées.
Ainsi, cette thèse explore le paysage des algorithmes existants avec une attention particulière aux problèmes semi-supervisés et démontre leur validité et leur utilité sur des cas de test réels en utilisant les données de l'expérience.
Dans le cadre de ce projet, l'infrastructure de surveillance a été encore optimisée et étendue, offrant des méthodes plus sensibles aux différents modes de défaillance.
Dans une démarche de co-création et d'innovation ouverte, les entreprises utilisent des plateformes de crowdsourcing d'idées pour collecter les idées des consommateurs.
Ce travail doctoral porte sur un nouveau modèle de plateforme, qui s'appuie simultanément sur la compétition et la coopération pour mobiliser la foule (le modèle de « co-opétition » ).
Son usage pose la question de son efficacité pour susciter des idées nombreuses et innovantes, par rapport aux modèles classiques basés uniquement sur la compétition ou la coopération.
Pour répondre à cette question, nous avons mis en place deux expérimentations qui nous permettent de comparer quantitativement l'effet de la co-opétition, de la compétition et de la coopération sur la créativité des participants.
Nos résultats indiquent que la co-opétition et la compétition renforcent la créativité des individus, contrairement à la coopération.
Ils montrent un effet d'interaction positif entre la coopération et la compétition, qui traduit une influence plus importante de la co-opétition sur la créativité que celle de la seule compétition.
Ces effets s'expliquent par le rôle médiateur de l'ambivalence motivationnelle, et non par celui de l'ambivalence émotionnelle.
L'attitude envers l'indépendance modère l'effet direct des modèles d'interdépendance sur la qualité des idées, tandis que l'attitude envers la compétition et l'attitude envers la coopération n'ont
Cette thèse s'articule autour de deux thèmes : les normes sociales et les réseaux de production.
Le premier chapitre porte sur une étude de cas où les normes sociales sont utilisées dans la lutte contre le discours haineux en ligne.
A l'aide de méthodes de machine learning, je montre que le fait de dénoncer les opinions haineuses est un moyen de dissuader d'autres discours haineux.
Cet effet s'explique par le fait que cette forme de contradiction sert de communiquer la présence d'une norme sociale ou en accentue l'importance.
Le deuxième chapitre porte sur le rôle que joue le goût pour l'image sociale pour expliquer l'effet des normes sociales sur le comportement.
De nombreuses études montrent que ces goûts affectent le comportement des gens en moyenne, mais nous ne savons pas encore quels individus sont les plus susceptibles d'adapter leur comportement.
Je présente une expérience novatrice conçue pour combler ce vide.
Elle permet de calculer une mesure individuelle de préoccupation pour l'image, montre qu'il y a une hétérogénéité substantielle et analyse sa corrélation avec d'autres préférences sociales.
L'intégration verticale des entreprises peut donner lieu à des comportements anticoncurrentiels.
J'aborde l'un de ces comportements, appelé verrouillage, par lequel les entreprises verticalement intégrées coupent l'approvisionnement de leurs concurrents en intrants essentiels.
J'utilise de nouvelles données sur les réseaux de production pour identifier les fusions et acquisitions entre entreprises verticalement liées.
Cette thèse s'inscrit dans le cadre de travaux relatifs au Web Social Sémantique, dans la perspective de la complémentarité et de la coévolution de deux aspects du Web, l'aspect social et sémantique.
Le développement du Web au cours de ces dernières années a fait émerger un énorme graphe de données structurées, sémantiques résultant en partie de l'activité des utilisateurs, le LOD.
Nous nous intéressons à l'utilisation de ce graphe afin de faciliter l'accès à l'information présente sur le Web, et ce de manière utile, informative et enrichissante pour l'utilisateur.
Cette problématique est notamment étudiée dans les scénarios de l'innovation sur le Web – pratiques visant à utiliser des technologies du Web pour contribuer à l'émergence de l'innovation.
Une spécificité de ce contexte, assez peu abordé dans la littérature existante, est sans doute le besoin d'inciter les découvertes inattendues et fortuites.
Au delà de la simple pertinence sollicitée dans toute situation de recherche et de recommandation sur le Web, le contexte d'innovation impose une certaine ouverture d'esprit pour permettre à l'utilisateur d'accéder aux informations inattendues mais néanmoins pertinentes, et permet par la même occasion de s'inspirer et de transposer des idées d'un domaine à l'autre.
La thèse s'appuie sur le concept de littératie médiatique.
Après l'avoir défini, elle s'emploie à le mettre en œuvre pour les enfants non-lisants (2 à 7 ans) du Togo et/ou sourds (7 à 12 ans) de France.
Partant de l'hypothèse que les contenus médiatiques peuvent contribuer au développement, aux apprentissages et à la culture de ces enfants, l'analyse infocommunicationnelle des mécanismes d'appropriation de ces contenus est effectuée pour ces deux populations cibles.
L'étude mobilise ainsi des expérimentations de terrain au Togo et en France.
Cette revue préliminaire des terrains d'étude montre que les politiques et les dispositifs de communication propres à notre cible restent insuffisants d'une part, et que le champ est très peu abordé dans les travaux antérieurs d'autre part.
La deuxième partie se penche sur le cadre théorique de la littératie médiatique et précise sa conceptualisation dans le champ des sciences de l'information et de la communication.
Le protocole VI.A.G.E est alors élaboré pour évaluer le processus d'appropriation de contenus médiatiques auprès des enfants.
La troisième partie est consacrée au dépouillement des trois expérimentations de terrain menées au Togo (visionnage expérimental du film Kirikou et la sorcière) et en France (visioguide sur DVD et i-Pad, et interaction sur écran géant tactile, notamment au Musée du Quai Branly).
Leurs résultats respectifs sont exposés.
A partir de l'hypothèse que la forme renseigne le sens, un corpus d'énoncés attestés a été constitué et rassemblé dans une base de données de manière à permettre l'observation de la combinatoire syntaxique, lexicale et sémantique de groupes prépositionnels compléments ("argumentaux") en dans avec un verbe.
La recherche d'une part identifie et décrit l'ensemble des verbes qui sous-catégorisent cette préposition et d'autre part relève les interprétations qui ressortent de ces combinaisons de manière à avancer dans l'identité sémantique de la préposition dans et du complément dans son ensemble.
Du point de vue syntaxique, le résultat de l'investigation consiste en une liste (d'emplois) de verbes définissables par leur complémentation en dans et caractérisés par un certain nombre de propriétés : l'étude aboutit ainsi à la définition d'une classe lexicale, paradigme également uni sur le plan sémantique.
La préposition dans est analysable dans tous les cas comme un marqueur de "coi͏̈ncidence", quelle que soit la valeur particulière d'un complément.
L'apport proprement linguistique de l'étude est défini et présenté de sorte à en permettre l'exploitation dans le domaine du traitement automatique des langues.
Dans le dialogue homme-machine, les anaphores traitees traditionnellement sont les anaphores pronominales.
Puis, en exploitant ce corpus d'un point de vue documentaire (typologie des requetes, strategies de recherches....) et d'un point de vue linguistique (structure de surface des enonces, anaphores, coherences, reference,...), on remet en cause l'usage de la langue naturelle pour certaines situations et l'on emet l'hypothese d'une nouvelle phraselogie de la communication propre au dialogue homme-machine.
Du point de vue traitement automatique, sont explicitees les differentes connaissances necessaires a la conduite du dialogue et a la resolution automatique des anaphores (modele de dialogue, modele de la tache, deroulement du dialogue,... Etc.), en s'appuyant d'une part sur les differentes approches linguistiques des phenomenes anaphoriques, et d'autre part sur des travaux effectues en t. A. L.
En se placant dans le cadre de l'analyse automatique de langue telle qu'elle est envisagee au centre de recherche en informatique appliquee aux sciences sociales de grenoble (criss), sont degagees des regles de reconnaissance des unites anaphoriques.
Cette thèse sera consacrée à l'étude d'utilisation du traitement de langage naturel en finance.
Nous étudierons d'abord les méthodes d'analyse du sentiment dans le cadre d'application sur les nouvelles financières.
Avant l'ère d'intelligence artificielle, les méthodes feature engineering et statistiques étaient les outils dominant dans ce domaine, par exemple, Naive Bayse Classifier et TF-IDF.
Cependant, la plupart de ces méthodes sont basées sur les embedding statiques et elles ignorent les informations contextuelles dans les corpus.
Par conséquent, nous allons étudier les améliorations en précision par adopter les embeddings contextualisés.
Nous étudierons aussi les méthodes qui nous permettent de générer les embeddings contextualisés, soit par définir et entraîner un modèle de zéro, soit par fine-tuner une modèle existante, par exemple, BERT ou XLNet.
Nous allons ensuite focaliser sur l'analyse du sentiment sur les textes longs.
En finance, il y a de différents types de documents longs, par exemple, les rapports annuel et les transcriptions de conférence.
Etant donné la longueur de ces corpus, nous ne pouvons pas appliquer directement les méthodes classiques.
En conséquence, nous proposons de le résumer dans un premier temps et ensuite d'entraîner un modèle de classification.
Les méthodes existantes comme LDA (Latent Dirichlet Allocation) ou BERTSum fonctionnent bien sur les textes plutôt uniformes.
Cependant, ils ont les performances limitées sur ces documents financiers avec les formats différents.
Nous allons investiguer la méthode transfer learning pour cette tâche.
Nous allons d'abord entraîner un modèle avec tous les corpus.
Ensuite, pour chaque type de document, nous allons considérer un modèle plus parcimonieux avec un nombre limité de donnée.
Notre résultat final sera obtenu par agréger les sorties des deux modèles.
Finalement, nous sommes intéressés par la possibilité d'appliquer les méthodes NLP sur la tâche de réduction de dimension des séries temporelles.
Les méthodes NLP fréquemment utilisées comme Auto-Encodeur, Generative Adversarial Network (GAN) et Variational Auto-Encoder (VAE) contiennent toutes une partie encodeur, qui nous permet de résumer les embeddings haut-dimensionnels en plusieurs vecteurs.
Par conséquent, en basant sur cette idée, nous allons chercher une structure qui prend les séries temporelles haut-dimensionnelles comme l'entrée et qui donne les séries temporelles basse-dimensionnelles comme la sortie.
Les brevets d'invention titres de propriété industrielle confèrent à leurs titulaires le monopole de l'invention brevetée.
On peut y trouver une sorte d'historique de l'évolution de l'artefact.
Dans ce contexte le concepteur est très souvent amené à faire des recherches dans les documents de brevets afin de bénéficier des connaissances qui y sont contenues en vue de structurer le processus inventif.
Développée pour assister les concepteurs dans leur démarche d'innovation, la Méthode de Conception Inventive (MCI), s'inscrit dans le modèle de la dialectique.
La MCI a précisé les concepts entrant en jeu dans la description des évolutions des systèmes techniques et des artefacts.
Ces items intéressent bien souvent les concepteurs et sont essentiels à la compréhension du problème sous-jacent et à la collecte de toutes les caractéristiques sur lesquelles on peut agir ; et de l'effet de leurs variations sur l'artefact.
Cette thèse consiste d'abord à analyser le document de brevet d'un point de vue linguistique, afin d'en connaitre la typologie.
Il s'agit, ensuite, de repérer dans le document de brevets les connaissances susceptibles d'être utiles à la MCI et à les formaliser sous forme de programme informatique.
L'approche que nous proposons est issue du text-mining.
Elle est à base de marqueurs linguistiques et utilise des patrons lexico-syntaxiques issus du domaine du traitement automatique des langues.
Cette méthode d'extraction des concepts utiles à la MCI permet l'établissement d'une sorte de cartographie initiale des évolutions passées et possibles des caractéristiques de l'artefact.
L'intérêt est en outre de faciliter grandement l'analyse préliminaire des connaissances relative au dit artefact.
Cette thèse s'intéresse à la question des changements de comportement, et notamment à la manière dont cette question s'applique au domaine informatique à travers les technologies persuasives.
Dans un contexte applicatif particulier, celui de la rénovation de logements, nous nous intéressons au rôle que peuvent jouent les informations à disposition des utilisateurs sur leur façon d'élaborer leur projet de rénovation.
Une façon de modifier les comportements des utilisateurs est de modifier les buts qu'ils poursuivent, soit de manière explicite, soit de manière implicite.
Si l'efficacité de la première a été montrée en contexte expérimental, elle semble toutefois moins adaptée à des situations naturelles.
Nous proposons donc une approche visant à modifier les buts poursuivis par les utilisateurs implicitement.
Dans cette optique, nous travaillons d'abord à l'emploi de normes sociales injonctives pour inciter les utilisateurs à travailler particulièrement sur la rénovation énergétique.
Au cours d'une première étude, nous comparons norme sociale injonctive et objectif arbitraire à une condition contrôle.
Nous nous intéressons à la performance des participants à la tâche (améliorer la performance énergétique d'un logement) ainsi qu'à la manière dont le projet se met en place tout au long de l'étude.
Les résultats montrent que norme sociale et objectif explicite ont un effet similaire sur la performance à la tâche mais différent sur l'organisation temporelle.
On observe ainsi des comportements plus stables dans le cas où la norme sociale est activée, et un effet qui semble globalement moins artificiel que dans le cas où on fixe un objectif explicite à l'utilisateur.
Cette première étude met également en avant la nécessité pour la norme d'être saillante, ou activée.
Nous nous intéressons donc dans une deuxième étude à ce qui caractérise la saillance du message normatif.
Dans la première étude, nous avions utilisé deux types d'informations différentes : le message normatif et des indices concrets relatifs au comportement désirable.
Cette deuxième étude vise à distinguer ces deux informations et tester leur effet respectif.
Les résultats montrent que le message normatif semble avoir un effet légèrement plus important sur la performance mais aussi plus artificiel sur les comportements des utilisateurs.
Dans une troisième étude, nous nous intéressons aux caractéristiques du message, en faisant l'hypothèse qu'un message mieux perçu pourrait appuyer la saillance de la norme qu'il porte.
Dans le cadre d'une collaboration avec des chercheurs en intelligence artificielle nous avons ainsi testé différents types de cadrage afin d'évaluer leur effet respectif sur la perception de l'argument auquel ils s'appliquaient.
Les résultats, mitigés, montrent essentiellement que le style argumentatif (rationnel et factuel plutôt qu'émotionnel ou moral) semble avoir un poids conséquent sur la perception de l'argument.
En outre, la thématique abordée par l'argument semble jouer un rôle non négligeable et devrait donc faire l'objet d'une attention particulière pour le développement d'interventions similaires.
Sur le plan applicatif, nos résultats mettent d'abord en évidence la pertinence de l'utilisation des normes sociales injonctives dans un contexte de technologie persuasive.
Ils montrent également que les messages portant la norme sociale doivent être conçus avec soin, en tenant compte de multiples facteurs.
Sur le plan théorique, nous montrons qu'une norme sociale peut avoir un effet comparable à celui d'un objectif explicitement fixé, mais que les deux génèrent la mise en place de processus cognitifs différents.
Enfin, sur le plan méthodologique, nous appliquons l'analyse de traces de l'activité au champ de l'influence sociale, ce qui, à notre connaissance, n'avait pas encore été mis en place.
L'utilisation des médicaments est souvent nécessaire au cours de la grossesse malgré un manque de connaissance de leurs effets indésirables sur la grossesse ou le fœtus.
L'identification des effets indésirables des médicaments commercialisés au moyen d'outils statistiques de détection de signaux repose classiquement sur l'exploitation de grandes bases de notifications spontanées dans lesquelles le statut de grossesse est peu présent.
Les bases médico-administratives sont de plus en plus utilisées en pharmacoépidémiologie y compris chez la femme enceinte.
Elles sont donc potentiellement intéressantes pour une pharmacovigilance automatisée.
En France, le Système National des Données de Santé couvre la quasi-totalité de la population française et les travaux de la thèse portent sur l'exploitation de son échantillon permanent, l'Echantillon Généraliste des Bénéficiaires.
Le premier axe décrit la prescription des médicaments chez la femme enceinte et plus particulièrement les médicaments tératogènes ou fœtotoxiques et les supplémentations recommandées au cours de la grossesse.
Nous trouvons que les femmes enceintes se voient délivrer beaucoup de médicaments au cours de la grossesse.
Les médicaments à risque connu sont peu délivrés et les supplémentations recommandées sont en augmentation durant la période d'étude.
Les femmes enceintes ayant de faibles revenus ont plus de délivrances de médicaments mais ont moins de délivrances de supplémentations.
Le deuxième axe porte sur le développement d'une méthodologie de détection de signal basée sur l'utilisation du score de propension ou du score pronostique en grande dimension.
Cette méthodologie, déclinée en 21 modalités, est appliquée pour identifier des médicaments possiblement associés à une augmentation du risque de prématurité.
Les résultats montrent que l'utilisation du p-RD dérivé du score pronostique permet de mieux prendre en compte la confusion et de limiter les biais d'indication et protopathique.
Pour le troisième axe, le p-RD est appliqué à vingt pathologies spécifiques ou non de la grossesse et combiné à un requêtage automatisé des mots-clés MeSH des articles de MEDLINE.
Le requêtage automatisé permet une annotation facilitée des effets indésirables médicamenteux connus.
En conclusion, cette thèse montre l'intérêt des données médico-administratives pour analyser l'évolution de la prescription de médicaments au cours de la grossesse et pour la détection de signaux en pharmacovigilance chez la femme enceinte.
Progressivement, les technologies numériques prennent une place plus importante dans la recherche sur les phénomènes socioculturels.
Des projets d'équipement se développent dans toutes les disciplines des sciences humaines et sociales (SHS) et des mouvements prônant une révolution instrumentale se multiplient.
Cette thèse en sciences de l'information et de la communication propose d'interroger l'avènement d'une recherche « numériquement équipée » en SHS à partir d'une réflexion générale sur les liens entre sciences, technique et écriture.
Quels sont les enjeux épistémologiques, mais aussi politiques, sous-jacents à ces logiques d'instrumentation numérique en tant qu'elles instituent de nouvelles techniques d'écriture au cœur des pratiques de recherche ?
Le mémoire présente un parcours en trois grandes parties.
La première partie inscrit la recherche dans une pensée des rapports fondamentaux entre instruments techniques et connaissance scientifique.
Il s'agit également de reconnaître les spécificités d'une approche « communicationnelle » de l'instrumentation scientifique, et en particulier de l'instrumentation numérique.
À partir de la théorie des médias informatisés et de l'écriture numérique, et sur la base d'une démarche d'analyse techno-sémiotique, la troisième partie interroge les formes et les pouvoirs de la médiation instrumentale numérique.
Sur un plan morphologique et praxéologique, en quoi consiste la conception et la mise en œuvre de tels instruments ?
Sur un plan plus politique, quels sont les effets « normatifs » de ces dispositifs instrumentaux sur l'épistémologie des disciplines qui s'en saisissent ?
Le succès du mouvement Open Access ces dernières années montre la pertinence de ce modèle pour le monde de la recherche.
Il s'agit de nouvelles pratiques de l'édition scientifique qui cherchent à fournir un accès libre et gratuit à l'information de la recherche et de faciliter ainsi la diffusion du savoir.
Pour aller plus loin, l'Open Science désigne une approche qui vise à rendre universel, libre et gratuit l'accès non seulement aux publications scientifiques, mais aussi à leurs données, méthodologie et résultats pour permettre une meilleure reproductibilité des recherches, faciliter la collaboration entre les chercheurs, et accélérer les découvertes.
La recherche scientifique d'aujourd'hui bénéficie de la société d'information et des « big data » , à travers l'exploitation de grands jeux de données, qui font partie intégrante des outils actuels pour la génération de nouvelles connaissances.
Cette thèse a pour objet d'analyser et de traiter automatiquement des articles scientifiques afin d'en extraire de nouvelles méta-données concernant les jeux de données (datasets) et les résultats de la recherche liés à ces jeux de données.
Nous étudierons les enjeux de l'Open Science et les différentes phénomènes concernant les articles scientifiques et leurs données, afin de proposer une typologie de jeux de données de la recherche sous forme d'ontologie.
Nous mettrons en place une approche automatique pour l'identification des segments textuels se référant aux jeux de données au sein des articles scientifiques.
Les nouvelles méta-données, produites suite à cette analyse automatique de corpora scientifiques, seront agrégées sous forme de Open Data afin de proposer de nouveaux outils à destination des chercheurs pour exploiter et analyser la production scientifique d'un domaine.
Les systèmes de recommandation jouent un rôle important dans l'orientation des choix des utilisateurs.
La recommandation se fait généralement par une optimisation d'une mesure de précision de l'adéquation entre un utilisateur et un produit.
Cependant, plusieurs travaux de recherche ont montré que l'optimisation de la précision ne produisait pas les recommandations les plus utiles pour les utilisateurs.
Un système trop précis peut contribuer à confiner les utilisateurs dans leur propre bulle de choix.
Ceci peut aussi produire un effet de foule qui va concentrer les usages autour de quelques articles populaires.
Par conséquent, il y a un manque de diversité et de nouveauté dans les recommandations et une couverture limitée du catalogue.
Même si la routine peut être sécurisante, l'être humain aime sortir des sentiers battus pour, par exemple, découvrir de nouveaux produits, tenter de nouvelles expériences.
Dans cette thèse, nous présentons deux familles de modèles qui cherchent à produire des résultats qui vont au-delà des aspects de précision pour des systèmes de recommandation pour des produits culturels basés sur le contenu.
Le premier modèle repose principalement sur une approche de clustering.
Dans ce modèle, nous proposons de la diversité à l'utilisateur tout en restant dans le périmètre de ses goûts.
Le second modèle est basé sur une fonction issue de la loi normale.
Nous faisons l'hypothèse de l'existence d'une zone intermédiaire définie entre des éléments considérés comme trop similaires et d'autres considérés comme trop différents.
Nos propositions sont testées sur des jeux de données standards et comparées à des algorithmes de l'état de l'art.
Les résultats de nos expériences montrent que nos approches apportent de la diversité et de la nouveauté et sont compétitives par rapport aux méthodes de l'état de l'art.
Nous proposons également une expérience utilisateur pour valider notre modèle basé sur la fonction issue de la loi normale.
Les résultats des expériences centrées sur l'utilisateur montrent que ce modèle correspond au comportement cognitif de l'être humain ainsi qu'à sa perception de la diversité.
D'abord, nous introduisons le concept de corpus parallèle.
Fratchèque est un corpus parallèle de ressources écrites dont les textes en français et en tchèque proviennent de la littérature écrite après 1945.
Il ne contient pas de balises XML, le logiciel ParaConc utilisé pour le traitement du corpus n'en a pas besoin.
L'élaboration du corpus est décrite d'une façon détaillée en suivant toutes les démarches et tout le paramétrage des logiciels utilisés.
Elle commence avec le logiciel de reconnaissance optique de caractères FineReader et après le contrôle de la qualité des textes numérisés sous MS Word 2002 on procède à la constitution d'un corpus parallèle géré par ParaConc.
La partie linguistique de la thèse s'appuie sur le corpus parallèle réalisé.
Elle aborde un phénomène connu en tchèque sous le terme částice qui n'a d'équivalent univoque en français.
Les termes le plus souvent liés en français à la question sont mots du discours et particules énonciatives.
Selon les descriptions existantes, il y a une relation étroite entre ces mots et le discours.
Cette constatation est démontrée pour deux částice – vždyt̕, přece et leurs variantes – sur les grands corpus tchèques (Analyse A) et Fratchèque (Analyse B).
L'étude continue avec l'analyse systématique des types variés d'usage de vždyt̕, přece dans le but de proposer une description lexicographique pour un dictionnaire bilingue tchèque-français.
Enfin, on discute quelques questions qui concernent la possibilité d'évaluer automatiquement la qualité de traductions liées à la présence de částice
Le travail de recherche exposé dans cette thèse concerne le développement d'approches d'apprentissage non-supervisé adaptés aux grands jeux de données relationnelles et dynamiques.
La combinaison de ces trois caractéristiques (taille, complexité et évolution)constitue un défi majeur dans le domaine de l'exploration de données et peu de solutions satisfaisantes existent pour le moment, malgré les besoins de plus en plus manifestes des entreprises.
C'est un véritable challenge, car les approches adaptées aux données relationnelle sont une complexité quadratique inadaptée à l'analyse de données dynamiques.
Nous proposons ici deux approches complémentaires pour l'analyse de ce type de données.
La seconde propose d'utiliser des points de support parmi les objets afin de construire un espace de représentation permettant de définir des prototypes représentatifs des clusters.
Enfin, nous appliquons les approches proposées au profilage en temps réel d'utilisateurs connectés.
L'enseignement assiste par ordinateur, heritier de l'enseignement programme, a fortement marque la didactique des ces vingt dernieres annees.
L'integration de l'informatique dans la pedagogie a permis de renouveler tant les reflexions que les pratiques liees a l'enseignement / apprentissage des langues en contexte educatif.
Les nouvelles applications doivent tenir compte egalement de certaines potentialites des environnements informatiques comme : l'acces direct, la simulation, l'interactivite.
Une analyse croisee des possibilites de traitement automatique de la parole et des parametres saillants du phenomene de comprehension de l'oral permet de proposer un modele pedagogique de l'enseignement / apprentissage de la comprehension orale s'appuyant sur quatre axes : quantitatif, qualitatif, strategique et enfin, communicatif.
Pour chacun de ces axes, l'informatique interactive multimedia dispose d'atouts evidents, et ce a deux niveaux : * au niveau + macro ;
Ce dernier niveau peut servir de base a la mise en oeuvre d'un champ d'activites totalement nouvelles, irrealisables sans le support de l'informatique interactive multimedia, et dont l'implication sur l'apprentissage de la competence orale reste, bien evidemment, a preciser.
Cette étude présente une typologie des prédicats de mouvement du hongrois.
Elle reflète une perception objective et simple du mouvement et de l'espace.
La classification s'appuie sur des propriétés sémantiques comme la directionnalité, le mode, le lieu de destination, le but et des propriétés aspectuelles.
Ces propriétés sont complétées par des propriétés morpho-syntaxiques nécessaires au traitement automatique.
L'aspect contrastif de notre étude a permis de proposer une meilleure description des classes de prédicats du hongrois et de relever les différences morpho-syntaxiques et combinatoires spécifiques des deux langues dans l'expression du mouvement, comme le rôle des préfixes verbaux, des compléments locatifs ainsi que l'importance des prédicats nominaux.
Dans cette thèse, nous proposons une méthodologie basée sur les modèles pour gérer la complexité de la conception des systèmes autonomiques cognitifs intégrant des objets connectés.
Cette méthodologie englobe un ensemble de patrons de conception dont nous avons défini pour modéliser la coordination dynamique des processus autonomiques pour gérer l'évolution des besoins du système, et pour enrichir les systèmes avec des propriétés cognitives qui permettent de comprendre les données et de générer des nouvelles connaissances.
De plus, pour gérer les problèmes reliés à la gestion des big data et à la scalabilité du système lors du déploiement des processus, nous proposons une plate-forme sémantique supportant le traitement des grandes quantités de données afin d'intégrer des sources de données distribuées et hétérogènes déployées sur le cloud pour générer des connaissances qui seront exposées en tant que service (KaaS).
Comme application de nos contributions, nous proposons un système cognitif prescriptif pour la gestion du plan de traitement du patient.
Ainsi, nous élaborons des modèles ontologiques décrivant les capteurs et le contexte du patient, ainsi que la connaissance médicale pour la prise de décision.
Le système proposé est évalué de point de vue clinique en collaborant avec des experts médicaux, et de point de vue performance en proposant des différentes configurations dans le KaaS.
Des agents devant prendre une décision collective sont souvent motivés par des buts individuels.
Dans ces situations, deux aspects clés doivent être abordés : sélectionner une alternative gagnante à partir des voix des agents et s'assurer que les agents ne manipulent pas le résultat.
Cette thèse étudie l'agrégation et la dimension stratégique des décisions collectives lorsque les agents utilisent un langage représenté de manière compacte.
Nous étudions des langages de type logique : de la logique propositionnelle aux CP-nets généralisés, en passant par la logique temporelle linéaire (LTL).
Notre principale contribution est l'introduction d'un cadre de vote sur les buts, dans lequel les agents soumettent des buts individuels exprimés comme des formules de la logique propositionnelle.
Les fonctions d'agrégation classiques issues du vote, de l'agrégation de jugements et de la fusion de croyances sont adaptées et étudiées de manière axiomatique et computationnelle.
Les propriétés axiomatiques connues dans la littérature sur la théorie du choix social sont généralisées à ce nouveau type d'entrée, ainsi que les problèmes de complexité visant à déterminer le résultat du vote.
Une autre contribution importante est l'étude de l'agrégation des CP-nets généralisés, c'est-à-dire des CP-nets où la précondition de l'énoncé de préférence est une formule propositionnelle.
Nous utilisons différents agrégateurs pour obtenir un classement collectif des résultats possibles.
Grâce à cette thèse, deux axes de recherche sont ainsi reliés : l'agrégation des CP-nets classiques et la généralisation des CP-nets à des préconditions incomplètes.
L'accent est mis sur trois règles de vote majoritaires qui se révèlent manipulables.
Par conséquent, nous étudions des restrictions à la fois sur le langage des buts et sur les stratégies des agents en vue d'obtenir des résultats de votes non manipulables.
Nous présentons par ailleurs une extension stratégique d'un modèle récent de diffusion d'opinion sur des réseaux d'influence.
Dans les jeux d'influence définis ici, les agents ont comme but des formules en LTL et ils peuvent choisir d'utiliser leur pouvoir d'influence pour s'assurer que leur but est atteint.
Des solutions classiques telles que la stratégie gagnante sont étudiées pour les jeux d'influence, en relation avec la structure du réseau et les buts des agents.
Enfin, nous introduisons une nouvelle classe de concurrent game structures (CGS) dans laquelle les agents peuvent avoir un contrôle partagé sur un ensemble de variables propositionnelles.
De telles structures sont utilisées pour interpréter des formules de logique temporelle en temps alternés (ATL), grâce auxquelles on peut exprimer l'existence d'une stratégie gagnante pour un agent dans un jeu itéré (comme les jeux d'influence mentionnés ci-dessus).
Le résultat principal montre qu'un CGS avec contrôle partagé peut être représenté comme un CGS avec contrôle exclusif.
En conclusion, cette thèse contribue au domaine de la prise de décision collective en introduisant un nouveau cadre de vote basé sur des buts propositionnels. Elle présente une étude de l'agrégation des CP-nets généralisés et une extension d'un cadre de diffusion d'opinion avec des agents rationnels qui utilisent leur pouvoir d'influence. Une réduction du contrôle partagé à un contrôle exclusif dans les CGS pour l'interprétation des logiques du raisonnement stratégique est également proposée.
Par le biais de langages logiques divers, les agents peuvent ainsi exprimer buts et préférences sur la décision à prendre, et les propriétés souhaitées pour le processus de décision peuvent en être garanties.
Les avancées significatives qu'ont connu les technologies de capteurs, leur utilisation croissante
L'information générée par de telles sources de données peut être qualifiée d'hétérogène sur plusieurs plans : types de mesures physiques, domaines et primitives temporelles, modèles de données etc.
Dans ce contexte, l'application de méthodes de fouille de motifs constitue une opportunité pour la découverte de relations temporelles non-triviales, directement utilisables et facilement interprétables décrivant des phénomènes complexes.
Nous proposons d'utiliser un ensemble d'abstraction temporelles pour construire une représentation unifiée, sous forme des flux d'intervalles (ou états), de l'information générée par un système hétérogène.
A partir de cette représentation, nous nous intéressons à la découverte de dépendances temporelles quantitatives (avec information de délais) entre plusieurs flux d'intervalles.
Nous introduisons le modèle de dépendances Complex Temporal Dependency (CTD)
Ce modèle permets d'exprimer un ensemble riche de relations temporelles complexes.
Pour ce modèle de dépendances nous proposons des algorithmes efficaces de découverte : CTD-Miner et ITLD
Cette recherche porte sur la description lexicologique des pragmatèmes.
Il s'agit de phrasèmes compositionnels non-libres, contraints par la situation de communication dans laquelle ils sont énoncés.
Dans ce travail, nous adoptons une approche contrastive français-espagnol.
Les expressions les plus courantes de la vie quotidienne impliquent beaucoup de contraintes dont nous ne sommes pas conscients.
Ainsi, saluer quelqu'un par un Bonjour !, ou finir une lettre par Cordialement, Bien à vous, n'a aucune difficulté pour un locuteur natif.
Ces énoncés qui ont l'air d'une grande simplicité du point de vue de leur contenu, de leur forme et des contextes de la vie ordinaire dans lesquels ils s'emploient sont très singuliers.
Ils sont rituellement émis dans des situations courantes auxquelles ils sont prototypiquement associés.
Les pragmatèmes passent souvent inaperçus dans la langue, en tant qu'unités phraséologiques, et c'est lors de la traduction qu'on s'aperçoit qu'ils ne peuvent pas être traduits littéralement dans une autre langue ;
il faut trouver une expression équivalente.
Il existe un lien entre les pragmatèmes et la culture.
À l'intérieur d'une communauté linguistique, les locuteurs se comprennent puisqu'ils partagent une compétence linguistique et une expérience culturelle.
Cependant, dans la communication dans une langue étrangère, il faut prendre en compte les éléments culturels qui conditionnent la situation dans laquelle l'échange a lieu.
Les moteurs de recherche verticaux, qui se concentrent sur des segments spécifiques du Web, deviennent aujourd'hui de plus en plus présents dans le paysage d'Internet.
Les moteurs de recherche thématiques, notamment, peuvent obtenir de très bonnes performances en limitant le corpus indexé à un thème connu.
Dans le cadre de cette thèse, nous nous intéressons plus précisément à la procédure de collecte de documents thématiques à partir du Web pour alimenter un moteur de recherche thématique.
La procédure de collecte peut être réalisée en s'appuyant sur un moteur de recherche généraliste existant (recherche orientée) ou en parcourant les hyperliens entre les pages Web (exploration orientée).Nous étudions tout d'abord la recherche orientée.
Dans ce contexte, l'approche classique consiste à combiner des mot-clés du domaine d'intérêt, à les soumettre à un moteur de recherche et à télécharger les meilleurs résultats retournés par ce dernier.
Après avoir évalué empiriquement cette approche sur 340 thèmes issus de l'OpenDirectory, nous proposons de l'améliorer en deux points.
En amont du moteur de recherche, nous proposons de formuler des requêtes thématiques plus pertinentes pour le thème afin d'augmenter la précision de la collecte.
Nous définissons une métrique fondée sur un graphe de cooccurrences et un algorithme de marche aléatoire, dans le but de prédire la pertinence d'une requête thématique.
En aval du moteur de recherche, nous proposons de filtrer les documents téléchargés afin d'améliorer la qualité du corpus produit.
Dans la seconde partie de cette thèse, nous nous focalisons sur l'exploration orientée du Web.
Au coeur de tout robot d'exploration orientée se trouve une stratégie de crawl qui lui permet de maximiser le rapatriement de pages pertinentes pour un thème, tout en minimisant le nombre de pages visitées qui ne sont pas en rapport avec le thème.
Nous proposons d'apprendre automatiquement une fonction d'ordonnancement indépendante du thème à partir de données existantes annotées automatiquement.
Les smartphones sont devenus omniprésents dans notre vie quotidienne à cause des options qu'ils proposent.
Aujourd'hui, Android est installé sur plus de 80% des smartphones.
Les applications mobiles recueillent une grande quantité d'informations sur l'utilisateur.
Par conséquent, Android est devenu une cible préférée des cybercriminels.
Comprendre le fonctionnement des malwares et comment les détecter est devenu un défi de recherche important.
Les malwares Android tentent souvent d'échapper à l'analyse statique en utilisant des techniques telles que l'obfuscation et le chargement dynamique du code.
Des approches d'analyse ont été proposées pour exécuter l'application et surveiller son comportement.
Néanmoins, les développeurs des malwares utilisent des bombes temporelles et logiques pour empêcher le code malveillant d'être exécuté sauf dans certaines circonstances.
Par conséquent, plus d'actions sont requises pour déclencher et surveiller leurs comportements.
Des approches récentes tentent de caractériser automatiquement le comportement malveillant en identifiant les endroits du code les plus suspicieux et en forçant leur exécution.
Ces approches analysent seulement le code d'application et ratent les chemins d'exécution générés quand l'application appelle une méthode du framework, qui appelle à son tour une autre méthode applicative.
Il fournit aussi des informations clés sur l'application analysée afin de comprendre comment le code suspicieux a été injecté dans l'application.
Nous évaluons que 72,69% des échantillons ont au moins un endroit suspicieux du code qui n'est atteignable qu'à travers des appels implicites.
Les approches de déclenchement actuelles utilisent principalement deux stratégies pour exécuter une partie du code applicatif.
La première stratégie consiste à modifier l'application excessivement pour lancer le code ciblé sans faire attention à son contexte originel.
La seconde stratégie consiste à générer des entrées pour forcer le flot de contrôle à prendre le chemin désiré sans modifier le code d'application.
Cependant, il est parfois difficile de lancer un endroit spécifique du code seulement en manipulant les entrées.
Nous proposons TriggerDroid, un outil qui a deux buts : forcer l'exécution du code suspicieux et garder le contexte originel de l'application.
Il fournit les événements framework requis pour lancer le bon composant et satisfait les conditions nécessaires pour prendre le chemin d'exécution désiré.
Pour valider notre approche, nous avons fait une expérience sur 135 malwares Android de 71 familles différentes.
Les résultats montrent que notre approche nécessite plus de raffinement et d'adaptation pour traiter les cas spéciaux dus à la grande diversité des échantillons analysés.
Finalement, nous fournissons un retour sur les expériences que nous avons conduites sur différentes collections, et nous expliquons notre processus expérimental.
Nous présentons le dataset Kharon, une collection de malwares Android bien documentés qui peuvent être utilisés pour comprendre le panorama des malwares Android.
L'objectif de cette thèse est de développer des méthodes sémantiques de réassemblage dans le cadre compliqué des collections patrimoniales, où certains blocs sont érodés ou manquants.
Le remontage de vestiges archéologiques est une tâche importante pour les sciences du patrimoine : il permet d'améliorer la compréhension et la conservation des vestiges et artefacts anciens.
Certains ensembles de fragments ne peuvent être réassemblés grâce aux techniques utilisant les informations de contour et les continuités visuelles.
Il est alors nécessaire d'extraire les informations sémantiques des fragments et de les interpréter.
Ces tâches peuvent être accomplies automatiquement grâce aux techniques d'apprentissage profond couplées à un solveur, c'est-à-dire un algorithme de prise de décision sous contraintes.
Cette thèse propose deux méthodes de réassemblage sémantique pour fragments 2D avec érosion, ainsi qu'un jeu de données et des métriques d'évaluation.
La première méthode, Deepzzle, propose un réseau de neurones auquel succède un solveur.
Le réseau de neurones est composé de deux réseaux convolutionnels siamois entraînés à prédire la position relative de deux fragments : il s'agit d'une classification à 9 classes.
Le solveur utilise l'algorithme de Dijkstra pour maximiser la probabilité jointe.
Deepzzle peut résoudre le cas de fragments manquants et surnuméraires, est capable de traiter une quinzaine de fragments par puzzle, et présente des performances supérieures à l'état de l'art de 25%.
La deuxième méthode, Alphazzle, s'inspire d'AlphaZero et de recherche arborescente Monte Carlo (MCTS) à un joueur.
Il s'agit d'une méthode itérative d'apprentissage profond par renforcement : à chaque étape, on place un fragment sur le réassemblage en cours.
Deux réseaux de neurones guident le MCTS : un prédicteur d'action, qui utilise le fragment et le réassemblage en cours pour proposer une stratégie, et un évaluateur, qui est entraîné à prédire la qualité du résultat futur à partir du réassemblage en cours.
Alphazzle prend en compte les relations entre tous les fragments et s'adapte à des puzzles de taille supérieure à ceux résolus par Deepzzle.
Par ailleurs, Alphazzle se place dans le cadre patrimonial : en fin de réassemblage, le MCTS n'accède pas à la récompense, contrairement à AlphaZero.
En effet, la récompense, qui indique si un puzzle est bien résolu ou non, ne peut être qu'estimée par l'algorithme, car seul un conservateur peut être certain de la qualité d'un réassemblage.
La réalisation de surface est une partie du processus global de génération de langue naturelle.
Étant donné une grammaire et une représentation du sens, le réalisateur de surface produit une chaîne en langue naturelle que la grammaire associe au sens donné en entrée.
Cette thèse présente trois extension de GenI, un réalisateur de surface pour une grammaire de type FB-LTAG.
La première extension augmente l'efficacité du réalisateur pour le traitement de l'ambiguïté lexicale.
C'est une adaptation de l'optimisation par « étiquetage électrostatique » qui existe déjà pour l'analyse.
La deuxième extension concerne le nombre de sorties retournées par le réalisateur.
En temps normal, l'algorithme GenI retourne toutes les phrases associées à une même forme logique.
Alors qu'on peut considérer que ces entrées ont le même sens, elles présentent souvent de subtiles nuances.
L'extension est permise par le fait que la grammaire FB-LTAG utilisée par le générateur a été construite à partir d'une « métagrammaire » , mettant explicitement en oeuvre les généralisations qu'elle code.
La dernière extension donne la possibilité au réalisateur de servir d'environnement de débuggage de la métagrammaire.
Les erreurs dans la métagrammaire peuvent avoir des conséquences importantes pour la grammaire.
Comme le réalisateur donne en sortie toutes les chaînes associées à une sémantique d'entrée, il peut être utilisé pour trouver ces erreurs et les localiser dans la métagrammaire.
Nos contributions couvrent trois applications différentes, mais partagent un dénominateur commun : l'extraction des représentations d'utilisateurs concernés.
Notre première application est la tâche de recommandation de produits, où les systèmes existant créent des profils utilisateurs et objets qui reflètent les préférences des premiers et les caractéristiques des derniers, en utilisant l'historique.
De nos jours, un texte accompagne souvent cette note et nous proposons de l'utiliser pour enrichir les profils extraits.
Notre espoir est d'en extraire une connaissance plus fine des goûts des utilisateurs.
Nous pouvons, en utilisant ces modèles, prédire le texte qu'un utilisateur va écrire sur un objet.
Notre deuxième application est l'analyse des sentiments et, en particulier, la classification de polarité.
Notre idée est que les systèmes de recommandation peuvent être utilisés pour une telle tâche.
Les systèmes de recommandation et classificateurs de polarité traditionnels fonctionnent sur différentes échelles de temps.
Nous proposons deux hybridations de ces modèles : la première a de meilleures performances en classification, la seconde exhibe un vocabulaire de surprise.
La troisième et dernière application que nous considérons est la mobilité urbaine.
Elle a lieu au-delà des frontières d'Internet, dans le monde physique.
Nous utilisons les journaux d'authentification des usagers du métro, enregistrant l'heure et la station d'origine des trajets, pour caractériser les utilisateurs par ses usages et habitudes temporelles.
Nous abordons dans cette thèse une étude sur la tâche de la désambiguïsation lexicale qui est une tâche centrale pour le traitement automatique des langues, et qui peut améliorer plusieurs applications telles que la traduction automatique ou l'extraction d'informations.
Les recherches en désambiguïsation lexicale concernent principalement l'anglais, car la majorité des autres langues manque d'une référence lexicale standard pour l'annotation des corpus, et manque aussi de corpus annotés en sens pour l'évaluation, et plus important pour la construction des systèmes de désambiguïsation lexicale.
En anglais, la base de données lexicale wordnet est une norme de-facto de longue date utilisée dans la plupart des corpus annotés et dans la plupart des campagnes d'évaluation.
Notre contribution porte sur plusieurs axes : dans un premier temps, nous présentons une méthode pour la création automatique de corpus annotés en sens pour n'importe quelle langue, en tirant parti de la grande quantité de corpus anglais annotés en sens wordnet, et en utilisant un système de traduction automatique.
Cette méthode est appliquée sur la langue arabe et est évaluée sur le seul corpus arabe, qui à notre connaissance, soit annoté manuellement en sens wordnet : l'OntoNotes 5.0 arabe que nous avons enrichi semi-automatiquement.
Son évaluation est réalisée grâce à la mise en œuvre de deux systèmes supervisés (SVM, LSTM) qui sont entraînés sur les corpus produits avec notre méthode.
Grâce ce travail, nous proposons ainsi une base de référence solide pour l'évaluation des futurs systèmes de désambiguïsation lexicale de l'arabe, en plus des corpus arabes annotés en sens que nous fournissons en tant que ressource librement disponible.
Dans un second temps, nous proposons une évaluation in vivo de notre système de désambiguïsation de l'arabe en mesurant sa contribution à la performance de la tâche de traduction automatique.
La détection de structures par blocs dans les matrices est un enjeu important.
D'abord en analyse de données, où les matrices sont classiquement utilisées pour représenter des données, par exemple via les tables de données ou les matrices d'adjacence.
Dans le premier cas, la détection d'une structure par blocs de lignes et de colonnes permet de trouver un co-clustering.
Dans le second cas, la détection d'une structure par blocs diagonaux dominants fournit un clustering.
Dans cette thèse, nous centrons notre analyse sur la détection de blocs diagonaux dominants par permutations symétriques des lignes et des colonnes.
De nombreux algorithmes pour trouver ces structures ont été créés.
La première est composée d'algorithmes qui projettent les lignes de la matrice dans un espace de faible dimension composé des vecteurs propres dominants avant d'appliquer une procédure de type k-means sur les données réduites.
Ces algorithmes ont le désavantage de nécessiter la connaissance du nombre de classes à découvrir.
La deuxième famille est composée de procédures itératives qui, à chaque itération, cherchent la k-ième meilleure partition en deux blocs.
Mais pour les matrices ayant plus de deux blocs, la partition optimale en deux blocs ne coïncide en général pas avec la véritable structure.
Nous proposons donc un algorithme spectral répondant aux deux problèmes évoqués ci-dessus.
Pour ce faire, nous prétraitons notre matrice via un équilibrage bi-stochastique permettant de stratifier les blocs.
D'abord, nous montrons les bénéfices de cet équilibrage sur la détection de structures par blocs en l'utilisant comme prétraitement de l'algorithme de Louvain pour détecter des communautés dans des réseaux.
Nous explorons aussi plusieurs mesures globales utilisées pour évaluer la cohérence d'une structure par blocs.
En adaptant ces mesures à nos matrices bi-stochastiques, nous remarquons que notre équilibrage tend à unifier ces mesures.
Ensuite, nous détaillons notre algorithme basé sur les éléments propres de la matrice équilibrée.
Il est construit sur le principe que les vecteurs singuliers dominants d'une matrice bi-stochastique doivent présenter une structure en escalier lorsque l'on réordonne leurs coordonnées dans l'ordre croissant, à condition que la matrice ait une structure par blocs.
Des outils de traitement du signal, initialement conçus pour détecter les sauts dans des signaux, sont appliqués aux vecteurs pour en détecter les paliers, et donc les séparations entre les blocs.
Cependant, ces outils ne sont pas naturellement adaptés pour cette utilisation.
Des procédures, mises en place pour répondre à des problèmes rencontrés, sont donc aussi détaillées.
Nous proposons ensuite trois applications de la détection de structures par blocs dans les matrices.
Pour ces applications, nous comparons les résultats de notre algorithme avec ceux d'algorithmes spécifiquement conçus à cet effet.
Enfin, la détection des actes de dialogues dans un discours en utilisant la base de données STAC qui consiste en un chat de joueurs des "Colons de Catane" en ligne.
Pour ce faire nous couplons des algorithmes de clustering non supervisés avec un réseau de neurones BiLSTM permettant de prétraiter les unités de dialogue.
Enfin, nous concluons en entamant une réflexion sur la généralisation de notre méthode au cas des matrices rectangulaires.
Cette dissertation explore le sujet des modèles génératifs appliqués aux images naturelles.
Cette tâche consiste a modéliser la distribution des données observées, et peut permettre de générer des données artificielles semblables aux données d'origine, où de compresser des images.
Les modèles à variable latentes, qui sont au cœur de cette thèse, cherchent a résumer les principaux facteurs de variation d'une image en une variable qui peut être manipulée.
Malheureusement ces modèles ont du mal à modéliser tous les modes de la distribution d'origine, ie ils ne couvrent pas les données dans toute leur variabilité.
A l'inverse, les modèles basés sur le maximum de vraisemblance tels que les VAEs couvrent typiquement toute la variabilité des données, et en offrent une mesure objective.
Mais ces modèles produisent des échantillons de qualité visuelle inférieure, qui sont plus facilement distingués de vrais images.
Le travail présenté dans cette thèse a pour but d'obtenir le meilleur des deux mondes : des échantillons de bonne qualité tout en modélisant tout le support de la distribution.
Nous proposons une procédure d'entrainement qui utilise une fonction de perte auxiliaire pour contrôler quelle information est capturée par la variable latent et quelle information est laissée à un décodeur autoregressif.
Au contraire des précédentes approches pour construire des modèles hybrides de ce genre, notre modèle de nécessite pas de contraindre la capacité du décodeur autoregressif pour empêcher des modèles dégénérés qui ignorent la variable latente.
La deuxième contribution est bâtie sur le modèle du GAN standard, qui utilise un discriminateur pour guider le modèle génératif.
Le discriminateur évalue généralement la qualité d'échantillons individuels, ce qui rend la tache d'évaluer la variabilité des données difficile.
A la place, nous proposons de fournir au discriminateur des ensembles de données, ou batches, qui mélangent des vraies images et des images générées.
Dans notre troisième contribution, nous montrons que les hypothèses paramétriques habituelles faites par les VAE produisent un conflit entre les deux, menant à des performances décevantes pour les modèles hybrides.
Nous proposons une solution basée sur des modèles profonds inversibles, qui entraine un espace de features dans lequel les hypothèses habituelles peuvent être faites sans poser problème.
Notre approche fourni des évaluations e vraisemblance dans l'espace des images tout en étant capable de tirer profit de l'entrainement adversaire.
Elle obtient des échantillons de qualité équivalente au modèle pleinement adversaires tout en améliorant les scores de maximum de vraisemblance au moment de la publication, ce qui constitue une amélioration significative.
Cette thèse se concentre sur la structuration du modèle acoustique pour améliorer la reconnaissance de la parole par modèle de Markov.
La structuration repose sur l'utilisation d'une classification non supervisée des phrases du corpus d'apprentissage pour tenir compte des variabilités dues aux locuteurs et aux canaux de transmission.
L'idée est de regrouper automatiquement les phrases prononcées en classes correspondant à des données acoustiquement similaires.
Pour la modélisation multiple, un modèle acoustique indépendant du locuteur est adapté aux données de chaque classe.
Quand le nombre de classes augmente, la quantité de données disponibles pour l'apprentissage du modèle de chaque classe diminue, et cela peut rendre la modélisation moins fiable.
Une façon de pallier ce problème est de modifier le critère de classification appliqué sur les données d'apprentissage pour permettre à une phrase d'être associée à plusieurs classes.
Ceci est obtenu par l'introduction d'une marge de tolérance lors de la classification ;
et cette approche est étudiée dans la première partie de la thèse.
L'essentiel de la thèse est consacré à une nouvelle approche qui utilise la classification automatique des données d'apprentissage pour structurer le modèle acoustique.
Ainsi, au lieu d'adapter tous les paramètres du modèle HMM-GMM pour chaque classe de données, les informations de classe sont explicitement introduites dans la structure des GMM en associant chaque composante des densités multigaussiennes avec une classe.
Pour exploiter efficacement cette structuration des composantes, deux types de modélisations sont proposés.
Dans la première approche on propose de compléter cette structuration des densités par des pondérations des composantes gaussiennes dépendantes des classes de locuteurs.
Pour cette modélisation, les composantes gaussiennes des mélanges GMM sont structurées en fonction des classes et partagées entre toutes les classes, tandis que les pondérations des composantes des densités sont dépendantes de la classe.
Lors du décodage, le jeu de pondérations des gaussiennes est sélectionné en fonction de la classe estimée.
Dans une deuxième approche, les pondérations des gaussiennes sont remplacées par des matrices de transition entre les composantes gaussiennes des densités.
Les approches proposées dans cette thèse sont analysées et évaluées sur différents corpus de parole qui couvrent différentes sources de variabilité (âge, sexe, accent et bruit)
La production documentaire en contexte professionnel entraîne généralement un processus de révision dans lequel les documents doivent être relus avant validation et publication.
Cette tâche importante fait face à de nouvelles difficultés avec le numérique.
En tant que technologie d'écriture numérique avancée, les chaînes éditoriales XML sont un cadre pertinent pour l'étude de la relecture de documents numériques.
Une partie des propositions faites dans ce mémoire a mené à la réalisation de prototypes ayant été expérimentés dans des situations d'usage des chaînes éditoriales Scenari en contexte pédagogique.
Ces prototypes s'appuient sur des formes linéaires de relecture permettant notamment la comparaison de deux versions du document en se basant sur un algorithme de différentiel.
Cette thèse aborde deux questions majeures du traitement automatique du langage naturel liées à l'analyse sémantique des textes : la détection des sentiments, et le résumé automatique.
Nous abordons ces deux questions par des approches d'apprentissage profond, qui permettent d'exploiter au mieux les données, en particulier lorsqu'elles sont disponibles en grande quantité.
Analyse des sentiments neuronale.
De nombreux réseaux de neurones convolutionnels profonds ont été adaptés du domaine de la vision aux tâches d'analyse des sentiments et de classification des textes.
Cependant, ces études ne permettent pas de conclure de manière satisfaisante quant à l'importance de la profondeur du réseau pour obtenir les meilleures performances en classification de textes.
Nous proposons une adaptation du réseau convolutionnel profond DenseNet pour la classification de texte et étudions l'importance de la profondeur avec différents niveaux de granularité en entrée (mots ou caractères).
En outre, nous proposons de modéliser conjointement sentiments et actes de dialogue, qui constituent un facteur explicatif influent pour l'analyse du sentiment.
Nous avons annoté manuellement les dialogues et les sentiments sur un corpus de micro-blogs, et entraîné un réseau multi-tâches sur ce corpus.
Nous montrons que l'apprentissage par transfert peut être efficacement réalisé entre les deux tâches et analysons de plus certaines corrélations spécifiques entre ces deux aspects.
Résumé de texte neuronal.
L'analyse de sentiments n'apporte qu'une partie de l'information sémantique contenue dans les textes et est insuffisante pour bien comprendre le texte d'origine et prendre des décisions fondées.
L'utilisateur d'un tel système a également besoin des raisons sous-jacentes pour vraiment comprendre les documents.
Dans cette partie, notre objectif est d'étudier une autre forme d'information sémantique fournie par les modèles de résumé automatique.
Nous proposons ainsi un modèle de résumé qui présente de meilleures propriétés d'explicabilité et qui est suffisamment souple pour prendre en charge divers modules d'analyse syntaxique.
Plus spécifiquement, nous linéarisons l'arbre syntaxique sous la forme de segments de texte superposés, qui sont ensuite sélectionnés par un apprentissage par renforcement (RL) et re-générés sous une forme compressée.
Par conséquent, le modèle proposé est capable de gérer à la fois le résumé par extraction et par abstraction.
Nous comparons ainsi de manière détaillée les modèles avec apprentissage par renforcement et les modèles exploitant une connaissance syntaxique supplémentaire des phrases ainsi que leur combinaison, selon plusieurs dimensions liées à la qualité perçue des résumés générés.
Nous montrons lorsqu'il existe une contrainte de ressources (calcul et mémoire) qu'il est préférable de n'utiliser que l'apprentissage par renforcement, qui donne des résultats presque aussi satisfaisants que des modèles syntaxiques, avec moins de paramètres et une convergence plus rapide.
Depuis que les progrès en analyse automatique du langage naturel ont rendu possible la prise en charge d'un dialogue finalisé de manière entièrement automatique (ex. réservation de voyage [1]), les système ont gagné en intelligence pour être capable de répondre à des questions sur des tâches complexes et des domaines plus ouverts (par ex. le système WATSON d'IBM qui a remporté en 2011 le jeux télévisé jeopardy [2]).
Récemment, les agents conversationnels autonomes (connus sous le nom de « chatbots » ) sont devenus des éléments incontournables dans la gestion de la relation client [3].
Dans ce contexte, la fonction d'agent conversationnel évolue de plus en plus vers celle de d'un conseiller virtuel duquel on attend un comportement de plus en plus intelligent allant au-delà de la collecte d'information ou fourniture de réponses simples, mais étant aussi capable de prendre des décisions comme par exemple effectuer des actions et en particulier de reconnaître ses propre limites et de savoir quand il convient de passer la main à un conseiller humain afin de préserver la qualité de service.
Verrou technologique
Un grand nombre de chatbots existent sur le marché, des plus simples souvent élaborés avec des règles faiblement contextuelles, aux plus complexes à base d'apprentissage automatique.
Les premiers  : deviennent vite silencieux si l'on sort des limites de leur domaine de compétence, tandis que les seconds sont capable d'un peu plus de généralisation, mais cela se paie en général au prix d'une plus grande imprécision dans leurs réponses.
Quel que soit le type de système considéré, tout repose sur les connaissances, c'est-à-dire la mémoire de l'agent conversationnel.
Chez l'humain, la mémoire se décline selon plusieurs types [4] par exemple : épisodique (événements vécu), sémantique (connaissances), procédurale (savoir-faire), perceptive (reconnaissance des voix, des odeurs etc.) ou de travail (bloc-note à court terme).
Elle peut être de nature explicite (accès et restitution conscients), implicite (subconscient, émotions) ou autobiographique (personnelle associant mémoire sémantique et épisodique).
Or ces caractéristiques de la mémoire d'un agent autonome conditionnent les représentations qu'il construit [5] et donc ses actions.
Outre les fonctionnalités mémorielles de l'agent conversationnel et leur impact sur la gestion du dialogue, les investigations scientifiques aborderont le problème de l'adaptation au domaine applicatif1 [8] et devront adresser dans ce cadre les avancées récentes en apprentissage automatique (approches neuronales profondes [6]), ceci à différent niveaux de granularité (analyse d'un énoncé, acquisition de connaissances de fond à partir de corpus etc.).
La validation des résultat expérimentaux intermédiaires sera effectuée par le biais de l'évaluation de démonstrateurs (système de dialogue prototypes) selon une procédure d'évaluation quantitative à base de corpus avec un ensemble de mesures de performance définies à partir des protocoles d'évaluation classiques pour le domaine [7].
L'apprentissage statistique cherche à modéliser un lien fonctionnel entre deux variables X et Y à partir d'un échantillon aléatoire de réalisations de (X,Y).
Lorsque la variable Y prend un nombre binaire de valeurs, l'apprentissage s'appelle la classification (ou discrimination en français) et apprendre le lien fonctionnel s'apparente à apprendre la frontière d'une variété dans l'espace de la variable X.
Dans cette thèse, nous nous plaçons dans le contexte de l'apprentissage actif, i.e. nous supposons que l'échantillon d'apprentissage n'est plus aléatoire et que nous pouvons, par l'intermédiaire d'un oracle, générer les points sur lesquels l'apprentissage de la variété va s'effectuer.
Dans le cas où la variable Y est continue (régression), des travaux précédents montrent que le critère de la faible discrépance pour générer les premiers points d'apprentissage est adéquat.
Nous montrons, de manière surprenante, que ces résultats ne peuvent pas être transférés à la classification.
Dans ce manuscrit, nous proposons alors le critère de la dispersion pour la classification.
Ce critère étant difficile à mettre en pratique, nous proposons un nouvel algorithme pour générer un plan d'expérience à faible dispersion dans le carré unité.
Après une première approximation de la variété, des approximations successives peuvent être réalisées afin d'affiner la connaissance de celle-ci.
Deux méthodes d'échantillonnage sont alors envisageables : le « selective sampling » qui choisit les points à présenter à un oracle parmi un ensemble fini de candidats et l' « adaptative sampling » qui permet de choisir n'importe quels points de l'espace de la variable X.
Le deuxième échantillonnage peut être vu comme un passage à la limite du premier.
Néanmoins, en pratique, il n'est pas raisonnable d'utiliser cette méthode.
Nous proposons alors un nouvel algorithme basé sur le critère de dispersion, menant de front exploitation et exploration, pour approximer une variété.
Du XVe au XVII siècle, le Portugal a occupé la première place parmi les Etats les plus avancés de son temps.
Or, le contact des peuples et des cultures a toujours été source d'influences réciproques de nature diverse et multiforme.
Nous nous proposons d'étudier, dans cette thèse, les empreintes lusitaniennes dans le Golfe de Guinée.
La recherche a été menée dans la partie australe de la Côte d'Ivoire, du Ghana, du Togo et du Bénin et elle se fonde sur un corpus composé de quelques centaines d'entrées que nous avons répertoriées à travers une recherche bibliographique et une enquête de huit ans sur le terrain.
L'analyse de ces données, se fait selon une méthode qui combine à la fois l'histoire et le structuralisme dans son approche contrastive car, il s'agit, en réalité, de comparer deux systèmes linguistiques : le portugais et des langues Niger-Congo des groupes langues kru et kwa.
L'entropie d'une distribution sur un ensemble de variables aléatoires discrètes est toujours bornée par l'entropie de la distribution factorisée correspondante.
Cette propriété est due à la sous-modularité de l'entropie.
Par ailleurs, les fonctions sous-modulaires sont une généralisation des fonctions de rang des matroïdes ; ainsi, les fonctions linéaires sur les polytopes associés peuvent être minimisées exactement par un algorithme glouton.
Dans ce manuscrit, nous exploitons ces liens entre les structures des modèles graphiques et les fonctions sous-modulaires. Nous utilisons des algorithmes gloutons pour optimiser des fonctions linéaires sur des polytopes liés aux matroïdes graphiques et hypergraphiques pour apprendre la structure de modèles graphiques, tandis que nous utilisons des algorithmes d'inférence sur les graphes pour optimiser des fonctions sous-modulaires.
La première contribution de cette thèse consiste à approcher par maximum de vraisemblance une distribution de probabilité par une distribution factorisable et de complexité algorithmique contrôlée.
Comme cette complexité est exponentielle dans la largeur arborescente du graphe, notre but est d'apprendre un graphe décomposable avec une largeur arborescente bornée, ce qui est connu pour être NP-difficile.
Nous posons ce problème comme un problème d'optimisation combinatoire et nous proposons une relaxation convexe basée sur les matroïdes graphiques et hypergraphiques.
Ceci donne lieu à une solution approchée avec une bonne performance pratique.
En troisième contribution, nous proposons et analysons des algorithmes visant à minimiser des fonctions sous-modulaires pouvant s'écrire comme somme de fonctions plus simples.
Nos algorithmes n'utilisent que des oracles de ces fonctions simple basés sur minimisation sous-modulaires et de variation totale de telle fonctions.
Dans ce manuscrit, des éléments de réponse sont proposés par la formalisation et l'implémentation d'un modèle multi-échelle de description de la structure d'un segment musical : les Graphes Polytopiques à Relations Latentes (GPRL/PGLR).
Dans ce travail, les segments considérés sont les sections successives qui forment une pièce musicale.
En suivant le formalisme PGLR, les relations de dépendance prédominantes entre éléments musicaux d'un segment sont celles qui relient les éléments situés à des positions homologues sur la grille métrique du segment.
Cette approche généralise sur le plan multi-échelle le modèle Système&amp;Contraste qui décrit sous la forme d'une matrice 2×2 le système d'attente logique au sein d'un segment et la surprise qui découle de la réalisation de cette attente.
Chaque nœud du polytope correspond à un élément musical fondamental (accord, motif, note...), chaque arête représente une relation entre deux nœuds et chaque face représente un système de relations.
Le but du modèle PGLR est à la fois de décrire les dépendances temporelles entre les éléments d'un segment et de modéliser l'attente logique et la surprise qui découlent de l'observation et de la perception des similarités et des différences entre ces éléments.
Cette approche a été formalisée et implémentée pour décrire la structure de séquences d'accords ainsi que de segments rythmiques et mélodiques, puis évaluée par sa capacité à prédire des segments inconnus.
Les résultats obtenus donnent un large avantage à la méthode multi-échelle proposée, qui semble mieux à même de décrire efficacement la structure des segments testés.
Culturel est défini comme l'ensemble des biens matériels et immatériels d'un groupe ou d'une société hérités des générations passées.
Les vases sont parmi les biens culturels les plus emblématiques d'une société.
Si certaines de ces collections ont été numérisées, elles sont rarement accessibles dans un format ouvert et restent isolées.
De plus, l'absence de terminologies clairement identifiées est un obstacle à la communication et au partage des connaissances.
Notre travail vise à répondre à cette problématique par la mise en œuvre de pratiques relevant du web sémantique et de l'ingénierie des connaissances, et plus particulièrement par la construction sous un format du W3C d'une ontologie dédiée aux vases chinois des dynasties Ming et Qing.
La construction de l'ontologie TAO CI ( « céramique » en Chinois) respecte la façon de penser des experts dans leur conceptualisation du domaine et tient compte des normes internationales en Terminologie (ISO 1087 et ISO 704).
Les deux approches reposent sur la notion de caractéristique essentielle et définissent un concept comme étant comme une combinaison unique de caractéristiques.
La recherche des différences entre objets, combinée à une analyse morphologique des termes chinois dont les caractères sont porteurs de sens au regard des connaissances du domaine, permet d'identifier les caractéristiques essentielles.
La définition des concepts repose sur l'idée qu'un concept est un ensemble de caractéristiques essentielles suffisamment stable pour être nommé en langue.
Nous avons ainsi proposé une méthode spécifique de construction d'ontologies guidée par les termes et les caractéristiques essentielles du domaine.
Nous avons également été amenés à introduire de nouveaux termes (néologismes) en anglais et des concepts sans désignation en langue à des fins de structuration de l'ontologie.
La construction de l'ontologie a été faite à l'aide de l'environnement Protégé, environnement le plus utilisé pour la construction d'ontologies au format du W3C (RDF/OWL).
La dimension terminologique a été réduite, comme c'est souvent le cas, à des annotations (SKOS, RDFS) sur les concepts.
L'ontologie TAO CI est liée à des ressources externes telles que CIDOC CRM et ATT Getty pour la partie conceptuelle, et à des musées pour les objets.
Enfin, nous avons évalué l'ontologie TAO CI du point de vue du domaine (couverture) et de son implémentation.
Elle est en accès libre à l'adresse : http : //www.dh.ketrc.com/otcontainer/data/OTContainer.owlLa dernière phase du projet a consisté à réaliser un site web dédié.
Ce site donne accès aux différentes ressources du projet et en particulier à un dictionnaire électronique bilingue (anglais, chinois) des vases des dynasties Ming et Qing.
Les entrées de ce dictionnaire correspondent aux classes OWL de l'ontologie : http : //www.dh.ketrc.com/L'ontologie TAO CI est, à notre connaissance, la première ontologie au format du web sémantique de vases en céramique chinois, ouverte et réutilisable.
Elle est une illustration d'une démarche guidée par les termes et les caractéristiques essentielles du domaine qui peut être appliquée à la construction d'ontologies dans d'autres domaines du patrimoine culturel chinois.
La reconnaissance des émotions est l'un des domaines scientifiques les plus complexes.
Ces dernières années, de plus en plus d'applications tentent de l'automatiser.
Nous traitons dans notre recherche les expressions émotionnelles faciales en s'intéressant spécifiquement aux six émotions de base à savoir la joie, la colère, la peur, le dégoût, la tristesse et la surprise.
Une étude comparative de deux méthodes de reconnaissance des émotions l'une basée sur les descripteurs géométriques et l'autre basée sur les descripteurs d'apparence est effectuée sur la base CK+, base d'émotions simulées, et la base FEEDTUM, base d'émotions spontanées.
Différentes contraintes telles que le changement de résolution, le nombre limité d'images labélisées dans les bases d'émotions, la reconnaissance de nouveaux sujets non inclus dans la base d'apprentissage sont également prises en compte.
Une évaluation de différents schémas de fusion est ensuite réalisée lorsque de nouveaux cas, non inclus dans l'ensemble d'apprentissage, sont considérés.
Les résultats obtenus sont prometteurs pour les émotions simulées (ils dépassent 86%), mais restent insuffisant pour les émotions spontanées.
Ces dernières améliorent les taux de reconnaissance des émotions spontanées.
L'analyse de donnée textuelle est facilitée par l'utilisation du text mining (TM) permettant l'automatisation de l'analyse de contenu et possède de nombreuses applications en santé.
L'une d'entre elles est l'utilisation du TM pour explorer le contenu des messages échangés sur Internet.
Nous avons effectué une revue de la littérature systématique afin d'identifier les applications du TM en santé mentale.
De plus, le TM a permis d'explorer les préoccupations des utilisateurs du forum Doctissimo.com au sujet des antidépresseurs et anxiolytiques entre 2013 et 2015 via l'analyse des fréquences des mots, des cooccurrences, de la modélisation thématique (LDA) et de la popularité des thèmes.
Les quatre applications du TM en santé mentale sont l'analyse des récits des patients (psychopathologie), le ressenti exprimé sur Internet, le contenu des dossiers médicaux, et les thèmes de la littérature médicale.
Quatre grands thèmes ont été identifiés sur le forum : le sevrage (le plus fréquent), l'escitalopram, l'anxiété de l'effet du traitement et les effets secondaires.
Alors que les effets indésirables des traitements est un sujet qui a tendance à décroitre, les interrogations sur les effets du sevrage et le changement de traitement sont grandissantes et associées aux antidépresseurs.
L'objectif de cette thèse est de montrer que, contrairement aux idées reçues, les mots de basses fréquences peuvent être mis à profit de façon efficace en traitement automatique des langues.
Nous les mettons à contribution en alignement sous-phrastique, tâche qui constitue la première étape de la plupart des systèmes de traduction automatique fondée sur les données (traduction probabiliste ou par l'exemple).
Nous montrons que les mots rares peuvent servir de fondement même dans la conception d'une méthode d'alignement sous-phrastique multilingue, à l'aide de techniques différentielles proches de celles utilisées en traduction automatique par l'exemple.
Cette méthode est réellement multilingue, en ce sens qu'elle permet le traitement simultané d'un nombre quelconque de langues.
Elle se veut de surcroît très simple, anytime, et permet un passage à l'échelle naturel.
Nous comparons notre implémentation, Anymalign, à deux ténors statistiques du domaine sur des tâches bilingues.
Bien qu'à l'heure actuelle ses résultats sont en moyenne légèrement en retrait par rapport à l'état de l'art en traduction automatique probabiliste par segments, nous montrons que la qualité propre des lexiques produits par notre méthode est en fait supérieure à celle de l'état de l'art.
Dans l'étude de l'usage des expressions référentielles, de nombreux travaux se sont intéressés à la relation entre formes linguistiques et fonctions saisies en termes informationnels, et la sensibilité précoce du jeune enfant à certaines facettes de la structuration informationnelle a été montré, notamment au statut attentionnel des référents et la dimension du topic-commentaire.
La linguistique interactionnelle a adopté un point de vue complémentaire sur les expressions référentielles, et a montré comment les locuteurs signalent et accomplissent par leur choix d'une expression référentielle non seulement la référence, mais également diverses tâches liées à la gestion de l'interaction même.
Nous avons souhaité, dans ce travail, apporter un éclairage multidimensionnel à la question de l'usage des expressions référentielles, et notamment au contraste entre formes faibles, fortes et disloquées, dans deux langues qui diffèrent quant aux moyens linguistiques employés pour marquer le topic.
Notre analyse repose sur un corpus transversal de 12 enfants francophones et germanophones, âgés entre 2 et 3 ans.
Nous avons analysé les formes et usages des expressions référentielles produites par les enfants et les adultes, en prenant en compte des facteurs morpho-syntaxiques, informationnels et interactionnels.
Nos résultats montrent la complémentarité de ces différents facteurs dans l'explication des usages chez l'enfant et chez l'adulte.
Nous avons pu mettre en avant les emplois spécifiques de certaines ressources linguistiques comme les dislocations, les pronoms démonstratifs der/die/das ainsi que des formes nulles et décrire leur fonctionnement dans des formats d'interaction qui facilitent l'inscription du jeune enfant dans la gestion du discours et de l'interaction.
Cette thèse présente Adetoa, système dédié au repérage et à l'annotation sémantique automatique d'expressions temporelles dans des pages Web pour une application de e-tourisme.
Une étude linguistique détaillée a permis de mettre en avant les caractéristiques et la complexité de l'expression de la temporalité dans les pages Web touristiques.
Ces analyses ont mené à l'élaboration d'un ensemble important de transducteurs (avec Unitex) pour les tâches de repérage et d'annotation des expressions temporelles, ce qui constitue une ressource pouvant être généralisée.
Des transducteurs de liage permettent de grouper toutes les informations concernant une même offre touristique.
Pour l'annotation et l'intégration d'Adetoa à la chaîne de traitement du projet Eiffel, un schéma d'annotation et des règles de transformations ont été mis au point.
Il permet ainsi de rester au plus près des expressions linguistiques de manière à les caractériser finement.
L'ontologie a ensuite pu être adaptée en conséquence, pour un meilleur stockage des données dans la base de connaissance qui lui correspond.
L'évaluation d'Adetoa, présentée dans cette thèse, a montré des résultats satisfaisants aussi bien d'un point de vue théorique que pour cette application industrielle.
Les interfaces cerveau-ordinateur (Brain-Computer Interface ou BCI) permettent la communication entre l'utilisateur et la machine, grâce à la traduction de l'activité cérébrale en commandes qui servent à contrôler différents dispositifs.
De nombreuses limitations empêchent la diffusion des systèmes BCI dans des applications réelles, telles que la phase de calibration qui résulte de la variabilité entre sessions et entre sujets.
Cette phase est fondamentale car elle permet de régler les paramètres nécessaires pour le bon fonctionnement du système, mais elle est considérée beaucoup trop longue et fatigante pour le sujet.
L'objectif de cette thèse est de surmonter ces limites par de nouvelles méthodes basées sur l'amélioration ou le remplacement de la phase de calibration traditionnelle, en proposant le développement d'une BCI centrée sur l'utilisateur.
D'abord, nous proposons deux systèmes BCI adaptés au sujet.
Le premier concerne un clavier virtuel basé sur des potentiels évoqués modulés par code-modulated Visual Evoked Potential (c-VEP) où la phase de calibration traditionnelle est remplacée par une phase dans laquelle les paramètres de stimulation sont réglés de manière adaptée au sujet.
Le deuxième concerne le développement d'un système basé sur l'imagination mentale (MI-BCI) pour un sujet à déficience motrice sévère (tétraplégie), dans le cadre d'une compétition internationale BCI en direct.
Les méthodes proposées ont montré des résultats prometteurs et ouvrent de nouvelles perspectives pour la diffusion des systèmes BCI plus adaptables à l'utilisateur.
Les données cliniques sont produites par différents professionnels de santé, dans divers lieux et sous diverses formes dans le cadre de la pratique de la médecine.
Elles présentent par conséquent une hétérogénéité à la fois au niveau de leur nature et de leur structure mais également une volumétrie particulièrement importante et qualifiable de massive.
Le travail réalisé dans le cadre de cette thèse s'attache à proposer une méthode de recherche d'information efficace au sein de ce type de données complexes et massives.
L'accès aux données cliniques se heurte en premier lieu à la nécessité de modéliser l'information clinique.
Ceci peut notamment être réalisé au sein du dossier patient informatisé ou, dans une plus large mesure, au sein d'entrepôts de données.
Je propose dans ce mémoire une preuve de concept d'un moteur de recherche permettant d'accéder à l'information contenue au sein de l'entrepôt de données de santé sémantique du Centre Hospitalier
Afin de fournir des fonctionnalités de recherche adaptées à cette représentation générique, un langage de requêtes permettant l'accès à l'information clinique par le biais des diverses entités qui la composent a été développé et implémenté dans le cadre de cette thèse.
En second lieu, la massivité des données cliniques constitue un défi technique majeur entravant la mise en oeuvre d'une recherche d'information efficace.
L'implémentation initiale de la preuve de concept sur un système de gestion de base de données relationnel a permis d'objectiver les limites de ces derniers en terme de performances.
Une migration vers un système NoSQL orienté clé
Enfin, l'apport de ce travail dans le contexte plus général de l'entrepôt de données de santé sémantique du CHU de Rouen a été évalué.
La preuve de concept proposée dans ce travail a ainsi été exploitée pour accéder aux descriptions sémantiques afin de répondre à des critères d'inclusion et d'exclusion de patients dans des études cliniques.
Dans cette évaluation, une réponse totale ou partielle a pu être apportée à 72,97% des critères.
De plus, la généricité de l'outil a également permis de l'exploiter dans d'autres contextes tels que la recherche d'information documentaire et bibliographique en santé.
En traitement automatique de la langue, les différentes étapes d'analyse usuelles ont tour à tour amélioré la façon dont le langage peut être modélisé par les machines.
Une étape d'analyse encore mal maîtrisée correspond à l'analyse sémantique.
Ce type d'analyse permettrait de nombreuses avancées, telles que de meilleures interactions homme-machine ou des traductions plus fiables.
Il existe plusieurs structures de représentation du sens telles que PropBank, les AMR et FrameNet.
FrameNet correspond à la représentation en cadres sémantiques dont la théorie a été décrite par Charles Fillmore.
Dans cette théorie, chaque situation prototypique et les différents éléments y intervenant sont représentés de telle sorte que deux situations similaires soient représentées par le même objet, appelé cadre sémantique.
Le projet FrameNet est une application de cette théorie, dans laquelle plusieurs centaines de situations prototypiques sont définies.
Le travail que nous décrirons ici s'inscrit dans la continuité des travaux déjà élaborés pour prédire automatiquement des cadres sémantiques.
Nous verrons également que notre analyse peut être améliorée en fournissant aux modèles de prédiction des informations raffinées au préalable, avec d'un côté une analyse syntaxique dont les liens profonds sont explicités et de l'autre des représentations vectorielles du vocabulaire apprises au préalable.
L'analyse des besoins est la première étape du processus de conception.
C'est aussi une source importante d'innovation en entreprise, notamment lorsqu'elle est partagée ausein de l'équipe de conception pluridisciplinaire.
La prospection et l'anticipation des besoins futurs des utilisateurs porte ainsi des enjeux stratégiques majeurs pour développer des produits nouveaux adaptés aux utilisateurs finaux.
L'objectif de cette thèse est de contribuer à optimiser l'anticipation des besoins dans le but de favoriser l'innovation.
Nos hypothèses portent sur des facteurs méthodologiques et technologiques qui permettent d'améliorer la collaboration de l'équipe pluridisciplinaire et la performance d'anticipation des besoins.
Ces hypothèses sont opérationnalisées comme trois déclinaisons de la méthode des Personas et sont testées dansle cadre de trois projets industriels.
Nous montrons qu'en proposant une méthode combinant plusieurs modes de raisonnements adaptés aux profils métiers de l'équipe pluridisciplinaire, l'anticipation des besoins est améliorée en quantité et en qualité (utilité perçue par les utilisateurs).
Nous montrons également que les technologies support jouent un rôle important dans l'efficacité des méthodes : une technologie collaborative et ludique comme une table interactive peut augmenter le nombre d'items stratégiques pour l'entreprise (c'est-à-dire utiles et faisables) ; une technologie immersive et ludique comme un monde virtuel permet d'orienterl'anticipation des besoins selon les objectifs du projet (techno-centrés ou centrés-utilisateurs).
Ces résultats ouvrent de nombreuses perspectives pour l'évolution méthodologique et technologique de la phase d'anticipation des besoins dans les projets d'innovation.
Cette thèse porte sur la transformation de la voix d'un locuteur dans l'objectif d'indiquer la distance de celui-ci : une transformation en voix chuchotée pour indiquer une distance proche et une transformation en voix criée pour une distance plutôt éloignée.
Nous effectuons dans un premier temps des analyses approfondies pour déterminer les paramètres les plus pertinentes dans une voix chuchotée et surtout dans une voix criée (beaucoup plus difficile).
La contribution principale de cette partie est de montrer la pertinence des paramètres prosodiques dans la perception de l'effort vocal dans une voix criée.
Nous proposons ensuite des descripteurs permettant de mieux caractériser les contours prosodiques.
Pour la transformation proprement dite, nous proposons plusieurs nouvelles règles de transformation qui contrôlent de manière primordiale la qualité des voix transformées.
Les résultats ont montré une très bonne qualité des voix chuchotées transformées ainsi que pour des voix criées pour des structures linguistiques relativement simples (CVC, CVCV, etc.).
La prise de décision est un domaine très étudié en sciences, que ce soit en neurosciences pour comprendre les processus sous tendant la prise de décision chez les animaux, qu'en robotique pour modéliser des processus de prise de décision efficaces et rapides dans des tâches en environnement réel.
En neurosciences, ce problème est résolu online avec des modèles de prises de décision séquentiels basés sur l'apprentissage par renforcement.
En robotique, l'objectif premier est l'efficacité, dans le but d'être déployés en environnement réel.
Cependant en robotique ce que l'on peut appeler le budget et qui concerne les limitations inhérentes au matériel, comme les temps de calculs, les actions limitées disponibles au robot ou la durée de vie de la batterie du robot, ne sont souvent pas prises en compte à l'heure actuelle.
Nous nous proposons dans ce travail de thèse d'introduire la notion de budget comme contrainte explicite dans les processus d'apprentissage robotique appliqués à une tâche de localisation en mettant en place un modèle basé sur des travaux développés en apprentissage statistique qui traitent les données sous contrainte de budget, en limitant l'apport en données ou en posant une contrainte de temps plus explicite.
Dans ce cadre, l'alternance entre recherche d'information pour la localisation et la décision de se déplacer pour un robot peuvent être indirectement liés à la notion de compromis exploration
Nous apprenons très jeunes une quantité de règles nous permettant d'interagir avec d'autres personnes : des conventions sociales.
Elles diffèrent des autres types d'apprentissage dans le sens où les premières personnes à les avoir utilisées n'ont fait qu'un choix arbitraire parmi plusieurs alternatives possibles : le côté de la route où conduire, la forme d'une prise électrique, ou inventer de nouveaux mots.
À cause de celà, lorsqu'une nouvelle convention se crée au sein d'une population d'individus interagissant entre eux, de nombreuses alternatives peuvent apparaître et conduire à une situation complexe où plusieurs conventions équivalentes coexistent en compétition.
Nous exerçons communément un contrôle actif sur nos situations d'apprentissage, en par exemple sélectionnant des activités qui ne soient ni trop simples ni trop complexes.
Il a été montré que ce type de comportement, dans des cas comme l'apprentissage sensori-moteur, aide à apprendre mieux, plus vite, et avec moins d'exemples.
Est-ce que de tels mécanismes pourraient aussi influencer la négociation de conventions sociales ?
Le lexique est un exemple particulier de convention sociale : quels mots associer avec tel objet ou tel sens ?
Une classe de modèles computationels, les Language Games, montrent qu'il est possible pour une population d'individus de construire un langage commun via une série d'interactions par paires.
En particulier, le modèle appelé Naming Game met l'accent sur la formation du lexique reliant mots et sens, et montre une typique explosion de la complexité avant de commencer à écarter les conventions synonymes ou homonymes et arriver à un consensus.
Différentes stratégies sont introduites, et ont des impacts différents sur à la fois le temps nécessaire pour converger vers un consensus et la quantité de mémoire nécessaire à chaque individu.
Premièrement, nous limitons artificiellement la mémoire des agents pour éviter l'explosion de complexité locale.
Quelques stratégies sont présentées, certaines ayant des propriétés similaires au cas standard en termes de temps de convergence.
Dans un deuxième temps, nous formalisons ce que les agents doivent optimiser, en se basant sur une représentation de l'état moyen de la population.
Deux stratégies inspirées de cette notion permettent de limiter les besoins en mémoire sans avoir à contraindre le système, et en prime permettent de converger plus rapidement.
Nous montrons ensuite que la dynamique obtenue est proche d'un comportement théorique optimal, exprimé comme une borne inférieure au temps de convergence.
Les résultats suggèrent qu'ils ont effectivement une politique active de choix de sujet de conversation, en comparaison avec un choix aléatoire.
Les contributions de ce travail de thèse incluent aussi une classification des modèles de Naming Games existants, et un cadriciel open-source pour les simuler.
Dans le contexte de l'interaction humain-agent, notre objectif était d'améliorer la qualité de l'interaction en : (1) dotant l'agent de la capacité d'exprimer des attitudes sociales telles que la dominance ou l'amicalité ce qui renforcent ses compétences sociales ; (2) adaptant le comportement de l'agent selon le comportement de l'utilisateur, par conséquent l'agent et l'utilisateur s'influencent mutuellement par le biais d'une boucle interactive ; (3) prédisant le niveau d'engagement de l'utilisateur et adaptant en conséquence le comportement de l'agent, ce qui contribue à maintenir l'intérêt et la motivation de l'utilisateur.
Nous nous basons sur les progrès récents dans le domaine de l'apprentissage automatique, plus particulièrement de l'extraction de séquences temporelles et des réseaux de neurones.
Le premier est utilisé pour apprendre des séquences pertinentes de signaux non-verbaux qui représentent au mieux les variations d'attitude, puis les reproduire par l'agent.
Le seconde est utilisé pour englober la dynamique des signaux non verbaux.
Deux cas d'utilisation ont été explorés à l'aide du modèle LSTM : l'adaptation du comportement de l'agent en fonction de l'historique de comportement de l'agent et de l'utilisateur ; et la prédiction de l'engagement de l'utilisateur basée sur son propre historique de comportement.
La pertinence des modèles et des algorithmes implémentés a été validée au moyen de nombreuses études approfondies et d'une évaluation quantitative rigoureuse des résultats obtenus.
De plus, les travaux réalisés ont été intégrés dans une plateforme d'agents virtuels.
L'interopérabilité des applications est devenue le leitmotiv des développeurs et concepteurs en ingénierie système.
La plupart des approches pour l'interopérabilité existant dans l'entreprise ont pour objectif principal l'ajustement et l'adaptation des types et structures de données nécessaire à la mise en œuvre de collaboration entre entreprises.
Dans le domaine des entreprises manufacturières, le produit est une composante centrale.
Des travaux scientifiques proposent des solutions pour la prise en compte des systèmes d'information issus des produits, tout au long de leur cycle de vie.
Mais ces informations sont souvent non corrélées.
La gestion des données de produit (PDM) est couramment mise en œuvre pour gérer toute l'information relative aux produits durant tout leur cycle de vie.
Cependant, ces modèles sont généralement des "îlots" indépendants ne tenant pas compte de la problématique d'interopérabilité des applications supportant ces modèles.
L'objectif de cette thèse est d'étudier cette problématique d'interopérabilité appliquée aux applications utilisées dans l'entreprise manufacturière et de définir un modèle ontologique de la connaissance des entreprises relatives aux produits qu'elles fabriquent, sur la base de leurs données techniques.
Le résultat de ce travail de recherche concerne la formalisation d'une méthodologie d'identification des informations de gestion techniques des produits, sous la forme d'une ontologie, pour l'interopérabilité des applications d'entreprises manufacturières, sur la base des standards existants tels que l'ISO 10303 et l'IEC 62264.
Les essais contrôlés randomisés et les revues systématiques qui les synthétisent sont des éléments indispensables à la pratique de la médecine fondée sur les preuves.
Celle-ci accorde un poids très important à la recherche biomédicale, et sera donc compromise si la base des éléments de preuve s'avère erronée.
La mauvaise qualité de la recherche biomédicale est dénoncée depuis plusieurs années, mais en 2009, I.Chalmers et P.Glasziou intègrent ces critiques dans un concept plus global, le gaspillage de la recherche.
Ils affirment que sans des rapports de recherche accessibles, honnêtes et utilisables, la recherche biomédicale ne pourra pas aider les patients et soignants à prendre des décisions éclairées, et pourrait ainsi être considérée comme gaspillée.
Une recherche mal conçue, mal réalisée, non ou mal rapportée serait ainsi gaspillée.
Ils ont estimé que près de de 85 % de la recherche biomédicale le serait.
Dans le cadre de cette thèse, nous nous sommes intéressés au gaspillage, évitable, de la recherche dans les essais cliniques.
Puis nous avons estimé dans quelle mesure ce gaspillage pouvait être évité de manière simple et peu coûteuse.
Nos travaux suggèrent que 1) que des ajustements méthodologiques simples et peu coûteux permettraient de limiter le risque de biais dans 50 % des essais, et ainsi réduire partiellement le gaspillage de la recherche, et 2) que de nombreux essais ne mesuraient ou ne rapportaient pas complètement les critères de jugement importants, mais que ce gaspillage aurait pu être partiellement évité pour la majorité des essais.
De plus, l'intérêt dans les langages définis sur des alphabets infinis ou de grande taille est croissant au fil des années.
Même si plusieurs propriétés et théories se généralisent à partir du cas fini, l'apprentissage de tels langages est une tâche difficile.
En effet, dans ce contexte, l'application naïve des algorithmes d'apprentissage traditionnel n'est pas possible.
Dans cette thèse, nous présentons un schéma algorithmique général pour l'apprentissage de langages définis sur des alphabets infinis ou de grande taille, comme par exemple des sous-ensembles bornés de N or R ou des vecteurs booléens de grandes dimensions.
Nous nous restreignons aux classes de langages qui sont acceptés par des automates déterministes symboliques utilisant des prédicats pour définir les transitions, construisant ainsi une partition finie de l'alphabet pour chaque état.
Notre algorithme d'apprentissage, qui est une adaptation du L* d'Angluin, combine l'apprentissage classique d'un automate par la caractérisation de ses états, avec l'apprentissage de prédicats statiques définissant les partitions de l'alphabet.
Nous utilisons l'apprentissage incrémental avec la propriété que deux types de requêtes fournissent une information suffisante sur le langage cible.
Les requêtes du premier type sont les requêtes d'adhésions, qui permettent de savoir si un mot proposé appartient ou non au langage cible.
Nous étudions l'apprentissage de langages définis sur des alphabets infinis ou de grande tailles dans un cadre théorique et général, mais notre objectif est de proposer des solutions concrètes pour un certain nombre de cas particuliers.
Ensuite, nous nous intéressons aux deux principaux aspects du problème.
Dans un premier temps, nous supposerons que les requêtes d'équivalence renvoient toujours un contre-exemple minimal pour un ordre de longueur-lexicographique quand l'automate proposé est incorrect.
Puis dans un second temps, nous relâchons cette hypothèse forte d'un oracle d'équivalence, et nous la remplaçons avec une hypothèse plus réaliste où l'équivalence est approchée par un test sur les requêtes qui utilisent un échantillonnage sur l'ensemble des mots.
Dans ce dernier cas, ce type de requêtes ne garantit pas l'obtention de contre-exemples, et par conséquent de contre-exemples minimaux.
Tout les algorithmes ont été implémentés, et leurs performances, en terme de construction d'automate et de taille d'alphabet, ont été évaluées empiriquement.
Dans de nombreux contextes statistiques, il est courant de s'appuyer sur un modèle linéaire (généralisé) pour effectuer des prévisions, ou sélectionner des variables.
Néanmoins, il est souvent réaliste de supposer que le deuxième, voire le troisième ordre d'interactions de variables pourraient aider et améliorer les décisions prises.
Le même besoin de modéliser des interactions se produit aussi fréquemment dans le contexte de la publicité en ligne, en particulier pour les enchères en temps réel et la modélisation des taux de clics (CTR).
En effet, avec les contraintes de temps réel pour la prédiction, bien que de nombreux échantillons puissent être collectés, seules quelques variables sont obtenues par les acteurs dans ce champ (par exemple le type de système d'exploitation, le type de navigateur, l'heure, etc).
En outre, le signal à récupérer est faible, avec seulement environ 0,1% de taux de clics (CTR) dans le meilleur des cas.
Par conséquent, il pourrait être bénéfique d'intégrer des interactions pour travailler avec des contraintes aussi drastiques.
Enfin et surtout, pour les données catégoriques, une telle interaction peut modéliser les raffinements hiérarchiques des catégories : cela présente un grand intérêt pour le traitement du langage naturel.
Par conséquent, proposer des méthodes efficaces pour faire face à de tels scénarios pourrait avoir un impact considérable dans diverses applications de l'apprentissage automatique.
Dans le scénario de haute dimension que nous visons, les méthodes de type Lasso avec interactions ont été proposées par Radchenko et James [2010], ainsi qu'une variante conçue pour gérer les interactions hiérarchiques étudiée par Bien et al. [2013].
Jusqu'à présent, nous pensons que ces méthodes sont restées sous-utilisées en raison de leurs limites computationnelles.
Nous pensons que l'adaptation des développements récents proposés par les deux dernières équipes mentionnées, ainsi que les stratégies d'ensembles actifs raffinés étudiées par Massias et al. [2018] pourrait conduire des mises en œuvre plus rapides que les solutions actuellement disponibles.
Enfin, la forme explicite de la régularisation utilisée est encore ouverte, notamment pour arriver à lutter contre le fléau de la dimension dans ce contexte.
Un robot d'assistance sociale (SAR) est destiné à engager les gens dans une interaction située comme la surveillance de l'exercice physique, la réadaptation neuropsychologique ou l'entraînement cognitif.
Alors que les comportements interactifs de ces systèmes sont généralement scriptés, nous discutons ici du cadre d'apprentissage de comportements interactifs multimodaux qui est proposé par le projet SOMBRERO.
Dans notre travail, nous avons utilisé l'apprentissage par démonstration afin de fournir au robot des compétences nécessaires pour effectuer des tâches collaboratives avec des partenaires humains.
Il y a trois étapes principales d'apprentissage de l'interaction par démonstration : (1) recueillir des comportements interactifs représentatifs démontrés par des tuteurs humains ; (2) construire des modèles des comportements observés tout en tenant compte des connaissances a priori (modèle de tâche et d'utilisateur, etc.) ; et ensuite (3) fournir au robot-cible des contrôleurs de gestes appropriés pour exécuter les comportements souhaités.
Les modèles multimodaux HRI (Human-Robot Interaction) sont fortement inspirés des interactions humain-humain (HHI).
Le transfert des comportements HHI aux modèles HRI se heurte à plusieurs problèmes : (1) adapter les comportements humains aux capacités interactives du robot en ce qui concerne ses limitations physiques et ses capacités de perception, d'action et de raisonnement limitées ; (2) les changements drastiques des comportements des partenaires humains face aux robots ou aux agents virtuels ; (3) la modélisation des comportements interactifs conjoints ; (4) la validation des comportements robotiques par les partenaires humains jusqu'à ce qu'ils soient perçus comme adéquats et significatifs.
Dans cette thèse, nous étudions et faisons des progrès sur ces quatre défis.
En particulier, nous traitons les deux premiers problèmes (transfert de HHI vers HRI) en adaptant le scénario et en utilisant la téléopération immersive.
A la fin de cette thèse, nous évaluons une première version de robot autonome équipé des modèles construits par apprentissage.
Cette thèse porte sur les problèmes de sécurité sur le réseau électrique français exploité par RTE, le Gestionnaire de Réseau de Transport (GRT).
Les progrès en matière d'énergie durable, d'efficacité du marché de l'électricité ou de nouveaux modes de consommation poussent les GRT à exploiter le réseau plus près de ses limites de sécurité.
Pour ce faire, il est essentiel de rendre le réseau plus "intelligent".
Pour s'attaquer à ce problème, ce travail explore les avantages des réseaux neuronaux artificiels.
Nous proposons de nouveaux algorithmes et architectures d'apprentissage profond pour aider les opérateurs humains (dispatcheurs) à prendre des décisions que nous appelons " guided dropout ".
Ceci permet de prévoir les flux électriques consécutifs à une modification volontaire ou accidentelle du réseau.
Pour se faire, les données continues (productions et consommations) sont introduites de manière standard, via une couche d'entrée au réseau neuronal, tandis que les données discrètes (topologies du réseau électrique) sont encodées directement dans l'architecture réseau neuronal.
L'architecture est modifiée dynamiquement en fonction de la topologie du réseau électrique en activant ou désactivant des unités cachées.
Le principal avantage de cette technique réside dans sa capacité à prédire les flux même pour des topologies de réseau inédites.
Le "guided dropout" atteint une précision élevée (jusqu'à 99% de précision pour les prévisions de débit) tout en allant 300 fois plus vite que des simulateurs de grille physiques basés sur les lois de Kirchoff, même pour des topologies jamais vues, sans connaissance détaillée de la structure de la grille.
Nous avons également montré que le "guided dropout" peut être utilisé pour classer par ordre de gravité des évènements pouvant survenir.
Dans cette application, nous avons démontré que notre algorithme permet d'obtenir le même risque que les politiques actuellement mises en œuvre tout en n'exigeant que 2 % du budget informatique.
Le classement reste pertinent, même pour des cas de réseau jamais vus auparavant, et peut être utilisé pour avoir une estimation globale de la sécurité globale du réseau électrique.
L'importance de la documentation du patrimoine culturel croit parallèlement aux risques auxquels il est exposé tels que les guerres, le développement urbain incontrôlé, les catastrophes naturelles, la négligence et les techniques ou stratégies de conservation inappropriées.
De plus, la documentation constitue un outil fondamental pour l'évaluation, la conservation, le suivi et la gestion du patrimoine culturel.
Dès lors, cet outil majeur nous permet d'estimer la valeur historique, scientifique, sociale et économique de ce patrimoine.
Selon plusieurs institutions internationales dédiées à la conservation du patrimoine culturel, il y a un besoin réel de développer et d'adapter de solutions informatiques capables de faciliter et de soutenir la documentation du patrimoine culturel peu documenté surtout dans les pays en développement où il y a un manque flagrant de ressources.
Parmi ces pays, la Palestine représente un cas d'étude pertinent dans cette problématique de carence en documentation de son patrimoine.
Pour répondre à cette problématique, nous proposons une approche d'acquisition et d'extraction de connaissances patrimoniales dans un contexte peu documenté.
Nous prenons comme cas d'étude l'église de la Nativité en Palestine et nous mettons en place notre approche théorique par le développement d'une plateforme d'acquisition et d'extraction de connaissances patrimoniales à l'aide d'un Framework pour la documentation de patrimoine culturel.
Notre solution est basée sur les technologies sémantiques, ce qui nous donne la possibilité, dès le début, de fournir une description ontologique riche, une meilleure structuration de l'information, un niveau élevé d'interopérabilité et un meilleur traitement automatique (lisibilité par les machines) sans efforts additionnels.
Dès lors, l'interaction entre les deux composants de notre système ainsi que les connaissances patrimoniales se développent et s'améliorent au fil de temps surtout que notre système utilise les contributions manuelles et validations des résultats automatiques (dans les deux composants) par les experts afin d'optimiser sa performance.
La théorie des graphes a longtemps été étudiée en mathématiques et en probabilité en tant qu'outil pour décrire la dépendance entre les nœuds.
Cependant, ce n'est que récemment qu'elle a été mise en œuvre sur des données, donnant naissance à l'analyse statistique des réseaux réels.
La topologie des réseaux économiques et financiers est remarquablement complexe : elle n'est généralement pas observée, et elle nécessite ainsi des procédures inférentielles adéquates pour son estimation, d'ailleurs non seulement les nœuds, mais la structure de la dépendance elle-même évolue dans le temps.
Des outils statistiques et économétriques pour modéliser la dynamique de changement de la structure du réseau font défaut, malgré leurs besoins croissants dans plusieurs domaines de recherche.
En même temps, avec le début de l'ère des “Big data”, la taille des ensembles de données disponibles devient de plus en plus élevée et leur structure interne devient de plus en plus complexe, entravant les processus inférentiels traditionnels dans plusieurs cas.
Cette thèse a pour but de contribuer à ce nouveau champ littéraire qui associe probabilités, économie, physique et sociologie en proposant de nouvelles méthodologies statistiques et économétriques pour l'étude de l'évolution temporelle des structures en réseau de moyenne et haute dimension.
Les enfants « musiciens » et les jeunes musiciens professionnels surpassent les participants de contrôle dans une série d'expériences, avec une plasticité cérébrale plus rapide, et une connectivité fonctionnelle plus forte, mesurées par électroencéphalographie.
Les résultats des musiciens plus âgés sont moins clairs, suggérant un impact limité de la formation musicale sur le déclin cognitif.
Enfin, les jeunes musiciens ont une meilleure mémoire à long terme des nouveaux mots, ce qui contribuerait à expliquer l'avantage observé.
Ces effets de transfert de la formation musicale au niveau sémantique et de la mémoire à long terme révèlent l'importance des fonctions cognitives générales et ouvrent de nouvelles perspectives pour l'éducation et la rééducation.
Le concept d'automate, central en théorie des langages, est l'outil d'appréhension naturel et efficace de nombreux problèmes concrets.
L'usage intensif des automates finis dans un cadre algorithmique s 'illustre par de nombreux travaux de recherche.
La correction et l 'évaluation sont les deux questions fondamentales de l'algorithmique.
Une méthode classique d'évaluation s'appuie sur la génération aléatoire contrôlée d'instances d'entrée.
Les travaux d´écrits dans cette thèse s'inscrivent dans ce cadre et plus particulièrement dans le domaine de la génération aléatoire uniforme d'automates finis.
Cette construction s'appuie sur la méthode symbolique.
Des résultats théoriques et une étude expérimentale sont exposés.
Un générateur aléatoire d'automates non-déterministes illustre ensuite la souplesse d'utilisation de la méthode de Monte-Carlo par Chaînes de Markov (MCMC) ainsi que la mise en œuvre de l'algorithme de Metropolis-Hastings pour l'échantillonnage à isomorphisme près.
Un résultat sur le temps de mélange est donné dans le cadre général.
L 'échantillonnage par méthode MCMC pose le problème de l'évaluation du temps de mélange dans la chaîne.
En s'inspirant de travaux antérieurs pour construire un générateur d'automates partiellement ordonnés, on montre comment différents outils statistiques permettent de s'attaquer à ce problème.
Les dernières tendances de l'informatique distribuées préconisent le Fog computing qui étend les capacités du Cloud en bordure du réseau, à proximité des objets terminaux etdes utilisateurs finaux localisé dans le monde physique.
Le Fog est un catalyseur clé des applications de l'Internet des Objets (IoT), car il résout certains des besoins que le Cloud ne parvient à satisfaire, tels que les faibles latences, la confidentialité des données sensibles, la qualité de service ainsi que les contraintes géographiques.
Pour cette raison, le Fog devient de plus en plus populaire et trouve des cas d'utilisations dans de nombreux domaines tels que la domotique, l'agriculture, la e-santé, les voitures autonomes, etc.
Le Fog, cependant, est instable car il est constitué de milliards d'objets hétérogènes au sein d'un écosystème dynamique.
Les objets de l'IoT tombent en pannes régulièrement parce qu'ils sont produits en masse à des couts très bas.
Dans un tel écosystème, les défaillances produisent des comportements incohérents qui peuvent provoquer des situations dangereuses et coûteuses dans le monde physique.
Cette Thèse propose une approche autonome de gestion de la résilience des applications IoT déployées en environnement Fog.
L'approche proposée comprend quatre tâches fonctionnelles : (i) sauvegarde d'état, (ii) surveillance, (iii) notification des défaillances, et (iv) reprise sur panne.
Chaque tâche est un regroupement de rôles similaires et est mise en oeuvre en tenant compte les spécificités de l'écosystème (e.g., hétérogénéité, ressources limitées).
La sauvegarde d'état vise à sauvegarder les informations sur l'état de l'application.
Ces informations sont constituées des données d'exécution et de la mémoire volatile, ainsi que des messages échangés et fonctions exécutées par l'application.
La surveillance vise à observer et à communiquer des informations sur le cycle de vie de l'application.
Lors d'une défaillance, des notifications sont propagées à la partie de l'application affectée par cette défaillance.
La propagation des notifications vise à limiter la portée de l'impact de la défaillance et à fournir un service partiel ou dégradé.
Pour établir une reprise sur panne, l'application est reconfigurée et les données enregistrées lors de la tâche de sauvegarde d'état sont utilisées afin de restaurer un état cohérent de l'application par rapport au monde physique.
La procédure de reprise sur panne en assurant la cohérence cyber-physique évite les impacts dangereux et coûteux de la défaillance sur le monde physique.
L'approche proposée a été validée à l'aide de techniques de vérification par modèle afin de vérifier que certaines propriétés importantes sont satisfaites.
Cette approche de résilience a été mise en oeuvre sous la forme d'une boîte à outils, F3ARIoT, destiné aux développeurs.
F3ARIoT a été évalué sur une application domotique.
Les résultats montrent la faisabilité de son utilisation sur des déploiements réels d'applications Fog-IoT, ainsi que des performances satisfaisantes par rapport aux utilisateurs.
Dans le milieu aéronautique professionnel (un des secteurs professionnels les plus sûr au monde), la gestion des conséquences des erreurs humaines doit être améliorée pour garantir une sécurité maximum.
Cependant, la mise en place de ces techniques est rendue difficile par les particularités des systèmes sociotechniques complexes (la certification, la complexité des systèmes conçus, le nombre de personnes impliquées…).
Notre étude a pour but de développer et de valider des outils d'aide à la conception centrée sur l'utilisateur, notamment pour le traitement automatique de grande quantité de données.
Les résultats de cette méthode par jugement d'expert ont été comparés à ceux obtenus à l'aide d'outils de traitement automatique.
De proposer une méthodologie permettant l'extraction automatique de situations à risque pouvant donner lieu à des études plus approfondies, sur simulateur par exemple.
Cette étape est primordiale dans cadre de la conception centrée utilisateur.
Les liens établis avec les études des incidents/accidents laissent envisager des impacts positifs sur la sécurité aérienne.
L'estimation de la pose humaine et la reconnaissance des activités humaines sont des étapes importantes dans de nombreuses applications comme la robotique, la surveillance et la sécurité, etc.
Actuellement abordées dans le domaine, ces tâches ne sont toujours pas résolues dans des environnements non-coopératifs particulièrement.
Dans un premier temps, nous nous sommes concentrés sur la reconnaissance des actions complexes depuis des vidéos.
Pour ceci, nous avons introduit une représentation spatio-temporelle indépendante du point de vue.
Plus précisément, nous avons capturé le mouvement de la personne en utilisant un capteur de profondeur et l'avons encodé en 3D pour le représenter.
Un descripteur 3D a ensuite été utilisé pour la classification des séquences avec la méthodologie bag-of-words.
Pour la deuxième partie, notre objectif était l'estimation de pose articulée, qui est souvent une étape intermédiaire pour la reconnaissance de l'activité.
Notre motivation était d'incorporer des informations à partir de capteurs multiples et de les fusionner pour surmonter le problème de l'auto-occlusion.
Nous avons démontré que les contraintes géométriques et les paramètres de cohérence d'apparence sont efficaces pour renforcer la cohérence entre les points de vue, aussi que les paramètres classiques.
Finalement, nous avons évalué ces nouvelles méthodes sur des datasets publics, qui vérifie que l'utilisation de représentations indépendantes de la vue et l'intégration d'informations à partir de points de vue multiples améliore la performance pour les tâches ciblées dans le cadre de cette manuscrit.
Les entités nommées extraites et leurs contextes textuels seront catégorisés à l'aide d'approches en Traitement Automatique des Langues afin de rendre compte de la sémantique des relations exprimées dans les textes.
La description précise des documents au travers de formats d'encodage tels que le XML-EAD, Dublin Core ou XML-TEI facilite ainsi la conception d'instruments de recherche, les tâches de fouille de données mais aussi le partage en ligne ou au sein d'une communauté de chercheurs.
Une ontologie sera proposée afin de répondre à ces besoins et nous permettre de formaliser cette problématique du point de vue du Web Sémantique.
Ce projet reposera sur l'utilisation et l'extension d'ontologies déjà existantes telles que DBPedia ou ISA Programme Person Core Vocabulary, dont l'intérêt dans des tâches de désambiguïsation des entités nommées a déjà été largement démontré.
Les documents numérisés seront traités pour la reconnaissance optique de caractères (OCR) et enfin encodés au format XML-TEI.
Les données produites seront enregistrées en suivant le standard RDF, nous assurant ainsi l'interopérabilité avec tous les services appartenant à l'OpenLinkedData, afin de produire des inférences textuelles basées sur SPARQL.
La méthodologie pour le peuplement de l'ontologie sera développée spécifiquement pour la tâche d'extraction et de catégorisation d'entités nommées et de données spatio-temporelles, afin de pouvoir extraire automatiquement les relations entre ces entités dans les documents.
Nous abordons, dans cette thèse, une étude sur les représentations continues de mots (en anglais word embeddings) appliquées à la détection automatique des erreurs dans les transcriptions de la parole.
Notre étude se concentre sur l'utilisation d'une approche neuronale pour améliorer la détection automatique des erreurs dans les transcriptions automatiques, en exploitant les word embeddings.
L'exploitation des embeddings repose sur l'idée que la détection d'erreurs consiste à trouver les possibles incongruités linguistiques ou acoustiques au sein des transcriptions automatiques.
L'intérêt est donc de trouver la représentation appropriée du mot qui permet de capturer des informations pertinentes pour pouvoir détecter ces anomalies.
Notre contribution dans le cadre de cette thèse porte sur plusieurs axes.
D'abord, nous commençons par une étude préliminaire dans laquelle nous proposons une architecture neuronale capable d'intégrer différents types de descripteurs, y compris les embeddings.
Ensuite, nous nous focalisons sur une étude approfondie des représentations continues de mots.
Cette étude porte d'une part sur l'évaluation de différents types d'embeddings linguistiques puis sur leurs combinaisons.
D'autre part, elle s'intéresse aux embeddings acoustiques de mots.
Puis, nous présentons une étude sur l'analyse des erreurs de classifications, qui a pour objectif de percevoir les erreurs difficiles à détecter.
Finalement, nous exploitons les embeddings linguistiques et acoustiques ainsi que l'information fournie par notre système de détections d'erreurs dans plusieurs cadres applicatifs.
Le principal objectif de cette étude est de situer les constructions verbales du wolof dans une perspective typologique.
Il s'agit tout d'abord de proposer une description synthétique du système de prédication verbale du wolof dans une perspective typologique, en nous appuyant sur les travaux de référence concernant la conjugaison du wolof.
L'analyse typologique de ces constructions périphrastiques nous sert de base empirique pour proposer une nouvelle approche de la notion d'auxiliaire.
Nous considérons que, dans une perspective typologique, l'auxiliaire ne doit pas être défini comme une catégorie lexicale spécifique, ni comme une étape dans un chemin de grammaticalisation, mais plutôt comme un élément prédicatif autonome ayant une fonction spécifique.
Par ailleurs, nous proposons une analyse constructionnelle de l'organisation du système de prédication verbale du wolof.
Nous considérons que les constructions verbales du wolof ne forment pas un ensemble non structuré d'entités indépendantes, mais plutôt un système extrêmement structuré (un réseau de constructions).
En outre, nous montrons que certaines idiosyncrasies apparentes dans le paradigme de conjugaison du wolof peuvent s'expliquer à la lumière de la diachronie.
Enfin, nous proposons une analyse comparative des constructions verbales des langues atlantiques afin de déterminer ce qui, dans la conjugaison du wolof, est issu du proto-atlantique.
Cette thèse propose un modèle permettant la mise en œuvre d'un système de raisonnement à partir de cas capable d'adapter des procédures représentées sous forme de texte en langue naturelle, en réponse à des requêtes d'utilisateurs.
Bien que les cas et les solutions soient sous forme textuelle, l'adaptation elle-même est d'abord appliquée à un réseau de contraintes temporelles exprimées à l'aide d'une algèbre qualitative, grâce à l'utilisation d'un opérateur de révision des croyances.
Des méthodes de traitement automatique des langues sont utilisées pour acquérir les représentations algébriques des cas ainsi que pour regénérer le texte à partir du résultat de l'adaptation
Le travail de cette thèse vise à proposer un système support à la créativité selon une architecture multi-agents afin de gérer les connaissances nécessaires et produites durant un atelier de créativité.
Ce travail contribue à la recherche scientifique à différents égards.
Au préalable de concevoir un quelconque système, une revue des systèmes actuels supportant la créativité est réalisée pour déterminer leurs limites en termes de processus de créativité et de modes de collaboration.
Pour répondre à ces limites, l'approche d'ingénierie des connaissances est adoptée.
A partir de la modélisation organisationnelle d'un atelier de créativité, l'organisation des agents informatiques qui vont contribuer à la gestion des connaissances en est déduite.
Par la suite, une ontologie de l'atelier de créativité est formalisée à partir de la modélisation de l'organisation afin d'apporter une représentation des connaissances et de l'environnement aux agents.
Ainsi, l'architecture multi-agents proposée pour concevoir un système support à la créativité permet d'explorer de nouveaux modes de traitement des connaissances notamment concernant l'évaluation des idées.
Une méthodologie d'évaluation des idées selon des méthodes d'analyse multicritère est proposée.
En complément de cette méthodologie, le traitement automatique des idées a été expérimenté afin d'aider les évaluateurs dans leur tâche
Identifier les leviers de satisfaction des consommateurs est aujourd'hui capital dans un monde où la relation que tisse une entreprise avec ses clients est sa plus grande richesse.
Le domaine de la fouille d'opinion, dans lequel s'inscrit cette thèse, propose des méthodes permettant de répondre à ce besoin.
Celles-ci nécessitent cependant une mise à jour constante de ressources spécialisées qui sont la pierre angulaire des outils d'analyse d'opinion.
Ce travail vise à développer des stratégies d'acquisition et de structuration de ces ressources, qui prennent la forme de lexiques, de patrons morpho-syntaxiques ou de textes annotés.
Nous traitons ensuite la question de l'apport des différents types de ressources, dont il ressort que la meilleure stratégie est de les utiliser de concert.
Enfin, nous proposons des méthodes d'acquisition pour chacune des ressources répondant non seulement aux besoins de la fouille d'opinion mais également aux contraintes du contexte industriel au sein duquel ces recherches sont menées.
La thèse vise à explorer des modèles et algorithmes d'extraction de connaissance et d'interconnexion de bases de données hétérogènes, appliquée à la gestion de contenus tels que rencontrés fréquemment dans le quotidien des journalistes.
Le travail se déroulera dans le cadre du projet ANR ContentCheck (2016-2019) qui fournit le financement et dans le cadre duquel nous collaborons aussi avec l'équipe "Les Décodeurs" (journalistes spécialisés dans le fact-checking) du journal Le Monde.
La démarche scientifique de la thèse se décompose comme suit :
1. Identifier les technologies et domaines de gestion de contenu (texte, données, connaissances) intervenant de façon recurrente (ou dont le besoin est ressenti comme important) dans l'activité des journalistes.
Il est par exemple déjà clair que ceux-ci ont l'habitude d'utiliser "en interne" quelques bases de données construites par les journalistes eux-mêmes ;
Parmi ces problèmes, identifier ceux pour lesquels des solutions techniques (informatiques) sont connues, et le cas échéant mis en oeuvre dans des systèmes existants.
2. S'attaquer aux problèmes ouverts (sur le plan de la recherche), pour lesquels des réponses satisfaisantes manquent, liés à la modélisation et à l'algorithmique efficace pour des contenus textuels, sémantiques, et des données, dans un contexte journalistique.
Cette thèse traite dela mise en correspondance de séquences appliquée au word spotting (localisation de motsclés dans des images de documents sans en interpréter le contenu).
De nombreux algorithmes existent mais très peu d'entre eux ont été évalués dans ce contexte.
Nous commençons donc par une étude comparative de ces méthodes sur plusieurs bases d'images de documents historiques.
Nous proposons ensuite un nouvel algorithme réunissant la plupart des possibilités offertes séparément dans les autres algorithmes.
Ainsi, le FSM (Flexible Sequence Matching) permet de réaliser des correspondances multiples sans considérer des éléments bruités dans la séquence cible, qu'ils se situent au début, à la fin ou bien au coeur de la correspondance.
Nous étendons ensuite ces possibilités à la séquence requête en définissant un nouvel algorithme (ESC : Examplary Sequence Cardinality).
Finalement, nous proposons une méthode d'appariement alternative utilisant une mise en correspondance inexacte de chaines de codes (shape code) décrivant les mots.
Le travail présenté dans cette thèse explore les méthodes pratiques utilisées pour faciliter l'entraînement et améliorer les performances des modèles de langues munis de très grands vocabulaires.
La principale limite à l'utilisation des modèles de langue neuronaux est leur coût computationnel : il dépend de la taille du vocabulaire avec laquelle il grandit linéairement.
La façon la plus aisée de réduire le temps de calcul de ces modèles reste de limiter la taille du vocabulaire, ce qui est loin d'être satisfaisant pour de nombreuses tâches.
La plupart des méthodes existantes pour l'entraînement de ces modèles à grand vocabulaire évitent le calcul de la fonction de partition, qui est utilisée pour forcer la distribution de sortie du modèle à être normalisée en une distribution de probabilités.
Ici, nous nous concentrons sur les méthodes à base d'échantillonnage, dont le sampling par importance et l'estimation contrastive bruitée.
Ces méthodes permettent de calculer facilement une approximation de cette fonction de partition.
L'examen des mécanismes de l'estimation contrastive bruitée nous permet de proposer des solutions qui vont considérablement faciliter l'entraînement, ce que nous montrons expérimentalement.
Enfin, nous exploitons les informations données par les unités sous-mots pour enrichir les représentations en sortie du modèle.
De nombreuses villes d'Asie du Sud-Est subissent de graves inondations liées d'une part à l'intensité croissante des précipitations et d'autre part à une urbanisation rapide souvent due à une planification urbaine non maitrisée.
L'évaluation quantitative des risques d'inondation nécessite deux éléments essentiels : (1) un modèle numérique de terrain (MNT) haute définition, et (2) une chronologie de précipitations la plus longue possible.
Un MNT haute définition est à la fois coûteux et long à acquérir.
Les chronologies de précipitations longues sont fréquemment indisponibles dans de nombreux sites et ne présentent pas toujours une durée suffisante pour une définition pertinente des valeurs extrêmes.
Cette thèse présente une approche opérationnelle pour générer des MNT haute définition et suggère une stratégie pour définir des pluies extrêmes en dehors de chronologies de précipitations longues.
Des données pour la production des MNT issues de capteurs satellitaires-mission SRTM (Shuttle Radar Topography Mission) et images multi spectrales Sentinel 2-ont été utilisées et mises en œuvre.
Un réseau de neurones artificiels (ANN) est utilisé afin d'améliorer la qualité du MNT.
A la suite de cet apprentissage, le réseau de neurones peut être mis en œuvre pour générer, à faible coût, des MNT haute résolution dans des secteurs où les données sont partiellement indisponibles.
Le MNT issu des données SRTM améliorées montre (1) une qualité nettement supérieure au MNT initial puisque le RMSE passe de 34% à 57% du RMSE ; (2) la clarté visuelle est largement améliorée ; et (3) le réseau de drainage calculé correspond davantage au réseau réel.
La production de ce MNT amélioré permet une meilleure modélisation des processus d'inondation et augmente la qualité des résultats des simulations hydrauliques.
Des données de précipitation issues d'un Modèle Climatologique Régional (RCM) haute résolution spatiale ainsi que des prévisions issues de données ERA-Interim (WRF / ERAI) ont été extraites, analysées et comparées avec les observations haute résolution enregistrées à Singapour.
Les comparaisons ont également été effectuées avec les courbes Intensité-Durée-Fréquence (IDF) qui sont utilisées pour l'évaluation des risques d'inondation.
Un modèle hydraulique détaillé a été construit avec le système de modélisation MIKE 21 pour toute la métropole de Jakarta à partir d'un MNT amélioré et des précipitations associées à des périodes de retour de 50 et 100 ans.
Des cartes d'inondation ont été générées et sont utilisées par les services gestionnaires.
Cet exemple démontre que les nouvelles méthodes et approches proposées dans cette thèse sont pertinentes pour produire une évaluation des risques d'inondation pertinente lorsque des données locales (MNT haute résolution et données pluviométriques sur une période longue) sont insuffisantes ou indisponibles.
Dans le traitement des langues, la représentation vectorielle des mots est une question clé, permettant l'emploi d'algorithmes basés sur des modèles mathématiques.
Récemment ont émergé de nouvelles méthodes de vectorisation et leur évaluation est cruciale.
Les évaluations actuelles portent surtout sur l'anglais, d'où le besoin d'évaluations multilingues.
Notre travail porte sur la généralisation des évaluations, leur comparaison, l'élaboration d'évaluations nouvelles, et sur WordNet, ressource multilingue.
Nous avons choisi 6 vectorisations : CBOW, SkipGram, GloVe, une plus ancienne comme base, et deux plus récentes.
Comme méthode indirecte, nous prenons la catégorisation sémantique avec des algorithmes de clustering pour comparer les vectorisations sous-jacentes.
Les algorithmes choisis sont : le plus utilisé (Kmeans), un neuronal (SOM) et un probabiliste (EM).Notre système applique les évaluations sur des corpus en anglais, français et arabe, et compare les vectorisations.
Nous proposons 5 méthodes d'évaluation, dont 4 fondées sur WordNet, et un protocole d'évaluation par sondage.
Nos résultats donnent trois classements des méthodes validés sur ces langues, s'accordant sur plusieurs points décisifs, et invalident certaines des évaluations existantes.
Pour nos propres évaluations, le protocole est validé, et, de nos 5 méthodes, une a été invalidée (nous avons analysé les causes de l'échec), une a été validée pour l'anglais et le français, mais pas pour l'arabe, deux ont été validées sur les trois langues, et une reste à explorer.
En particulier, la démocratisation des dispositifs mobiles (comme les PCs, Smartphones, Tablettes, etc.) a rendu l'information accessible par le grand public partout et à tout moment, ce qui est l'origine du concept d'informatique ubiquitaire.
L'approche classique des systèmes de l'informatique ubiquitaire, qui répondent aux besoins des utilisateurs indépendants les uns des autres, a été bouleversée par l'introduction de la dimension sociale.
Ce rapprochement est à l'origine d'une discipline naissante « le pervasive social computing » ou l'informatique socio-pervasive.
Les applications socio-pervasives connaissent une véritable expansion.
Ces dernières intègrent de plus en plus la notion de communauté.
Elle est, aujourd'hui, au cœur des problématiques de personnalisation et d'adaptation des applications informatiques.
Dans le cadre de cette thèse, nous étudions sous différents aspects les applications informatiques centrées communautés existantes et soulignons un certain nombre de carences au niveau même de la notion de communauté, des modèles de communautés, ou encore des architectures dédiées à ces applications communautaires, etc.
Pour remédier à ces défauts, nous proposons trois principales contributions :
Une architecture dynamiquement reconfigurable pour promouvoir les communautés spontanées en aidant les utilisateurs nomades à intégrer des communautés environnantes et à découvrir les services dédiés.
Nous montrons la faisabilité de nos propositions pour la conception et le développement d'applications communautaires spontanées grâce au prototype Taldea.
Enfin, nous testons les approches proposées de découverte de communauté et de services à travers plusieurs scénarios caractérisés par la mobilité et l'ubiquité.
Le catalogue occupe une place privilégiée dans l'offre de service des bibliothèques universitaires, pivot de l'intermédiation.
Depuis 10 ans, il traverse une crise grave, voyant les usagers le délaisser à la faveur des moteurs de recherche généralistes.
Le web, plus qu'un sérieux concurrent, devance aujourd'hui les systèmes d'information documentaires, et devient le point d'entrée principal pour la recherche d'information.
Les bibliothèques tentent de structurer un espace documentaire qui soit habité par les usagers, au sein duquel se développe l'offre de service, mais celle-ci se présente encore comme une série de silos inertes, sans grande possibilité de navigation, malgré de considérables efforts d'ingénierie et des pistes d'évolution vers les outils de découverte.
La profession, consciente de cette crise profonde, après avoir accusé les remous occasionnés par la dimension disruptive du numérique, cherche des moyens pour adapter et diversifier son offre, fluidifier la diffusion de l'information, et se réinvente un rôle d'intermédiation en cherchant à tirer profit des nouvelles pratiques des usagers, de leurs nouvelles attentes, et de nouvelles perspectives.
Les bibliothèques placent leur espoir dans de nouveaux modèles de données, tentent d'y ajouter un niveau d'abstraction favorisant les liaisons avec l'univers de la connaissance.
L'évolution vers le web sémantique semble une opportunité à saisir pour valoriser les collections et les rendre exploitables dans un autre contexte, au prix d'importants efforts que cette analyse tente de mesurer.
Une approche constructiviste fondée sur l'observation participante et le recueil de données offre une vision issue de l'intérieur de la communauté des bibliothèques sur l'évolution des catalogues et des outils d'intermédiation, et ouvre des perspectives sur leurs enjeux.
Capturer automatiquement des équivalences sémantiques entre des unités de texte est une tâche complexe mais qui s'avère indispensable dans de nombreux contextes.
Dans cette thèse, nous proposons une étude détaillée de la tâche d'acquisition de paraphrases sous-phrastiques à partir de paires d'énoncés sémantiquement liés.
Nous démontrons empiriquement que les corpus parallèles monolingues, bien qu'extrêmement rares, constituent le type de ressource le plus adapté pour ce genre d'étude.
Nos expériences mettent en jeu cinq techniques d'acquisition, représentatives de différentes approches et connaissances, en anglais et en français.
Un résultat important de notre étude est l'identification de paraphrases qui défient actuellement les techniques étudiées, lesquelles sont classées et quantifiées en anglais et français.
Nous examinons également dans cette thèse l'impact de la langue, du type du corpus et la comparabilité des paires des énoncés utilisés sur la tâche d'acquisition de paraphrases sous-phrastiques.
Nous présentons le résultat d'une analyse de la performance des différentes méthodes testées en fonction des difficultés d'alignement des paires de paraphrases d'énoncés.
Nous donnons, ensuite, un compte rendu descriptif et quantitatif des caractéristiques des paraphrases trouvées dans les différents types de corpus étudiés ainsi que celles qui défient les approches actuelles d'identification automatique.
La fraude par carte de crédit est devenue un problème majeur dans le secteur des paiements électroniques.
Dans cette thèse, nous étudions la détection de fraude basée sur les données transactionnelles et abordons plusieurs de ces défis complexes en utilisant des méthodes d'apprentissage automatique visant à identifier les transactions frauduleuses qui ont été émises illégitimement au nom du titulaire légitime de la carte.
En particulier, nous explorons plusieurs moyens d'exploiter les informations contextuelles au-delà des attributs de base d'une transaction, notamment au niveau de la transaction, au niveau de la séquence et au niveau de l'utilisateur.
Au niveau des transactions, nous cherchons à identifier les transactions frauduleuses qui présentent des caractéristiques distinctes des transactions authentiques.
Nous avons mené une étude empirique de l'influence du déséquilibre des classes et des horizons de prévision sur la performance d d'un classifieur de type random forest.
Nous augmentons les transactions avec des attributs supplémentaires extraits de sources de connaissances externes et montrons que des informations sur les pays et les événements du calendrier améliorent les performances de classification, particulièrement pour les transactions ayant lieu sur le Web.
Au niveau de la séquence, nous cherchons à détecter les fraudes qui sont difficiles à identifier en elles-mêmes, mais particulières en ce qui concerne la séquence à court terme dans laquelle elles apparaissent.
Nous utilisons un réseau de neurone récurrent (LSTM) pour modéliser la séquence de transactions.
Nos résultats suggèrent que la modélisation basée sur des LSTM est une stratégie prometteuse pour caractériser des séquences de transactions ayant lieu en face à face, mais elle n'est pas adéquate pour les transactions ayant lieu sur le Web.
Au niveau de l'utilisateur, nous travaillons sur une stratégie existante d'agrégation d'attributs et proposons un concept flexible nous permettant de calculer de nombreux attributs au moyen d'une syntaxe simple.
Nous fournissons une implémentation basée sur CUDA pour pour accélerer le temps de calcul de deux ordres de grandeur.
Notre étude de sélection des attributs révèle que les agrégats extraits de séquences de transactions des utilisateurs sont plus utiles que ceux extraits des séquences de marchands.
En ce qui concerne les travaux futurs, nous évoquons des méthodes d'apprentissage artificiel simples et transparentes pour la détection des fraudes par carte de crédit et nous esquissons une modélisation simple
Les études diachroniques nous permettent d'étudier les changements, d'analyser les régularités dans le changement, afin de mieux connaître la faculté de langage.
Les études sont des études qualitatives avec une méthode descriptive et comparative en utilisant l'analyse de corpus des listes lexicales anciennes.
La formalisation nous permet d'automatiser et de simplifier les règles de la langue sans transgresser le système grammatical de la langue pour les appliquer dans le domaine de la traduction automatique.
En passant par les études de la morphonologie et de la morphologie, nous pouvons voir clairement la structure et le mécanisme de la grammaire malaise pour chercher une solution face au problème de la combinatoire affixale du lexique malais de façon systématique.
L'étude d'un tel sujet a été réalisée pour comprendre clairement les structures internes de l'allomorphe, de la dérivation verbale et nominale à partir de radicaux verbaux du malais qui nous aident à formuler un modèle de traitement automatique du langage ou un outil pédagogique par le schéma et des règles.
Nous avons étudié la corrélation entre les verbes et les affixes pour reconnaître, analyser et interpréter les données du corpus pour voir la relation du verbe avec le système de dérivation malais.
Le résultat de la statistique du corpus nous a montré la fiabilité de notre modèle parce qu'il existe la cohérence entre les données et le modèle fabriqué
Ce sujet de recherche est centré sur la problématique de gestion des connaissances et des savoir-faire au sein d'une l'organisation humanitaire intervenant en situations d'urgence.
Plus précisément, il s'agira d'expliquer pourquoi et comment les décisions ont été prises en situation d'urgence.
Il s'agit donc ici de développer une méthodologie adaptée pour répondre aux problématiques spécifiques des organisations humanitaires.
La méthodologie proposée sera testée et validée avec une ou plusieurs études de cas et permettra la formalisation de l'expérience, essentielle pour la rendre exploitable.
La plateforme LegalCluster propose de définir des « clusters » juridiques dans une approche écosystémique, en vue de garantir la pérennité des informations produites par les acteurs juridiques.
Dans ce contexte, LegalCluster cherche à proposer un service de recommandations juridiques pour les cabinets d'expert, les directions et les clients des fonctions juridiques, localement, dans un cluster donné ou à plus large échelle.
De fait, plusieurs contraintes majeures sont à relever pour permettre de délivrer un service pertinent et s'intégrant pleinement dans le milieu juridique : le secret professionnel, contexte juridique de l'utilisateur et la prise en compte de la jurisprudence.
Il est alors nécessaire de proposer une solution générale pouvant être appliquée dans un cluster local, associé à des ressources et connaissances externes.
Pour se faire, cette approche repose sur une combinaison entre le traitement naturel de la langue (TALN) et la recherche d'information (RI).
Le traitement de la langue, reposant sur des modèles d'apprentissage, a pour but de catégoriser les questions juridiques.
Du fait d'avoir des clusters locaux et différents codes juridiques à associer, notre approche s'intègre pleinement dans le cadre du Topic Modeling et de la génération de résumés abstractifs.
Afin d'intégrer à la fois l'aspect métier et l'aspect protection du secret professionnel, le modèle devra être entrainer sur un corpus global, puis localement, ce qui implique la caractérisation des modèles dans ces deux composantes tout en reposant sur des données implicites comme les textes de lois et la jurisprudence.
La partie Recherche d'Information a pour but de faciliter l'analyse des corpus juridiques tout en donnant de nouvelles fonctionnalités en vue de définir un moteur de recherche juridique dédié.
Ainsi, l'indexation classique permet de faire la correspondance entre des mots-clés et les documents les plus pertinents associés, mais également sur les métadonnées du métier et permettra de produire une vision synthétique « locale » des données métiers.
Une première étape d'intégration est de faire évoluer le modèle de structure de données pour l'adapter aux métadonnées, mais également modifier le cœur du moteur de recherche pour lui permettre de fournir des fonctions de scoring adaptés aux besoins de LegalCluster.
Le moteur de recherche facilitera l'analyse et l'association de ces textes nécessaire à la brique Apprentissage.
Modéliser les préférences des utilisateurs est incontournable dans de nombreux problèmes de la vie courante, que ce soit pour la prise de décision individuelle ou collective ou le raisonnement stratégique par exemple.
Comme les agents ne connaissent pas complètement leurs préférences à l'avance, nous avons seulement deux moyens de les déterminer pour pouvoir raisonner ensuite : nous pouvons les inférer soit de ce que les agents disent, soit de leurs actions non-linguistiques.
Dans ce travail, nous proposons une nouvelle approche pour extraire et raisonner sur les préférences exprimées dans des dialogues de négociation.
Après avoir extrait les préférences de chaque tour de dialogue, nous utilisons la structure discursive pour suivre leur évolution au fur et à mesure de la conversation.
Nous utilisons les CP-nets, un modèle de représentation des préférences, pour formaliser et raisonner sur ces préférences extraites.
Cette méthode est d'abord évaluée sur différents corpus de négociation pour lesquels les résultats montrent que la méthode est prometteuse.
Nous l'appliquons ensuite dans sa globalité avec des raisonnements issus de la Théorie des Jeux pour prédire les échanges effectués, ou non, dans le jeu de marchandage
Cette thèse présente donc une nouvelle approche à la croisée de plusieurs domaines : le Traitement Automatique des Langues (pour l'extraction automatique des préférences et le raisonnement sur leur verbalisation), l'Intelligence Artificielle (pour la modélisation et le raisonnement sur les préférences extraites) et la Théorie des Jeux (pour la prédiction des actions stratégiques dans un jeu de marchandage)
En français, plus de 8 schémas morphologiques sont disponibles pour construire des noms d'événements à partir de verbes.
Les suffixations en-age,-ment,-ion,-ure,-ance,-ade,-aison,-erie, ainsi que la conversion de verbe à nom (défendre-&gt; défense, arriver-&gt; arrivée), partagent toutes des fonctions équivalentes sur le plan sémantique, et sélectionnent parfois les mêmes bases verbales (e.g. : gouverner-&gt; gouvernance, gouvernement, gouverne).
S'inscrivant dans le cadre théorique de la morphologie lexématique (Matthews 1974 ; Anderson 1992 ; Aronoff 1994 ; Fradin 2003 ; Booij 2005), et à la suite des travaux de Lindsay and Aronoff (2013) et Aronoff (2014, 2015, 2017, 2019), cette thèse propose d'élaborer une réflexion sur les dynamiques à l'origine de la coexistence de ces multiples schémas de nominalisation rivaux.
En prenant appui sur une base de relations morphologiques construite à partir de corpus massifs, nous faisons l'utilisation de méthodes computationnelles (word embeddings, modèles statistiques, modèles analogiques ; Arndt-Lappe 2014, Lapraye 2017, Wauquier et al. 2018, Bonami and Thuilier 2019 pour d'autres cas de compétition morphologique) de sorte à faire émerger ces contraintes, et proposons de modéliser quantitativement la répartition du lexique construit par ces schémas.
Il s'agit de minimiser l'écart sémantique existant entre le document et le domaine considérés.
La méthode s'appuie sur une collection d'enrichissement constituée automatiquement en lien avec le domaine d'intérêt et procède par extraction de mots-clés et détection de thèmes (topics).
La méthode d'enrichissement proposé a été appliquée à des pages web.
Elle est robuste au bruit indépendant du domaine considéré et facile transporter dans différentes langues.
Elle est pauvre en connaissances mais elle exploite les résultats de moteurs de recherche de manière optimisée.
L'approche a été testée sur différentes langues.
L'évaluation a été conduite sur le français et sur 10 domaines différents.
Les résultats ont été évalués par des utilisateurs dans un contexte applicatif réel et par comparaison avec des approches de références.
On observe une bonne précision des résultats et une bonne cohérence sémantique au sein de chaque thème, avec une amélioration significative par rapport aux méthodes d'extraction des mots-clé et de détection de thèmes de l'état de l'art.
Cette thèse s'inscrit dans le cadre d'un travail descriptif en phonétique acoustique, avec comme objet d'étude les productions des voyelles du luxembourgeois dans la parole native et non native.
L'intérêt est de concilier la variation du luxembourgeois, une langue principalement parlée, composée de nombreuses variétés régionales, évoluant dans un contexte multilingue, et son apprentissage dans le cadre de l'enseignement des langues étrangères au Grand-Duché de Luxembourg.
Comme nous partons du fait que l'apprentissage d'une langue implique la connaissance des traits contrastifs des sons, nous nous intéressons aux productions de locuteurs dont la langue maternelle possède des traits différents de ceux du luxembourgeois, comme le français, afin de voir si ces traits sont reproduits dans la parole non native.
Les productions vocaliques de locuteurs francophones sont étudiées en comparaison aux productions de locuteurs natifs de la région située autour de la capitale du Grand-Duché de Luxembourg, dont la variété sert de référence à l'enseignement du luxembourgeois en tant que langue étrangère.
Le but de l'analyse est : -d'étendre les descriptions sur les propriétés acoustiques des voyelles produites dans une variété régionale du Grand-Duché de Luxembourg,-de relever les difficultés de productions de locuteurs francophones qui apprennent le luxembourgeois,-d'interpréter les résultats dans le cadre de l'enseignement du luxembourgeois en tant que langue étrangère.
Une partie importante du travail empirique a été consacrée à la collecte des données et la création d'un corpus obtenu à travers des enregistrements de 10 locuteurs luxembourgophones et de 10 locuteurs francophones.
Le corpus de compose de 12h30 de parole lue et spontanée, incluant de la parole native et non native du luxembourgeois, ainsi que de la parole native du français.
Ce corpus constitue un premier corpus sur la parole native et non native du luxembourgeois et permet de faire divers analyses comparatives.
Dans notre étude, nous avons fait des analyses acoustiques sur les données de la parole lue.
La méthodologie utilisée a permis d'effectuer des comparaisons entre les données de la parole native et non native du luxembourgeois ainsi qu'entre les données de la L1 et la L2 des francophones.
Les résultats ont apporté des informations tant sur les productions natives que sur les productions non natives des voyelles.
Ces résultats, ainsi que les descriptions approfondies sur les voyelles dans la parole native, enrichissent non seulement les connaissances sur le luxembourgeois, mais aussi sur la variété servant de référence au luxembourgeois en tant que langue étrangère.
En outre, ils ouvrent des perspectives d'étude sur le luxembourgeois en problématisant l'instauration de règles pour ce type d'enseignement, malgré l'absence d'un enseignement suivi de la langue dans les écoles et l'évolution des variétés régionales sur un territoire géographique concentré.
Nous montrons également que les CNNs prédisent clairement la distribution des taux d'erreurs sur une collection d'enregistrements, contrairement à l'approche état de l'art qui génère une distribution éloignée de la réalité.
Ensuite, nous analysons des facteurs impactant les deux approches de prédiction.
Nous évaluons également l'impact de la quantité d'apprentissage des systèmes de prédiction ainsi que la robustesse des systèmes appris avec les sorties d'un système de RAP particulier et utilisés pour prédire la performance sur une nouvelle collection de données.
Nos résultats expérimentaux montrent que les deux approches de prédiction sont robustes et que la tâche de prédiction est plus difficile sur des tours de parole courts ainsi que sur les tours de parole ayant un style de parole spontané.
Enfin, nous essayons de comprendre quelles informations sont capturées par notre modèle neuronal et leurs liens avec différents facteurs.
Nos expériences montrent que les représentations intermédiaires dans le réseau encodent implicitement des informations sur le style de la parole, l'accent du locuteur ainsi que le type d'émission.
Pour tirer profit de cette analyse, nous proposons un système multi-tâche qui se montre légèrement plus efficace sur la tâche de prédiction de performance.
Le Web d'aujourd'hui est formé, entre autres, de deux types de contenus que sont les données structurées et liées du Web sémantique et les contributions subjectives des utilisateurs du Web social.
L'approche ViewpointS a été conçue comme un formalisme creuset apte à intégrer ces deux types de contenus, en préservant la subjectivité des interactions du Web Social.
ViewpointS est une approche de représentation subjective des connaissances.
L'approche propose aussi un second degré de subjectivité.
En effet, viewpoints peuvent être interprétés différemment selon l'utilisateur grâce au mécanisme de perspective.
Les ressources du Web sont représentées et liées par les viewpoints dans le Graphe de Connaissances.
A partir du Graphe de Connaissances contenant les viewpoints et les ressources du Web une Carte de Connaissances composée de synapses et de ressources est créée qui est le fruit de l'interprétation et de l'agrégation des viewpoints.
Chaque viewpoint contribue à la création, au renforcement ou à l'affaiblissement d'une synapse qui relie deux ressources.
L'échange de viewpoints est le processus de sélection qui permet l'évolution des synapses d'une manière analogue à celles qui évoluent dans le cerveau au fil d'un sélectionnisme neuronal.
Nous investiguons dans cette étude l'impact que peut avoir la représentation subjective des connaissances dans divers scénarii de construction collective des connaissances.
Nous définissons un cadre d'expression de contraintes et proposons deux approches : l'une naïve et l'autre de réécriture, permettant de filtrer dynamiquement les réponses valides obtenues à partir des sources éventuellement non-valides, ceci au moment de la requête et non pas en cherchant à les valider dans les sources des données.
Ces deux approches ont été évaluées et ont montré la praticabilité de notre système.
Ceci est notre principale contribution : nous étendons l'ensemble de systèmes de réécriture déjà connus(Chase, C&amp;BC, PerfectRef, Xrewrite, etc.) avec une nouvelle solution efficace pour ce nouveau défi qu'est le filtrage des résultats en fonction d'un contexte utilisateur.
Nous généralisons également les conditions de déclenchement de contraintes par rapport aux solutions existantes, en utilisant la notion de one-way MGU.
Les données de type tabulaire contiennent souvent des variables catégorielles, considérées comme des entrées non numériques avec un nombre fixe et limité d'éléments uniques, appelés catégories.
De nombreux algorithmes d'apprentissage statistique nécessitent une représentation numérique des variables catégorielles.
Pour cela, plusieurs stratégies existent, dont la plus courante est celle de l'encodage one-hot, qui fonctionne bien dans le cadre de l'analyse statistique classique (en termes de puissance de prédiction et d'interprétation) lorsque le nombre de catégories reste faible.
Cependant, les données catégorielles non-uniformisées présentent le risque d'avoir une grande cardinalité et des redondances.
En effet, les entrées peuvent partager des informations sémantiques et/ou morphologiques, et par conséquent, plusieurs entrées peuvent refléter la même entité.
Sans une étape de nettoyage ou d'agrégation au préalable, les méthodes d'encodage courantes peuvent perdre en efficacité du fait d'une représentation vectorielle erronée.
Même avec des données volumineuses, ces méthodes s'avèrent être performantes, et dans certains cas, elles génèrent des vecteurs facilement interprétables.
Par conséquent, nos méthodes peuvent être appliquées à l'apprentissage statistique automatique (AutoML) sans aucune intervention humaine.
Dans cette thèse, nous nous intéressons au problème spécifique de l'apprentissage de structure de modèles graphiques probabilistes, c'est-à-dire trouver la structure la plus efficace pour représenter une distribution, à partir seulement d'un ensemble d'échantillons D ∼ p(v).
Dans une première partie, nous passons en revue les principaux modèles graphiques probabilistes de la littérature, des plus classiques (modèles dirigés, non-dirigés) aux plus avancés (modèles mixtes, cycliques etc.).
Puis nous étudions particulièrement le problème d'apprentissage de structure de modèles dirigés (réseaux Bayésiens), et proposons une nouvelle méthode hybride pour l'apprentissage de structure, H2PC (Hybrid Hybrid Parents and Children), mêlant une approche à base de contraintes (tests statistiques d'indépendance) et une approche à base de score (probabilité postérieure de la structure).
Dans un second temps, nous étudions le problème de la classification multi-label, visant à prédire un ensemble de catégories (vecteur binaire y P (0, 1)m) pour un objet (vecteur x P Rd).
Dans ce contexte, l'utilisation de modèles graphiques probabilistes pour représenter la distribution conditionnelle des catégories prend tout son sens, particulièrement dans le but minimiser une fonction coût complexe.
Nous passons en revue les principales approches utilisant un modèle graphique probabiliste pour la classification multi-label (Probabilistic Classifier Chain, Conditional Dependency Network, Bayesian Network Classifier, Conditional Random Field, Sum-Product Network), puis nous proposons une approche générique visant à identifier une factorisation de p(y|x) en distributions marginales disjointes, en s'inspirant des méthodes d'apprentissage de structure à base de contraintes.
Nous démontrons plusieurs résultats théoriques, notamment l'unicité d'une décomposition minimale, ainsi que trois procédures quadratiques sous diverses hypothèses à propos de la distribution jointe p(x, y).
L'utilisation croissante des réseaux sociaux et de capteurs génère une grande quantité de données qui peuvent être représentées sous forme de graphiques complexes.
Il y a de nombreuses tâches allant de l'analyse de l'information à la prédiction et à la récupération que l'on peut imaginer sur ces données où la relation entre les noeuds de graphes devrait être informative.
Tous les modèles proposés utilisent le cadre d'apprentissage de la représentation dans sa variante déterministe ou gaussienne.
Dans un premier temps, nous avons proposé deux algorithmes pour la tâche de marquage de graphe hétérogène, l'un utilisant des représentations déterministes et l'autre des représentations gaussiennes.
Contrairement à d'autres modèles de pointe, notre solution est capable d'apprendre les poids de bord lors de l'apprentissage simultané des représentations et des classificateurs.
Deuxièmement, nous avons proposé un algorithme pour la prévision des séries chronologiques relationnelles où les observations sont non seulement corrélées à l'intérieur de chaque série, mais aussi entre les différentes séries.
Nous utilisons des représentations gaussiennes dans cette contribution.
C'était l'occasion de voir de quelle manière l'utilisation de représentations gaussiennes au lieu de représentations déterministes était profitable.
Enfin, nous appliquons l'approche d'apprentissage de la représentation gaussienne à la tâche de filtrage collaboratif.
Ceci est un travail préliminaire pour voir si les propriétés des représentations gaussiennes trouvées sur les deux tâches précédentes ont également été vérifiées pour le classement.
L'objectif de ce travail était de généraliser ensuite l'approche à des données plus relationnelles et pas seulement des graphes bipartis entre les utilisateurs et les items.
Dans le domaine médical, l'informatisation des professions de santé et le développement du dossier médical personnel (DMP) entraîne une progression rapide du volume d'information médicale numérique.
Le besoin de convertir et de manipuler toute ces informations sous une forme structurée constitue un enjeu majeur.
C'est le point de départ de la mise au point d'outils d'interrogation appropriés pour lesquels, les méthodes issues du traitement automatique du langage naturel (TALN) semblent bien adaptées.
Les travaux de cette thèse s'inscrivent dans le domaine de l'analyse de documents médicaux et traitent de la problématique de la représentation de l'information biomédicale (en particulier du domaine radiologique) et de son accès.
Nous montrons l'intérêt de l'hypothèse de non séparation entre les différents types de connaissances dans le cadre d'une analyse de documents.
Ce réseau combine poids et annotations sur des relations typées entre des termes et des concepts ainsi qu'un mécanisme d'inférence dont l'objet est d'améliorer la qualité et la couverture du réseau.
Nous décrivons comment à partir d'informations sémantiques présentes dans le réseau, il est possible de définir une augmentation des index bruts construits pour chaque comptes rendus afin d'améliorer la recherche documentaire.
Nous présentons, ensuite, une méthode d'extraction de relations sémantiques entre des termes ou concepts.
Cette extraction est réalisée à l'aide de patrons linguistiques auxquels nous avons rajouté des contraintes sémantiques.
Les résultats des évaluations montrent que l'hypothèse de non séparation entre les différents types de connaissances améliorent la pertinence de l'indexation.
L'augmentation d'index permet une amélioration du rappel alors que les contraintes sémantiques améliorent la précision de l'extraction de relations.
L'évolution moléculaire procède par divergence depuis un ancêtre commun et en combinant des fragments d'objets évoluant d'origines différentes, par des processus introgressifs.
Les transferts horizontaux de gènes sont probablement les plus connus de ces processus, mais l'introgression affecte aussi d'autres niveaux d'organisation biologique.
Ainsi, la plupart des objets biologiques évoluant peuvent être composés de parties d'origines phylogénétiques différentes et décrits comme composites.
Cette évolution modulaire se modélise mal par des arbres, puisque les objets composites ne sont pas seulement le résultat d'une divergence depuis un ancêtre.
Les réseaux sont bien plus aptes à modéliser la modularité, et la théorie des graphes peut être utilisée pour chercher dans ces réseaux des patrons caractéristiques d'une évolution réticulée.
Pendant cette thèse, j'ai développé le logiciel CompositeSearch qui détecte les gènes composites dans des jeux de données de séquences massifs, jusqu'à plusieurs millions de séquences.
Cet algorithme a été utilisé pour identifier et quantifier l'abondance des gènes composites dans des environnements de sols pollués ainsi que dans les plasmides.
Les résultats montrent que d'importantes adaptations et nouveautés biologiques découlent de processus œuvrant au niveau subgénique.
De plus, les réseaux fournissent un cadre conceptuel dont l'utilité va bien au-delà de l'évolution moléculaire et je les ai appliqués à d'autres objets évoluant, comme les animaux (réseaux de traits morphologiques) et les langues (réseaux de mots).
Dans les deux cas, la modularité se révèle être une conséquence évolutive majeure, et obéit à des règles encore à préciser.
Cette dernière décennie a donné lieu à la réémergence des méthodes d'apprentissage machine basées sur les réseaux de neurones formels sous le nom d'apprentissage profond.
Bien que ces méthodes aient permis des avancées majeures dans le domaine de l'apprentissage machine, plusieurs obstacles à la possibilité d'industrialiser ces méthodes persistent, notamment la nécessité de collecter et d'étiqueter une très grande quantité de données ainsi que la puissance de calcul nécessaire pour effectuer l'apprentissage et l'inférence avec ce type de réseau neuronal.
Dans cette thèse, nous proposons d'étudier l'adéquation entre des algorithmes d'inférence et d'apprentissage issus des réseaux de neurones biologiques pour des architectures matérielles massivement parallèles.
Nous montrons avec trois contributions que de telles adéquations permettent d'accélérer drastiquement les temps de calculs inhérents au réseaux de neurones.
Nous proposons également l'introduction d'une architecture hiérarchique basée sur des cellules complexes.
Nous montrons que l'adéquation pour GPU accélère les traitements par un facteur sept, tandis que l'architecture hiérarchique atteint un facteur mille.
La deuxième contribution présente trois algorithmes de propagation de décharges neuronales adaptés aux architectures parallèles.
Nous réalisons une étude complète des modèles computationels de ces algorithmes, permettant de sélectionner ou de concevoir un système matériel adapté aux paramètres du réseau souhaité.
Dans notre troisième axe nous présentons une méthode pour appliquer la règle Spike-Timing-Dependent-Plasticity à des données images afin d'apprendre de manière non-supervisée des représentations visuelles.
Nous montrons que notre approche permet l'apprentissage d'une hiérarchie de représentations pertinente pour des problématiques de classification d'images, tout en nécessitant dix fois moins de données que les autres approches de la littérature.
L'utilisation de commandes gestuelles est une nouvelle méthode d'interaction sur interface tactile.
Une bonne méthode pour faciliter la mémorisation de ces commandes gestuelles est de laisser l'utilisateur les personnaliser.
Ce contexte applicatif induit une situation d'apprentissage croisé, où l'utilisateur doit mémoriser le jeu de symboles elle système doit apprendre à reconnaître les différents symboles.
Cela implique un certain nombre de contraintes, à la fois sur le système de reconnaissance de symboles ct sur le système de supervision de son apprentissage.
Il faut par exemple que le classifieur puisse apprendre à partir de peu de données, continuer à apprendre pendant son utilisation et suivre toute évolution des données indéfiniment.
Le superviseur doit quant à lui optimiser la coopération entre l'utilisateur et le système de reconnaissance pour minimiser les interactions tout en maximisant l'apprentissage.
Cette thèse présente d'une part, le système d'apprentissage évolutif Evolve oo, capable d'apprendre rapidement il partir de peu de données et de suivre les changements de concepts.
L'intégration d'oubli dans le processus d'apprentissage permet de maintenir le gain de l'apprentissage indéfiniment, permettant ainsi l'ajout de classes à n'importe quel moment de l'utilisation du système ct garantissant son évolutivité « à vie » .
Le superviseur actif en-ligne lntuiSup permet d'optimiser les interactions avec l'utilisateur pour entraîner un système d'apprentissage lorsque l'utilisateur est dans la boucle.
Il permet de faire évoluer la proportion de données que l'utilisateur doit étiqueter en fonction de la difficulté du problème et de l'évolution de l'environnement (changements de concepts).
L'utilisation d'une méthode de « dopage » de l'apprentissage permet d'optimiser la répartition de ces interactions avec l'utilisateur pour maximiser leur impact sur l'apprentissage.
Le travail présenté ici est pour une première partie à l'intersection de l'apprentissage profond et anonymisation.
Un cadre de travail complet est développé dans le but d'identifier et de retirer, dans une certaine mesure et de manière automatique, les caractéristiques privées d'une identité pour des données de type image.
Deux méthodes différentes de traitement des données sont étudiées.
Ces deux méthodes partagent une même architecture de réseau en forme de Y et cela malgré des différences concernant les types de couches de neurones utilisés conséquemment à leur objectif d'utilisation.
La première méthode de traitement des données concerne la création ex nihilo de représentations anonymisées permettant un compromis entre la conservation des caractéristiques pertinentes et l'altération des caractéristiques privées.
Ce cadre de travail a abouti à une nouvelle fonction de perte.
Par conséquent les représentations anonymisées sont de même nature que les données initiales (une image est transformée en une image anonymisée).
Cette tâche a conduit à un autre type d'architecture (toujours en forme de Y) et a fourni des résultats fortement sensibles au type des données.
La seconde partie de mon travail concerne une autre sorte d'information utile : cette partie se concentre sur la surveillance du comportement des prédicteurs.
Dans le cadre de l'analyse de "modèle boîte noire", on a uniquement accès aux probabilités que le prédicteur fournit (sans aucune connaissance du type de structure/architecture qui produit ces probabilités).
L'étude de ces probabilités peut servir d'indicateur d'inadéquation potentiel entre les statistiques des données et les statistiques du modèle.
Deux méthodes utilisant différents outils sont présentées.
La première compare la fonction de répartition des statistiques de sortie d'un ensemble connu et d'un ensemble de données à tester.
La seconde fait intervenir deux outils : un outil reposant sur l'incertitude du classifieur et un autre outil reposant sur la matrice de confusion.
Ces méthodes produisent des résultats concluants.
L'évolution constante des besoins des clients et des utilisateurs exige une réponse rapide de la part des équipes logicielles.
Cela crée une forte demande pour un fonctionnement sans rupture des processus logiciels.
L'intégration, la livraison et le déploiement continus, également connus sous le nom de DevOps, ont fait d'énormes progrès en rendant les processus logiciels réactifs au changement.
Aujourd'hui, la plupart des besoins sont exprimés en langage naturel.
Cette approche a un grand pouvoir expressif, mais au détriment d'autres aspects de la qualité des exigences telles que la traçabilité, la réutilisabilité, la vérifiabilité et la compréhensibilité.
Le défi est ici d'améliorer ces aspects sans sacrifier l'expressivité.
Cette approche a motive et inspire les travaux de la présente thèse.
Alors que l'approche multiexigences se concentre sur la traçabilité et la compréhensibilité, l'approche Seamless Object-Oriented Requirements (SOOR) présentée dans cette thèse prend en compte la vérifiabilité, la réutilisabilité et la compréhensibilité.
Cette thèse explore l'hypothèse de Martin Glinz selon laquelle, pour soutenir la continuité, les exigences logicielles devraient être des objets.
L'exploration confirme l'hypothèse et aboutit à un ensemble de méthodes basées sur des outils pour spécifier, valider, vérifier et réutiliser les exigences orientées objets.
La contribution technique réutilisable la plus importante de cette thèse est une bibliothèque Eiffel prête à l'emploi de patrons de classes, qui capturent les modèles d'exigences logicielles récurrents.
Les exigences orientées objets, concrètes et sans rupture, héritent de ces patrons et deviennent des clients du logiciel spécifié.
Cette thèse s'appuie sur plusieurs expériences et montre que la nouvelle approche propose favorise la vérifiabilité, la réutilisabilité et la compréhensibilité des exigences tout en maintenant l'expressivité à un niveau acceptable.
Les expérimentations mettent en oeuvre plusieurs exemples, dont certains sont des standards de l'état de l'art de l'ingénierie des exigences.
Chaque expérimentation illustre un problème par un exemple, propose une solution générale et montre comment la solution règle le problème.
Alors que l'expérimentation s'appuie sur Eiffel et son support d'outils avancés, tels que la preuve et les tests automatisés, chaque idée présentée dans l'approche SOOR s'adapte conceptuellement à tout langage de programmation orienté objet typé statiquement, possédant un mécanisme de généricité et un support élémentaire pour les contrats.
Dans cette thèse, nous proposons d'utiliser des techniques fondées sur l'analyse factorielle pour la modélisation acoustique pour le traitement automatique de la parole, notamment pour la Reconnaissance Automatique de la parole.
Nous nous sommes, dans un premier temps, intéressés à la réduction de l'empreinte mémoire des modèles acoustiques.
Notre méthode à base d'analyse factorielle a démontré une capacité de mutualisation des paramètres des modèles acoustiques, tout en maintenant des performances similaires à celles des modèles de base.
Nous proposons, comme alternative, une représentation vectorielle des états : les fac- teur d'états.
Ces facteur d'états nous permettent de mesurer efficacement la similarité entre les états des MMC au moyen d'une distance euclidienne, par exemple.
Grâce à cette représenation vectorielle, nous proposons une méthode simple et efficace pour la construction de modèles acoustiques avec des états partagés.
Cette procédure s'avère encore plus efficace dans le cas de langues peu ou très peu dotées en ressouces et en connaissances linguistiques.
Enfin, nos efforts se sont portés sur la robustesse des systèmes de reconnaissance de la parole face aux variabilités acoustiques, et plus particulièrement celles générées par l'environnement.
Nous nous sommes intéressés, dans nos différentes expérimentations, à la variabilité locuteur, à la variabilité canal et au bruit additif.
Grâce à notre approche s'appuyant sur l'analyse factorielle, nous avons démontré la possibilité de modéliser ces différents types de variabilité acoustique nuisible comme une composante additive dans le domaine cepstral.
Nous soustrayons cette composante des vecteurs cepstraux pour annuler son effet pénalisant pour la reconnaissance de la parole
Les points de repère sont présentés dans les applications de différents domaines tels que le biomédical ou le biologique.
C'est également l'un des types de données qui ont été utilisés dans différentes analyses, par exemple, ils ne sont pas seulement utilisés pour mesurer la forme de l'objet, mais également pour déterminer la similarité entre deux objets.
En biologie, les repères sont utilisés pour analyser les variations inter-organismes. Cependant, l'offre de repères est très lourde et le plus souvent, ils sont fournis manuellement.
Ces dernières années, plusieurs méthodes ont été proposées pour prédire automatiquement les points de repère, mais la dureté existe, car elles se sont concentrées sur des données spécifiques.
Cette thèse porte sur la détermination automatique de points de repère sur des images biologiques, plus spécifiquement sur d'images 2D des coléoptères.
Dans le cadre de nos recherches, nous avons collaboré avec des biologistes pour créer un ensemble de données comprenant les images de 293 coléoptères.
Pour chaque coléoptère de cette donnée, 5 images correspondent à 5 parties prises en compte, par exemple tête, élytre, pronotum, mandibule gauche et droite.
Avec chaque image, un ensemble de points de repère a été proposé manuellement par les biologistes.
La première étape, nous avons apporté une méthode qui a été appliquée sur les ailes de mouche, à appliquer sur notre jeu de données dans le but de tester la pertinence des techniques de traitement d'image sur notre problème.
Deuxièmement, nous avons développé une méthode en plusieurs étapes pour fournir automatiquement les points de repère sur les images.
Ces deux premières étapes ont été effectuées sur les images de la mandibule qui sont considérées comme évidentes pour l'utilisation des méthodes de traitement d'images.
Troisièmement, nous avons continué à considérer d'autres parties complexes restantes de coléoptères.
En conséquence, nous avons utilisé l'aide de Deep Learning.
Nous avons conçu un nouveau modèle de Convolutional Neural Network, nommé EB-Net, pour prédire les points de repère sur les images restantes.
De plus, nous avons proposé une nouvelle procédure pour augmenter le nombre d'images dans notre jeu de données, ce qui est considéré comme notre limite à appliquer Deep Learning.
Enfin, pour améliorer la qualité des coordonnées prédites, nous avons utilisé Transfer Learning, une autre technique de Deep Learning.
Pour ce faire, nous avons formé EB-Net sur les points clés publics du visage.
Ensuite, ils ont été transférés pour affiner les images de coléoptère.
Les résultats obtenus ont été discutés avec les biologistes et ils ont confirmé que la qualité des repéres prédits est suffisamment bonne sur la plane statistique pour remplacer les repères manuels pour la plupart des analyses de morphométrie différentes.
Cette thèse aborde la problématique de l'accès à l'information scientifique et technique véhiculée par de grands ensembles documentaires.
Le modèle résultant permet une immersion documentaire, et ce grâce à trois types de processus complémentaires : des processus endogènes (exploitant le corpus pour analyser le corpus), exogènes (faisant appel à des ressources externes) et anthropogènes (dans lesquels les compétences de l'utilisateur sont considérées comme ressource) sont combinés.
Tous concourent à l'attribution d'une place centrale à l'utilisateur dans le système, en tant qu'agent interprétant de l'information et concepteur de ses connaissances, dès lors qu'il est placé dans un contexte industriel ou spécialisé.
Le présent travail aborde le thème de la complexité sémantique dans le langage naturel, et il propose une hypothèse basée sur certaines caractéristiques des phrases du langage naturel qui déterminent la difficulté pour l'interpretation humaine.
Nous visons à introduire un cadre théorique général de la complexité sémantique de la phrase, dans lequel la difficulté d'élaboration est liée à l'interaction entre deux composants : la Mémoire, qui est responsable du rangement des représentations d'événements extraites par des corpus, et l'Unification, qui est responsable de la combinaison de ces unités dans des structures plus complexes.
Nous proposons que la complexité sémantique depend de la difficulté de construire une représentation sémantique de l'événement ou de la situation exprimée par une phrase, qui peut être récupérée directement de la mémoire sémantique ou construit dynamiquement en satisfaisant les contraintes contenus dans les constructions.
Pour tester nos intuitions, nous avons construit un Distributional Semantic Model pour calculer le coût de composition de l'unification des phrases.
Les tests sur des bases de données psycholinguistiques ont révélé que le modèle est capable d'expliquer des phénomènes sémantiques comme la mise à jour context-sensitive des attentes sur les arguments et les métonymies logiques.
Au cours des dernières années, innombrables produits ont été conçus en utilisant des modèles numériques 3D, où les courants logiciels pour la conception et le dessin technique utilisent des modèles CAO (Conception Assistée par Ordinateur).
Ces logiciels sont utilisés dans de nombreux domaines, tels que l'automobile, la marine, l'aérospatiale et plus encore.
Par conséquent, il est utile de disposer de solutions technologiques capables d'évaluer les similitudes de différents produits afin que l'utilisateur puisse récupérer des modèles existants et avoir ainsi accès à des informations utiles pour la nouvelle conception.
Le concept de similarité a été largement étudié dans la littérature et il est bien connu que deus objets puissent être similaire de plusieurs façons.
Ces multiples possibilités rendent complexe l'évaluation de la similarité entre deux objets.
À ce jour, de nombreuses méthodes ont été proposées pour l'identification de différentes similitudes entre les pièces, mais peu de travaux abordent cet problème en évoquant d'assemblages de pièces.
Sur la base de ces exigences, nous proposons de définir un système qui permettant la récupération des assemblages des pièces similaires en fonction de multiple critères de similarité.
Pour ce faire, il faut avoir un descripteur qui peut gérer les informations nécessaires pour caractériser les différentes similitudes entre les deux modèles.
Par conséquent, l'un des points principaux de ce travail sera la définition d'un descripteur capable de coder les données nécessaires à l'évaluation des similarités.
De plus, certaines des informations du descripteur peuvent être disponibles dans le modèle CAO, tandis que d'autres devront être extraites de manière appropriée.
Par conséquent, des algorithmes seront proposés pour extraire les informations nécessaires pour remplir les champs du descripteur.
Enfin, pour une évaluation de la similarité, plusieurs mesures entre les modèles seront définies, de sorte que chacune d'entre elles évaluent un aspect particulier de leur similarité.
La traduction automatique (TA) a connu des progrès significatifs ces dernières années et continue de s'améliorer.
La TA est utilisée aujourd'hui avec succès dans de nombreux contextes, y compris les environnements professionnels de traduction et les scénarios de production.
Cependant, le processus de traduction requiert souvent des connaissances plus larges qu'extraites de corpus parallèles.
Étant donné qu'une injection de connaissances humaines dans la TA est nécessaire, l'un des moyens possibles d'améliorer TA est d'assurer une collaboration optimisée entre l'humain et la machine.
À cette fin, de nombreuses questions sont posées pour la recherche en TA : Comment détecter les passages où une aide humaine devrait être proposée ?
Comment faire pour que les machines exploitent les connaissances humaines obtenues afin d'améliorer leurs sorties ?
Enfin, comment optimiser l'échange : minimiser l'effort humain impliqué et maximiser la qualité de TA ?
Diverses solutions sont possibles selon les scénarios de traductions considérés.
Dans cette thèse, nous avons choisi de nous concentrer sur la pré-édition, une intervention humaine en TA qui a lieu ex-ante, par opposition à la post-édition, où l'intervention humaine qui déroule ex-post.
En particulier, nous étudions des scénarios de pré-édition ciblés où l'humain doit fournir des traductions pour des segments sources difficiles à traduire et choisis avec soin.
Les scénarios de la pré-édition impliquant la pré-traduction restent étonnamment peu étudiés dans la communauté.
De plus, dans un contexte multilingue, des difficultés communes peuvent être résolues simultanément pour de nombreuses langues.
De tels scénarios s'adaptent donc parfaitement aux contextes de production standard, où l'un des principaux objectifs est de réduire le coût de l'intervention humaine et où les traductions sont généralement effectuées à partir d'une langue vers plusieurs langues à la fois.
Dans ce contexte, nous nous concentrons sur la TA de revues systématiques en médecine.
En considérant cet exemple, nous proposons une méthodologie indépendante du système pour la détection des difficultés de traduction.
Nous définissons la notion de difficulté de traduction de la manière suivante : les segments difficiles à traduire sont des segments pour lesquels un système de TA fait des prédictions erronées.
Nous formulons le problème comme un problème de classification binaire et montrons que, en utilisant cette méthodologie, les difficultés peuvent être détectées de manière fiable sans avoir accès à des informations spécifiques au système.
Nous intégrons les résultats de notre procédure de détection des difficultés dans un protocole de pré-édition qui permet de résoudre ces difficultés par pré-traduction.
Nous évaluons le protocole dans un cadre simulé et montrons que la pré-traduction peut être à la fois utile pour améliorer la qualité de la TA et réaliste en termes d'implication des efforts humains.
En outre, les effets indirects sont significatifs.
Les résultats de ces expériences pilotes confirment les résultats obtenus dans le cadre simulé et ouvrent des perspectives encourageantes pour des tests ultérieures.
Ce travail de recherche se situe dans le champ interdisciplinaire des sciences de l'information et de la communication (SIC) et a pour but d'explorer la question de l'usage du web sémantique en bibliothèques numériques.
Le web oblige les bibliothèques à repenser leurs organisations, leurs activités, leurs pratiques et leurs services, afin de se repositionner en tant qu'instituts de références pour la diffusion des savoirs.
Dans cette thèse, nous souhaitons comprendre les contextes d'usage du web sémantique en bibliothèques numériques françaises.
Il s'agit de s'interroger sur les apports du web sémantique au sein de ces bibliothèques, ainsi que sur les défis et les obstacles qui accompagnent sa mise en place.
Ensuite, nous nous intéressons aux pratiques documentaires et à leurs évolutions suite à l'introduction du web sémantique en bibliothèques numériques.
La problématique s'attache au rôle que peuvent jouer les professionnels de l'information dans la mise en place du web sémantique en bibliothèques numériques.
Après avoir sélectionné 98 bibliothèques numériques suite à une analyse de trois recensements, une enquête s'appuyant sur un questionnaire vise à recueillir des données sur l'usage du web sémantique dans ces bibliothèques.
Ensuite, une deuxième enquête réalisée au moyen d'entretiens permet de mettre en évidence les représentations qu'ont les professionnels de l'information du web sémantique et de son usage en bibliothèque, ainsi que de l'évolution de leurs pratiques professionnelles.
Les résultats montrent que la représentation des connaissances dans le cadre du web sémantique nécessite une intervention humaine permettant de fournir le cadre conceptuel pour déterminer les liens entre les données.
Enfin, les professionnels de l'information peuvent devenir des acteurs du web sémantique, dans le sens où leurs rôles ne se limitent pas à l'utilisation du web sémantique mais aussi au développement de ses standards pour assurer une meilleure organisation des connaissances.
Le travail présenté dans cette thèse vise à étudier la coordination entre gestes manuels et parole lors de la production d'énoncés multimodaux.
Les études menées s'intéressent plus particulièrement aux relations temporelles entre les deux modalités.
Cette coordination a été étudiée plus précisément dans le cadre de la désignation qui est réalisable à la fois dans la modalité manuelle (geste de pointage) et dans la modalité parole ( « montrer avec la voix » , en utilisant la focalisation et/ou les démonstratifs par exemple).
Les études présentées ont été menées dans un environnement contrôlé de laboratoire afin d'obtenir des mesures précises et reproductibles en minimisant les facteurs extérieurs de variations intra-et inter-participants.
Les productions des locuteurs peuvent ainsi être comparées entre-elles en se focalisant sur les facteurs d'intérêt toutes choses maintenues le plus possible égales par ailleurs.
Un travail particulier de mise en place des protocoles a néanmoins permis de maintenir une tâche assez naturelle afin de ne pas induire des productions trop artificielles.
Les deux premières études se sont intéressées à la production conjointe de gestes manuels et de parole contenant de la focalisation.
Plusieurs types de gestes ont été comparés (geste de pointage, geste de battement et geste d'appui sur un bouton) lors d'une tâche de désignation.
Il a été montré que la production de focalisation attire le geste manuel quel que soit son type mais que l'attraction est plus « précise » et fine pour le pointage.
Par ailleurs, l'apex du geste de pointage semble être cooccurent à une cible articulatoire plutôt qu'acoustique.
La seconde étude manipule le lien de désignation le geste de pointage et la parole.
Elle montre, en exhibant deux stratégies adoptées par les participants, la complexité des mécanismes mis en jeu dans cette coordination.
Finalement, une troisième étude s'intéresse à la coordination dans une tâche interactive et collaborative plus naturelle.
Les résultats montrent une cooccurrence de la partie du geste qui montre avec l'information qui lui est complémentaire en parole, i.e. avec le nom de l'objet à poser à l'endroit désigné par le geste de pointage, plutôt qu'avec la partie de la parole qui désigne, i.e. le démonstratif.
Ce mémoire propose par ailleurs une exploration des procédés d'annotation multimodaux mis en place pour l'annotation de tâches semi-contrôlées mais applicables à des cas plus généraux.
Le manuscrit se conclut par une mise en perspective des résultats pour l'amélioration de certains modèles de production conjointe gestes manuels/parole et fournit quelques pistes utilisables dans le domaine des agents conversationnels ainsi que pour la détection de pathologies.
Le travail est mené dans une perspective contrastive français – russe et ce, sur des corpus informatisés de données issues des deux langues.
Nous avons constitué deux types de corpus : comparable (comportant les textes originaux, 60 M de mots, de la base de Frantext et Ruscorpora) et parallèle (coprus de traduction, 10 M de mots, aligné avec le logiciel Alinea d'O. Kraif).
Les questions qui sous-tendent ce travail concernent les trois points suivants.
Nous vérifions cela grâce à l'analyse de la combinatoire syntaxique et lexicale de ces constructions.
Nous estimons que l'approche contrastive permet de mieux expliciter les similitudes et les différences aspectuelles au sein des CVN dans les deux langues, ainsi que de mettre en évidence les différences dans l'expression de l'aspect en français et en russe.
Cette thèse se centre sur les discordances dans l'ellipse périphérique (RNR) et propose une analyse basée sur l'identité de lexème entre le matériel manquant et le matériel périphérique.
Dans cette thèse, nous contestons cette hypothèse.
Nous avons analysé 5 types de discordance dans l'ellipse périphérique : discordances de polarité, de possessifs, de prépositions, de voix et de formes verbales.
Les discordances sont assez nombreuses même dans des écrits soignés.
Dans tous les cas, les discordances sont résolues par la forme qui correspond au second conjoint.
Les résultats des expériences de jugements d'acceptabilité et de mouvements oculaires permettent d'intégrer les discordances dans la grammaire.
Les résultats sont compatibles avec les analyses qui postulent l'identité sémantique entre le matériel manquant et l'antécédent pour l'ellipse.
Nous proposons une analyse formelle en HPSG.Nous comparons les résultats obtenus avec les cas de coordination lexicale.
Nous montrons que l'accord de proximité s'applique (Villavicencio et al. (2005)) et nous proposons une analyse HPSG pour la coordination de verbes et de prépositions.
Le développement de spécifications formelles correctes pour des systèmes et logiciels commence par l'analyse et la compréhension des besoins du client.
Entre ces besoins décrits en langage naturel et leur spécification définie dans un langage formel précis, un écart existe et rend la tâche de développement de plus en plus difficile à accomplir.
Nous sommes face à deux mondes distincts.
Ce travail de thèse a pour objectif d'expliciter et d'établir des interactions entre ces deux mondes et de les faire évoluer en même temps.
Par interaction, nous désignons les liens, les échanges et les activités se déroulant entre les différents documents.
Parmi ces activités, nous présentons la validation comme un processus rigoureux qui démarre dès l'analyse des besoins et continue tout au long de l'élaboration de leur spécification formelle.
Au fur et à mesure du développement, des choix sont effectués et les retours des outils de vérification et de validation permettent de détecter des lacunes aussi bien dans les besoins que dans la spécification.
L'évolution des deux mondes est décrite via l'introduction d'un nouveau besoin dans un système existant et à travers l'application de patrons de développement.
Ils facilitent la tâche de développement et aident à éviter les risques d'oublis.
Quel que soit le choix, des questions se posent tout au long du développement et permettent de déceler des lacunes, oublis ou ambiguïtés dans l'existant.
Ce travail vise à permettre un accès efficace à des informations pertinentes malgré le volume croissant des données disponibles au format électronique.
Ainsi, nous avons proposé une méthode mixte combinant des techniques de traitement automatique des langues pour extraire des connaissances à partir de textes et la réutilisation de ressources sémantiques existantes pour l'étape de conceptualisation.
Nous avons par ailleurs développé une méthode d'alignement de termes français-anglais pour l'enrichissement terminologique de l'ontologie.
L'application de notre méthodologie a permis de créer une ontologie bilingue de la maladie d'Alzheimer.
Ensuite, nous avons élaboré des algorithmes pour supporter la RI sémantique guidée par une ontologie.
Les concepts issus d'une ontologie ont été utilisés pour décrire automatiquement les documents mais aussi pour reformuler les requêtes.
Nous nous sommes intéressés à : 1) l'identification de concepts représentatifs dans des corpus, 2) leur désambiguïsation, 3), leur pondération selon le modèle vectoriel, adapté aux concepts et 4) l'expansion de requêtes.
Ces propositions ont permis de mettre en œuvre un portail de RI sémantique dédié à la maladie d'Alzheimer.
Par ailleurs, le contenu des documents à indexer n'étant pas toujours accessible dans leur ensemble, nous avons exploité des informations incomplètes pour déterminer les concepts pertinents permettant malgré tout de décrire les documents.
Pour cela, nous avons proposé deux méthodes de classification de documents issus d'un large corpus, l'une basée sur l'algorithme des k plus proches voisins et l'autre sur l'analyse sémantique explicite.
Ces méthodes ont été évaluées sur de larges collections de documents biomédicaux fournies lors d'un challenge international.
Le traitement automatique de documents consiste en la transformation dans un format compréhensible par un système informatique de données présentes au sein de documents et compréhensibles par l'Homme.
L'analyse de document et la compréhension de documents sont les deux phases du processus de traitement automatique de documents.
Étant donnée une image de document constituée de mots, de lignes et d'objets graphiques tels que des logos, l'analyse de documents consiste à extraire et isoler les mots, les lignes et les objets, puis à les regrouper au sein de blocs.
La compréhension de documents fait correspondre à cette structure géométrique une structure logique en considérant des liaisons logiques (à gauche, à droite, au-dessus, en-dessous) entre les objets du document.
Un système de traitement de documents doit être capable de : (i) localiser une information textuelle, (ii) identifier si cette information est pertinente par rapport aux autres informations contenues dans le document, (iii) extraire cette information dans un format compréhensible par un programme informatique.
Pour la réalisation d'un tel système, les difficultés à surmonter sont liées à la variabilité des caractéristiques de documents, telles que le type (facture, formulaire, devis, rapport, etc.), la mise en page (police, style, agencement), la langue, la typographie et la qualité de numérisation du document.
Dans ce mémoire, nous considérons en particulier des documents numérisés, également connus sous le nom d'images de documents.
Plus précisément, nous nous intéressons à la localisation d'informations textuelles au sein d'images de factures.
Les factures sont des documents très utilisés mais non standards.
En effet, elles contiennent des informations obligatoires (le numéro de facture, le numéro siret de l'émetteur, les montants, etc.) qui, selon l'émetteur, peuvent être localisées à des endroits différents.
Les contributions présentées dans ce mémoire s'inscrivent dans le cadre de la localisation et de l'extraction d'informations textuelles fondées sur des régions identifiées au sein d'une image de document.
Tout d'abord, nous présentons une approche de décomposition d'une image de documents en sous-régions fondée sur la décomposition quadtree.
La méthode fondée sur cette approche, que nous proposons, permet de déterminer efficacement les régions contenant une information d'intérêt à extraire.
Dans une autre approche, incrémentale et plus flexible, nous proposons un système d'extraction d'informations textuelles qui consiste en un ensemble de régions prototypes et de chemins pour parcourir ces régions prototypes.
Le cycle de vie de ce système comprend cinq étapes :
- Construction d'un jeu de données synthétiques à partir d'images de factures réelles contenant les informations d'intérêts.
- Partitionnement des données produites.
- Détermination des régions prototypes à partir de la partition obtenue.
- Détermination des chemins pour parcourir les régions prototypes, à partir du treillis de concepts d'un contexte formel convenablement construit.
- Mise à jour du système de manière incrémentale suite à l'insertion de nouvelles données
L'étude des surfaces continentales constitue un enjeu majeur à l'échelle mondiale pour le suivi et la gestion des territoires, notamment en matière de répartition entre l'expansion urbaine, terres agricoles et espaces naturels.
Dans ce contexte, les cartes d'OCcupation des Sols (OCS) caractérisant la couverture biophysique des terres émergées sont un atout essentiel pour l'analyse des surfaces continentales.
Les algorithmes de classification supervisée permettent, à partir de séries temporelles annuelles d'images satellites et de données de référence, de produire automatiquement la carte de la période correspondante.
Cependant, les données de référence sont une information coûteuse à obtenir surtout sur de grandes étendues.
En effet, les campagnes de relevés terrain requièrent un fort coût humain, et les bases de données sont associées à de longs délais de mises à jour.
De plus, ces données de référence disposent d'une validité limitée à la période correspondante, en raison des changements d'OCS.
Ces changements concernent essentiellement l'expansion urbaine au détriment des surfaces naturelles, et les terres agricoles soumises à la rotation des cultures.
L'objectif général de la thèse vise à proposer des méthodes de production de cartes d'OCS sans exploiter les données de référence de la période correspondante.
Les travaux menés s'appuient sur un historique d'OCS.
Cet historique regroupe toutes les informations disponibles pour la zone concernée : cartes d'OCS, séries temporelles, données de référence, modèles de classification, etc.
Une première partie des travaux considère que l'historique ne contient qu'une seule période.
Ainsi, nous avons proposé un protocole de classification naïve permettant d'exploiter un classifieur déjà entraîné sur une nouvelle période.
Les performances obtenues ont montré que cette approche se révèle insuffisante, requérant ainsi des méthodes plus performantes.
L'adaptation de domaine permet d'aborder ce type de problématique.
Nous avons considéré deux approches : la projection de données via une analyse canonique des corrélations et le transport optimal.
Ces deux approches permettent de projeter les données de l'historique afin de réduire les différences avec l'année à traiter.
Néanmoins ces approches offrent des résultats équivalents à la classification naïve pour des coûts de production bien plus significatifs.
Dans de nombreux domaines où il existe des risques pour l'homme, comme la médecine, le nucléaire ou l'avionique, il est nécessaire de passer par une phase de certification visant à garantir le bon fonctionnement d'un système ou d'un produit.
La certification se fait en fonction de documents normatifs qui expriment les exigences de justifications auxquelles le produit et le processus de développement doivent se conformer.
Un audit de certification consiste alors à produire une documentation attestant la conformité avec ce cadre réglementaire.
Pour faire face à ce besoin de justifications visant à assurer la conformité avec les normes en vigueur et la complétude des justifications apportées, il faut dès lors être capable de cibler les exigences de justification à revendiquer pour un projet et produire les justifications durant le développement du projet.
Dans ce contexte, éliciter les exigences de justifications à partir des normes et produire les justifications nécessaires et suffisantes sont des enjeux pour assurer le respect des normes et éviter la sur-justification.
Dans ces travaux nous cherchons à structurer les exigences de justification pour ensuite aider à la production des justifications associées tout en restant attentif à la confiance que l'on peut placer en elles.
Pour relever ces défis, nous avons défini une sémantique formelle pour une modélisation existante des justifications : les Diagrammes de Justification.
A partir de cette sémantique, nous avons pu définir un ensemble d'opérations permettant de contrôler le cycle de vie des justifications pour assurer la conformité des justifications au regard des exigences de justification.
Par ce formalisme, nous avons également pu guider, voire automatiser dans certains cas, la production des justifications et la vérification de la conformité.
Ces contributions ont été appliquées dans le contexte des technologies médicales pour l'entreprise AXONIC, porteuse de ces travaux.
Cette thèse, prenant place en Traitement Automatique des Langues, a pour objectif d'aider les utilisateurs dans de telles situations.
Les systèmes traditionnellement proposés (tels les moteurs de recherche) ne donnent pas toujours satisfaction aux utilisateurs pour des tâches répétées, prenant peu en considération leur point de vue et leurs interactions avec le matériau textuel.
Nous proposons dans cette thèse que la personnalisation et l'interaction soient au centre de nouveaux outils d'aide pour l'accès au contenu d'ensembles de textes.
Ainsi, nous représentons le point de vue de l'utilisateur sur ses domaines d'intérêt par des ensembles de termes décrits et organisés selon un modèle de sémantique lexicale différentielle.
Nous exploitons de telles représentations pour construire des supports cartographiques d'interactions entre l'utilisateur et l'ensemble de textes, supports lui permettant de visualiser des regroupements, des liens et des différences entre textes de l'ensemble, et ainsi d'appréhender son contenu.
Afin d'opérationnaliser de telles propositions, nous avons mis au point la plate-forme ProxiDocs.
« Qu'ai-je besoin de connaître minimalement d'une chose pour la connaître ? »
Le fait que cette question aux allures de devinette s'avère cognitivement difficile à appréhender de par son degré de généralité explique sans peine la raison pour laquelle son élucidation demeura plusieurs millénaires durant l'apanage d'une discipline unique : la Philosophie.
Dans ce contexte, énoncer des critères à même de distinguer les composants primitifs de la réalité – ou le "mobilier du monde" – ainsi que leurs relations revient à produire une Ontologie.
Cet ouvrage s'attelle à la tâche d'élucider le tournant historique curieux, en apparence anodin, que constitue l'émergence de ce type de questionnement dans le champ de deux disciplines connexes que constituent l'Intelligence Artificielle et l'Ingénierie des Connaissances.
Nous montrons plus particulièrement ici que leur import d'une forme de méthodologie ontologique appliquée à la cognition ou à la représentation des connaissances ne relève pas de la simple analogie mais soulève un ensemble de questions et d'enjeux pertinents tant sur un plan appliqué que spéculatif.
Plus spécifiquement, nous montrons ici que certaines des solutions techniques au problème de la data-masse (Big Data) – i.e. la multiplication et la diversification des données en ligne – constitue un point d'entrée aussi nouveau qu'inattendu dans de nombreuses problématiques traditionnellement philosophiques relatives à la place du langage et des raisonnements de sens commun dans la pensée ou encore l'existence d'une structuration de la réalité indépendante de l'esprit humain.
Le développement d'outils de traitement automatique pour les dialectes de l'arabe se heurte à l'absence de ressources pour ces derniers.
Dans ce travail, nous nous intéressons particulièrement au traitement du dialecte tunisien.
Nous proposons un système de conversion du tunisien vers une forme approximative de l'arabe standard pour laquelle l'application des outils conçus pour ce dernier permet d'obtenir de bons résultats.
Ce dernier permet d'assigner des étiquettes morphosyntaxiques à la sortie de notre système de conversion.
Ces étiquettes sont finalement projetées sur le tunisien.
Notre système atteint une précision de 89% suite à la conversion qui représente une augmentation absolue de ∼20% par rapport à l'étiquetage d'avant la conversion.
Les enfants souffrant de troubles du langage, comme la dyslexie, rencontrent de grandes difficultés dans l'apprentissage de la lecture et dans toute tâche de lecture, par la suite.
Depuis une quinzaine d'années, des outils développés dans le domaine du Traitement Automatique des Langues sont détournés pour être utilisés comme stratégie d'aide et de compensation pour les élèves en difficultés.
Parallèlement, l'usage de cartes conceptuelles ou de cartes heuristiques pour aider les enfants dyslexiques à formuler leurs pensées, ou à retenir certaines connaissances, s'est développé.
Ce projet a abouti, premièrement, à la réalisation d'une expérimentation exploratoire, sur l'aide à la compréhension de texte grâce aux cartes heuristiques, qui permet de définir de nouveaux axes de recherche ;
Une collection documentaire est généralement représentée comme un ensemble de documents mais cette modélisation ne permet pas de rendre compte des relations intertextuelles et du contexte d'interprétation d'un document.
Le modèle documentaire classique trouve ses limites dans les domaines spécialisés où les besoins d'accès à l'information correspondent à des usages spécifiques et où les documents sont liés par de nombreux types de relations.
Le premier modèle est basée sur l'analyse formelle et relationnelle de concepts, le deuxième est basée sur les technologies du web sémantique.
Le travail présenté dans cette thèse fait partie d'un programme de recherche qui vise à comprendre comment le cerveau traite et représente les structures symboliques dans des domaines comme le langage ou les mathématiques.
L'existence de structures composées de sous-éléments, tel que les morphèmes, les mots ou les phrases est très fortement suggérée par les analyses linguistiques et les données expérimentales de la psycholinguistique.
En revanche, l'implémentation neuronale des opérations et des représentations qui permettent la nature combinatoire du langage reste encore essentiellement inconnue.
Certaines opérations de composition élémentaires permettant une représentation interne stable des objets dans le cortex sensoriel, tel que la reconnaissance hiérarchique des formes, sont aujourd'hui mieux comprises [5].
En revanche, les modèles concernant les opérations de liaisons(binding) nécessaires à la construction de structures symboliques complexes et possiblement hiérarchiques, pour lesquelles des manipulations précises des composants doit être possible, sont encore peu testés de façon expérimentale et incapables de prédire les signaux en neuroimagerie.
Combler le fossé entre les données de neuroimagerie expérimentale et les modèles proposés pour résoudre le problème de binding est une étape cruciale pour mieux comprendre les processus de traitements et de représentation des structures symboliques.
Au regard de ce problème, l'objectif de ce travail était d'identifier et de tester expérimentalement les théories basées sur des réseaux neuronaux, capables de traiter des structures symboliques pour lesquelles nous avons pu établir des prédictions testables, contre des mesures existantes de neuroimagerie fMRI et ECoG dérivées de tâches de traitement du langage.
Nous avons identifié deux approches de modélisation pertinentes.
La première approche s'inscrit dans le contexte des architectures symboliques vectorielles (VSA), qui propose une modélisation mathématique précise des opérations nécessaires pour représenter les structures dans des réseaux neuronaux artificiels.
C'est le formalisme de Paul Smolensky[10], utilisant des produit tensoriel (TPR) qui englobe la plupart des architectures VSA précédemment proposées comme, par exemple, les modèles d'Activation synchrones[9], les représentations réduites holographique[8], et les mémoires auto-associatives récursives[1].
L'architecture du Blackboard repose sur des changements de connectivité transitoires des circuits d'assemblages neuronaux, de sorte que le potentiel de l'activité neurale permise par les mécanismes de mémoire de travail après un processus de liaison, représente implicitement les structures symboliques.
Dans la première partie de cette thèse, nous détaillons la théorie derrière chacun de ces modèles et les comparons, du point de vue du problème de binding.
Les deux modèles sont capables de répondre à la plupart des défis théoriques posés par la modélisation neuronale des structures symboliques, notamment ceux présentées par Jackendo[3].
La NBA en revanche, considère les dynamiques temporelles de décharge de neurones artificiels, avec des représentations spatialement instables implémentées par des assemblages neuronaux.
L'assistance informatique est devenue une partie indispensable pour la réalisation de procédures chirurgicales modernes.
Le désir de créer une nouvelle génération de blocs opératoires intelligents a incité les chercheurs à explorer les problèmes de perception et de compréhension automatique de la situation chirurgicale.
De grands progrès ont été réalisés pour la reconnaissance des phases et des gestes chirurgicaux.
Pourtant, il existe encore un vide entre ces deux niveaux de granularité dans la hiérarchie du processus chirurgical.
Très peu de recherche se concentre sur les activités chirurgicales portant des informations sémantiques vitales pour la compréhension de la situation.
Deux facteurs importants entravent la progression.
Tout d'abord, la reconnaissance et la prédiction automatique des activités chirurgicales sont des tâches très difficiles en raison de la courte durée d'une activité, de leur grand nombre et d'un flux de travail très complexe et une large variabilité.
Deuxièmement, une quantité très limitée de données cliniques ne fournit pas suffisamment d'informations pour un apprentissage réussi et une reconnaissance précise.
À notre avis, avant de reconnaître les activités chirurgicales, une analyse soigneuse des éléments qui composent l'activité est nécessaire pour choisir les bons signaux et les capteurs qui faciliteront la reconnaissance.
Nous avons utilisé une approche d'apprentissage profond pour évaluer l'impact de différents éléments sémantiques de l'activité sur sa reconnaissance.
Grâce à une étude approfondie, nous avons déterminé un ensemble minimum d'éléments suffisants pour une reconnaissance précise.
Les informations sur la structure anatomique et l'instrument chirurgical sont de première importance.
Nous avons également abordé le problème de la carence en matière de données en proposant des méthodes de transfert de connaissances à partir d'autres domaines ou chirurgies.
Les méthodes de ''word embedding''et d'apprentissage par transfert ont été proposées.
Ils ont démontré leur efficacité sur la tâche de prédiction d'activité suivante offrant une augmentation de précision de 22%.
De plus, des observations pertinentes
Malgré l'importance d'une nomenclature internationale, le domaine de la chimie souffre encore de quelques problèmes linguistiques, liés notamment à ses unités terminologiques simples et complexes, pouvant gêner la communication scientifique.
A cela s'ajoute l'emploi récurrent d'emprunts.
La question est de savoir comment représenter les unités terminologiques simples et complexes de cette langue spécialisée.
En d'autres termes, formaliser les caractéristiques terminologiques en étudiant les mécanismes de la construction morphosyntaxique des termes de la chimie en arabe.
Cette étude devrait aboutir à la mise en place d'un outil de désambigüisation sémantique qui vise à constituer un outil d'extraction des termes de la chimie en arabe et de leurs relations.
La construction de cette grammaire d'identification nécessite la modélisation des patrons morphosyntaxiques à partir de leur observation en corpus etdébouche sur la définition de règles de grammaire et de contraintes.
Le développement technologique a ses avantages et ses inconvénients.
Nous pouvons facilement partager et télécharger du contenu numérique en utilisant l'Internet.
En outre, les utilisateurs malveillants peuvent aussi modifier, dupliquer et diffuser illégalement tout type d'informations, comme des images et des documents.
Par conséquent, nous devons protéger ces contenus et arrêter les pirates.
Le but de cette thèse est de protéger les documents PDF et les images en utilisant la technique de tatouage numérique Spread Transform Dither Modulation (STDM), tout en tenant compte des exigences principales de transparence, de robustesse et de sécurité.
La méthode de tatouage STDM a un bon niveau de transparence et de robustesse contre les attaques de bruit.
La clé principale dans cette méthode de tatouage est le vecteur de projection qui vise à diffuser le message sur un ensemble d'éléments.
Cependant, un tel vecteur clé peut être estimée par des utilisateurs non autorisés en utilisant les techniques de séparation BSS (Blind Source Separation).
Dans notre première contribution, nous présentons notre méthode de tatouage proposé CAR-STDM (Component Analysis Resistant-STDM), qui garantit la sécurité tout en préservant la transparence et la robustesse contre les attaques de bruit.
STDM est également affecté par l'attaque FGA (Fixed Gain Attack).
Dans la deuxième contribution, nous présentons notre méthode de tatouage proposé N-STDM qui résiste l'attaque FGA et améliore la robustesse contre l'attaque Additive White Gaussian Noise (AWGN), l'attaque de compression JPEG, et diversité d'attaques de filtrage et géométriques.
Les expérimentations ont été menées sur des documents PDF et des images dans le domaine spatial et le domaine fréquentiel.
Récemment, l'Apprentissage Profond et les Réseaux de Neurones atteints du développement et d'amélioration notable, en particulier dans le traitement d'image, la segmentation et la classification.
Des modèles tels que CNN (Convolutional Neural Network) sont utilisés pour la dé-bruitage des images.
CNN a une performance adéquate de dé-bruitage, et il pourrait être nocif pour les images tatouées.
Dans la troisième contribution, nous présentons l'effet du FCNN (Fully Convolutional Neural Network), comme une attaque de dé-bruitage, sur les images tatouées.
Les méthodes de tatouage STDM et SS (Spread Spectrum) sont utilisés durant les expérimentations pour intégrer les messages dans les images en appliquant plusieurs scénarios.
Cette évaluation montre qu'un tel type d'attaque de dé-bruitage préserve la qualité de l'image tout en brisant la robustesse des méthodes de tatouages évalués.
La microscopie confocale de réflectance in-vivo (RCM) est un outil puissant pour visualiser les couches cutanées à une résolution cellulaire.
Des descripteurs du vieillissement cutané ont été mis en évidence à partir d'images confocales.
Cependant, leur évaluation nécessite une analyse visuelle des images par des dermatologues expérimentés.
L'objectif de cette thèse est le développement d'une technologie innovante pour quantifier automatiquement le phénomène du vieillissement cutané en utilisant la microscopie confocale de réflectance in vivo.
Premièrement, la quantification de l'état de l'épiderme est abordée.
Les mesures proposées mettent en évidence une différence significative entre les groupes d'âge et l'exposition au soleil.
Enfin, les méthodes proposées sont validées par des études cliniques et d'efficacité de produits cosmétiques
De nos jours, l'utilisation des techniques de gestion de processus au sein des entreprises permet un gain significatif d'efficacité du système opérationnel.
Ces techniques assistent les experts métier lors de la modélisation des processus de l'entreprise, de leur mise en oeuvre, de leurs analyses et de leurs améliorations.
Le contexte d'exécution d'un processus métier contient des informations permettant d'identifier et de comprendre les interactions entre lui-même et d'autres processus.
Le premier processus est déclenché suite à un besoin client (processus opérationnel) et les autres, suite à un besoin du processus opérationnel (processus supports).
La satisfaction d'un besoin client dépend d'une interaction efficace entre le processus opérationnel et les processus supports qui lui sont liés.
Ces interactions se définissent au travers des données opérationnelles, manipulées par le processus opérationnel, et des données supports, manipulées par les processus supports.
Nos travaux se fondent sur l'Ingénierie dirigée par les Modèles.
L'approche proposée dans cette thèse est fondée sur l'annotation des modèles de processus opérationnels ou supports.
Cette annotation est réalisée avec l'aide d'une ontologie définissant le domaine métier décrit par ces processus.
Ces annotations sont ensuite exploitées afin de constituer un ensemble de données, dites données contextuelles.
L'analyse des traces de l'exécution du processus et de ces données contextuelles permet de sélectionner les meilleurs d'entre elles, au sens métier.
Ainsi, un processus opérationnel pourra être associé à un ensemble de processus supports via les données contextuelles.
À l'échelle mondiale, les chiffres relatifs à la migration des personnes ont évolué juste après la période de la Guerre froide.
Les populations de réfugiés ont augmenté, passant de 2,4 millions en 1975 à 12,1 millions en 2000 (UNHCR 1995 ; UNHCR 2000).
Aujourd'hui, les taux de migration en Europe font apparaître un total de plus de 4 millions d'immigrants dans différents pays européens.
En outre, la France et le Royaume-Uni sont considérés comme deux des principales destinations de migration en Europe, attirant à la fois les migrants et les demandeurs de l'asile. Selon l'Office statistique de l'Union européenne situé à Luxembourg, « l'Allemagne a enregistré le plus grand nombre d'immigrants (917,1 milliers) en 2017, suivie du Royaume-Uni (644,2 milliers), de l'Espagne (532,1 milliers), de la France (370 milliers et plus), de l'Italie (343,4 milliers) » .
En général, la majorité des migrants et des réfugiés vivent dans des conditions de vie difficiles. Cependant, la situation des femmes s'avère encore plus précaire en raison des inégalités socio-économiques, les femmes migrantes représentant environ 52% de l'ensemble des migrants en Europe.
D'autre part, certaines femmes immigrées et réfugiées se sont chargées d'exprimer leur trouble et d'attirer l'attention du public sur leur situation difficile.
L'objectif de cette recherche est, d'une part, d'approfondir les mécanismes et appareils internes qui façonnent la culture et le patrimoine européen grâce aux contributions culturelles des migrants, et d'autre part, d'en étudier et d'analyser les moyens utilisés pour canaliser leurs pensées et partager leurs histoires avec le reste du monde entre le 20ème et 21ème siècle.
Cette recherche couvre plusieurs disciplines artistiques et littéraires, allant des arts visuels aux performances théâtrales, en passant par la production cinématographique, la filmographie et la photographie ; elle s'intéressera aussi aux œuvres littéraires, telles que les romans, ateliers d'écritures et bandes dessinées, qui pourraient être utilisées à des fins diverses pour restaurer l'identité et l'appartenance, ou comme un moyen de thérapie.
Sur le théâtre, l'étude couvre les textes des pionniers et les représentations qui ont façonné l'histoire de la migration dans les deux régions européennes au cours des XXème et XXIème siècles, ainsi que les troupes et les groupes qui ciblent et tendent plus particulièrement à inclure les réfugiés.
Nous pourrons nous référer à la dramaturge franco-algérienne Salika Amara qui a travaillé au sein de la troupe de théâtre française nommée Kahina, fondée en 1976 par des femmes d'Aubervilliers et principalement composée de femmes-actrices, et leurs homologues au Royaume-Uni, Winsomme Pinnock et Bola Agbaje, une dramaturge contemporaine anglo-nigérienne.
Il va sans dire que le travail théâtral ne se limite pas aux scènes des salles de théâtre, mais couvre tous les types de représentations, formels et informels. On pense par exemple à l'association Ornina qui a été fondée par une franco-syrienne, cette compagnie travaillant avec des récits de réfugiés venant de l'Est de la France ; nous verrons aussi le travail du Bureau d'Accueil et d'Accompagnement des Migrants (BAAM), de la compagnie de théâtre CK. Point, The Good Chance Theatre au Royaume-Uni et même les pièces représentées dans des festivals internationaux (migrant'scène, Avignon, Edinburgh).
En ce qui concerne le cinéma et la filmographie, nous essayerons de couvrir le cinéma minoritaire qui reflète les conditions d'exil et d'aliénation et qui est principalement produit dans les marges.
Ceci est bien décrit par Naficy comme des films comprenant notamment un « Forme ouverte et style visuel de forme fermée ; structure narrative fragmentée, multilingue, épistolaire, auto-réflexive et juxtaposée de manière critique ; caractères amphiboliques, doublés, croisés et perdus ; sujets et thèmes qui impliquent le voyage, l'historicité, l'identité et le déplacement ; structures dysphoriques, euphoriques, nostalgiques, synesthésiques, liminales et politisées des modes de production du sentiment, des modes de production interstitiel et collectif ; et inscription de la (dé) localisation biographique, sociale et cinématographique des cinéastes. » (Naficy 2001 : 4)
Toutes ces caractéristiques sont généralement présentes dans les films de migration ; de plus, ces productions représentent l'état d'esprit psychologique de l'immigrant ou du réfugié, en particulier si les producteurs ou les acteurs principaux sont eux-mêmes migrants ou réfugiés, comme dans le court métrage Unbroken Paradise interprété par un véritable réfugié syrien.
D'autres films sur les migrations, par contre, tendent à sensibiliser l'opinion publique à certains problèmes et à défendre les droits de la femme ou les droits civils, une sorte déconstruction du 'regard masculin' et l'adoption d'une approche autoréflexive défensive.
Toujours dans le domaine des arts visuels, le rôle de la photographie est indéniable.
Pendant des siècles, les photos ont été un moyen d'expression à la fois fort et silencieux.
Dans le cas des migrations, la tâche et le fardeau apparaissent plus lourds que d'habitude.
Il suffit d'un clic pour rediriger une vision, une idée et changer la manière de voir d'un pays, qu'elle soit positive ou négative.
Dans cette étude, nous pensons que le rôle des images/photos est très important et pertinent pour le parcours des migrants.
Tout comme les œuvres d'art susmentionnées, la photographie sera un moyen de ré-identifier et de raconter à nouveau des histoires de migrants.
Omar Imam par exemple, un photographe syrien travaillant sur la crise de la migration syrienne, avec son projet Live, Love, Refugee dans les camps des réfugiés, raconte des histoires de réfugiés avec des clichés créatifs et surréalistes et des images fixes de différentes actions visant à refléter certaines émotions et pensées.
De nombreux autres projets photographiques seront abordés dans le cadre de la recherche dans le but de réécrire les identités des femmes migrantes, comme le projet Refugees Learning and Storytelling through Participatory Photography lancé par trois femmes leaders dans le domaine de la migration et de la photographie et visant à enseigner aux femmes migrantes les bases de la photographie pour les responsabiliser et les aider à raconter leurs propres histoires (Brigham, 2018).
L'analyse de la littérature sur la migration comprendra l'analyse de la présence des femmes dans les textes littéraires ; cela signifiera de travailler sur des romans, des ateliers d'écriture et des oeuvres d'écrits créatifs tels que des poèmes, des journaux, des récits de voyage, etc.
Selon Franz Fanon, 'l'Europe est littéralement la création du tiers monde' (Fanon, 1961, p. 99), qui fait référence à l'écriture continue de la périphérie en tant que moyen de réécrire et de réinventer l'identité "l'expérience de la migration agit comme un catalyseur et un canal pour les sentiments naissants, une re-conception de notre sens de soi et de nos relations avec les autres" (Jacobs, 2011 : 142).
Cependant, l'écriture créative a une part importante dans cette recherche, principalement à cause de l'évolution de son utilisation dans les camps de réfugiés et par différents organismes afin de combler le fossé entre locaux et migrants en tant que thérapie sociologique et psychologique.
Les bandes dessinées sont également une nouvelle tendance à la croissance rapide utilisée par les journalistes et les artistes afin de décrire les expériences réelles de migrants de différents endroits du monde.
Grâce à un texte minimisé mais à des dessins expressifs, elles attirent l'attention des lecteurs appartenant à des origines et de statuts sociétaux différents aux conditions, épreuves et les horreurs rencontrées par les individus, les familles et les enfants à la recherche d'un abri et d'un refuge sur des sols étrangers, tels que l'excellente bande dessinée numérique Over Under Sideways Down par Karrie Fransman, et Fuyant vers l'inconnu créé par l'organisation PositiveNegative.
L'analyse couvre toutes les œuvres créées par des migrants hommes et femmes, ainsi que d'autres concernant des artistes / écrivains locaux ou internationaux, avec une référence aux femmes migrantes et à la migration.
Suivant une approche historique et pluridisciplinaire, l'analyse comparative vise à identifier les nouvelles tendances et méthodes utilisées pour représenter les femmes migrantes et à répondre aux questions suivantes : existe-t-il une différence entre les discours utilisés par les migrants dans les deux pays (France et Royaume-Uni) ?
Quelle est la différence entre le discours habituel employé lors de la représentation artistique féminine et celui de la représentation des migrantes ?
Comment est-il utilisé et à quelles fins ?
Existe-t-il une différence entre le discours masculin en référence aux femmes et le discours féminin en référence aux femmes (autoréférence) ?
Toutes ces disciplines ont-elles le même objectif vis-à-vis de la situation et de l'image des migrants ?
Au cours des dernières années, le Web a connu une énorme croissance en matière de contenus et d'utilisateurs.
Ce phénomène a entraîné des problèmes liés à la surcharge d'information face à laquelle les utilisateurs ont des difficultés à trouver les bonnes informations.
Des systèmes de recommandation ont été développés pour résoudre ce problème afin de guider les utilisateurs dans ce flux d'informations.
Les approches de recommandation se sont multipliées et ont été mises en œuvre avec succès, notamment au travers d'approches telles que le filtrage collaboratif.
Cependant, il existe encore des défis et des limites qui offrent des opportunités pour de nouvelles recherches.
Parmi ces défis, la conception de systèmes de recommandation de lectures est devenue un axe de recherche en pleine expansion suite à l'apparition des bibliothèques numériques.
Traditionnellement, les bibliothèques jouent un rôle passif dans l'interaction avec les lecteurs et ce, faute d'outils efficaces de recherche et de recommandation.
Dans ce manuscrit, nous nous sommes penchée sur la création d'un système de recommandation de lectures.
Cette thèse traite de l'apprentissage en profondeur appliqu'e aux tâches de classification des images.
La principale motivation du travail est de rendre les techniques d'apprentissage en profondeur actuelles plus efficaces et de faire face aux changements dans la distribution des données.
Nous travaillons dans le cadre élargi de l'apprentissage continu, dans le but d'avoir à l'avenir des modèles d'apprentissage automatique pouvant être améliorés en permanence.
Nous examinons d'abord la modification de l'espace étiquette d'un ensemble de données, les échantillons de données restant les mêmes.
Nous considérons une hiérarchie d'étiquettes sémantiques à laquelle appartiennent les étiquettes.
Nous étudions comment nous pouvons utiliser cette hiérarchie pour obtenir des améliorations dans les modèles formés à différents niveaux de cette hiérarchie.
Les deuxième et troisième contributions impliquent un apprentissage continu utilisant un modèle génératif.
Nous analysons la facilité d'utilisation des échantillons d'un modèle génératif dans le cas de la formation de bons classificateurs discriminants.
Nous proposons des techniques pour améliorer la sélection et la génération d'échantillons à partir d'un modèle génératif.
Ensuite, nous observons que les algorithmes d'apprentissage continu subissent certaines pertes de performances lorsqu'ils sont entraînés séquentiellement à plusieurs tâches.
Nous analysons la dynamique de la formation dans ce scénario et comparons avec la formation sur plusieurs tâches simultanément.
Nous faisons des observations qui indiquent des difficultés potentielles dans l'apprentissage de modèles dans un scénario d'apprentissage continu.
Enfin, nous proposons un nouveau modèle de conception pour les réseaux de convolution.
Cette architecture permet de former des modèles plus petits sans compromettre les performances.
De plus, la conception se prête facilement à la parallélisation, ce qui permet une formation distribuée efficace.
En conclusion, nous examinons deux types de scénarios d'apprentissage continu.
Nous proposons des méthodes qui conduisent à des améliorations.
Notre analyse met également en évidence des problèmes plus importants, dont nous aurions peut-être besoin de changements dans notre procédure actuelle de formation de réseau neuronal.
L'ère des grandes masses de données (big data) nous a mis face à de nouvelles problématiques de gestion et de traitement des données.
La théorie des AMAS (Adaptive Multi-Agent Systems) propose de résoudre par autoorganisation des problèmes complexes pour lesquels aucune solution algorithmique n'est connue.
Le comportement coopératif des agents permet au système de s'adapter à un environnement dynamique pour maintenir le système dans un état de fonctionnement adéquat.
Les systèmes ambiants présentent un exemple typique de système complexe nécessitant ce genre d'approche, et ont donc été choisis comme domaine d'application pour notre travail.
Cette thèse vise à explorer et décrire comment la théorie des Systèmes Multi-Agents
Adaptatifs peut être appliquée aux grandes masses de données en fournissant des capacités d'analyse dynamique, en utilisant un nouvel outil analytique qui mesure en temps réel la similarité des évolutions des données.
Cette recherche présente des résultats prometteurs et est actuellement appliquée dans l'opération neOCampus, le campus ambiant de l'Université Toulouse III.
Le contenu visuel se concentre souvent sur les humains.
L'analyse automatique des humains à partir de données visuelles revêt donc une grande importance pour de nombreuses applications.
Le but de cette thèse est d'apprendre des représentations visuelles pour l'analyse des humains.
Un accent particulier est mis sur deux domaines étroitement liés de la vision artificielle : l'analyse du corps humain et la reconnaissance des actions.
L'augmentation considérable de la quantité des données textuelles rend aujourd'hui difficile leur analyse sans l'assistance d'outils.
Or, un texte rédigé en langue naturelle est une donnée non-structurée, c'est-à-dire qu'elle n'est pas interprétable par un programme informatique spécialisé, sans lequel les informations des textes restent largement sous-exploitées.
Pour réaliser cette tâche, nous proposons une nouvelle approche par alignement de deux types de représentations vectorielles d'entités capturant une partie de leur sens : les plongements lexicaux pour les mentions textuelles et des “plongements ontologiques” pour les concepts, conçus spécifiquement pour ce travail.
L'alignement entre les deux se fait par apprentissage supervisé.
Les méthodes développées ont été évaluées avec un jeu de données de référence du domaine biologique et elles représentent aujourd'hui l'état de l'art pour ce jeu de données.
Ces méthodes sont intégrées dans une suite logicielle de traitement automatique des langues et les codes sont partagés librement.
Le processus de découverte de médicaments a un succès limité malgré tous les progrès réalisés.
En effet, on estime actuellement que le développement d'un médicament nécessite environ 1,8 milliard de dollars américains sur environ 13 ans.
Leurs applications sont polyvalentes : elles permettent d'identifier des candidats médicaments pour des cibles thérapeutiques connues, d'anticiper des effets secondaires potentiels, ou de proposer de nouvelles indications thérapeutiques pour des médicaments connus.
Cette thèse est conçue selon deux cadres d'approches de criblage virtuel : les approches dans lesquelles les données sont décrites numériquement sur la base des connaissances des experts, et les approches basées sur l'apprentissage automatique de la représentation numérique à partir du graphe moléculaire et de la séquence protéique.
Nous discutons ces approches et les appliquons pour guider la découverte de médicaments.
Dans chaque organisation, les processus métier sont aujourd'hui incontournables.
Cette thèse vise à développer une méthode pour les améliorer.
Dans le domaine de la santé, les organisations hospitalières déploient beaucoup d'efforts pour mettre leurs processus sous contrôle, notamment à cause de la très faible marge d'erreur admise.
Les parcours des patients au sein des structures de santé constituent l'application qui a été choisie pour démontrer les apports de cette méthode.
Cette thèse propose donc les contributions suivantes : la méthode DIAG elle-même qui, grâce à quatre différents états, extrait les informations des données de géolocalisation ; le méta-modèle DIAG qui a deux utilités : d'une part, interpréter les données de géolocalisation et donc passer des données brutes aux informations utilisables, et, d'autre part contribuer à vérifier l'alignement des données avec le domaine grâce à deux méthodes de diagnostic décrites plus bas ; deux algorithmes de découverte de processus qui utilisent la stabilité statistique des logs d'évènements ; une nouvelle approche de process mining utilisant SPC (Statistical Process Control) pour l'amélioration ; l'algorithme proDIST qui mesure les distances entre les modèles de processus ; deux méthodes de diagnostic automatique de processus pour détecter les causes des déviations structurelles dans des cas individuels et pour des processus communs.
Le contexte de cette thèse confirme la nécessité de proposer de telles solutions.
Une étude de cas dans le cadre de ce travail de recherche illustre l'applicabilité de la méthodologie DIAG et des fonctions et méthodes mentionnées.
Cette thèse pour but de développer des méthodes de segmentation pour des scènes fortement structurées (ex. bâtiments et environnements urbains) ou faiblement structurées (ex. paysages ou objets naturels).
En particulier, les images de bâtiments peuvent être décrites en termes d'une grammaire de formes, et une dérivation de cette grammaire peut être inférée pour obtenir une segmentation d'une image.
Cependant, il est difficile et long d'écrire de telles grammaires.
Pour répondre à ce problème, nous avons développé une nouvelle méthode qui permet d'apprendre automatiquement une grammaire à partir d'un ensemble d'images et de leur segmentation associée.
Des expériences montrent que des grammaires ainsi apprises permettent une inférence plus rapide et produisent de meilleures segmentations.
De manière surprenante, même sans connaissance spécifique sur le type de scène particulier observé, nous obtenons des gains significatifs en qualité de segmentation sur plusieurs jeux de données.
Enfin, nous avons développé une technique basée sur les réseaux de neurones convolutifs (CNN) pour segmenter des images de scènes faiblement structurées.
Un filtrage adaptatif est effectué à l'intérieur même du réseau pour permettre des dépendances entre zones d'images distantes.
Des expériences sur plusieurs jeux de données à grande échelle montrent là aussi un gain important sur la qualité de segmentation
Le critère de l'information k-means étend le critère des k-means en utilisant la divergence de Kullback comme fonction de perte.
La fragmentation est une généralisation supplémentaire permettant l'approximation de chaque signal par une combinaison de fragments.
Nous proposons un nouvel algorithme de fragmentation pour les signaux numériques se présentant comme un algorithme de compression avec perte.
Nous avons testé la méthode sur des images en niveaux de gris sur lesquelles il a été possible de détecter des configurations translatées ou transformées par une rotation.
Ceci donne l'espoir d'apporter une réponse à la reconnaissance invariante par transformations fondée sur un critère de compression très général.
D'un point de vue mathématique, nous avons prouvé deux types de bornes.
Ce résultat contribue à expliquer la pertinence de nos arbres syntaxiques.
Ensuite, nous établissons des bornes de généralisation non asymptotiques et indépendantes de la dimension pour les différents critères des k-means et critères de fragmentation que nous avons introduits.
Grâce à une nouvelle méthode de chaînage PAC-Bayésien, nous prouvons aussi une borne en O(log(n/k) sqrt{k log(k)/n}).
Ainsi, les lacs de données sont devenus une solution attractive par rapport aux entrepôts de données classiques coûteux et fastidieux (nécessitant une démarche ETL), pour les entreprises qui souhaitent stocker leurs données.
Malgré leurs volumes, les données stockées dans les lacs de données des entreprises sont souvent incomplètes voire non mises à jour vis-à-vis des besoins (requêtes) des utilisateurs.
Les sources de données locales ont donc besoin d'être enrichies.
Ainsi, afin de permettre d'accéder et de récupérer l'information de manière simple et interopérable, les sources de données sont de plus en plus intégrées dans les services Web.
Il s'agit plus précisément des services de données, y compris les services DaaS du Cloud Computing.
L'enrichissement manuel des sources locales implique plusieurs tâches fastidieuses telles que l'identification des services pertinents, l'extraction et l'intégration de données hétérogènes, la définition des mappings service
Cela permettrait de satisfaire les requêtes des utilisateurs tout en respectant leurs préférences en terme de coût d'exécution et de temps de réponse et en garantissant la qualité des résultats obtenus.
Les travaux de la thèse portent sur l'estimation de la saillance du mouvement dans des séquences d'images.
Dans une première partie, nous avons traité un sujet très peu abordé : la détection des images présentant un mouvement saillant.
Pour cela, nous nous appuyons sur un réseau de neurones convolutif et sur la compensation du mouvement de la caméra.
Dans une seconde partie, nous avons conçu une méthode originale d'estimation de cartes de saillance du mouvement.
Cette méthode ne requiert pas d'apprentissage.
L'indice de saillance est obtenu par une étape d'inpainting du flot optique, suivie d'une comparaison avec le flot initial.
Dans un troisième temps, nous nous sommes intéressés à l'estimation de la saillance de trajectoires pour appréhender une saillance progressive.
Nous construisons une méthode faiblement supervisée s'appuyant sur un réseau auto-encodeur récurrent, qui représente chaque trajectoire avec un code latent.
Toutes ces méthodes ont été validées sur des données de vidéo réelles.
La gestion efficace de grandes quantités d'informations est devenue un défi de plus en plus important pour les systèmes d'information.
Tous les jours, de nouvelles sources d'informations émergent sur le web.
Un humain peut assez facilement retrouver ce qu'il cherche, lorsqu'il s'agit d'un article,d'une vidéo, d'un artiste précis.
En revanche, il devient assez difficile, voire impossible, d'avoir une démarche exploratoire pour découvrir de nouveaux contenus.
Les systèmes de recommandation sont des outils logiciels ayant pour objectif d'assister l'humain afin de répondre au problème de surcharge d'informations.
Les travaux présentés dans ce document proposent une architecture pour la recommandation efficace d'articles d'actualité.
Contenu dans une ontologie, ce vocabulaire constitue une modélisation formelle de la vue métier sur le domaine traité.
Réalisés en collaboration avec la société Actualis SARL, ces travaux ont permis la commercialisation d'un nouveau produit hautement compétitif, FristECO Pro'fil.
Face à la complexité significative du domaine mammographique ainsi que l'évolution massive de ses données, le besoin de contextualiser les connaissances au sein d'une modélisation formelle et exhaustive devient de plus en plus impératif pour les experts.
C'est dans ce cadre que s'inscrivent nos travaux de recherche qui s'intéressent à unifier différentes sources de connaissances liées au domaine au sein d'une modélisation ontologique cible.
D'une part, plusieurs modélisations ontologiques mammographiques ont été proposées dans la littérature, où chaque ressource présente une perspective distincte du domaine d'intérêt.
D'autre part, l'implémentation des systèmes d'acquisition des mammographies rend disponible un grand volume d'informations issues des faits passés, dont la réutilisation devient un enjeu majeur.
Toutefois, ces fragments de connaissances, présentant de différentes évidences utiles à la compréhension de domaine, ne sont pas interopérables et nécessitent des méthodologies de gestion de connaissances afin de les unifier.
C'est dans ce cadre que se situe notre travail de thèse qui s'intéresse à l'enrichissement d'une ontologie de domaine existante à travers l'extraction et la gestion de nouvelles connaissances (concepts et relations) provenant de deux courants scientifiques à savoir : des ressources ontologiques et des bases de données comportant des expériences passées.
Notre approche présente un processus de couplage entre l'enrichissement conceptuel et l'enrichissement relationnel d'une ontologie mammographique existante.
Le but étant de réduire l'étape d'alignement de deux ontologies entières en un alignement de deux groupements de concepts de tailles réduits.
La deuxième étape consiste à aligner les deux structures des clusters relatives aux ontologies cible et source.
Les alignements validés permettent d'enrichir l'ontologie de référence par de nouveaux concepts permettant d'augmenter le niveau de granularité de la base de connaissances.
Le deuxième processus s'intéresse à l'enrichissement relationnel de l'ontologie mammographique cible par des relations déduites de la base de données de domaine.
Cette dernière comporte des données textuelles des mammographies recueillies dans les services de radiologies.
Cette dernière consiste à filtrer et classer les règles afin de faciliter leur interprétation et validation par l'expert vi) L'enrichissement de l'ontologie par de nouvelles associations entre les concepts.
Cette approche a été mise en 'uvre et validée sur des ontologies mammographiques réelles et des données des patients fournies par les hôpitaux Taher Sfar et Ben Arous.
Dans cette thèse nous avons exploré la capacité des modèles magnétiques de la physique statistique à extraire l'information essentielle contenue dans les textes.
Les documents ont été représentés comme des ensembles d'unités en interaction magnétique, l'intensité de telles interactions a été mesurée et utilisée pour calculer de quantités qui sont des indices de l'importance de l'information portée.
Nous proposons deux nouvelles méthodes.
Cette quantité a été utilisée comme indicatrice de pertinence et appliquée à une vaste palette de tâches telles que le résumé automatique, la recherche d'information, la classification de documents et la segmentation thématique.
Par ailleurs, et de façon encore exploratoire, nous proposons un deuxième algorithme qui définie un couplage grammatical pour conserver les termes importants et produire des contractions.
De cette façon, la compression d'une phrase est l'état fondamental de la chaîne de termes.
Comme cette compression n'est pas forcement bonne, il a été intéressant de produire des variantes en permettant des fluctuations thermiques.
Nous avons fait des simulations Métropolis Monte-Carlo avec le but de trouver l'état fondamental de ce système qui est analogue au verre de spin.
Les mécanismes sous-jacents en compétition déterminent l'évolution du gaz dans le milieu interstellaire.
La relaxation électronique de l'état excité le plus brillant a été simulée pour des polyacènes neutres constitués de 2 à 7 cycles aromatiques.
Les résultats montrent une alternance marquée dans les temps de dépopulation de l'état initial pour les polyacènes contenant jusqu'à 6 cycles aromatiques, ce qui est corrélé avec une alternance des écarts d'énergie entre l'état initial et l'état situé juste dessous.
Les résultats montrent que la population électronique excitée du chrysène décroît un ordre de grandeur plus rapidement que celle du tétracène.
Ceci est aussi corrélé à une différence significative des écarts d'énergie entre l'état initial et l'état situé juste dessous.
Un dernier développement majeur concerne l'utilisation d'algorithmes "Machine Learning" (ML) proposés comme un moyen d'éviter la plupart des calculs de structure électronique, très coûteux en temps calcul.
Les performances d'algorithmes de réseaux de neurones appliqués à la dynamique des états excités ont été évaluées.
Le cas de la relaxation électronique dans le phénanthrène neutre a été choisi comme test en raison de divers résultats expérimentaux disponibles.
L'apprentissage de plusieurs réseaux de neurones a été effectué et leurs précision et efficacité analysés.
De plus, des approximations de trajectoires à sauts de surface ont été interfacées à l'approche ML, résultant en un coût négligeable des simulations de dynamique non-adiabatique.
L'efficacité des diverses approches simplifiées a été comparée à FSSH.
Dans l'ensemble, ML se révèle un outil très prometteur pour la dynamique dans les états excités à l'échelle de la nanoseconde.
Ce travail de thèse ouvre de nouvelles voies pour étudier la photophysique théorique de complexes moléculaires de grande taille.
Enfin, les outils développés et implémentés dans deMon-Nano, de manière modulaire, peuvent être combinés avec d'autres approches DFTB sophistiquées (tel que "Configuration Interaction") plus adaptées aux états à transfert de charge.
En effet, pour être performants, ces modèles ont besoin de corpus annotés de taille importante.
Par conséquent, uniquement les langues bien dotées peuvent bénéficier directement de l'avancée apportée par les RNs, comme par exemple les formes formelles des langues.
Dans le cadre de cette thèse, nous proposons des méthodes d'apprentissage par transfert neuronal pour la construction d'outils de TAL pour les langues et domaines peu dotés en exploitant leurs similarités avec des langues et des domaines bien dotés.
Précisément, nous expérimentons nos approches pour le transfert à partir du domaine source des textes formels vers le domaine cible des textes informels (langue utilisée dans les réseaux sociaux).
Tout au long de cette thèse nous présentons différentes contributions.
Tout d'abord, nous proposons deux approches pour le transfert des connaissances encodées dans les représentations neuronales d'un modèle source, pré-entraîné sur les données annotées du domaine source, vers un modèle cible, adapté par la suite sur quelques exemples annotés du domaine cible.
Ensuite, nous effectuons une série d'analyses pour repérer les limites des méthodes proposées.
Nous constatons que, même si l'approche d'apprentissage par transfert proposée améliore les résultats du domaine cible, un transfert négatif « dissimulé » peut atténuer le gain final apporté par l'apprentissage par transfert.
De plus, une analyse interprétative du modèle pré-entraîné montre que les neurones pré-entraînés peuvent être biaisés par ce qu'ils ont appris du domaine source, et donc peuvent avoir des difficultés à apprendre des « ~patterns~ » spécifiques au domaine cible.
Suite à cette analyse, nous proposons un nouveau schéma d'adaptation qui augmente le modèle cible avec des neurones normalisés, pondérés et initialisés aléatoirement permettant une meilleure adaptation au domaine cible tout en conservant les connaissances apprises du domaine source.
Enfin, nous proposons une approche d'apprentissage par transfert qui permet de tirer profit des similarités entre différentes tâches, en plus des connaissances pré-apprises du domaine source.
On s'attend à ce qu'elle soit diagnostiquée à son stade précoce, Mild Cognitive Impairment (MCI), pour pouvoir intervenir et retarder son apparition.
La tomographie par émission de positons au fluorodésoxyglucose (TEP-FDG) est considérée comme une modalité efficace pour diagnostiquer la MA et la phase précoce correspondante, car elle peut capturer les changements métaboliques dans le cerveau, indiquant ainsi des régions anormales.
A cette fin, trois nouvelles méthodes indépendantes sont proposées.
Ces connectivités sont représentées par des similarités ou des mesures graphiques entre régions.
Combinées ensuite aux propriétés de chaque région, ces caractéristiques sont intégrées dans un cadre de classification d'
Le gradient spatial est quantifié par un histogramme 2D d'orientation et exprimé sous forme multi-échelle.
Les résultats sont obtenus en intégrant différentes échelles de gradients spatiaux dans différentes régions.
Une telle architecture peut faciliter les opérations de convolution, de la 3D à la 2D, tout en tenant compte des relations spatiales, qui bénéficient d'une nouvelle couche de cartographie.
Les expériences menées sur des ensembles de données publics montrent que les trois méthodes proposées peuvent atteindre des performances significatives et, de surcroît, dépasser les approches les plus avancées.
Cette thèse aborde les problèmes de variabilité et confusabilité phonémique du point de vue des modèles de prononciation pour un système de reconnaissance automatique de la parole.
En particulier, plusieurs directions de recherche sont étudiées.
Cependant, ajouter plusieurs prononciations par mot au vocabulaire peut introduire des homophones (ou quasi-homophones) et provoquer une augmentation de la confusabilité du système.
Une nouvelle mesure de cette confusabilité est proposée pour analyser et étudier sa relation avec la performance d'un système de reconnaissance de la parole.
Cette “confusabilité de prononciation” est plus élevée si des probabilités pour les prononciations ne sont pas fournies et elle peut potentiellement dégrader sérieusement la performance d'un système de reconnaissance de la parole.
Il convient, par conséquent, qu'elle soit prise en compte lors de la génération de prononciations.
On étudie donc des approches d'entraînement discriminant pour entraîner les poids d'un modèle de confusion phonémique qui autorise différentes facons de prononcer un mot tout en contrôlant le problème de confusabilité phonémique.
La fonction objectif à optimiser est choisie afin de correspondre à la mesure de performance de chaque tâche particulière.
Dans cette thèse, deux tâches sont étudiées : la tâche de reconnaissance automatique de la parole et la tâche de détection de mots-clés.
Pour les expériences menées sur la détection de mots-clés, le “Figure of Merit” (FOM), une mesure de performance de la détection de mots-clés, est directement optimisée.
Le present travail est une etude terminologique sur le domaine de la finance, en vue d'un traitement automatique.
On y analyse les formes derivees et composees d'un corpus, mettant evidence sur la relation syntactico-semantique qu'entretiennent leurs constituants, dans le but de determiner quels formants sont productifs dans la langue economique.
Les expressions idiomatiques ou recurrentes du corpus sont egalement traitees.
L'etude met egalement en contraste, dans le dernier temps, les termes economiques du coreen et ceux du francais en vue de proposer une aide a la redaction et a la traduction.
La spectroscopie vibrationnelle englobe les techniques optiques spécifiques de la spectroscopie infrarouge à transformée de Fourier (IRTF) et Raman (RS).
Ces techniques sondent les vibrations moléculaires de l'échantillon lorsque la lumière interagit avec celui-ci, ce qui représente des 'empreintes moléculaires' de la composition chimique globale.
Les deux techniques sont très prometteuses pour le diagnostic en santé, notamment dans le cadre des 'biopsies liquides', en particulier les biofluides.
Cette étude a porté sur le développement des méthodologies bio-spectroscopiques pour l'analyse biochimique du sérum à visée diagnostic rapide et détection de pathologies.
Au-delà de la preuve–de-concept et des études sur les variations préanalytiques (qui n'ont montré aucun effet sur le profil spectral sérique) par la congélation/décongélation du sérum et le séchage en milieu ambiant, trois études diagnostiques ont été menées sur des sérums provenant de patients avec différentes pathologies : cirrhotiques avec ou sans un carcinome hépatocellulaire, différents stades de fibrose et différents stades de tumeurs cérébrales.
Tout au long de cette thèse, une série de techniques spectroscopiques IRTF et Raman ont été développées/utilisées, telles que l'ATR-IRTF, HTS-IRTF (IRTF à haut débit), spectroscopie Raman sur sérums humains séchés et liquides.
Des approches chimiométriques avancées ont été utilisées telles que PCA, HCA, PLS-DA, LDA avancé, SVM-LOOCV avec fonction de base radiale et classifieurs Random Forest, avec pour but de développer un classificateur robuste de diagnostique d'une pathologie.
Dans toutes les études de diagnostic, les résultats ont montré une capacité diagnostique modérée à bonne.
Ces travaux démontrent que la spectroscopie vibrationnelle associée à des méthodes chimiométriques avancées peut constituer une approche complémentaire pour le diagnostic clinique, tels que les zones de soins.
Cette thèse s'intéresse au rôle de la cohésion lexicale dans différentes approches de l'analyse du discours.
Nous y explorons deux hypothèses principales :
- l'analyse distributionnelle, qui permet de rapprocher des unités lexicales sur la base des contextes syntaxiques qu'elles partagent, met au jour des relations sémantiques variées pouvant être exploitées pour la détection de la cohésion lexicale des textes;
- les indices lexicaux constituent des éléments de signalisation de l'organisation du discours pouvant être exploités aussi bien à un niveau local (identification de relations rhétoriques entre constituants élémentaires du discours) qu'à un niveau global (repérage ou caractérisation de segments de niveau supérieur dotés d'une fonction rhétorique et garantissant la cohérence et la lisibilité du texte, par exemple passages à unité thématique).
Concernant le premier point, nous montrons la pertinence d'une ressource distributionnelle pour l'appréhension d'une large gamme de relations impliquées dans la cohésion lexicale des textes.
Nous présentons les méthodes de projection et de filtrage que nous avons mises en œuvre pour la production de sorties exploitables.
Concernant le second point, nous fournissons une série d'éclairages qui montrent l'apport d'une prise en compte réfléchie de la cohésion lexicale pour une grande variété de problématiques liées à l'étude et au repérage automatique de l'organisation textuelle : segmentation thématique de textes, caractérisation des structures énumératives, étude de la corrélation entre lexique et structure rhétorique du discours et enfin détection de réalisations d'une relation de discours particulière, la relation d'élaboration.
Malgré le développement de la surveillance syndromique et des méthodes modernes d'aide à la décision, la surveillance des épidémies dans le milieu hospitalier reste généralement une tâche manuelle.
Pourtant, l'utilisation d'algorithmes de détection permettrait d'améliorer l'efficacité de la surveillance, d'en étendre les contours et de réduire le temps dévolu à cette tâche.
La qualité méthodologique des évaluations publiées à ce jour est cependant trop faible pour conclure sur l'efficacité réelle de ces outils.
Nous proposons donc dans cette thèse quelques éléments clés d'un cadre méthodologique suffisant pour la détection précoce, ainsi qu'un premier jeu de données comportant des événements à risque épidémique étiquetés.
Ce jeu de données permettra à tous les chercheurs de développer, évaluer et comparer des algorithmes de détection.
Cependant, comme l'ont montré les recherches sur les systèmes de surveillance et les outils d'intelligence artificiel appliquée aux soins, la mise en œuvre de ces nouvelles technologies peut s'avérer difficile, et les performances observées en vie réelle peuvent être différentes de celles recueillies lors du développement.
Il est donc nécessaire d'inclure d'autres indicateurs et d'autres types de méthodologie d'évaluation pour mesurer la réelle utilité de ces outils, en prenant notamment en compte leur acceptabilité, leur facilité d'utilisation, leurs coûts et leurs impacts sur les pratiques.
Dans le contexte du vieillissement de la population, le but de cette thèse est d'inclure au domicile des personnes âgées un système de reconnaissance automatique de la parole (RAP) capable de reconnaître des appels de détresse pour alerter les secours.
Les modèles acoustiques des systèmes de RAP sont généralement appris avec de la parole non âgée, prononcé de façon neutre et lue.
Or, dans notre contexte, nous sommes loin de ces conditions idéales (voix âgée et émue), et le système doit donc être adapté à la tâche.
A partir de ces corpus, une étude sur les différences entre voix jeunes/âgées d'une part, et entre voix neutre/émue d'autre part nous ont permis de développer un système de RAP adapté à la tâche.
Celui-ci a ensuite été évalué sur des données issues d'une expérimentation en situation réaliste incluant des chutes jouées.
L'inversion acoustique-articulatoire de la parole consiste à récupérer la forme du conduit vocal à partir d'un signal de parole.
Ce problème est abordé à l'aide d'une méthode d'analyse par synthèse reposant sur un modèle physique de production de la parole contrôlé par un petit nombre de paramètres décrivant la forme du conduit vocal : l'ouverture de la mâchoire, la forme et la position de la langue et la position des lèvres et du larynx.
Afin de s'approcher de la géométrie de notre locuteur, le modèle articulatoire est construit à l'aide de contours articulatoires issus d'images cinéradiographiques présentant une vue sagittale du conduit vocal.
Ce synthétiseur articulatoire nous permet de créer une table formée de couples associant un vecteur articulatoire au vecteur acoustique correspondant.
Nous n'utiliserons pas les formants (fréquences de résonance du conduit vocal) comme vecteur acoustique car leur extraction n'est pas toujours fiable provoquant des erreurs lors de l'inversion.
Les coefficients cepstraux sont utilisés comme vecteur acoustique.
De plus, l'effet de la source et les disparités entre le conduit vocal du locuteur et le modèle articulatoire sont pris en compte explicitement en comparant les spectres naturels à ceux produits par le synthétiseur car nous disposons des deux signaux
Cette thèse propose une méthode pour coordonner flexiblement des Systèmes Multi-Agents (SMA).
Plus en détails, nous étudions comment influencer des agents artificiels afin que, collectivement, ils atteignent des objectifs complexes et/ou dynamiques dans des environnements eux-aussi complexes et dynamiques (ex : un groupe de robots pour secourir les victimes lors d'un désastre, qui peut s'adapter à une grande variété de dangers, conditions climatiques, état des victimes).
Dans ce but, nous avons d'abord étudié pourquoi, dans les sociétés humaines, les humains parviennent à coordonner relativement flexiblement mais pas leurs contreparties artificielles (agents des SMA).
Cette opposition peut être grandement expliquée à l'aide d'un facteur clef : la culture.
Les humains qui partagent un même bagage culturel se coordonnent flexiblement plus facilement, car ils ont une idée commune de ce que "travailler ensemble" veut dire.
A contrario, les agents n'ont pas ce bagage et leurs échecs pour travailler ensemble s'apparente souvent à des chocs culturels.
Ainsi, notre objectif consiste à répondre à la question suivante : peut-on utiliser une culture semblable à celle des humains comme un outil coordonner les SMA (et si oui, comment) ?
Pour répondre à cette question, il nous faut d'abord expliquer : comment intégrer une culture semblable à celle des humains dans un SMA ?
Cette seconde question en soulève une troisième à étudier en premier : comment est-ce que la culture influence la manière dont la coordination se passe dans les sociétés humaines ?
1. Nous montrons que de manière générale, la culture influence les décisions individuelles prises en situation d'interaction (ex : au travers d'attentes, de manière d'agir et de raisonner).
Cette influence mène à l'occurrence de schémas d'interaction abstraits, récurrent et cohérents, qui, généralement, améliorent la performance collective.
Ensuite, nous spécifions comment les principaux mécanismes l'influence connue de la culture (ex : importance culturelle accordée au pouvoir, aux règles) appliquent spécifiquement en situation de coordination (ex : la culture influence si les dirigeants donnent des ordres vs. des propositions à leurs subordonnés).
2. Nous montrons comment répliquer les mécanismes l'influence de la culture sur la coordination dans les SMA.
Tout d'abord, puisque la culture est fondée dans les décisions individuelles, nous mettons en avant un mécanisme de décision humain clef qui, à la fois, est sensible à la culture et influence la coordination.
Ce mécanisme se trouve dans les valeurs, ce que les gens considèrent comme "bien" ou "important" (ex : honnêteté, discipline, autonomie).
Ensuite, nous intégrons ces valeurs dans une architecture agent capable de prendre des décisions en situation de coordination.
Enfin, nous illustrons que notre architecture peut en effet reproduire l'influence de la culture sur la coordination à travers de deux simulations qui répliquent des phénomènes culturels en situation de coordination connus.
3. Nous étudions comment ces valeurs, inspirées des valeurs humaines, peuvent être utilisées coordonner des SMA.
Tout d'abord, nous étudions pour quels problèmes les valeurs offrent un moyen opérationnel pour soutenir la coordination.
A l'instar des sociétés humaines, les valeurs sont particulièrement offrent un haut niveau de flexibilité, quand les agents doivent raisonner eux-même pour établir une coordination.
Puis, nous étudions les détails techniques à considérer pour utiliser en pratique des valeurs pour coordonner flexiblement des SMA (ex : quelles valeurs choisir ? Comment les représenter ?).
La psychiatrie est une spécialité médicale qui vise à fournir un diagnostic et à traiter des troubles mentaux.
Malgré des classifications internationalement reconnues, la catégorisation des patients selon des critères diagnostiques reste problématique.
Les catégories actuelles peinent à prendre en compte l'hétérogénéité interindividuelle, les difficultés de délimitations des syndromes et l'influence sur les symptômes de nombreux facteurs dans l'histoire individuelle ou dans l'environnement.
La recherche en psychiatrie nécessite une amélioration de la description des comportements, des syndromes ou des dysfonctionnements associés aux troubles psychiatriques.
À cette fin, nous proposons OntoPsychia, une ontologie pour la psychiatrie, divisée en deux modules : les facteurs sociaux et environnementaux des troubles mentaux, et les troubles mentaux.
L'utilisation d'OntoPsychia associée à des outils dédiés permettra la prise en compte des facteurs sociaux et environnementaux, la représentation de la comorbidité et une proposition de consensus autour des catégories descriptives des troubles psychiatriques.
Dans un premier temps, nous avons développé les deux modules ontologiques selon deux méthodes différentes.
La première propose une analyse de comptes rendus d'hospitalisation, tandis que la deuxième propose un alignement de différentes classifications psychiatriques, pour répondre au besoin de consensus.
Dans un deuxième temps, nous avons développé un cadre méthodologique pour valider la structure et la sémantique des ontologies.
La communication multimodale, qui est primordiale dans les relations interpersonnelles, reste encore très limitée dans les interfaces homme-machine actuelles.
Parmi les différentes modalités qui ont été adoptées par les recherches en interaction homme-machine, la modalité posturale a été moins explorée que d'autres modalités comme la parole ou les expressions faciales.
Les postures corporelles sont pourtant indispensables pour interpréter et situer l'interaction entre deux personnes, que ce soit en termes de contexte spatial ou de contexte social.
Il manque cependant des modèles informatiques reliant ces médias et modalités aux fonctions de communication pertinentes dans les interactions interpersonnelles comme celles liées à l'espace ou aux émotions.
L'objectif de cette thèse est de concevoir un premier modèle informatique permettant d'exploiter les postures dans les interactions homme-machine.
Comment spécifier les comportements posturaux de personnages virtuels ?
L'approche proposée consiste dans un premier temps à prendre comme point de départ des corpus vidéo filmés dans différentes situations.
Nous avons défini un schéma de codage pour annoter manuellement les informations posturales à différents niveaux d'abstraction et pour les différentes parties du corps.
Ces représentations symboliques ont été exploitées pour effectuer des analyses des relations spatiales et temporelles entre les postures exprimées par deux interlocuteurs.
Ces représentations symboliques de postures ont été utilisées dans un deuxième temps pour simuler des expressions corporelles de personnages virtuels.
Nous nous sommes intéressés à un composant des émotions particulièrement pertinent pour les études sur les postures : les tendances à l'action.
Enfin, dans un troisième temps, les expressions corporelles d'un personnage virtuel ont été conçues dans une application mixte faisant intervenir un personnage virtuel et un utilisateur dans un cadre d'interaction ambiante.
Les postures et gestes du personnage virtuel ont été utilisées pour aider l'utilisateur à localiser des objets du monde réel.
L'objectif de cette thèse est de montrer les différentes facettes de l'annotation de corpus dans la langue arabe.
Nous présentons nos travaux scientifiques sur l'annotation de corpus et sur la création de ressources lexicales dans la langue arabe.
D'abord, nous discutons des méthodes, des difficultés linguistiques, des guides d'annotation, de l'optimisation de l'effort d'annotation, ainsi que de l'adaptation à la langue arabe de procédures d'annotation existantes.
Ensuite, nous montrons la complémentarité entre les différentes couches d'annotation.
Enfin, nous illustrons l'importance de ces travaux pour le traitement automatique des langues en illustrant quelques exemples de ressources et d'applications.
Depuis l'invention du PageRank par Google pour les requêtes Web à la fin des années 1990, les algorithmes de graphe font partie de notre quotidien.
Au milieu des années 2000, l'arrivée des réseaux sociaux a amplifié ce phénomène, élargissant toujours plus les cas d'usage de ces algorithmes.
Les relations entre entités peuvent être de multiples sortes : relations symétriques utilisateur-utilisateur pour Facebook ou LinkedIn, relations asymétriques follower-followee pour Twitter, ou encore, relations bipartites utilisateur-contenu pour Netflix ou Amazon.
Toutes soulèvent des problèmes spécifiques et les applications sont nombreuses : calcul de centralité pour la mesure d'influence, le partitionnement de nœuds pour la fouille de données, la classification de nœuds pour les recommandations ou l'embedding pour la prédiction de liens en sont quelques exemples.
En parallèle, les conditions d'utilisation des algorithmes de graphe sont devenues plus contraignantes.
D'une part, les jeux de données toujours plus gros avec des millions d'entités et parfois des milliards de relations limite la complexité asymptotique des algorithmes pour les applications industrielles.
D'autre part, dans la mesure où ces algorithmes influencent nos vies, les exigences d'explicabilité et d'équité dans le domaine de l'intelligence artificielle augmentent.
L'Union européenne a par exemple publié un guide de conduite pour une IA fiable.
Ceci implique de pousser encore plus loin l'analyse des modèles actuels, voire d'en proposer de nouveaux.
Cette thèse propose des réponses ciblées via l'analyse d'algorithmes classiques, mais aussi de leurs extensions et variantes, voire d'algorithmes originaux.
La capacité à passer à l'échelle restant un critère clé.
Dans le sillage de ce que le projet Scikit-learn propose pour l'apprentissage automatique sur données vectorielles, nous estimons qu'il est important de rendre ces algorithmes accessibles au plus grand nombre et de démocratiser la manipulation de graphes.
Nous avons donc développé un logiciel libre, Scikit-network, qui implémente et documente ces algorithmes de façon simple et efficace.
Grâce à cet outil, nous pouvons explorer plusieurs tâches classiques telles que l'embedding de graphe, le partitionnement, ou encore la classification semi-supervisée.
L'une des capacités humaines fondamentales est la capacité d'interpréter des symboles.
Malgré plusieurs décennies de travaux en neuropsychologique et neuroimagerie sur le substrat cognitif et neuronal des représentations sémantiques, de nombreuses questions restent sans réponse.
Dans la première partie, nous passons en revue les différentes positions théoriques concernant les corrélats cognitifs et neuraux des représentations sémantiques.
De plus, nous proposons une distinction opérationnelle entre les dimensions moto-perceptives (c'est-à-dire les attributs des objets auxquels les mots se réfèrent perçus par les sens) et conceptuelles (c'est-à-dire l'information construite par l'intégration des multiples caractéristiques perceptives).
Dans la deuxième partie, nous présentons les résultats des études menées afin d'étudier l'automaticité de la récupération, l'organisation topographique et la dynamique temporelle des dimensions moto-perceptives et conceptuelles de la signification des mots.
En particulier, ils soulignent l'importance d'une intégration fructueuse entre les théories cognitives et les méthodes statistiques avancées afin d'éclairer les mystères entourant les représentations sémantiques.
Les représentations des mots sont à la base du plupart des systèmes modernes pour le traitement automatique du langage, fournissant des résultats compétitifs.
Cependant, d'importantes questions se posent concernant les défis auxquels ils sont confrontés pour faire face aux phénomènes complexes du langage naturel et leur capacité à saisir la variabilité du langage naturel.
Pour mieux gérer les phénomènes complexes du langage, de nombreux travaux ont été menées pour affiner les représentations génériques de mots ou pour créer des représentations spécialisées.
Bien que cela puisse aider à distinguer la similarité sémantique des autres types de relations sémantiques, il peut ne pas suffire de modéliser certains types de relations, telles que les relations logiques d'implication ou de contradiction.
La première partie de la thèse étudie l'encodage de la notion d'implication textuelle dans un espace vectoriel en imposant l'inclusion d'information.
Des opérateurs d'implication sont ensuite développées et le cadre proposé peut être utilisé pour réinterpréter un modèle existant de la sémantique distributionnelle.
Des évaluations sont fournies sur la détection d'hyponymie en tant que une instance d'implication lexicale.
Un autre défi concerne la variabilité du langage naturel et la nécessité de désambiguïser les unités lexicales en fonction du contexte dans lequel elles apparaissent.
Les représentations génériques de mots ne réussissent pas à elles seules, des architectures différentes étant généralement utilisées pour aider à la désambiguïsation.
Étant donné que les représentations de mots sont construites à partir de statistiques de cooccurrence sur de grands corpus et qu'elles reflètent ces statistiques, elles fournissent une seule représentation pour un mot donné, malgré ses multiples significations.
Même dans le cas de mots monosémiques, cela ne fait pas la distinction entre les différentes utilisations d'un mot en fonction de son contexte.
Dans ce sens, on pourrait se demander s'il est possible d'exploiter directement les informations linguistiques fournies par le contexte d'un mot pour en ajuster la représentation.
Ces informations seraient-elles utiles pour créer une représentation enrichie du mot dans son contexte ?
Et si oui, des informations de nature syntaxique peuvent-elles aider au processus ou le contexte local suffit ?
Dans la deuxième partie de la thèse, nous étudions une façon d'incorporer la connaissance contextuelle dans les représentations de mots eux-mêmes, en exploitant les informations provenant de l'analyse de dépendance de phrase ainsi que les informations de voisinage local.
Nous proposons des représentations de mots contextualisées sensibles à la syntaxe (SATokE) qui capturent des informations linguistiques spécifiques et encodent la structure de la phrase dans leurs représentations.
Cela permet de passer des représentations de type générique (invariant du contexte) à des représentations spécifiques (tenant compte du contexte).
Alors que la syntaxe était précédemment considérée pour les représentations de mots, ses avantages n'ont peut-être pas été entièrement évalués au-delà des modèles qui exploitent ces informations à partir de grands corpus.
Les représentations obtenues sont évaluées sur des tâches de compréhension du langage naturel : classification des sentiments, détection de paraphrases, implication textuelle et analyse du discours.
Nous démontrons empiriquement la supériorité de ces représentations par rapport aux représentations génériques et contextualisées des mots existantes.
Le travail proposé dans la présente thèse contribue à la recherche dans le domaine de la modélisation de phénomènes complexes tels que l'implication textuelle, ainsi que de la variabilité du langage par le biais de la proposition de représentations contextualisés.
Il s'agit dans le présent projet d'initier la langue kabyle au domaine du traitement automatique des langues naturelles (TALN) en la dotant d'une base de données, sur le logiciel NooJ, permettant la reconnaissance des unités linguistiques d'un corpus écrit.
Le travail est devisé en quatre parties.
Dans la première nous avons donné un aperçu historique de la linguistique formelle et présenté le domaine du TALN, le logiciel NooJ et les unités linguistiques traitées.
La deuxième est consacrée à la description de processus suivi dans le traitement et l'intégration des verbes dans NooJ.
Nous avons construit un dictionnaire contenant 4508 entrées et 8762 dérivés et des modèles de flexion pour chaque type d'entrée.
Dans la troisième nous avons expliqué le traitement des noms et des autres unités.
Nous avons, pour les noms, construit un dictionnaire (3508 entrées et 501 dérivés) que nous avons reliés à leurs modèles de flexion et pour les autres unités (870 unités dont, adverbes, prépositions, conjonctions, interrogatifs, pronoms personnels, etc.), il s'agit seulement de listes (sans flexion).
Chacune de ces deux parties (deuxième et troisième) est complétée par des exemples d'applications sur un texte, chose qui nous a permis de voir, à l'aide des annotations, les différents types d'ambiguïtés.
Dans la dernière partie, après avoir dégagé une liste de différents types d'amalgame, nous avons essayé de décrire, à l'aide de quelques exemples de grammaire syntaxiques, l'étape de la désambiguïsation.
Le traitement des entités nommées fait aujourd'hui figure d'incontournable en Traitement Automatique des Langues.
Fort de ce succès, le traitement des entités nommées s'oriente désormais vers de nouvelles perspectives, avec la désambiguïsation et une annotation enrichie de ces unités.
Ces nouveaux défis rendent cependant d'autant plus cruciale la question du statut théorique des entités nommées, lequel n'a guère été discuté jusqu'à aujourd'hui.
Deux axes de recherche ont été investis durant ce travail de thèse avec, d'une part, la proposition d'une définition des entités nommées et, d'autre part, des méthodes de désambiguïsation.
A la suite d'un état des lieux de la tâche de reconnaissance de ces unités, il fut nécessaire d'examiner, d'un point de vue méthodologique, comment aborder la question de la définition les entités nommées.
La démarche adoptée invita à se tourner du côté de la linguistique (noms propres et descriptions définies) puis du côté du traitement automatique, ce parcours visant au final à proposer une définition tenant compte tant des aspects du langage que des exigences des systèmes informatiques.
La suite du mémoire rend compte d'un travail davantage expérimental, avec l'exposé d'une méthode d'annotation fine tout d'abord, de résolution de métonymie enfin.
Une recherche consacrée à l'étymologie et à l'histoire de la terminologie linguistique permettra un gain important en connaissances historico-étymologiques.
Ce gain dans la description de termes scientifiques particuliers a des répercussions plus générales : une meilleure connaissance des processus de formation du vocabulaire de la linguistique, et des indications sur les conséquences pour la sous-discipline étymologique elle-même d'un progrès massif dans la description d'une part significative du lexique (les internationalismes savants).
La thèse contribue aussi à un progrès méthodologique dans un secteur faible de la science étymologique, progrès d'autant plus significatif que celui-ci est crucial pour notre connaissance du fonctionnement actuel de la création lexicale en français.
Cette thèse présente la particularité de se situer sur plusieurs axes de travail, couvrant la lexicologie (et la lexicographie, car elle contient une partie consacrée à des articles lexicographiques), la terminologie (le sujet de ces articles étant les termes de la linguistique) et l'étymologie.
En outre, la part philologique de ce travail est importante, car l'analyse des débuts de l'existence des termes étudiés nous a amenée à analyser des textes de diverses natures et de périodes différentes.
Nous exploitons ensuite les nouvelles données offertes par nos articles lexicographiques dans le but de porter un regard synthétique sur la constitution de la terminologie linguistique française.
L'objectif principal de cette thèse est le développement d'algorithmes événementiels pour la détection et le suivi d'objets.
Ces algorithmes sont spécifiquement conçus pour travailler avec une sortie produite par des caméras neuromorphiques.
Ce type de caméras sont un nouveau type de capteurs bio inspirés, dont le principe de fonctionnement s'inspire de la rétine : chaque pixel est indépendant et génère des événements de manière asynchrone lorsqu'un changement de luminosité suffisamment important est détecté à la position correspondante du plan focal.
Cette nouvelle façon d'encoder l'information visuelle requiert de nouvelles méthodes pour la traiter.
Le système mécanique virtuel résultant est mis à jour pour chaque événement.
Le chapitre suivant présente un algorithme de détection de lignes et de segments, pouvant constituer une caractéristique (feature) événementielle de bas niveau.
Ensuite, deux méthodes événementielles pour l'estimation de la pose 3D sont présentées.
Le premier de ces algorithmes 3D est basé sur l'hypothèse que l'estimation de la pose est toujours proche de la position réelle, et requiert donc une initialisation manuelle.
Le deuxième de ces algorithmes 3D est conçu pour surmonter cette limitation.
Toutes les méthodes présentées mettent à jour l'estimation de la position (2D ou 3D) pour chaque événement.
Cette thèse porte sur l'inversion acoustique-articulatoire, c'est-à-dire la récupération des mouvements des articulateurs de la parole à partir du signal sonore.
Nous présentons dans ce mémoire une évolution importante des méthodes de tabulation à codebooks utilisant une table de correspondants acoustique-articulatoire précalculée à l'aide d'un modèle de synthèse acoustique.
En dehors de la méthode d'inversion proprement dite, nous proposons également l'introduction de deux types de contraintes liées au contexte d'élocution : des contraintes phonétiques génériques, issues de l'analyse par des experts humains de l'invariance articulatoire des voyelles, et des contraintes visuelles, en l'occurrence des contraintes obtenues automatiquement à partir de l'enregistrement et l'analyse d'images en stéréovision du locuteur.
Ces dernières années ont connu un regain d'intérêt pour l'utilisation des graphes comme moyen fiable de représentation et de modélisation des données, et ce, dans divers domaines de l'informatique.
En particulier, pour les grandes masses de données, les graphes apparaissent comme une alternative prometteuse aux bases de données relationnelles.
Plus particulièrement, le recherche de sous-graphes s'avère être une tâche cruciale pour explorer ces grands jeux de données.
Dans cette thèse, nous étudions deux problématiques principales.
Ce problème vise à rechercher les k-meilleures correspondances (top-k) d'un graphe motif dans un graphe de données.
Pour cette problématique, nous introduisons un nouveau modèle de détection de motifs de graphe nommé la Simulation Relaxée de Graphe (RGS), qui permet d'identifier des correspondances de graphes avec un certain écart (structurel) et ainsi éviter le problème de réponse vide.
Ensuite, nous formalisons et étudions le problème de la recherche des k-meilleures réponses suivant deux critères, la pertinence (la meilleure similarité entre le motif et les réponses) et la diversité (la dissimilarité entre les réponses).
Nous considérons également le problème des k-meilleures correspondances diversifiées et nous proposons une fonction de diversification pour équilibrer la pertinence et la diversité.
En outre, nous développons des algorithmes efficaces basés sur des stratégies d'optimisation en respectant le modèle proposé.
Notre approche est efficiente en terme de temps d'exécution et flexible en terme d'applicabilité.
L'analyse de la complexité des algorithmes et les expérimentations menées sur des jeux de données réelles montrent l'efficacité des approches proposées.
Dans un second temps, nous abordons le problème de recherche agrégative dans des documents XML.
Dans un premier temps nous présentons la motivation derrière ce paradigme de recherche agrégative et nous expliquons les gains potentiels par rapport aux méthodes classiques de requêtage.
Ensuite nous proposons une nouvelle approche qui a pour but de construire, dans la mesure du possible, une réponse cohérente et plus complète en agrégeant plusieurs résultats provenant de plusieurs sources de données.
Les expérimentations réalisées sur plusieurs ensembles de données réelles montrent l'efficacité de cette approche en termes de pertinence et de qualité de résultat.
Les problèmes multi-objectifs se posent dans plusieurs scénarios réels dans le monde où on doit trouver une solution optimale qui soit un compromis entre les différents objectifs en compétition.
On étudie deux méthodes d'apprentissage multi-objectif en détail.
Dans la première méthode, on étudie le problème de trouver le classifieur optimal pour réaliser des mesures de performances multivariées.
Dans la seconde méthode, on étudie le problème de classer des informations diverses dans les missions de recherche des informations.
Ensuite, nous avons constitué un corpus à partir de traductions d'experts métier et nous l'avons passé en revue pour renforcer l'analyse des différences.
La différence la plus flagrante est l'utilisation de la traduction automatique (TA) ainsi que le contexte de production des traductions.
En étudiant les technologies de TA actuelles, nous constatons qu'elles permettent soit une post-édition en langue cible après le processus de traduction, soit une pré-édition en langue source avant le processus de traduction.
Nous suggérons de tirer profit de la situation inédite de rédacteur traduisant, pour utiliser l'expertise du rédacteur pendant le processus de traduction et de développer une fonctionnalité de TA permettant une édition en cours de processus.
La présente thèse traite de l'analyse de scènes extraites d'environnements sonores, résultat auditif du mélange de sources émettrices distinctes et concomitantes.
Ouvrant le champ des sources et des recherches possibles au-delà des domaines plus spécifiques que sont la parole ou la musique, l'environnement sonore est un objet complexe.
Son analyse, le processus par lequel le sujet lui donne sens, porte à la fois sur les données perçues et sur le contexte de perception de ces données.
Tant dans le domaine de la perception que de l'apprentissage machine, toute expérience suppose un contrôle fin de l'expérimentateur sur les stimuli proposés.
Néanmoins, la nature de l'environnement sonore nécessite de se placer dans un cadre écologique, c'est à dire de recourir à des données réelles, enregistrées, plutôt qu'à des stimuli de synthèse.
Conscient de cette problématique, nous proposons un modèle permettant de simuler, à partir d'enregistrements de sons isolés, des scènes sonores dont nous maîtrisons les propriétés structurelles -- intensité, densité et diversité des sources.
Appuyé sur les connaissances disponibles sur le système auditif humain, le modèle envisage la scène sonore comme un objet composite, une somme de sons sources.
Le premier concerne la perception, et la notion d'agrément perçu dans des environnements urbains.
L'usage de données simulées nous permet d'apprécier finement l'impact de chaque source sonore sur celui-ci.
Le deuxième concerne la détection automatique d'événements sonores et propose une méthodologie d'évaluation des algorithmes mettant à l'épreuve leurs capacités de généralisation.
Ce travail de recherche vise à étudier les usages linguistiques non standard de certains scripteurs peu lettrées pendant la Grande Guerre, à partir de leurs correspondances privées.
Les applications d'analyse de données apportent des améliorations fondamentales dans de nombreux domaines tels que les sciences, la santé et la sécurité.
Cela a stimulé la croissance des volumes de données (le déluge du Big Data).
Pour extraire des informations utiles à partir de cette quantité énorme d'informations, différents modèles de traitement des données ont émergé tels que MapReduce, Hadoop, et Spark.
Les traitements Big Data sont traditionnellement exécutés à grande échelle (les systèmes HPC et les Clouds) pour tirer parti de leur puissance de calcul et de stockage.
Habituellement, ces plateformes à grande échelle sont utilisées simultanément par plusieurs utilisateurs et de multiples applications afin d'optimiser l'utilisation des ressources.
Bien qu'il y ait beaucoup d'avantages à partager de ces plateformes, plusieurs problèmes sont soulevés dès lors qu'un nombre important d'utilisateurs et d'applications les utilisent en même temps, parmi lesquels la gestion des E / S et des défaillances sont les principales qui peuvent avoir un impact sur le traitement efficace des données.
Nous nous concentrons tout d'abord sur les goulots d'étranglement liés aux performances des E/S pour les applications Big Data sur les systèmes HPC.
Nous commençons par caractériser les performances des applications Big Data sur ces systèmes.
Nous identifions les interférences et la latence des E/S comme les principaux facteurs limitant les performances.
Ensuite, nous nous intéressons de manière plus détaillée aux interférences des E/S afin de mieux comprendre les causes principales de ce phénomène.
De plus, nous proposons un système de gestion des E/S pour réduire les dégradations de performance que les applications Big Data peuvent subir sur les systèmes HPC.
Deuxièmement, nous nous concentrons sur l'impact des défaillances sur la performance des applications Big Data en étudiant la gestion des pannes dans les clusters MapReduce partagés.
Nous présentons un ordonnanceur qui permet un recouvrement rapide des pannes, améliorant ainsi les performances des applications Big Data.
Le contexte de la thèse se situe dans la perspective de travaux sur l'interprétation automatisée de manifestations complexes de faits de discours non saisissables par les méthodes actuelles de l'analyse de discours (AD) dans des données issues de transcription d'interviews politiques.
Il se focalise sur le mécanisme linguistique de la « nomination » , en lien avec les concepts de dénomination, désignation, référenciation.
En particulier, les sorties des reconnaissances d'entités et de coréférences pourront être exploitées, afin de déterminer leur apport pour un système expérimental de focalisé sur les nominations.
Un retour sera fait à chaque traitement TAL afin d'évaluer son apport dans la reconnaissance des nominations, dans une optique d'intégration aux outils traditionnels de l'AD.
Un des enjeux de cette thèse est aussi de proposer un système de classification pour l'entreprise Reticular afin de qualifier différents acteurs de la vie politique.
Ce sujet de thèse porte sur la traduction automatique de contenus générés par les utilisateur (par exemple sur les réseaux sociaux).
Il remet en cause les méthodes de traduction de l'état de l'art en soulevant deux défis scientifiques : 1) la nécessité de dépasser les limites d'une traduction phrase par phrase en prenant en compte le contexte de l'énoncé et 2) le caractère très bruité des énoncés qui contrairement aux textes journalistiques généralement considérés en TAL sont extrêmement diversifiés, truffés d'abréviations, de fautes d'orthographe ou typographiques et d'erreurs grammaticales.
Cette thèse s'inscrit dans le cadre de la recherche d'informations sur le Web à l'aide de méthodes inspirées des recherches en informatique linguistique du laboratoire LaLICC.
Notre travail avait pour but de développer un outil qui permet d'assister de manière interactive, lors d'une session de recherche, un utilisateur souhaitant collecter des informations disponibles sur le Web sur une notion ou un sujet donné.
L'idée fondamentale mise en œuvre dans l'outil réalisé, appelé RAP, a consisté à orienter la recherche selon un ou plusieurs points de vue prédéfinis qui permettent de satisfaire d'une manière graduelle les besoins informationnels de l'utilisateur.
Conceptuellement, une partie importante de notre travail a consisté à étudier la manière de caractériser la notion de besoin d'un utilisateur qui constitue le fondement intuitif sur lequel repose la notion de points de vue.
Pour cela, les connaissances linguistiques sur lesquelles nous nous sommes appuyée nous ont permis de ne plus voir la notion de besoin comme étant nécessairement liée à une communauté d'utilisateurs particulière.
Nos réflexions nous ont alors amené à poser les notions de besoin informationnel élémentaire ou complexe comme cadre théorique de notre recherche.
A ces besoins correspondent les points de vue que l'utilisateur peut sélectionner pour orienter la recherche d'informations.
Techniquement, orienter la recherche selon un point de vue revient à reformuler la requête utilisateur en y intégrant les marqueurs linguistiques relatifs au point de vue choisi, par exemple celui de la Causalité ou celui de la Citation.
La reformulation a alors pour but d'une part, de réduire de façon notable le bruit, et d'autre part, de cibler des pages Web possédant un contenu sémantique riche.
La réalisation des points de vue par cette technique de reformulation implique l'utilisation de marqueurs linguistiques issus des travaux de l'équipe LaLICC sur le filtrage sémantique des textes.
Chaque classe de marqueurs relative au point de vue choisi intervient dans le processus de reformulation des requêtes de l'utilisateur à travers la technique de reformulation que nous avons développée, ensuite dans l'extraction des parties, paragraphes ou segments textuels du document où la manifestation textuelle de ce point de vue est détectée, aidant ainsi l'utilisateur à mieux sélectionner les pages Web intéressantes parmi les pages résultats du moteur de recherche consulté.
L'ensemble de la démarche a été concrétisé par la construction de l'outil RAP écrit en Java et comprenant une Interface Homme-Machine conviviale, dans lequel 27 points de vue ont été implémentés découlant des différentes approches de six points de vue principaux : Causalité, Relations descriptives, Citation, Thème/Position, Problème/Solution, Acteurs.
Analyse probabiliste est l'un des domaines de recherche les plus attractives en langage naturel En traitement.
Analyseurs probabilistes succès actuels nécessitent de grandes treebanks qui Il est difficile, prend du temps et coûteux à produire.
Par conséquent, nous avons concentré notre l'attention sur des approches moins supervisés.
Nous avons proposé deux catégories de solution : l'apprentissage actif et l'algorithme semi-supervisé.
Stratégies d'apprentissage actives permettent de sélectionner les échantillons les plus informatives pour annotation.
La plupart des stratégies d'apprentissage actives existantes pour l'analyse reposent sur la sélection phrases incertaines pour l'annotation.
Nous montrons dans notre recherche, sur quatre différents langues (français, anglais, persan, arabe), que la sélection des phrases complètes ne sont pas une solution optimale et de proposer un moyen de sélectionner uniquement les sous-parties de phrases.
Comme nos expériences ont montré, certaines parties des phrases ne contiennent aucune utiles information pour la formation d'un analyseur, et en se concentrant sur les sous-parties incertains des phrases est une solution plus efficace dans l'apprentissage actif.
Nous nous intéressons dans cette thèse aux systèmes de communication homme-machine multimodale qui utilisent les modes suivants : la parole, le geste et le visuel.
L'usager communique avec le système par un énoncé oral en langue naturelle et/ou un geste.
Dans sa requête, encodée sur les différentes modalités, l'usager exprime son but et désigne des objets (référents) nécessaires à la réalisation de ce but.
Le système doit identifier de manière précise et non ambiguë ces objets désignés.
Les principaux aspects de la réalisation consistent en les modélisations du traitement de la langue naturelle dans le contexte de la parole, du traitement du geste et du contexte visuel (utilisation de la saillance visuelle) en prenant en compte les difficultés inhérentes en contexte de la communication multimodale : erreur de reconnaissance de la parole, ambiguïté de la langue naturelle, imprécision du geste due à la performance de l'usager, ambiguïté dans la désignation due à la perception des objets affichés ou à la topologie de l'affichage.
Pour l'interprétation complète de la requête nous proposons une méthode de fusion/vérification des résultats des traitements de chaque modalité pour trouver les objets désignés par l'usager.
La reconnaissance automatique des émotions dans la parole est un sujet de recherche relativement récent dans le domaine du traitement de la parole, puisqu'il est abordé depuis une dizaine d'années environs.
Ce sujet fait de nos jours l'objet d'une grande attention, non seulement dans le monde académique mais aussi dans l'industrie, grâce à l'augmentation des performances et de la fiabilité des systèmes.
Les premiers travaux étaient fondés sur des donnés jouées par des acteurs, et donc non spontanées.
Même aujourd'hui, la plupart des études exploitent des séquences pré-segmentées d'un locuteur unique et non une communication spontanée entre plusieurs locuteurs.
Cette méthodologie rend les travaux effectués difficilement généralisables pour des informations collectées de manière naturelle.
Les travaux entrepris dans cette thèse se basent sur des conversations de centre d'appels, enregistrés en grande quantité et mettant en jeu au minimum 2 locuteurs humains (un client et un agent commercial) lors de chaque dialogue.
Notre but est la détection, via l'expression émotionnelle, de la satisfaction client.
Dans une première partie nous présentons les scores pouvant être obtenus sur nos données à partir de modèles se basant uniquement sur des indices acoustiques ou lexicaux.
Nous montrons que pour obtenir des résultats satisfaisants une approche ne prenant en compte qu'un seul de ces types d'indices ne suffit pas.
Nous proposons pour palier ce problème une étude sur la fusion d'indices de types acoustiques, lexicaux et syntaxico-sémantiques.
Nous montrons que l'emploi de cette combinaison d'indices nous permet d'obtenir des gains par rapport aux modèles acoustiques même dans les cas ou nous nous basons sur une approche sans pré-traitements manuels (segmentation automatique des conversations, utilisation de transcriptions fournies par un système de reconnaissance de la parole).
Dans une seconde partie nous remarquons que même si les modèles hybrides acoustiques/linguistiques nous permettent d'obtenir des gains intéressants la quantité de données utilisées dans nos modèles de détection est un problème lorsque nous testons nos méthodes sur des données nouvelles et très variées (49h issus de la base de données de conversations).
Pour remédier à ce problème nous proposons une méthode d'enrichissement de notre corpus d'apprentissage.
Ces ajouts nous permettent de doubler la taille de notre ensemble d'apprentissage et d'obtenir des gains par rapport aux modèles de départ.
Enfin, dans une dernière partie nous choisissons d'évaluées nos méthodes non plus sur des portions de dialogues comme cela est le cas dans la plupart des études, mais sur des conversations complètes.
Nous utilisons pour cela les modèles issus des études précédentes (modèles issus de la fusion d'indices, des méthodes d'enrichissement automatique) et ajoutons 2 groupes d'indices supplémentaires :
i) Des indices « structurels » prenant en compte des informations comme la durée de la conversation, le temps de parole de chaque type de locuteurs.
ii) des indices « dialogiques » comprenant des informations comme le thème de la conversation ainsi qu'un nouveau concept que nous nommons « implication affective » .
Celui-ci a pour but de modéliser l'impact de la production émotionnelle du locuteur courant sur le ou les autres participants de la conversation.
Nous montrons que lorsque nous combinons l'ensemble de ces informations nous arrivons à obtenir des résultats proches de ceux d'un humain lorsqu'il s'agit de déterminer le caractère positif ou négatif d'une conversation.
La vision de l'informatique ubiquitaire permettant de construire des espaces intelligents interactifs dans l'environnement physique passe, peu à peu, du domaine de la recherche à la réalité.
La capacité de calcul ne se limite plus à l'ordinateur personnel mais s'intègre dans de multiples appareils du quotidien, et ces appareils deviennent, grâce à plusieurs interfaces, capables de communiquer directement les uns avec les autres ou bien de se connecter à Internet.
Dans cette thèse, nous nous sommes intéressés à un type d'environnement cible de l'informatique ubiquitaire qui forme ce que nous appelons un réseau hybride à connexions intermittentes (ICHN).
Un ICHN est un réseau composé de deux parties : une partie fixe et une partie mobile.
La partie fixe est constituée de plusieurs infostations fixes (potentiellement reliées entre elles avec une infrastructure fixe, typiquement l'Internet).
La partie mobile, quant à elle, est constituée de smartphones portés par des personnes nomades.
Tandis que la partie fixe est principalement stable, la partie mobile pose un certain nombre de défis propres aux réseaux opportunistes.
En effet, l'utilisation de moyens de communication à courte portée couplée à des déplacements de personnes non contraints et à des interférences radio induit des déconnexions fréquentes.
Le concept du "store, carry and forward" est alors habituellement appliqué pour permettre la communication sur l'ensemble du réseau.
Avec cette approche, un message peut être stocké temporairement sur un appareil avant d'être transféré plus tard quand les circonstances sont plus favorables.
Ainsi, n'importe quel appareil devient un relai de transmission opportuniste qui permet de faciliter la propagation d'un message dans le réseau.
Dans ce contexte, la fourniture de services est particulièrement problématique, et exige de revisiter les composants principaux du processus de fourniture, tels que la découverte et l'invocation de service, en présence de ruptures de connectivité et en l'absence de chemins de bout en bout.
Cette thèse aborde les problèmes de fourniture de service sur l'ensemble d'un ICHN et propose des solutions pour la découverte de services, l'invocation et la continuité d'accès.
En ce qui concerne le défi de la découverte de services, nous proposons TAO-DIS, un protocole qui met en œuvre un mécanisme automatique et rapide de découverte de services.
TAO-DIS tient compte de la nature hybride d'un ICHN et du fait que la majorité des services sont fournis par des infostations.
Il permet aux utilisateurs mobiles de découvrir tous les services dans l'environnement afin d'identifier et de choisir les plus intéressants.
Pour permettre aux utilisateurs d'interagir avec les services découverts, nous introduisons TAO-INV.
TAO-INV est un protocole d'invocation de service spécialement conçu pour les ICHN.
Il se fonde sur un ensemble d'heuristiques et de mécanismes qui assurent un acheminement efficace des messages (des requêtes et des réponses de services) entre les infostations fixes et les clients mobiles tout en conservant un surcoût et des temps de réponses réduits.
Puisque certaines infostations dans le réseau peuvent être reliées entre elles, nous proposons un mécanisme de continuité d'accès (handover) qui modifie le processus d'invocation pour réduire les délais de délivrance.
Dans sa définition, il est tenu compte de la nature opportuniste de la partie mobile de l'ICHN.
Nous avons mené diverses expérimentations pour évaluer nos solutions et les comparer à d'autres protocoles conçus pour des réseaux ad hoc et des réseaux opportunistes.
Les résultats obtenus tendent à montrer que nos solutions surpassent ces autres protocoles, notamment grâce aux optimisations que nous avons développées pour les ICHN.
À notre avis, construire des protocoles spécialisés qui tirent parti des techniques spécifiquement conçues pour les ICHN est une approche à poursuivre en complément des recherches sur des protocoles de communication polyvalents
Cette thèse présente des résultats appartenant aux trois thèmes fondamentaux de la cryptographie à clé publique : l'intégrité, l'authentification et la confidentialité.
Au sein de chaque thème nous concevons des nouvelles primitives et améliorons des primitives existantes.
Le premier chapitre, dédié à l'intégrité, introduit une preuve non-interactive de génération appropriée de clés publiques RSA et un protocole de co-signature dans lequel tout irrespect de l'équité laisse automatiquement la partie lésée en possession d'une preuve de culpabilité incriminant la partie tricheuse.
Le second chapitre, ayant pour sujet l'authentification, montre comme une mesure de temps permet de raccourcir les engagements dans des preuves à divulgation nulle et comment des biais, introduits à dessin dans le défi, permettent d'accroitre l'efficacité de protocoles.
Ce chapitre généralise également le protocole de Fiat-Shamir à plusieurs prouveurs et décrit une fraude très sophistiquée de cartes-à-puce illustrant les dangers de protocoles d'authentification mal-conçus.
Au troisième chapitre nous nous intéressons à la confidentialité.
Nous y proposons un cryptosystème à clé publique où les hypothèses de complexité traditionnelles sont remplacées par un raffinement du concept de CAPTCHA et nous explorons l'application du chiffrement-pot-de-miel au langage naturel.
Ce travail de recherche se positionne dans la continuité de la thèse [21] effectuée au laboratoire LRI.
Dans le cadre de cette thèse, nous mettrons en place des méthodes d'extraction et d'analyse des données internes aux réseaux sociaux mais également issues d'autres sources de données en vue de leur réutilisation dans la plateforme Octopeek.
Néanmoins, la réalisation d'un tel système demeure un défi scientifique qui devra prendre en compte le volume de données, le temps d'exécution de la recherche et la sémantique des termes afin de fournir les meilleures réponses.
En effet, pour chaque information recherchée, il faudra trouver la méthode, l'algorithme optimisé qui prend en compte la rapidité de calcul, et le scoring associé.
Nous serons amenés à implémenter des algorithmes pour une utilisation à la volée et sur plusieurs personnes en parallèle.
La méthodologie de recherche mise en œuvre est une démarche empirique qui s'appuie sur l'analyse de situations de conception.
Au cours du doctorat, un protocole expérimental mis en œuvre a été dupliqué trois fois dans des laboratoires partenaires.
Cette recherche aboutit à un mémoire de thèse qui présente plusieurs contributions :
La première contribution se situe autour de la méthodologie de recherche proposée.
Enfin la troisième contribution porte sur l'analyse des interactions dans les activités de conception étudiées.
Les analyses identifient et qualifient les impacts des méthodes étudiées sur le contenu des interactions dans les phases amont de la conception.
Cette thèse est une contribution à la branche professionnelle de l'anglais de spécialité et au domaine de l'anglais comme lingua franca.
Le contexte de la recherche est le milieu de l'entreprise où les employés échangent des courriels dans le cadre de la réalisation d'actions professionnelles routinières.
Dans ce contexte, l'anglais est considéré comme une langue internationale et, dans la situation où les employés sont natifs d'autres langues que l'anglais, la lingua franca.
La première partie traite des quatre concepts fondamentaux de cette recherche : l'anglais comme langue internationale, le registre, la phraséologie et les discours professionnels.
La seconde partie présente la démarche méthodologique dont l'objectif estla constitution d'un corpus de 500 courriels professionnels à partir d'une base de données plus large que nous avons constituée lors de notre enquête de terrain dans le monde de l'entreprise.
Le corpus est tout d'abord défini selon quatre situations linguistiques que nous présentons ci-dessous : 1. scripteurs natifs et destinataires natifs 2. scripteurs natifs et destinataires non natifs 3. scripteurs non natifs et destinataires natifs 4. scripteurs non natifs et destinataires non natifs
Il est ensuite défini selon les quatre situations professionnelles suivantes : 1. achats et ventes de produits 2. management d'équipes distantes 3. administration des ressources humaines 4. résolution de problèmes techniques
A partir de ce corpus, nous menons une étude de la variation sur trois ensembles de traits linguistico-discursifs et paralinguistiques qui nous permettent d'évaluer le degré de minimalisme dans les courriels, le degré d'imbrication du texte dans le contexte ainsi que de mesurer le caractère interpersonnel et intime de ce type d'échange.
Notre étude nous mène tout d'abord à confirmer que l'analyse de registre est une approche efficace pour la caractérisation des discours ordinaires et routiniers dans les entreprises.
Elle interroge ensuite la solidité des normes et du concept de communauté de discours en présentant l'anglais en circulation sur les réseaux professionnels, éphémères et mondiaux, comme une variété fluide.
Ce travail de thèse porte sur un nouveau paradigme pour la synthèse de la parole à partir du texte, à savoir la synthèse incrémentale.
L'objectif est de délivrer la parole de synthèse au fur et à mesure de la saisie du texte par l'utilisateur, contrairement aux systèmes classiques pour lesquels la synthèse est déclenchée après la saisie d'une ou plusieurs phrases.
L'application principale visée est l'aide aux personnes présentant un trouble sévère de la communication orale, et communiquant principalement à l'aide d'un synthétiseur vocal.
Un synthétiseur vocal incrémental permettrait de fluidifier une conversation en limitant le temps que passe l'interlocuteur à attendre la fin de la saisie de la phrase à synthétiser.
Un des défi que pose ce paradigme est la synthèse d'un mot ou d'un groupe de mot avec une qualité segmentale et prosodique acceptable alors que la phrase qui le contient n'est que partiellement connue au moment de la synthèse.
Pour ce faire, nous proposons différentes adaptations des deux principaux modules d'un système de synthèse de parole à partir du texte : le module de traitement automatique de la langue naturelle (TAL) et le module de synthèse sonore.
Pour le TAL en synthèse incrémentale, nous nous sommes intéressé à l'analyse morpho-syntaxique, qui est une étape décisive pour la phonétisation et la détermination de la prosodie cible.
Nous décrivons un algorithme d'analyse morpho-syntaxique dit "à latence adaptative".
Ce dernier estime en ligne si une classe lexicale (estimée à l'aide d'un analyseur morpho-syntaxique standard basé sur l'approche n-gram), est susceptible de changer après l'ajout par l'utilisateur d'un ou plusieurs mots.
Si la classe est jugée instable, alors la synthèse sonore est retardée, dans le cas contraire, elle peut s'effectuer sans risque a priori de dégrader de la qualité segmentale et suprasegmentale.
Cet algorithme exploite une ensemble d'arbre de décisions binaires dont les paramètres sont estimés par apprentissage automatique sur un large corpus de texte.
Les résultats des évaluations objectives et perceptives montrent l'intérêt de la méthode proposée pour la langue française.
Enfin, nous décrivons un prototype complet qui combine les deux méthodes proposées pour le TAL et la synthèse par HMM incrémentale.
Une évaluation perceptive de la pertinence et de la qualité des groupes de mots synthétisés au fur et à mesure de la saisie montre que notre système réalise un compromis acceptable entre réactivité (minimisation du temps entre la saisie d'un mot et sa synthèse) et qualité (segmentale et prosodique) de la parole de synthèse.
S'il est indéniable que de nos jours la traduction automatique (TA) facilite la communication entre langues, et plus encore depuis les récents progrès des systèmes de TA statistiques, ses résultats sont encore loin du niveau de qualité des traductions obtenues avec des traducteurs humains.
Ce constat résulte en partie du mode de fonctionnement d'un système de TA statistique, très contraint sur la nature des modèles qu'il peut utiliser pour construire et évaluer de nombreuses hypothèses de traduction partielles avant de parvenir à une hypothèse de traduction complète.
En conséquence, de tels modèles complexes sont typiquement uniquement utilisés en TA pour effectuer le reclassement de listes de meilleures hypothèses complètes.
Bien que ceci permette dans les faits de tirer profit d'une meilleure modélisation de certains aspects des traductions, cette approche reste par nature limitée : en effet, les listes d'hypothèses reclassées ne représentent qu'une infime partie de l'espace de recherche du décodeur, contiennent des hypothèses peu diversifiées, et ont été obtenues à l'aide de modèles dont la nature peut être très différente des modèles complexes utilisés en reclassement.
Nous formulons donc l'hypothèse que de telles listes d'hypothèses de traduction sont mal adaptées afin de faire s'exprimer au mieux les modèles complexes utilisés.
Les travaux que nous présentons dans cette thèse ont pour objectif de permettre une meilleure exploitation d'informations riches pour l'amélioration des traductions obtenues à l'aide de systèmes de TA statistique.
Notre première contribution s'articule autour d'un système de réécriture guidé par des informations riches.
Des réécritures successives, appliquées aux meilleures hypothèses de traduction obtenues avec un système de reclassement ayant accès aux mêmes informations riches, permettent à notre système d'améliorer la qualité de la traduction.
L'originalité de notre seconde contribution consiste à faire une construction de listes d'hypothèses par passes multiples qui exploitent des informations dérivées de l'évaluation des hypothèses de traduction produites antérieurement à l'aide de notre ensemble d'informations riches.
Notre système produit ainsi des listes d'hypothèses plus diversifiées et de meilleure qualité, qui s'avèrent donc plus intéressantes pour un reclassement fondé sur des informations riches.
De surcroît, notre système de réécriture précédent permet d'améliorer les hypothèses produites par cette deuxième approche à passes multiples.
Notre troisième contribution repose sur la simulation d'un type d'information idéalisé parfait qui permet de déterminer quelles parties d'une hypothèse de traduction sont correctes.
Cette idéalisation nous permet d'apporter une indication de la meilleure performance atteignable avec les approches introduites précédemment si les informations riches disponibles décrivaient parfaitement ce qui constitue une bonne traduction.
Cette approche est en outre présentée sous la forme d'une traduction interactive, baptisée « pré-post-édition » , qui serait réduite à sa forme la plus simple : un système de TA statistique produit sa meilleure hypothèse de traduction, puis un humain apporte la connaissance des parties qui sont correctes, et cette information est exploitée au cours d'une nouvelle recherche pour identifier une meilleure traduction.
La RAP non native souffre encore d'une chute significative de précision.
Cette dégradation est due aux erreurs d'accent et de prononciation que produisent les locuteurs non natifs.
Les recherches que nous avons entreprises ont pour but d'atténuer l'impact des accents non natifs sur les performances des systèmes de RAP.
Nous avons proposé une nouvelle approche pour la modélisation de prononciation non native permettant de prendre en compte plusieurs accents étrangers.
Cette approche automatique utilise un corpus de parole non native et deux ensembles de modèles acoustiques : le premier ensemble représente l'accent canonique de la langue cible et le deuxième représente l'accent étranger.
Les modèles acoustiques du premier ensemble sont modifiés par l'ajout de nouveaux chemins d'états HMM.
Nous avons proposé une nouvelle approche pour la détection de la langue maternelle basée sur la détection de séquences discriminantes de phonèmes.
Par ailleurs, nous avons proposé une approche de modélisation de prononciation non native multi-accent permettant de prendre en compte plusieurs accents étrangers simultanément.
Nous avons conçu une approche automatique pour la detection des contraintes graphémiques et leur prise en compte pour l'approche de RAP non native.
En outre, Nous avons proposé trois nouvelles approches efficaces dont le but est l'accélération du calcul de vraisemblance sans dégradation de la précision.
Les systèmes de recommandation actuels ont besoin de recommander des objets pertinents aux utilisateurs (exploitation), mais pour cela ils doivent pouvoir également obtenir continuellement de nouvelles informations sur les objets et les utilisateurs encore peu connus (exploration).
Il s'agit du dilemme exploration/exploitation.
Un tel environnement s'inscrit dans le cadre de ce que l'on appelle " apprentissage par renforcement ".
Dans la littérature statistique, les stratégies de bandit sont connues pour offrir des solutions à ce dilemme.
Les contributions de cette thèse multidisciplinaire adaptent ces stratégies pour appréhender certaines problématiques des systèmes de recommandation, telles que la recommandation de plusieurs objets simultanément, la prise en compte du vieillissement de la popularité d'un objet ou encore la recommandation en temps réel.
Dans de nombreux domaines tels que l'apprentissage statistique, la recherche opérationnelle ou encore la conception de circuits, une tâche est modélisée par un jeu de paramètres que l'on cherche à optimiser pour prendre la meilleure décision possible.
Mathématiquement, le problème revient à minimiser une fonction de l'objectif recherché par des algorithmes itératifs.
Le développement de ces derniers dépend alors de la géométrie de la fonction ou de la structure du problème.
Dans une première partie, cette thèse étudie comment l'acuité d'une fonction autour de ses minima peut être exploitée par le redémarrage d'algorithmes classiques.
Les schémas optimaux sont présentés pour des problèmes convexes généraux.
Ils nécessitent cependant une description complète de la fonction, ce qui est rarement disponible.
Des stratégies adaptatives sont donc développées et prouvées être quasi-optimales.
Une analyse spécifique est ensuite conduite pour les problèmes parcimonieux qui cherchent des représentations compressées des variables du problème.
Leur géométrie conique sous-jacente, qui décrit l'acuité de la fonction de l'objectif, se révèle contrôler à la fois la performance statistique du problème et l'efficacité des procédures d'optimisation par une seule quantité.
Une seconde partie est dédiée aux problèmes d'
Ceux-ci effectuent une analyse prédictive de données à l'aide d'un large nombre d'
Des méthodes algorithmiques systématiques sont développées en analysant la géométrie induite par une partition des données.
Une analyse théorique est finalement conduite lorsque les variables sont groupées par analogie avec les méthodes parcimonieuses.
Ce mémoire traite des modèles génératifs profonds appliqués à la génération automatique de musique symbolique.
Nous nous attacherons tout particulièrement à concevoir des modèles génératifs interactifs, c'est-à-dire des modèles instaurant un dialogue entre un compositeur humain et la machine au cours du processus créatif.
En effet, les récentes avancées en intelligence artificielle permettent maintenant de concevoir de puissants modèles génératifs capables de générer du contenu musical sans intervention humaine.
En revanche, la conception d'assistants puissants, flexibles et expressifs destinés aux créateurs de contenus musicaux me semble pleine de sens.
Que ce soit dans un but pédagogique ou afin de stimuler la créativité artistique, le développement et le potentiel de ces nouveaux outils de composition assistée par ordinateur sont prometteurs.
Dans ce manuscrit, je propose plusieurs nouvelles architectures remettant l'humain au centre de la création musicale.
Les modèles proposés ont en commun la nécessité de permettre à un opérateur de contrôler les contenus générés.
Afin de rendre cette interaction aisée, des interfaces utilisateurs ont été développées ;
les possibilités de contrôle se manifestent sous des aspects variés et laissent entrevoir de nouveaux paradigmes compositionnels.
Le Web est une source proliférante d'objets multimédia, décrits dans différentes langues naturelles.
Afin d'utiliser les techniques du Web sémantique pour la recherche de tels objets (images, vidéos, etc.), nous proposons une méthode d'extraction de contenu dans des collections de textes multilingues, paramétrée par une ou plusieurs ontologies.
Le processus d'extraction est utilisé pour indexer les objets multimédia à partir de leur contenu textuel, ainsi que pour construire des requêtes formelles à partir d'énoncés spontanés.
Il est basé sur une annotation interlingue des textes, conservant les ambiguïtés de segmentation et la polysémie dans des graphes.
Cette première étape permet l'utilisation de processus de désambiguïsation “factorisés” au niveau d'un lexique pivot (de lexèmes interlingues).
Le passage d'une ontologie en paramètre du système se fait en l'alignant de façon automatique avec le lexique interlingue.
Il est ainsi possible d'utiliser des ontologies qui n'ont pas été conçues pour une utilisation multilingue, et aussi d'ajouter ou d'étendre l'ensemble des langues et leurs couvertures lexicales sans modifier les ontologies.
Un démonstrateur pour la recherche multilingue d'images, développé pour le projet ANR OMNIA, a permis de concrétiser les approches proposées.
Le passage à l'échelle et la qualité des annotations produites ont ainsi pu être évalués.
De par leur grand nombre et leur sévérité, les maladies rares (MR) constituent un enjeu de santé majeur.
Des bases de données de référence, comme Orphanet et Orphadata, répertorient les informations disponibles à propos de ces maladies.
Cependant, il est difficile pour ces bases de données de proposer un contenu complet et à jour par rapport à ce qui est disponible dans la littérature.
En effet, des millions de publications scientifiques sur ces maladies sont disponibles et leur nombre augmente de façon continue.
Cette thèse s'intéresse à l'extraction de connaissances à partir de textes et propose d'utiliser les résultats de l'extraction pour enrichir une ontologie de domaine.
Nous avons étudié trois directions de recherche : (1) l'extraction de connaissances à partir de textes, et en particulier l'extraction de relations maladie-phénotype (M-P) ; (2) l'identification d'entité nommées complexes, en particulier de phénotypes de MR ; et (3) l'enrichissement d'une ontologie en considérant les connaissances extraites à partir de texte.
Tout d'abord, nous avons fouillé une collection de résumés d'articles scientifiques représentés sous la forme graphes pour un extraire des connaissances sur les MR.
Nous nous sommes concentrés sur la complétion de la description des MR, en extrayant les relations M-P.
Cette trouve des applications dans la mise à jour des bases de données de MR telles que Orphanet.
Pour cela, nous avons développé un système appelé SPARE* qui extrait les relations M-P à partir des résumés PubMed, où les phénotypes et les MR sont annotés au préalable par un système de reconnaissance des entités nommées.
SPARE* suit une approche hybride qui combine une méthode basée sur des patrons syntaxique, appelée SPARE, et une méthode d'apprentissage automatique (les machines à vecteurs de support ou SVM).
SPARE* bénéficié à la fois de la précision relativement bonne de SPARE et du bon rappel des SVM.
Ensuite, SPARE* a été utilisé pour identifier des phénotypes candidats à partir de textes.
Pour cela, nous avons sélectionné des patrons syntaxiques qui sont spécifiques aux relations M-P uniquement.
Ensuite, ces patrons sont relaxés au niveau de leur contrainte sur le phénotype pour permettre l'identification de phénotypes candidats qui peuvent ne pas être références dans les bases de données ou les ontologies.
Ces candidats sont vérifiés et validés par une comparaison avec les classes de phénotypes définies dans une ontologie de domaine comme HPO.
Cette comparaison repose sur une modèle sémantique et un ensemble de règles de mises en correspondance définies manuellement pour cartographier un phénotype candidate extrait de texte avec une classe de l'ontologie.
Nos expériences illustrent la capacité de SPARE* à des phénotypes de MR déjà répertoriés ou complètement inédits.
Nous avons appliqué SPARE* à un ensemble de résumés PubMed pour extraire les phénotypes associés à des MR, puis avons mis ces phénotypes en correspondance avec ceux déjà répertoriés dans l'encyclopédie Orphanet et dans Orphadata ;
Enfin, nous avons appliqué les structures de patrons pour classer les MR et enrichir une ontologie préexistante.
Tout d'abord, nous avons utilisé SPARE* pour compléter les descriptions en terme de phénotypes de MR disponibles dans Orphadata.
Ensuite, nous proposons de compter et grouper les MR au regard de leur description phénotypique, et ce en utilisant les structures de patron.
Cette thèse présente une nouvelle approche de la détection automatique des frontières prosodiques et de la structure prosodique en français, basée sur une représentation théorique hiérarchique de cette structure.
Nous avons utilisé une théorie descriptive du système prosodique du français pour créer un modèle prosodique linguistique adapté au traitement automatique de la parole spontanée.
Ce modèle permet de détecter de façon automatique les frontières des groupes prosodiques et de les regrouper dans une structure hiérarchique.
La structure prosodique de chaque énoncé est ainsi représentée sous forme d'un arbre prosodique.
Nous avons démontré que ce modèle représentation était adapté pour le traitement automatique de la parole spontanée en français.
La segmentation prosodique ainsi obtenue a été comparée à la segmentation prosodique manuelle.
La pertinence de la structure prosodique a été également vérifiée manuellement.
Nous avons appliqué notre modèle à différents types de données de parole continue spontanée avec différents types de segmentations phonétiques et lexicales : segmentation manuelle ainsi que différentes segmentations automatiques, et notamment aux données segmentées par le système de reconnaissance automatique de la parole.
L'utilisation de cette segmentation a fourni une performance satisfaisante.
Nous avons également établi une corrélation entre le niveau du noeud dominant dans l'arbre prosodique et la fiabilité de la détection de la frontière correspondante.
Ainsi, il est envisageable d'enrichir la détection de frontières prosodiques en attribuant une mesure de confiance à la frontière en fonction de son niveau dans l'arbre prosodique.
Avec le développement des dispositifs de capture et d'Internet, les gens accèdent à un nombre croissant d'images.
L'évaluation de l'esthétique visuelle a des applications importantes dans plusieurs domaines, de la récupération d'image et de la recommandation à l'amélioration.
L'évaluation de la qualité esthétique de l'image vise à déterminer la beauté d'une image pour les observateurs humains.
De nombreux problèmes dans ce domaine ne sont pas bien étudiés, y compris la subjectivité de l'évaluation de la qualité esthétique, l'explication de l'esthétique et la collecte de données annotées par l'homme.
La prédiction conventionnelle de la qualité esthétique des images vise à prédire le score moyen ou la classe esthétique d'une image.
Cependant, la prédiction esthétique est intrinsèquement subjective, et des images avec des scores / classe esthétiques moyens similaires peuvent afficher des niveaux de consensus très différents par les évaluateurs humains.
Des travaux récents ont traité de la subjectivité esthétique en prédisant la distribution des scores humains, mais la prédiction de la distribution n'est pas directement interprétable en termes de subjectivité et pourrait être sous-optimale par rapport à l'estimation directe des descripteurs de subjectivité calculés à partir des scores de vérité terrain.
De plus, les étiquettes des ensembles de données existants sont souvent bruyantes, incomplètes ou ne permettent pas des tâches plus sophistiquées telles que comprendre pourquoi une image est belle ou non pour un observateur humain.
Dans cette thèse, nous proposons tout d'abord plusieurs mesures de la subjectivité, allant de simples mesures statistiques telles que l'écart type des scores, aux descripteurs nouvellement proposés inspirés de la théorie de l'information.
Nous évaluons les performances de prédiction de ces mesures lorsqu'elles sont calculées à partir de distributions de scores prédites et lorsqu'elles sont directement apprises à partir de données de vérité terrain.
Nous constatons que cette dernière stratégie donne en général de meilleurs résultats.
Nous utilisons également la subjectivité pour améliorer la prédiction des scores esthétiques, montrant que les mesures de subjectivité inspirées de la théorie de l'information fonctionnent mieux que les mesures statistiques.
Ensuite, nous proposons un ensemble de données EVA (Explainable Visual Aesthetics), qui contient 4070 images avec au moins 30 votes par image.
EVA a été collecté en utilisant une approche plus disciplinée inspirée des meilleures pratiques d'évaluation de la qualité.
Il offre également des caractéristiques supplémentaires, telles que le degré de difficulté à évaluer le score esthétique, l'évaluation de 4 attributs esthétiques complémentaires, ainsi que l'importance relative de chaque attribut pour se forger une opinion esthétique.
L'ensemble de données accessible au public devrait contribuer aux recherches futures sur la compréhension et la prédiction de l'esthétique de la qualité visuelle.
De plus, nous avons étudié l'explicabilité de l'évaluation de la qualité esthétique de l'image.
Une analyse statistique sur EVA démontre que les attributs collectés et l'importance relative peuvent être combinés linéairement pour expliquer efficacement les scores d'opinion moyenne esthétique globale.
Nous avons trouvé que la subjectivité a une corrélation limitée avec la difficulté personnelle moyenne dans l'évaluation esthétique, et la région du sujet, le niveau photographique et l'âge affectent de manière significative l'évaluation esthétique de l'utilisateur.
Ce travail vise à améliorer la compréhension des signaux sismiques dérivés des fonctions de corrélation inter-récepteur du bruit sismique, ce qui est critique pour une imagerie fiable de la Terre profonde basée sur le bruit.
La thèse comprend sept chapitres.
Le chapitre 1 introduit les connaissances de base sur le bruit sismique, de la terminologie à ses origines diverses.
Le chapitre 2 fournit une vue d'ensemble de la littérature sur l'historique et le développement de la méthode récente de corrélation de bruit, et passe en revue diverses techniques pour le prétraitement des données de bruit sismique et le post-traitement des fonctions de corrélation de bruit.
Des méthodes de traitement du bruit basées sur les statistiques et un schéma modifié pour calculer la fonction de corrélation sont développés dans ce chapitre.
Le chapitre 3 propose plusieurs techniques basées sur la transformée de Radon pour mesurer les lenteurs des champs d'ondes corrélés et analyser en termes de phases sismiques les signaux dérivés du bruit.
Le chapitre 6 discute des conditions dans lesquelles apparaissent des phases sans correspondance dans la réponse physique de la Terre qui peuvent fausser les analyses des structures profondes basées sur le bruit..
Le dernier chapitre fournit un résumé sur les contributions de cette thèse et une perspective de plusieurs travaux soit en cours soit envisagés pour le futur.
La théorie du grounding (ancrage) de Clark \&amp;
Schaefer (1989) suggère que les participants à un dialogue cherchent à atteindre la compréhension mutuelle en produisant des preuves de leur compréhension et peut alors permettre d'améliorer la robustesse des systèmes.
Les modélisations informatiques de l'ancrage qui visent à résoudre ce problème font l'objet de plusieurs simplifications ou sont trop complexes à mettre en oeuvre.
Cette modélisation a été implémentée et adjointe à un système d'interprétation symbolique classique.
L'évaluation du système a été réalisée par simulation sur corpus en générant des dialogues d'ancrage de manière artificielle entre deux instances du système.
Les résultats obtenus à l'issue de l'évaluation montrent un gain significatif de compréhension et valident en cela l'approche générale.
Dans cette thèse, nous présentons une étude sur la visualisation des résultats Web d'images sur les dispositifs nomades.
Nos principales conclusions ont été inspirées par les avancées récentes dans deux principaux domaines de recherche – la recherche d'information et le traitement automatique du langage naturel.
Tout d'abord, nous avons examiné différents sujets tels que le regroupement des résultats Web, les interfaces mobiles, la fouille des intentions sur une requête, pour n'en nommer que quelques-uns.
Ensuite, nous nous sommes concentré sur les mesures d'association lexical, les métriques de similarité d'ordre élevé, etc.
Notamment afin de valider notre hypothèse, nous avons réalisé différentes expériences avec des jeux de données spécifiques de la tâche.
De nombreuses caractéristiques sont évaluées dans les solutions proposées.
Premièrement, la qualité de regroupement en utilisant à la fois des métriques d'évaluation classiques, mais aussi des métriques plus récentes.
Deuxièmement, la qualité de l'étiquetage de chaque groupe de documents est évaluée pour s'assurer au maximum que toutes les intentions des requêtes sont couvertes.
Finalement, nous évaluons l'effort de l'utilisateur à explorer les images dans une interface basée sur l'utilisation des galeries présentées sur des dispositifs nomades.
Un chapitre entier est consacré à chacun de ces trois aspects dans lesquels les jeux de données-certains d'entre eux construits pour évaluer des caractéristiques spécifiques-sont présentés.
Comme résultats de cette thèse, nous sommes développés : deux algorithmes adaptés aux caractéristiques du problème, deux jeux de données pour les tâches respectives et un outil d'évaluation pour le regroupement des résultats d'une requête (SRC pour les sigles en anglais).
Concernant les algorithmes, Dual C-means est notre principal contribution.
Il peut être vu comme une généralisation de notre algorithme développé précédemment, l'AGK-means.
Les deux sont basés sur des mesures d'association lexical à partir des résultats Web.
Un nouveau jeu de données pour l'évaluation complète d'algorithmes SRC est élaboré et présenté.
De même, un nouvel ensemble de données sur les images Web est développé et utilisé avec une nouvelle métrique à fin d'évaluer l'effort fait pour les utilisateurs lors qu'ils explorent un ensemble d'images.
Enfin, nous avons développé un outil d'évaluation pour le problème SRC, dans lequel nous avons mis en place plusieurs mesures classiques et récentes utilisées en SRC.
Nos conclusions sont tirées compte tenu des nombreux facteurs qui ont été discutés dans cette thèse.
Cependant, motivés par nos conclusions, des études supplémentaires pourraient être développés.
Celles-ci sont discutées à la fin de ce manuscrit et notre résultats préliminaires suggère que l'association de plusieurs sources d'information améliore déjà la qualité du regroupement.
Dans l'expression de la relation concrète, ils sont généralement classés dans la catégorie des locatifs topologiques qui expriment une relation d'inclusion ou de contact entre deux entités.
Dans les emplois métaphoriques, ils peuvent tous exprimer un cadre, bien que chaque locatif ait des emplois spécifiques propres.
Ces quatre locatifs sont parfois interchangeables aussi bien pour les emplois concrets que métaphoriques.
Notre étude comparative en chinois contemporain est basée sur un corpus de textes de différents styles : écrit, oral et écrit avec des caractéristiques de l'oral.
Les statistiques d'emploi des locatifs recueillies montrent l'importance des styles de textes dans le choix des locatifs.
La pragmatique et des contextes sémantiques sont aussi des facteurs déterminants.
L'analyse diachronique réalisée sur un corpus de 26 documents représentatifs des périodes archaïque, médiévale et pré-moderne indique l'évolution de chaque locatif et la tendance d'
Cette thèse étudie le rôle des anticipations dans les cycles économiques en analysant trois types d'anticipations différentes.
Dans un premier temps, je me concentre sur une explication théorique des cycles économiques générée par des changements d'anticipations qui se révèlent auto-réalisatrices.
Ce chapitre contribue à améliorer un puzzle provenant de la littérature sunspot, soutenant ainsi une interprétation des cycles économiques basée sur les prophéties auto-réalisatrices.
Dans un deuxième temps, j'analyse empiriquement comment les annonces de la banque centrale se propagent à l'économie via la modification des croyances des acteurs du marché.
Ce chapitre montre que des annonces crédibles sur les futures politiques monétaires non conventionnelles peuvent être utilisées comme un instrument de coordination des anticipations dans un contexte de crise de la dette souveraine.
Dans un troisième temps, je m'intéresse à un concept plus large d'anticipations et étudie le pouvoir prédictif du climat politique sur la tarification du risque souverain.
Ce chapitre montre que le climat politique apporte un pouvoir prédictif supplémentaire aux spreads des obligations d'Etat, au-delà des déterminants traditionnels.
Différentes méthodologies sont utilisées dans cette thèse, notamment des analyses théoriques et empiriques, du web scraping ainsi que des méthodes d'apprentissage automatique et d'analyse textuelle.
Par ailleurs, j'exploite dans cette thèse des données innovantes provenant du réseau social Twitter.
Tous mes résultats transmettent le même message : les anticipations comptent, tant pour la recherche en économie que pour l'élaboration de politiques économiques.
Dans ces travaux de thèse nous proposons de tirer parti de la théorie des ensembles flous afin d'améliorer les interactions entre les systèmes de bases de données et les utilisateurs.
Les mécanismes coopératifs visent à aider les utilisateurs à mieux interagir avec les SGBD.
Ces mécanismes doivent faire preuve de robustesse : ils doivent toujours pouvoir proposer des réponses à l'utilisateur.
Empty set (0,00 sec) est un exemple typique de réponse qu'il serait désirable d'éradiquer.
Le caractère informatif des explications de réponses est parfois plus important que les réponses elles-mêmes : ce peut être le cas avec les réponses vides et pléthoriques par exemple, d'où l'intérêt de mécanismes coopératifs robustes, capables à la fois de contribuer à l'explication ainsi qu'à l'amélioration des résultats.
Par ailleurs, l'utilisation de termes de la langue naturelle pour décrire les données permet de garantir l'interprétabilité des explications fournies.
Permettre à l'utilisateur d'utiliser des mots de son propre vocabulaire contribue à la personnalisation des explications et améliore l'interprétabilité.
Ces axes définissent des approches coopératives où l'intérêt des explications est de permettre à l'utilisateur de comprendre comment sont calculés les résultats proposés dans un effort de transparence.
Le caractère informatif des explications apporte une valeur ajoutée aux résultats bruts, et forme une réponse coopérative.
Les procédés de traduction constituent un sujet important pour les traductologues et les linguistes.
Face à un certain mot ou segment difficile à traduire, les traducteurs humains doivent appliquer les solutions particulières au lieu de la traduction littérale, telles que l'équivalence idiomatique, la généralisation, la particularisation, la modulation syntaxique ou sémantique, etc.
En revanche, ce sujet a reçu peu d'attention dans le domaine du Traitement Automatique des Langues (TAL).
Notre problématique de recherche se décline en deux questions : est-il possible de reconnaître automatiquement les procédés de traduction ?
Pour vérifier notre hypothèse, nous avons annoté un corpus parallèle anglais-français en procédés de traduction, tout en établissant un guide d'annotation.
Notre typologie de procédés est proposée en nous appuyant sur des typologies précédentes, et est adaptée à notre corpus.
L'accord inter-annotateur (0,67) est significatif mais dépasse peu le seuil d'un accord fort (0,61), ce qui reflète la difficulté de la tâche d'annotation.
En nous fondant sur des exemples annotés, nous avons ensuite travaillé sur la classification automatique des procédés de traduction.
Même si le jeu de données est limité, les résultats expérimentaux valident notre hypothèse de travail concernant la possibilité de reconnaître les différents procédés de traduction.
Nous avons aussi montré que l'ajout des traits sensibles au contexte est pertinent pour améliorer la classification automatique.
En vue de tester la généricité de notre typologie de procédés de traduction et du guide d'annotation, nos études sur l'annotation manuelle ont été étendues au couple de langues anglais-chinois.
Ce couple de langues partagent beaucoup moins de points communs par rapport au couple anglais
Le guide d'annotation a été adapté et enrichi.
Dans le but de valider l'intérêt de ces études, nous avons conçu un outil d'aide à la compréhension écrite pour les apprenants de français langue étrangère.
Une expérience sur la compréhension écrite avec des étudiants chinois confirme notre hypothèse de travail et permet de modéliser l'outil.
D'autres perspectives de recherche incluent l'aide à la construction de ressource de paraphrases, l'évaluation de l'alignement automatique de mots et l'évaluation de la qualité de la traduction automatique.
De nos jours, l'IA repose en grande partie sur l'utilisation de données de grande taille et sur des méthodes d'apprentissage machine améliorées qui consistent à développer des algorithmes de classification et d'inférence en tirant parti de grands ensembles de données de grande taille.
Ces grandes dimensions induisent de nombreux phénomènes contre-intuitifs, conduisant généralement à une mauvaise compréhension du comportement de nombreux algorithmes d'apprentissage machine souvent conçus avec des intuitions de petites dimensions de données.
En tirant parti du cadre multidimensionnel (plutôt que d'en souffrir), la théorie des matrices aléatoires (RMT) est capable de prédire les performances de nombreux algorithmes non linéaires aussi complexes que certains réseaux de neurones aléatoires, ainsi que de nombreuses méthodes du noyau telles que les SVM, la classification semi-supervisée, l'analyse en composantes principales ou le regroupement spectral.
Pour caractériser théoriquement les performances de ces algorithmes, le modèle de données sous-jacent est souvent un modèle de mélange gaussien (MMG) qui semble être une hypothèse forte étant donné la structure complexe des données réelles (par exemple, des images).
En outre, la performance des algorithmes d'apprentissage automatique dépend du choix de la représentation des données (ou des caractéristiques) sur lesquelles ils sont appliqués.
Encore une fois, considérer les représentations de données comme des vecteurs gaussiens semble être une hypothèse assez restrictive.
S'appuyant sur la théorie des matrices aléatoires, cette thèse vise à aller au-delà de la simple hypothèse du MMG, en étudiant les outils classiques d'apprentissage machine sous l'hypothèse de vecteurs aléatoires concentrés qui généralisent les vecteurs Gaussiens.
Cette hypothèse est particulièrement motivée par l'observation que l'on peut utiliser des modèles génératifs (par exemple, les GAN) pour concevoir des structures de données complexes et réalistes telles que des images, grâce à des transformations Lipschitzienne de vecteurs gaussiens.
Cela suggère notamment que l'hypothèse de concentration sur les données mentionnée ci-dessus est un modèle approprié pour les données réelles et qui est tout aussi mathématiquement accessible que les MMG.
Par conséquent, nous démontrons à travers cette thèse, en nous appuyant sur les GANs, l'intérêt de considérer le cadre des vecteurs concentrés comme un modèle pour les données réelles.
En particulier, nous étudions le comportement des matrices de Gram aléatoires qui apparaissent au cœur de divers modèles linéaires, des matrices à noyau qui apparaissent dans les méthodes à noyau et également des méthodes de classification qui reposent sur une solution implicite (par exemple, la couche de Softmax dans les réseaux de neurones), avec des données aléatoires supposées concentrées.
L'analyse de ces méthodes pour des données concentrées donne le résultat surprenant qu'elles ont asymptotiquement le même comportement que pour les données de MMG.
Ce résultat suggère fortement l'aspect d'universalité des grands classificateurs d'apprentissage machine par rapport à la distribution sous-jacente des données.
La reconnaissance des entités nommées (EN) reste un problème pour de nombreuses applications de Traitement Automatique des Langues Naturelles.
Dans cette version minimale, Nemesis atteint environ 90% en précision et 80% en rappel.
Pour augmenter le rappel, nous proposons différents modules optionnels (examen d'un contexte encore plus large et utilisation du Web comme source de nouveaux contextes) et une étude pour la réalisation d'un module de désambiguïsation et d'apprentissage de règles.
La lecture est un des savoirs fondamentaux acquis à l'école primaire.
D'abord centré sur le décodage dans les premières années, l'enseignement se focalise ensuite essentiellement sur la compréhension et l'automatisation de la lecture.
Mais a lecture fluente du lecteur expert ne se résume pas seulement à une vitesse de lecture élevée, elle se caractérise également par une prosodie adaptée au texte, notamment en termes de phrasé et d'expressivité.
En omettant l'aspect prosodique de la fluence, on tend à entretenir une confusion entre fluence et vitesse de lecture.
Les dimensions prosodiques de la fluence ont longtemps été négligées dans l'étude du développement de la lecture.
Seules quelques études récentes se sont intéressées à leur développement dans diverses langues, mais il n'en existe aucune en français.
La dimension prosodique de la fluence mérite d'être plus largement étudiée, notamment chez l'apprenti lecteur, et c'est l'objectif de cette thèse.
Nous abordons ces questions en utilisant trois types de mesures complémentaires de la prosodie : une mesure subjective à l'aide d'une échelle multidimensionnelle et deux mesures objectives que sont les marqueurs acoustiques de phrasé et d'expressivité et une méthode d'évaluation automatique basé sur l'analyse des signaux de parole.
Dans un premier temps, nous avons abordé le développement des compétences prosodiques en lecture d'un point de vue subjectif, en adaptant une échelle anglophone d'évaluation de la prosodie au français.
L'étude des corrélations entre scores subjectifs et marqueurs acoustiques a permis de mettre en évidence les marqueurs affectant le jugement de l'auditeur.
Les données acoustiques ont ensuite été utilisées pour mieux comprendre le lien entre prosodie et compréhension.
Ces données ont permis de proposer un modèle de croissance pour chaque dimension de la fluence et étudier les liens de causalité entre automaticité, prosodie et compréhension.
Les connaissances acquises dans cette thèse sur le développement de la prosodie en lecture et son lien avec la compréhension écrite chez l'enfant français nous permettent de proposer de nouveaux outils d'évaluation de la fluence incluant la prosodie, et d'envisager le développement d'outils d'entrainement à la lecture prosodique.
Ces outils offrent de nouvelles perspectives pour l'enseignement de la lecture ainsi que pour le diagnostic etla prise en charge des enfants en difficulté d'apprentissage de la lecture.
Les problématiques abordées dans ma thèse sont de définir une adaptation unifiée entre la sélection des documents et les stratégies de recherche de la réponse à partir du type des documents et de celui des questions, intégrer la solution au système de Questions
Nous développons et étudions une méthode basée sur une approche de Recherche d'Information pour la sélection de documents en QR.
Celle-ci s'appuie sur un modèle de langue et un modèle de classification binaire de texte en catégorie pertinent ou non pertinent d'un point de vue QR.
Cette méthode permet de filtrer les documents sélectionnés pour l'extraction de réponses par un système QR.
Nous présentons la méthode et ses modèles, et la testons dans le cadre QR à l'aide de RITEL.
L'évaluation est faite en français en contexte web sur un corpus de 500 000 pages web et de questions factuelles fournis par le programme Quaero.
Celle-ci est menée soit sur des documents complets, soit sur des segments de documents.
L'hypothèse suivie est que le contenu informationnel des segments est plus cohérent et facilite l'extraction de réponses.
Dans le premier cas, les gains obtenus sont faibles comparés aux résultats de référence (sans filtrage).
Dans le second cas, les gains sont plus élevés et confortent l'hypothèse, sans pour autant être significatifs.
Une étude approfondie des liens existant entre les performances de RITEL et les paramètres de filtrage complète ces évaluations.
Le système de segmentation créé pour travailler sur des segments est détaillé et évalué.
Son évaluation nous sert à mesurer l'impact de la variabilité naturelle des pages web (en taille et en contenu) sur la tâche QR, en lien avec l'hypothèse précédente.
En général, les résultats expérimentaux obtenus suggèrent que notre méthode aide un système QR dans sa tâche.
Cependant, de nouvelles évaluations sont à mener pour rendre ces résultats significatifs, et notamment en utilisant des corpus de questions plus importants.
Les projets en humanités numériques utilisent de plus en plus des méthodes de collaboration axées sur le public, telles que le crowdsourcing pour atteindre les objectifs de recherche, de conservation et d'édition scientifique en sciences humaines et sociales.
Par exemple, le crowdsourcing représente une opportunité pour accélérer les projets de transcription pour des communautés de chercheurs qui travaillent traditionnellement dans des circuits
En outre, l'efficacité du crowdsourcing pour les humanités numériques n'est pas documenté dans la littérature.
Se pose ainsi la question de savoir si le public peut produire du matériel pouvant être par la suite utilisé pour des éditions scientifiques, auxquels cas, pour quel type de projet et combien de post-traitement ou corrections seront nécessaires.
Cette thèse de doctorat examinera le potentiel apport du crowdsourcing des transcriptions pour les projets d'édition scientifique en humanités numériques.
Pour cela, nous allons premièrement explorer les technologies et les techniques disponibles pour produire les transcriptions sous format XML en ligne.
Deuxièmement, ayant développé et testé une plateforme internet de transcription que nous présenterons, nous pourrons examiner les besoins des utilisateurs vis-à-vis des environnements de travail collaboratifs fondées sur les retours des utilisateurs et les environments de crowdsourcing industriels existants.
Troisièmement, les données récoltées seront soumises à une analyse numérique qui permettra de comparer les productions des experts et celle des non-experts en s'appuyant sur les mesures de distances entre documents.
Les résultats obtenus permettront de déterminer le potentiel apport du crowdsourcing pour les projets d'édition numérique scientifique.
Enfin, le travail se terminera avec une discussion sur les implications des travaux actuels et présentera des opportunités pour des recherches futures sur le terrain.
Cette thèse aborde les problématiques liées à la construction de Topic Maps et à leur utilisation pour la recherche d'information dans le cadre défini par le Web sémantique (WS).
Un contenu à organiser étant très souvent volumineux et sujet à enrichissement perpétuel, il est pratiquement impossible d'envisager une création et gestion d'une Topic Map, le décrivant, de façon manuelle.
Plusieurs travaux de recherche ont concerné la construction de Topic Maps à partir de documents textuels [Ellouze et al. 2008a].
Cependant, aucune d'elles ne permet de traiter un contenu multilingue.
De plus, bien que les Topic Maps soient, par définition, orientées utilisation (recherche d'information), peu d'entre elles prennent en compte les requêtes des utilisateurs.
Notre approche est incrémentale et évolutive, elle est basée sur un processus automatisé, qui prend en compte des documents multilingues et l'évolution de la Topic Map selon le changement du contenu en entrée et l'usage de la Topic Map.
Pour enrichir la Topic Map, nous nous basons sur deux ontologies générales et nous explorons toutes les questions potentielles relatives aux documents sources.
Dans ACTOM, en plus des liens d'occurrences reliant un Topic à ses ressources, nous catégorisons les liens en deux catégories : (a) les liens ontologiques et (b) les liens d'usage.
Nous proposons également d'étendre le modèle des Topic Maps défini par l'ISO en rajoutant aux caractéristiques d'un Topic des méta-propriétés servant à mesurer la pertinence des Topics plus précisément pour l'évaluation de la qualité et l'élagage dynamique de la Topic Map.
Cette thèse s'intéresse à la description didactique de la variation linguistique dans une approche à contraintes.
Cette approche nous permet d'envisager la variation dans une perspective fonctionnelle plutôt que normative et de décrire les variantes « non-standard » comme plus ou moins appropriées à une tâche plutôt que comme des déviations de la norme.
Pour illustrer notre approche, nous l'appliquons à la description de la dislocation clitique à gauche en français.
Nous suggérons que ces contraintes sont toutes de nature pragmatique et que leur interaction influe sur l'emploi de la dislocation clitique à gauche en français.
Ces hypothèses sont testées empiriquement via une étude de corpus, une série de test de jugements d'acceptabilité et un test de Matched Guise.
De plus, nous postulons que l'apprentissage des contraintes pragmatiques en langue étrangère dépend de leur enseignement explicite et l'exposition répétée à la construction dans des contextes acceptables.
Suivant l'hypothèse de l'interface dynamique (Ellis, 2005), nous suggérons que l'apprentissage explicite des contraintes de la dislocation clitique à gauche dans le contexte de la classe de langue facilite leur apprentissage implicite lorsque les apprenant se retrouve dans une situation de communication avec des locuteurs natifs du français.
Le rôle de l'exposition est exploré empiriquement en répliquant un test de jugements d'acceptabilité et le test de Matched Guise avec des participants non-natifs.
Les contraintes stylistiques sont décrites via la compétence de savoir-être et les registres sociolinguistiques (CECR, 2001).
À partir d'une conception dynamique et plurielle des émotions, cette thèse propose une réflexion sur l'inscription discursive des affects dans les interactions numériques de type WhatsApp.
Elle s'inscrit dans le domaine de l'analyse du discours en interaction (Kerbrat-Orecchioni, 2005) en faisant dialoguer le cadre du discours numérique (Paveau, 2017) avec les propositions théoriques des sciences de l'information et la communication (Allard, 2017).
Si les linguistes ont longtemps souligné l'infinité des marques langagières des émotions (Kerbrat-Orecchioni, 2000) voire même leurs hétérogénéités (Plantin, 2011 ; Micheli, 2014), ce postulat est encore plus attesté dans les écosystèmes numériques où l'expression des affects se trouve distribuée dans toute l'interface numérique intégrant indifféremment les mots et les gestes (Jeanneret et Souchier, 1999).
Suite à une réflexion méthodologique autour de la constitution du corpus numérique WhatsApp, ce travail porte sur les ressources sémiotiques et discursives dans l'expression des émotions.
Ces dernières se voient matérialisées dans de nouvelles formes verbales comme les émotimots lol et mdr, entendus comme un sociolecte de l'affecte.
Mais dans le cadre des messageries numériques, l'expression des émotions dépassent le verbal pour s'incarner dans des formes l'iconique marquées les photo discours, où la capture photographique se tisse avec le verbale pour co-construire le sens.
À partir d'analyses qualitatives des observables prélevés, cette thèse montre comment les locuteurs et locutrices renouvellent sans cesses les formes d'expression des émotions et comment ils et elles composent avec les affordances numériques du système, pour explorer des versions inédites dans la gestion interactionnelle des affects.
Les humains et les robots travaillant en toute sécurité et en parfaite harmonie dans un environnement est l'un des objectifs futurs de la communauté robotique.
Quand les humains et les robots peuvent travailler ensemble dans le même espace, toute une catégorie de tâches devient prête à l'automatisation, allant de la collaboration pour l'assemblage de pièces, à la manutention de pièces et de materiels ainsi qu'à leur livraison.
Des normes existent sur la collaboration entre robots et humains, cependant elles se focalisent à limiter les distances d'approche et les forces de contact entre l'humain et le robot.
Un outil clé pour la sécurité entre des robots et des humains travaillant dans un environnement inclut la reconnaissance de l'intention dans lequel le robot tente de comprendre l'intention d'un agent (l'humain) en reconnaissant tout ou partie des actions de l'agent pour l'aider à prévoir les actions futures de cet agent.
Dans cette thèse, nous présentons une approche qui est capable de déduire l'intention d'un agent grâce à la reconnaissance et à la représentation des informations de l'état.
Cette approche est différente des nombreuses approches présentes dans la littérature qui se concentrent principalement sur la reconnaissance de l'activité (par opposition à la reconnaissance de l'état) et qui « devinent » des raisons pour expliquer les observations.
Dans les domaines de spécialité, les applications telles que la recherche d'information ou la traduction automatique, s'appuient sur des ressources terminologiques pour prendre en compte les termes, les relations sémantiques ou les regroupements de termes.
Pour faire face au coût de la constitution de ces ressources, des méthodes automatiques ont été proposées.
Parmi celles-ci, l'analyse distributionnelle s'appuie sur la redondance d'informations se trouvant dans le contexte des termes pour établir une relation.
Alors que cette hypothèse est habituellement mise en oeuvre grâce à des modèles vectoriels, ceux-ci souffrent du nombre de dimensions considérable et de la dispersion des données dans la matrice des vecteurs de contexte.
En corpus de spécialité, ces informations contextuelles redondantes sont d'autant plus dispersées et plus rares que les corpus ont des tailles beaucoup plus petites.
De même, les termes complexes sont généralement ignorés étant donné leur faible nombre d'occurrence.
Des relations sémantiques acquises en corpus sont utilisées pour généraliser et normaliser ces contextes.
Nous avons évalué la robustesse de notre méthode sur quatre corpus de tailles, de langues et de domaines différents.
L'analyse des résultats montre que, tout en permettant de prendre en compte les termes complexes dans l'analyse distributionnelle, l'abstraction des contextes distributionnels permet d'obtenir des groupements sémantiques de meilleure qualité mais aussi plus cohérents et homogènes.
La Génération Automatique de Langue Naturelle vise à produire des textes dans une langue humaine à partir d'un ensemble de données non-linguistiques.
La dernière sous-tâche est connue comme la tâche de Réalisation de Surface (RS).
Dans ma thèse, j'étudie la tâche de RS quand les données d'entrée sont extraites de Bases de Connaissances (BC).
Je présente deux nouvelles approches pour la réalisation de surface à partir de bases de connaissances : une approche supervisée et une approche faiblement supervisée.
Dans l'approche supervisée, je présente une méthode basée sur des corpus pour induire une grammaire à partir d'un corpus parallèle de textes et de données.
Dans l'approche faiblement supervisée, j'explore une méthode pour la réalisation de surface à partir de données extraites d'une BC qui ne requière pas de corpus parallèle.
À la place, je construis un corpus de textes liés au domaine et l'utilise pour identifier les lexicalisations possibles des symboles de la BC et leurs modes de verbalisation.
J'évalue les phrases générées et analyse les questions relatives à l'apprentissage à partir de corpus non-alignés.
Dans chacune de ces approches, les méthodes proposées sont génériques et peuvent être facilement adaptées pour une entrée à partir d'autres ontologies
La présente recherche s'intéresse à la syntaxe des constructions exceptives (CE) et sa correspondance avec la sémantique au sein de deux langues : le français et l'arabe.
Nous situerons notre analyse des marqueurs sauf, excepté, hormis, etc. dans le cadre des listes/entassements paradigmatiques, constructions dans lesquelles deux éléments occupent la même position syntaxique et dont le cas le plus connu est la coordination.
Cette analyse s'éloigne de celle généralement associée à ces marqueurs dans les grammaires et les dictionnaires français qui les traitent comme des prépositions.
Nos analyses nous amènent à considérer les marqueurs ʾillā, ġayr et siwā en arabe comme des conjonctions de coordination.
Ces items, comme leurs homologues français, mettent en relation deux éléments X et Y où X à droite du marqueur et Y à gauche forment des listes/entassements paradigmatiques au sens où ils partagent la même fonction syntaxique dans l'énoncé.
Nous analysons les unités ʿadā (mā-ʿadā), ẖalā (mā-ẖalā), ḥāšā (mā-ḥāšā) comme des verbes.
Ces verbes introduisent une proposition qui entretient une relation de parataxe avec la proposition précédente.
Nous considérons, enfin, les unités bistiṯnāʾi et biẖilāfi comme des locutions prépositives introduisant une séquence qui entretient une relation de subordination avec la proposition principale.
Les informations échangées dans les textes des courriels sont généralement concernées par des événements complexes ou des processus métier dans lesquels les entités qui échangent des courriels collaborent pour atteindre les objectifs finaux des processus.
Ainsi, le flux d'informations dans les courriels envoyés et reçus constitue une partie essentielle, les activités métier de l'entreprise.
L'extraction d'informations sur les processus métier à partir des courriels peut aider à améliorer la gestion des courriels pour les utilisateurs.
Il peut également être utilisé pour trouver des réponses riches à plusieurs questions analytiques sur les employés et les organisations.
Aucun des travaux précédents n'a résolu le problème de la transformation automatique des journaux de courriels en journaux d'événements pour éventuellement en déduire les processus métier non documentés.
Dans ce but, nous travaillons dans cette thèse sur un framework qui induit des informations de processus métier à partir d'emails.
Nous introduisons des approches qui contribuent à ce qui suit : (1) découvrir pour chaque courriel le sujet de processus qui le concerne, (2) découvrir l'instance de processus métier à laquelle appartient chaque courriel, (3) extraire les activités de processus métier des courriels et associer ces activités aux métadonnées qui les décrivent, (4) améliorer la performance de la découverte des instances de processus métier et des activités métier en utilisant la relation entre ces deux problèmes, et enfin (5) estimer au préalable la date/heure réelle d'un activité métier.
En utilisant les résultats des approches mentionnées, un journal d'événements est généré qui peut être utilisé pour déduire les modèles de processus métier d'un journal de courriels.
L'efficacité de toutes les approches ci-dessus est prouvée par l'application de plusieurs expériences sur l'ensemble de données de courriel ouvert d'
Cette thèse s'inscrit dans le cadre de l'émergence de l'apprentissage profond et aborde la compréhension de la parole assimilée à l'extraction et à la représentation automatique du sens contenu dans les mots d'une phrase parlée.
Nous étudions une tâche d'étiquetage en concepts sémantiques dans un contexte de dialogue oral évaluée sur le corpus français MEDIA.
Depuis une dizaine d'années, les modèles neuronaux prennent l'ascendant dans de nombreuses tâches de traitement du langage naturel grâce à des avancées algorithmiques ou à la mise à disposition d'outils de calcul puissants comme les processeurs graphiques.
De nombreux obstacles rendent la compréhension complexe, comme l'interprétation difficile des transcriptions automatiques de la parole étant donné que de nombreuses erreurs sont introduites par le processus de reconnaissance automatique en amont du module de compréhension.
Nous présentons un état de l'art décrivant la compréhension de la parole puis les méthodes d'apprentissage automatique supervisé pour la résoudre en commençant par des systèmes classiques pour finir avec des techniques d'apprentissage profond.
Les contributions sont ensuite exposées suivant trois axes.
Puis nous abordons la gestion des erreurs de reconnaissance automatique et des solutions pour limiter leur impact sur nos performances.
Enfin, nous envisageons une désambiguïsation de la tâche de compréhension permettant de rendre notre système plus performant.
Des items ciblant les CP ont été créés pour élaborer une tâche de réception sur la base d'énoncés en LSF filmés et une tâche de production à partir d'images.
Une seconde tâche de production a également été proposée pour évaluer les compétences narratives et les constructions prédicatives en situation de récit.
En séparant le matériel sur lequel fonctionnent les fonctions/services réseau et le logiciel qui réalise et contrôle de ces fonctions/services, les réseaux logiciels (ou SDN, Software
Les paradigmes SDN et NFV introduisent plus de flexibilité et permettent un meilleur contrôle, ainsi les technologies associées devraient dominer une grande partie du marché du réseautage dans les prochaines années (estimé à 3,68 milliards USD en 2017 et prévu par certains pour atteindre 54 milliards USD d'ici 2022 à un taux de croissance annuel composé (TCAC) de 71,4%).
Cependant, l'un des soucis majeurs des opérateurs à propos de Network Softwarization est la sécurité.
Dans cette thèse, nous avons d'abord conçu et implémenté un framework de test de pénétration (pentesting) pour les contrôleurs SDN.
Nous avons proposé un ensemble d'algorithmes pour l'empreinte digitale d'un contrôleur SDN distant, i.e.
En utilisant notre framework, les opérateurs réseau peuvent évaluer la sécurité de leurs déploiements SDN (y compris Opendaylight, Floodlight et Cisco Open SDN Controller) avant de les mettre en production.
En outre, sOFTDP surpasse OFDP de plusieurs ordres de grandeur en terme de performance, ce que nous avons confirmé par des tests approfondis.
Le deuxième axe de notre recherche dans cette thèse est la gestion intelligente et automatique des réseaux logiciels.
Inspirés par les avancées récentes dans les techniques d'apprentissage automatique, notamment les réseaux de neurones profonds (DNN, Deep Neural Networks), nous avons créé un moteur d'ingénierie de trafic pour le SDN appelé NeuRoute, entièrement basé sur les DNN.
Les contrôleurs SDN/OpenFlow actuels utilisent par défaut un routage basé sur l'algorithme de Dijkstra pour les chemins les plus courts mais fournissent des API pour développer des applications de routage personnalisées.
Après Septembre 2008, du fait du gel du marché interbancaire, d'un manque de liquidité, d'une perte de confiance et des difficultés des institutions financières, la transmission de la politique monétaire au sein de la zone euro a été sévèrement altérée.
La Banque Centrale Européenne (BCE) a donc dû avoir recours à des politiques monétaires non-conventionnelles.
En considérant, au sein de la zone euro, les contraintes imposées à la banque centrale et la fragmentation des marchés financiers, l'objectif de cette thèse empirique est d'évaluer les canaux de transmission des politiques monétaires conventionnelles et non-conventionnelles de la BCE.
Les comportements de prêts des banques étant liés à leurs coûts de financement, le premier essai se focalise sur le canal de transmission des prêts bancaires.
Il étudie l'évolution des activités de prêts syndiqués d'institutions financières européennes et leur réaction aux politiques de la BCE.
La communication de la banque centrale revêt une importance toute particulière dans une union monétaire.
Les deuxième et troisième essais se concentrent sur le canal des signaux.
Le deuxième essai étudie sur la communication durant les conférences de presse mensuelles ainsi que ses effets sur la prévisibilité des décisions de politique monétaire et sur les rendements et la volatilité des marchés financiers.
Le dernier essai se focalise sur l'utilisation du guidage des taux d'intérêt futurs, une communication non-conventionnelle informant les marchés du niveau futur des taux d'intérêt de court-terme.
Il étudie l'efficacité de cette annonce et sa capacité à influencer les prévisions de taux d'intérêt faites par les acteurs de marché.
La tendance actuelle de l'intelligence artificielle (IA) est de s'appuyer fortement sur des systèmes capables d'apprendre à partir d'exemples, tels que les modèles d'apprentissage profond ou 'Deep Learning'(DL), une incarnation moderne des réseaux de neurones artificiels.
Bien que de nombreuses applications aient été mises sur le marché ces dernières années (y compris les voitures autonomes, les assistants automatisés, les services de réservation et les chatbots, les améliorations des moteurs de recherche, les recommandations et la publicité, et les applications de soins de santé, pour n'en nommer que quelques-uns), les modèles DL sont notoirement difficiles à déployer dans de nouvelles applications.
En particulier, ils nécessitent un grand nombre d'exemples d'apprentissage, des heures d'entrainement sur GPU et des ingénieurs hautement qualifiés pour ajuster manuellement leurs architectures.
Cette thèse contribuera à réduire la barrière d'entrée dans l'utilisation des modèles DL pour de nouvelles applications, une étape vers la 'démocratisation de l'IA'.
L'angle pris sera de développer de nouvelles approches d'apprentissage par transfer ou 'Transfert Learning'(TL), basées sur des architectures DL modulaires.
L'apprentissage par transfert englobe toutes les techniques pour accélérer l'apprentissage en capitalisant sur l'exposition à des tâches similaires précédentes.
Par exemple, l'utilisation de réseaux pré-entrainés est une tactique clé de TL utilisée par les gagnants du récent défi AutoDL https : //autodl.chalearn.org/.
Le doctorant fera avancer la notion de réutilisation des réseaux pré-formés en tout ou en partie (modularité).
Plusieurs questions importantes se posent dans ce contexte.
D'un point de vue technique, les limites actuelles du pré-apprentissage sont les suivantes :
(T1) Dans de nombreux domaines, il n'y a pas de réseaux pré-entrainés disponibles, en raison du manque d'ensembles de données massifs dans les domaines connexes ;
(T2) Les nouvelles architectures de réseaux telles que les « réseaux de neurones graphiques » (GNN) ne se prêtent pas facilement au pré-apprentissage ;
(T3) Outre le simple recyclage de la dernière couche et le réglage fin des couches internes, les moyens de réutiliser les réseaux pré-entrainés dans de nouveaux contextes sont sous-développés.
Ces trois problèmes offrent des opportunités de recherche stimulantes pour utiliser efficacement les connaissances antérieures, des simulateurs de données et / ou l'augmentation des données et développer de nouveaux algorithmes et architectures qui apprennent de manière modulaire et réutilisable.
Du point de vue de la recherche fondamentale, la modularité et l'héritage des modules d'apprentissage pré-entrainés dans les systèmes d'apprentissage d'inspiration biologique est un sujet brûlant en IA.
Les questions sans réponse comprennent :
(F1) La modularité du cerveau augmente-t-elle son efficacité ou s'agit-il d'un héritage d'évolution qui ne joue aucun rôle particulier ;
(F2) De même, dans quel contexte et comment la modularité aide-t-elle dans les systèmes artificiels (par exemple, pour mettre en œuvre des invariances, pour aider à transférer l'apprentissage, etc.) ;
(F3) La spécialisation du module gêne-t-elle ou aide-t-elle à généraliser de nouvelles modalités de données (par exemple, de nouvelles données de capteur), et si oui, comment ?
Dans ce contexte, le doctorant étudiera une nouvelle approche de transfert d'apprentissage que nous appelons « Apprentissage Modulaire Profond » .
Le candidat abordera le problème de l'apprentissage de grands réseaux de neurones artificiels dont les architectures sont modulaires et dont les modules sont éventuellement réutilisables.
Une méthode possible pour aborder le problème sera d'utiliser des algorithmes d'optimisation à plusieurs niveaux, abordant l'optimisation du système global (atteindre un objectif de niveau supérieur) sous la contrainte que les modules atteignent un objectif de niveau inférieur (réutilisabilité).
Un objectif scientifique sera de remettre en cause l'hypothèse selon laquelle la modularité est essentielle pour les systèmes d'apprentissage, en ce qu'elle accélère l'apprentissage en rendant possible une forme efficace d'apprentissage par transfert, une fonctionnalité centrale de l'IA.
Plusieurs principes / conjectures / hypothèses peuvent guider cette recherche, notamment :
(P1) Le principe de la parcimonie ou du 'rasoir d'Ockham'incarné dans la théorie de l'apprentissage moderne sous le nom de 'régularisation', qui, selon les mots du profane, déclare que 'de deux théories équivalentes puissantes pour reproduire observations, il faut préférer la plus simple '' ; en effet, les architectures modulaires partageant des sous-modules identiques ont moins de paramètres ajustables et peuvent donc être considérées comme moins complexes que par exemple les réseaux entièrement connectés.
(P2) L'hypothèse de l'innéité : les capacités de résolution de tâches sont une combinaison de compétences innées et acquises.
Est-ce une caractéristique des systèmes intelligents de s'appuyer davantage sur des compétences acquises telles que la langue plutôt que d'en hériter ?
Est-il vrai que la langue peut être complètement apprise 'à partir de zéro' ?
(P3) Induction, déduction, conceptualisation et causalité : les systèmes d'apprentissage intelligents reposent-ils sur la modularité de la conceptualisation, de l'acquisition du langage et de l'inférence causale ?
Pour mettre un cadre en pratique à cette recherche, l'étudiant choisira des applications dans des domaines tels que la biomédecine (par exemple, toxicité moléculaire ou efficacité), l'écologie, l'économétrie, la reconnaissance vocale, le traitement du langage naturel, le traitement d'images ou de vidéos, etc.
Voir par exemple http : //snap.stanford.edu/data/.
Le résumé automatique de document repose généralement sur des méthodes par extraction qui sélectionnent dans le texte des passages pertinents et les juxtaposent pour former un résumé.
Ces méthodes sont peu adaptées à la problématique du résumé de conversations orales de part la nature spontanée de celles-ci et l'importance de l'interaction entre les locuteurs.
En ne sélectionnant que certains passages, les résumés par extraction ne contiennent qu'un verbatim de ce qui a été dit, et non pas une description synthétique de ce qui s'est passé lors de la conversation.
C'est pourquoi des approche abstractives basées sur la détection de concepts permettrait de palier ces difficultés.
Puis nous étudions l'intérêt de l'utilisation de modèles sémantiques dans la tâche de résumé automatique.
Enfin nous proposons une méthode de résumé à base de patrons.
Les méthodes de résumé par remplissage de patrons ont montré leur intérêt dans des domaines spécifiques pour le résumé automatique de texte.
Dans notre cas, elles permettent de traiter du problème de différence de genre entre les données source (transcriptions de conversations) et la forme des résumés à générer (narration synthétique).
Toutefois, elles nécessitent l'écriture manuelle de patrons de résumés et l'annotation manuelle de quantités de données source en concepts à détecter pour remplir ces patrons.
Ce mémoire concerne la reconnaissance automatique des Actes de Dialogues (ADs) en tchéque et en français.
Les ADs sont des unités au niveau de la phrase qui représentent des différents états d'un dialogue, comme par exemple les questions, les affirmations, les hésitations, etc.
Les résultats expérimentaux confirment que chaque type d'attributs apporte des informations pertinentes et complémentaires.
Les méthodes proposées qui exploitent la position des mots sont particulièrement intéresantes, parce qu'elles utilisent une information globale sur la structure de la phrase.
Une autre contribution conséquente, relative au manque de corpus étiquettés dans le domaine de la reconnaissance automatique des ADs, concerne le développement et l'étude de méthodes d'
Les résultats expérimentaux démontrent que la méthode proposée est une approche intéressante pour la création de nouveaux corpus d'actes de dialogues à moindre coût.
La désambiguïsation lexicale (DL) et la traduction automatique (TA) sont deux tâches centrales parmi les plus anciennes du traitement automatique des langues (TAL).
Bien qu'ayant une origine commune, la DL ayant été conçue initialement comme un problème fondamental à résoudre pour la TA, les deux tâches ont par la suite évolué très indépendamment.
En effet, d'un côté, la TA a su s'affranchir d'une désambiguïsation explicite des termes grâce à des modèles statistiques et neuronaux entraînés sur de grandes quantités de corpus parallèles, et de l'autre, la DL, qui est confrontée à certaines limitations comme le manque de ressources unifiées et un champs d'application encore restreint, reste un défi majeur pour permettre une meilleure compréhension de la langue en général.
Aujourd'hui, dans un contexte où les méthodes à base de réseaux de neurones et les représentations vectorielles des mots prennent de plus en plus d'ampleur dans la recherche en TAL, les nouvelles architectures neuronales et les nouveaux modèles de langue pré-entraînés offrent non seulement de nouvelles possibilités pour développer des systèmes de DL et de TA plus performants, mais aussi une opportunité de réunir les deux tâches à travers des modèles neuronaux joints, permettant de faciliter l'étude de leurs interactions.
Dans cette thèse, nos contributions porteront dans un premier temps sur l'amélioration des systèmes de DL, par l'unification des données nécessaires à leur mise en oeuvre, la conception de nouvelles architectures neuronales et le développement d'approches originales pour l'amélioration de la couverture et des performances de ces systèmes.
Ensuite, nous développerons et comparerons différentes approches pour l'intégration de nos systèmes de DL état de l'art et des modèles de langue, dans des systèmes de TA, pour l'amélioration générale de leur performance.
Enfin, nous présenterons une nouvelle architecture pour l'apprentissage d'un modèle neuronal joint pour la DL et la TA, s'appuyant sur nos meilleurs systèmes neuronaux pour l'une et l'autre tâche.
Cette thèse se propose d'étudier la grammaticalisation, processus d'évolution linguistique par lequel les éléments fonctionnels de la langue se trouvent remplacés au cours du temps par des mots ou des constructions de contenu, c'est-à-dire servant à désigner des entités plus concrètes.
La grammaticalisation est donc un cas particulier de remplacement sémantique.
Or, la langue faisant l'objet d'un consensus social bien établi, il semble que le changement sémantique s'effectue à contre-courant de la bonne efficacité de la communication ;
pourtant, il est attesté dans toutes les langues, toutes les époques et, comme le montre la grammaticalisation, toutes les catégories linguistiques.
Ces profils de fréquence sont extraits de la base de données de Frantext, qui permet de couvrir une période de sept siècles.
Les distributions statistiques des observables décrivant ces deux phénomènes sont obtenues et quantifiées.
Un modèle de marche aléatoire est ensuite proposé reproduisant ces deux phénomènes.
La latence s'y trouve expliquée comme un phénomène critique, au voisinage d'une bifurcation point-col.
Une extension de ce modèle articulant l'organisation du réseau sémantique et les formes possibles de l'évolution est ensuite discutée.
L'objectif de cette thèse est d'étudier la fonction argumentale afin d'élaborer une méthode pour l'acquisition automatique des termes d'une manière pertinente et efficace.
Nous avons d'abord discuté du profilage du corpus et de la constitution du corpus web pour le traitement automatique des langues.
Ensuite, trois méthodes ont été développées en nous fondant sur les caractéristiques morphologiques des unités lexicales et la relation d'appropriation entre les prédicats appropriés et leurs arguments.
La méthode distributionnelle a pour objet d'exploiter les structures prédicat-argument pour repérer les arguments de la classe sémantique donnée.
La méthode morphosémantique est développée en se fondant sur les structures internes des unités lexicales en vue d'étendre la liste de termes.
La méthode combinatoire qui associe les deux premières approches permet d'améliorer la pertinence du résultat.
Finalement, nous avons développé une réflexion sur la particularité de la langue, la classe sémantique, la langue de spécialité et la récursivité de la langue dans la perspective du traitement automatique des langues.
En revanche, lorsqu'il est question de traiter les langues peu dotées, on est souvent confronté au manque d'outils et de données.
Dans cette thèse, on s'intéresse à certaines formes vernaculaires de l'arabe utilisées au Maghreb.
Ces formes sont connues sous le terme de dialecte que l'on peut classer dans la catégorie des langues peu dotées.
Exceptés des textes brutes extraits généralement des réseaux sociaux, il existe très peu de ressources permettant de traiter les dialectes arabes.
Ces derniers, comparativement aux autres langues peu dotées possèdent plusieurs spécificités qui les rendent plus difficile à traiter.
Nous pouvons citer notamment l'absence de règles d'écriture de ces dialectes, ce qui conduit les usagers à écrire le dialecte sans suivre des règles précises, par conséquent un même mot peut avoir plusieurs graphies.
Les mots en arabe dialectal peuvent s'écrire en utilisant le script arabe et/ou le script latin (écriture dite arabizi).
Pour les dialectes arabes du Maghreb, ils sont particulièrement influencés par des langues étrangères comme le français et l'anglais.
En plus de l'emprunt de mots de ces langues, un autre phénomène est à prendre en compte en traitement automatique des dialectes.
Il s'agit du problème connu sous le terme de code-switching.
Ce phénomène est connu en linguistique sous le terme de diglossie.
Cela a pour conséquence de laisser libre cours à l'utilisateur qui peut écrire en plusieurs langues dans une même phrase.
Il peut ainsi commencer en dialecte arabe et au milieu de la phrase, il peut "switcher" vers le français, l'anglais ou l'arabe standard.
En plus de cela, il existe plusieurs dialectes dans un même pays et a fortiori plusieurs dialectes différents dans le monde arabe.
Il est donc clair que les outils NLP classiques développés pour l'arabe standard ne peuvent être utilisés directement pour traiter les dialectes.
L'objectif principal de ce travail consiste à proposer des méthodes permettant la construction automatique de ressources pour les dialectes arabes en général et les dialectes du Maghreb en particulier.
Cela représente notre contribution à l'effort fourni par la communauté travaillant sur le traitement automatique des dialectes arabes.
Nous avons ainsi produit des méthodes permettant de construire des corpus comparables, des ressources lexicales contenant les différentes formes d'une entrée et leur polarité.
Par ailleurs, nous avons développé des méthodes pour le traitement de l'arabe standard sur des données de Twitter et également sur les transcriptions provenant d'un système de reconnaissance automatique de la parole opérant sur des vidéos en arabe extraites de chaînes de télévisions arabes telles que Al Jazeera, France24, Euronews, etc.
Nous avons ainsi comparé les opinions des transcriptions automatiques provenant de sources vidéos multilingues différentes et portant sur le même sujet en développant une méthode fondée sur la théorie linguistique dite Appraisal.
Le raisonnement spatial et temporel qualitatif est un domaine principal d'études de l'intelligence artificielle et, en particulier, du domaine de la représentation des connaissances, qui traite des concepts cognitifs fondamentaux de l'espace et du temps de manière abstraite.
Dans notre thèse, nous nous focalisons sur les formalismes du domaine du raisonnement spatial et temporel qualitatif représentant les informations par des contraintes et apportons des contributions sur plusieurs aspects.
En particulier, étant donnée des bases de connaissances d'informations qualitatives sur l'espace ou le temps, nous définissons des nouvelles conditions de consistance locale et des techniques associées afin de résoudre efficacement les problèmes fondamentaux se posant.
En outre, nous enrichissons le domaine des formalismes spatio-temporels par des contributions concernant une logique spatio-temporelle combinant la logique temporelle propositionnelle (PTL) avec un langage de contraintes qualitatives spatiales et une étude de la problématique consistant à gérer une séquence temporelle de configurations spatiales qualitatives devant satisfaire des contraintes de transition.
Nous employons une méthode différentielle, non-compositionnelle et endogène.
Notre but est de maximiser la factorisation afin de permettre le traitement de nouvelles langues avec un coût marginal minimal.
Pour ce faire nous exploitons les propriétés du genre journalistique et tout particulièrement la répétition de certains éléments à des positions clés.
Notre grain d'analyse est le grain caractère de façon à être indépendant des contraintes posées par le concept de mot graphique dans un grand nombre de langues.
Nous aboutissons à l'implantation du système DAnIEL (Data Analysis for Information Extraction in any Language).
DAnIEL opère une classification des documents selon qu'ils décrivent ou non des faits épidémiologiques et les regroupe par faits épidémio-logiques sous la forme de paires maladie-lieu.
DAnIEL est rapide et efficace en comparaison des systèmes existants.
Il nécessite des ressources légères pour fonctionner, facilitant ainsi le traitement de nouvelles langues.
L'attention est le processus qui consiste à filtrer les informations utiles à l'activité, de celles qui lui sont inutiles.
La réalité augmentée (RA) de surlignage guide ce processus de sélection de l'information, en mettant en valeur certains éléments par rapport à d'autres.
Telle qu'envisagée actuellement pour la conduite automobile, la RA de surlignage met en évidence des éléments liés à l'activité générale de conduite (e.g. panneaux en cas de mauvaise visibilité, direction à emprunter), mais indépendamment des manœuvres.
Or, la littérature sur l'attention visuelle en activité nous montre que les parcours oculaires sont très spécifiques aux buts et sous-buts immédiats.
Une RA qui ne respecte pas cette priorisation « naturelle » du traitement de l'information risque donc de perturber la prise d'informations.
Le premier objectif de cette recherche est de déterminer dans quelle mesure l'allocation de l'attention visuelle en conduite automobile se concentre sur les informations liées à la manœuvre.
Le second objectif est d'étudier l'impact de la RA sur cette allocation de l'attention.
Nous avons mis en place trois expérimentations dans lesquelles les participants visualisaient des scènes statiques et dynamique de conduite automobile, et devaient décider s'ils pouvaient réaliser une manœuvre.
Nous avons analysé les variations d'allocation de l'attention visuelle selon les manœuvres notifiées et les conditions de RA à l'aide d'enregistrements oculométriques.
Nos résultats montrent que l'attention visuelle est fortement allouée aux indices permettant la prise de décision, mais qu'elle ne néglige pas les indices permettant la compréhension générale de la scène.
La RA optimise l'attention visuelle lorsqu'elle met en évidence des indices liés à la manœuvre, mais elle perturbe l'attention visuelle dans les autres conditions.
Ces résultats permettent d'identifier et de caractériser différents risques inhérents à la RA de surlignage, et de discuter des pistes de conception pour les prendre en compte.
Les Maladies Neurodégénératives Rares à Expression Motrice (MNDREM) sont un groupe très hétérogène de pathologies neurodégénératives de physiopathologie acquise ou innée, pouvant se déclarer à tout âge.
Elles impactent de façon ciblée, les structures neurologiques centrales ou périphériques impliquées dans l'activité motrice.
Ces structures sont fonctionnellement et topographiquement proches des centres de cognition.
C'est ainsi que dans certaines conditions, la neurodégénérescence des structures motrices impacte les fonctions cognitives.
Elles se traduisent par des troubles de la marche et /ou de l'équilibre, une diminution ou une absence de mouvement, une atteinte cognitive variée pouvant aller jusqu'à la démence.
Les MNDREM sont sporadiques ou familiales.
La littérature scientifique indique un nombre important de variants génétiques pathogènes responsables des MNDREM, ce qui aide la classification mais la complexifie également puisqu'on constate qu'un même gène peut être impliqué dans plusieurs pathologies et une pathologie peut être causée par différents gènes.
Par conséquent, dans les MNDREM, on observe des symptomatologies atypiques, des chevauchements ou « overlap » cliniques des maladies alléliques.
Aux Antilles françaises, les données de la littérature en matière de MNDREM sont limitées mais riches de certaines observations confortant les atypies comme par exemple le syndrome parkinsonien atypique des Caraïbes.
Sur le plan génétique, les investigations diagnostiques sont restreintes aux filières médicales basées en métropole et sur la comparaison de données génomiques de populations le plus souvent caucasiennes.
L'état des lieux des MNDREM en Martinique, et le travail expérimental d'analyse de données de NGS, sont les premiers travaux de ce type dans le domaine et dans notre région.
Elle n'avait presque jamais été utilisée dans l'analyse des MNDREM.
Nous l'avons employée car c'est une méthode de choix de recherche à la fois de variants simples et de variants de structures nouveaux.
Les résultats montrent une rentabilité diagnostic de 58% puisque nous avons identifié un variant probablement pathogène chez 7 patients sur 12 testés.
Nous avons trouvé ces variants dans des gènes connus de MNDREM parfois décrits dans des populations asiatiques, mais aussi, d'ascendance africaine ou caucasienne.
Certains de ces gènes peuvent être impliqués dans plusieurs symptomatologies, confortant le constat de chevauchement clinique dans ces maladies.
Ce travail jette les bases épidémiologiques des MNDREM aux Antilles et propose un socle de registre en la matière.
Sur le plan expérimental, il a permis de proposer une étiologie moleculaire impliquant des variants préalablement décrit ou nouveaux.
Il est également une preuve de concept quant aux moyens bioinformatiques d'analyse de données de NGS en Martinique.
Il ouvre la voie vers d'autres travaux du même genre, susceptibles de dresser les spécificités géniques des MNDREM de notre région et étoffer les données de la littérature scientifiques et médicales.
La thèse consiste en l'apprentissage d'une tâche complexe de robotique de manipulation en utilisant très peu d'aprioris.
Plus précisément, la tâche apprise consiste à atteindre un objet avec un robot série. L'objectif est de réaliser cet apprentissage sans paramètres de calibrage des caméras, modèles géométriques directs, descripteurs faits à la main ou des démonstrations d'expert.
L'apprentissage par renforcement profond est une classe d'algorithmes particulièrement intéressante dans cette optique.
En effet, l'apprentissage par renforcement permet d'apprendre une compétence sensori-motrice en se passant de modèles dynamiques.
Par ailleurs, l'apprentissage profond permet de se passer de descripteurs faits à la main pour la représentation d'état.
Cependant, spécifier les objectifs sans supervision humaine est un défi important.
Certaines solutions consistent à utiliser des signaux de récompense informatifs ou des démonstrations d'experts pour guider le robot vers les solutions.
D'autres consistent à décomposer l'apprentissage.
Par exemple, l'apprentissage "petit à petit" ou "du simple au compliqué" peut être utilisé.
Cependant, cette stratégie nécessite la connaissance de l'objectif en termes d'état.
D'autres approches utilisant plusieurs robots en parallèle peuvent également être utilisés mais nécessite du matériel coûteux.
Ainsi, nous décomposons la tâche d'atteinte en 3 sous tâches.
La première tâche consiste à apprendre à fixer un objet avec un système de deux caméras pour le localiser dans l'espace.
Cette tâche est apprise avec de l'apprentissage par renforcement profond et un signal de récompense faiblement supervisé.
Pour la tâche suivante, deux compétences sont apprises en parallèle : la fixation d'effecteur et une fonction de coordination main-oeil.
Le but de cette tâche est d'être capable de localiser l'effecteur du robot à partir des coordonnées articulaires.
En plus de la tâche d'atteinte, un predicteur d'atteignabilité d'objet est appris.
La principale contribution de ces travaux est l'apprentissage d'une tâche de robotique complexe en n'utilisant que très peu de supervision.
Cette thèse traite de l'apprentissage automatique pour la classification de données.
Nous proposons une nouvelle mesure d'incertitude qui permet de caractériser l'importance des données et qui améliore les performances de l'apprentissage actif par rapport aux mesures existantes.
Cette mesure détermine le plus petit poids nécessaire à associer à une nouvelle donnée
Les méthodes existantes d'apprentissage actif à partir de flux de données, sont initialisées avec quelques données étiquetées qui couvrent toutes les classes possibles.
Cependant, dans de nombreuses applications, la nature évolutive du flux fait que de nouvelles classes peuvent apparaître à tout moment.
Nous proposons une méthode efficace de détection active de nouvelles classes dans un flux de données multi-classes.
Cette méthode détermine de façon incrémentale une zone couverte par les classes connues, et détecte les données qui sont extérieures à cette zone et proches entre elles, comme étant de nouvelles classes.
Enfin, il est souvent difficile d'obtenir un étiquetage totalement fiable car l'opérateur humain est sujet à des erreurs d'étiquetage qui réduisent les performances du classifieur appris.
Cette problématique a été résolue par l'introduction d'une mesure qui reflète le degré de désaccord entre la classe donnée manuellement et la classe prédite et une nouvelle mesure d'"informativité" permettant d'exprimer la nécessité pour une donnée mal étiquetée d'être réétiquetée par un opérateur alternatif
Les systèmes de recommandation se sont imposés comme étant des outils indispensables face à une quantité de données qui ne cesse chaque jour de croître depuis l'avènement d'Internet.
Leur objectif est de proposer aux utilisateurs des items susceptibles de les intéresser sans que ces derniers n'aient besoin d'agir pour les obtenir.
Après s'être majoritairement focalisés sur la précision de la prédiction d'intérêt, ces systèmes ont évolué pour prendre en compte d'autres critères dans leur processus de recommandation, tels que les facteurs humains inhérents à la prise de décision, afin d'améliorer la qualité et l'utilité des recommandations.
Cependant, la prise en compte de certains facteurs humains tels que la diversité et le contexte demeure critiquable.
Alors que le contexte des utilisateurs est inféré sur la base d'informations collectées à l'insu de leur vie privée, la prise en compte de la diversité est quant à elle réduite à une dimension qu'un système se doit de maximiser.
Partant du postulat inverse selon lequel l'analyse de l'évolution de la diversité au cours du temps permet de définir le contexte de l'utilisateur, nous proposons dans ce manuscrit une nouvelle approche de modélisation contextuelle basée sur la diversité.
En effet, nous soutenons qu'une variation de diversité remarquable peut être la conséquence d'un changement de contexte et qu'il faut alors adapter la stratégie de recommandation en conséquence.
Nous présentons la première approche de la littérature permettant de modéliser en temps réel l'évolution de la diversité, ainsi qu'une nouvelle famille de contextes dits implicites n'exploitant aucune donnée sensible.
La possibilité de remplacer les contextes traditionnels (explicites) par les contextes implicites est confirmée de plusieurs manières.
Premièrement, nous démontrons sur deux corpus issus d'applications réelles qu'il existe un fort recouvrement entre les changements de contextes explicites et les changements de contextes implicites.
Deuxièmement, une étude utilisateur impliquant de nombreux participants nous permet de démontrer l'existence de liens entre les contextes explicites et les caractéristiques des items consultés dans ces derniers.
Fort de ces constats et du potentiel offert par nos modèles, nous présentons également plusieurs approches de recommandation et de prise en compte des besoins des utilisateurs
Ces travaux de recherche visent à développer des méthodes d'apprentissage automatique pour l'analyse de l'électroencéphalogramme (EEG) continu.
L'EEG continu est une modalité avantageuse pour l'évaluation fonctionnelle des états cérébraux en réanimation ou pour d'autres applications.
Les sous-parties de ce travail s'articulent autour de l'évaluation pronostique du coma post-anoxique, choisie comme application pilote.
Un petit nombre d'enregistrement longue durée a été réalisé, et des enregistrements existants ont été récupérés au CHU Grenoble.
Nous commençons par valider l'efficacité des réseaux de neurones profonds pour l'analyse EEG d'échantillons bruts.
Nous choisissons à cet effet de travailler sur la classification de stades de sommeil.
Nous utilisons un réseau de neurones convolutionnel adapté pour l'EEG que nous entrainons et évaluons sur le jeu de données SHHS (Sleep Heart Health Study).
Cela constitue le premier system neuronal à cette échelle (5000 patients) pour l'analyse du sommeil.
Les performances de classification atteignent ou dépassent l'état de l'art.
En utilisation réelle, pour la plupart des applications cliniques le défi principal est le manque d'annotations adéquates sur les patterns EEG ou sur de court segments de données (et la difficulté d'en établir).
Les annotations disponibles sont généralement haut niveau (par exemple, le devenir clinique) est sont donc peu nombreuses.
Nous recherchons comment apprendre des représentations compactes de séquences EEG de façon non-supervisée/semi-supervisée.
Le domaine de l'apprentissage non supervisé est encore jeune.
Pour se comparer aux travaux existants nous commençons avec des données de type image, et investiguons l'utilisation de réseaux adversaires génératifs (GANs) pour l'apprentissage adversaire non-supervisé de représentations.
La qualité et la stabilité de différentes variantes sont évaluées.
Nous appliquons ensuite un GAN de Wasserstein avec pénalité sur les gradients à la génération de séquences EEG.
Le système, entrainé sur des séquences mono-piste de patients en coma post anoxique, est capable de générer des séquences réalistes.
Nous développons et discutons aussi des idées originales pour l'apprentissage de représentations en alignant des distributions dans l'espace de sortie du réseau représentatif.
Pour finir, les signaux EEG multipistes ont des spécificités qu'il est souhaitable de prendre en compte dans les architectures de caractérisation.
Chaque échantillon d'EEG est un mélange instantané des activités d'un certain nombre de sources.
Partant de ce constat nous proposons un système d'analyse composé d'un sous-système d'analyse spatiale suivi d'un sous-système d'analyse temporelle.
Le sous-système d'analyse spatiale est une extension de méthodes de séparation de sources construite à l'aide de couches neuronales avec des poids adaptatifs pour la recombinaison des pistes, c'est à dire que ces poids ne sont pas appris mais dépendent de caractéristiques du signal d'entrée.
Nous montrons que cette architecture peut apprendre à réaliser une analyse en composantes indépendantes, si elle est entrainée sur une mesure de non-gaussianité.
Pour l'analyse temporelle, des réseaux convolutionnels classiques utilisés séparément sur les pistes recombinées peuvent être utilisés.
Les concepts de découverte et d'extraction de connaissances ainsi que d'inférencesont abordés sous différents angles au sein de la littérature scientifique.
En effet, de nombreux domaines s'y intéressent allant de la recherche d'information, à l'implication textuelle en passant par les modèles d'enrichissement automatique des bases de connaissances.
Ces concepts suscitent de plus en plus d'intérêt à la fois dans le monde académique et industriel favorisant le développement de nouvelles méthodes.
Cette thèse propose une approche automatisée pour l'inférence et l'évaluation de connaissances basée sur l'analyse de relations extraites automatiquement à partir de textes.
L'originalité de cette approche repose sur la définition d'un cadre tenant compte (i) de l'incertitude linguistique et de sa détection dans le langage naturel réalisée au travers d'une méthode d'apprentissage tenant compte d'une représentation vectorielle spécifique des phrases, (ii) d'une structuration des objets étudiés (e.g. syntagmes nominaux) sous la forme d'un ordre partiel tenant compte à la fois des implications syntaxiques et d'une connaissance a priori formalisée dans un modèle de connaissances de type taxonomique (iii) d'une évaluation des relations extraites et inférées grâce à des modèles de sélection exploitant une organisation hiérarchique des relations considérées.
Cette organisation hiérarchique permet de distinguer différents critères en mettant en oeuvre des règles de propagation de l'information permettant ainsi d'évaluer la croyance qu'on peut accorder à une relation en tenant compte de l'incertitude linguistique véhiculée.
Bien qu'a portée plus large, notre approche est ici illustrée et évaluée au travers de la définition d'un système de réponse à un questionnaire, généré de manière automatique, exploitant des textes issus du Web.
Nous montrons notamment le gain informationnel apporté par la connaissance a priori, l'impact des modèles de sélection établis et le rôle joué par l'incertitude linguistique au sein d'une telle chaîne de traitement.
Les travaux sur la détection de l'incertitude linguistique et la mise en place de la chaîne de traitement ont été validés par plusieurs publications et communications nationales et internationales.
Ce travail se situe dans le contexte de la recherche d'information (RI) utilisant des techniques d'intelligence artificielle (IA) telles que l'apprentissage profond (DL).
Il s'intéresse à des tâches nécessitant l'appariement de textes, telles que la recherche ad-hoc, le domaine du questions-réponses et l'identification des paraphrases.
L'objectif de cette thèse est de proposer de nouveaux modèles, utilisant les méthodes de DL, pour construire des modèles d'appariement basés sur la sémantique de textes, et permettant de pallier les problèmes de l'inadéquation du vocabulaire relatifs aux représentations par sac de mots, ou bag of words (BoW), utilisées dans les modèles classiques de RI.
En effet, les méthodes classiques de comparaison de textes sont basées sur la représentation BoW qui considère un texte donné comme un ensemble de mots indépendants.
Le processus d'appariement de deux séquences de texte repose sur l'appariement exact entre les mots.
La principale limite de cette approche est l'inadéquation du vocabulaire.
Ce problème apparaît lorsque les séquences de texte à apparier n'utilisent pas le même vocabulaire, même si leurs sujets sont liés.
Par exemple, la requête peut contenir plusieurs mots qui ne sont pas nécessairement utilisés dans les documents de la collection, notamment dans les documents pertinents.
Les représentations BoW ignorent plusieurs aspects, tels que la structure du texte et le contexte des mots.
Ces caractéristiques sont très importantes et permettent de différencier deux textes utilisant les mêmes mots et dont les informations exprimées sont différentes.
Un autre problème dans l'appariement de texte est lié à la longueur des documents.
Les parties pertinentes peuvent être réparties de manières différentes dans les documents d'une collection.
Ceci est d'autant vrai dans les documents volumineux qui ont tendance à couvrir un grand nombre de sujets et à inclure un vocabulaire variable.
Un document long pourrait ainsi comporter plusieurs passages pertinents qu'un modèle d'appariement doit capturer.
Contrairement aux documents longs, les documents courts sont susceptibles de concerner un sujet spécifique et ont tendance à contenir un vocabulaire plus restreint.
L'évaluation de leur pertinence est en principe plus simple que celle des documents plus longs.
Dans cette thèse, nous avons proposé différentes contributions répondant chacune à l'un des problèmes susmentionnés.
Tout d'abord, afin de résoudre le problème d'inadéquation du vocabulaire, nous avons utilisé des représentations distribuées des mots (plongement lexical) pour permettre un appariement basé sur la sémantique entre les différents mots.
Ces représentations ont été utilisées dans des applications de RI où la similarité document-requête est calculée en comparant tous les vecteurs de termes de la requête avec tous les vecteurs de termes du document, indifféremment.
Contrairement aux modèles proposés dans l'état-de-l'art, nous avons étudié l'impact des termes de la requête concernant leur présence/absence dans un document.
Nous avons adopté différentes stratégies d'appariement document/requête.
L'intuition est que l'absence des termes de la requête dans les documents pertinents est en soi un aspect utile à prendre en compte dans le processus de comparaison.
En effet, ces termes n'apparaissent pas dans les documents de la collection pour deux raisons possibles : soit leurs synonymes ont été utilisés ; soit ils ne font pas partie du contexte des documents en questions.
Cette thèse s'inscrit dans le domaine de la synthèse de la parole à partir du texte et traite, plus précisément, de la synthèse par corpus.
Nous présentons une méthode alternative de sélection de corpus : une méthode basée sur un algorithme glouton avec la divergence de Kullback-Leibler comme critère de sélection de phrases.
Cette approche vise à construire un corpus dont la distribution des unités tend vers une distribution cible fixée a priori.
Nous proposons également une mise à jour efficace du critère ce qui permet de diminuer significativement le temps de sélection du corpus.
C'est pourquoi la seconde partie de notre travail porte sur l'utilisation de la méthode proposée dans le cadre de l'adaptation de la base acoustique réduite pour une application précise.
Nous montrons que l'adaptation de la base réduite permet d'améliorer la qualité de la synthèse par rapport à celle obtenue avec des bases réduites mais non adaptées.
Un défi pour les systèmes de recherche basée sur le contenu réside dans la nécessité d'avoir une base annotée.
Cette thèse propose un système d'annotation d'images interactif par le regard afin d'alléger la tâche d'annotation.
Le but est de classer un petit ensemble d'images en fonction d'une catégorie cible (classification binaire) pour classer un grand ensemble d'images.
Parmi les caractéristiques du regard pointées comme informatives sur l'intention des utilisateurs, nous avons élaboré un estimateur d'intention par le regard, calculable en temps réel, indépendant de l'utilisateur et de la catégorie cible.
Cette annotation implicite est meilleure qu'une annotation aléatoire mais reste incertaine.
Dans une deuxième partie, les images ainsi annotées sont utilisées pour classifier un plus grand ensemble d'images avec un algorithme prenant en compte l'incertitude des labels : P-SVM combinant classification et régression.
Nous avons déterminé parmi différentes stratégies un critère de pertinence pour discriminer les labels les plus fiables, utilisés pour la classification, des labels les plus incertains, utilisés pour la régression.
La précision du P-SVM est évaluée dans différents contextes et peut atteindre les performances d'un algorithme de classification standard entraîné avec les labels certains.
Ces évaluations ont tout d'abord été menées sur un benchmark standard pour se comparer à l'état de l'art, et dans un second temps, sur une base d'images de nourriture.
La France possède une large base de données nationale regroupant les données de liquidation de l'Assurance Maladie, de mortalité et des données hospitalières : le Système National des Données de Santé (SNDS).
Celui-ci couvre actuellement la quasi-totalité de la population française de la naissance (ou immigration), au décès (ou émigration), en incluant tous les remboursements de frais médicaux ou paramédicaux.
En recueillant de manière systématique et prospective les dispensations médicamenteuses, les événements hospitaliers et les décès, le SNDS est doté d'un fort potentiel pour l'évaluation du médicament en vie réelle.
Suite au retrait mondial du rofecoxib en 2004, de nombreuses initiatives visant au développement et à l'évaluation de méthodologies adaptées aux bases de données populationnelles pour la surveillance des risques liés à l'usage du médicament ont vu le jour, en particulier le réseau EU-ADR en Europe (Exploring and Understanding Adverse Drug Reactions by integrative mining of clinical records and biomedical knowledge) et OMOP (Observational Outcomes Partnership) aux États-Unis.
Ces travaux ont démontré l'utilité des approches pharmaco-épidémiologiques pour la détection de signaux de pharmacovigilance.
Cependant, le SNDS n'a jamais été testé dans cette optique.
Sur les 3 approches étudiées, c'est la série de cas autocontrôlés qui a montré les meilleures performances dans UGIB et ALI avec des AUC respectifs de 0,80 et 0,94 et des MSE de 0,07 et 0,12.
Pour UGIB, les performances optimales ont été observées lorsque l'ajustement tenait compte des traitements concomitants et lorsque les 30 premiers jours d'exposition au médicament d'intérêt étaient utilisés comme fenêtre de risque.
Pour ALI, les performances optimales ont été également obtenues lors de l'ajustement en fonction des traitements concomitants, mais en utilisant une fenêtre de risque correspondant à l'ensemble de la période couverte par les dispensations de médicament d'intérêt.
L'utilisation de médicaments témoins négatifs a montré que l'erreur systématique résultant de l'application de l'approche et des paramètres optimaux dans le SNDS semblait faible, mais que les biais protopathiques et de confusion restaient présents.
Au total, ces travaux ont montré que les séries de cas autocontrôlées sont à considérer comme une approche adaptée à la détection d'alertes de pharmacovigilance associées à ALI et à UGIB dans le SNDS.
Un point de vue clinique demeure toutefois nécessaire pour écarter tout risque de faux positif résultant de potentiels biais résiduels.
L'application d'une telle approche à d'autres événements d'intérêt et son utilisation en routine constitueraient des progrès substantiels en matière de pharmacovigilance en France.
Les Machines de Boltzmann restreintes (RBM) sont des modèles graphiques capables d'apprendre simultanément une distribution de probabilité et une représentation des données.
Malgré leur architecture relativement simple, les RBM peuvent reproduire très fidèlement des données complexes telles que la base de données de chiffres écrits à la main MNIST.
Cependant, toutes les variantes de ce modèle ne sont pas aussi performantes les unes que les autres, et il n'y a pas d'explication théorique justifiant ces observations empiriques.
Dans la première partie de ma thèse, nous avons cherché à comprendre comment un modèle si simple peut produire des distributions de probabilité si complexes.
Nous avons pu caractériser théoriquement un régime compositionnel pour les RBM, et montré sous quelles conditions (statistique des poids, choix de la fonction de transfert) ce régime peut ou ne peut pas émerger.
Les prédictions qualitatives et quantitatives de cette analyse théorique sont en accord avec les observations réalisées sur des RBM entraînées sur des données réelles.
Nous avons ensuite appliqué les RBM à l'analyse et à la conception de séquences de protéines.
De part leur grande taille, il est en effet très difficile de simuler physiquement les protéines, et donc de prédire leur structure et leur fonction.
Il est cependant possible d'obtenir des informations sur la structure d'une protéine en étudiant la façon dont sa séquence varie selon les organismes.
Par exemple, deux sites présentant des corrélations de mutations importantes sont souvent physiquement proches sur la structure.
Dans le même esprit, nous avons montré sur plusieurs familles de protéines que les RBM peuvent aller au-delà de la structure, et extraire des motifs étendus d'acides aminés en coévolution qui reflètent les contraintes phylogénétiques, structurelles et fonctionnelles des protéines.
De plus, on peut utiliser les RBM pour concevoir de nouvelles séquences avec des propriétés fonctionnelles putatives par recombinaison de ces motifs.
La veille en santé animale, notamment la détection précoce de l'émergence d'agents pathogènes exotiques et émergents à l'échelle mondiale, est l'un des moyens de lutte contre l'introduction de ces agents pathogènes en France.
Récemment, il y a eu une réelle prise de conscience par les autorités sanitaires de l'utilité de l'information non-structurée concernant les maladies infectieuses publiée sur le Web.
Toutefois, pour l'élaborer, nous l'appliquons à cinq maladies animales infectieuses exotiques : la peste porcine africaine, la fièvre aphteuse, la fièvre catarrhale ovine, la maladie du virus Schmallenberg et l'influenza aviaire.
Nous démontrons que des techniques de fouille de textes, complétées par les connaissances d'experts du domaine, sont la fondation d'une veille sanitaire du Web à la fois efficace et réactive pour détecter des émergences de maladies exotiques au niveau international.
Notre outil sera utilisé par le dispositif de veille sanitaire internationale en France, et facilitera la détection précoce de signaux de dangers sanitaires émergents dans les articles médias du Web.
Les résultats obtenus montrent que (1) la perception et la production des voyelles sont fortement influencées par la L1 des apprenants aussi bien au début qu'après 9 mois d'apprentissage ; (2) les performances en perception et en production des néo-apprenants dépendent plus des voyelles elles-mêmes que de leur statut en L2 par rapport à la L1 (voyelles nouvelles, similaires, ou identiques) ; (3) les entraînements phonétiques que nous avons administrés n'apportent pas de bénéfice sur la perception et la production des voyelles orales françaises par les néo-apprenants tunisien.
Aujourd'hui, un utilisateur peut interagir avec des assistants virtuels, comme Alexa, Siri ou Cortana, pour accomplir des tâches dans un environnement numérique.
Dans ces systèmes, les liens entre des ordres exprimés en langage naturel et leurs réalisations concrètes sont précisées lors de la phase de conception.
Une approche plus adaptative consisterait à laisser l'utilisateur donner des instructions en langage naturel ou des démonstrations lorsqu'une tâche est inconnue de l'assistant.
Une solution adaptative devrait ainsi permettre à l'assistant d'agir sur un environnement numérique plus vaste composé de multiples domaines d'application et de mieux répondre aux besoins des utilisateurs.
Des systèmes robotiques, inspirés par des études portant sur le développement du langage chez l'humain, ont déjà été développés pour fournir de telles capacités d'adaptation.
Ici, nous étendons cette approche à l'interaction humaine avec un assistant virtuel qui peut, premièrement, apprendre le lien entre des commandes verbales et la réalisation d'actions basiques d'un domaine applicatif spécifique.
Ensuite, il peut apprendre des liens plus complexes en combinant ses connaissances procédurales précédemment acquises en interaction avec l'utilisateur.
La flexibilité du système est démontrée par sa forte adaptabilité au langage naturel, sa capacité à apprendre des actions dans de nouveaux domaines (Email, Wikipedia,...), et à former des connaissances procédurales hybrides en utilisant plusieurs services numériques, par exemple, en combinant une recherche Wikipédia avec un service de courrier électronique
Les systèmes actuels d'improvisation musicales sont capables de générer des séquences musicales unidimensionnelles par recombinaison du matériel musical.
Cependant, la prise en compte de plusieurs dimensions (mélodie, harmonie...) et la modélisation de plusieurs niveaux temporels sont des problèmes difficiles.
Dans cette thèse, nous proposons de combiner des approches probabilistes et des méthodes issues de la théorie des langages formels afin de mieux apprécier la complexité du discours musical à la fois d'un point de vue multidimensionnel et multi-niveaux dans le cadre de l'improvisation où la quantité de données est limitée.
Dans un premier temps, nous présentons un système capable de suivre la logique contextuelle d'une improvisation représentée par un oracle des facteurs tout en enrichissant son discours musical à l'aide de connaissances multidimensionnelles représentées par des modèles probabilistes interpolés.
Ensuite, ces travaux sont étendus pour modéliser l'interaction entre plusieurs musiciens ou entre plusieurs dimensions par un algorithme de propagation de croyance afin de générer des improvisations multidimensionnelles.
Enfin, nous proposons un système capable d'improviser sur un scénario temporel avec des informations multi-niveaux représenté par une grammaire hiérarchique.
Nous proposons également une méthode d'apprentissage pour l'analyse automatique de structures temporelles hiérarchiques.
Tous les systèmes sont évalués par des musiciens et improvisateurs experts lors de sessions d'
L'augmentation du nombre de documents multimédias rend nécessaire la mise en place de méthodes de structuration automatique capables de faciliter l'accès à l'information contenue dans tout type de documents.
Dans ce cadre, nous proposons deux types de structuration, linéaire et hiérarchique, s'appuyant sur les transcriptions automatiques de la parole prononcée dans les documents.
Les transcriptions sont exploitées par le biais de méthodes issues du traitement automatiques des langues adaptées aux spécificités des transcriptions automatiques – erreurs de transcription, faible nombre de répétitions de vocabulaire – grâce à la prise en compte de connaissances linguistiques et d'informations issues de la reconnaissance automatique de la parole et du signal.
Les expérimentations menées sur trois corpora composés de journaux télévisés et d'émissions de reportages montrent que ces approches conduisent à une amélioration des performances des méthodes de structuration développées.
Nos travaux de thèse ont pour but d'étudier la manifestation de la néologie dans un dictionnaire de langue française.
Nous avons choisi de travailler à partir de la version électronique du "Nouveau Petit Robert" parce qu'il incarne un dictionnaire de référence.
Nous avons étudié les mots nouveaux attestés entre 1990 et 2012 dans le "Nouveau Petit Robert Électronique" 2012.
Le corpus dont nous disposons se compose de 477 mots nouveaux attestés entre 1990 et 2012.
Nous avons aussi consacré une partie à la lexicographie où nous avons mis en lumière les particularités lexicographiques du "Nouveau Petit Robert Électronique" 2012 en comparant quelques extraits des mots du corpus avec deux autres dictionnaires : "Le Petit Larousse Illustré" 2016 et le "Wiktionnaire".
Les modes de formation les plus performants des mots du corpus sont ceux relevant de la matrice interne avec un total de 266 mots et ceux relevant de la matrice externe qui regroupe 186 mots.
Les mots du corpus appartenant à la matrice interne sont les plus nombreux, cela montre que le lexique se renouvelle par lui-même.
Nous avons observé qu'il y avait un déclin des sciences humaines au profit de sciences techniques.
Cette thèse est une étude de l'interlangue prosodique des apprenants d'anglais dont la langue maternelle est le français ou l'espagnol.
Elle est organisée en deux parties principales.
La première est une étude sur les méthodes de conception et de représentation de la prosodie pour l'analyse de l'interlangue, un système linguistique hybride présentant des caractéristiques de la langue maternelle de l'apprenant, des caractéristiques de la langue cible et des caractéristiques intermédiaires ou développementales.
Cette partie débouche sur un cadre méthodologique pour l'analyse phonétique et pour l'interprétation phonologique de ce type de systèmes prosodiques.
Les résultats montrent des traces de l'influence de leurs langues maternelles respectives à un niveau phonétique et phonologique, ainsi que des caractéristiques développementales communes aux deux groupes de locuteurs.
Les résultats servent de base pour une réflexion sur les niveaux d'abstraction de l'étude de la prosodie et sur les priorités didactiques pour l'enseignement de l'anglais oral au niveau universitaire.
Les études sur le figement ont été remises à l'honneur par le traitement automatique des langues naturelles.
En effet, la fréquence des expressions figées dans les textes rend leur description indispensable.
Pour cette raison, nous avons entrepris l'analyse des adjectivaux prédicatifs.
Ainsi sur la base du modèle des classes d'objets, nous avons élaboré une typologie sémantico-syntaxique de ces suites.
Notre recherche a pour objectif final la création d'un dictionnaire électronique des adjectivaux prédicatifs.
Ce travail s'articule autour de trois points essentiels : le figement des adjectivaux, leurs propriétés définitionnelles et enfin leur classification fondée sur des propriétés distributionnelles communes.
Ces travaux de recherche portent sur l'aide à l'exploration de bases de données.
La particularité de l'approche proposée repose sur un principe de co-évolution de l'utilisateur et d'une interface intelligente.
Cette dernière devant permettre d'apporter une aide à la compréhension du domaine représenté par les données.
Pour cela, une métaphore de musée virtuel vivant a été adoptée.
Ce musée évolue de façon incrémentale au fil des interactions de l'utilisateur.
Il incarne non seulement les données mais également des informations sémantiques explicitées par un modèle de connaissances spécifique au domaine exploré.
A travers l'organisation topologique et l'évolution incrémentale, le musée personnalise en ligne le parcours de l'utilisateur.
L'approche est assurée par trois mécanismes principaux : l'évaluation du profil de l'utilisateur modélisé par une pondération dynamique d'informations sémantiques, l'utilisation de ce profil dynamique pour établir une recommandation ainsi que l'incarnation des données dans le musée.
L'approche est appliquée au domaine du patrimoine dans le cadre du projet ANTIMOINE, financé par l'Agence Nationale de la Recherche (ANR).
La généricité de cette dernière a été démontrée à travers son application à une base de données de publications mais également à travers l'utilisation de types d'interfaces variés (site web, réalité virtuelle).Des expérimentations ont permis de valider l'hypothèse que notre système s'adapte aux évolutions des comportements de l'utilisateur et qu'il est capable, en retour, d'influencer ce dernier.
Elles ont également permis de comparer une interface 2D avec une interface 3D en termes de qualité de perception, de guidage, de préférence et d'efficacité.
Cette hypothèse permet de développer directement des garanties théoriques sur la précision de l'apprentissage.
Cependant, elle n'est pas réaliste dans un grand nombre de domaines applicatifs qui ont émergé au cours des dernières années.
Dans cette thèse, nous nous intéressons à quatre problèmes différents en intelligence artificielle, unis par un point commun : tous impliquent un transfer de connaissance d'un domaine vers un autre.
Le premier problème est le raisonnement par analogie et s'intéresse à des assertions de la forme "A est à B ce que C est à D".
Le second est l'apprentissage par transfert et se concentre sur des problèmes de classification dans des contextes où les données d'entraînement et de test ne sont pas de même distribution (ou n'appartiennent même pas au même espace).
Le troisième est l'apprentissage sur flux de données, qui prend en compte des données apparaissant continument une à une à haute fréquence, avec des changements de distribution.
Le dernier est le clustering collaboratif et consiste à faire échanger de l'information entre algorithmes de clusterings pour améliorer la qualité de leurs prédictions.
Ce cadre s'appuie sur la notion de complexité de Kolmogorov, qui mesure l'information continue dans un objet.
Cet outil est particulièrement adapté au problème de transfert, du fait qu'il ne repose pas sur la notion de probabilité tout en étant capable de modéliser les changements de distributions.
En plus de cet effort de modélisation, nous proposons dans cette thèse diverses discussions sur d'autres aspects ou applications de ces problèmes.
Ces discussions s'articulent autour de la possibilité de transfert dans différents domaines et peuvent s'appuyer sur d'autres outils que la complexité.
L'anticipation des besoins des clients est cruciale pour toute entreprise — c'est particulièrement vrai des banques d'investissement telles que BNP Paribas Corporate and Institutional Banking au vu de leur rôle dans les marchés financiers.
Cette thèse s'intéresse au problème de la prédiction des intérêts futurs des clients sur les marchés financiers, et met plus particulièrement l'accent sur le développement d'algorithmes ad hoc conçus pour résoudre des problématiques spécifiques au monde financier.
Ce manuscrit se compose de cinq chapitres, répartis comme suit :
- Le chapitre 1 expose le problème de la prédiction des intérêts futurs des clients sur les marchés financiers.
Le but de ce chapitre est de fournir aux lecteurs toutes les clés nécessaires à la bonne compréhension du reste de cette thèse.
Ces clés sont divisées en trois parties : une mise en lumière des jeux de données à notre disposition pour la résolution du problème de prédiction des intérêts futurs et de leurs caractéristiques, une vue d'ensemble, non exhaustive, des algorithmes pouvant être utilisés pour la résolution de ce problème, et la mise au point de métriques permettant d'évaluer la performance de ces algorithmes sur nos jeux de données.
Ce chapitre se clôt sur les défis que l'on peut rencontrer lors de la conception d'algorithmes permettant de résoudre le problème de la prédiction des intérêts futurs en finance, défis qui seront, en partie, résolus dans les chapitres suivants ;
- Le chapitre 2 compare une partie des algorithmes introduits dans le chapitre 1 sur un jeu de données provenant de BNP Paribas CIB, et met en avant les difficultés rencontrées pour la comparaison d'algorithmes de nature différente sur un même jeu de données, ainsi que quelques pistes permettant de surmonter ces difficultés.
Ce comparatif met en pratique des algorithmes de recommandation classiques uniquement envisagés d'un point de vue théorique au chapitre précédent, et permet d'acquérir une compréhension plus fine des différentes métriques introduites au chapitre 1 au travers de l'analyse des résultats de ces algorithmes ;
-Le chapitre 3 introduit un nouvel algorithme, Experts Network, i.e., réseau d'experts, conçu pour résoudre le problème de l'hétérogénéité de comportement des investisseurs d'un marché donné au travers d'une architecture de réseau de neurones originale, inspirée de la recherche sur les mélanges d'experts.
Dans ce chapitre, cette nouvelle méthodologie est utilisée sur trois jeux de données distincts : un jeu de données synthétique, un jeu de données en libre accès, et un jeu de données provenant de BNP Paribas CIB.
Ce chapitre présente aussi en plus grand détail la genèse de l'algorithme et fournit des pistes pour l'améliorer ;
- Le chapitre 4 introduit lui aussi un nouvel algorithme, appelé History-augmented collaborative filtering, i.e., filtrage collaboratif augmenté par historiques, qui proposes d'augmenter les approches de factorisation matricielle classiques à l'aide des historiques d'interaction des clients et produits considérés.
Ce chapitre poursuit l'étude du jeu de données étudié au chapitre 2 et étend l'algorithme introduit avec de nombreuses idées.
Plus précisément, ce chapitre adapte l'algorithme de façon à permettre de résoudre le problème du cold start, i.e., l'incapacité d'un système de recommandation à fournir des prédictions pour de nouveaux utilisateurs, ainsi qu'un nouveau cas d'application sur lequel cette adaptation est essayée ;
- Le chapitre 5 met en lumière une collection d'idées et d'algorithmes, fructueux ou non, qui ont été essayés au cours de cette thèse.
Ce chapitre se clôt sur un nouvel algorithme mariant les idées des algorithmes introduits aux chapitres 3 et 4.
Les diagrammes de décision Multi-valués (MDD) sont des structures de données efficaces et largement utilisées dans les domaines tels que la vérification, l'optimisation et la programmation dynamique.
Dans cette thèse, nous commençons par améliorer les principaux algorithmes tels que la réduction de MDD, permettant aux MDD de potentiellement compresser exponentiellement des ensembles de tuples, ou la combinaison de MDD, tels que l'intersection ou l'union.
Ensuite, nous proposons des versions parallèles de ces algorithmes ainsi que des versions permettant de travailler avec la version non déterministe des MDD.
De plus, dans le domaine des MDD relâchés, un domaine de plus en plus étudié, nous définissons les notions de réduction et combinaison relâchés, ainsi que leurs algorithmes associés.
Nous résolvons le problème de l'échantillonnage des solutions d'un MDD avec respect de loi de probabilité tels que des fonctions de probabilité de masse ou des chaines de Markov.
Grâce à eux, nous montrons que nous pouvons reformuler plusieurs contraintes et en définir de nouvelles tout en étant basés sur des MDD.
Finalement nous appliquons nos algorithmes à des problèmes industriels réels de génération de texte et musique, et de modélisation de réservoir de pétrole.
Un des principaux pans du traitement automatique des langues (TAL) est l'extraction sous forme structurée des informations contenues dans un document.
Cette extraction est généralement constituée de trois étapes : l'extraction d'entités nommées, des relations les liant au sein du texte et enfin celle des événements.
La notion d'événement recouvre différents phénomènes caractérisés par un nombre variable d'actants.
L'extraction d'événements consiste alors à identifier la présence d'un événement puis à en déterminer les arguments, c'est-à-dire les différentes entités y remplissant des rôles spécifiques.
Les meilleures approches actuelles, reposant sur différents modèles neuronaux, se focalisent sur le voisinage direct du mot dans la phrase.
Les informations présentes dans le reste du document sont alors généralement ignorées.
Cette thèse présente donc différentes approches visant à exploiter ce contexte distant au sein du document.
Nous reproduisons en premier lieu un modèle convolutif obtenant des performances à l'état de l'art et en analysons plusieurs paramètres.
Nous réalisons ensuite une expérience permettant d'illustrer le fait que ce modèle, malgré ses bonnes performances, n'exploite effectivement qu'un contexte très restreint au niveau phrastique.
Dans un deuxième temps, nous présentons deux méthodes de production et d'intégration d'une représentation du contexte distant à un modèle neuronal opérant au niveau intra-phrastique.
Nous montrons par ailleurs la supériorité de cette approche sur une représentation générique du document.
Une seconde contribution, répondant aux limitations de la première méthode, permet d'exploiter dynamiquement, pour chaque cible de prédiction, une représentation des phrases les plus pertinentes au sein du contexte grâce à un modèle de convolution de graphe.
Cette méthode permet d'obtenir les meilleures performances pour un modèle simple sur différents jeux de données.
Enfin, dans un troisième temps, nous considérons une autre approche de la prise en compte du contexte inter-phrastique.
Nous cherchons à modéliser plus directement les interdépendances entre les différentes instances d'événements au sein d'un document afin de réaliser une prédiction jointe.
Nous utilisons pour cela le cadre d'apprentissage PSL (Probabilistic Soft Logic) qui permet de modéliser de telles interdépendances sous forme de règles logiques.
La résistance aux antimicrobiens (AMR) est une préoccupation majeure de santé publique mondiale.
Le Plan d'Action National du Viet Nam sur la résistance aux antimicrobiens a reconnu la surveillance comme l'un des éléments essentiels du contrôle.
Cependant, le système actuel de surveillance de l'AMR (AMRSS) au Vietnam est susceptible de sur-représenter les infections graves et les infections nosocomiales (HAI), ce qui pourrait entraîner une surestimation de la résistance des infections acquises dans la communauté (CAI).
Cette thèse vise à évaluer l'AMRSS au Viet Nam et à faire des suggestions pour optimiser l'efficacité de l'AMRSS en fournissant des données AMR précises et représentatives pour les patients CAI dans ce contexte.
Une revue systématique de la littérature a été menée pour produire un aperçu des AMRSS qui ont été mis en œuvre à l'échelle mondiale et des évaluations de ces systèmes.
Il n'y a pas de cadre normalisé ni de lignes directrices pour mener une évaluation de l'AMRSS.
Moins de 10% des systèmes ont signalé une évaluation du système, en se concentrant sur quelques attributs tels que la représentativité, l'opportunité, le biais, le coût, la couverture et la sensibilité.
La sensibilité de l'AMRSS était de l'ordre de 2 à 5% et est restée similaire entre les deux périodes.
Il y a eu un retard dans la soumission des données des hôpitaux, ce qui a affecté la rapidité de la surveillance.
Aucune évaluation du système de surveillance n'a été effectuée pour identifier les problèmes et mettre en œuvre des solutions rapides.
Dans l'ensemble, les résultats ont montré que la précision des données sur l'AMR est améliorée lorsque le nombre d'hôpitaux augmente (diminution de 0,6% de l'erreur quadratique moyenne pour un hôpital).
Pour un budget donné, le nombre optimal d'hôpitaux par type peut être déterminé en utilisant cette approche de modélisation pour identifier un système avec les meilleures valeurs pour chaque attribut de performance.
Les résultats indiquent que l'AMRSS actuel peut augmenter les proportions d'hôpitaux spécialisés et provinciaux pour accroître la précision des données et la représentativité du système.
Les résultats sont valables pour un AMRSS avec des structures organisationnelles et des protocoles de collecte de données similaires de VINARES.
Le budget que le gouvernement et les partenaires de développement étrangers sont prêts à consacrer à la surveillance de l'AMR est également un facteur important pour identifier la combinaison optimale des hôpitaux pour l'AMRSS.
Nous nous intéressons à la fois à la modélisation des fréquences des mots dans les collections textuelles et aux modèles probabilistes de recherche d'information (RI).
Concernant les modèles statistiques de fréquences de mots, nous portons notre attention sur l'étude du phénomène de rafale (burstiness).
Nous établissons une propriété sur les distributions de probabilité caractérisant leur capacité à modéliser ce phénomène et nous étudions ensuite les distributions Beta Negative Binomial et Log-Logistique pour la modélisation des fréquences de mots.
Nous portons ensuite notre attention sur les modèles probabilistes de RI et leur propriétés fondamentales.
Nous pouvons montrer que les modèles classiques ne reposent pas sur des lois de probabilité en rafale, même si des propriétés fondamentales comme la concavité des modèles permettent implicitement de le prendre en compte.
Enfin, nous étudions empiriquement et théoriquement les modèles de rétro-pertinence.
Nous proposons un cadre théorique qui permet ainsi d'expliquer leurs caractéristiques empiriques et leur performances.
Ceci permet entre autres de mettre en avant les propriétés importantes des modèles de retro-pertinence et de montrer que certains modèles de référence sont déficients.
Cette thèse traite de la détection de fraude par carte de crédit.
Selon la Banque Centrale Européenne, la valeur des fraudes utilisant des cartes en 2016 s'élevait à 1,8 milliard d'euros.
Le défis pour les institutions est de réduire ces fraudes.
En règle générale, les systèmes de détection de la fraude sont consistués d'un système automatique construit à base de règles "si-alors" qui contrôlent toutes les transactions en entrée et déclenchent une alerte si la transaction est considérée suspecte.
Un groupe expert vérifie l'alerte et décide si cette dernière est vrai ou pas.
Les critères utilisés dans la sélection des règles maintenues opérationnelles sont principalement basés sur la performance individuelle des règles.
Cette approche ignore en effet la non-additivité des règles.
Nous proposons une nouvelle approche utilisant des indices de puissance.
Cette approche attribue aux règles un score normalisé qui quantifie l'influence de la règle sur les performances globales du groupe de règles.
Les indice utilisés sont le "Shapley Value" et le "Banzhaf Value".
Leurs applications sont : 1) Aide à la décision de conserver ou supprimer une règle ; 2) Sélection du nombre k de règles les mieux classées, afin de travailler avec un ensemble plus compact.
En utilisant des données réelles de fraude par carte de crédit, nous montrons que :
1) Cette approche permet de mieux évaluer les performances du groupe plutot que de les évaluer isolément.
2) La performance de l'ensemble des règles peut être atteinte en conservant le dixième des règles.
Nous observons que cette application peut être comsidérée comme une tâche de sélection de caractéristiques : ainsi nous montrons que notre approche est comparable aux algorithmes courants de sélection des caractéristiques.
Il présente un avantage dans la gestion des règles, car attribue un score normalisé à chaque règle.
Ce qui n'est pas le cas pour la plupart des algorithmes, qui se concentrent uniquement sur une solution d'ensemble.
Nous proposons une nouvelle version du Banzhaf Value, à savoir le k-Banzhaf ; qui surclasse la précedente en terme de temps de calcul et possède des performances comparables.
Enfin, nous mettons en œuvre un processus d'auto-apprentissage afin de renforcer l'apprentissage dans un algorithme.
Nous comparons ces derniers avec nos trois indices de puissance pour effectuer une classification sur les données de fraude par carte de crédit.
En conclusion, nous observons que la sélection de caractéristiques basée sur les indices de puissance a des résultats comparables avec les autres algorithmes dans le processus d'auto-apprentissage.
Les systèmes de dialogue incrémentaux sont capables d'entamer le traitement des paroles de l'utilisateur au moment même où il les prononce (sans attendre de signal de fin de phrase tel un long silence par exemple).
Ils peuvent ainsi prendre la parole à n'importe quel moment et l'utilisateur peut faire de même (et interrompre le système).
De ce fait, ces systèmes permettent d'effectuer une plus large palette de comportements de prise de parole en comparaison avec les systèmes de dialogue traditionnels.
Cette thèse s'articule autour de la problématique suivante : est-il possible pour un système de dialogue incrémental d'apprendre une stratégie optimale de prise de parole de façon autonome ?
Tout d'abord, une analyse des mécanismes sous-jacents à la dynamique de prise de parole dans une conversation homme-homme a permis d'établir une taxonomie de ces phénomènes.
Ensuite, une nouvelle architecture permettant de doter les systèmes de dialogues conventionnels de capacités de traitement incrémentales de la parole, à moindre coût, a été proposée.
Dans un premier temps, un simulateur de dialogue destiné à répliquer les comportements incrémentaux de l'utilisateur et de la reconnaissance vocale a été développé puis utilisé pour effectuer les premier tests de stratégies de dialogue incrémentales.
Ces dernières ont été développées à base de règles issues de l'analyse effectuée lors de l'établissement de la taxonomie des phénomènes de prise de parole.
Les résultats de la simulation montrent que le caractère incrémental permet d'obtenir des interactions plus efficaces.
La meilleure stratégie à base de règles a été retenue comme référence pour la suite.
Dans un second temps, une stratégie basée sur l'apprentissage par renforcement a été implémentée.
Elle est capable d'apprendre à optimiser ses décisions de prise de parole de façon totalement autonome étant donnée une fonction de récompense.
Une première comparaison, en simulation, a montré que cette stratégie engendre des résultats encore meilleurs par rapport à la stratégie à base de règles.
En guise de validation, une expérience avec des utilisateurs réels a été menée (interactions avec une maison intelligente).
Pouvoir manipuler et de comparer de mesures de probabilité est essentiel pour de nombreuses applications en apprentissage automatique.
Le transport optimal (TO) définit des divergences entre distributions fondées sur la géométrie des espaces sous-jacents : partant d'une fonction de coût définie sur l'espace dans lequel elles sont supportées, le TO consiste à trouver un couplage entre les deux mesures qui soit optimal par rapport à ce coût.
En dépit de ces avantages, l'emploi du TO pour les sciences des données a longtemps été limité par les difficultés mathématiques et computationnelles liées au problème d'optimisation sous-jacent.
Pour contourner ce problème, une approche consiste à se concentrer sur des cas particuliers admettant des solutions en forme close, ou pouvant se résoudre efficacement.
En particulier, le TO entre mesures elliptiques constitue l'un des rares cas pour lesquels le TO admet une forme close, définissant la géométrie de Bures-Wasserstein (BW).
Cette thèse s'appuie tout particulièrement sur la géométrie de BW, dans le but de l'utiliser comme outil de base pour des applications en sciences des données.
Pour ce faire, nous considérons des situations dans lesquelles la géométrie de BW est tantôt utilisée comme un outil pour l'apprentissage de représentations, étendue à partir de projections sur des sous-espaces, ou régularisée par un terme entropique.
Dans une première contribution, la géométrie de BW est utilisée pour définir des plongements sous la forme de distributions elliptiques, étendant la représentation classique sous forme de vecteurs de R^d.
Dans une deuxième contribution, nous prouvons l'existence de transports qui extrapolent des applications restreintes à des projections en faible dimension, et montrons que ces plans "sous-espace optimaux" admettent des formes closes dans le cas de mesures gaussiennes.
La troisième contribution de cette thèse consiste à obtenir des formes closes pour le transport entropique entre des mesures gaussiennes non-normalisées, qui constituent les premières expressions non triviales pour le transport entropique.
Finalement, dans une dernière contribution nous utilisons le transport entropique pour imputer des données manquantes de manière non-paramétrique, tout en préservant les distributions sous-jacentes.
Les assistants vocaux font partie de notre vie quotidienne.
Leurs performances sont mises à l'épreuve en présence de distorsions du signal, telles que le bruit, la réverbération et les locuteurs simultanés.
Cette thèse aborde le problème de l'extraction du signal d'intérêt dans de telles conditions acoustiques difficiles en localisant d'abord le locuteur cible puis en utilisant la position spatiale pour extraire le signal de parole correspondant.
Dans un premier temps, nous considérons la situation courante où le locuteur cible prononce un mot ou une phrase connue, comme le mot de réveil d'un système de commande vocale mains-libres.
Nous proposons une méthode afin d'exploiter cette information textuelle pour améliorer la localisation du locuteur en présence de locuteurs simultanés.
La solution proposée utilise un système de reconnaissance vocale pour aligner le mot de réveil au signal vocal corrompu.
Un spectre de référence représentant les phones alignés est utilisé pour calculer un identifiant qui est ensuite utilisé par un réseau de neurones profond pour localiser le locuteur cible.
Les résultats sur des données simulées montrent que la méthode proposée réduit le taux d'erreur de localisation par rapport à la méthode classique GCC-PHAT.
Des améliorations similaires sont constatées sur des données réelles.
Étant donnée la position spatiale estimée du locuteur cible, la séparation de la parole est effectuée en trois étapes.
Dans la première étape, une simple formation de voie delay-and-sum (DS) est utilisée pour rehausser le signal provenant de cette direction, qui est utilisé dans la deuxième étape par un réseau de neurones pour estimer un masque temps-fréquence.
Ce masque est utilisé pour calculer les statistiques du second ordre et pour effectuer une formation de voie adaptative dans la troisième étape.
Un ensemble de données réverbéré, bruité avec plusieurs canaux et plusieurs locuteurs---inspiré du célèbre corpus WSJ0-2mix---a été généré et la performance de la méthode proposée a été étudiée en terme du taux d'erreur sur les mots (WER).
Pour rendre le système plus robuste aux erreurs de localisation, une approche par déflation guidée par la localisation (SLOGD) qui estime les sources de manière itérative est proposée.
À chaque itération, la position spatiale d'un locuteur est estimée puis utilisée pour estimer un masque correspondant à ce même locuteur.
La source estimée est retirée du mélange avant d'estimer la position et le masque de la source suivante.
La méthode proposée surpasse Conv-TasNet.
Enfin, le problème d'expliquer la robustesse des réseaux de neurones utilisés pour calculer les masques temps-fréquence à des conditions de bruit différentes.
Nous utilisons la méthode dite SHAP pour quantifier la contribution de chaque point temps-fréquence du signal d'entrée au masque temps-fréquence estimé.
Nous définissons une métrique qui résume les valeurs SHAP et montrons qu'elle est corrélée au WER obtenu sur la parole séparée.
À notre connaissance, il s'agit de la première étude sur l'explicabilité des réseaux de neurones dans le contexte de la séparation de la parole.
Cette thèse propose plusieurs contributions sur le thème de la détection d'implications textuelles (DIT).
La DIT est la capacité humaine, étant donné deux textes, à pouvoir dire si le sens du second texte peut être déduit à partir de celui du premier.
Une des contributions apportée au domaine est un système de DIT hybride prenant les analyses d'un analyseur syntaxique stochastique existant afin de les étiqueter avec des rôles sémantiques, puis transformant les structures obtenues en formules logiques grâce à des règles de réécriture pour tester finalement l'implication à l'aide d'outils de preuve.
L'autre contribution de cette thèse est la génération de suites de tests finement annotés avec une distribution uniforme des phénomènes couplée avec une nouvelle méthode d'évaluation des systèmes utilisant les techniques de fouille d'erreurs développées par la communauté de l'analyse syntaxique permettant une meilleure identification des limites des systèmes.
Pour cela nous créons un ensemble de formules sémantiques puis nous générons les réalisations syntaxiques annotées correspondantes à l'aide d'un système de génération existant.
Nous testons ensuite s'il y a implication ou non entre chaque couple de réalisations syntaxiques possible.
Enfin nous sélectionnons un sous-ensemble de cet ensemble de problèmes d'une taille donnée et satisfaisant un certain nombre de contraintes à l'aide d'un algorithme que nous avons développé.
Cette thèse est consacrée à l'étude des systèmes de recommandation basés sur des réseaux de neurones artificiels appris pour faire de l'ordonnancement de produits avec des retours implicites (sous forme de clics).
Dans ce sens, nous proposons un nouveau modèle neuronal qui apprend conjointement la représentation des utilisateurs et des produits dans un espace latent, ainsi que la relation de préférence des utilisateurs sur les produits.
Nous montrons que le modèle proposé est apprenable au sens du principe de la minimisation du risque empirique et performant par rapport aux autres modèles de l'état de l'art sur plusieurs collections.
En outre, nous contribuons à la création de deux nouvelles collections, produites grâce aux enregistrements des comportements de clients de Kelkoo (https : //www.kelkoo.com/);
Les deux jeux de données recueillent des retours implicites des utilisateurs sur des produits, ainsi qu'un grand nombre d'informations contextuelles concernant à la fois les clients et les produits.
La collections de données de Purch contient en plus une information sur la popularité des produits ainsi que des commentaires textuelles associés.
Mots clés.
Systèmes de recommandation, apprentissage d'ordonnancement, réseaux de neurones, recommandations avec des retours implicites, Modèles probabilistes latents temporels
En tant que syntagmes sémantiquement non-compositionnels, les locutions sont des unités lexicales à part entière, qui doivent avoir leur propre entrée dans un modèle du lexique.
De plus, en vertu de leur signifiant syntagmatique, les locutions témoignent – à des degrés divers – d'une flexibilité formelle (passivation, insertion de modificateurs, substitution de certains constituants, etc.).
Notre thèse défend l'idée selon laquelle une description des locutions combinant à la fois l'identification des unités lexicales qui les composent et l'identification des relations de dépendance syntaxique qui unissent les unités constituantes, permettra de prédire leurs différents emplois possibles dans la phrase.
Une telle description n'est possible que dans un modèle du lexique décrivant précisément la combinatoire des lexies.
La thèse a deux principaux apports.
Le premier est le développement d'un modèle de description lexico-syntaxique relativement fine des locutions du français.
Le second est l'identification et l'étude de différentes variations structurales, syntaxiques et lexicales liées à la flexibilité formelle des locutions.
Les variations des locutions sont mises en corrélation avec leurs structures lexico-syntaxiques, mais également avec leurs définitions lexicographiques.
Ceci nous conduit à introduire la notion de projection structurale, centrale dans le continuum de la flexibilité formelle des locutions
L'objectif de cette thèse est de faciliter l'utilisation de l'apprentissage supervisé dans les systèmes de détection pour renforcer la détection.
Dans ce but, nous considérons toute la chaîne de traitement de l'apprentissage supervisé (annotation, extraction d'attributs, apprentissage, et évaluation) en impliquant les experts en sécurité.
Tout d'abord, nous donnons des conseils méthodologiques pour les aider à construire des modèles de détection supervisés qui répondent à leurs contraintes opérationnelles.
De plus, nous concevons et nous implémentons DIADEM, un outil de visualisation interactif qui aide les experts en sécurité à appliquer la méthodologie présentée.
DIADEM s'occupe des rouages de l'apprentissage supervisé pour laisser les experts en sécurité se concentrer principalement sur la détection.
Par ailleurs, nous proposons une solution pour réduire le coût des projets d'annotations en sécurité informatique.
Nous concevons et implémentons un système d'apprentissage actif complet, ILAB, adapté aux besoins des experts en sécurité.
Nos expériences utilisateur montrent qu'ils peuvent annoter un jeu de données avec une charge de travail réduite grâce à ILAB.
Enfin, nous considérons la génération automatique d'attributs pour faciliter l'utilisation de l'apprentissage supervisé dans les systèmes de détection.
Nous définissons les contraintes que de telles méthodes doivent remplir pour être utilisées dans le cadre de la détection de menaces.
Nous comparons trois méthodes de l'état de l'art en suivant ces critères, et nous mettons en avant des pistes de recherche pour mieux adapter ces techniques aux besoins des experts en sécurité.
Apprendre une nouvelle langue signifie exploiter, plus ou moins consciemment,l'espace de la proximité du système linguistique cible avec d'autres langues.
Nous nous intéressons à ces transferts et à l'influence que ceux-ci exercent sur l'interlangue, dans le cas des apprenants roumains de FLE, étudiant aussi l'italien et l'espagnol.
L'analyse des données nous permet de rendre compte du rapport qui s'établit entre la conformité avec la norme et la complexité de l'interlangue et aussi de déterminer le rôle des transferts linguistiques positifs et négatifs par rapport à ces deux dimensions de l'interlangue (la complexité et la conformité)
L'enjeu majeur de cette thèse réside dans l'amélioration de l'adéquation entre l'information retournée et les attentes des utilisateurs à l'aide de profils riches et efficaces.
Il s'agit donc d'exploiter au maximum les retours utilisateur (qu'ils soient donnés sous la forme de clics, de notes ou encore d'avis écrits) et le contexte.
Durant ces travaux de thèse, nous avons choisi d'exploiter les textes écrit par les utilisateurs pour affiner leurs profils et contextualiser la recommandation.
À cette fin, nous avons utilisé les avis postés sur les sites spécialisés (IMDb, RateBeer, BeerAdvocate) et les boutiques en ligne (Amazon) ainsi que les messages postés sur Twitter.
En plus d'aider à l'amélioration des performances du système, elle permet d'apporter une forme d'explication quant aux items proposés.
Ainsi, nous proposons d'accompagner l'utilisateur dans son accès à l'information au lieu de le contraindre à un ensemble d'items que le système juge pertinents.
Elles ont donné lieu à de nombreuses études en Traitement Automatique du Langage Naturel.
En effet, leur étude et leur identification précise sont primordiales, sur les plans théorique et applicatif.
Cependant, la majorité des travaux de recherche sur le sujet portent sur des usages de langage quotidien : dialogues " à bâtons rompus ", demandes d'horaire, discours, etc.
Mais qu'en est-il des productions orales spontanées produites dans un cadre contraint ?
Aucune étude n'a à notre connaissance été menée dans ce contexte.
Or, on sait que l'utilisation d'une " langue de spécialité " dans le cadre d'une tâche donnée entraîne des comportements spécifiques.
Notre travail de thèse est consacré à l'étude linguistique et informatique des disfluences dans un tel cadre.
Il s'agit de dialogues de contrôle de trafic aérien, aux contraintes pragmatiques et linguistiques.
Nous effectuons une étude exhaustive des phénomènes de disfluences dans ce contexte.
Dans un premier temps nous procédons à l'analyse fine de ces phénomènes.
Ensuite, nous les modélisons à un niveau de représentation abstrait, ce qui nous permet d'obtenir les patrons correspondant aux différentes configurations observées.
Enfin nous proposons une méthodologie de traitement automatique.
Celle-ci consiste en plusieurs algorithmes pour identifier les différents phénomènes, même en l'absence de marqueurs explicites.
Elle est intégrée dans un système de traitement automatique de la parole.
Enfin, la méthodologie est validée sur un corpus de 400 énoncés.
Prendre en compte l'aspect sémantique des données textuelles lors de la tâche de classification s'est imposé comme un réel défi ces dix dernières années.
Cette difficulté vient s'ajouter au fait que la plupart des données disponibles sur les réseaux sociaux sont des textes courts, ce qui a notamment pour conséquence de rendre les méthodes basées sur la représentation "bag of words" peu efficientes.
L'approche proposée dans ce projet de recherche est différente des approches proposées dans les travaux antérieurs sur l'enrichissement des messages courts et ce pour trois raisons.
Tout d'abord, nous n'utilisons pas des bases de connaissances externes comme Wikipedia parce que généralement les messages courts qui sont traités par l'entreprise proveniennent des domaines spécifiques.
Deuxièment, les données à traiter ne sont pas utilisées pour la constitution de ressources à cause du fonctionnement de l'outil.
Dans cette thèse, nous proposons la création de ressources permettant d'enrichir les messages courts afin d'améliorer la performance de l'outil du regroupement sémantique de l'entreprise Succeed Together.
Ce dernier implémente des méthodes de classification supervisée et non supervisée.
Pour constituer ces ressources, nous utilisons des techniques de fouille de données séquentielles.
Le but de cette thèse était de conduire des recherches sur la synthèse et transformation expressive de voix chantée, en vue de pouvoir développer un synthétiseur de haute qualité capable de générer automatiquement un chant naturel et expressif à partir d'une partition et d'un texte donnés.
Cette thèse apporte diverses contributions dans chacune de ces 3 directions.
Tout d'abord, un système de synthèse complet a été développé, basé sur la concaténation de diphones.
L'architecture modulaire de ce système permet d'intégrer et de comparer différent modèles de signaux.
Ensuite, la question du contrôle est abordée, comprenant la génération automatique de la f0, de l'intensité, et des durées des phonèmes.
La modélisation de styles de chant spécifiques a également été abordée par l'apprentissage des variations expressives des paramètres de contrôle modélisés à partir d'enregistrements commerciaux de chanteurs célèbres.
Enfin, des investigations sur des transformations expressives du timbre liées à l'intensité et à la raucité vocale ont été menées, en vue d'une intégration future dans notre synthétiseur.
La recherche d'information sémantique (RIS), cherche à proposer des modèles qui permettent de s'appuyer, au delà des calculs statistiques, sur la signification et la sémantique des mots du vocabulaire, afin de mieux caractériser les documents pertinents au regard du besoin de l'utilisateur et de les retrouver.
Le but est ainsi de dépasser les approches classiques purement statistiques (de « sac de mots » ), fondées sur des appariements de chaînes de caractères sur la base des fréquences des mots et de l'analyse de leurs distributions dans le texte.
Pour ce faire, les approches existantes de RIS, à travers l'exploitation de ressources sémantiques externes (thésaurus ou ontologies), procèdent en injectant des connaissances dans les modèles classiques de RI de manière à désambiguïser le vocabulaire ou à enrichir la représentation des documents et des requêtes.
Les ressources sémantiques, ainsi exploitées, sont « aplaties » , les calculs se cantonnent, généralement, à des calculs de similarité sémantique.
Afin de permettre une meilleure exploitation de la sémantique en RI, nous mettons en place un nouveau modèle, qui permet d'unifier de manière cohérente et homogène les informations numériques (distributionnelles) et symboliques (sémantiques) sans sacrifier la puissance des analyses.
Le réseau sémantico-documentaire ainsi modélisé est traduit en graphe pondéré.
Le mécanisme d'appariement est assuré par une propagation d'activation dans le graphe.
Ce nouveau modèle permet à la fois de répondre à des requêtes exprimées sous forme de mots clés, de concepts oumême de documents exemples.
L'algorithme de propagation a le mérite de préserver les caractéristiques largement éprouvéesdes modèles classiques de recherche d'information tout en permettant une meilleure prise en compte des modèles sémantiques et de leurs richesse.
Selon que l'on introduit ou pas de la sémantique dans ce graphe, ce modèle permet de reproduire une RI classique ou d'assurer en sus certaines fonctionnalités sémantiques.
La co-occurrence dans le graphe permet alors de révélerune sémantique implicite qui améliore la précision en résolvant certaines ambiguïtés sémantiques.
L'exploitation explicite des concepts ainsi que des liens du graphe, permettent la résolution des problèmes de synonymie, de term mismatch et de couverture sémantique.
Ces fonctionnalités sémantiques, ainsi que le passage à l'échelle du modèle présenté, sont validés expérimentalement sur un corpus dans le domaine médical.
La thèse a deux objectifs : le premier est de développer un analyseur qui permet d'analyser automatiquement des sources textuelles en chinois simplifié afin de segmenter les textes en mots et de les étiqueter par catégories grammaticales, ainsi que de construire les relations syntaxiques entre les mots.
Le deuxième est d'extraire des informations autour des entités et des actions qui nous intéressent à partir des textes analysés.
Afin d'atteindre ces deux objectifs, nous avons traité principalement les problématiques suivantes : les ambiguïtés de segmentation, la catégorisation ; le traitement des mots inconnus dans les textes chinois ; l'ambiguïté de l'analyse syntaxique ; la reconnaissance et le typage des entités nommées.
Le texte d'entrée est traité phrase par phrase.
L'analyseur commence par un traitement typographique au sein des phrases afin d'identifier les écritures latines et les chiffres.
Ensuite, nous segmentons la phrase en mots à l'aide de dictionnaires.
Un modèle de langue n-gramme élaboré à partir d'un corpus d'apprentissage permet de sélectionner le meilleur résultat de segmentation et de catégorisation.
Une analyse en dépendance est utilisée pour marquer les relations entre les mots.
Nous effectuons une première identification d'entités nommées à la fin de l'analyse syntaxique.
Ceci permet d'identifier les entités nommées en unité ou en groupe nominal et également de leur attribuer un type.
Ces entités nommées sont ensuite utilisées dans l'extraction.
Les règles d'extraction permettent de valider ou de changer les types des entités nommées.
L'extraction des connaissances est composée des deux étapes : extraire et annoter automatiquement des contenus à partir des textes analysés ; vérifier les contenus extraits et résoudre la cohérence à travers une ontologie.
Les travaux présentés dans cette thèse se situent dans le cadre de la synthèse de la parole à partir du texte et, plus précisément, dans le cadre de la synthèse paramétrique utilisant des règles statistiques.
Nous nous intéressons à l'influence des descripteurs linguistiques utilisés pour caractériser un signal de parole sur la modélisation effectuée dans le système de synthèse statistique HTS.
Pour cela, deux méthodologies d'évaluation objective sont présentées.
La première repose sur une modélisation de l'espace acoustique, généré par HTS par des mélanges gaussiens (GMM).
En utilisant ensuite un ensemble de signaux de parole de référence, il est possible de comparer les GMM entre eux et ainsi les espaces acoustiques générés par les différentes configurations de HTS.
La seconde méthodologie proposée repose sur le calcul de distances entre trames acoustiques appariées pour pouvoir évaluer la modélisation effectuée par HTS de manière plus locale.
Cette seconde méthodologie permet de compléter les diverses analyses en contrôlant notamment les ensembles de données générées et évaluées.
Cette thèse présente une méthodologie pour résoudre des problèmes de classification, en particulier ceux concernant le classement séquentiel pour les tâches de traitement du langage naturel.
Elle propose l'utilisation d'une méthode itérative, basée sur l'analyse des erreurs, pour améliorer la performance de classification.
Ce sont des experts du domaine qui suggèrent l'intégration des connaissances spécifiques du domaine dans le processus d'apprentissage automatique.
De plus, cette thèse propose un schéma de classes pour représenter des analyse des phrases dans une structure de donnés unique, y compris les résultats de divers analyses linguistiques.
Cela nous permet de mieux gérer le processus itératif d'amélioration du classificateur, où des ensembles d'attributs différents pour l'apprentissage sont utilisés à chaque itération.
Nous proposons également de stocker des attributs dans un modèle relationnel, plutôt que des structures traditionnelles à base de texte, pour faciliter l'analyse et la manipulation de données nécessaires pour l'apprentissage.
La nécessité d'estimer la taille d'un logiciel pour pouvoir en estimer le coût et l'effort nécessaire à son développement est une conséquence de l'utilisation croissante des logiciels dans presque toutes les activités humaines.
De plus, la nature compétitive de l'industrie du développement logiciel rend courante l'utilisation d'estimations précises de leur taille, au plus tôt dans le processus de développement.
Traditionnellement, l'estimation de la taille des logiciels était accomplie a posteriori à partir de diverses mesures appliquées au code source.
Cependant, avec la prise de conscience, par la communauté de l'ingénierie logicielle, que l'estimation de la taille du code est une donnée cruciale pour la maîtrise du développement et des coûts, l'estimation anticipée de la taille des logiciels est devenue une préoccupation répandue.
Une fois le code écrit, l'estimation de sa taille et de son coût permettent d'effectuer des études contrastives et éventuellement de contrôler la productivité.
D'autre part, les bénéfices apportés par l'estimation de la taille sont d'autant plus grands que cette estimation est effectuée tôt pendant le développement.
Notre recherche se positionne autour des mesures d'estimation de la taille fonctionnelle, couramment appelées Analyse des Points de Fonctions, qui permettent d'estimer la taille d'un logiciel à partir des fonctionnalités qu'il doit fournir à l'utilisateur final, exprimées uniquement selon son point de vue, en excluant en particulier toute considération propre au développement.
Un problème significatif de l'utilisation des points de fonction est le besoin d'avoir recours à des experts humains pour effectuer la quotation selon un ensemble de règles de comptage.
Le processus d'estimation représente donc une charge de travail conséquente et un coût important.
D'autre part, le fait que les règles de comptage des points de fonction impliquent nécessairement une part d'interprétation humaine introduit un facteur d'imprécision dans les estimations et rend plus difficile la reproductibilité des mesures.
Nous proposons de fournir aux experts humains une aide automatique dans le processus d'estimation, en identifiant dans le texte des spécifications, les endroits les plus à même de contenir des points de fonction.
Enfin, l'identification non ambiguë des points de fonction permettra de faciliter et d'améliorer la reproductibilité des mesures.
À notre connaissance, les travaux présentés dans cette thèse sont les premiers à se baser uniquement sur l'analyse du contenu textuel des spécifications, applicable dès la mise à disposition des spécifications préliminaires et en se basant sur une approche générique reposant sur des pratiques établies d'analyse automatique du langage naturel.
Les applications futures de la robotique, en particulier pour des robots de service à la personne, exigeront des capacités d'adaptation continue à l'environnement, et notamment la capacité à reconnaître des nouveaux objets et apprendre des nouveaux mots via l'interaction avec les humains.
Bien qu'ayant fait d'énormes progrès en utilisant l'apprentissage automatique, les méthodes actuelles de vision par ordinateur pour la détection et la représentation des objets reposent fortement sur de très bonnes bases de données d'entrainement et des supervisions d'apprentissage idéales.
En revanche, les enfants de deux ans ont une capacité impressionnante à apprendre à reconnaître des nouveaux objets et en même temps d'apprendre les noms des objets lors de l'interaction avec les adultes et sans supervision précise.
Par conséquent, suivant l'approche de le robotique développementale, nous développons dans la thèse des approches d'apprentissage pour les objets, en associant leurs noms et leurs caractéristiques correspondantes, inspirées par les capacités des enfants, en particulier l'interaction ambiguë avec l'homme en s'inspirant de l'interaction qui a lieu entre les enfants et les parents.
L'idée générale est d'utiliser l'apprentissage cross-situationnel (cherchant les points communs entre différentes présentations d'un objet ou d'une caractéristique) et la découverte de concepts multi-modaux basée sur deux approches de découverte de thèmes latents : la Factorisation en Natrices Non-Négatives (NMF) et l'Allocation de Dirichlet latente (LDA).
Cette thèse souligne les solutions algorithmiques requises pour pouvoir effectuer un apprentissage efficace de ces associations de mot-référent à partir de données acquises dans une configuration d'acquisition simplifiée mais réaliste qui a permis d'effectuer des simulations étendues et des expériences préliminaires dans des vraies interactions homme-robot.
Nous avons également apporté des solutions pour l'estimation automatique du nombre de thèmes pour les NMF et LDA.Nous avons finalement proposé deux stratégies d'apprentissage actives : la Sélection par l'Erreur de Reconstruction Maximale (MRES) et l'Exploration Basée sur la Confiance (CBE), afin d'améliorer la qualité et la vitesse de l'apprentissage incrémental en laissant les algorithmes choisir les échantillons d'apprentissage suivants.
Nous avons comparé les comportements produits par ces algorithmes et montré leurs points communs et leurs différences avec ceux des humains dans des situations d'apprentissage similaires.
D'une part, nous considérons le problème de l'apprentissage de mesure de similarité pour deux tâches : (i) la détection de rupture dans des signaux multivariés et (ii) le problème de déformation temporelle entre paires de signaux.
Les méthodes généralement utilisées pour résoudre ces deux problèmes dépendent fortement d'une mesure de similarité.
Nous présentons des algorithmes usuels de prédiction structuré, efficaces pour effectuer l'apprentissage.
Nous considérons la partition comme une représentation symbolique donnant (i) une information complète sur l'ordre des symboles et (ii) une information approximative sur la forme de l'alignement attendu.
Nous apprenons un classifieur pour chaque symbole avec ces informations.
Nous développons une méthode d'apprentissage fondée sur l'optimisation d'une fonction convexe.
Nous démontrons la validité de l'approche sur des données musicales.
L'augmentation rapide de la population combinée à la mobilité croissante des individus a engendré le besoin de systèmes de gestion d'identités sophistiqués.
À cet effet, le terme biométrie se réfère généralement aux méthodes permettant d'identifier les individus en utilisant des caractéristiques biologiques ou comportementales.
Les méthodes les plus populaires, c'est-à-dire la reconnaissance d'empreintes digitales, d'iris ou de visages, se basent toutes sur des méthodes de vision par ordinateur.
L'adoption de réseaux convolutifs profonds, rendue possible par le calcul générique sur processeur graphique, ont porté les récentes avancées en vision par ordinateur.
Ces avancées ont permis une amélioration drastique des performances des méthodes conventionnelles en biométrie, ce qui a accéléré leur adoption pour des usages concrets, et a provoqué un débat public sur l'utilisation de ces techniques.
Dans ce contexte, les concepteurs de systèmes biométriques sont confrontés à un grand nombre de challenges dans l'apprentissage de ces réseaux.
Dans cette thèse, nous considérons ces challenges du point de vue de l'apprentissage statistique théorique, ce qui nous amène à proposer ou esquisser des solutions concrètes.
Premièrement, nous répondons à une prolifération de travaux sur l'apprentissage de similarité pour les réseaux profonds, qui optimisent des fonctions objectif détachées du but naturel d'ordonnancement recherché en biométrie.
Précisément, nous introduisons la notion d'ordonnancement par similarité, en mettant en évidence la relation entre l'ordonnancement bipartite et la recherche d'une similarité adaptée à l'identification biométrique.
Nous étendons ensuite la théorie sur l'ordonnancement bipartite à ce nouveau problème, tout en l'adaptant aux spécificités de l'apprentissage sur paires, notamment concernant son coût computationnel.
La thèse aborde ces trois exemples, en propose une étude statistique minutieuse, ainsi que des méthodes pratiques qui donnent les outils nécessaires aux concepteurs de systèmes biométriques pour adresser ces problématiques, sans compromettre la performance de leurs algorithmes.
Notre travail vise a confronter certains paradigmes informatiques aux problemes que pose le traitement automatique de la construction du sens.
Il s'agit d'une part de determiner ces comportements et d'autre part de les representer.
Il en va de meme pour les comportements lexicaux : les savoirs associes aux mots ne sont pas donnes une fois pour toutes.
On utilise donc le prototype (dans le cadre de la programmation a prototypes) comme un outil de representation de certains faits linguistiques dans la mesure ou nous pensons qu'il peut repoemes que posent ces faits de langue.
Cet outil de representation conduit a construire des structures de representation simples et ajustables pour rendre compte justement des problemes d'ajustements qui sont a l'oeuvre dans la construction du sens dans le langage naturel.
La pap encourage une approche de representation faite de petits sauts successifs qui ameliore la qualite de la representation produite.
Cette demarche s'accorde aussi avec la necessaire approche artisanale que constitue le travail du linguiste dans sa volonte de decrire les comportements des faits de langue.
Cette étude compare le fonctionnement de l'énoncé averbal (EAV) en kabyle et en allemand, en prenant comme cadre théorique la triade sémantico-logique établie par Zemb (1978), i. e. le thème (ce dont on parle), le rhème (ce qu'on en dit) et le phème (lieu d'articulation de la modalisation et de la négation) appliquée par Behr et Quintin (1996) et Behr (2013) à la catégorisation des EAV de l'allemand.
Nous postulons que chaque langue dispose de moyens morphosyntaxiques, contextuelles et situationnelles contribuant à la réalisation d'EAV et que ces moyens sont plus étendus en kabyle.
Nous supposons qu'il existe des structures sémantico-logiques uniques qui pourraient s'exprimer à travers des structures morphosyntaxiques variées.
Nous supposons enfin que les EAV réalisent toutes les modalités, disposent de moyens morphologiques et/ ou contextuels permettant de les localiser dans le cadre temporel.
Parmi les résultats, nous avons constaté que les EAV sont plus fréquents en kabyle grâce aux structures prédicatives grammaticalisées, sauf l'EAV représentant une continuité syntaxique avec le segment de gauche dont la fréquence en allemand est due au scrambling.
Au niveau syntaxique, la pré-/postposition du thème par rapport au rhème obéit à des contraintes liées à la langue, i. e. l'état du nom en kabyle et la définitude du GN en allemand ; des contraintes propres à l'EAV se manifestent dans la prédilection pour l'ordre rhème-thème en allemand.
Les EAV expriment toutes les modalités, ils sont situés dans le temps par les circonstants, certains démonstratifs ou le contexte, et les nominalisations en tant que rhème existentiel expriment l'aspectualité télique et atélique.
Les réunions de concertations pluridisciplinaires en oncologie permettent aux experts des différentes spécialités de choisir les meilleures options thérapeutiques pour les patients.
Les données nécessaires à ces réunions sont souvent collectées manuellement, avec un risque d'erreur lors de l'extraction et un coût important pour les professionnels de santé.
Le projet ASIMOV vise à utiliser le traitement des langues naturelles et l'intégration de données pour identifier et extraire dans les entrepôts de données et les textes cliniques les informations cruciales à la prise de décision, en particulier sur le plan temporel.
Le projet se penche sur la question de l'apprentissage automatique dans un contexte pauvre en annotations.
Nous nous appuierons sur des méthodes mixtes et développerons des outils pour généraliser nos approches.
L'énergie est fondamentale pour maintenir le confort et façonne notre vie moderne.
Avec la demande excédentaire en énergie, les systèmes de gestion de l'énergie résidentielle apparaissent avec le temps.
Ils visent à réduire ou moduler la consommation d'énergie tout en maintenant un niveau de confort acceptable.
Des systèmes efficaces de gestion de l'énergie domestique devraient intégrer une représentation comportementale d'un système domestique, y compris les habitants.
Il établit des relations entre différentes variables environnementales et des phénomènes hétérogènes présents dans une maison.
Par conséquent, ces systèmes sont complexes à construire et à comprendre pour les habitants.
Cela était justifié car il était presque impossible d'impliquer les occupants et de créer une relation entre les occupants et les systèmes énergétiques.
Ce concept crée différents problèmes car les occupants sont détachés du système énergétique et ne comprennent pas ses fonctionnalités ni son fonctionnement.
Pour surmonter cette difficulté, ce travail met en avant le concept de "faire avec" en essayant d'impliquer l'occupant dans la boucle avec son système de gestion de l'énergie.
C'est là que l'explication est nécessaire pour permettre aux occupants de découvrir les connaissances du système énergétique et de développer leur capacité à comprendre comment le système fonctionne et pourquoi il recommande différentes actions.
L'explication est le moyen de découvrir de nouvelles connaissances et, par conséquent, d'impliquer les occupants.
Pour les humains, l'explication joue un rôle important dans la vie.
C'est l'un des principaux outils d'apprentissage et de compréhension.
Il est même utilisé dans la communication et les aspects sociaux.
Les gens ont tendance à l'utiliser en plus d'apprendre à montrer leurs connaissances sur un sujet pour gagner la confiance des autres ou pour clarifier une situation.
Mais générer des explications n'est pas une tâche facile.
C'est l'un des problèmes scientifiques récurrents de plusieurs décennies.
Les explications ont de nombreuses formes, types et niveaux de clarté.
Cette étude se concentre sur les explications causales.
Comme il s'agit de la forme d'explication la plus intuitive à comprendre par les occupants, elle est conçue pour transférer les connaissances issues de systèmes complexes tels que les modèles énergétiques.
Le défi scientifique est de savoir comment construire des explications de causalité pour les habitants à partir d'un flux de données de capteurs observées.
Des études récentes montrent une part croissante de requêtes sur les moteurs de recherche du Web comportant des critères géographiques.
Cette part est encore plus conséquente sur des corpus plus spécifiques tels que des documents patrimoniaux (récits de voyages par exemple).
On admet que l'information géographique est composée de trois facettes : le spatial, le temporel et le thématique.
L'objet de ce travail de thèse est de combiner les trois facettes pour effectuer des recherches multicritère.
Ce travail s'intègre au croisement de plusieurs disciplines : Traitement Automatique des Langages Naturels (TALN), Systèmes d'Information Géographique (SIG), Recherche d'Information classique (RI) et Recherche d'Information Géographique (RIG).
Notre première contribution porte sur une méthode originale de combinaison des index spécifiques.
Pour pouvoir effectuer cette combinaison, nous proposons d'imiter les approches d'homogénéisation utilisées dans les stratégies de RI classiques portant sur des termes et les lemmes correspondants.
Notre deuxième contribution porte sur une approche d'uniformisation générique mise en oeuvre sur l'information spatiale et l'information temporelle.
La dernière contribution consiste en un cadre d'évaluation d'un système de recherche géographique.
Grâce à ce cadre nous avons pu vérifier et quantifier l'apport de la combinaison de critères géographiques ainsi que comparer différentes approches de combinaisons.
La veille anticipative stratégique et intelligence collective (VASIC) proposée par Lesca est une méthode aidant les entreprises à se mettre à l'écoute de leur environnement pour anticiper des opportunités ou des risques.
Cette méthode nécessite la collecte d'informations. Or, avec le développement des technologies de l'information, les salariés font face à une surabondance d'informations.
Afin d'aider à pérenniser le dispositif de veille stratégique, il est nécessaire de mettre en place des outils pour gérer la surinformation.
Dans cette thèse, nous proposons une mesure de voisinage pour estimer si deux informations sont proches ; nous avons créé un prototype, nommé Alhena, basé sur cette mesure.
Nous démontrons les propriétés de notre mesure ainsi que sa pertinence dans le cadre de la veille stratégique.
Nous montrons également que le prototype peut servir dans d'autres domaines tels que la littérature, l'informatique et la psychologie.
Ce travail est pluridisciplinaire : il aborde des aspects de veille stratégique (en sciences de gestion), de la recherche d'informations, d'informatique linguistique et de mathématiques.
Nous nous sommes attachés à partir d'un problème concret en sciences de gestion à proposer un outil qui opérationnalise des techniques informatiques et mathématiques en vue d'une aide à la décision (gain de temps, aide à la lecture,...).
Cette thèse se situe à l'intersection de trois domaines de recherche : le raisonnement à partir de cas, l'extraction de connaissances et la représentation des connaissances.
Raisonner à partir de cas consiste à résoudre un nouveau problème en utilisant un ensemble de problèmes déjà résolus, appelés cas.
Dans cette thèse, un langage de représentation des variations entre cas est introduit.
Nous montrons comment ce langage peut être utilisé pour représenter les connaissances d'adaptation et pour modéliser la phase d'adaptation en raisonnement à partir de cas.
Un processus d'extraction de connaissances, appelé CabamakA, est mis au point.
Ce processus permet d'apprendre des connaissances d'adaptation par généralisation à partir d'une représentation des variations entre cas.
Une discussion est ensuite menée sur les conditions d'opérationnalisation de CabamakA au sein d'un processus d'acquisition de connaissances.
L'étude aboutit à la proposition d'un nouveau type d'approche pour l'acquisition de connaissances d'adaptation dans lequel le processus d'extraction de connaissances est déclenché de manière opportuniste au cours d'une session particulière de résolution de problèmes.
Les différents concepts introduits dans la thèse sont illustrés dans le domaine culinaire à travers leur application au système de raisonnement à partir de cas Taaable, qui constitue le contexte applicatif de l'étude.
Cette thèse s'inscrit dans une série d'études récemment entamées qui cherchent à caractériser les lectes d'apprenants avancés de l'anglais.
Nous présentons une analyse de quelques facteurs sémantiques, discursifs et inter-linguistiques qui sous-tendent l'emploi des formes verbales en anglais langue étrangère par des apprenants avancés francophones et catalanophones en milieu guidé.
À partir d'un corpus de narrations orales élicitées à partir d'un livre en images, nous examinons la distribution de la morphologie temporo-aspectuelle par rapport à l'aspect sémantique des prédicats (l'hypothèse de l'aspect) et le type d'information temporelle que ces prédicats encodent dans la narration (l'hypothèse du discours).
L'emploi de la morphologie verbale est considéré également du point de vue du style rhétorique de l'apprenant, c'est-à-dire des choix systématiques faits dans une tâche communicative spécifique à partir d'un répertoire appris de formes cibles, mais aussi à travers le filtre inconscient du mode de sélection et d'organisation de l'information en langue maternelle.
Même si l'anglais, le français et le catalan grammaticalisent des distinctions aspectuelles, ceci ne permet pas aux apprenants étudiés de faire un emploi de la morphologie verbale tout à fait semblable à celui des locuteurs natifs.
Des coalitions prototypiques entre la sémantique des prédicats et celle de la forme verbale, qui caractérisent l'emploi de la morphologie verbale aux stades moins avancés, persistent dans l'emploi des prédicats duratifs (a)téliques et débouchent sur un emploi généralisé du progressif en anglais, souvent dans des contextes où la présence de ce marqueur génère une tension avec le type d'information temporelle encodée.
Les moyens d'encoder le déroulement dans la langue maternelle des apprenants semble brouiller leurs hypothèses relatives à l'emploi du progressif en discours dans la langue cible.
Seul un sous-ensemble d'apprenants très avancés utilise la morphologie verbale d'une façon véritablement libérée du sémantisme de la construction verbale, de façon similaire aux locuteurs natifs.
Pour ces apprenants, le progressif acquiert une fonction discursive et sa présence n'est plus systématique dans les contextes où l'information sur le caractère non-borné d'une situation peut être récupérée à partir d'autres éléments, sémantiques ou syntaxiques.
Il existe en effet des légères différences entre les productions des apprenants et des locuteurs natifs en ce qui concerne la palette de fonctions discursives que les formes verbales présentent dans la narration.
Notre étude ouvre des pistes de recherche sur l'étanchéité des oppositions grammaticales dans le domaine de la morphologie verbale, sur les coalitions atypiques qui peuvent surgir en discours et la façon dont ces usages périphériques peuvent s'apprendre (et s'enseigner) dans un milieu guidé.
Il en résulte aussi que la production orale chez des apprenants avancés se construit à travers le filtre d'une façon de penser le monde qui reste, de façon irréductible, celui de la langue maternelle.
Les environnements côtiers en Normandie sont propices à de multiples aléas (érosion, submersion marine, inondation par débordement de cours d'eau ou remontée de nappe, crue turbide par ruissellement, mouvement de versant côtier ou continental).
Des interactions entre aléas vont se produire au sein des versants et des vallées où les populations côtières et leurs activités tendent à se densifier depuis le XIXe siècle.
Dans le cadre de cette thèse, ainsi que dans le programme ANR RICOCHET, trois sites d'étude ont été sélectionnés à l'embouchure des rivières littorales : de Auberville à Pennedepie, de Quiberville à Dieppe et de Criel-sur-Mer à Ault en raison d'importants enjeux socio-économiques et de fortes interactions entre les phénomènes hydrologiques et gravitaires.
Les méthodologies classiques utilisées pour la détection de piétons
Nous considérons un système à plusieurs classifieurs (Multiple Classifier System, MCS), composé de deux ensembles différents, le premier basé sur les classifieurs SVM (SVM-ensemble) et le deuxième basé sur les CNN (CNN-ensemble), combinés dans le cadre de la Théorie des Fonctions de Croyance (TFC).
La TFC nous permet de prendre en compte une valeur d'imprécision supposée correspondre soit à une imprécision dans la procédure de calibration, soit à une imprécision spatiale.
Cet algorithme s'appuie sur des mesures évidentielles déduites des fonctions de croyance.
Pour le second ensemble, pour exploiter les avancées de l'apprentissage profond, nous avons reformulé notre problème comme une tâche de segmentation en soft labels.
Une architecture entièrement convolutionelle a été conçue pour détecter les petits objets grâce à des convolutions dilatées.
Pour conclure, nous montrons que la sortie du MCS peut être utile aussi pour le comptage de personnes.
Cette thèse traite de la reconnaissance d'entités dans les documents océrisés guidée par une base de données.
Une entité peut être, par exemple, une entreprise décrite par son nom, son adresse, son numéro de téléphone, son numéro TVA, etc. ou des méta-données d'un article scientifique tels que son titre, ses auteurs et leurs affiliations, le nom de son journal, etc.
Disposant d'un ensemble d'entités structurées sous forme d'enregistrements dans une base de données et d'un document contenant une ou plusieurs de ces entités, nous cherchons à identifier les entités contenues dans le document en utilisant la base de données.
Ce travail est motivé par une application industrielle qui vise l'automatisation du traitement des images de documents administratifs arrivant en flux continu.
Nous avons abordé ce problème comme un problème de rapprochement entre le contenu du document et celui de la base de données.
Les difficultés de cette tâche sont dues à la variabilité de la représentation d'attributs d'entités dans la base et le document et à la présence d'attributs similaires dans des entités différentes.
À cela s'ajoutent les redondances d'enregistrements et les erreurs de saisie dans la base de données et l'altération de la structure et du contenu du document, causée par l'OCR.
Devant ces problèmes, nous avons opté pour une démarche en deux étapes : la résolution d'entités et la reconnaissance d'entités.
La première étape consiste à coupler les enregistrements se référant à une même entité et à les synthétiser dans un modèle entité.
Pour ce faire, nous avons proposé une approche supervisée basée sur la combinaison de plusieurs mesures de similarité entre attributs.
Ces mesures permettent de tolérer quelques erreurs sur les caractères et de tenir compte des permutations entre termes.
La deuxième étape vise à rapprocher les entités mentionnées dans un document avec le modèle entité obtenu.
Nous avons procédé par deux manières différentes, l'une utilise le rapprochement par le contenu et l'autre intègre le rapprochement par la structure.
Pour le rapprochement par le contenu, nous avons proposé deux méthodes : M-EROCS et ERBL.
M-EROCS, une amélioration/adaptation d'une méthode de l'état de l'art, consiste à faire correspondre les blocs de l'OCR avec le modèle entité en se basant sur un score qui tolère les erreurs d'OCR et les variabilités d'attributs.
ERBL consiste à étiqueter le document par les attributs d'entités et à regrouper ces labels en entités.
Pour le rapprochement par les structures, il s'agit d'exploiter les relations structurelles entre les labels d'une entité pour corriger les erreurs d'étiquetage.
La méthode proposée, nommée G-ELSE, consiste à utiliser le rapprochement inexact de graphes attribués modélisant des structures locales, avec un modèle structurel appris pour cet objectif.
Cette thèse étant effectuée en collaboration avec la société ITESOFT-Yooz, nous avons expérimenté toutes les étapes proposées sur deux corpus administratifs et un troisième corpus extrait du Web
Cette thèse étudie les rapports entre l'expression d'attitudes agressives contrôlées et la perception de la dominance, à partir d'extraits de séances télévisées du conseil Municipal de Montreuil (93100) durant l'année 2013, période marquée par un climat politique vif et hostile.
Un corpus a été constitué à partir d'extraits de parole spontanée de la Maire, Dominique Voynet, et de quatre de ses opposants.
Les cinq locuteurs ont participé à l'enregistrement d'une relecture neutre du corpus de leurs propres extraits de parole (25 stimuli par locuteur) ainsi qu'à l'auto-évaluation perceptive de leurs stimuli (profils émotionnels), dont les résultats ont été comparés à l'évaluation perceptive des extraits par des auditeurs naïfs.
Les extraits originaux et relus ont été comparés au niveau de leur structuration prosodico-syntaxique et de leurs caractéristiques temporelles et mélodiques.
Les résultats montrent que 1) certains locuteurs semblent plus s'appuyer sur des paramètres mélodiques et d'autres sur des paramètres temporels ; 2) on peut néanmoins dégager les tendances générales concernant les corrélats dans la parole de l'hostilité et de la dominance dans notre corpus : a) des écarts entre structuration syntaxique et prosodique des extraits, b) la réduction ou l'absence d'allongements syllabiques finaux pré-pausaux, c) de fortes variations de plage de variation de F0 de part et d'autre des pauses silencieuses.
Aujourd'hui de plus en plus de données de différents types sont accessibles.
L'Analyse Formelle de Concepts (AFC) et les pattern structures sont des systèmes formels qui permettent de traiter les données ayant une structure complexe.
Mais le nombre de concepts trouvé par l'AFC est fréquemment très grand.
Pour faire face à ce problème, on peut simplifier la représentation des données, soit par projection de pattern structures, soit par introduction de contraintes pour sélectionner les concepts les plus pertinents.
Le manuscrit commence avec l'application de l'AFC à l'exploration de structures moléculaires et la recherche de structures particulières.
Avec l'augmentation de la taille des ensembles de données, de bonnes contraintes deviennent essentielles.
Pour cela on explore la stabilité d'un concept et on l'applique à l'exploration d'un ensemble de données de substances chimiques mutagènes.
La recherche de concepts stables dans cet ensemble de données nous a permis de trouver de nouveaux candidats mutagènes potentiels qui peuvent être interprétés par les chimistes.
Cependant, pour les cas plus complexes, la représentation simple par des attributs binaires ne suffit pas.
En conséquence, on se tourne vers des pattern structures qui peuvent traiter différents types de données complexes.
On étend le formalisme original des projections pour avoir plus de liberté dans la manipulation de données.
On montre que cette extension est essentielle pour analyser les trajectoires de patients décrivant l'historique de l'hospitalisation des patients.
Finalement, le manuscrit se termine par une approche originale et très efficace qui permet de trouver directement des motifs stables.
Dans le Web des données, des graphes de connaissances de plus en plus nombreux sont simultanément publiés, édités, et utilisés par des agents humains et logiciels.
Cette large adoption rend essentielles les tâches d'appariement et de fouille.
L'appariement identifie des unités de connaissances équivalentes, plus spécifiques ou similaires au sein et entre graphes de connaissances.
Cette tâche est cruciale car la publication et l'édition parallèles peuvent mener à des graphes de connaissances co-existants et complémentaires.
Cependant, l'hétérogénéité inhérente aux graphes de connaissances (e.g., granularité, vocabulaires, ou complétude) rend cette tâche difficile.
Motivés par une application en pharmacogénomique, nous proposons deux approches pour apparier des relations n-aires représentées au sein de graphes de connaissances : une méthode symbolique à base de règles et une méthode numérique basée sur le plongement de graphe.
Nous les expérimentons sur PGxLOD, un graphe de connaissances que nous avons construit de manière semi-automatique en intégrant des relations pharmacogénomiques de trois sources du domaine.
La tâche de fouille permet quant à elle de découvrir de nouvelles unités de connaissances à partir des graphes de connaissances.
Leur taille croissante et leur nature combinatoire entraînent des problèmes de passage à l'échelle que nous étudions dans le cadre de la fouille de patrons de chemins.
Nous proposons également l'annotation de concepts, une méthode d'amélioration des graphes de connaissances qui étend l'Analyse Formelle de Concepts, un cadre mathématique groupant des entités en fonction de leurs attributs communs.
Au cours de tous nos travaux, nous nous sommes particulièrement intéressés à tirer parti des connaissances de domaines formalisées au sein d'ontologies qui peuvent être associées aux graphes de connaissances.
Nous montrons notamment que, lorsqu'elles sont prises en compte, ces connaissances permettent de réduire l'impact des problèmes d'hétérogénéité et de passage à l'échelle dans les tâches d'appariement et de fouille.
Ce travail de thèse vise à combler le fossé entre les domaines de la sémantique du Web et de la physique des particules expérimentales.
En prenant comme cas d'utilisation un type spécifique d'expérience de physique, les expériences d'irradiation utilisées pour tester la résistance des composants au rayonnement, un modèle de domaine, ce qui, dans le domaine de la sémantique du Web, est appelé ontologie, a été créé pour décrire les principaux concepts de la gestion des données des expériences d'irradiation.
Cette thèse débute par l'étude d'architectures profondes à noyaux pour les données complexes.
L'une des clefs du succès des algorithmes d'apprentissage profond est la capacité des réseaux de neurones à extraire des représentations pertinentes.
Cependant, les raisons théoriques de ce succès nous sont encore largement inconnues, et ces approches sont presque exclusivement réservées aux données vectorielles.
L'architecture proposée consiste à remplacer les blocs élémentaires des réseaux usuels par des fonctions appartenant à des vv-RKHSs.
Bien que très différents à première vue, les espaces fonctionnels ainsi définis sont en réalité très similaires, ne différant que par l'ordre dans lequel les fonctions linéaires/non-linéaires sont appliquées.
En plus du contrôle théorique sur les couches, considérer des fonctions à noyau permet de traiter des données structurées, en entrée comme en sortie, étendant le champ d'application des réseaux aux données complexes.
Nous conclurons cette partie en montrant que ces architectures admettent la plupart du temps une paramétrisation finie-dimensionnelle, ouvrant la voie à des méthodes d'optimisation efficaces pour une large gamme de fonctions de perte.
La seconde partie de cette thèse étudie des alternatives à la moyenne empirique comme substitut de l'espérance dans le cadre de la Minimisation du Risque Empirique (Empirical Risk Minimization, ERM).
En effet, l'ERM suppose de manière implicite que la moyenne empirique est un bon estimateur.
Cependant, dans de nombreux cas pratiques (e.g. données à queue lourde, présence d'anomalies, biais de sélection), ce n'est pas le cas.
La Médiane-des-Moyennes (Median-of-Means, MoM) est un estimateur robuste de l'espérance construit comme suit : des moyennes empiriques sont calculées sur des sous-échantillons disjoints de l'échantillon initial, puis est choisie la médiane de ces moyennes.
Nous proposons et analysons deux extensions de MoM, via des sous-échantillons aléatoires et/ou pour les U-statistiques.
Il est ainsi prouvé que la minimisation d'un estimateur MoM (aléatoire) est robuste aux anomalies, tandis que les méthodes de tournoi MoM sont étendues au cas de l'apprentissage sur les paires.
Enfin, nous proposons une méthode d'apprentissage permettant de résister au biais de sélection.
De nos jours, les réseaux sociaux ont considérablement changé la façon dont les personnes prennent des photos qu'importe le lieu, le moment, le contexte.
Plus que 500 millions de photos sont partagées chaque jour sur les réseaux sociaux, auxquelles on peut ajouter les 200 millions de vidéos échangées en ligne chaque minute.
Plus particulièrement, avec la démocratisation des smartphones, les utilisateurs de réseaux sociaux partagent instantanément les photos qu'ils prennent lors des divers événements de leur vie, leurs voyages, leurs aventures, etc.
Partager ce type de données présente un danger pour la vie privée des utilisateurs et les expose ensuite à une surveillance grandissante.
Ajouté à cela, aujourd'hui de nouvelles techniques permettent de combiner les données provenant de plusieurs sources entre elles de façon jamais possible auparavant.
Cependant, la plupart des utilisateurs des réseaux sociaux ne se rendent même pas compte de la quantité incroyable de données très personnelles que les photos peuvent renfermer sur eux et sur leurs activités (par exemple, le cas du cyberharcèlement).
Premièrement, nous fournissons un framework capable de mesurer le risque éventuel de ré-identification des personnes et d'assainir les documents multimédias destinés à être publiés et partagés.
Deuxièmement, nous proposons une nouvelle approche pour enrichir le profil de l'utilisateur dont on souhaite préserver l'anonymat.
Pour cela, nous exploitons les évènements personnels à partir des publications des utilisateurs et celles partagées par leurs contacts sur leur réseau social.
Nous décrivons les expérimentations que nous avons menées sur des jeux de données réelles et synthétiques.
Les résultats montrent l'efficacité de nos différentes contributions.
L'utilisation massive de l'Internet et les ordinateurs ont changé plusieurs aspects de notre vie quotidienne et la façon que nous postulons pour un travail n'y fait pas exception.
Depuis les 15 dernières années, les chercheurs du Traitement de la Langue Naturelle ont étudié comment améliorer les performances des recruteurs avec l'aide du recrutement électronique.
Beaucoup de systèmes ont été développés dans ce domaine, depuis les moteurs de recherche de candidats ou de postes jusqu'au classement automatique de candidats.
Dans ce dernier cas, les systèmes développés font, pour la plupart, la comparaison entre les CV des candidats et les offres d'emploi.
Seul un système utilise les CV de processus de sélection relevant du passé pour classer les candidats à un nouveau poste.
Dans le cadre de cette thèse, nous avons étudié la possibilité et la façon d'utiliser les CV, sans avoir à exploiter aucun processus de sélection précédent, pour développer nouvelles méthodes applicables aux systèmes de recrutement électronique.
Plus spécifiquement, nous commençons par le traitement automatique d'un grand ensemble de CV utilisés pendant des processus réels de recrutement et sélection.
Ensuite, nous analysons et appliquons différentes mesures de proximité pour savoir lesquelles sont les plus appropriées pour étudier les CV des candidats.
Après, nous introduisons une méthode innovante qui repose sur le Relevance Feedback et l'utilisation de mesures de proximité seulement sur les CV pour pouvoir classer les candidats d'un poste.
Dans cette thèse, nous montrons que les CV contiennent assez d'information sur le processus de sélection pour pouvoir classer les candidats.
Néanmoins, il est important de choisir correctement les mesures de proximité à utiliser.
D'ailleurs, nous présentons des résultats intéressants de la triple comparaison entre les CV et les offres d'emploi.
Les résultats obtenus dans cette thèse forment une base pour la conception de nouveaux prototypes de systèmes de recrutement électronique et possiblement le début d'une nouvelle façon pour les développer.
Les premiers documents attestant l'utilisation d'une chaise à roues utilisée pour transporter une personne avec un handicap datent du 6ème siècle en Chine.
À l'exception des fauteuils roulants pliables X-frame inventés en 1933, 1400 ans d'évolution de la science humaine n'ont pas changé radicalement la conception initiale des fauteuils roulants.
Pendant ce temps, les progrès de l'informatique et le développement de l'intelligence artificielle depuis le milieu des années 1980 ont conduit inévitablement à la conduite de recherches sur des fauteuils roulants intelligents.
Plutôt que de se concentrer sur l'amélioration de la conception sous-jacente, l'objectif principal de faire un fauteuil roulant intelligent est de le rendre le plus accessible.
Même si l'invention des fauteuils roulants motorisés ont partiellement atténué la dépendance d'un utilisateur à d'autres personnes pour la réalisation de leurs actes quotidiens, certains handicaps qui affectent les mouvements des membres, le moteur ou la coordination visuelle, rendent impossible l'utilisation d'un fauteuil roulant électrique classique.
L'accessibilité peut donc être interprétée comme l'idée d'un fauteuil roulant adaptée à la pathologie de l'utilisateur de telle sorte que il / elle soit capable d'utiliser les outils d'assistance.
S'il est certain que les robots intelligents sont prêts à répondre à un nombre croissant de problèmes dans les industries de services et de santé, il est important de comprendre la façon dont les humains et les utilisateurs interagissent avec des robots afin d'atteindre des objectifs communs.
En particulier dans le domaine des fauteuils roulants intelligents d'assistance, la préservation du sentiment d'autonomie de l'utilisateur est nécessaire, dans la mesure où la liberté individuelle est essentielle pour le bien-être physique et social.
De façon globale, ce travail vise donc à caractériser l'idée d'une assistance par contrôle partagé, et se concentre tout particulièrement sur deux problématiques relatives au domaine de la robotique d'assistance appliquée au fauteuil roulant intelligent, à savoir une assistance basée sur la vision et la navigation en présence d'humains.
En ciblant les tâches fondamentales qu'un utilisateur de fauteuil roulant peut avoir à exécuter lors d'une navigation en intérieur, une solution d'assistance à bas coût, basée vision, est conçue pour la navigation dans un couloir.
Le système fournit une assistance progressive pour les tâches de suivi de couloir et de passage de porte en toute sécurité.
L'évaluation du système est réalisée à partir d'un fauteuil roulant électrique de série et robotisé.
A partir de la solution plug and play imaginée, une formulation adaptative pour le contrôle partagé entre l'utilisateur et le robot est déduite.
De plus, dans la mesure où les fauteuils roulants sont des dispositifs fonctionnels qui opèrent en présence d'humains, il est important de considérer la question des environnements peuplés d'humains pour répondre de façon complète à la problématique de la mobilité en fauteuil roulant.
En s'appuyant sur les concepts issus de l'anthropologie, et notamment sur les conventions sociales spatiales, une modélisation de la navigation en fauteuil roulant en présence d'humains est donc proposée.
De plus, une stratégie de navigation, qui peut être intégrée sur un robot social (comme un fauteuil roulant intelligent), permet d'aborder un groupe d'humains en interaction de façon équitable et de se joindre à eux de façon socialement acceptable.
Enfin, à partir des enseignements tirés des solutions proposées d'aide à la mobilité en fauteuil roulant, nous pouvons formaliser mathématiquement un contrôle adaptatif partagé pour la planification de mouvement relatif à l'assistance à la navigation.
La validation de ce formalisme permet de proposer une structure générale pour les solutions de navigation assistée en fauteuil roulant et en présence d'humains.
La reconnaissance des entités nommées et une discipline cruciale du domaine du TAL.
Elle sert à l'extraction de relations entre entités nommées, ce qui permet la construction d'une base de connaissance (Surdeanu and Ji, 2014), le résumé automatique (Nobata et al., 2002), etc...
Nous nous intéressons ici aux phénomènes de structurations qui les entourent.
Nous distinguons ici deux types d'éléments structurels dans une entité nommée.
Les premiers sont des sous-chaînes récurrentes, que nous appelerons les affixes caractéristiques d'une entité nommée.
Le second type d'éléments est les tokens ayant un fort pouvoir discriminant, appelés des tokens déclencheurs.
Nous détaillerons l'algorithme que nous avons mis en place pour extraire les affixes caractéristiques, que nous comparerons à Morfessor (Creutz and Lagus, 2005b).
Nous appliquerons ensuite notre méthode pour extraire les tokens déclencheurs, utilisés pour l'extraction d'entités nommées du Français et d'adresses postales.
Nous proposons un type de cascade d'étiqueteurs linéaires qui n'avait jusqu'à présent jamais été utilisé pour la reconnaissance d'entités nommées, généralisant les approches précédentes qui ne sont capables de reconnaître des entités de profondeur finie ou ne pouvant modéliser certaines particularités des entités nommées structurées.
Didactique du discours : le français langue d'écrit universitaire en Algérie.
Étude contrastive entre filières scientifiques et sciences humaines » aborde la question de l'enseignement et/ou l'apprentissage des langues étrangères en Algérie à travers les caractéristiques du genre scientifique.
Il s'est agi de savoir si ce genre conserve sa stabilité dans l'écrit universitaire quand il est question de pratique d'une langue étrangère, en l'occurrence, le français.
La langue française n'existe pas pour elle-même.
Elle est la langue de scolarité à l'université algérienne et pose entre autres causes un obstacle à la réussite.
Se basant sur un corpus hétérogène constitué de douze mémoires d'études soutenus en Algérie, nous voudrions contraster, parmi ces rédactions, l'écrit des sciences humaines et sociales avec l'écrit des sciences dures et de la nature.
L'application d'une démarche assistée par Hyperbase, un logiciel de traitement automatique des langues, est nécessaire vu la taille volumineuse du corpus.
De plus, elle continue à développer de nouvelles techniques pour la recherche scientifique.
Afin de nous familiariser au contexte de l'usage de la langue français en Algérie, nous avons effectué une enquête par questionnaire auprès des apprenants et des enseignants universitaires algériens.
Le résultat principal obtenu de cette recherche montre que le genre est toujours dominant même dans un cadre précis de l'usage d'une langue étrangère.
Cette thèse a pris la forme d'un partenariat entre l'équipe VORTEX du laboratoire de recherche en informatique IRIT et l'entreprise Andil, spécialisée dans l'informatique pour l'e-learning.
Ce partenariat est conclu autour d'une thèse CIFRE, dispositif soutenu par l'État via l'ANRT.
La doctorante, Angela Bovo, a travaillé au sein de l'Université Toulouse 1 Capitole.
Un partenariat a également été noué avec l'institut de formation Juriscampus, qui nous a fourni des données issues de formations réelles pour nos expérimentations.
Notre objectif principal avec ce projet était d'améliorer les possibilités de suivi des étudiants en cours de formation en ligne pour éviter leur décrochage ou leur échec.
Nous avons proposé des possibilités de suivi par apprentissage automatique classique en utilisant comme données les traces d'activité des élèves.
Nous avons également proposé, à partir de nos données, des indicateurs de comportement des apprenants.
Avec Andil, nous avons conçu et réalisé une application web du nom de GIGA, déjà commercialisée et appréciée par les responsables de formation, qui implémente ces propositions et qui a servi de base à de premières expériences de partitionnement de données qui semblent permettre d'identifier les étudiants en difficulté ou en voie d'abandon.
Les implémentations ne sont toutefois pas encore parvenues à produire des résultats probants.
Notre étude s'intéresse à un aspect phonologique : le comportement du phonème /R/ du français parlé à Niamey, capitale du Niger, pays de l'Afrique subsaharienne.
Nos enquêtes de terrain ont été effectuées dans une ville où le français demeure la langue officielle, et où l'on retrouve aussi d'autres langues nationales et/ou locales (haousa, songhaï-zarma, touareg, peul, kanuri et arabes).
Une présentation des rhotiques du point de vue phonétique et phonologique s'est avéré nécessaire, avant la classification et l'analyse de nos données.
D'une part nous avons analysé les allophones de /R/ réalisés par les enquêtés.
Ces analyses montrent que la réalisation largement majoritaire est la vibrante alvéolaire [r], suivie de loin par la fricative uvulaire [ʁ], puis par les réalisations [ɰ], [χ], [ɻ] et la non-réalisation de /R/ [ø].
Tous ces résultats ont été comparés ensuite à ceux d'autres points d'enquêtes PFC dans le monde.
D'autre part, nous nous sommes intéressés à la chute de /R/ dans les groupes consonantiques et en finales, pour aboutir à la conclusion que ce phénomène dépend du lexique et, plus exactement, concerne généralement la prononciation des chiffres (par exemple quatre [katR]&gt;
La microcirculation désigne le sous-ensemble du système circulatoire où s'effectuent les échanges gazeux et liquidiens extracellulaires.
Elle est composée des artérioles, des capillaires et des veinules.
Les objectifs de ce travail sont d'étudier, de comprendre et d'identifier de nouvelles étiologies iatrogènes à ces pathologies microvasculaires, ainsi que d'évaluer et de comparer l'efficacité et la sécurité des traitements utilisés dans ces pathologies.
Nous avons, à cette fin, réalisé plusieurs études à partir des bases de données de pharmacovigilances, de données d'essais cliniques et de la littérature.
Ce travail de thèse nous a permis d'explorer le rôle des médicaments dans ces pathologies microvasculaires, champs qui restait encore peu étudié dans la littérature.
Ces travaux nous ont permis d'identifier de nombreuses classes pharmacologiques dont le rôle était encore non décrit dans ces pathologies.
L'étude des mécanismes pharmacologiques à l'origine de ces effets indésirables permet également d'émettre de nouvelles hypothèses physiopathologiques à l'origine de ces maladies.
Les traitements utilisés dans ces différentes pathologies microcirculatoires sont à l'heure actuelle encore peu spécifiques et des travaux de recherche important doivent encore être réalisés afin de personnaliser la prise en charge des patients.
Notre thèse s'inscrit dans le cadre des langages contrôlés pour le génie logiciel.
Elle a pour but de faciliter l'adoption de l'approche par règles métier (ARM) par les entreprises en créant un langage contrôlé en vue d'aider à la spécification des règles métier par les experts métier.
Notre solution va permettre de réduire la distance sémantique entre les experts métier et les experts système afin de répondre non seulement au besoin d'intercompréhension entre ces derniers mais aussi pour réaliser un transfert automatique de la description des règles métier vers les systèmes d'information (SI).
Ce langage contrôlé que nous avons créé permettra d'assurer en plus la consistance et la traçabilité de ces règles avec leur implantation
Les facteurs humains figurent parmi les causes originelles de trop nombreux accidents, dans les transports, l'industrie ou encore dans les parcours de soins.
La formation des équipes interprofessionnelles à la gestion des risques dans un environnement reproduisant fidèlement le contexte professionnel est un enjeu majeur.
La motivation de cette thèse est de proposer un environnement virtuel multi-joueurs destiné à la formation à la gestion des risques liés à des défauts de communication ou de prises de décision.
Pour cela, une méthode de création de scénarios interactifs destinés à la formation à la gestion des risques a été présentée.
L'environnement multi-joueurs interactif s'appuie sur cet ensemble cohérent.
Ils permettent aussi le contrôle de la situation pédagogique dans son ensemble.
Une méthode à forte valeur d'innovation a aussi été proposée pour structurer le débriefing d'une formation à la gestion des risques.
Le nombre d'objets connectés ne cesse de croître à tel point que des milliards d'objets sont attendus dans un futur proche.
L'approche de cette thèse met en place un système de gestion autonomique pour des systèmes à base d'objets connectés, en les combinant avec d'autres services comme par exemple des services météo accessibles sur internet.
Des paramètres comme le temps d'exécution ou l'énergie consommée sont aussi considérés afin d'optimiser les choix d'actions à effectuer et de services utilisés.
Un prototype concret a été réalisé dans un scénario de ville intelligente et de bus connectés dans le projet investissement d'avenir S2C2.
Les fausses informations se multiplient et se propagent rapidement sur les réseaux sociaux.
Dans cette thèse, nous analysons les publications d'un point de vue multimodal entre le texte et l'image associée.
Plusieurs études ont été menées durant cette thèse.
La première compare plusieurs types de médias présents sur les réseaux sociaux et vise à les discriminer de manière automatique.
La second permet la détection et la localisation de modifications dans une image grâce à la comparaison avec une ancienne version de l'image.
Enfin, nous nous sommes intéressés à des fusions de connaissances basées sur les prédictions d'autres équipes de recherche afin de créer un système unique.
Une présentation des grands principes de l'intercompréhension multilingue introduit la description des projets de recherche dans l'Union européenne (et en Amérique latine) et de leurs applications, classés selon leurs positionnements réceptif (compréhension) ou réciproque (communication interactive) : ils se caractérisent surtout par l'exploitation de la linguistique de surface ou par celle de l'extralinguistique suivant que les langues sont parentes ou plus éloignées.
Un large emprunt est fait aux résultats des travaux sur le traitement automatique des langues (TAL : P. Pognan et D. Lemay.)
Une liste des institutions officielles permettra de mieux les approcher.
En conclusion, nous proposons parmi les stratégies, les méthodes et les outils explorés, une sélection de ceux que l'on pourrait développer en premier lieu.
Cette thèse s'intéresse à la notion de budget pour étudier des problèmes de complexité (complexité en calculs, tâche complexe pour un agent, ou complexité due à une faible quantité de données).
En effet, l'objectif principal des techniques actuelles en apprentissage statistique est généralement d'obtenir les meilleures performances possibles, sans se soucier du coût de la tâche.
La notion de budget permet de prendre en compte ce paramètre tout en conservant de bonnes performances.
Nous nous concentrons d'abord sur des problèmes de classification en grand nombre de classes : la complexité en calcul des algorithmes peut être réduite grâce à l'utilisation d'arbres de décision (ici appris grâce à des techniques d'apprentissage par renforcement budgétisées) ou à l'association de chaque classe à un code (binaire).
Nous nous intéressons ensuite aux problèmes d'apprentissage par renforcement et à la découverte d'une hiérarchie qui décompose une tâche en plusieurs tâches plus simples, afin de faciliter l'apprentissage et la généralisation.
Cette découverte se fait ici en réduisant l'effort cognitif de l'agent (considéré dans ce travail comme équivalent à la récupération et à l'utilisation d'une observation supplémentaire).
Nous analyserons en premier lieu l'ensemble des contraintes concourant à l'exercice de l'interprétation en langue des signes pouvant se distinguer de celles généralement observées en interprétation entre langues vocales (nous incluons les langues vocales syntaxiquement très éloignées) telles que les contraintes socio-économiques, les contraintes linguistiques et enfin les contraintes d'espace.
Nous procéderons ensuite à une analyse cognitive du processus de l'interprétation en nous référant au modèle d'Efforts de l'interprétation simultanée de Gile (Effort d'Écoute et d'Analyse, Effort de Mémorisation à court terme, Effort de Production, Effort de Coordination de ces trois activités simultanées), et nous chercherons à envisager sa transposition aux langues des signes.
Pour mieux comprendre les mécanismes constitutifs du processus, nous observerons particulièrement le concept de scénarisation (Séro-Guillaume, 2008) pour une première analyse de la charge cognitive de l'interprète en action.
Cette capacité de représentation synthétique visuelle est-elle plus ou moins grande si on prend en compte le degré d'abstraction du discours, la technicité de l'énoncé, le manque de correspondances lexicales, le contexte de l'interprétation (pédagogique, conférence, etc.), la préparation ?
Notre analyse du processus se base sur un corpus constitué de plusieurs études empiriques d'interprétations vers la langue des signes : une étude semi-expérimentale, une étude de cas naturaliste et une étude expérimentale, ainsi que sur des interviews d'interprètes et un focus group.
Les observations faites sur l'ensemble de ces études nous ont permis de croiser nos données et de dégager les éléments pertinents de nos résultats pour une avancée dans la compréhension du processus cognitif de l'interprétation en langue des signes.
De nombreuses applications en traitement du langage naturel (TALN) reposent sur les représentations de mots, ou “word embeddings”.
Ces représentations doivent capturer à la fois de l'information syntaxique et sémantique pour donner des bonnes performances dans les tâches en aval qui les utilisent.
Cependant, les méthodes courantes pour les apprendre utilisent des textes génériques comme Wikipédia qui ne contiennent pas d'information sémantique précise.
De plus, un espace mémoire important est requis pour pouvoir les sauvegarder car le nombre de représentations de mots à apprendre peut être de l'ordre du million.
J'ai développé dict2vec, un modèle qui utilise l'information des dictionnaires linguistiques lors de l'apprentissage des word embeddings.
Les word embeddings appris par dict2vec obtiennent des scores supérieurs d'environ 15% par rapport à ceux appris avec d'autres méthodes sur des tâches de similarités sémantiques de mots.
La seconde partie de mes travaux consiste à réduire la taille mémoire des word embeddings.
J'ai développé une architecture basée sur un auto-encodeur pour transformer des word embeddings à valeurs réelles en vecteurs binaires, réduisant leur taille mémoire de 97% avec seulement une baisse de précision d'environ 2% dans des tâches de TALN en aval.
Notre objectif est de décrire les propriétés formelles des phrases à verbes supports en français et en malais et de les comparer du point de vue syntaxique et sémantique.
Nous avons appliqué quatre tests de reconnaissance du verbe support afin de déterminer le statut de ces trois verbes dans les constructions étudiées.
Notre étude est structurée en six chapitres.
Le deuxième chapitre donne une présentation générale du malais.
Le troisième chapitre présente le verbe support buat/membuat (faire) en malais.
Le quatrième chapitre présente l'étude du verbe support beri/memberi (donner) en malais.
Le cinquième chapitre présente l'étude du verbe support ambil/mengambil (prendre) en malais.
Le sixième chapitre présente l'étude comparative des verbes supports faire (membuat), donner (memberi) et prendre (mengambil) en français et en malais.
Les résultas obtenus nous ont montré que le français et le malais partagent les mêmes caractéristiques générales des verbes supports.
Ces résultats nous ont permis également de montrer que malgré l'universalité du phénomène, chaque langue possède ses propres mécanismes concernant le fonctionnement des verbes supports et le système de la détermination.
Notre travail, vise à construire, à partir d'une méthode d'analyse automatique profonde ou neuronale, des représentations sémantiques de valeurs aspecto-temporelles (associées aux indices linguistiques) du passé composé français.
La tâche d'alignement d'un texte dans une langue source avec sa traduction en langue cible est souvent nommée alignement de bi-textes.
Elle a pour but de faire émerger les relations de traduction qui peuvent s'exprimer à différents niveaux de granularité entre les deux faces du bi-texte.
De nombreuses applications de traitement automatique des langues naturelles s'appuient sur cette étape afin d'accéder à des connaissances linguistiques de plus haut niveau.
Parmi ces applications, nous pouvons citer bien sûr la traduction automatique, mais également l'extraction de lexiques et de terminologies bilingues, la désambigüisation sémantique ou l'apprentissage des langues assisté par ordinateur.
La complexité de la tâche d'alignement de bi-textes s'explique par les différences linguistiques entre les langues.
Dans le cadre des approches probabilistes, l'alignement de bi-textes est modélisé par un ensemble de variables aléatoires cachés.
Afin de réduire la complexité du problème, le processus aléatoire sous-jacent fait l'hypothèse simplificatrice qu'un mot en langue source est lié à au plus un mot en langue cible, ce qui induit une relation de traduction asymétrique.
Néanmoins, cette hypothèse est simpliste, puisque les alignements peuvent de manière générale impliquer des groupes de mots dans chacune des langues.
Afin de rétablir cette symétrie, chaque langue est considérée tour à tour comme la langue source et les deux alignements asymétriques résultants sont combinés à l'aide d'une heuristique.
Cette étape de symétrisation revêt une importance particulière dans l'approche standard en traduction automatique, puisqu'elle précède l'extraction des unités de traduction, à savoir les paires de segments.
La spécificité de notre approche consiste à remplacer les heuristiques utilisées par des modèles d'apprentissage discriminant.
L'interaction entre les liens d'alignement est alors prise en compte par l'empilement ("stacking'') d'un second modèle prenant en compte la structure à prédire sans pour autant augmenter la complexité globale.
Cette formulation peut être vue comme une manière d'apprendre la combinaison de différentes méthodes d'alignement : le modèle considère ainsi l'union des alignements d'entrées pour en sélectionner les liens jugés fiables.
Ces améliorations sont mesurées en terme de taux d'erreur sur les alignements et aussi en terme de qualité de traduction via la métrique automatique BLEU.Nous proposons également un modèle permettant à la fois de sélectionner et d'évaluer les unités de traduction extraites d'un bi texte aligné.
Ce cadre permet l'utilisation de caractéristiques riches et nombreuses favorisant ainsi une décision robuste.
Nous proposons une méthode simple et efficace pour annoter les paires de segments utiles pour la traduction.
Le problème d'apprentissage automatique qui se pose alors est particulier, puisque nous disposons que d'exemples positifs.
Nous proposons donc d'utiliser l'approche SVM à une classe afin de modéliser la sélection des unités de traduction.
Grâce à cette approche, nous obtenons des améliorations significatives en terme de score BLEU pour un système entrainé avec un petit ensemble de données.
Le développement d'un système de reconnaissance de la parole exige la disponibilité d'une grande quantité de ressources à savoir, grands corpus de texte et de parole, un dictionnaire de prononciation.
Néanmoins, ces ressources ne sont pas disponibles directement pour des dialectes arabes.
De ce fait, le développement d'un SRAP pour les dialectes arabes se heurte à de multiples difficultés à savoir, l''abence de grandes quantités de ressources et l'absence d''une orthographe standard vu que ces dialectes sont parlés et non écrit.
Dans cette perspective, les travaux de cette thèse s'intègrent dans le cadre du développement d'un SRAP pour le dialecte tunisien.
Une première partie des contributions consiste à développer une variante de CODA (Conventional Orthography for Arabic Dialectal) pour le dialecte tunisien.
En fait, cette convention est conçue dans le but de fournir une description détaillée des directives appliquées au dialecte tunisien.
Compte tenu des lignes directives de CODA, nous avons constitué notre corpus nommé TARIC : Corpus de l'interaction des chemins de fer de l'arabe tunisien dans le domaine de la SNCFT.
Outre ces ressources, le dictionnaire de prononciation s'impose d'une manière indispensable pour le développement d'un SRAP.
À ce propos, dans la deuxième partie des contributions, nous visons la création d'un système nommé conversion (Graphème-Phonème) G2P qui permet de générer automatiquement ce dictionnaire phonétique.
Toutes ces ressources décrites avant sont utilisées pour adapter un SRAP pour le MSA du laboratoire LIUM au dialecte tunisien dans le domaine de la SNCFT.
L'évaluation de notre système donné lieu WER de 22,6% sur l'ensemble de test.
Cet accident est le résultat d'un décalage inédit entre l'état de l'art des heuristiques des ingénieurs de forage et celui des ingénieurs antipollution.
Deepwater Horizon est en ce sens un cas d'ingénierie en situation extrême, tel que défini par Guarnieri et Travadel.
Nous proposons d'abord de revenir sur le concept général d'accident au moyen d'une analyse linguistique poussée présentant les espaces sémantiques dans lesquels se situe l'accident.
Cela permet d'enrichir son « noyau de sens » et l'élargissement de l'acception commune de sa définition.
Puis, nous amenons que la revue de littérature doit être systématiquement appuyée par une assistance algorithmique pour traiter les données compte tenu du volume disponible, de l'hétérogénéité des sources et des impératifs d'exigences de qualité et de pertinence.
En effet, plus de huit cent articles scientifiques mentionnant cet accident ont été publiés à ce jour et une vingtaine de rapports d'enquêtes, constituant notre matériau de recherche, ont été produits.
Notre méthode montre les limites des modèles d'accidents face à un cas comme Deepwater Horizon et l'impérieuse nécessité de rechercher un moyen de formalisation adéquat de la connaissance.
De ce constat, l'utilisation des ontologies de haut niveau doit être encouragée.
L'ontologie DOLCE a montré son grand intérêt dans la formalisation des connaissances à propos de cet accident et a permis notamment d'élucider très précisément une prise de décision à un moment critique de l'intervention.
La population, la création d'instances, est le coeur de l'exploitation de l'ontologie et son principal intérêt mais le processus est encore très largement manuel et non exempts d'erreurs.
Cette thèse propose une réponse partielle à ce problème par un algorithme NER original de population automatique d'une ontologie.
Enfin, l'étude des accidents n'échappe pas à la détermination des causes et à la réflexion sur les « faits socialement construits » .
Cette thèse propose les plans originaux d'un « pipeline sémantique » construit à l'aide d'une série d'algorithmes qui permet d'extraire la causalité exprimée dans un document et de produire un graphe représentant ainsi le « cheminement causal » sous-jacent au document.
On comprend l'intérêt pour la recherche scientifique ou industrielle de la mise en lumière ainsi créée du raisonnement afférent de l'équipe d'enquête.
Cette thèse est un travail d'assembleur, d'architecte, qui amène à la fois un regard premier sur le cas Deepwater Horizon et propose le forage des données, une méthode et des moyens originaux pour aborder un évènement, afin de faire émerger du matériau de recherche des réponses à des questionnements qui échappaient jusqu'alors à la compréhension.
Notre thèse s'inscrit dans la théorie de l'énonciation à la suite des travaux d'Antoine Culioli et de Jean-Pierre Desclés.
Tout énoncé nécessite un énonciateur.
Selon que l'énonciateur se réfère ou non à la situation énonciative plusieurs types d'énoncés apparaissent.
Nous avons comparé deux types d'énoncés décrits par Aristote avec les représentations établies par Jean-Pierre Desclés dans la Grammaire Applicative et Cognitive.
Dans les deux cas, un changement est représenté entre deux situations stables.
Dans les deux cas, un mouvement en train de se déployer est décrit dans son inaccomplissement.
Nous avons proposé de comprendre la notion d'entéléchie utilisée par Aristote lorsqu'il définit le mouvement, à partir de la notion de processus inaccompli décrite par Jean-Pierre Desclés.
Alors que l'énoncé d'un changement nécessite de valider deux états stables pour une unique référence temporelle, l'énoncé d'un mouvement en train de se déployer nécessite que l'énonciateur constate effectivement un mouvement qu'il savait possible.
Un paramètre temporel reste commun à ces deux types d'énoncés.
L'écriture d'un programme informatique a permis d'extraire automatiquement les valeurs de ce paramètre temporel et de distinguer les différents énoncés d'un corpus textuel numérisé.
L'application informatique met en œuvre la description théorique dans un composant logiciel utilisable par d'autres programmes pour organiser temporellement un texte en vue du traitement automatique des langues naturelles.
Le traitement automatique de la langue quechua (TALQ) ne dispose pas actuellement d'un dictionnaire électronique des verbes, du français-quechua.
Pourtant, un projet visant la traduction automatique nécessite au préalable, entre autres, cette importante ressource.
La réalisation d'un tel dictionnaire peut ouvrir également de nouvelles perspectives dans l'enseignement à distance, dans les domaines de l'accès multilingue aux informations, l'annotation/l'indexation des documents, la correction orthographique et pour le TAL en général.
La première difficulté consiste à sélectionner un dictionnaire français comme base de travail.
Parmi les nombreux dictionnaires français, il en existe très peu en format électronique, et moins encore ceux dont les sources soient en libre accès au public.
Parmi ces derniers, l'ouvrage Les verbes français (LVF), contenant 25 610 sens verbaux, que Jean Dubois et Françoise Dubois-Charlier ont publié chez Larousse en 1997, est un dictionnaire particulièrement complet ; de plus il a l 'avantage d'avoir une licence « open source » et un format compatible avec la plateforme NooJ.
En tenant en compte ces considérations nous avons choisi traduire ce dictionnaire en quechua.
Cependant, cette tâche se heurte à un obstacle considérable : le lexique quechua de verbes simples compte moins de l 500 entrées.
Comment faire correspondre 25 610 sens verbaux français avec seulement 1 500 verbes quechua ?
Sommes-nous condamnés à utiliser beaucoup de polysémies ?
Par exemple, dans LVF il y a 27 sens verbaux du verbe « tourner » ; doit-on tous les traduire par muyuy ?
Ou bien, pouvons-nous utiliser une stratégie particulière et remarquable de la langue pour répondre à ce défi : la génération de nouveaux verbes par dérivation suffixale ?
Nous avons inventorié tous les suffixes du quechua qui permettent d'obtenir une forme dérivée possédant le comportement d'un verbe simple.
Cet ensemble de suffixes que nous appelons SIP_DRV, contient 27 éléments.
Ainsi chaque verbe quechua transitif ou intransitif donne naissance à au moins 27 verbes dérivés.
Il reste cependant à formaliser les paradigmes et grammaires qui vont nous permettre d'obtenir les dérivations compatibles avec la morphosyntaxe de la langue.
Cela a été réalisé avec NooJ.
L'application de ces grammaires nous a permis d'obtenir 40 500 unités linguistiques conjugables (ULAV) à partir de 1 500 verbes simples quechua.
Ce résultat encourageant nous permet d'envisager une solution favorable à notre projet de traduction des 25 000 sens verbaux du français en quechua.
Afin d'obtenir la traduction de ces ULAV, nous avons besoin d'abord de connaître la modalité d'énonciation qu'apporte chaque SIP quand il s'agglutine au radical verbal pour le transformer.
Chaque suffixe peut avoir plusieurs modalités d'énonciation.
Nous les avons obtenus à partir du corpus, de notre propre expérience et quelques enregistrements dans le terrain.
Nous avons ainsi construit un tableau indexé contenant toutes ces modalités.
Ensuite, nous utilisons des opérateurs de NooJ pour programmer les grammaires qui présentent la traduction automatique en une forme glosés de modalités d'énonciation.
Finalement, nous avons développé un algorithme qui nous a permis d'obtenir la traduction réciproque du français vers le quechua de plus de 8 500 sens verbaux de niveau 3 et un certain nombre de sens verbaux de niveau 4 et 5.
Les réseaux sociaux, et Twitter en particulier, sont devenus une source d'information privilégiée pour les journalistes ces dernières années.
Beaucoup effectuent une veille sur Twitter, à la recherche de sujets qui puissent être repris dans les médias.
Cette thèse vise à étudier et à quantifier l'effet de ce changement technologique sur les décisions prises par les rédactions.
Par la suite, nous étudions différents types d'algorithmes pour découvrir automatiquement les tweets qui se rapportent aux mêmes événements.
Nous testons différentes représentation vectorielles de tweets, en nous intéressants aux représentations vectorielles de texte, et aux représentations texte-image.
Enfin, nous concevons un instrument économétrique pour identifier un effet causal de la popularité d'un événement sur Twitter sur sa couverture par les médias traditionnels.
Nous montrons que la popularité d'un événement sur Twitter a un effet sur le nombre d'articles qui lui sont consacrés dans les médias traditionnels, avec une augmentation d'environ 1 article pour 1000 tweets supplémentaires.
La synthèse de la parole par corpus (sélection d'unités) est le sujet principal de cette thèse.
Tout d'abord, une analyse approfondie et un diagnostic de l'algorithme de sélection d'unités (algorithme de recherche dans le treillis d'unités) sont présentés.
L'importance de l'optimalité de la solution est discutée et une nouvelle mise en œuvre de la sélection basée sur un algorithme A* est présenté.
Trois améliorations de la fonction de coût sont également présentées.
La première est une nouvelle façon – dans le coût cible – de minimiser les différences spectrales en sélectionnant des séquences d'unités minimisant un coût moyen au lieu d'unités minimisant chacune un coût cible de manière absolue.
Ce coût est testé pour une distance sur la durée phonémique mais peut être appliqué à d'autres distances.
Notre deuxième proposition est une fonction de coût cible visant à améliorer l'intonation en se basant sur des coefficients extraits à travers une version généralisée du modèle de Fujisaki.
Les paramètres de ces fonctions sont utilisés au sein d'un coût cible.
Enfin, notre troisième contribution concerne un système de pénalités visant à améliorer le coût de concaténation.
Il pénalise les unités en fonction de classes reposant sur une hiérarchie du degré de risque qu'un artefact de concaténation se produise lors de la concaténation sur un phone de cette classe.
Ce système est différent des autres dans la littérature en cela qu'il est tempéré par une fonction floue capable d'adoucir le système de pénalités pour les unités présentant des coûts de concaténation parmi les plus bas de leur distribution.
La quantité d'informations, de produits et de relations potentielles dans les réseaux sociaux a rendu indispensable la mise à disposition de recommandations personnalisées.
L'activité d'un utilisateur est enregistrée et utilisée par des systèmes de recommandation pour apprendre ses centres d'intérêt.
Les recommandations sont également utiles lorsqu'estimer la pertinence d'un objet est complexe et repose sur l'expérience.
L'apprentissage automatique offre d'excellents moyens de simuler l'expérience par l'emploi de grandes quantités de données.
Cette thèse examine le démarrage à froid en recommandation, situation dans laquelle soit un tout nouvel utilisateur désire des recommandations, soit un tout nouvel objet est proposé à la recommandation.
En l'absence de données d'intéraction, les recommandations reposent sur des descriptions externes.
En optimisation, il est possible d'aborder le choix d'algorithme dans un portfolio d'algorithmes comme un problème de recommandation.
Notre première contribution concerne un système à deux composants, un sélecteur et un ordonnanceur d'algorithmes, qui vise à réduire le coût de l'optimisation d'une nouvelle instance d'optimisation tout en limitant le risque d'un échec de l'optimisation.
Les deux composants sont entrainés sur les données du passé afin de simuler l'expérience, et sont alternativement optimisés afin de les faire coopérer.
Ce système a remporté l'Open Algorithm Selection Challenge 2017.L'appariement automatique de chercheurs d'emploi et d'offres est un problème de recommandation très suivi par les plateformes de recrutement en ligne.
Une seconde contribution concerne le développement de techniques spécifiques pour la modélisation du langage naturel et leur combinaison avec des techniques de recommandation classiques afin de tirer profit à la fois des intéractions passées des utilisateurs et des descriptions textuelles des annonces.
Une discussion sur la pertinence des différents systèmes de recommandations pour des applications similaires est proposée.
Dans cette thèse, nous étudions deux problèmes d'apprentissage automatique : (I) la détection des communautés et (II) l'appariement adaptatif.
I) Il est bien connu que beaucoup de réseaux ont une structure en communautés.
La détection de ces communautés nous aide à comprendre et exploiter des réseaux de tout genre.
Cette thèse considère principalement la détection des communautés par des méthodes spectrales utilisant des vecteurs propres associés à des matrices choisiesavec soin.
Nous faisons une analyse de leur performance sur des graphes artificiels.
Au lieu du modèle classique connu sous le nom de « Stochastic Block Model » (dans lequel les degrés sont homogènes) nous considérons un modèle où les degrés sont plus variables : le « Degree-Corrected Stochastic Block Model » (DC-SBM).
Nous étudions ce modèle dans deux régimes : le régime dense et le régime « épars » , ou « dilué » .
Dans le régime dense, nous prouvons qu'un algorithme basé sur une matrice d'adjacence normalisée réussit à classifier correctement tous les nœuds sauf une fraction négligeable.
Dans le régime épars il existe un seuil en termes de paramètres du modèle en-dessous lequel n'importe quel algorithme échoue par manque d'information.
En revanche, nous prouvons qu'un algorithme utilisant la matrice « non-backtracking » réussit jusqu'au seuil - cette méthode est donc très robuste.
Pour montrer cela nous caractérisons le spectre des graphes qui sont générés selon un DC-SBM dans son régime épars.
Nous concluons cette partie par des tests sur des réseaux sociaux.
II) Les marchés d'intermédiation en ligne tels que des plateformes de Question-Réponse et des plateformes de recrutement nécessitent un appariement basé sur une information incomplète des deux parties.
Nous développons un modèle de système d'appariement entre tâches et serveurs représentant le comportement de telles plateformes.
Pour ce modèle nous donnons une condition nécessaire et suffisante pour que le système puisse gérer un certain flux de tâches.
Nous introduisons également une politique de « back-pressure » sous lequel le débit gérable par le système est maximal.
Nous prouvons que cette politique atteint un débit strictement plus grand qu'une politique naturelle « gloutonne » .
Nous concluons en validant nos résultats théoriques avec des simulations entrainées par des données de la plateforme Stack-Overflow.
Nos travaux portent sur les Expressions Numériques Approximatives (ENA), définies comme des expressions linguistiques impliquant des valeurs numériques et un adverbe d'approximation, telles que "environ 100".
Nous nous intéressons d'abord à l'interprétation d'ENA non contextualisées, dans ses aspects humain et computationnel.
Nous avons ensuite proposé deux modèles d'interprétation, basés sur un même principe de compromis entre la saillance cognitive des bornes des intervalles et leur distance à la valeur de référence de l'ENA, formalisé par un front de Pareto.
Leur validation expérimentale à partir de données réelles montre qu'ils offrent de meilleures performances que les modèles existants.
Nous avons également montrél'intérêt du modèle flou en l'implémentant dans le cadre des requêtes flexibles de bases de données.
Nous avons ensuite montré, par une étude empirique, que le contexte et les interprétations, implicite vs explicite, ont peu d'effet sur les intervalles.
Nous nous intéressons enfin à l'addition et à la multiplication d'ENA, par exemple pour évaluer la surface d'une pièce d'"environ 10" par "environ 20 mètres".
Nous avons mené une étude dont les résultats indiquent que les imprécisions liées aux opérandes ne sont pas prises en compte lors des calculs.
Ainsi, les contributions principales de cette série de travaux sont centrées autour de 1) l'étude des dépendances spatiales, temporelles, linguistique et du réseau liées aux inégalités et 2) l'inférence du statut socioéconomique à partir de ces signaux multimodaux.
De l'autre nous cherchons nous même à fournir des éléments de réponse aux questions posées par les sciences sociales qui se sont avérées trop intractable pour être abordées sans le volume et la qualité de données nécessaires.
Le but essentiel de la thèse est de développer une ontologie juridique bien fondée pour l'utiliser dans le raisonnement à base des règles.
Cette thèse en informatique porte sur la problématique de la capitalisation des processus d'analyse de traces d'apprentissage au sein de la communauté des Learning Analytics (LA).
Il s'agit de permettre de partager, adapter et réutiliser ces processus d'analyse de traces.
Cela empêche de les partager, mais aussi de les ré-exploiter simplement en dehors de leurs contextes initiaux, quand bien même les nouveaux contextes seraient similaires.
L'objectif de cette thèse est de fournir des modélisations et des méthodes permettant la capitalisation des processus d'analyse de traces d'apprentissage, ainsi que d'assister les différents acteurs de l'analyse, notamment durant la phase de réutilisation.
Notre deuxième contribution répond au premier verrou lié à la dépendance technique des processus d'analyse actuels, et à leur partage.
Nous proposons un méta-modèle qui permet de décrire les processus d'analyse indépendamment des outils d'analyse.
Ce méta-modèle formalise la description des opérations utilisées dans les processus d'analyse, des processus eux-mêmes et des traces utilisées, afin de s'affranchir des contraintes techniques occasionnées par ces outils.
Ce formalisme commun aux processus d'analyse permet aussi d'envisager leur partage.
Il a été mis en œuvre et évalué dans un de nos prototypes.
Notre troisième contribution traite le deuxième verrou sur la ré-exploitation des processus d'analyse.
Nous proposons un framework ontologique pour les processus d'analyse, qui permet d'introduire de manière structurée des éléments sémantiques dans la description des processus d'analyse.
Cette approche narrative enrichit ainsi le formalisme précédent et permet de satisfaire les propriétés de compréhension, d'adaptation et de réutilisation nécessaires à la capitalisation.
Cette approche ontologique a été mise en œuvre et évaluée dans un autre de nos prototypes.
Enfin, notre dernière contribution répond au dernier verrou identifié et concerne de nouvelles pistes d'assistances aux acteurs, notamment une nouvelle méthode de recherche des processus d'analyse,
Nous utilisons également le réseau sémantique sous-jacent à cette modélisation ontologique pour renforcer l'assistance aux acteurs en leur fournissant des outils d'inspection et de compréhension lors de la recherche.
Cette assistance a été mise en œuvre dans un de nos prototypes, et évaluée empiriquement.
Située au cœur de l'océan Indien et ayant une superficie de 2 040 km2, la République de Maurice est un pays insulaire qui regroupe quatre îles : Maurice (l'île principale), Rodrigues, Agaléga et Saint-Brandon.
Près de 1,3 million de personnes y habitent et constituent ensemble une communauté linguistique plurilingue au sein de laquelle plus de 10 langues sont pratiquées.
En dépit de cette richesse linguistique, façonnée par son histoire propice au contact de langues, et des compétences linguistiques plurilingues des Mauriciens, la langue la plus couramment parlée, par 86,50 % de la population mauricienne au sein du domicile familial (selon le recensement ministériel de 2011), est le mauricien : une langue créole à base française.
Les études sur le créole mauricien ont débuté pendant la période de colonisation.
Au XIXe siècle, Baissac (1880) a proposé une « étude sur le patois créole mauricien » et au XXe siècle, après l'indépendance en mars 1968, Baker (1972) a publié un ouvrage sur sa description linguistique.
La seule grammaire contemporaine disponible est celle de Police-Michel et al. (2012).
Diverses thèses ont aussi été soutenues sur les catégories syntaxiques du créole mauricien notamment sur le nom (Alleesaib, 2012), le verbe (Henri, 2010) et l'adverbe (Hassamal, 2017).
Toutefois, parmi tous les travaux recensés, aucun ne s'est purement intéressé au domaine et à la thématique du Traitement Automatique des Langues (TAL).
David (2019) a réalisé un travail de recherche sur l'étiquetage morphosyntaxique du créole mauricien, mais globalement les travaux en traitement automatique demeurent lacunaires pour cette langue peu dotée informatiquement.
À partir de ces travaux en syntaxe ainsi que des méthodes et des outils de TAL développés par David (2019), cette thèse vise à construire et à exploiter un corpus arboré (un corpus écrit annoté syntaxiquement) pour le créole mauricien, à l'exemple du French Treebank pour le français (Abeillé et al., 2003) et du Penn Treebank pour l'anglais (Taylor et al., 2003).
La démarche méthodologique envisagée dans le cadre de ce travail se regroupera autour de 5 principales phases.
D'abord, la première phase s'attèlera à la constitution, à la normalisation, et à la structuration d'un corpus électronique écrit.
Les outils informatiques indispensables à ce travail seront développés ou réadaptés au cours de la deuxième phase.
La troisième phase se consacrera à l'annotation du corpus à partir d'un schéma d'annotation cohérent et structuré.
Les expérimentations seront menées durant la quatrième phase.
Enfin, la cinquième phase s'occupera d'évaluer la qualité des annotations réalisées et la performance des outils développés.
Finalement, l'objectif ultime de cette thèse est de parvenir à la réalisation d'analyses syntaxiques automatiques en créole mauricien, tout en dotant cette langue de tous les éléments nécessaires à son traitement automatique, en particulier dans une perspective d'analyse syntaxique.
L'objectif de cette thèse a été de proposer une approche robuste pour traiter le problème de la recherche de la réponse précise à une question.
Notre première contribution a été la conception et la mise en oeuvre d'un modèle de représentation robuste de l'information et son implémentation.
Son objectif est d'apporter aux phrases des documents et aux questions de l'information structurelle, composée de groupes de mots typés (segments typés) et de relations entre ces groupes.
Ce modèle a été évalué sur différents corpus (écrits, oraux, web) et a donné de bons résultats, prouvant sa robustesse.
Notre seconde contribution a consisté en la conception d'une méthode de réordonnancement des candidats réponses retournés par un système de questions-réponses.
Cette méthode a aussi été conçue pour des besoins de robustesse, et s'appuie sur notre première contribution.
L'idée est de comparer une question et le passage d'où a été extraite une réponse candidate, et de calculer un score de similarité, en s'appuyant notamment sur une distance d'édition.
Le réordonnanceur a été évalué sur les données de différentes campagnes d'évaluation.
Les résultats obtenus sont particulièrement positifs sur des questions longues et complexes.
Ces résultats prouvent l'intérêt de notre méthode, notre approche étant particulièrement adaptée pour traiter les questions longues, et ce quel que soit le type de données.
Le réordonnanceur a ainsi été évalué sur l'édition 2010 de la campagne d'évaluation Quaero, où les résultats sont positifs.
Nous sommes intéressé par la reconstitution et la prédiction des séries temporelles multivariées à partir des données partiellement observées et/ou agrégées.
Après investiguer le krigeage, qui est une méthode de la litérature de la statistique spatio-temporelle, et une méthode hybride basée sur le clustering des individus, nous proposons un cadre général de reconstitution et de prédiction basé sur la factorisation de matrice nonnégative.
Ce cadre prend en compte de manière intrinsèque la corrélation entre les séries temporelles pour réduire drastiquement la dimension de l'espace de paramètres.
Une fois que le problématique est formalisé dans ce cadre, nous proposons deux extensions par rapport à l'approche standard.
La première extension prend en compte l'autocorrélation temporelle des individus.
Cette information supplémentaire permet d'améliorer la précision de la reconstitution.
La deuxième extension ajoute une composante de régression dans la factorisation de matrice nonnégative.
Celle-ci nous permet d'utiliser dans l'estimation du modèle des variables exogènes liées avec la consommation électrique, ainsi de produire des facteurs plus interprétatbles, et aussi améliorer la reconstitution.
Sur le côté théorique, nous nous intéressons à l'identifiabilité du modèle, ainsi qu'à la propriété de la convergence des algorithmes que nous proposons.
La performance des méthodes proposées en reconstitution et en prédiction est testé sur plusieurs jeux de données de consommation électrique à niveaux d'agrégation différents.
Cette thèse pluridisciplinaire, à l'interface entre informatique et linguistique historique, a pour objet la modélisation computationnelle des changements phonétiques, en s'appuyant sur l'identification et la prédiction de cognats.
Les divergences phonologiques entre cognats capturent en effet pour une part les divergences entre l'évolution phonétique de langues apparentées.
Le travail se structure en trois étapes principales.
La première consiste en la création d'une base de données lexicales étymologiques de taille suffisamment importante pour permettre l'entraînement de modèles neuronaux modélisant les correspondances phonétiques entre langues par le biais de la tâche de prédiction de cognats.
La conception et l'entraînement de tels réseaux constitue la deuxième étape du travail.
Les réseaux de neurones étudiés sont des réseaux de séquence à séquence, similaires à ceux qui constituent aujourd'hui l'état de l'art en traduction automatique, mais adaptés aux spécificités de ce travail, et notamment au faible volume de données.
Le troisième étape consiste en un travail de validation et d'analyse des résultats produits par nos modèles neuronaux, en collaboration avec des linguistes historiques.
Les entités nommées ont été l'objet de nombreuses études durant les années 1990.
Il est par exemple intéressant de savoir qu'un texte contient des occurrences des mots « Google » et « Youtube » ; mais l'analyse devient plus intéressante si le système est capable de détecter une relation entre ces deux éléments, voire de les typer comme étant une relation d'achat (Google ayant racheté Youtube en 2006).
Notre contribution s'articule autour de deux grands axes : tracer un contour plus précis autour de la définition de la relation entre entités nommées, notamment au regard de la linguistique, et explorer des techniques pour l'élaboration de systèmes d'extraction automatique qui sollicitent des linguistes.
Leur rôle est fondamentale dans de nombreux cadres d'application comme la reconnaissance automatique de la parole, la traduction automatique, l'extraction et la recherche d'information.
Ce type de modèle prédit un mot uniquement en fonction des n-1 mots précédents.
Pourtant, cette approche est loin d'être satisfaisante puisque chaque mot est traité comme un symbole discret qui n'a pas de relation avec les autres.
Ainsi les spécificités du langage ne sont pas prises en compte explicitement et les propriétés morphologiques, sémantiques et syntaxiques des mots sont ignorées.
Sa construction repose sur le dénombrement de successions de mots, effectué sur des données d'entrainement.
Ce sont donc uniquement les textes d'apprentissage qui conditionnent la pertinence de la modélisation n-gramme, par leur quantité (plusieurs milliards de mots sont utilisés) et leur représentativité du contenu en terme de thématique, époque ou de genre.
Cette représentation continue confère aux modèles neuronaux une meilleure capacité de généralisation et leur utilisation a donné lieu à des améliorations significative en reconnaissance automatique de la parole et en traduction automatique.
Ainsi par le passé, les modèles neuronaux ont été utilisés soit pour des tâches avec peu de données d'apprentissage, soit avec un vocabulaire de mots à prédire limités en taille.
La première contribution de cette thèse est donc de proposer une solution qui s'appuie sur la structuration de la couche de sortie sous forme d'un arbre de classification pour résoudre ce problème de complexité.
Le modèle se nomme Structure OUtput Layer (SOUL) et allie une architecture neuronale avec les modèles de classes.
La deuxième contribution de cette thèse est d'analyser les représentations continues induites et de comparer ces modèles avec d'autres architectures comme les modèles récurrents.
Enfin, la troisième contribution est d'explorer la capacité de la structure SOUL à modéliser le processus de traduction.
Les résultats obtenus montrent que les modèles continus comme SOUL ouvrent des perspectives importantes de recherche en traduction automatique.
Cette thèse propose une étude des constructions Verbe causatif + Nom d'émotion (susciter l'étonnement, déclencher l'enthousiasme, attiser la jalousie) selon la méthodologie établie dans le cadre du projet ANR
Avec l'émergence et la prolifération des applications du Web sémantique, de nombreuses et récentes larges bases de connaissances (BC) sont disponibles sur le Web.
Ces BC contiennent des entités (nommées) et des faits sur ces entités.
Elles contiennent également les classes sémantiques de ces entités et leurs liens mutuels.
De plus, plusieurs BC peuvent être interconnectées au niveau entités, formant ainsi le noyau du Web des données liées (ou ouvertes).
Une caractérisation essentielle de ces BC est qu'elles contiennent des millions à des billions de triplets RDF incertains.
Les causes de cette incertitude sont diverses et multiples.
Elle peut résulter de l'intégration de sources de données de différents niveaux de fiabilité ou elle peut être causée par des considérations de préservation de la confidentialité.
Aussi, elle peut être due à des facteurs li´es au manque d'informations, à la limitation des équipements de mesures ou à l'évolution d'informations.
L'objectif de ce travail de thèse est d'améliorer l'ergonomie et la convivialité des systèmes modernes visant à exploiter des BC entachées d'incertitude.
En particulier, ce travail propose des techniques coopératives et intelligentes aidant l'utilisateur dans ses prises de décisions quand ses recherches retournent des résultats insatisfaisants en termes de quantité ou de fiabilité.
L'approche proposée pour le traitement de ce problème est guidée par la requête initiale et offre un double avantage : (i) elle permet de fournir une explication sur l'échec de la requête en identifiant les MFS (Minimal Failing Sub-queries) et, (ii) elle permet de calculer des requêtes alternatives appelées XSS (maXimal Succeeding Subqueries),sémantiquement proches de la requête initiale et dont les réponses sont non-vides.
Par ailleurs, d'un point de vue utilisateur, cette solution présente un niveau élevé de flexibilité dans le sens o`u plusieurs degrés d'incertitude peuvent être simultanément considérés.
L'ensemble de nos propositions ont été validées par une série d'expérimentations portant sur différentes larges bases de connaissances en présence d'incertitude (WatDiv et LUBM).
Nous avons aussi utilisé plusieurs Triplestores pour mener nos tests.
Les tables du Lexique-Grammaire, dont le développement a été initié par Gross (1975), constituent un lexique syntaxique très riche pour le français.
Elles couvrent diverses catégories lexicales telles que les verbes, les noms, les adjectifs et les adverbes.
Cette base de données linguistiques n'est cependant pas directement exploitable informatiquement car elle est incomplète et manque de cohérence.
Pour rendre ces tables exploitables, il faut expliciter les propriétés intervenant dans chacune d'entre elles.
De plus, un grand nombre de ces propriétés doivent être renommées dans un souci de cohérence.
Notre objectif est d'adapter les tables pour les rendre utilisables dans diverses applications de Traitement Automatique des Langues (TAL), notamment l'analyse syntaxique.
Nous expliquons les problèmes rencontrés et les méthodes adoptées pour permettre leur intégration dans un analyseur syntaxique.
Nous proposons LGExtract, un outil générique pour générer un lexique syntaxique pour le TAL à partir des tables du Lexique-Grammaire.
Il est relié à une table globale dans laquelle nous avons ajouté les propriétés manquantes et un unique script d'extraction incluant toutes les opérations liées à chaque propriété devant être effectuées pour toutes les tables.
Nous présentons également LGLex, le nouveau lexique syntaxique généré des verbes, des noms prédicatifs, des expressions figées et des adverbes.
Ensuite, nous montrons comment nous avons converti les verbes et les noms prédicatifs de ce lexique au format Alexina, qui est celui du lexique Lefff (Lexique des Formes Fléchies du Français) (Sagot, 2010), un lexique morphologique et syntaxique à large couverture et librement disponible pour le français.
Ceci permet son intégration dans l'analyseur syntaxique FRMG (French MetaGrammar) (Thomasset et de La Clergerie, 2005), un analyseur profond à large couverture pour le français, basé sur les grammaires d'arbres adjoints (TAG), reposant habituellement sur le Lefff.
Cette étape de conversion consiste à extraire l'information syntaxique codée dans les tables du Lexique-Grammaire.
Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu.
Nous évaluons l'analyseur syntaxique FRMG sur le corpus de référence de la campagne d'évaluation d'analyseurs du français Passage (Produire des Annotations Syntaxiques à Grande Échelle) (Hamon et al., 2008), en comparant sa version basée sur le Lefff avec notre version reposant sur les tables du Lexique-Grammaire converties.
Les systèmes de vidéosurveillance sont des outils importants pour les agences chargées de l'application de la loi dans la lutte contre la criminalité.
Les chambres de contrôle de la vidéosurveillance ont deux fonctions principales : surveiller en direct les zones de surveillance et résoudre les infractions en enquêtant les archives.
Pour soutenir ces tâches difficiles, plusieurs solutions significatives issues des domaines de la recherche et du marché ont été proposées.
Cependant, le manque de modèles génériques et précis pour la représentation du contenu vidéo fait de la construction d'un système intelligent et automatisé capable d'analyser et de décrire des vidéos une tâche ardue.
De plus, le domaine d'application montre toujours un écart important entre le domaine de la recherche et les besoins réels, ainsi qu'un manque entre ces besoins réels et les outils d'analyse vidéo dans le marché.
Par conséquence, jusqu'à présent dans les systèmes de surveillance conventionnels, la surveillance en direct et la recherche dans des archives reposent principalement sur des opérateurs humains.
Cette thèse propose une nouvelle approche pour la description textuelle de contenus importants dans des scènes de vidéosurveillance, basée sur une nouvelle "ontologie VSSD" générique, sans contexte, centrée sur les interactions entre deux objets.
L'ontologie proposée est générique, flexible et extensible, dédiée à la description de scènes de vidéosurveillance.
Tout en analysant les différentes scènes vidéo, notre approche introduit de nombreux nouveaux concepts et méthodes concernant la médiation et l'action distante, la description synthétique, ainsi qu'une nouvelle façon de segmenter la vidéo et de classer les scènes.
Nous introduisons une nouvelle méthode heuristique de distinction entre les objets déformables et non déformables dans les scènes.
Les systèmes de dialogue homme-machine ont pour objectif de permettre un échange oral efficace et convivial entre un utilisateur humain et un ordinateur.
Leurs domaines d'applications sont variés, depuis la gestion d'échanges commerciaux jusqu'au tutorat ou l'aide à la personne.
Cependant, les capacités de communication de ces systèmes sont actuellement limités par leur aptitude à comprendre la parole spontanée.
Nos travaux s'intéressent au module de compréhension de la parole et présentent une proposition entièrement basée sur des approches stochastiques, permettant l'élaboration d'une hypothèse sémantique complète.
Nous avons eu recours au formalisme FrameNet pour assurer une généricité maximale à notre représentation sémantique.
Le développement d'un système à base de règles et d'inférences logiques nous a ensuite permis d'annoter automatiquement le corpus.
Pour parvenir à la représentation sémantique globale complète, nous proposons et évaluons un algorithme de composition d'arbres décliné selon deux variantes.
Le module de compréhension construit au cours de ce travail peut être adapté au traitement de tout type de dialogue.
Il repose sur une représentation sémantique riche et les modèles utilisés permettent de fournir des listes d'hypothèses sémantiques scorées.
Le volume et la complexité des données générées par les systèmes d'information croissent de façon singulière dans les entrepôts de données.
Le domaine de l'informatique décisionnelle (aussi appelé BI) a pour objectif d'apporter des méthodes et des outils pour assister les utilisateurs dans leur tâche de recherche d'information.
est alors une tâche ardue, alors que les employés d'une entreprise cherchent généralement à réduire leur charge de travail.
Pour faire face à ce constat, le domaine « Enterprise Search » s'est développé récemment, et prend en compte les différentes sources de données appartenant aussi bien au réseau privé d'entreprise qu'au domaine public (telles que les pages Internet).
Pourtant, les utilisateurs de moteurs de recherche actuels souffrent toujours de du volume trop important d'information à disposition.
Nous pensons que de tels systèmes pourraient tirer parti des méthodes du traitement naturel des langues associées à celles des systèmes de questions/réponses.
En effet, les interfaces en langue naturelle permettent aux utilisateurs de rechercher de l'information en utilisant leurs propres termes, et d'obtenir des réponses concises et non une liste de documents dans laquelle l'éventuelle bonne réponse doit être identifiée.
Un challenge lors de la construction d'un tel système consiste à interagir avec les différentes applications, et donc avec les langages utilisés par ces applications d'une part, et d'être en mesure de s'adapter facilement à de nouveaux domaines d'application d'autre part.
Notre rapport détaille un système de questions/réponses configurable pour des cas d'utilisation d'entreprise, et le décrit dans son intégralité.
Dans les systèmes traditionnels de l'informatique décisionnelle, les préférences utilisateurs ne sont généralement pas prises en compte, ni d'ailleurs leurs situations ou leur contexte.
Les systèmes état-de-l'art du domaine tels que Soda ou Safe ne génèrent pas de résultats calculés à partir de l'analyse de la situation des utilisateurs.
Ce rapport introduit une approche plus personnalisée, qui convient mieux aux utilisateurs finaux.
Notre expérimentation principale se traduit par une interface de type search qui affiche les résultats dans un dashboard sous la forme de graphes, de tables de faits ou encore de miniatures de pages Internet.
En fonction des requêtes initiales des utilisateurs, des recommandations de requêtes sont aussi affichées en sus, et ce dans le but de réduire le temps de réponse global du système.
En ce sens, ces recommandations sont comparables à des prédictions.
Notre travail se traduit par les contributions suivantes : tout d'abord, une architecture implémentée via des algorithmes parallélisés et qui prend en compte la diversité des sources de données, à savoir des données structurées ou non structurées dans le cadre d'un framework de questions/réponses qui peut être facilement configuré dans des environnements différents.
De plus, une approche de traduction basée sur la résolution de contrainte, qui remplace le traditionnel langage-pivot par un modèle conceptuel et qui conduit à des requêtes multidimensionnelles mieux personnalisées.
En outre, en ensemble de patrons linguistiques utilisés pour traduire des questions BI en des requêtes pour bases de données, qui peuvent être facilement adaptés dans le cas de configurations différentes.
Cette thèse propose de s'intéresser au problème de la prédiction en apprentissage statistique sous contrainte de coût, notamment du coût de l'information utilisée par le système de prédiction.
Les approches classiques d'apprentissage statistique utilisent généralement le seul aspect de la performance en prédiction pour évaluer la qualité d'un modèle, ignorant le coût potentiel du modèle, par exemple en quantité de données utilisées en apprentissage (nombre d'exemples, nombre d'étiquette, mémoire) ou en inférence (quantité de features -ou caractéristiques-).
Nous développons trois modèles qui intègrent pendant l'apprentissage une notion du coût de l'information utilisée pour la prédiction, avec pour objectif de contraindre le coût de la prédiction en inférence.
Nous utilisons des méthodes d'apprentissage de représentations avec des architectures type réseau de neurones récurrents et des algorithmes par descente de gradient pour l'apprentissage.
La dernière partie du manuscrit s'intéresse au coût lié aux étiquettes, usuellement dénommé apprentissage actif dans la littérature.
Nous présentons nos travaux pour une approche nouvelle de ce problème en utilisant le méta-apprentissage ainsi qu'une première instanciation basée sur des réseaux récurrents bi-directionnels.
La vidéosurveillance représente l'un des domaines de recherche privilégiés en vision par ordinateur.
Le défi scientifique dans ce domaine comprend la mise en œuvre de systèmes automatiques pour obtenir des informations détaillées sur le comportement des individus et des groupes.
En particulier, la détection de mouvements anormaux de groupes d'individus nécessite une analyse fine des frames du flux vidéo.
Dans le cadre de cette thèse, la détection de mouvements anormaux est basée sur la conception d'un descripteur d'image efficace ainsi que des méthodes de classification non linéaires.
Nous proposons trois caractéristiques pour construire le descripteur de mouvement : (i) le flux optique global, (ii) les histogrammes de l'orientation du flux optique (HOFO) et (iii) le descripteur de covariance (COV) fusionnant le flux optique et d'autres caractéristiques spatiales de l'image.
Sur la base de ces descripteurs, des algorithmes de machine learning (machines à vecteurs de support (SVM)) mono-classe sont utilisés pour détecter des événements anormaux.
Deux stratégies en ligne de SVM mono-classe sont proposées : la première est basée sur le SVDD (online SVDD) et la deuxième est basée sur une version « moindres carrés » des algorithmes SVM (online LS-OC-SVM)
La notion de métrique joue un rôle clef dans les problèmes d'apprentissage automatique tels que la classification, le clustering et le ranking.
L'apprentissage à partir de données de métriques adaptées à une tâche spécifique a suscité un intérêt croissant ces dernières années.
Ce domaine vise généralement à trouver les meilleurs paramètres pour une métrique donnée sous certaines contraintes imposées par les données.
La métrique apprise est utilisée dans un algorithme d'apprentissage automatique dans le but d'améliorer sa performance.
La plupart des méthodes d'apprentissage de métriques optimisent les paramètres d'une distance de Mahalanobis pour des vecteurs de features.
Les méthodes actuelles de l'état de l'art arrivent à traiter des jeux de données de tailles significatives.
En revanche, le sujet plus complexe des séries temporelles multivariées n'a reçu qu'une attention limitée, malgré l'omniprésence de ce type de données dans les applications réelles.
Une importante partie de la recherche sur les séries temporelles est basée sur la dynamic time warping (DTW), qui détermine l'alignement optimal entre deux séries temporelles.
L'état actuel de l'apprentissage de métriques souffre de certaines limitations.
La plus importante est probablement le manque de garanties théoriques concernant la métrique apprise et sa performance pour la classification.
La théorie des fonctions de similarité (ℰ, ϓ, T)-bonnes a été l'un des premiers résultats liant les propriétés d'une similarité à celles du classifieur qui l'utilise.
Une deuxième limitation vient du fait que la plupart des méthodes imposent des propriétés de distance, qui sont coûteuses en terme de calcul et souvent non justifiées.
Dans cette thèse, nous abordons les limitations précédentes à travers deux contributions principales.
La première est un nouveau cadre général pour l'apprentissage conjoint d'une fonction de similarité et d'un classifieur linéaire.
Cette formulation est inspirée de la théorie de similarités (ℰ, ϓ, τ)-bonnes, fournissant un lien entre la similarité et le classifieur linéaire.
Elle est convexe pour une large gamme de fonctions de similarité et de régulariseurs.
Nous dérivons deux bornes de généralisation équivalentes à travers les cadres de robustesse algorithmique et de convergence uniforme basée sur la complexité de Rademacher, prouvant les propriétés théoriques de notre formulation.
Notre deuxième contribution est une méthode d'apprentissage de similarités basée sur DTW pour la classification de séries temporelles multivariées.
Le problème est convexe et utilise la théorie des fonctions (ℰ, ϓ, T)-bonnes liant la performance de la métrique à celle du classifieur linéaire associé.
A l'aide de la stabilité uniforme, nous prouvons la consistance de la similarité apprise conduisant à la dérivation d'une borne de généralisation.
L'objectif de cette recherche est d'étudier un phénomène linguistique qui s'est développé en Tunisie dans le cyber-territoire des réseaux sociaux.
Cette étude a deux objectifs principaux :
(1) examiner la langue de la jeunesse tunisienne de la capitale, et en particulier sa réalisation en arabish/arabizi (caractères latines).
(2) Construire un corpus d'arabish tunisien annoté avec plusieurs niveaux d'annotation (Translittération, Partie du discours), en utilisant des techniques TAL.
La grammaire électronique est une des ressources les plus importantes pour le traitement automatique des langues naturelles.
Parce que le développement manuel d'une grammaire est une tâche coûteuse, beaucoup d'efforts pour le développement automatique de grammaires ont été fournis pendant la décennie dernière.
Le développement automatique d'une grammaire signifie qu'un système extrait une grammaire à partir d'un corpus arboré.
Les étiquettes syntaxiques et morphologiques du corpus nous permettent d'extraire les traits syntaxiques automatiquement.
Pendant les expériences d'extraction, nous modifions le corpus pour améliorer les grammaires extraites et extrayons cinq types de grammaires, donc quatre grammaires lexicalisées et une grammaire lexicalisée avec traits.
Les grammaires extraites sont évaluées par la taille, la couverture et l'ambiguïté moyenne.
La croissance du nombre de schémas d'arbres n'est pas stabilisée à l'issue de l'extraction, ce qui semble indiquer que la taille du corpus n'es pas suffisante pour atteindre la convergence des grammaires.
Cependant le nombre de schémas apparaissant au moins deux fois dans le corpus est quasiment stabilisé à l'issue de l'extraction et le nombre de schémas des grammaires supérieures (celles qui sont extraites après la modification du corpus) est aussi plus stabilisé que les grammaires inférieurs.
Nous évaluons notre programme d'extraction en l'appliquant à un autre corpus arboré.
Enfin, nous comparons nos grammaires avec celle de Han et al. (2001) écrite à la main.
Depuis quelques décennies, de nombreux scientifiques alertent au sujet de la disparition des langues qui ne cesse de s'accélérer.
Face au déclin alarmant du patrimoine linguistique mondial, il est urgent d'agir afin de permettre aux linguistes de terrain, a minima, de documenter les langues en leur fournissant des outils de collecte innovants et, si possible, de leur permettre de décrire ces langues grâce au traitement des données assisté par ordinateur.
C'est ce que propose ce travail, en se concentrant sur trois axes majeurs du métier de linguiste de terrain : la collecte, la transcription et l'analyse.
Les enregistrements audio sont primordiaux, puisqu'ils constituent le matériau source, le point de départ du travail de description.
De plus, tel un instantané, ils représentent un objet précieux pour la documentation de la langue.
Les fonctionnalités implémentées permettent d'enregistrer différents types de discours (parole spontanée, parole élicitée, parole lue) et de partager les enregistrements avec les locuteurs.
L'application permet, en outre, la construction de corpus alignés " parole source (peu dotée) - parole cible (bien dotée) ", " parole-image", " parole-vidéo " qui présentent un intérêt fort pour les technologies de la parole, notamment pour l'apprentissage non supervisé.
Afin de compléter l'aide apportée aux linguistes, nous proposons d'utiliser des techniques de traitement automatique de la langue pour lui permettre de tirer partie de la totalité de ses données collectées.
Parmi celles-ci, la RAP peut être utilisée pour produire des transcriptions, d'une qualité satisfaisante, de ses enregistrements.
Une fois les transcriptions obtenues, le linguiste peut s'adonner à l'analyse de ses données.
Afin qu'il puisse procéder à l'étude de l'ensemble de ses corpus, nous considérons l'usage des méthodes d'alignement forcé.
Nous démontrons que de telles techniques peuvent conduire à des analyses linguistiques fines.
En retour, nous montrons que la modélisation de ces observations peut mener à des améliorations des systèmes de RAP.
L'asthme résulte de multiples facteurs génétiques et environnementaux et des interactions entre ces facteurs.
L'objectif de cette thèse a été de proposer des stratégies d'analyses d'interactions gène-gène et gène-environnement pour identifier de nouveaux gènes associés à l'asthme et l'atopie.
Les tests d'interactions entre variants génétiques sont appliqués aux paires de gènes sélectionnées.
Ces analyses, menées dans trois études familiales (n=3244), ont permis d'identifier une interaction entre deux gènes (ADGRV1 et DNAH5) impliqués dans la mobilité ciliaire, mécanisme émergent dans l'asthme.
Une méta-analyse de GWAS du délai de survenue de l'asthme, menée dans neuf études (n=19348), a permis d'identifier un nouveau locus (16q12) et d'en confirmer quatre autres.
Cinq de ces études comportaient des données sur l'exposition au tabagisme passif pendant la petite enfance (ELTS) (n=8273).
Face à la quantité d'information textuelle disponible sur le web en langue arabe, le développement des Systèmes de Recherche d'Information (SRI) efficaces est devenu incontournable pour retrouver l'information pertinente.
La plupart des SRIs actuels de la langue arabe reposent sur la représentation par sac de mots et l'indexation des documents et des requêtes est effectuée souvent par des mots bruts ou des racines.
Nous apportons quatre contributions au niveau de processus de représentation, d'indexation et de recherche d'information en langue arabe.
La première contribution consiste à représenter les documents à la fois par des termes simples et des termes complexes.
Cela est justifié par le fait que les termes simples seuls et isolés de leur contexte sont ambigus et moins précis pour représenter le contenu des documents.
Ainsi, nous avons proposé une méthode hybride pour l'extraction de termes complexes en langue arabe, en combinant des propriétés linguistiques et des modèles statistiques.
Le filtre linguistique repose à la fois sur l'étiquetage morphosyntaxique et la prise en compte des variations pour sélectionner les termes candidats.
Pour sectionner les termes candidats pertinents, nous avons introduit une mesure d'association permettant de combiner l'information contextuelle avec les degrés de spécificité et d'unité.
Par conséquent, nous étudions plusieurs extensions des modèles existants de RI pour l'intégration des termes complexes.
En outre, nous explorons une panoplie de modèles de proximité.
Pour la prise en compte des dépendances de termes dans les modèles de RI, nous introduisons une condition caractérisant de tels modèle et leur validation théorique.
La troisième contribution permet de pallier le problème de disparité des termes en proposant une méthode pour intégrer la similarité entre les termes dans les modèles de RI en s'appuyant sur les représentations distribuées des mots (RDMs).
L'idée sous-jacente consiste à permettre aux termes similaires à ceux de la requête de contribuer aux scores des documents.
La dernière contribution concerne l'amélioration des modèles de rétro-pertinence (Pseudo Relevance Feedback PRF).
Étant basée également sur les RDM, notre méthode permet d'intégrer la similarité entre les termes d'expansions et ceux de la requête dans les modèles standards PRF.
La validation expérimentale de l'ensemble des contributions apportées dans le cadre de cette thèse est effectuée en utilisant la collection standard TREC 2002/2001 de la langue arabe.
Les modèles graphiques probabilistes codent les dépendances cachées entre les variables aléatoires pour la modélisation des données.
L'estimation des paramètres est une partie cruciale et nécessaire du traitement de ces modèles probabilistes.
Ces modèles très généraux ont été utilisés dans de nombreux domaines tels que la vision par ordinateur, le traitement du signal, le traitement du langage naturel et bien d'autres.
Nous nous sommes surtout concentrés sur les modèles log-supermodulaires, qui constituent une partie spécifique des distributions familiales exponentielles, où la fonction potentielle est supposée être négative d'une fonction sous-modulaire.
Cette propriété sera très pratique pour le maximum d'estimations a posteriori et d'apprentissage des paramètres.
Malgré la restriction apparente des modèles d'intérêt, ils couvrent une grande partie des familles exponentielles, puisqu'il y a beaucoup de fonctions qui sont sous-modulaires, par exemple, les coupes graphiques, l'entropie et autres.
Il est bien connu que le traitement probabiliste est un défi pour la plupart des modèles, mais nous avons été en mesure de relever certains des défis au moins approximativement.
Dans ce manuscrit, nous exploitons les idées perturb-and-MAP pour l'approximation de la fonction de partition et donc un apprentissage efficace des paramètres.
De plus, le problème peut également être interprété comme une tâche d'apprentissage de structure, où chaque paramètre ou poids estimé représente l'importance du terme correspondant.
Nous proposons une méthode d'estimation et d'inférence approximative des paramètres pour les modèles où l'apprentissage et l'inférence exacts sont insolubles dans le cas général en raison de la complexité du calcul des fonctions de partition.
La première partie de la thèse est consacrée aux garanties théoriques.
Étant donné les modèles log-supermodulaires, nous tirons parti de la propriété de minimisation efficace liée à la sous-modularité.
En introduisant et en comparant deux limites supérieures existantes de la fonction de partition, nous sommes en mesure de démontrer leur relation en prouvant un résultat théorique.
Nous introduisons une approche pour les données manquantes comme sous-routine naturelle de la modélisation probabiliste.
Il semble que nous puissions appliquer une technique stochastique à l'approche d'approximation par perturbation et carte proposée tout en maintenant la convergence tout en la rendant plus rapide dans la pratique.
La deuxième contribution principale de cette thèse est une généralisation efficace et évolutive de l'approche de l'apprentissage paramétrique.
Dans cette section, nous développons de nouveaux algorithmes pour effectuer l'estimation des paramètres pour diverses fonctions de perte, différents niveaux de supervision et nous travaillons également sur l'évolutivité.
En particulier, en travaillant principalement avec des coupes graphiques, nous avons pu intégrer différentes techniques d'accélération.
Nous traitons d'un problème général d'apprentissage des signaux continus.
Dans cette partie, nous nous concentrons sur les représentations de modèles graphiques clairsemés.
Nous utilisons des régularisateurs à faible densité commune comme potentiels basés sur les antécédents.
Les techniques de débruitage proposées ne nécessitent pas le choix d'un redresseur précis à l'avance.
Pour effectuer l'apprentissage de la représentation clairsemée, la communauté utilise souvent les pertes symétriques comme l1, mais nous proposons de paramétrer la perte et d'apprendre le poids de chaque composante de perte à partir des données.
C'est possible grâce à l'approche que nous avons proposée dans les sections précédentes.
Pour tous les aspects de l'estimation des paramètres mentionnés ci-dessus, nous avons effectué les calculs suivants des expériences nationales visant à approuver l'idée ou à la comparer à des repères existants, et à démontrer sa performance dans la pratique.
Les trois principaux résultats sont les suivants.
Tout d'abord, nous étudions les propriétés structurelles des graphes obtenus à partir de bases de connaissances exprimées avec des règles existentielles et nous donnons plusieurs indications sur la manière dont leur génération peut être améliorée.
Deuxièmement, nous proposons une technique pour générer un graphe d'argumentation où plusieurs arguments peuvent attaquer collectivement, remplaçant ainsi la relation d'attaque binaire classique et montrons expérimentalement les avantages de cette technique.
Troisièmement, nous nous intéressons aux approches fondées sur les classements pour le raisonnement en logique et en argumentation.
L'expansion de la radio et le développement de nouveaux standards enrichissent la diversité et la quantité de données contenues sur les ondes de radiodiffusion.
Il devient alors judicieux de développer un moteur de recherches qui aurait la capacité de rendre toutes ces données accessibles comme le font les moteurs de recherche sur internet à l'image de Google.
Les possibilités offertes par un tel moteur s'il existe sont nombreuses.
Ainsi, le projet SurfOnHertz, qui a été lancé en 2010 et s'est terminé en 2013, avait pour but de mettre au point un navigateur qui serait capable d'indexer les flux audios de toutes les stations radios.
Cette indexation se traduirait, entre autres, par de la détection de mots clés dans les flux audios, la détection de publicités, la classification de genres musicaux.
Le navigateur une fois mis au point deviendrait le premier moteur de recherches de genre à traiter les contenus radiodiffusés.
Relever un tel challenge nécessite d'avoir un dispositif pour capter toutes les stations en cours de diffusion dans la zone géographique concernée, les démoduler et transmettre les contenus audios à un moteur d'indexation.
Ainsi, les travaux de cette thèse visent à proposer des architectures numériques portées sur une plateforme SDR pour extraire, démoduler, et mettre à disposition le contenu audio de chacune des stations diffusées dans la zone géographique du récepteur.
Vu le grand nombre de standards radio existants aujourd'hui, la thèse porte principalement les standards FM et DRM30.
Cependant les méthodologies proposées sont extensibles à d'autres standards.
Le choix de ce type de comcomposant est justifié de par les grandes possibilités qu'il offre en termes de parallélisme de traitements, de maitrise de ressources disponibles, et d'embarquabilité.
Le développement des algorithmes a été fait dans un souci de minimisation de la quantité de blocs de calculs utilisés.
D'ailleurs, bon nombre d'implémentations ont été réalisées sur un Stratix II, technologie aux ressources limitées par rapport aux FPGAs d'aujourd'hui disponibles sur le marché.
Cela atteste la viabilité des algorithmes présentés.
Les algorithmes proposés opèrent ainsi l'extraction simultanée de tous les canaux radios lorsque les stations ne peuvent occuper que des emplacements uniformément espacés comme la FM en Europe occidentale, et aussi, pour des standards dont la répartition des stations dans le spectre semble plutôt aléatoire comme le DRM30.
Une autre partie des discussions porte sur le moyen de les démoduler simultanément.
L'étudiant pourra s'inspirer des 'grounds'de Prawitz (2009), la logique dialogique de Lorenzen (1961) ou la ludique de Girard (2001) et de la 'proof theoretical semantics'de Francez (2015).
Cela permet de distinguer des expression de sens voisin comme 'un peu'/ 'peu'ou 'chaque'/ 'tout'.
La méthode d'analyse devra s'intégrer à la plateforme Grail d'analyse logique et grammaticale du français à large échelle.
Cette approche et ces outils est particulièrement pertinente pour l'analyse automatique d'une argumentation, d'un dialogue argumentatif ou d'un débats.
Dans cette thèse, nous proposons une nouvelle approche WCUM (Web Content and Usage Mining based approach) permettant de relier l'analyse du contenu à l'analyse de l'usage d'un site Web afin de mieux comprendre le comportement général des visiteurs du site.
Afin de pallier le problème de détermination du nombre de classes sur les lignes et les colonnes, nous proposons de généraliser certains indices proposés initialement pour évaluer les partitions obtenues par des algorithmes de classification simple, aux algorithmes de classification simultanée.
Pour évaluer la performance de ces indices nous proposons un algorithme de génération de biclasses artificielles pour effectuer des simulations et valider les résultats.
Des expérimentations sur des données artificielles ainsi qu'une application sur des données réelles ont été réalisées pour évaluer l'efficacité de l'approche proposée.
L'extraction de sous-structures significatives a toujours été un élément clé de l'étude des graphes.
Dans le cadre de l'apprentissage automatique, supervisé ou non, ainsi que dans l'analyse théorique des graphes, trouver des décompositions spécifiques et des sous-graphes denses est primordial dans de nombreuses applications comme entre autres la biologie ou les réseaux sociaux.
Dans cette thèse, nous cherchons à étudier la dégénérescence de graphe, en partant d'un point de vue théorique, et en nous appuyant sur nos résultats pour trouver les décompositions les plus adaptées aux tâches à accomplir.
C'est pourquoi, dans la première partie de la thèse, nous travaillons sur des résultats structurels des graphes à arête-admissibilité bornée, prouvant que de tels graphes peuvent être reconstruits en agrégeant des graphes à degré d'arête quasi-borné.
Nous fournissons également des garanties de complexité de calcul pour les différentes décompositions de la dégénérescence, c'est-à-dire si elles sont NP-complètes ou polynomiales, selon la longueur des chemins sur lesquels la dégénérescence donnée est définie.
Dans la deuxième partie, nous unifions les cadres de dégénérescence et d'admissibilité en fonction du degré et de la connectivité.
Dans ces cadres, nous choisissons les plus expressifs, d'une part, et les plus efficaces en termes de calcul d'autre part, à savoir la dégénérescence 1-arête-connectivité pour expérimenter des tâches de dégénérescence standard, telle que la recherche d'influenceurs.
Suite aux résultats précédents qui se sont avérés peu performants, nous revenons à l'utilisation du k-core mais en l'intégrant dans un cadre supervisé, i.e. les noyaux de graphes.
Ainsi, en fournissant un cadre général appelé core-kernel, nous utilisons la décomposition k-core comme étape de prétraitement pour le noyau et appliquons ce dernier sur chaque sous-graphe obtenu par la décomposition pour comparaison.
Nous sommes en mesure d'obtenir des performances à l'état de l'art sur la classification des graphes au prix d'une légère augmentation du coût de calcul.
Enfin, nous concevons un nouveau cadre de dégénérescence de degré s'appliquant simultanément pour les hypergraphes et les graphes biparties, dans la mesure où ces derniers sont les graphes d'incidence des hypergraphes.
Cette décomposition est ensuite appliquée directement à des architectures de réseaux de neurones pré-entrainés étant donné qu'elles induisent des graphes biparties et utilisent le core d'appartenance des neurones pour réinitialiser les poids du réseaux.
Cette méthode est non seulement plus performant que les techniques d'initialisation de l'état de l'art, mais il est également applicable à toute paire de couches de convolution et linéaires, et donc adaptable à tout type d'architecture.
Le corpus est la matière première de la linguistique informatique et du traitement automatique du langage.
Or ces ressources contiennent beaucoup de bruit (menus, publicités, etc.).
Le filtrage des données parasites et des répétitions nécessite un nettoyage à grand échelle que les chercheurs font en général à la main.
Cette thèse propose un système automatique de constitution de corpus web nettoyés de leur bruit.
Le système est évalué sous l'angle de l'efficacité de la suppression du bruit et du temps d'exécution.
Nos expérimentations, faites sur quatre langues, sont évaluées à l'aide de notre propre corpus de référence.
Nous comparons notre méthode avec trois méthodes traitant du même problème que la nôtre, Nutch, BootCat et JusText.
Les performances de notre système sont meilleures pour la qualité d'extraction, même si pour le temps de calcul, Nutch et BootCat dominent.
La modélisation informatique et la simulation sont des activités de plus en plus répandues lors de la conception de systèmes complexes et critiques tels que ceux embarqués dans les avions.
Une proposition pour la conception et réalisation d'abstractions compatibles avec les objectifs de simulation est présentée basés sur la théorie de l'informatique, le contrôle et le système des concepts d'ingénierie.
Il adresse deux problèmes fondamentaux de fidélité dans la simulation, c'est-à-dire, pour une spécification du système et quelques propriétés d'intérêt, comment extraire des abstractions pour définir une architecture de produit de simulation et jusqu'où quel point le comportement du modèle de simulation représente la spécification du système.
Une notion générale de cette fidélité de la simulation, tant architecturale et comportementale, est expliquée dans les notions du cadre expérimental et discuté dans le contexte des abstractions de modélisation et des relations d'inclusion.
Une approche semi-formelle basée sur l'ontologie pour construire et définir l'architecture de produit de simulation est proposée et démontrée sur une étude d'échelle industrielle.
Une approche formelle basée sur le jeu théorique et méthode formelle est proposée pour différentes classes de modèles des systèmes et des simulations avec un développement d'outils de prototype et cas des études.
Les problèmes dans la recherche et implémentation de ce cadre de fidélité sont discutées particulièrement dans un contexte industriel.
Un système informatique ne peut traiter un texte sans que certaines informations, comme les mots ou les phrases, ne soient annotées.
Or, à ce jour, aucun système ne réalise automatiquement une annotation parfaite d'un texte.
Ce constat fait, une question s'impose : quel système de traitement automatique des langues obtient les meilleures performances, un système qui intégre l'imperfection des annotations dans son processus de raisonnement ou un système prevu pour raisonner à partir d'annotations parfaites mais travaillant avec des annotations imparfaites ?
Pour y répondre nous avons proposé un modèle d'inférence probabiliste reposant sur les réseaux bayésiens (RB), un formalisme adapté pour travailler sur des données imparfaites.
Nous avons travaillé sur le problème de la résolution du pronom "it" anaphorique dans les textes anglais et validé notre modèle en évaluant deux RB sur des corpus différents : un RB pour la reconnaissances des pronoms impersonnels et un RB pour le choix de l'antécédant.
Stimulée par l'usage intensif des téléphones mobiles, l'exploitation conjointe des données textuelles et des données spatiales présentes dans les objets géotextuels (p. ex. tweets, photos Flickr, critiques de points d'intérêt)
Cependant, ces approches traditionnelles se sont révélées peu efficaces face aux textes issus des réseaux sociaux.
En effet, ces derniers sont généralement de courte longueur, utilisent des mots non conventionnels ou ambiguës et peuvent difficilement être mis en correspondance avec d'autres documents, notamment à cause de l'inadéquation du vocabulaire.
De fait, les approches proposées jusqu'à présent conduisent généralement à de faibles taux de rappel et de précision.
Nous proposons ainsi de tirer parti des contextes géographiques et de la sémantique distributionnelle pour résoudre la tâche de prédiction sémantique de l'emplacement.
Concernant l'amélioration des représentations de textes, nous proposons une approche de régularisation a posteriori qui intègre l'information spatiale dans l'apprentissage des plongements lexicaux.
L'objectif sous-jacent est de révéler d'éventuelles relations sémantiques locales entre les mots, ainsi que la multiplicité des sens d'un même mot.
Cette thèse en informatique s'intéresse à la structuration et à l'exploration de collections journalistiques.
Elle fait appel à plusieurs domaines de recherche : sciences sociales, à travers l'étude de la production journalistique ; ergonomie ;
L'hyperliage consiste à construire automatiquement des liens entre documents multimédias.
Nous étendons ce concept en l'appliquant à l'entièreté d'une collection afin d'obtenir un hypergraphe, et nous intéressons notamment à ses caractéristiques topologiques et à leurs conséquences sur l'explorabilité de la structure construite.
Nous proposons dans cette thèse des améliorations de l'état de l'art selon trois axes principaux : une structuration de collections d'actualités à l'aide de graphes mutli-sources et multimodaux fondée sur la création de liens inter-documents, son association à une diversité importante des liens permettant de représenter la grande variété des intérêts que peuvent avoir différents utilisateurs, et enfin l'ajout d'un typage des liens créés permettant d'expliciter la relation existant entre deux documents.
Ces différents apports sont renforcés par des études utilisateurs démontrant leurs intérêts respectifs.
Le Web est en croissance continue, et une quantité énorme de données est générée par les réseaux sociaux, permettant aux utilisateurs d'échanger une grande diversité d'informations.
En outre, les textes au sein des réseaux sociaux sont souvent subjectifs.
L'exploitation de cette subjectivité présente au sein des textes peut être un facteur important lors d'une recherche d'information.
En particulier, cette thèse est réalisée pour répondre aux besoins de la plate-forme Books de Open Edition en matière d'amélioration de la recherche et la recommandation de livres, en plusieurs langues.
La plateforme offre des informations générées par des utilisateurs, riches en sentiments.
Par conséquent, l'analyse précédente, concernant l'exploitation de sentiment en recherche d'information, joue un rôle important dans cette thèse et peut servir l'objectif d'une amélioration de qualité de la recherche de livres en utilisant les informations générées par les utilisateurs.
Par conséquent, nous avons choisi de suivre une voie principale dans cette thèse consistant à combiner les domaines analyse de sentiment (AS) et recherche d'information (RI), dans le but d'améliorer les suggestions de la recherche de livres.
Nos objectifs peuvent être résumés en plusieurs points :
• Une approche d'analyse de sentiment, facilement applicable sur différentes langues, peu coûteuse en temps et en données annotées.
• De nouvelles approches pour l'amélioration de la qualité lors de la recherche de livres, basées sur l'utilisation de l'analyse de sentiment dans le filtrage, l'extraction et la classification des informations
Les langues en Malaisie meurent à un rythme alarmant.
A l'heure actuelle, 15 langues sont en danger alors que deux langues se sont éteintes récemment.
Une des méthodes pour sauvegarder les langues est de les documenter, mais c'est une tâche fastidieuse lorsque celle-ci est effectuée manuellement.
Un système de reconnaissance automatique de la parole (RAP) serait utile pour accélérer le processus de documentation de ressources orales.
Cependant, la construction des systèmes de RAP pour une langue cible nécessite une grande quantité de données d'apprentissage comme le suggèrent les techniques actuelles de l'état de l'art, fondées sur des approches empiriques.
L'objectif principal de cette thèse est d'étudier les effets de l'utilisation de données de langues étroitement liées, pour construire un système de RAP pour les langues à faibles ressources en Malaisie.
Des études antérieures ont montré que les méthodes inter-lingues et multilingues pourraient améliorer les performances des systèmes de RAP à faibles ressources.
Dans cette thèse, nous essayons de répondre à plusieurs questions concernant ces approches : comment savons-nous si une langue est utile ou non dans un processus d'apprentissage trans-lingue ?
Comment la relation entre la langue source et la langue cible influence les performances de la reconnaissance de la parole ?
Nous étudions les effets de l'utilisation des données du malais, une langue locale dominante qui est proche de l'iban, pour développer un système de RAP pour l'iban, sous différentes contraintes de ressources.
Nous proposons plusieurs approches pour adapter les données du malais afin obtenir des modèles de prononciation et des modèles acoustiques pour l'iban.
Celui-ci est fondé sur des techniques d'amorçage, pour améliorer la correspondance entre les données du malais et de l'iban.
Pour augmenter la performance des modèles acoustiques à faibles ressources, nous avons exploré deux techniques de modélisation : les modèles de mélanges gaussiens à sous-espaces (SGMM) et les réseaux de neurones profonds (DNN).
Les résultats montrent que l'utilisation de données du malais est bénéfique pour augmenter les performances des systèmes de RAP de l'iban.
Par ailleurs, nous avons également adapté les modèles SGMM et DNN au cas spécifique de la transcription automatique de la parole non native (très présente en Malaisie).
Nous avons proposé une approche fine de fusion pour obtenir un SGMM multi-accent optimal.
En outre, nous avons développé un modèle DNN spécifique pour la parole accentuée.
Les deux approches permettent des améliorations significatives de la précision du système de RAP.
De notre étude, nous observons que les modèles SGMM et, de façon plus surprenante, les modèles DNN sont très performants sur des jeux de données d'apprentissage en quantité limités.
Ce travail de thèse prend pour objet d'étude les concepts de référence et de trouble de la référence au regard de l'aspect dynamique, interlocutoire, du processus de co-construction dialogique de la référence.
Partant d'une revue critique de la littérature concernant l'abord théorico-méthodologique du processus de co-construction dialogique de la référence, notre première étude se propose de réinterroger l'abord actuel de ce phénomène au regard des aspects structurels, actionnels et représentationnels des interactions verbales.
Nos résultats aboutissent à l'établissement d'un nouveau modèle, qualitativement plus sensible.
Son potentiel heuristique est d'ailleurs éprouvé au sein de travaux complémentaires, prenant désormais pour objet d'étude le trouble de la référence tel qu'il est susceptible de s'exprimer en interaction verbale schizophrénique.
Les résultats obtenus aux travers de deux études fournissent des critères permettant la caractérisation dynamique des contextes d'émergence et d'expression du trouble de la référence.
Ils sont par ailleurs en amont d'une interprétation des processus cognitifs impliqués au regard de la théorie de la modularité massive.
Par ailleurs, les résultats fournis par une étude supplémentaire font état d'une relative indépendance du trouble quant à l'intensité symptomatologique de la pathologie schizophrénique.
Pointant l'intérêt d'un abord dynamique des troubles, la contribution de nos résultats est essentiellement à concevoir au regard des objectifs de l'approche pragmatique en psychopathologie, offrant des pistes de recherche si bien dans le cadre du renouvellement des classifications que dans celui d'une clinique de l'efficience cognitive.
En 2015, le nombre de nouveaux cas de cancer du sein en France s'élève à 54 000.
Le taux de survie 5 ans après le diagnostic est de 89 %.
Si les traitements modernes permettent de sauver des vies, certains sont difficiles à supporter.
De nombreux projets de recherche clinique se sont donc focalisés sur la qualité de vie (QdV) qui fait référence à la perception que les patients ont de leurs maladies et de leurs traitements.
La QdV est un critère d'évaluation clinique pertinent pour évaluer les avantages et les inconvénients des traitements que ce soit pour le patient ou pour le système de santé.
Dans cette thèse, nous nous intéresserons aux histoires racontées par les patients dans les médias sociaux à propos de leur santé, pour mieux comprendre leur perception de la QdV.
Ce nouveau mode de communication est très prisé des patients car associé à une grande liberté du discours due notamment à l'anonymat fourni par ces sites.
L'originalité de cette thèse est d'utiliser et d'étendre des méthodes de fouille de données issues des médias sociaux pour la langue Française.
Les contributions de ce travail sont les suivantes : (1) construction d'un vocabulaire patient/médecin ; (2) détection des thèmes discutés par les patients ; (3) analyse des sentiments des messages postés par les patients et (4) mise en relation des différentes contributions citées.
Dans un premier temps, nous avons utilisé les textes des patients pour construire un vocabulaire patient/médecin spécifique au domaine du cancer du sein, en recueillant divers types d'expressions non-expertes liées à la maladie, puis en les liant à des termes biomédicaux utilisés par les professionnels de la santé.
Nous avons combiné plusieurs méthodes de la littérature basées sur des approches linguistiques et statistiques.
Pour évaluer les relations obtenues, nous utilisons des validations automatiques et manuelles.
Nous avons ensuite transformé la ressource construite dans un format lisible par l'être humain et par l'ordinateur en créant une ontologie SKOS, laquelle a été intégrée dans la plateforme BioPortal.
Dans un deuxième temps, nous avons utilisé et étendu des méthodes de la littérature afin de détecter les différents thèmes discutés par les patients dans les médias sociaux et de les relier aux dimensions fonctionnelles et symptomatiques des auto-questionnaires de QdV (EORTC QLQ-C30 et EORTC QLQ-BR23).
Afin de détecter les thèmes, nous avons appliqué le modèle d'apprentissage non supervisé LDA avec des prétraitements pertinents.
Ensuite, nous avons proposé une méthode permettant de calculer automatiquement la similarité entre les thèmes détectés et les items des auto-questionnaires de QdV.
Nous avons ainsi déterminé de nouveaux thèmes complémentaires à ceux déjà présents dans les questionnaires.
Ce travail a ainsi mis en évidence que les données provenant des forums de santé sont susceptibles d'être utilisées pour mener une étude complémentaire de la QdV.
Dans un troisième temps, nous nous sommes focalisés sur l'extraction de sentiments (polarité et émotions).
Pour cela, nous avons évalué différentes méthodes et ressources pour la classification de sentiments en Français.
Ces expérimentations ont permis de déterminer les caractéristiques utiles dans la classification de sentiments pour différents types de textes, y compris les textes provenant des forums de santé.
Finalement, nous avons utilisé les différentes méthodes proposées dans cette thèse pour quantifier les thèmes et les sentiments identifiés dans les médias sociaux de santé.
De manière générale, ces travaux ont ouvert des perspectives prometteuses sur diverses tâches d'analyse des médias sociaux pour la langue française et en particulier pour étudier la QdV des patients à partir des forums de santé.
Le projet de thèse porte sur l'application des approches neuronales pour la représentation de textes et l'appariement de textes en recherche d'information en vue de lever le verrou du fossé sémantique.
Notre première contribution comprend des modèles neuronaux pour l'apprentissage en ligne et apprentissage hors ligne des représentations de texte à plusieurs niveaux (mot, sens, document).
Ces modèles intègrent les contraintes relationnelles issues des ressources externes par régularisation de la fonction objectif ou par enrichissement sémantique des instances d'apprentissage.
Ce réseau apprend à mesurer un score de pertinence entre un document et une requête à partir des vecteurs de représentation en entrée modélisant des objets (concepts, entités) identifiés dans la requêtes et documents et leurs relations issues des ressources externes.
Le développement et la multiplication de dispositifs connectés, en particulier avec les \textit{smartphones}, nécessitent la mise en place de moyens d'authentification.
Afin d'améliorer les performances et la fiabilité des authentifications, différentes sources biométriques sont susceptibles d'être utilisées dans un processus de fusion.
La biométrie multimodale réalise, en particulier, la fusion des informations extraites de différentes modalités biométriques.
Cette thèse s'inscrit dans le cadre de l'apprentissage profond appliqué à la compréhension de la parole.
Jusqu'à présent, cette tâche était réalisée par l'intermédiaire d'une chaîne de composants mettant en oeuvre, par exemple, un système de reconnaissance de la parole, puis différents traitements du langage naturel, avant d'impliquer un système de compréhension du langage sur les transcriptions automatiques enrichies.
Récemment, des travaux dans le domaine de la reconnaissance de la parole ont montré qu'il était possible de produire une séquence de mots directement à partir du signal acoustique.
Tout d'abord, nous présentons un état de l'art décrivant les principes de l'apprentissage neuronal profond, de la reconnaissance de la parole, et de la compréhension de la parole.
Nous décrivons ensuite les contributions réalisées selon trois axes principaux.
Nous proposons un premier système répondant à la problématique posée et l'appliquons à une tâche de reconnaissance des entités nommées.
Puis, nous proposons une stratégie de transfert d'apprentissage guidée par une approche de type curriculum learning.
Cette stratégie s'appuie sur les connaissances génériques apprises afin d'améliorer les performances d'un système neuronal sur une tâche d'extraction de concepts sémantiques.
Ensuite, nous effectuons une analyse des erreurs produites par notre approche, tout en étudiant le fonctionnement de l'architecture neuronale proposée.
Enfin, nous mettons en place une mesure de confiance permettant d'évaluer la fiabilité d'une hypothèse produite par notre système.
Cette thèse explore l'usage des Grammaires Categorielles Abstraites (CGA) pour la Génération Automatique de Texte (GAT) dans un contexte industriel.
Les systèmes GAT basés sur des théories linguistiques ont un long historique, cependant ils sont relativement peu utilisés en industrie, qui préfère les approches plus "pragmatiques", le plus souvent pour des raisons de simplicité et de performance.
Cette étude montre que les avancées récentes en linguistique computationnelle permettent de concilier le besoin de rigueur théorique avec le besoin de performance, en utilisant CGA pour construire les principaux modules d'un système GAT de qualité industrielle ayant des performances comparables aux méthodes habituellement utilisées en industrie.
Le travail de recherche consistera à mieux comprendre l'évaluation des actifs verts ainsi que les risques extrêmes qu'ils font porter aux investisseurs par rapport à d'autres actifs financiers.
Pour cela, j'utiliserai des techniques d'apprentissage automatique, en particulier d'analyse textuelle pour la labélisation de ces actifs, ou d'apprentissage supervisé pour l'évaluation des primes de risques de ces actifs.
L'évaluation des capacités motrices est une activité essentielle de l'analyse de mouvement.
Cette activité permet de quantifier la performance d'un patient et ainsi d'être capable de suivre et de contrôler son évolution pour assurer un traitement adapté.
Les kinésithérapeutes ont donc besoin d'outils précis leurs permettant de mesurer cette performance.
Pour cela, ils ont développé leurs propres outils basés sur l'observation et des exercices normés.
Pourtant, cette activité pourrait être supportée et augmentée par l'utilisation de technologies avancées.
Il existe une catégorie d'outils technologiques permettant le suivi et la capture de ces mouvements.
Leur utilisation dans des systèmes d'aide à l'évaluation pourrait affiner l'évaluation des thérapeutes et également augmenter sa reproductibilité.
Pour assurer l'utilisation dans la durée de ce type d'outils, il est nécessaire de répondre à la question suivante : « quels sont les enjeux et les critères de développement spécifiques aux systèmes d'évaluation des capacités motrices ? » .
Dans ce travail de thèse, cette question a été restructurée suivant 3 axes : « comment mesurer les capacités motrices ? » , « comment analyser et communiquer le résultat ? » et enfin « comment intégrer ce système dans la pratique médicale ? » .
Pour chacun de ces axes, les critères clés de développement ont été investigués et des contributions sont présentées.
Afin d'illustrer ces critères, une étude de cas a été menée : l'instrumentation, à l'aide de nouvelles technologies de capture de mouvements, d'un protocole de mesure de capacités motrices (aussi appelé MFM ou Mesure de la Fonction Motrice).
Le but du Registre National du Cancer (RNC) du Luxembourg est de collecter des données sur le cancer et la qualité des traitements au Luxembourg.
Afin d'obtenir des données de haute qualité et comparables avec celles d'autres registres ou pays, le RNC suit les règles et standards internationaux de codification comme la Classification International des Maladies pour l'Oncologie (COM-O).
Ces standards sont complexes et considérables, compliquant fortement le processus de collecte des données.
Les encodeurs en charge de la collecte des données sont souvent confrontés à des situations dans lesquelles des données sont manquantes ou contradictoires, les empêchant d'appliquer les règles fournies.
Pour les aider dans leur tâche, les exports de codification du RNC répondent aux questions de codage des encodeurs.
Cependant, ces réponses requièrent beaucoup de temps des experts.
Le but de ce projet est de réduire le temps d'expert nécessaire et de faciliter le travail des encodeurs.
D'un point de vue scientifique, cette thèse s'intéresse au problème de synthèse d'informations à partir d'un ensemble de données provenant de différentes sources avec des contraintes et recommandations à respecter.
Le raisonnement à partir de cas est utilisé pour résoudre ce problème car cette méthodologie ressemble à cette employée par les experts.
La méthode de résolution conçue utilise des arguments fournis par les experts de codification dans le cadre de questions posées précédemment par les encodeurs.
Ce document décrit comment ces arguments servent à identifier des questions similaires et à expliquer la réponse aux encodeurs et aux experts.
Une évaluation préliminaire a été réalisée pour évaluer la performance de la méthode et identifier des pistes d'améliorations.
Dans un premier temps, le travail produit porte sur les registres du cancers et la codification médicale, cependant l'approche est généralisable à d'autres domaines.
Le secteur médical est un domaine dynamique en constante évolution, nécessitant des améliorations continues de ses processus métier et une assistance intelligente aux acteurs impliqués.
Ce travail de thèse se focalise sur le processus de soins nécessitant l'implantation d'une prothèse.
La particularité de ce processus est qu'il met en interaction deux cycles de vie appartenant respectivement au domaine médical et celui de l'ingénierie.
Ceci implique plusieurs actions de collaboration entre des acteurs métier très variés.
Cependant, des problèmes de communication et de partage de connaissances peuvent exister en raison de l'hétérogénéité de la sémantique utilisée et des pratiques métiers propres à chaque domaine.
Pour se faire, un cadre conceptuel est proposé pour analyser les connexions entre les cycles de vie de maladie (domaine Médical)et de la prothèse (domaine d'ingénierie).
Sur la base de cette analyse, un modèle sémantique sous forme d'une ontologie pour le domaine médical est définit dans le cadre de la construction d'une approche PLM à base de connaissances.
L'application de cette proposition est démontrée à travers l'implémentation de quelques fonctions utiles dans un outil PLM du marché nommé AUDROS.
Notre thèse vise à illustrer la réalisation et l'utilisation d'un formalisme (architecture à plusieurs modules) de traitement de la référence à des fins de recherches en traitement automatique de l'oral.
Nous ne prétendons pas avoir trouvé une solution miracle pour la résolution de la référence.
Nous supposons seulement que le traitement des structures coréférentielles orales nécessite la mise en pratique de formalismes standards aussi complexe que la théorie du gouvernement et du liage.
La langue orale à sa propre logique mais les formes standards de la langue y apparaissent largement
Le traitement automatique des langues et plus particulièrement la compréhension automatique de documents a pour objectif de proposer des méthodes permettant d'en extraire les informations pertinentes.
Les approches les plus efficaces aujourd'hui font appel à des méthodes d'apprentissage artificel supervisé et à de très grandes quantités d'exemple annotés manuellement.
Ce sujet de thèse propose de répondre à deux problématiques : 1) comment générer des données synthétiques ?
L'objectif de cette thèse est de proposer un système de recommandations permettant aux grands distributeurs d'améliorer leurs assortiments de produits distribués à travers de nombreux points de vente.
Dans ce contexte, la problématique adressée est celle de la planification d'assortiment qui consiste à éliciter les meilleurs produits, e.g., ceux faisant le plus de chiffre d'affaires.
Pour ce faire, nous proposons dans un premier temps une comparaison des méthodes pragmatiques mises en place dans l'industrie avec l'état de l'art associé à la planification d'assortiment.
Cette comparaison permet de mettre en lumière le problème de transversalité des connaissances utilisées aujourd'hui pour améliorer l'assortiment.
Pour pallier ce problème, nous proposons des structures de connaissances propres à la grande distribution.
Grâce à ces structures, une méthode Agile d'optimisation de l'assortiment pouvant être intégrée dans un processus d'amélioration continue est formalisée.
Cette méthode permet d'intégrer l'expertise humaine, que nous considérons comme indispensable, aux différents leviers actuellement adoptés.
Pour souligner la modularité de notre approche, nous proposons ensuite une analyse sémantique des magasins qui, en plus d'améliorer la précision de nos simulations, permet de définir un nouvel axe d'amélioration de l'assortiment.
Cette analyse se base sur les structures de connaissances propres à chaque enseigne et sur les mesures de similarités sémantiques.
Enfin, pour perfectionner notre méthode et aller plus loin dans l'exploitation ces structures, nous proposons une analyse sémantique des consommateurs qui sont les cibles finales de l'assortiment.
Cette seconde analyse sémantique permet d'apporter de nouvelles connaissances pour les distributeurs et d'apporter de nouvelles contraintes sur les assortiments.
En parallèle de ces contributions scientifiques, différentes applications ont été développées pour souligner l'interopérabilité de nos contributions avec des notions propres à différents types de distributeurs (e.g. Alimentaire, Bricolage...).
Ces applications sont présentées dans le manuscrit dans la limite du respect de la confidentialité et de la propriété intellectuelle.
La résolution des pronoms est le processus par lequel un pronom anaphorique est mis en relation avec son antécédent.
Les humains en sont capables sans efforts notables en situation normale.
En revanche, les systèmes automatiques ont une performance qui reste loin derrière, malgré des algorithmes de plus en plus sophistiqués, développés par la communauté du Traitement Automatique des Langues.
La recherche en psycholinguistique a montré à travers des expériences qu'au cours de la résolution de nombreux facteurs sont pris en compte par les locuteurs.
Une question importante se pose : comment les facteurs interagissent et quel poids faut-il attribuer à chacun d'entre eux ?
Une deuxième question qui se pose alors est comment les théories linguistiques de la résolution des pronoms incorporent tous les facteurs.
Nous proposons une nouvelle approche à ces problématiques : la simulation computationnelle de la charge cognitive de la résolution des pronoms.
De cette façon, les modèles computationnels représentent une alternative aux expériences classiques avec des items expérimentaux construits manuellement.
D'abord, nous avons simulé la charge cognitive des pronoms en utilisant des poids de facteurs de résolution appris sur corpus.
Ensuite, nous avons testé si les concepts de la Théorie de l'Information sont pertinents pour prédire la charge cognitive des pronoms.
Finalement, nous avons procédé à l'évaluation d'un modèle psycholinguistique sur des données issues d'un corpus enrichi de mouvements oculaires.
Les résultats de nos expériences montrent que la résolution des pronoms est en effet multi-factorielle et que l'influence des facteurs peut être estimée sur corpus.
Nos résultats montrent aussi que des concepts de la Théorie de l'Information sont pertinents pour la modélisation des pronoms.
Nous concluons que l'évaluation des théories sur des données de corpus peut jouer un rôle important dans le développement de ces théories et ainsi amener dans le futur à une meilleure prise en compte du contexte discursif.
Les travaux de cette thèse portent sur les conséquences du développement du numérique sur la pratique de recherche en SHS au sens large et en histoire en particulier.
L'introduction du numérique bouleverse les pratiques de recherche en histoire en mettant à disposition du chercheur un grand volume de sources numérisées ainsi que de nombreux outils d'analyse et d'écriture.
Si ces nouveaux moyens de recherche permettent à la discipline d'adopter de nouvelles approches et de renouveler certains points de vue, ils posent également des questions sur les plans méthodologique et épistémologique.
Devant ce constat, nous avons choisi d'étudier plus en détail l'impact des outils de recherche d'information, bibliothèques numériques et moteurs de recherche de sources sur l'activité de recherche en histoire.
Ces systèmes offrent un accès à un grand volume de documents historiques mais leur fonctionnement repose sur des traitements informatiques pour la plupart invisibles aux yeux des utilisateurs, qui peuvent ainsi s'apparenter à des boîtes noires.
L'objectif principal de cette thèse est donc de donner les moyens aux utilisateurs d'observer et de comprendre ces processus dans l'optique de leur permettre d'en intégrer les effets de bord à leur méthodologie.
Afin de mieux positionner notre objet d'étude, nous proposons un cadre conceptuel reposant sur la notion de ressource numérique.
Sur la base de ce cadre conceptuel, nous proposons une analyse des bibliothèques numériques et moteurs de recherche de sources en fonction de chacun des contextes.
Ces indicateurs sont ensuite croisés avec le fonctionnement du système, dans ces contextes de production et d'exécution, pour en révéler les biais méthodologiques.
À l'issue de ces analyses, nous proposons un réinvestissement de ces résultats sous la forme d'un outil logiciel dédié à l'enseignement d'une approche critique de la recherche d'information en ligne pour les apprentis historiens.
Ces travaux sont évalués par une démarche expérimentale.
Ce prototype a fait l'objet de plusieurs phases d'expérimentation liées à son développement, l'évaluation de ces fonctionnalités et de son impact sur la pratique dans un contexte de formation.
Cette thèse explore la reconnaissance de gestes à partir de capteurs inertiels pour Smartphone.
Ces gestes consistent en la réalisation d'un tracé dans l'espace présentant une valeur sémantique, avec l'appareil en main.
Notre étude porte en particulier sur l'apprentissage de métrique entre signatures gestuelles grâce à l'architecture "Siamoise" (réseau de neurones siamois, SNN), qui a pour but de modéliser les relations sémantiques entre classes afin d'extraire des caractéristiques discriminantes.
Les stratégies classiques de formation d'ensembles d'apprentissage sont essentiellement basées sur des paires similaires et dissimilaires, ou des triplets formés d'une référence et de deux échantillons respectivement similaires et dissimilaires à cette référence.
Ainsi, nous proposons une généralisation de ces approches dans un cadre de classification, où chaque ensemble d'apprentissage est composé d'une référence, un exemple positif, et un exemple négatif pour chaque classe dissimilaire.
Par ailleurs, nous appliquons une régularisation sur les sorties du réseau au cours de l'apprentissage afin de limiter les variations de la norme moyenne des vecteurs caractéristiques obtenus.
A l'aide de deux bases de données inertielles, la base MHAD (Multimodal Human Activity Dataset) ainsi que la base Orange, composée de gestes symboliques inertiels réalisés avec un Smartphone, les performances de chaque contribution sont caractérisées.
Ainsi, des protocoles modélisant un monde ouvert, qui comprend des gestes inconnus par le système, mettent en évidence les meilleures capacités de détection et rejet de nouveauté du SNN.
En résumé, le SNN proposé permet de réaliser un apprentissage supervisé de métrique de similarité non-linéaire, qui extrait des vecteurs caractéristiques discriminants, améliorant conjointement la classification et le rejet de gestes inertiels.
Dans le cadre de cette thèse, nous nous sommes concentrés sur la reconnaissance et la traduction automatique de la parole de vidéos arabes et dialectales.
Les approches statistiques proposées dans la littérature pour la reconnaissance automatique de la parole (RAP) sont indépendantes de la langue et elles sont applicables à l'arabe standard.
Cependant, cette dernière présente quelques caractéristiques que nous devons prendre en considération afin de booster les performances du système de RAP.
Parmi ces caractéristiques on peut citer l'absence de l'indication des voyelles dans le texte ce qui rend difficile leur apprentissage par le modèle acoustique.
Nous avons proposé plusieurs approches de modélisation acoustique et/ou de langage afin de mieux reconnaître la parole arabe.
L'arabe standard n'est pas la langue maternelle, c'est pourquoi dans les conversations quotidiennes, on utilise le dialecte, un arabe inspiré de l'arabe standard, mais pas seulement.
Nous avons travaillé sur l'adaptation du système développé pour l'arabe standard au dialecte algérien qui est l'une des variantes de la langue arabe les plus difficiles à reconnaître par les systèmes de RAP.
Cela est dû aux mots empruntés d'autres langues, au code-switching et au manque de ressources.
Notre proposition pour remédier à ces problèmes est de tirer profit des données orales et textuelles d'autres langues impactant le dialecte.
Le texte résultant de la RAP arabe a été utilisé pour la traduction automatique (TA).
Nous avons réalisé dans un premier temps une étude comparative entre l'approche statistique à base de segments et l'approche neuronale utilisées dans le cadre de la TA.
Ensuite, nous nous sommes intéressés à l'adaptation de ces deux approches pour traduire le texte code-switché.
Notre étude portait sur le mélange de l'arabe et de l'anglais dans des documents officiels des nations unies.
Pour pallier les différents problèmes dus à la propagation des erreurs dans le système séquentiel, nous avons travaillé sur l'adaptation du vocabulaire du système de RAP et sur la proposition d'une nouvelle modélisation permettant la traduction directe de la parole.
Donner du sens aux données textuelles est une besoin essentielle pour faire les ordinateurs comprendre notre langage.
Pour extraire des informations exploitables du texte, nous devons les représenter avec des descripteurs avant d'utiliser des techniques d'apprentissage.
Dans ce sens, le but de cette thèse est de faire la lumière sur les représentations hétérogènes des mots et sur la façon de les exploiter tout en abordant leur nature implicitement éparse.
Dans un premier temps, nous proposons un modèle de réseau basé sur des hypergraphes qui contient des données linguistiques hétérogènes dans un seul modèle unifié.
En d'autres termes, nous introduisons un modèle qui représente les mots au moyen de différentes propriétés linguistiques et les relie ensemble en fonction desdites propriétés.
Notre proposition diffère des autres types de réseaux linguistiques parce que nous visons à fournir une structure générale pouvant contenir plusieurstypes de caractéristiques descriptives du texte, au lieu d'une seule comme dans la plupart des représentations existantes.
Cette représentation peut être utilisée pour analyser les propriétés inhérentes du langage à partir de différents points de vue, oupour être le point de départ d'un pipeline de tâches du traitement automatique de langage.
Deuxièmement, nous utilisons des techniques de fusion de caractéristiques pour fournir une représentation enrichie unique qui exploite la nature hétérogènedu modèle et atténue l'eparsité de chaque représentation.
Ces types de techniques sont régulièrement utilisés exclusivement pour combiner des données multimédia.
Dans notre approche, nous considérons différentes représentations de texte comme des sources d'information distinctes qui peuvent être enrichies par elles-mêmes.
Cette approche n'a pas été explorée auparavant, à notre connaissance.
Troisièmement, nous proposons un algorithme qui exploite les caractéristiques du réseau pour identifier et grouper des mots liés sémantiquement en exploitant les propriétés des réseaux.
Contrairement aux méthodes similaires qui sont également basées sur la structure du réseau, notre algorithme réduit le nombre de paramètres requis et surtout, permet l'utilisation de réseaux lexicaux ou syntaxiques pour découvrir les groupes de mots, au lieu d'un type unique des caractéristiques comme elles sont habituellement employées.
Nous nous concentrons sur deux tâches différentes de traitement du langage naturel : l'induction et la désambiguïsation des sens des mots (en anglais, Word Sense, Induction and Disambiguation, ou WSI/WSD) et la reconnaissance d'entité nommées(en anglais, Named Entity Recognition, ou NER).
Au total, nous testons nos propositions sur quatre ensembles de données différents.
Les résultats obtenus nous permettent de montrer la pertinence de nos contributions et nous donnent également un aperçu des propriétés des caractéristiques hétérogènes et de leurs combinaisons avec les méthodes de fusion.
Aussi, nous analysons les opérateurs de fusion utilisés afin de mieux comprendre la raison de ces améliorations.
Et deuxièmement, nous abordons encore une fois la tâche WSI/WSD, cette fois-ci avec la méthode à base de graphes proposée afin de démontrer sa pertinence par rapport à la tâche.
Nous discutons les différents résultats obtenus avec des caractéristiques lexicales ou syntaxiques.
Ces dernières années, les techniques d'apprentissage profond ont fondamentalement transformé l'état de l'art de nombreuses applications de l'apprentissage automatique, devenant la nouvelle approche standard pour plusieurs d'entre elles.
Les architectures provenant de ces techniques ont été utilisées pour l'apprentissage par transfert, ce qui a élargi la puissance des modèles profonds à des tâches qui ne disposaient pas de suffisamment de données pour les entraîner à partir de zéro.
Le sujet d'étude de cette thèse couvre les espaces de représentation créés par les architectures profondes.
Dans un premier temps, nous étudions les propriétés de leurs espaces, en prêtant un intérêt particulier à la redondance des dimensions et la précision numérique de leurs représentations.
Nos résultats démontrent un fort degré de robustesse, pointant vers des schémas de compression simples et puissants.
Ensuite, nous nous concentrons sur le l'affinement de ces représentations.
Nous choisissons d'adopter un problème multi-tâches intermodal et de concevoir une fonction de coût capable de tirer parti des données de plusieurs modalités, tout en tenant compte des différentes tâches associées au même ensemble de données.
Afin d'équilibrer correctement ces coûts, nous développons également un nouveau processus d'échantillonnage qui ne prend en compte que des exemples contribuant à la phase d'apprentissage, c'est-à-dire ceux ayant un coût positif.
Enfin, nous testons notre approche sur un ensemble de données à grande échelle de recettes de cuisine et d'images associées.
Notre méthode améliore de 5 fois l'état de l'art sur cette tâche, et nous montrons que l'aspect multitâche de notre approche favorise l'organisation sémantique de l'espace de représentation, lui permettant d'effectuer des sous-tâches jamais vues pendant l'entraînement, comme l'exclusion et la sélection d'ingrédients.
Les résultats que nous présentons dans cette thèse ouvrent de nombreuses possibilités, y compris la compression de caractéristiques pour les applications distantes, l'apprentissage multi-modal et multitâche robuste et l'affinement de l'espace des caractéristiques.
Pour l'application dans le contexte de la cuisine, beaucoup de nos résultats sont directement applicables dans une situation réelle, en particulier pour la détection d'allergènes, la recherche de recettes alternatives en raison de restrictions alimentaires et la planification de menus.
Les essaims de robots sont des systèmes composés d'un grand nombre de robots relativement simples.
Du fait du grand nombre d'unités, ces systèmes ont de bonnes propriétés de robustesse et de passage à l'échelle.
Néanmoins, il reste en général difficile de concevoir manuellement des contrôleurs pour les essaims de robots, à cause de la grande complexité des interactions inter-robot.
Par conséquent, les approches automatisées pour l'apprentissage de comportements d'essaims de robots constituent une alternative attrayante.
Dans cette thèse, nous étudions l'adaptation de comportements d'essaim de robots avec des méthodes de Embodied Evolutionary Robotics (EER) distribuée.
Ainsi, nous fournissons trois contributions principales :
(1) Nous étudions l'influence de la pression à la sélection dirigée vers une tâche dans un essaim d'agents robotiques qui utilisent une approche d'EER distribuée.
Nous évaluons l'impact de différents opérateurs de sélection dans un algorithme d'EER distribuée pour un essaim de robots.
Nos résultats montrent que le plus forte la pression à la sélection est, les meilleures performances sont atteintes lorsque les robots doivent s'adapter à des tâches particulières.
(2) Nous étudions l'évolution de comportements collaboratifs pour une tâche de récolte d'objets dans un essaim d'agents robotiques qui utilisent une approche d'EER distribuée.
Nous réalisons un ensemble d'expériences où un essaim de robots s'adapte à une tâche collaborative avec un algorithme d'EER distribuée.
Nos résultats montrent que l'essaim s'adapte à résoudre la tâche, et nous identifions des limitations concernant le choix d'action.
(3) Nous proposons et validons expérimentalement un mécanisme complètement distribué pour adapter la structure des neurocontrôleurs des robots dans un essaim qui utilise une approche d'EER distribuée, ce qui permettrait aux neurocontrôleurs d'augmenter leur expressivité.
Nos expériences montrent que notre mécanisme, qui est complètement décentralisé, fournit des résultats similaires à un mécanisme qui dépend d'une information globale
Les smartphones et les tablettes sont devenus des outils presque incontournables dans la vie de tout les jours.
Cependant, la diversité des applications mobiles, de leurs sources et de leur simplicité d'installation et d'utilisation rendent complexes la gestion de la sécurité de ce type de terminaux.
Pour faire face à ces menaces, il existe plusieurs types d'approche.
On peut en citer : l'analyse statique, l'analyse dynamique ou encore l'analyse comportementale.
Pradeo offre à ses clients une solution d'audit de sécurité automatique des applications mobiles.
Ce produit a été développé principalement dans le cadre de plusieurs thèses CIFRE en collaboration avec le laboratoire LIRMM.
Cependant, ces travaux n'incluent pas de méthode d'apprentissage automatique permettant d'exploiter des données collectées à partir des terminaux mobiles afin d'anticiper des situations à risque.
Dans cette thèse, nous nous intéresserons donc à l'application de méthodes d'analyse de données et d'intelligence artificielle afin de corréler des données provenant de plusieurs sources et ainsi implémenter une analyse comportementale efficace.
Nous jugeons que les méthodes existantes ne s'appliquent pas directement à la problématique posée par Pradeo en raison de la vélocité des données, de leur caractère hétérogène et des spécificités de l'environnement mobile, comme développé dans les verrous ci-après :
- La non-modification de l'OS ; l'agent de collecte de données sur un terminal mobile doit pouvoir fonctionner sans qu'il soit nécessaire de jailbreaker le terminal
- La consommation d'énergie, de mémoire de stockage et de données réseau de l'agent mobile doit être minimale
- L'aspect temps réel "souple" doit être pris en considération lors du traitement de gros volumes de données (données provenant de +1 million de terminaux)
- La marge d'erreur du modèle de prédiction doit être minimale afin de réduire au maximum toute vérification manuelle
- La prise en compte d'un large panel de terminaux mobiles, OS, constructeurs
Le but de cette thèse est d'enrichir la technologie Pradeo afin d'identifier des situations à risque sur un terminal mobile.
L'objectif sera d'ajouter une brique d'analyse dynamique et intelligente du contexte global du terminal mobile qui corrélera plusieurs sources de données :
- Des événements (système, réseaux et applicatifs) remontés en temps réel par les terminaux
- Les rapports d'analyse statique des applications mobiles-Des sources de menaces / vulnérabilités externes
Notre recherche repose sur l'analyse des politiques publiques françaises et marocaines en matière d'adoption des technologies du Cloud Computing et du Big Data.
Enjeux pour lesquels l'Etat doit apporter aujourd'hui des réponses politiques et techniques.
— Développer des outils numériques du type Big Data articulés à des solutions « Cloud Computing » afin d'améliorer des services publics.
— Développer et assurer la présence de l'Etat et de ses administrations dans le cyberespace ;
— Mettre les outils du type Coud Computing au service de la sécurité nationale pour faire face aux dispositifs de cyber-renseignement étrangers.
Dans un contexte de transformations profondes de la société induites par le numérique, l'Etat doit réaffirmer ses droits sur son propre territoire.
Pour faire face aux risques sécuritaires, l'Etat français comme l'Etat marocain se sont dotés des instruments juridiques et techniques qui s'appuient précisément sur les technologies du Cloud Computing et du Big Data.
L'arsenal juridique français s'est vu renforcé dernièrement par l'adoption successive et accélérée — sans débat national — de la Loi de programmation militaire (2014-2019) puis sur les lois anti-terroriste (2014) et sur le Renseignement (2015).
Ces différents textes ont agité le débat politique en instillant une inquiétude grandissante relative au déploiement de dispositifs numériques de surveillance.
Nous avons ainsi pu traiter des notions comme celles de cyber-sécurité, de cyber-souveraineté et de cyber-surveillance au sein du Cyber-Etat.
3. Les enjeux liés au traitement des données personnelles au sein du Cyber-Etat et produites par les activités quotidiennes du cyber-citoyen.
Le modèle de la forme, deux polygones décrivant les contours interne et externe des lèvres, est appris par ACP.
Le modèle d'apparence est un réseau de neurones qui classifie les points de l'image.
Son entraînement nécessite de repérer les lèvres sur des images naturelles et, plutôt que de recourir à un étiquetage manuel, nous proposons une nouvelle méthode automatique utilisant deux répétitions d'une même phrase, avec et sans maquillage bleu.
Le maquillage permet d'extraire le contour des lèvres et l'alignement par DTW des canaux acoustiques des deux séquences permet d'estimer la forme des lèvres sur les images naturelles, grâce aux formes extraites avec le maquillage.
L'essor des travaux en informatique affective voit la naissance de diverses questions de recherches pour étudier les interactions agents /humains.
Parmi elles, se pose la question de l'impact des relations interpersonnelles sur les stratégies de communications.
Les interactions entre un agent conversation et un utilisateur humain prennent généralement place dans des environnements collaboratifs où les interlocuteurs partagent des buts communs.
La relation interpersonnelle que les individus créent durant leurs interactions affecte leurs stratégies de communications.
Par ailleurs, des individus qui collaborent pour atteindre un but commun sont généralement amenés à négocier.
Ce type de négociation permet aux négociateurs d'échanger des informations afin de mieux collaborer.
L'objectif cette thèse est d'étudier l'impact de la relation interpersonnelle de dominance sur les stratégies de négociation collaborative entre un agent et un humain.
Ce travail se base sur des études en psychologie sociale qui ont défini les comportements liés à la manifestation de la dominance dans une négociation.
Nous proposons un modèle de négociation collaborative dont le modèle décisionnel est régi par la relation de dominance.
En effet, en fonction de sa position dans le spectre de dominance, l'agent est capable d'exprimer une stratégie de négociation spécifique.
En parallèle, l'agent simule une relation interpersonnelle de dominance avec son interlocuteur.
Pour ce faire, nous avons doté l'agent d'un modèle de théorie de l'esprit qui permet à l'agent de raisonner sur les comportements de son interlocuteur afin de prédire sa position dans le spectre de dominance.
Ensuite, il adapte sa stratégie de négociation vers une stratégie complémentaire à celle détectée chez son interlocuteur.
Nos résultats ont montré que les comportements de dominance exprimés par notre agent sont correctement perçus.
Par ailleurs, le modèle de la théorie de l'esprit est capable de faire de bonnes prédictions avec seulement une représentation partielle de l'état mental de l'interlocuteur.
Enfin, la simulation de la relation interpersonnelle de dominance a un impact positif sur la négociation : les négociateurs atteignent de bon taux de gains communs.
Des études précédentes démontrent qu'avoir accès à la structure syntaxique des phrases aide les enfants à découvrir le sens des mots nouveaux.
Cela implique que les enfants doivent avoir accès à certains aspects de la structure syntaxique avant même de connaître beaucoup de mots.
Étant donné que dans toutes les langues du monde la structure prosodique d'une phrase corrèle avec sa structure syntaxique, et que par ailleurs les mots et morphèmes grammaticaux sont utiles pour déterminer la catégorie syntaxique des mots, il se pourrait que les enfants utilisent la prosodie et les mots grammaticaux pour initialiser leur acquisition lexicale et syntaxique.
Dans cette thèse, j'ai étudié le rôle de la prosodie phrasale et des mots grammaticaux pour guider l'analyse syntaxique chez les enfants (PARTIE 1) et la possibilité que les jeunes enfants exploitent cette information pour apprendre le sens des mots nouveaux (PARTIE 2).
Dans la partie 1, j'ai construit des paires minimales de phrases en français et en anglais afin de tester si les enfants exploitent la relation entre les structures prosodique et syntaxique pour guider leur interprétation des homophones noms-verbes.
J'ai démontré que les enfants d'âge préscolaire utilisent la prosodie phrasale en temps réel pour guider leur analyse syntaxique.
En écoutant des phrases telles que [La petite ferme] [..., les enfants interprètent ferme comme un nom, mais pour les phrases telles que [La petite][ferme...], ils interprètent ferme comme un verbe (Chapitre 3).
Cette capacité a également été observée chez les enfants américains : en écoutant des phrases telles que « The baby flies … » , ils utilisent la prosodie des phrases pour décider si flies est un nom ou un verbe (Chapitre 4).
Par la suite, j'ai démontré que même les enfants d'environ 20 mois utilisent la prosodie des phrases pour récupérer leur structure syntaxique et pour en déduire la catégorie syntaxique des mots (Chapitre 5), une capacité qui serait extrêmement utile pour découvrir le sens des mots inconnus.
C'est cette hypothèse que j'ai testé dans la partie 2, à savoir si l'information syntaxique obtenue à partir de la prosodie phrasale et des mots grammaticaux permet aux enfants d'apprendre le sens des mots.
Une première série d'études s'appuie sur des phrases disloquées à droite contenant un verbe nouveau en français : [ili dase], [le bébéi] qui est minimalement différente de la phrase transitive [il dase le bébé].
Mes résultats montrent que les enfants de 28 mois exploitent les informations prosodiques de ces phrases pour contraindre leur interprétation du sens du nouveau verbe (Chapitre 6).
Dans une deuxième série d'études, j'ai étudié si la prosodie et les mots grammaticaux guident l'acquisition de noms et de verbes.
J'ai utilisé des phrases comme « Regarde la petite bamoule » qui peuvent être produites soit comme [Regarde la petite bamoule !], où « bamoule » est un nom, ou [Regarde], [la petite] [bamoule !], où bamoule est un verbe.
Les enfants de 18 mois ont correctement analysé ces phrases et ont attribué une interprétation de nom ou de verbe au mot bamoule selon sa position dans la structure prosodique-syntaxique des phrases (Chapitre 7).
Ensemble, ces études montrent que les jeunes enfants exploitent les mots grammaticaux et la structure prosodique des phrases pour inférer la structure syntaxique et contraindre ainsi l'interprétation possible du sens des mots.
Ce mécanisme peut permettre aux enfants de construire une représentation initiale de la structure syntaxique des phrases, avant même de connaître la signification des mots.
Bien que les informations prosodiques et les mots grammaticaux puissent prendre des formes différentes selon les langues, nos études suggèrent que cette information pourrait représenter un outil universel et qui permettrait aux enfants d'accéder à certaines informations syntaxiques des phrasesqu'ils entendent, et d'initialiser l'acquisition du langage.
Même si le capital immatériel représente une part de plus en plus importante de la valeur de nos organisations, il n'est pas toujours possible de stocker, tracer ou capturer les connaissances et les expertises, par exemple dans des projets de taille moyenne.
Le courrier électronique est encore largement utilisé dans les projets d'entreprise en particulier entre les équipes géographiquement dispersées.
Dans cette étude, nous présentons une nouvelle approche pour détecter les zones à l'intérieur de courriels professionnels où des éléments de connaissances sont susceptibles de se trouver.
Nous définissons un contexte étendu en tenant compte non seulement du contenu du courrier électronique et de ses métadonnées, mais également des compétences et des rôles des utilisateurs.
Également l'analyse pragmatique linguistique est mêlée aux techniques usuelles du traitement de langage naturel.
Après avoir décrit notre méthode KTR et notre modèle, nous l'appliquons à un corpus réel d'entreprise et évaluons les résultats en fonction des algorithmes d'apprentissage, de filtrage et de recherche
L'analyse automatique des expressions faciales représente à l'heure actuelle une problématique importante associée à de multiples applications telles que la reconnaissance de visages ou encore les interactions homme machine.
Dans cette thèse, nous nous attaquons au problème de la reconnaissance d'expressions faciales à partir d'une image ou d'une séquence d'images.
Nous abordons le problème sous trois angles.
Tout d'abord, nous étudions les macro-expressions faciales et nous proposons de comparer l'efficacité de trois descripteurs différents.
Nous examinons aussi l'apport de descripteurs spatio-temporels capables de prendre en compte des informations dynamiques utiles pour séparer les classes ambigües.
La grosse limitation des méthodes de classification supervisée est qu'elles sont très coûteuses en termes de labélisation de données.
Ainsi nous nous sommes intéressés à l'adaptation de domaine et à l'apprentissage avec peu ou pas de données labélisées.
La méthode proposée nous permet de traiter des données non labélisées provenant de distributions différentes de celles du domaine source de l'apprentissage ou encore des données qui ne concernent pas les mêmes labels mais qui partagent le même contexte.
Le transfert de connaissance s'appuie sur un apprentissage euclidien et des réseaux de neurones convolutifs de manière à définir une fonction de mise en correspondance entre les informations visuelles provenant des expressions faciales et un espace sémantique issu d'un modèle de langage naturel.
Dans un troisième temps, nous nous sommes intéressés à la reconnaissance des micro-expressions faciales.
Ainsi nous proposons un réseau de neurones auto-encodeur récurrent destiné à capturer les changements spatiaux et temporels associés à toutes les déformations du visage autres que celles dues aux micro-expressions.
Ensuite, nous apprenons un modèle statistique basé sur un mélange de gaussiennes afin d'estimer la densité de probabilité de ces déformations autres que celles dues aux micro-expressions.
Tous nos algorithmes sont testés et évalués sur des bases d'expressions faciales actées et/ou spontanées.
L'objectif de ces travaux de recherche est d'étudier les méthodes d'évaluation de la qualité des images biométriques multimodales sur des échantillons acquis de manière non contrainte.
De nombreuses s études ont noté l'importance de la qualité de l'échantillon pour un système de reconnaissance ou un algorithme de comparaison,puisque la performance du système biométrique est intrinsèquement dépendant de la qualité des images de l'échantillon.
Dès lors, la nécessité d'évaluer la qualité des échantillons biométriques pour plusieurs modalités (empreintes digitales, iris,visage, etc.) est devenue primordiale notamment avec l'apparition de systèmes biométriques multimodaux de haute précision.
Après une introduction présentant un historique de la biométrie et des préceptes liés à la qualité des échantillons biométriques, nous présentons le concept d'évaluation de la qualité des échantillons pour plusieurs modalités.
Les normes de qualité ISO / CEI récemment établies pour les empreintes digitales, l'iris et le visage sont présentées.
De plus, des approches d'évaluation de la qualité des échantillons conçues spécifiquement pour les empreintes digitales avec et sans contact, pour l'iris(dont une image est capturée en proche infrarouge et dans le domaine visible),ainsi que le visage sont étudiées.
Finalement, des techniques d'évaluation des performances des mesures de qualité des échantillons biométriques sont également étudiées.
Sur la base des conclusions formulées suite à l'étude des solutions algorithmiques portant sur l'évaluation de la qualité des échantillons biométriques, nous proposons un cadre commun pour l'évaluation de la qualité d'image biométrique pour plusieurs modalité.
Après avoir étudié les attributs de qualité basés sur l'image par modalité biométrique, nous examinons quelle intersection existe pour l'ensemble des modalités.
Ensuite, nous sélectionnons et redéfinissons les attributs de qualité basés sur l'image qui sont les plus importants afin de définir un cadre commun.
Afin de relier ces attributs de qualité aux vrais échantillons biométriques,nous développons une nouvelle base de données de qualité d'image biométrique multi-modalité qui contient des images échantillons de haute qualité et des images dégradées pour l'empreinte digitale acquise sans contact, l'iris (dont l'acquisition est réalisée dans le spectre visible) et le visage.
Les types de dégradation appliqués sont liés aux attributs de qualité qui sont communs aux diverses modalités et qui sont basés sur l'image.
Un autre aspect important du cadre commun proposé est la qualité de l'image et ses applications en biométrie.
Nous avons d'abord introduit et classifié les métriques de qualité d'image existantes, puis effectué un bref aperçu des métriques de qualité d'image sans référence, qui peuvent être appliquées pour l'évaluation de la qualité des échantillons biométriques.
De plus, nous étudions comment les mesures de qualité d'image sans référence ont été utilisées pour l'évaluation de la qualité des empreintes digitales, de l'iris et des modalités biométriques du visage.
Des expériences pour l'évaluation de la performance des métriques de qualité d'image sans référence sur les images de visage et de l'iris sont effectuées.
Les résultats expérimentaux indiquent qu'il existe plusieurs métriques qui peuvent évaluer la qualité des échantillons biométriques de l'iris et du visage avec un fort coefficient de correlation.
À travers le travail réalisé dans cette thèse, nous avons démontré l'applicabilité des métriques de qualité d'image sans référence pour l'évaluation d'échantillons biométriques multi-modalité non contraints.
Ce travail de thèse, qui se situe à l'interface entre traitement du signal, informatique et statistiques, vise à l'élaboration de méthodes d'apprentissage automatique à grande échelle et de garanties théoriques associées.
Le schéma de compression utilisé permet de tirer profit d'une architecture distribuée ou de traiter des données en flux, et a déjà été utilisé avec succès sur plusieurs tâches d'apprentissage non-supervisé  : partitionnement type k-moyennes, modélisation de densité avec modèle de mélange gaussien, analyse en composantes principales.
Les contributions de la thèse s'intègrent dans ce cadre de plusieurs manières.
D'une part, il est montré qu'en bruitant le sketch, des garanties de confidentialité (différentielle) peuvent être obtenues ;
des bornes exactes sur le niveau de bruit requis sont données, et une comparaison expérimentale permet d'établir que l'approche proposée est compétitive vis-à-vis d'autres méthodes récentes.
Ensuite, le schéma de compression est adapté pour utiliser des matrices aléatoires structurées, qui permettent de réduire significativement les coûts de calcul et rendent possible l'utilisation de méthodes compressives sur des données de grande dimension.
Enfin, un nouvel algorithme basé sur la propagation de convictions est proposé pour résoudre la phase d'apprentissage (à partir du sketch) pour le problème de partitionnement type k-moyennes.
Elles nous permettent par exemple d'effectuer des paiements ou encore de signer des documents numériques.
Parce que les cartes à puces contiennent des informations personnelles et sensibles relatives à leur propriétaire légitime, elles sont convoitées par les attaquants.
En particulier, ces attaquants peuvent utiliser le fuzzing.
Cette attaque consiste à tester le plus de messages de communication possible avec un programme afin de pouvoir détecter des vulnérabilités.
Cette thèse vise à protéger les cartes à puces face aux attaques par fuzzing.
Deux approches de détection automatique d'erreurs d'implémentation sont proposées.
La première, est l'adaptation pour Java et son amélioration d'un outil issu de l'état de l'art.
Il repose sur une technique de fouille de code sources automatique.
La seconde approche est également basée sur la fouille de code sources, en prenant en compte les limites de la première.
En particulier, la précision et la réduction des dimensions est améliorée par l'utilisation de techniques issues du Traitement Automatique du Langage Naturel.
De plus, une étude des techniques de plagiat augmente la robustesse de l'analyse face aux différences d'implémentation des applets.
Cette évaluation repose sur trois oracles construits manuellement à partir de programmes AES et d'applets OpenPGP.
Les résultats montrent que la seconde approche permet de détecter des vulnérabilités avec plus de précision, de rappel, et en moins de temps que la première approche.
Aussi, son implémentation nommée Confiance
Cette thèse étudie la coarticulation C-à-V en français et son interaction avec d'autres sources de variations dans le but de mieux comprendre ce qui la module et ce qui gouverne la variation dans la parole.
Pour cela, à partir de grands corpus de parole, nous avons testé comment la coarticulation C-à-V était fonction : 1) des caractéristiques articulatoires des consonnes et voyelles impliquées à partir de 18.5k voyelles /i, e, ɛ, a, x, u, o, ɔ/ (/x/=/ø, œ, ə/) en contexte ALVéolaire, UVulaire et VÉLaire ;
Cependant, certains résultats suggèrent que la modulation de la coarticulation par la position prosodique et le style de parole, ont des fonctions linguistiques différentes dont les implications sur la variation dans la parole seront discutées.
Enfin, une réflexion sur les changements de sons en lien avec la préférence universelle pour l'antériorisation des voyelles postérieures fermées sera proposée à partir des différences observées entre les voyelles.
Les principaux enjeux technico-scientifiques du 21ème siècle sont caractérisés par une interdisciplinarité et une convergence des technologies de plus en plus importantes.
L'évolution des produits et services basés sur la bio- et la nanotechnologie sont parmi les exemples les plus connus.
Il manque cependant des processus et des méthodes permettant d'organiser et de structurer la résolution de problème dans des environnements interdisciplinaires – ce terme faisant ici référence à la collaboration entre ingénieurs et chercheurs scientifiques.
Ainsi, la question de recherche de ce travail de doctorat est la suivante : Comment soutenir et faciliter la résolution créative de problème interdisciplinaire et l'intégration des technologies dans des domaines basés sur la connaissance ?
Pour répondre à cette question, trois hypothèses ont été formulées :
La deuxième hypothèse suggère un impact du support méthodologique sur le processus de résolution de problème en groupe ainsi que sur les résultats de ce processus.
La troisième hypothèse suggère quant à elle que les concepts et notions clés des méthodes analytiques comme TRIZ et USIT peuvent être utilisés dans un processus d'intégration de technologie et peuvent soutenir ce processus.
Alors que la composition de groupe impacte principalement les aspects quantitatifs et qualitatifs des solutions proposées, le support méthodologique influence quant à lui le processus de résolution de problème ainsi que les aspects qualitatifs des solutions.
Plus important, l'impact des méthodes semble être dépendant de la composition des groupes.
Pour tester la troisième hypothèse, les résultats de la première expérimentation ont été utilisés pour générer un modèle permettant de structurer la recherche et l'intégration d'une ou plusieurs technologies dans le cadre du développement de nouveaux produits.
Ce modèle, qui intègre des méthodes et outils provenant de différentes méthodologies, a été testé par des ingénieurs lors d'une étude de cas industrielle dans le secteur des roulements à bille.
L'évaluation du modèle montre qu'il semble faciliter le transfert de connaissance et améliorer la créativité des concepts développés comparé aux approches déjà existantes.
En ce qui concerne l'effort nécessaire pour l'apprentissage et la mise en œuvre du modèle développé, les performances sont comparables à celles obtenues avec les méthodes préexistantes.
Les résultats de ce travail sont particulièrement intéressants pour les équipes de la R et D et leur management dans les secteurs de la haute technologie ainsi que dans des domaines à l'interface entre l'ingénierie et les sciences naturelles.
Le but de cette thèse est de proposer des méthodes pour récupérer les noms propres manquants dans un système de reconnaissance.
Nous proposons de modéliser le contexte sémantique et d'utiliser des informations thématiques contenus dans les documents audio à transcrire.
Des modèles probabilistes de thème et des projections dans un espace continu obtenues à l'aide de réseaux de neurones sont explorés pour la tâche de récupération des noms propres pertinents.
Une évaluation approfondie de ces représentations contextuelles a été réalisée.
En s'appuyant sur ce modèle, nous proposons un nouveau modèle (Neural Bag-of-Weighted-Words, NBOW2) qui permet d'estimer un degré d'importance pour chacun des mots du document et a la capacité de capturer des mots spécifiques à ce document.
Des expériences de reconnaissance automatique de bulletins d'information télévisés montrent l'efficacité du modèle proposé.
L'évaluation de NBOW2 sur d'autres tâches telles que la classification de textes montre des bonnes performances
Le développement des technologies de l'information et de la communication à modifié en profondeur la manière dont nous avons accès aux connaissances.
Face à l'afflux de données et à leur diversité, il est nécessaire de meure su point des technologies performantes et robustes pour y rechercher des informations.
Après une caractérisation de leur nature linguistique, nous proposons une approche par instructions, fondée sur les marqueurs (balises) d'annotation, qui considère ces éléments isolément (début ou fin d'une annotation).
En seconde partie, nous faisons état des travaux en fouille de données et présentons un cadre formel pour explorer les données.
Nous y proposons une formulation alternative par segments, qui limite la combinatoire lors de l'exploration.
Les motifs corrélés à un ou plusieurs marqueurs d'annotation sont extraits comme règles d'annotation.
La dernière partie décrit le cadre expérimental, quelques spécificités de l'implémentation du système (mXS) et les résultats obtenus.
Nous montrons l'intérêt d'extraire largement les règles d'annotation et expérimentons les motifs de segments.
Nous fournissons des résultats chiffrés relatifs aux performances du système à divers point de vue et dans diverses configurations.
Ils montrent que l'approche que nous proposons est compétitive et qu'elle ouvre des perspectives dans le cadre de l'observation des langues naturelles et de l'annotation automatique.
Cette thèse propose de développer des nouvelles méthodes pour apprendre des stratégies de dialogue par apprentissage par renforcement afin de résoudre le double défi des systèmes de dialogue permettant d'accomplir une tache précise : créer des agents conversationnels capables de résoudre efficacement la tache tout en générant un procédé de communication de niveau égal à celui d'un humain.
À mesure que la production de textes numériques croît exponentiellement, un besoin grandissant d'analyser des corpus de textes se manifeste dans beaucoup de domaines d'application, tant ces corpus constituent des sources inépuisables d'information et de connaissance partagées.
Ainsi proposons-nous dans cette thèse une nouvelle approche de visualisation analytique pour l'analyse de corpus textuels, mise en œuvre pour les besoins spécifiques du journalisme d'investigation.
Motivées par les problèmes et les tâches identifiés avec une journaliste d'investigation professionnelle, les visualisations et les interactions ont été conçues suivant une méthodologie centrée utilisateur, impliquant l'utilisateur durant tout le processus de développement.
En l'occurrence, les journalistes d'investigation formulent des hypothèses, explorent leur sujet d'investigation sous tous ses angles, à la recherche de sources multiples étayant leurs hypothèses de travail.
La réalisation de ces tâches, très fastidieuse lorsque les corpus sont volumineux, requiert l'usage de logiciels de visualisation analytique se confrontant aux problématiques de recherche abordées dans cette thèse.
D'abord, la difficulté de donner du sens à un corpus textuel vient de sa nature non structurée.
Nous avons donc recours au modèle vectoriel et son lien étroit avec l'hypothèse distributionnelle, ainsi qu'aux algorithmes qui l'exploitent pour révéler la structure sémantique latente du corpus.
Bien que l'exploration des sujets de haut niveau aide à localiser des sujets d'intérêt ainsi que leur voisinage, l'identification de faits précis, de points de vue ou d'angles d'analyse, en lien avec un événement ou une histoire, nécessite un niveau de structuration plus fin pour représenter des variantes de sujet.
Cette structure imbriquée révélée par Bimax, une méthode de biclustering basée sur des motifs avec chevauchement, capture au sein des biclusters les co-occurrences de termes partagés par des sous-ensembles de documents pouvant dévoiler des faits, des points de vue ou des angles associés à des événements ou des histoires communes.
Cette thèse aborde les problèmes de visualisation de biclusters avec chevauchement en organisant les biclusters terme-document en une hiérarchie qui limite la redondance des termes et met en exergue les parties communes et distinctives des biclusters.
Nous avons évalué l'utilité de notre logiciel d'abord par un scénario d'utilisation doublé d'une évaluation qualitative avec une journaliste d'investigation.
En outre, les motifs de co-occurrence des variantes de sujet révélées par Bima. sont déterminés par la structure de sujet englobante fournie par une méthode d'extraction de sujet.
Cependant, la communauté a peu de recul quant au choix de la méthode et son impact sur l'exploration et l'interprétation des sujets et de ses variantes.
Ainsi nous avons conduit une expérience computationnelle et une expérience utilisateur contrôlée afin de comparer deux méthodes d'extraction de sujet.
Depuis ces dernières décennies, des millions d'internautes produisent et échangent des données sur le Web.
Actuellement, une grande quantité de descriptions RDF est disponible en ligne, notamment grâce à des projets de recherche qui traitent du Web de données liées, comme par exemple DBpedia et LinkedGeoData.
De plus, de nombreux fournisseurs de données ont adopté les technologies issues de cette communauté du Web de données en partageant, connectant, enrichissant et publiant leurs informations à l'aide du standard RDF, comme les gouvernements (France, Canada, Grande-Bretagne, etc.), les universités (par exemple Open University) ainsi que les entreprises (BBC, CNN, etc.).
Il en résulte que de nombreux acteurs actuels (particuliers ou organisations) produisent des quantités gigantesques de descriptions RDF qui sont échangées selon différents formats (RDF/XML, Turtle, N-Triple, etc.).
Pour ce faire, nous proposons une approche intitulée R2NR qui à partir de différentes descriptions relatives à une même information produise une et une seule description normalisée qui est optimisée en fonction de multiples paramètres liés à une application cible.
Notre approche est illustrée en décrivant plusieurs cas d'étude (simple pour la compréhension mais aussi plus réaliste pour montrer le passage à l'échelle) nécessitant l'étape de normalisation.
La contribution de cette thèse peut être synthétisée selon les points suivants :
i. Produire une description RDF normalisée (en sortie) qui préserve les informations d'une description source (en entrée),
ii. Éliminer les redondances et optimiser l'encodage d'une description normalisée,
iii. Engendrer une description RDF optimisée en fonction d'une application cible (chargement rapide, stockage optimisée...),
iv. Définir de manière complète et formelle le processus de normalisation à l'aide de fonctions, d'opérateurs, de règles et de propriétés bien fondées, etc.
v. Fournir un prototype RDF2NormRDF (avec deux versions : en ligne et hors ligne) permettant de tester et de valider l'efficacité de notre approche.
Afin de valider notre proposition, le prototype RDF2NormRDF a été utilisé avec une batterie de tests.
Nos résultats expérimentaux ont montré des mesures très encourageantes par rapport aux approches existantes, notamment vis-à-vis du temps de chargement ou bien du stockage d'une description normalisée, tout en préservant le maximum d'informations.
Certaines applications du traitement automatique des langues sont amenées à traiter des flux de données textuelles caractérisés par l'emploi d'un vocabulaire en perpétuelle évolution, que ce soit au niveau de la création des mots que des sens de ceux existant déjà.
En partant de ce constat, nous avons mis au point un algorithme incrémental pour construire automatiquement et faire évoluer une base lexicale qui répertorie des unités lexicales non étiquetées sémantiquement observées dans des flux.
Cette représentation est complétée par une modélisation vectorielle visualisable qui tient compte des aspects continus du sens et de la proximité sémantique entre concepts.
Ce modèle est alors exploité pour propager l'étiquetage manuel d'un petit nombre d'entités nommées (EN : unités lexicales qui se référent habituellement à des personnes, des lieux, des organisations...) à d'autres EN non étiquetées observées dans un flux pendant la construction incrémentale du treillis.
Les concepts de ce treillis sont enrichis avec les étiquettes d'EN observées dans un corpus d'apprentissage.
Ces concepts et leurs étiquettes attachées sont respectivement employés pour l'annotation non supervisée et la classification supervisée des EN d'un corpus de test.
La modélisation de données séquentielles est utile à de nombreux domaines : reconnaissance de parole, de gestes, d'écriture, ou encore la synthèse d'animations pour des avatars virtuels.
Notre modélisation part du constat qu'une part importante de la variabilité entre les séquences d'observations peut être la conséquence de quelques variables contextuellesfixes le long de la séquence ou qui varient en fonction du temps.
Une phrase peut être exprimée différemment en fonction de l'humeur du locuteur, un geste peut être plus ample en fonction de la taille de l'acteur etc...
Cette méthode réalise du partage d'information entre les classes la ou les approches génératives apprennent des modèles de classes indépendants.
La structure d'un réseau de neurones détermine dans une large mesure son coût d'entraînement et d'utilisation, ainsi que sa capacité à apprendre.
Ces deux aspects sont habituellement en compétition : plus un réseau de neurones est grand, mieux il remplira la tâche qui lui a été assignée, mais plus son entraînement nécessitera des ressources en mémoire et en temps de calcul.
Dans ce contexte, des réseaux de neurones aux structures variées doivent être entraînés, ce qui nécessite un nouveau jeu d'hyperparamètres d'entraînement à chaque nouvelle structure testée.
L'objectif de la thèse est de traiter différents aspects de ce problème.
La première contribution est une méthode d'entraînement de réseau qui fonctionne dans un vaste périmètre de structures de réseaux et de tâches à accomplir, sans nécessité de régler le taux d'apprentissage.
La deuxième contribution est une technique d'entraînement et d'élagage de réseau, conçue pour être insensible à la largeur initiale de celui-ci.
La dernière contribution est principalement un théorème qui permet de traduire une pénalité d'entraînement empirique en a priori bayésien, théoriquement bien fondé.
Ce travail résulte d'une recherche des propriétés que doivent théoriquement vérifier les algorithmes d'entraînement et d'élagage pour être valables sur un vaste ensemble de réseaux de neurones et d'objectifs.
Les services géolocalisés (LBS) sont destinés à délivrer de l'information adéquate aux utilisateurs quelque soit le temps et l'endroit et ceci en se basant sur leur profil, contexte et position géographique.
En général, ces données sont stockées de plusieurs bases de données géographiques (BDG) dans le monde entier.
D'autre part, le nombre croissant des différentes BDG couvrant la même zone géographique et la récupération des données/métadonnées non erronées pour un service quelconque, impliquent de nombreux raisonnements et de contrôles d'accès aux BDG afin de résoudre les ambiguïtés dues à la présence des objets homologues dupliqués sur l'écran mobile.
Donc, mon but ultime sera de générer automatiquement une carte unique intégrant plusieurs interfaces des fournisseurs sur laquelle les objets homologues seront intégrés avant de les visualiser sur l'écran mobile.
Nos nouveaux concepts, basés sur certains algorithmes de fusion, sur l'ontologie pour assurer l'intégration au niveau sémantique et cartographique, sur l'orchestration des géo web services, sont implémentés dans des prototypes modulaires et évalués.
L'accès aux informations pertinentes, adaptées aux besoins et au profil de l'utilisateur est un enjeu majeur dans le cadre actuel caractérisé par une prolifération massive des ressources d'information hétérogènes.
En effet, nous proposons une approche de recommandation contextuelle et proactive dans un environnement mobile qui permet de recommander des informations pertinentes à l'utilisateur sans attendre à ce que ce dernier initie une interaction.
Cela permettra de réduire les efforts, le temps et l'interaction de l'utilisateur avec son appareil mobile et de présenter les informations pertinentes au bon moment et au bon endroit.
Cette approche prend aussi en considération les situations où la recommandation pourrait déranger l'utilisateur.
Il s'agit d'équilibrer le processus de recommandation contre les interruptions intrusives.
En effet, il existe différents facteurs et situations qui rendent l'utilisateur moins ouvert aux recommandations.
Comme nous travaillons dans le contexte des appareils mobiles, nous considérons que les applications mobiles telles que la caméra, le clavier, l'agenda, etc., sont de bons représentants de l'interaction de l'utilisateur avec son appareil puisqu'ils représentent en quelque sorte la plupart des activités qu'un utilisateur pourrait entreprendre avec son appareil mobile au quotidien, comme envoyer des messages, converser, tweeter, naviguer ou prendre des photos.
La motivation principale de cette thèse est de proposer un système de recommandation personnalisé pour les plateformes d'informations.
Pour cela, nous avons démontré que les opinions peuvent constituer un descripteur efficace pour améliorer la qualité de la recommandation.
Au cours de cette thèse, nous avons abordé ce problème en proposant trois contributions principales.
Le modèle de profil proposé repose sur trois éléments : les entités nommées, les aspects et les sentiments.
Deuxièmement, nous avons proposé une approche de classement des opinions permettant de filtrer et sélectionner seulement les opinions pertinentes.
Les résultats montrent que notre approche surpasse deux approches récemment proposées pour le classement des opinions.
Les résultats montrent que l'enrichissement des contenus des articles de presse
La structure discursive d'un texte est un élément essentiel à la compréhension du contenu véhiculé par ce texte.
Elle affecte, par exemple, la structure temporelle du texte, ou encore l'interprétation des expressions anaphoriques.
Dans cette thèse, nous aborderons les effets de la structure discursive sur l'analyse de sentiments.
L'analyse des sentiments est un domaine de recherche extrêmement actif en traitement automatique des langues.
Devant l'abondance de données subjectives disponibles, l'automatisation de la synthèse des multiples avis devient cruciale pour obtenir efficacement une vue d'ensemble des opinions sur un sujet donné.
La plupart des travaux actuels proposent une analyse des opinions au niveau du document ou au niveau de la phrase en ignorant la structure discursive.
Dans cette thèse, nous nous plaçons dans le contexte de la théorie de la SDRT (Segmented Discourse Representation Theory) et proposons de répondre aux questions suivantes :
- Existe-t-il un lien entre la structure discursive d'un document et les opinions émises dans ce même document ?
- Quel est le rôle des relations de discours dans la détermination du caractère objectif ou subjectif d'un segment textuel ?
- Quel est l'impact de la structure discursive lors de la détermination de l'opinion globale véhiculée dans un document ?
- Est-ce qu'une approche basée sur le discours apporte une réelle valeur ajoutée comparée à une approche classique basée sur la notion de 'sacs de mots' ?
Le traitement d'événements complexes (Complex Event Processing – CEP) consiste en l'analyse de flux de données afin d'en extraire des motifs et comportements particuliers décrits, en général, dans un formalisme logique.
Dans l'approche classique, les données d'un flux – ou événements – sont supposées être l'observation complète et parfaite du système produisant ces événements.
Cependant, dans de nombreux cas, les moyens permettant la collecte de ces données, tels que des capteurs, ne sont pas pour autant infaillibles et peuvent manquer la détection d'un événement particulier ou au contraire en produire.
Dans cette thèse, nous nous sommes employé à étudier les modèles possibles de représentation de l'incertain et, ainsi, offrir au CEP une robustesse vis-à-vis de cette incertitude ainsi que les outils nécessaires pour permettre la reconnaissance de comportement complexe de façon pertinente les flux d'événements en se basant sur le formalisme des chroniques.
Dans cette optique, trois approches ont été considérées.
La première se base sur les réseaux logiques de Markov pour représenter la structure des chroniques sous un ensemble de formules logiques adjointe dune valeur de confiance.
Nous montrons que ce modèle, bien que largement appliqué dans la littérature, est inapplicable pour une application concrète au regard des dimensions d'un tel problème.
La seconde approche se basent sur des techniques issues de la communauté SAT pour énumérer l'ensemble des solutions possibles d'un problème donné et ainsi produire une valeur de confiance pour la reconnaissance dune chronique exprimée, encore une fois, sous une requête logique.
Finalement, nous proposons une dernière approche basée sur les chaînes de Markov pour produire un ensemble d'échantillons expliquant l'évolution du modèle en accord avec les données observées.
Ces échantillons sont ensuite analysés par en système de reconnaissance pour compter les occurrences dune chronique particulière.
Le Transport Optimal régularisé par l'Entropie (TOE) permet de définir les Divergences de Sinkhorn (DS), une nouvelle classe de distance entre mesures de probabilités basées sur le TOE.
Celles-ci permettent d'interpoler entre deux autres distances connues : le Transport Optimal (TO) et l'Ecart Moyen Maximal (EMM).
Les DS peuvent être utilisées pour apprendre des modèles probabilistes avec de meilleures performances que les algorithmes existants pour une régularisation adéquate.
Rendre les analyseurs sémantiques robustes aux variations lexicales et stylistiques est un véritable défi pour de nombreuses applications industrielles.
De nos jours, l'analyse sémantique nécessite de corpus annotés spécifiques à chaque domaine afin de garantir des performances acceptables.
Les techniques d'apprenti-ssage par transfert sont largement étudiées et adoptées pour résoudre ce problème de manque de robustesse et la stratégie la plus courante consiste à utiliser des représentations de mots pré-formés.
Cependant, les meilleurs analyseurs montrent toujours une dégradation significative des performances lors d'un changement de domaine, mettant en évidence la nécessité de stratégies d'apprentissage par transfert supplémentaires pour atteindre la robustesse.
Ce travail propose une nouvelle référence pour étudier le problème de dépendance de domaine dans l'analyse sémantique.
Nous utilisons un nouveau corpus annoté pour évaluer les techniques classiques d'apprentissage par transfert et pour proposer et évaluer de nouvelles techniques basées sur les réseaux antagonistes.
Toutes ces techniques sont testées sur des analyseurs sémantiques de pointe.
Nous affirmons que les approches basées sur les réseaux antagonistes peuvent améliorer les capacités de généralisation des modèles.
Nous testons cette hypothèse sur différents schémas de représentation sémantique, langages et corpus, en fournissant des résultats expérimentaux à l'appui de notre hypothèse.
Cette thèse a pour but de contribuer à améliorer les interfaces Homme-machine.
En particulier, nos appareils devraient répliquer notre capacité à traiter continûment des flux d'information.
Cependant, le domaine de l'apprentissage statistique dédié à la reconnaissance de séries temporelles pose de multiples défis.
Nos travaux utilisent la reconnaissance de gestes comme exemple applicatif, ces données offrent un mélange complexe de poses corporelles et de mouvements, encodées sous des modalités très variées.
Pour ce faire, nous avons implémenté un environnement de test partagé qui est plus favorable à une étude comparative équitable.
Nous proposons des ajustements sur les fonctions de coût utilisées pour entraîner les réseaux de neurones et sur les expressions du modèle hybride afin de gérer un large déséquilibre des classes de notre base d'apprentissage.
Bien que les publications récentes semblent privilégier l'architecture BD
Dans un second temps, nous présentons une étude de l'apprentissage dit « en un coup » appliqué aux gestes.
Ce paradigme d'apprentissage gagne en attention mais demeure peu abordé dans le cas de séries temporelles.
Nous proposons une architecture construite autour d'un réseau de neurones bidirectionnel.
Son efficacité est démontrée par la reconnaissance de gestes isolés issus d'un dictionnaire de langage des signes.
À partir de ce modèle de référence, nous proposons de multiples améliorations inspirées par des travaux dans des domaines connexes, et nous étudions les avantages ou inconvénients de chacun
Les développeurs sont impatients de créer diverses applications Web pour répondre à la demande croissante des gens.
Pour construire une application Web, les développeurs doivent connaître quelques technologies de programmation de base.
De plus, ils préfèrent utiliser certains composants tiers (tels que les bibliothèques côté serveur, côté client, services REST) dans les applications web.
En incluant ces composants, ils pourraient bénéficier de la maintenabilité, de la réutilisabilité, de la lisibilité et de l'efficacité.
Dans cette thèse, nous proposons d'aider les développeurs à utiliser des composants tiers lorsqu'ils créent des applications web.
Nous présentons trois obstacles lorsque les développeurs utilisent les composants tiers : Quelles sont les meilleures bibliothèques JavaScript à utiliser ?
Comment obtenir les spécifications standard des services REST ?
Comment s'adapter aux changements de données des services REST ?
C'est pourquoi nous présentons trois approches pour résoudre ces problèmes.
Ces approches ont été validées par plusieurs études de cas et données industrielles.
Nous décrivons certains travaux futurs visant à améliorer nos solutions et certains problèmes de recherche que nos approches peuvent cibler.
L'Optimisation Combinatoire (OC) est un domaine de recherche qui est en perpétuel changement.
Résoudre un problème d'optimisation combinatoire (POC) consiste essentiellement à trouver la ou les meilleures solutions dans un ensemble des solutions réalisables appelé espace de recherche qui est généralement de cardinalité exponentielle en la taille du problème.
Pour résoudre des POC, plusieurs méthodes ont été proposées dans la littérature.
On distingue principalement les méthodes exactes et les méthodes d'approximation.
Ne pouvant pas viser une résolution exacte de problèmes NP-Complets lorsque la taille du problème dépasse une certain seuil, les chercheurs on eu de plus en plus recours, depuis quelques décennies, aux algorithmes dits hybrides (AH) ou encore à au calcul parallèle.
Dans cette thèse, nous considérons la classe POC des problèmes de conception d'un réseau fiable.
Nous présentons un algorithme hybride parallèle d'approximation basé sur un algorithme glouton, un algorithme de relaxation Lagrangienne et un algorithme génétique, qui produit des bornes inférieure et supérieure pour les formulations à base de flows.
Afin de valider l'approche proposée, une série d'expérimentations est menée sur plusieurs applications : le Problème de conception d'un réseau k-arête-connexe avec contrainte de borne (kHNDP) avec L = 2,3, le problème de conception d'un réseau fiable Steiner k-arête-connexe (SkESNDP) et ensuite deux problèmes plus généraux, à savoir le kHNDP avec L >= 2 et le problème de conception d'un réseau fiable k-arête-connexe (kESNDP).
L'étude expérimentale de la parallélisation est présentée après cela.
Dans la dernière partie de ce travail, nous présentons deux algorithmes parallèles exactes : un Branch-and-Bound distribué et un Branch-and-Cut distribué.
Une série d'expérimentation a été menée sur une grappe de 128 processeurs, et des accélération intéressantes ont été atteintes pour la résolution du problèmes kHNDP avec k=3 et L=3.
De nombreuses substances chimiques sont utilisées par l'industrie cosmétique pour entrer dans la composition de formules.
En dehors de la nécessité d'évaluer leur efficacité, l'industrie cosmétique se doit surtout d'évaluer la sécurité de leurs substances pour l'humain.
L'évaluation toxicologique des substances chimiques est réalisée dans le but de révéler un effet toxique potentiel de la substance testée.
Parmi les effets potentiels que l'on souhaite détecter, la toxicité du développement (tératogénicité), c'est-à-dire la capacité d'une substance à provoquer l'apparition d'anomalies lors du développement embryonnaire, est fondamentale.
En accord avec les législations internationales qui interdisent à l'industrie cosmétique d'avoir recours à des tests sur animaux de laboratoire pour l'évaluation de leurs substances, l'évaluation toxicologique de ces substances se base sur les résultats de tests in silico, in vitro et de tests faits sur des modèles alternatifs aux animaux de laboratoire.
Pour le moment cependant, peu de méthodes alternatives existent et ont été validées pour la toxicologie du développement.
Le développement de nouvelles méthodes alternatives est donc requis.
D'autre part, en plus de l'évaluation de la sécurité des substances chez l'humain, l'évaluation de la toxicité pour l'environnement est nécessaire.
L'usage de la plupart des produits cosmétiques et d'hygiène corporelle conduit, après lavage et rinçage, à un rejet à l'égout et donc dans les cours d'eau.
Il en résulte que les environnements aquatiques (eaux de surface et milieux marins côtiers) sont parfois exposés aux substances chimiques incluses dans les formules cosmétiques.
Ainsi, l'évaluation toxicologique environnementale des cosmétiques et de leurs ingrédients nécessite de connaître leur toxicité sur des organismes représentatifs de chaînes alimentaires aquatiques.
Dans ce contexte, le modèle embryon de poisson présente un double avantage pour l'industrie cosmétique.
Il est donc pertinent pour évaluer la toxicité environnementale des substances chimiques.
D'autre part, ce modèle apparaît prometteur pour évaluer l'effet tératogène de substances chimiques chez l'humain.
Pour ces raisons, un test d'analyse de la tératogénicité des substances chimiques est actuellement développé.
Ce test se base sur l'analyse d'embryons de medaka (Oryzias Latipes) à 9 jours post fertilisation, après exposition des embryons par balnéation à des substances à concentrations déterminées.
L'analyse de paramètres fonctionnels et morphologiques conduit au calcul d'un indice tératogène, qui permet de tirer une conclusion quant à l'effet tératogène de la substance testée.
Cet indice est calculé à partir des mesures du taux de mortalité et du taux de malformations chez les embryons.
L'objectif de ce projet est d'automatiser le test d'analyse de la tératogénicité, par classification automatique des embryons faite à partir d'image et de vidéo.
Nous nous sommes ensuite concentrés sur deux types de malformations courantes qui sont les malformations axiales, et l'absence de vessie natatoire, en utilisant une méthode d'apprentissage automatique.
Cette analyse doit être complétée par l'analyse d'autres malformations et conduire à un calcul du taux de malformations et de l'indice tératogène pour la substance testée
Notre travail de recherche vise deux objectifs étroitement liés.
Le premier consiste à proposer une aide à l'évaluation des écrits scientifique et pour cause : le nombre de publication augmente, les limites entre les domaines deviennent floues, et il devient difficile de trouver des publications pertinentes si bien qu'un besoin pratique d'évaluation surgit.
Il s'agit aussi de trouver les moyens d'une aide à l'expertise, appuyée sur des indices discursifs permettant d'aider le lecteur à repérer les points clés d'une publication (phase préalable à une évaluation) notamment à travers l'identification du problème de recherche.
Ces indices qui annoncent la formulation du problème de recherche dans les articles scientifiques sont repérables sous forme de « formules de discours » .
Notre recherche ne s'étend pas sur la formulation de la problématique scientifique au vu de la complexité de cette notion et de la difficulté de la définir d'un point de vue de l'extraction d'information.
Nous proposons une modélisation de ces formules de discours que nous avons intégrée dans l'analyseur syntaxique Xerox Incremental Parser (XIP) sous forme de règles de reconnaissance.
Nous avons utilisé un corpus d'articles de recherche en sciences de l'éducation extraits du corpus Scientext pour y détecter ces formules de discours.
Le choix du domaine est motivé par ma participation au projet européen EERQI dont le but est de renforcer et d'améliorer la visibilité mondiale et la compétitivité de la recherche européenne en éducation.
Différentes approches méthodologiques ont été adoptées afin de procéder à une étude linguistique fine de ces formules dîtes de discours entre autres : l'analyse de discours (M. Pecman, 2004, K. Hyland, 2005, Á. Sándor, A. Kaplan, G. Randeau, 2006, D. Siepman, 2007, A. Tutin, 2007-2010), robust parsing (S. Aït-Mokhtar, J. Chanod, R. Roux, 2002).
Il s'agit donc de mettre en œuvre une approche applicative en vue de l'aide à la lecture experte à travers l'identification, la typologie et le fonctionnement des associations lexicales véhiculant le problème de recherche.
Cette thèse dont la problématique est partie de la polémique au niveau du nombre des affixes dans la langue fon, elle nous a conduits à nous demander si l'on peut procéder à la dérivation affixale sur les noms propres fon ?
Nous avons résolu cette problématique à travers les différentes parties qui constituent cette thèse.
Nous avons d'entrée, abordé la théorie de la "Commutation" dont l'application commence par l'identification des unités phonologiques, leur définition et leur classement en fonction de leurs traits oppositionnels, et contrastifs, et aussi comment ces unités se combinent entre elles.
Nous réalisons que dans langue fon, la composition et la dérivation sont aussi des facteurs de formation des noms et des verbes.
Notre démarche dans ce domaine a été de suivre un ordre logique en partant des unités les plus petites en allant vers les plus grandes : phonèmes, syllabes, mots phonologiques.
Un état de l'art de la lexicologie et de la lexicographie est fait.
Il reste néanmoins encore embrionnaire, malgré le fait qu'il ait été débuté avec la pénétration européenne, les œuvres des missionnaires, et la mise en œuvre de nouveaux chantiers à l'avènement de l'indépendance.
Une partie, traite de la morphologie dérivationnelle, c'est une des parties les plus importantes de la thèse.
Dans une approche onomastique, elle met en œuvre la dérivation de la nomenclature du nom des rois d'Abomey, la structure organisationnelle du pouvoir à la cour royale à laquelle s'ajoute la méthode matricielle de création de nouveau noms.
Dans le même orde d'idée, une étude syntaxico-morphologique du système de numération a été faite afin de facilter le comptage en langue fon.
A travers une analyse ethnolinguistique, nous avons traité à travers une typologie variée, les anthroponymes événementiels : choix des noms de personnes ayant trait à la vie, au sort, à la destinée, à la mort, à la famille, à la fécondité, à l'amitié et à la réussite, du nom des jours, des mois et les noms de personnes qui sont nées tel ou tel jour de la semaine, ces noms qui tiennent compte des réalités locales et ethnologiques.
Dans une perspective dynamique, après avoir fait le bilan de la lexicologie et de la lexicographie depuis la période mécanographique jusqu'au début d'informatisation, c'est-à-dire la lexicologie et l'automatisation, la dictionnairique pour aboutir à la création d'un dictionnaire étymologique bilingue dans les langues fon et français.
Les apports sont à la fois quantitatifs et qualitatifs, car, notre problématique ayant été résolue, nous avons ouvert une perspective vers l'informatisation des langues d'une part et d'autre part sur le problème d'émergence des langues nationales en tant que facteurs de développement pour répondre aux Objectifs du Millénaire pour le Développement (OMD).
L'expansion du média Internet pour le recrutement a entraîné ces dernières années la multiplication des canaux dédiés à la diffusion des offres d'emploi.
Dans un contexte économique où le contrôle des coûts est primordial, évaluer et comparer les performances des différents canaux de recrutement est devenu un besoin pour les entreprises.
Cette thèse a pour objectif le développement d'un outil d'aide à la décision destiné à accompagner les recruteurs durant le processus de diffusion d'une annonce.
Il fournit au recruteur la performance attendue sur les sites d'emploi pour un poste à pourvoir donné.
Nous proposons dans un second temps un algorithme prédictif de la performance des offres d'emploi, basé sur un système hybride de recommandation, adapté à la problématique de démarrage à froid.
Ce système, basé sur une mesure de similarité supervisée, montre des résultats supérieurs à ceux obtenus avec des approches classiques de modélisation multivariée.
La dystrophie myotonique (DM) est considérée comme l'une des maladies neuromusculaires les plus complexes.
Bien que les travaux de recherche de ces 30 dernières années aient permis de mieux comprendre les mécanismes moléculaires sous-jacents, la nature de l'anomalie génétique hors norme, son expression multisystémique et son large spectre clinique ne permettent pas, à l'heure actuelle, une prise en charge optimale des patients.
Mon travail a eu pour but d'approfondir les connaissances et de préciser l'histoire naturelle de cette maladie rare.
La première partie du manuscrit est consacrée à la présentation de l'observatoire DM-Scope, sur lequel s'appuie tout mon travail de thèse.
Après la description du concept, du fonctionnement et de la plateforme de recueil, les caractéristiques de la cohorte DM1, à partir de laquelle les analyses ont été réalisées, sont présentées : spectre clinique couvert, atteinte multisystémique, corrélations génotype/phénotype, interrelations entre les symptômes et comparaison à la dystrophie myotonique de type II (DM2).
Ensuite, dans une deuxième partie, nous abordons les avancées majeures obtenues dans les DM grâce à DM-Scope et aux analyses réalisées pendant ma thèse :
(i) précision de l'histoire naturelle de la maladie, notamment avec la proposition d'une nouvelle classification ;
(ii) mise en exergue de facteurs déterminants du phénotype comme le genre, la taille de la mutation ou les interrelations entre les symptômes.
Ces travaux ont conduit à des recommandations de soins, notamment pour la transition enfants-adultes mais aussi la validation de critères d'inclusion importants pour les essais cliniques comme le genre.
DM-Scope permet d'accéder à des échantillons biologiques pour des études de recherche fondamentale et valider de nouvelles approches thérapeutiques.
Il est aujourd'hui un leader à l'international et un outil incontournable dans la recherche translationnelle dans la DM.
Ce concept transférable à n'importe quelle autre population, peut être utilisé pour la prise en charge d'autres maladies rares.
Enfin, le développement d'un modèle de survie construit à partir de la cohorte DM de l'observatoire est présenté.
Ce modèle a trois spécificités :
(i) il est applicable en grande dimension, à des cas comme DM-Scope, où l'on a un nombre important de variables ;
(ii) il prend en compte les risques compétitifs, lorsque les patients sont exposés simultanément à plusieurs évènements.
Dans notre observatoire, l'étude des décès de cause respiratoire est biaisée sans la prise en compte des évènements concurrents tels que le décès de cause cardiaque ;
(iii) il modélise l'hétérogénéité entre les groupes de patients (effets centres), potentiellement due à une prise en charge différente.
L'analyse des données de DM-Scope nécessite cette spécificité issue des modèles à fragilité car l'observatoire est multicentrique (55 centres).
Le modèle est transférable et applicable à d'autres données car de plus en plus de bases sont de grandes dimensions, la majorité des analyses de survie ont une censure liée à la survenue de l'événement d'intérêt et les études multicentriques sont de plus en plus communes.
Le travail envisagé dans le cadre de la thèse vise à proposer une méthode et un outil pour mesurer et améliorer la qualité des modèles de processus métier.
L'originalité de l'approche est qu'elle vise non seulement la qualité syntaxique mais aussi la qualité sémantique et pragmatique en s'appuyant notamment sur les connaissances du domaine.
La classification se base sur un jeu de données étiquetées par un expert.
Plus le jeu de données est grand, meilleure est la performance de classification.
Pourtant, la requête à un expert peut parfois être coûteuse.
Le but de l'apprentissage actif est alors de minimiser le nombre de requêtes à l'expert.
Améliorer la précision de l'estimation nécessite d'annoter de nouvelles données.
Il y a donc un dilemme entre utiliser le budget d'annotations disponible pour améliorer la performance du classifieur selon l'estimation actuelle du critère ou pour améliorer la précision sur le critère.
Ce dilemme est bien connu dans le cadre de l'optimisation en budget fini sous le nom de dilemme entre exploration et exploitation.
Les solutions usuelles pour résoudre ce dilemme dans ce contexte font usage du principe d'Optimisme Face à l'Incertitude.
Dans cette thèse, nous montrons donc qu'il est possible d'adapter ce principe au problème d'apprentissage actif pour la classification.
Pour cela, plusieurs algorithmes ont été être développés pour des classifieurs de complexité croissante, chacun utilisant le principe de l'Optimisme Face à l'Incertitude, et leurs résultats ont été évalués empiriquement
Les lexiques bilingues sont des ressources particulièrement utiles pour la Traduction Automatique et la Recherche d'Information Translingue.
Leur construction manuelle nécessite une expertise forte dans les deux langues concernées et est un processus coûteux.
Plusieurs méthodes automatiques ont été proposées comme une alternative, mais elles qui ne sont disponibles que dans un nombre limité de langues et leurs performances sont encore loin derrière la qualité des traductions manuelles.
Notre travail porte sur l'extraction de ces lexiques bilingues à partir de corpus de textes parallèles et comparables, c'est à dire la reconnaissance et l'alignement d'un vocabulaire commun multilingue présent dans ces corpus.
L'avènement et la disponibilité de l'apprentissage automatique et de l'intelligence artificielle ont fait la une des journaux ces dernières années, ouvrant les portes à leur application dans divers secteurs tels que la banque, la finance, le médical, etc.
L'apprentissage automatique peut aider à prévenir la fraude et à améliorer la gestion des risques, les prévisions d'investissement et la prise de décision.
Les modèles d'apprentissage sont également devenus plus importants dans le secteur de la santé avec de nombreuses applications potentielles offrant des soins de haute qualité et rentables aux patients.
Les évolutions rapides des modèles d'apprentissage facilitent le développement d'outils de traitement du langage naturel (PNL) qui peuvent exploiter et manipuler d'énormes quantités de données.
Portée par les motivations susmentionnées, l'objectif principal de cette thèse de doctorat est de contribuer à de nouvelles méthodes d'apprentissage pour des systèmes complexes dynamiques dans le secteur de la santé.
Le candidat qui est employé par un centre médical en Arabie saoudite a accès à une grande quantité de données cliniques anonymes.
Il doit explorer une gamme d'outils, de techniques et de cadres d'IA et de les appliquer aux données cliniques afin d'extraire des connaissances précieuses et de générer des résultats prédictifs précis.
Ce processus comprend de nombreuses étapes :
Anonymisation des données
Nettoyage des données : suppression du bruit et détection des valeurs aberrantes
Création d'ensembles de données d'apprentissage, de validation et de test (équilibré ou déséquilibré)
Proposer et comparer différents modèles de langage tels que Bert, doc2vec, etc.
Ajout de nouvelles règles pour affiner les modèles de langage existants
Application d'algorithmes d'apprentissage automatique pour entraîner les modèles
Comparaison de la précision et des performances des modèles formés
Les modèles formés peuvent être utilisés pour résoudre de nombreux problèmes de classification ou de prédiction.
L'un des principaux résultats de l'étude est un outil d'autodiagnostic explicable en clinique qui peut aider les médecins.
Cette thèse a pour objet l'analyse des dénominations monoréférentielles (DM) dénommant des référents singuliers ─ lieu, personne, événement, institution ou produit de l'activité humaine ─ dans un corpus établi
La théorie de l'information a influencé un grand nombre de domaines scientifiques depuis son introduction par Claude Shannon en 1948.
A part la loi de Fitts et la loi de Hick, qui sont apparus lorsque les psychologues expérimentaux étaient encore enthousiastes à l'idée d'appliquer la théorie de l'information aux différents domaines de la psychologie, les liens entre la théorie de l'information et l'interaction humain
Cette thèse démontre ainsi que la théorie de l'information peut être utilisée comme un outil unifié pour comprendre et concevoir la communication et l'interaction humain-machine.
Le présent travail se propose d'examiner, par une analyse contrastive, les positionnements qui ressortent d'une terminologie relative aux politiques publiques sécuritaires.
Cette recherche s'appuie sur une sélection de textes juridiques non contraignants, en français et en espagnol, publiés entre 2001 et 2018 par la Commission européenne et deux États membres : la France et l'Espagne.
Le choix d'analyser les actes émis par les autorités européennes et nationales découle de certaines spécificités qui caractérisent la production du discours institutionnel.
Les études récentes dans ce champ de recherche ont démontré que celles-ci tendent à favoriser une rhétorique consensuelle qui soit à même de désamorcer le débat politique.
Or, ces stratégies discursives comportent en elles-mêmes la trace de positionnements idéologiques précis.
L'observation de la circulation des termes, dans un contexte pluriel comme celui de l'UE, permet alors de détecter les discordances qui caractérisent les productions discursives plurielles concernant la sécurité commune.
Pour ce faire, nous avons adopté une approche théorique qui articule la terminologie à certaines notions de l'analyse du discours « à la française » (ADF).
La terminologie place le terme, à savoir l'unité lexicale utilisée dans un domaine spécialisé de la connaissance, au centre de sa réflexion.
Nous focalisons donc notre étude sur la valeur que le lexique acquiert lorsqu'il est prononcé par une autorité légitime – l'institution – dans le secteur spécifique des politiques sécuritaires.
La recherche terminologique a progressivement montré que les termes, comme les unités lexicales, sont liés au contexte d'utilisation et aux conditions de production du discours dans lequel ils s'insèrent.
Les variantes dénominatives, qui émergent des sous-corpus, dépendent donc du contexte linguistique et extralinguistique qui entoure l'utilisation du terme.
À partir de cet arrière-plan, nous nous sommes demandé si les variantes pouvaient être le symptôme de positions idéologiques discordantes.
En ce sens, l'ADF, qui s'intéresse traditionnellement aux idéologies sous-jacentes au langage, nous a fourni les notions nécessaires pour comprendre les raisons pouvant expliquer la variation d'un terme.
L'approche méthodologique nous a permis de combiner une analyse lexicométrique du corpus à une observation détaillée du terme dans son contexte, dans les reprises intertextuelles et dans les sources terminographiques.
Nos résultats sont présentés après un parcours d'analyse qui commence, selon une méthode déductive, par le choix de certains termes : « prévention » , « détection » , « répression » , « combattant terroriste étranger » et « criminalité transfrontalière » .
Ces derniers ont été sélectionnés sur la base de recherches menées en amont dans la littérature des relations internationales et sont représentatifs de certaines tensions qui alimentent le débat académique, politique et juridique.
Il s'agit, d'une part, d'observer les termes concernant les actions stratégiques ( « prévention » , « détection » , « répression » ) et, d'autre part, de réfléchir à la conceptualisation de la menace et de l'ennemi ( « combattant terroriste étranger » et « criminalité transfrontalière » ).
En conclusion, notre travail vise à observer les décalages et les ouvertures interprétatives qui se créent lorsque des termes circulent et sont utilisés pour légitimer des pratiques discursives.
La thèse montre que le discours institutionnel sur la sécurité finit par occulter les débats qui pourtant sont bien présents et qui devraient donc être explicitement inclus dans l'espace public
L'augmentation massive du volume de données générées chaque jour par les individus sur Internet offre aux chercheurs la possibilité d'aborder la question de la prédictibilité des marchés financiers sous un nouvel angle.
Sans prétendre apporter une réponse définitive au débat entre les partisans de l'efficience des marchés et les chercheurs en finance comportementale, cette thèse vise à améliorer notre compréhension du processus de formation des prix sur les marchés financiers grâce à une approche Big Data.
En examinant les caractéristiques propres à chaque utilisateur (niveau d'expérience, approche d'investissement, période de détention), cet essai fournit des preuves empiriques montrant que le comportement des investisseurs naïfs, sujets à des périodes d'excès d'optimisme ou de pessimisme
Le deuxième essai propose une méthodologie permettant de mesurer l'attention des investisseurs aux informations en temps réel, en combinant les données des médias traditionnels avec le contenu des messages envoyés par une liste d'experts sur la plateforme Twitter.
Cet essai démontre que lorsqu'une information attire l'attention des investisseurs, les mouvements de marchés sont caractérisés par une forte hausse des volumes échangés, une hausse de la volatilité et des sauts de prix.
Le troisième essai étudie le risque de manipulation informationnelle en examinant un nouveau jeu de données de messages publiés sur Twitter à propos des entreprises de petite capitalisation.
Cet essai propose une nouvelle méthodologie permettant d'identifier les comportements anormaux de manière automatisée en analysant les interactions entre les utilisateurs.
Ce travail s'inscrit dans la terminologie conceptuelle et vise à modéliser les connaissances (concepts et termes dénotant ceux-ci) dans la balance des paiements et la position extérieure.
Pour ce faire, des outils de traitement automatique de la langue (TAL) et de développement d'ontoterminologies seront mis en œuvre ainsi que des échanges avec les experts du domaine.
Les ontoterminologies élaborées seront exportées dans des formats d'échanges compréhensibles par un ordinateur, en particulier ceux du W3C.
Enfin, cette thèse vise à constituer un dictionnaire électronique facilitant la transmission des connaissances du domaine à des non-experts.
Avec l'augmentation du nombre de capteurs et d'actuateurs dans les avions et le développement de liaisons de données fiables entre les avions et le sol, il est devenu possible d'améliorer la sécurité et la fiabilité des systèmes à bord en appliquant des techniques d'analyse en temps réel.
Cependant, étant donné la disponibilité limité des ressources de calcul embarquées et le coût élevé des liaisons de données, les solutions architecturelles actuelles ne peuvent pas exploiter pleinement toutes les ressources disponibles, limitant leur précision.
Notre but est de proposer un algorithme distribué de prédiction de panne qui pourrait être exécuté à la fois à bord de l'avion et dans une station au sol tout en respectant un budget de communication.
Dans cette approche, la station au sol disposerait de ressources de calcul rapides et de données historiques et l'avion disposerait de ressources de calcul limitées et des données de vol actuelles.
Dans cette thèse, nous étudierons les spécificités des données aéronautiques et les méthodes déjà existantes pour produire des prédictions de pannes à partir de ces dernières et nous proposerons une solution au problème posé.
Notre contribution sera détaillé en trois parties.
Premièrement, nous étudierons le problème de prédiction d'événements rares créé par la haute fiabilité des systèmes aéronautiques.
Beaucoup de méthodes d'apprentissage en classification reposent sur des jeux de données équilibrés.
Plusieurs approches existent pour corriger le déséquilibre d'un jeu de donnée et nous étudierons leur efficacité sur des jeux de données extrêmement déséquilibrés.
Deuxièmement, nous étudierons le problème d'analyse textuelle de journaux car de nombreux systèmes aéronautiques ne produisent pas d'étiquettes ou de valeurs numériques faciles à interpréter mais des messages de journaux textuels.
Nous étudierons les méthodes existantes basées sur une approche statistique et sur l'apprentissage profond pour convertir des messages de journaux textuels en une forme utilisable en entrée d'algorithmes d'apprentissage pour classification.
Nous proposerons notre propre méthode basée sur le traitement du langage naturel et montrerons comment ses performances dépassent celles des autres méthodes sur un jeu de donnée public standard.
Enfin, nous offrirons une solution au problème posé en proposant un nouvel algorithme d'apprentissage distribué s'appuyant sur deux paradigmes d'apprentissage existant, l'apprentissage actif et l'apprentissage fédéré.
Nous détaillerons notre algorithme, son implémentation et fournirons une comparaison de ses performances avec les méthodes existantes
Le neuroblastome est le cancer solide extra-cranial le plus fréquent chez l'enfant.
Il est caractérisé par une très grande hétérogénéité tant au niveau clinique que moléculaire.
Pour répondre à cette question, il convient d'identifier chez les patients ayant rechuté, les différentes populations clonales coexistant au diagnostic et/ou à la rechute.
Cela permet, entre autre, d'étudier les voies différemment altérées entre ces deux temps.
Sur ces données, l'application de notre méthode a permis d'identifier des différences dans le ratio de variants prédits fonctionnels par rapport à ceux prédits passagers entre les populations ancestrales, enrichies à la rechute ou appauvries à la rechute.
L'objectif de cette thèse est de formaliser une méthodologie visant à valider l'architecture d'un système d'aide à l'installation d'outils d'assistance pour le pilotage d'un fauteuil électrique.
Nous voulons redonner un début de mobilité suffisant à la personne embarquée, en utilisant des expériences de navigations dans des environnements virtuels.
Pour cela il faut concevoir un système capable de trouver les améliorations pertinentes à apporter au fauteuil électrique.
Nous avons élaboré une architecture basée sur des cycles "Expérience ? Analyse ? Modification du fauteuil roulant".
Cette architecture se décompose en trois modules : un simulateur, un évaluateur, un configurateur.
Ce travail s'inscrit dans le projet VAHM (développé au LASC) qui utilise des fauteuils électriques du commerce sur lesquels ont été ajoutés une série de capteurs et un calculateur.
Notre simulateur remplace la partie physique du VHAM.
A partir de l'analyse des données issues de l'étape d'expérience, nous avons calculé des critères représentatifs de comportements ou/et de situations particulières survenues lors de la navigation.
Nous avons répertorié, en qualité de roboticiens, l'ensemble des situations problématiques selon différents aspects.
Nous avons identifié et caractérisé chaque aspect par un ensemble de critères.
Le vecteur contenant les valeurs des critères constitue l'entrée d'un système d'aide à la décision qui indiquera en sortie la ou les fonctionnalités à installer pour améliorer la mobilité du patient en fauteuil électrique.
La base de connaissance du système d'aide à la décision s'appuie sur des réflexions, concernant les comportements et situations particulières qu'une personne handicapée va rencontrer pendant la navigation.
L'originalité de notre travail provient de l'application orientée handicap permettant d'évaluer et de mettre en évidence les difficultés d'un groupe de personnes très physiquement handicapées, ne pouvant pas se déplacer en fauteuil électrique dans des mondes réels, en utilisant des mises en situation dans des environnements virtuels.
L'architecture du système a permis de fournir des solutions d'assistance en passant par un système d'aide à la décision évolutif basé sur la logique floue.
De nos jours, les médias sociaux en ligne ont transformé notre façon de créer, de partager et d'accéder à l'information.
Ces plateformes reposent sur de gigantesques réseaux favorisent le libre échange d'informations entre des centaines de millions de personnes à travers le monde entier, et cela de manière instantanée.
Qu'ils soient en lien avec un évènement global ou en lien avec un évènement local, ces messages peuvent influencer une société et peuvent contenir des informations utiles pour la détection ou la prédiction de phénomènes du monde réel.
Cependant, certains messages diffusés peuvent avoir un impact très négatif dans la vie réelle.
Ces messages contenant une « infox » peuvent avoir des conséquences désastreuses.
Pour éviter et anticiper ces situations dramatiques, suivre les rumeurs, éviter les mauvaises réputations, il est nécessaire d'étudier puis de modéliser la propagation de l'information.
Or, la plupart des modèles de diffusion introduits reposent sur des hypothèses axiomatiques représentées par des modèles mathématiques.
Par conséquent, ces modèles sont éloignés des comportements de diffusion des utilisateurs dans la mesure où ils n'intègrent pas les observations faites sur des cas concrets de diffusion.
Dans nos travaux, nous étudions le phénomène de diffusion de l'information à deux échelles.
À une échelle microscopique, nous avons observé les comportements de diffusion selon des traits de personnalité des utilisateurs en analysant les messages qu'ils publient en termes de sentiments et d'émotions.
À une échelle macroscopique, nous avons analysé l'évolution du phénomène de diffusion en prenant en compte la dimension géographique des utilisateurs.
Les réseaux sociaux en ligne (OSNs) recueillent une masse de données à caractère privé.
Notre thèse propose certaines réponses.
Dans le premier chapitre nous analysons l'impact du partage des données personnelles de l'utilisateur sur sa vie privée.
Tout d'abord, nous montrons comment les intérêts d'un utilisateur--à titre d'exemple ses préférences musicales--peuvent être à l'origine de fuite d'informations sensibles.
Pour ce faire, nous inférons les attributs non divulgués du profil de l'utilisateur en exploitant d'autres profils partageant les même ''goûts musicaux''.
Notre approche extrait la sémantique des intérêts en utilisant Wikipedia, les partitionne sémantiquement et enfin regroupe les utilisateurs ayant des intérêts semblables.
Nos expérimentations réalisées sur plus de 104 milles profils publics collectés sur Facebook et plus de 2000 profils privés de bénévoles, montrent que notre technique d'inférence prédit efficacement les attributs qui sont très souvent cachés par les utilisateurs.
Dans un deuxième temps, nous exposons les conséquences désastreuses du partage des données privées sur la sécurité.
Nous nous focalisons sur les informations recueillies à partir de profils publics et comment celles-ci peuvent être exploitées pour accélérer le craquage des mots de passe.
Premièrement, nous proposons un nouveau « craqueur » de mot de passe basé sur les chaînes de Markov permettant le cassage de plus de 80% des mots de passe, dépassant ainsi toutes les autres méthodes de l'état de l'art.
Notre travail se base sur la plate-forme publicitaire d'estimationd'utilisateurs de Facebook pour calculer l'entropie de chaque attribut public.
Ce calcul permet d'évoluer l'impact du partage de ces informations publiquement.
Nos résultats, basées sur un échantillon de plus de 400 mille profils publics Facebook, montrent que la combinaison de sexe, ville de résidence et age permet d'identifier d'une manière unique environ 18% des utilisateurs.
Dans la deuxième section de notre thèse nous analysons les interactions entre la plate-forme du réseau social et des tiers et son impact sur à la vie privée des utilisateurs.
Nos résultats indiquent que le « tracking » utilisé par les OSNs couvre la quasi-totalité des catégories Web, indépendamment du contenu et de l'auditoire.
Finalement, nous développons une plate-forme de mesure pour étudier l'interaction entre les plates-formes OSNs, les applications sociales et les « tierces parties » (e.g., fournisseurs de publicité).
Nous démontrons que plusieurs applications tierces laissent filtrer des informations relatives aux utilisateurs à des tiers non autorisés.
Ce comportement affecte à la fois Facebook et RenRen avec une sévérité variable : 22 % des applications Facebook testées transmettent au moins un attribut à une entité externe.
L'apprentissage machine (Machine Learning) est actuellement utilisé et testé dans de nombreux secteurs.
L'utilisation de telles méthodes présente un intérêt évident pour la conception et la fabrication.
Des applications intéressantes de ces approches lors du processus de conception et de fabrication concernent la prise de décision pour un choix de technologie de fabrication, une optimisation des paramètres de conception et de fabrication ou encore un suivi dynamique du processus de fabrication.
Malheureusement, plusieurs barrières rendent difficiles leurs utilisations dans ces domaines spécifiques.
En effet, le Machine Learning dans les approches actuellement médiatisées (apprentissage profond, réseaux convolutifs) nécessite l'utilisation d'un grand nombre de données pour entraîner les modèles.
En conception et fabrication, la collecte d'une quantité suffisante de données est une opération parfois impossible, surtout lors des phases de préconception où la pièce et le produit n'existent pas encore.
Par ailleurs, si la collecte de données est possible pour certaines phases du processus, les modèles entrainés sont souvent spécifiques à certaines technologies ou géométries.
Par conséquent, ils sont difficilement généralisables.
Plus spécifiquement, comment se rapprocher des performances cognitives humaines pour apprendre à partir d'un nombre de cas réduits dans le domaine de la fabrication ?
Répondre à ces questions est essentielle pour permettre une application du Machine Learning aux secteurs de la conception et de la fabrication.
Résoudre ces problèmes permettrait de disposer de modèles d'apprentissage pour la conception et la fabrication capables de généraliser.
La prise de décisions concernant la conception, la technologie utilisée et la méthode de fabrication ainsi que l'optimisation dynamique des paramètres de fabrication sont des applications innovantes envisagées.
Au cours de cette thèse, plusieurs pistes de recherche seront explorées.
Toutes ces pistes considèrent le méta-apprentissage qui vise à se rapprocher des performances de l'apprentissage humain.
La combinaison d'un travail de développement de métriques, de modèles et de méthodes d'optimisations constitue la base de la stratégie de recherche.
Des métriques métiers capables de synthétiser la connaissance en conception et fabrication seront utilisées en s'appuyant sur la méthode DACM, une méthode de métamodélisation développée par l'université de Tampere.
Cette méthode permet de créer un graphe causal qui représente un processus via une représentation graphique des liens de causes à effets entre les variables du problème considéré.
La méthode DACM a été développée au cours d'une thèse en cotutelle précédente entre l'université Grenoble Alpes et l'université de Tampere.
L'exploitation de ces précédents travaux est possible au travers des réseaux Bayésiens qui peuvent être vus comme l'étape qui suit la génération de graphes causaux.
Ces réseaux Bayésiens permettent d'intégrer et découvrir de nouvelles corrélations et apportent une vision statistique du modèle causal.
Ces approches sont utilisées en traitement du signal, mais aussi dans les systèmes de recommandation ou dans les systèmes d'analyse du langage.
Ces méthodes ont démontré leurs performances dans les concours annuels organisés par Netflix.
Une première preuve de concept a été déjà réalisée dans le cadre de la collaboration UGA / Tampere et vient d'être soumise en journal.
Durant la thèse en cotutelle, l'objectif est d'explorer ces différentes approches pour des applications de prise de décision, de choix de paramètres optimaux de fabrication et conception et éventuellement si les progrès le permettent de contrôle en phase de fabrication.
Un plan ambitieux de publications en journal et conférences dans les domaines de la fabrication et de la conception mais aussi de l'informatique est prévu.
La thèse en cotutelle devrait être réalisée par compilation d'articles de journaux (4 minimum).
La modélisation des relations spatiales est essentielle dans une grande variété d'applications de réalité virtuelle, tel que des environnements d'apprentissage humain, des musées virtuels, des systèmes d'aides à la navigation.
Cependant, les relations spatiales ont été considérées comme des informations abstraites et donc, difficiles à spécifier.
Afin d'aborder cette question, cette thèse propose une approche pour modéliser les relations spatiales entre les objets virtuels dans des environnements de réalité virtuelle.
Nous formalisons un modèle formel des relations spatiales dédié aux environnements de réalité virtuelle.
Ensuite, nous proposons un langage et un cadre pour spécifier les relations spatiales à un niveau conceptuel.
Nous montrons que le langage proposé est une base pertinente pour spécifier des contraintes spatiales liées aux activités des agents et des utilisateurs dans les environnements de réalité virtuelle.
Cette thèse se situe à l'intersection de deux domaines de recherche scientifique la Reconnaissance d'
Alors que le second se focalise sur la résolution de problèmes d'
Parmi les problèmes difficiles existants en ROS, le problème de la distance d'édition entre graphes (DEG) a été sélectionné comme le cœur de ce travail.
Les contributions portent sur la conception de méthodes adoptées du domaine RO pour la résolution du problème de DEG.
Explicitement, des nouveaux modèles linéaires en nombre entiers et des matheuristiques ont été développé à cet effet et de très bons résultats ont été obtenus par rapport à des approches existantes.
L'une des difficultés d'une langue peu dotée est l'inexistence des services liés aux technologies du traitement de l'écrit et de l'oral.
Dans cette thèse, nous avons affronté la problématique de l'étude acoustique de la parole isolée et de la parole continue en Fongbe dans le cadre de la reconnaissance automatique de la parole.
La complexité tonale de l'oral et la récente convention de l'écriture du Fongbe nous ont conduit à étudier le Fongbe sur toute la chaîne de la reconnaissance automatique de la parole.
En plus des ressources linguistiques collectées (vocabulaires, grands corpus de texte, grands corpus de parole, dictionnaires de prononciation) pour permettre la construction des algorithmes, nous avons proposé une recette complète d'algorithmes (incluant des algorithmes de classification et de reconnaissance de phonèmes isolés et de segmentation de la parole continue en syllabe), basés sur une étude acoustique des différents sons, pour le traitement automatique du Fongbe.
Dans ce manuscrit, nous avons aussi présenté une méthodologie de développement de modèles accoustiques et de modèles du langage pour faciliter la reconnaissance automatique de la parole en Fongbe.
Dans cette étude, il a été proposé et évalué une modélisation acoustique à base de graphèmes (vu que le Fongbe ne dispose pas encore de dictionnaire phonétique) et aussi l'impact de la prononciation tonale sur la performance d'un système RAP en Fongbe.
Enfin, les ressources écrites et orales collectées pour le Fongbe ainsi que les résultats expérimentaux obtenus pour chaque aspect de la chaîne de RAP en Fongbe valident le potentiel des méthodes et algorithmes que nous avons proposés.
On constate actuellement un engouement général pour la recherche d'informations dans les textes, dû sans doute à la disponibilité d'une masse considérable d'informations sur le réseau internet.
D'où la nécessité des dictionnaires reliant étroitement sémantique et syntaxe.
Ils'agit des verbes, des noms et des adjectifs qui impliquent un "dire" et dont nous avons fait le recensement le plus exhaustif possible.
Les structures syntaxiques sont determinées par le contenu sémantique des prédicats, de sorte qu'il est patent que ces deux niveaux sont étroitement imbriqués.
Ces classes cependant déterminent une structure argumentale qui leur est propre.
Pour pouvoir restituer des informations qui correspondent aux besoins de l'utilisateur, les mécanismes d'adaptation doivent disposer de métadonnées sur celui-ci telles que ses caractéristiques personnelles, ses préférences générales, ses centres d'intérêt.
De ce fait, le profil utilisateur construit à partir de celles-ci devient central dans tout système basé sur la personnalisation.
Nous appelons les techniques ou processus associés à cette approche " profilage social ".
Le terme " profil social " désigne un profil construit à l'aide du réseau social de l'utilisateur.
Un profil social contient les métadonnées traduisant les intérêts de l'utilisateur extraits à partir des informations partagées par les individus de son réseau social.
Les intérêts de l'utilisateur évoluant au fil du temps dans la vie réelle, il en est de même pour ceux extraits depuis son réseau social : pertinents à un moment donné, ils peuvent ne plus être significatifs ultérieurement.
Pour prendre en compte l'évolution des intérêts dans le profil social, nous avons proposé d'améliorer l'efficacité des processus de construction du profil social existants en intégrant la prise en compte de l'évolution du réseau social de l'utilisateur.
Nous proposons d'intégrer un facteur temporel dans ces processus (approche basée sur des individus et approche basée sur les communautés).
La solution permet de privilégier les intérêts provenant d'informations significatives et à jour.
Il s'agit donc d'intégrer une mesure temporelle dans l'étape d'extraction et pondération des intérêts.
Cette mesure est calculée d'une part, à partir de la pertinence temporelle des informations utilisées pour extraire cet intérêt et d'autre part, à partir de la pertinence temporelle de l'individu qui partage ces informations.
Nous mettons en œuvre la méthode proposée au travers d'expérimentations dans deux réseaux sociaux différents : DBLP, un réseau de publications scientifiques et Twitter, un réseau de micro-blogs.
Les résultats de ces expérimentations nous ont permis de montrer l'efficacité de la méthode temporelle proposée par rapport aux processus de construction du profil social qui ne prennent pas en compte des critères temporels.
En étudiant les résultats en fonction des techniques de pondération des intérêts ou fonctions temporelles utilisées, nous constatons que la fonction temporelle et la technique utilisées donnant les meilleurs résultats varient selon l'approche de construction du profil social choisie, selon la taille et la densité du réseau étudié mais aussi selon sur le type de réseau.
Cette thèse porte sur la construction automatique d'outils et de ressources pour l'analyse linguistique de textes des langues peu dotées.
Nous proposons une approche utilisant des réseaux de neurones récurrents (RNN-Recurrent Neural Networks) et n'ayant besoin que d'un corpus parallèle ou mutli-parallele entre une langue source bien dotée et une ou plusieurs langues cibles moins bien ou peu dotées.
Ce corpus parallèle ou mutli-parallele est utilisé pour la construction d'une représentation multilingue des mots des langues source et cible.
Nous avons utilisé cette représentation multilingue pour l'apprentissage de nos modèles neuronaux et nous avons exploré deux architectures neuronales : les RNN simples et les RNN bidirectionnels.
Nous avons aussi proposé plusieurs variantes des RNN pour la prise en compte d'informations linguistiques de bas niveau (informations morpho-syntaxiques) durant le processus de construction d'annotateurs linguistiques de niveau supérieur (SuperSenses et dépendances syntaxiques).
Nous avons démontré la généricité de notre approche sur plusieurs langues ainsi que sur plusieurs tâches d'annotation linguistique.
Notre approche a les avantages suivants : (a) elle n'utilise aucune information d'alignement des mots, (b) aucune connaissance concernant les langues cibles traitées n'est requise au préalable (notre seule supposition est que, les langues source et cible n'ont pas une grande divergence syntaxique), ce qui rend notre approche applicable pour le traitement d'un très grand éventail de langues peu dotées, (c) elle permet la construction d'annotateurs multilingues authentiques (un annotateur pour N langages).
Ma thèse a pour but l'étude des désignations nominales des événements pour l'extraction automatique.
Mes travaux s'inscrivent en traitement automatique des langues, soit dans une démarche pluridisciplinaire qui fait intervenir linguistique et informatique.
L'extraction d'information a pour but d'analyser des documents en langage naturel et d'en extraire les informations utiles à une application particulière.
Dans ce but général, de nombreuses campagnes d'extraction d'information ont été menées~ : pour chaque événement considéré, il s'agit d'extraire certaines informations relatives (participants, dates, nombres, etc.).
Pourtant, ces travaux ne s'intéressent que peu aux mots utilisés pour décrire l'événement (particulièrement lorsqu'il s'agit d'un nom).
L'événement est vu comme un tout englobant, comme la quantité et la qualité des informations qui le composent.
Contrairement aux travaux en extraction d'informations générale, notre intérêt principal est porté uniquement sur la manière dont sont nommés les événements qui se produisent et particulièrement à la désignation nominale utilisée.
Pour nous, l'événement est ce qui arrive, ce qui vaut la peine qu'on en parle.
Les événements plus importants font l'objet d'articles de presse ou apparaissent dans les manuels d'Histoire.
Un événement peut être évoqué par une description verbale ou nominale.
Dans cette thèse, nous avons réfléchi à la notion d'événement.
Nous avons observé et comparé les différents aspects présentés dans l'état de l'art jusqu'à construire une définition de l'événement et une typologie des événements en général, et qui conviennent dans le cadre de nos travaux et pour les désignations nominales des événements.
Nous avons aussi dégagé de nos études sur corpus différents types de formation de ces noms d'événements, dont nous montrons que chacun peut être ambigu à des titres divers.
Pour toutes ces études, la composition d'un corpus annoté est une étape indispensable, nous en avons donc profité pour élaborer un guide d'annotation dédié aux désignations nominales d'événements.
Nous avons étudié l'importance et la qualité des lexiques existants pour une application dans notre tâche d'extraction automatique.
Nous avons aussi, par des règles d'extraction, porté intérêt au cotexte d'apparition des noms pour en déterminer l'événementialité.
À la suite de ces études, nous avons extrait un lexique pondéré en événementialité (dont la particularité est d'être dédié à l'extraction des événements nominaux), qui rend compte du fait que certains noms sont plus susceptibles que d'autres de représenter des événements.
Utilisée comme indice pour l'extraction des noms d'événements, cette pondération permet d'extraire des noms qui ne sont pas présents dans les lexiques standards existants.
Enfin, au moyen de l'apprentissage automatique, nous avons travaillé sur des traits d'apprentissage contextuels en partie fondés sur la syntaxe pour extraire de noms d'événements.
Dans le but de faciliter la tâche d'évaluation du niveau de sécurité incendie aux ingénieurs et permettre aux spécialistes impliqués dans le domaine d'utiliser leurs langages et outils préférés, nous proposons de créer un langage dédié au domaine de la sécurité incendie générant automatiquement une simulation en prenant en considération les langages métiers utilisés par les spécialistes intervenants dans le domaine.
Ce DSL nécessite la définition, la formalisation, la composition et l'intégration de plusieurs modèles, par rapport aux langages spécifiques utilisés par les spécialistes impliqués dans le domaine.
Le langage spécifique dédié au domaine de la sécurité incendie est conçu par composition et intégration de plusieurs autres DSLs décrits par des langages techniques et naturels (ainsi que des langages naturels faisant référence à des langages techniques).
Ces derniers sont modélisés de manière à ce que leurs composants soient précis et fondés sur des bases mathématiques permettant de vérifier la cohérence du système (personnes et matériaux sont en sécurité) avant sa mise en œuvre.
Dans ce contexte, nous proposons d'adopter une approche formelle, basée sur des spécifications algébriques, pour formaliser les langages utilisés par les spécialistes impliqués dans le système de génération, en se concentrant à la fois sur les syntaxes et les sémantiques des langages dédiés.
Dans l'approche algébrique, les concepts du domaine sont abstraits par des types de données et les relations entre eux.
La sémantique des langages spécifiques est décrite par les relations, le mapping (correspondances) entre les types de données définis et leurs propriétés.
Le langage de simulation est basé sur un langage conçu par la composition de plusieurs DSL spécifiques précédemment décrits et formalisés.
Les différents DSLs sont implémentés en se basant sur les concepts de la programmation fonctionnelle et le langage fonctionnel Haskell bien adapté à cette approche.
Le résultat de ce travail est un outil informatique dédié à la génération automatique de simulation, dans le but de faciliter la tâche d'évaluation du niveau de sécurité incendie aux ingénieurs.
Nous proposons un modèle de vérification grammaticale automatique gauche-droite issu de l'analyse d'un corpus d'erreurs tapuscrites.
Les travaux menés en psychologie cognitive ont montré que le processus de révision procède au travers de la confrontation d'une attente à un résultat.
Ainsi, la détection d'une erreur grammaticale reposerait, chez l'humain, sur une attente du réviseur non comblée.
Ce principe est à la base du modèle que nous avons élaboré.
Pour faciliter la gestion des attentes du point de vue traitement numérique, nous convions deux concepts courants en TAL : le principe d'unification et la segmentation en chunks.
Le premier est particulièrement adapté à la vérification des accords et le second constitue une unité de calcul intermédiaire permettant de définir des bornes simplifiant la recherche d'incohérences grammaticales.
Enfin, l'originalité de ce modèle réside dans une analyse gauche-droite construite au fur et à mesure de la lecture/écriture.
Cette thèse se situe dans le contexte des modèles logique de Recherche d'Information (RI).
Cependant, en étudiant les modèles actuels de RI basés sur la logique, nous montrons que ces modèles ont généralement des lacunes.
Premièrement, les modèles de RI logiques proposent normalement des représentations complexes de document et des requête et difficile à obtenir automatiquement.
Deuxièmement, la décision de pertinence d-&gt;q, qui représente la correspondance entre un document d et une requête q, pourrait être difficile à vérifier.
Enfin, la mesure de l'incertitude U(d-&gt;q) est soit ad-hoc ou difficile à mettre en oeuvre.
Dans cette thèse, nous proposons un nouveau modèle de RI logique afin de surmonter la plupart des limites mentionnées ci
Nous représentons les documents et les requêtes comme des phrases logiques écrites en Forme Normale Disjonctive.
Nous argumentons également que la décision de pertinence d-&gt;q pourrait être remplacée par la validité de l'implication matérielle.
Pour vérifier si d-&gt;q est valide ou non, nous exploitons la relation potentielle entre PL et la théorie des treillis.
Nous proposons d'abord une représentation intermédiaire des phrases logiques, où elles deviennent des noeuds dans un treillis ayant une relation d'ordre partiel équivalent à la validité de l'implication matérielle.
En conséquence, nous transformons la vérification de validité de d-&gt;q, ce qui est un calcul intensif, en une série de vérifications simples d'inclusion d'ensembles.
Afin de mesurer l'incertitude de la décision de pertinence U(d-&gt;q), nous utilisons la fonction du degré d'inclusion Z, qui est capable de quantifier les relations d'ordre partielles définies sur des treillis.
Enfin, notre modèle est capable de travailler efficacement sur toutes les phrases logiques sans aucune restriction, et est applicable aux données à grande échelle.
Notre modèle apporte également quelques conclusions théoriques comme : la formalisation de l'hypothèse de van Rijsbergen sur l'estimation de l'incertitude logique U(d-&gt;q) en utilisant la probabilité conditionnelle P(q|d), la redéfinition des deux notions Exhaustivité et Spécificité, et finalement ce modèle a également la possibilité de reproduire les modèles les plus classiques de RI.
De manière pratique, nous construisons trois instances opérationnelles de notre modèle.
Une instance pour étudier l'importance de Exhaustivité et Spécificité, et deux autres pour montrer l'insuffisance de l'hypothèse sur l'indépendance des termes.
Nos résultats expérimentaux montrent un gain de performance lors de l'intégration Exhaustivité et Spécificité.
Le travail présenté dans cette thèse doit être poursuivit par plus d'expérimentations, en particulier sur l'utilisation de relations, et par des études théoriques en profondeur, en particulier sur les propriétés de la fonction Z.
Ce travail sur la réduction segmentale (i.e. délétion ou réduction temporelle) en français spontané nous a permis non seulement de proposer deux méthodes de recherche pour les études en linguistique, mais également de nous interroger sur l'influence de différents facteurs de variation sur divers phénomènes de réduction et d'apporter des connaissances sur la propension à la réduction des segments.
Nous avons appliqué la méthode descendante qui utilise l'alignement forcé avec variantes lorsqu'il s'agissait de phénomènes de réduction spécifiques.
Lorsque ce n'était pas le cas, nous avons utilisé la méthode ascendante qui examine des segments absents et courts.
Trois phénomènes de réduction ont été choisis : l'élision du schwa, la chute du /ʁ/ et la propension à la réduction des segments.
La méthode descendante a été utilisée pour les deux premiers.
Les facteurs en commun étudiés sont le contexte post-lexical, le style, le sexe et la profession.
L'élision du schwa en syllabe initiale de mots polysyllabiques et la chute du /ʁ/ post-consonantique en finale de mots ne sont pas toujours influencées par les mêmes facteurs.
De même, l'élision du schwa lexical et celle du schwa épenthétique ne sont pas conditionnées par les mêmes facteurs.
L'étude sur la propension à la réduction des segments nous a permis d'appliquer la méthode ascendante et d'étudier la réduction des segments de manière générale.
Les résultats suggèrent que les liquides et les glides résistent moins à la réduction que les autres consonnes et que les voyelles nasales résistent mieux à la réduction que les voyelles orales.
Parmi les voyelles orales, les voyelles hautes arrondies ont tendance à être plus souvent réduites que les autres voyelles orales.
L'identification automatique d'expressions polylexicales (EP) est un pré-requis pour de nombreuses applications de traitement automatique des langues.
Cette tâche représente un défi car les EP, et en particulier les verbales (EPV) telles que 'casser sa pipe'(signifiant 'mourir'), ont des formes de surface très variables ('cassera-t-il un jour sa pipe ?
On se penche ici sur un sous-problème de l'identification d'EPV, à savoir l'identification d'occurrences d'EPV vues dans d'autres contextes, quelque soit leur forme de surface, ce qui nécessite de prendre en compte l'ambiguïté pour éviter des lectures littérales ('casser sa vieille pipe') ou des co-occurrences fortuites ('casser le tuyau de sa pipe').
On considère pour cela deux approches : la première se fonde sur une mesure de la variabilité des EPV indépendante de la langue.
La seconde consiste à modéliser le problème comme une tâche de classification d'après des traits pertinents pour la variabilité morpho-syntaxique des EPV, ce qui nous a conduit à développer un système (VarIDE), qui a participé à la compétition PARSEME d'identification automatique d'EPV en 2018.
Cette thèse se place dans le contexte d'Accordys, un projet d'ingénierie des connaissances qui vise à fournir un système de rapprochement de cas en fœtopathologie, qui est le domaine de l'étude des maladies rares et dysmorphies du fœtus.
Ce projet se base sur un corpus de comptes rendus d'examens fœtaux.
Ce matériel consiste en des comptes rendus en texte brut présentant un vocabulaire très spécifique (qui n'est que partiellement formalisé dans des terminologies médicales en français), des économies linguistiques (un style "prise de notes" très prononcé rendant difficile l'utilisation d'outils analysant la grammaire du texte) et une mise en forme matérielle exhibant une structuration commune latente (un découpage en sections, sous-sections, observations).
La mise en correspondance entre cas et modèle (instanciation du modèle) est réalisée via un mapping d'arbres ayant pour base une méthode de Monte Carlo.
Nous comparerons ceci avec des mesures de similarités obtenues en représentant nos comptes rendus (soit tels quels, soit enrichis sémantiquement grâce à un annotateur sémantique) dans un modèle vectoriel.
Cette thèse aborde le problème de la détection automatique des comparaisons figuratives dans des textes littéraires en prose écrits en français ou en anglais et propose un canevas pour décrire ces comparaisons d'un point de vue stylistique.
Une comparaison figurative correspond ici à toute structure syntaxique qui met en parallèle au moins deux entités, déroge au principe de compositionnalité et crée une image mentale dans l'esprit de ceux à qui elle est destinée.
Trois éléments principaux distinguent notre approche des travaux précédents : son ancrage dans les théories linguistiques et cognitives sur les comparaisons littérales et figuratives, sa capacité à gérer des marqueurs appartenant à différentes catégories grammaticales et sa flexibilité qui lui permet d'envisager différents scénarios syntaxiques.
De fait, nous proposons une méthode comprenant trois modules complémentaires : - un module syntaxique qui utilise des dépendances syntaxiques et des règles manuelles pour identifier les comparaisons potentielles ainsi que leurs composantes ; - un module sémantique qui mesure la saillance des motifs détectés et la similarité sémantique des termes comparés en se basant sur une base de données préétablie ; - et un module d'annotation qui fournit entre autres des informations sur le type de comparaison (idiomatique, sensorielle…) et sur les catégories sémantiques employées.
Pour finir, au vu des données recueillies au cours des deux campagnes d'annotation que nous avons menées, il paraît clair que la détection automatique des comparaisons figuratives doit tenir compte de plusieurs facteurs parmi lesquels la saillance, la catégorisation et la syntaxe de la phrase.
Notre travail a pour objectif une analyse syntaxique des adverbes de temps coréens dont l'interprétation correspond à une durée ou à une date (e. G. 3sigan dongan (pendant 3 heures), 5uel 6il (le 6 mai)).
Pour la linguistique formelle comme pour le traitement informatique des langues, une description aussi exhaustive et explicite que possible est indispensable.
La méthodologie du lexique-grammaire (M. Gross 1975, 1986) nous a fourni un modèle de description formelle et systématique de la langue naturelle.
Nous avons choisi de décrire les combinaisons lexicales concernées par des graphes d'automates finis, qui constituent autant de "grammaires locales" représentant les différents types de séquences adverbiales possibles.
Nos graphes peuvent être intégrés directement à un analyseur syntaxique automatique pour localiser les adverbes de durée et de date en coréen dans des textes quelconques.
Nous consacrons le chapitre 3 à l'analyse des formes interprétables comme des durées et le chapitre 4 à celle des formes interprétables comme des dates.
Nous analysons comment les groupes nominaux de temps étudiés dans le deuxième chapitre peuvent entrer dans des phrases qui donnent lieu à des interprétations de durée ou de date
Un système SHM par ondes guidées a pour but d'évaluer l'intégrité d'une grande variété de structures fines, telles que les fuselages d'avions, les tuyaux, les réservoirs, etc.
Un tel système est basé sur un réseau de capteurs piézoélectriques pour l'excitation et la mesure des ondes guidées.
Cette thèse présente les travaux menés dans le but de développer un système de SHM par ondes guidées capable de détecter, localiser et dimensionner efficacement les défauts dans des structures aéronautiques assimilables à des plaques, en matériaux composites ou en aluminium.
Ce travail comprend également une étude approfondie des algorithmes d'imagerie DAS, MV et Excitelet, les plus prometteurs parmi ceux de la littérature, une évaluation de leurs performances par analyse statistique sur une grande base de données de résultats de simulations d'imagerie par ondes guidées et propose une méthode d'imagerie parcimonieuse.
Alors que la détection et la localisation des défauts à partir de l'analyse des images est aisée, le dimensionnement du défaut est un problème plus complexe en raison de sa forte dimensionnalité et de sa non-linéarité.
Il est démontré que ce problème peut être résolu par des méthodes d'apprentissage automatique sur une grande base de données de résultats de simulations d'imagerie par ondes guidées.
Elles sont efficaces dans des conditions opérationnelles stationnaires mais sont sensibles aux variations de l'environnement et notamment aux fluctuations de température.
Ce travail présente donc l'étude de la robustesse face aux effets thermiques des méthodes d'imagerie par ondes guidées et propose un modèle de détection de défauts capable d'analyser des résultats d'imagerie détériorés.
Plusieurs techniques de compensation des effets thermiques sont étudiées et des améliorations sont proposées.
Leur efficacité est validée pour les plaques d'aluminium mais des améliorations supplémentaires sont nécessaires pour les étendre aux plaques de composites.
La présente thèse étudie la modélisation conjointe des contenus visuels et textuels extraits à partir des documents multimédias pour résoudre les problèmes intermodaux.
Ces tâches exigent la capacité de ``traduire''l'information d'une modalité vers une autre.
Un espace de représentation commun, par exemple obtenu par l'Analyse Canonique des Corrélation ou son extension kernelisée est une solution généralement adoptée.
Sur cet espace, images et texte peuvent être représentés par des vecteurs de même type sur lesquels la comparaison intermodale peut se faire directement.
Néanmoins, un tel espace commun souffre de plusieurs déficiences qui peuvent diminuer la performance des ces tâches.
Le premier défaut concerne des informations qui sont mal représentées sur cet espace pourtant très importantes dans le contexte de la recherche intermodale.
Le deuxième défaut porte sur la séparation entre les modalités sur l'espace commun, ce qui conduit à une limite de qualité de traduction entre modalités.
Pour faire face au premier défaut concernant les données mal représentées, nous avons proposé un modèle qui identifie tout d'abord ces informations et puis les combine avec des données relativement bien représentées sur l'espace commun.
Les évaluations sur la tâche d'illustration de texte montrent que la prise en compte de ces information fortement améliore les résultats de la recherche intermodale.
La contribution majeure de la thèse se concentre sur la séparation entre les modalités sur l'espace commun pour améliorer la performance des tâches intermodales.
Nous proposons deux méthodes de représentation pour les documents bi-modaux ou uni-modaux qui regroupent à la fois des informations visuelles et textuelles projetées sur l'espace commun.
Nos approches permettent d'obtenir des résultats de l'état de l'art pour la recherche intermodale ou la classification bi-modale et intermodale.
Les objectifs de cette thèse s'inscrivent dans la large problématique de recherche d'information dans les données issues du Dossier Patient Informatisé (DPI).
Les aspects abordés dans cette problématique sont multiples : d'une part la mise en oeuvre d'une recherche d'information clinomique au sein du DPI et d'autre part la recherche d'information au sein de données non structurées issues du DPI.
Dans un premier temps, l'un des objectifs de cette thèse est d'intégrer au sein du DPI des informations dépassant le cadre de la médecine pour intégrer des données, informations et connaissances provenant de la biologie moléculaire ;
L'intégration de ce type de données permet d'améliorer les systèmes d'information en santé, leur interopérabilité ainsi que le traitement et l'exploitation des données à des fins cliniques.
Un enjeu important est d'assurer l'intégration de données hétérogènes, grâce à des recherches sur les modèles conceptuels de données, sur les ontologies et serveurs terminologiques et sur les entrepôts sémantiques.
L'intégration de ces données et leur interprétation selon un même modèle de données conceptuel sont un verrou important.
Enfin, il est important d'intégrer recherche clinique et recherche fondamentale afin d'assurer une continuité des connaissances entre recherche et pratique clinique et afin d'appréhender la problématique de personnalisation des soins.
Cette thèse aboutit ainsi à la conception et au développement d'un modèle générique des données omiques exploité dans une application prototype de recherche et visualisation dans les données omiques et cliniques d'un échantillon de 2 000 patients.
Le second objectif de ma thèse est l'indexation multi terminologique de documents médicaux à travers le développement de l'outil Extracteur de Concepts Multi-Terminologique (ECMT).
Il exploite les terminologies intégrées au portail terminologique Health Terminology/Ontology Portal (HeTOP) pour identifier des concepts dans des documents non structurés.
Ainsi, à partir d'un document rédigé par un humain, et donc porteur potentiellement d'erreurs de frappe, d'orthographe ou de grammaire,l'enjeu est d'identifier des concepts et ainsi structurer l'information contenue dans le document.
Pour la recherche d'information médicale, l'indexation présente un intérêt incontournable pour la recherche dans les documents non structurés, comme lescomptes
Les réseaux ad hoc mobiles sont des réseaux qui se forment spontanément grâce à la présence de terminaux mobiles.
Ces réseaux sans fil sont de faible capacité.
Les nœuds se déplacent librement et de manière imprévisible et ils se déchargent très rapidement.
En conséquence, un réseau MANET est très enclin à subir des partitionnements fréquents.
La réplication des données constitue un mécanisme prometteur pour pallier ce problème.
Cependant, la mise en œuvre d'un tel mécanisme dans un environnement aussi contraint en ressources constitue un réel défi.
L'objectif principal est donc de réaliser un mécanisme peu consommateur en ressources.
Le second objectif de la réplication est de permettre le rééquilibrage de la charge induite par les requêtes de données.
Dans cette thèse, nous proposons CReaM (Community-Centric and Resource-Aware Replication Model”) un modèle de réplication adapté à un réseau MANET.
CReaM fonctionne en mode autonomique : les prises de décisions se basent sur des informations collectées dans le voisinage du nœud plutôt que sur des données globalement impliquant tous les nœuds, ce qui permet de réduire le trafic réseau lié à la réplication.
Pour réduire l'usage des ressources induit par la réplication sur un nœud, les niveaux de consommation des ressources sont contrôlés par un moniteur.
Toute consommation excédant un seuil prédéfini lié à cette ressource déclenche le processus de réplication.
Pour permettre le choix de la donnée à répliquer, une classification multi critères a été proposée (rareté de la donnée, sémantique, niveau de demande) ; et un moteur d'inférence qui prend en compte l'état de consommation des ressources du nœud pour désigner la catégorie la plus adaptée pour choisir la donnée à répliquer.
Pour permettre de placer les réplicas au plus près des nœuds intéressés, CReaM propose un mécanisme pour l'identification et le maintien à jour des centres d'intérêt des nœuds.
Les utilisateurs intéressés par un même sujet constituent une communauté.
Par ailleurs, chaque donnée à répliquer est estampillée par le ou les sujets au(x)quel(s) elle s'apparente.
Un nœud désirant placer un réplica apparenté à un sujet choisira le nœud ayant la plus grande communauté sur ce sujet.
Les résultats d'expérimentations confirment la capacité de CReaM à améliorer la disponibilité des données au même niveau que les solutions concurrentes, tout en réduisant la charge liée à la réplication.
Cette thèse cherche à établir si les différents processus de catégorisation influençant les évaluations des audiences sur les marchés conduisent à une stabilisation ou à une plus grande variabilité de leurs évaluations.
Bien que les travaux de recherche fondateurs portant sur la catégorisation aient insisté sur le rôle stabilisateur des catégories sur les marchés, la recherche récente suggère que les évaluations des audiences peuvent varier substantiellement, même sur des marchés dotés de catégories pré-existantes bien établies.
Cette variabilité résulte notamment des préférences hétérogènes des audiences pour les offres typiques, de changements dans les significations associées aux catégories ou de l'utilisation par les audiences de plusieurs modes d'évaluation.
En se basant sur ces nouveaux résultats, cette thèse cherche pourquoi les évaluations des audiences sont si variables et explore en détail le rôle joué par les catégories de marché dans cette variabilité.
Cette thèse propose que i) les catégories ambigües, ii) l'influence d'attractions temporaires parmis les audiences aux côtés des catégories plus stables et iii) la co-existence de plusieurs types d'évaluateurs contribuent à produire de la variabilité dans les évaluations des audiences.
Les deux premiers essais empiriques utilisent des données sur des entreprises cotées en bourse aux Etats-Unis.
Dans ces essais, la similarité des entreprises aux prototypes des catégories existantes ou l'attraction temporaire des audiences vers certains attributs sont mesurés à l'aide de contenus sémantiques extraits d'un large corpus de rapports annuels et de prospectus d'entrée en bourse.
Le troisième essai est un modèle théorique.
Cette thèse contribue à la littérature sur le rôle des catégories sur les marchés, à la recherche émergente sur le niveau de distinction optimal et aux approches computationelles de l'étude des organisations.
Les indices du genre sont construits à un double niveau : morphosyntaxique, à partir d'un système de descripteurs original et adapté, et textuel, en fonction des composantes et des thématiques.
Cette partition est méthodologique, l'interaction étant constante dans la structuration du texte.
Après une description d'ensemble du corpus, une recherche contrastive met en évidence des principes de variation extrinsèques : incidence du style d'auteur, confrontation de l'article à d'autres formes de genres scientifiques, variations d'un domaine à l'autre et incidence de la langue choisie (103 articles de linguistique en anglais) ;
Cette étude quantitative et qualitative a permis de saisir les différents aspects d'un objet normatif complexe et multidimensionnel en le caractérisant sur différents plans de sa réalisation et de ses régulations linguistiques.
La difficulté de lecture de Una meditación a été soulignée tant par la critique que par Juan Benet lui-même.
Ce travail essaie de caractériser cette difficulté et, par ce biais, la spécificité de l'expérience de lecture du roman de Benet.
Notre étude s'appuie sur la psycholinguistique de la compréhension des textes, qui nous permet de définir la norme de lisibilité implicite par rapport à laquelle Una meditación est jugé « difficile » .
Nous étudions les deux aspects qui, par rapport à cette norme, constituent les principales sources de difficulté du texte bénétien : la disposition de la matière romanesque (au niveau du récit et de la phrase) et le système de référenciation des personnages.
Sur le plan de la disposition, le récit et – à son échelle – la phrase se caractérisent par une forte discontinuité pourtant dissimulée, par un développement temporel de forme spirale, et par le brouillage des relations hiérarchiques entre les événements.
Sur le plan de la référenciation, la notion de nom du personnage perd son sens traditionnel, car les noms sont peu employés, ambigus, multiples, ou inexistants ;
mais c'est surtout l'omniprésence de la référenciation pronominale qui déroute le lecteur en lui exigeant implicitement de ne pas oublier un seul détail du texte.
Nous analysons aussi la figure du narrateur et nuançons une lecture courante selon laquelle le texte serait le produit d'une remémoration.
Nous concluons que la « difficulté » de Una meditación semble être au service d'une écriture qui, à travers l'indifférenciation des personnages et des histoires, dépasse la fiction et vise un portait générique de la nature humaine.
En été 2013, le terme de "Big Data" fait son apparition et suscite un fort intérêt auprès des entreprises.
Cette thèse étudie ainsi l'apport de ces méthodes aux sciences actuarielles.
Elle aborde aussi bien les enjeux théoriques que pratiques sur des thématiques à fort potentiel comme l'textit{Optical Character Recognition} (OCR), l'analyse de texte, l'anonymisation des données ou encore l'interprétabilité des modèles.
Commençant par l'application des méthodes du machine learning dans le calcul du capital économique, nous tentons ensuite de mieux illustrer la frontrière qui peut exister entre l'apprentissage automatique et la statistique.
Mettant ainsi en avant certains avantages et différentes techniques, nous étudions alors l'application des réseaux de neurones profonds dans l'analyse optique de documents et de texte, une fois extrait.
L'utilisation de méthodes complexes et la mise en application du Réglement Général sur la Protection des Données (RGPD) en 2018 nous a amené à étudier les potentiels impacts sur les modèles tarifaires.
En appliquant ainsi des méthodes d'anonymisation sur des modèles de calcul de prime pure en assurance non-vie, nous avons exploré différentes approches de généralisation basées sur l'apprentissage non-supervisé.
Enfin, la réglementation imposant également des critères en terme d'explication des modèles, nous concluons par une étude générale des méthodes qui permettent aujourd'hui de mieux comprendre les méthodes complexes telles que les réseaux de neurones
La thèse vise à évaluer de nouvelles formes d'interaction Homme-Machine.
Ainsi, comportements et réactions d'utilisateurs sont recueillis à l'aide de méthodes ergonomiques et techniques de suivi du regard (eye-tracking).
Une approche expérimentale a été adoptée afin d'évaluer l'apport de chaque axe.
Pour cela, des participants (étudiants de niveau Licence) ont manipulé un logiciel de création d'animation qui leur était inconnu (Flash) afin de réaliser trois scénarios.
Tout au long de leur découverte du logiciel, les participants étaient accompagnés d'un dispositif d'aide intégrant, suivant l'expérimentation, un ACA (fourni par FT R &amp;
D) ou une technique d'adaptation (détection d'intention et évolution suivant les connaissances).
Les différentes études réalisées montrent que les deux sources d'innovation employées ont été perçues positivement par la majorité des participants.
Elles ont montré d'autre part qu'un ACA a un effet rassurant et qu'il peut vraisemblablement être utilisé lors de la prise en main d'un logiciel.
Pour le système adaptatif, le fait que le système évolue de manière autonome n'a pas perturbé les participants, mais n'améliore guère les performances.
Cette thèse concerne l'étude des débats argumentatifs entre plusieurs agents artificiels.
Notre travail est motivé par les difficultés qui surgissent quand un nombre important d'utilisateurs interagissent et débattent sur le Web, en échangeant des arguments sur différents sujets.
Ces difficultés se situent au niveau de la représentation des connaissances des agents impliqués dans le débat, de la représentation du débat, de la façon de tirer les conclusions du débat, de l'évaluation de la qualité du débat, de la définition des protocoles spécifiques d'interaction, et de l'étude des stratégies des agents qui souhaitent atteindre un but précis via le débat.
La contribution de cette thèse consiste donc en :
a) la modélisation d'un débat argumentatif entre plusieurs agents, la modélisation des expertises de ces derniers, et l'agrégation des opinions des différents experts sur différentes parties d'un débat ;
b) l'apport d'une aide à l'agent responsable de la gestion d'un débat donné, la proposition d'une méthode permettant d'évaluer la qualité des débats argumentatifs en fonction de la confiance que l'on peut avoir en leurs conclusions, ainsi que la proposition de solutions permettant d'améliorer la qualité des débats dont les conclusions ne sont pas clairement établies ;
c) l'apport d'une aide permettant aux agents qui participent à un débat argumentatif de déterminer quels arguments placer dans le débat, l'étude des systèmes argumentatifs munis d'une structure dynamique, l'étude des moyens disponibles permettant à un agent d'influencer un système dynamique afin d'atteindre son but, l'étude des modifications minimales permettant à un agent d'atteindre un objectif donné, l'étude des stratégies argumentatives basées sur ce changement minimal ;
d) la définition, l'étude et l'évaluation des protocoles argumentatifs multi-agents, ainsi que la définition de protocoles de différents types (1) basés sur une évaluation numérique d'arguments et (2) basés sur des extensions d'arguments, l'utilisation des différentes techniques pour assurer la cohérence d'un débat tout en laissant une liberté d'expression aux agents, et enfin un grand nombre d'expérimentations (sur des débats) permettant de tester différentes stratégies et de les évaluer en fonction de différents critères.
La prise de son et le traitement multicanale sont des objets de recherche à Orange Labs.
Une thèse en cours traite du filtrage multicanal par apprentissage et montre la valeur ajoutée des réseaux de neurones pour estimer un filtre de rehaussement spatial robuste.
La thèse proposée vise à utiliser de nouvelles méthodes d'apprentissage pour fournir des informations nécessaires au filtrage : les début et fin de phrase et la position au cours du temps des sources sonores.
L'idée est de coupler une antenne ambisonique avec des réseaux de neurones profonds.
Ainsi, la thèse s'intéresse à 3 aspects de la localisation : l'estimation du nombre de sources dans le mélange, l'estimation des directions d'arrivées des sources et le suivi des sources au cours du temps (tracking).
La linguistique informatique est un domaine de recherche qui se concentre sur les méthodes et les perspectives de la modélisation formelle (statistique ou symbolique) de la langue naturelle.
La première partie de la recherche présentée ici se concentre sur la création d'un analyseur syntaxique de surface (ou analyseur en chunks) pour le hongrois.
La sortie de l'analyseur de surface est conçue pour servir d'entrée pour un traitement ultérieur visant à annoter les relations de dépendance entre le prédicat et ses compléments essentiels et circonstanciels.
L'analyseur profond est mis en œuvre dans NooJ (Silberztein, 2004) en tant qu'une cascade de grammaires.
Le deuxième objectif de recherche était de proposer une représentation lexicale pour la structure argumentale en hongrois.
Cette représentation doit pouvoir gérer la vaste gamme de phénomènes qui échappent à la dichotomie traditionnelle entre un complément essentiel et un circonstanciel (p. ex.
Nous avons eu recours à des résultats de la recherche récente sur la réalisation d'arguments et choisi un cadre qui répond à nos critères et qui est adaptable à une langue non-configurationnelle.
La première étape consistait à définir les règles de codage et de construire un vaste base de données lexicale pour les verbes et leurs compléments.
Et, plus généralement, quelle est la nature des alternances spécifiques aux classes verbales en hongrois ?
Dans la phase finale de la recherche, nous avons étudié le potentiel de l'acquisition automatique pour extraire des classes de verbes à partir de corpus.
Ma recherche prend la forme d'une enquête au sein des milieux de production des savoirs français contemporains qui vise à comprendre les différentes significations du terme open en sciences.
J'ai considéré le qualificatif open comme une formule, dont l'analyse de ses traductions en français (ouvert, libre, gratuit) tout autant que des noms qui lui sont associés (science, data, access) constitue le fil directeur de son étude.
Cette enquête, qui a débuté en 2013, s'est surtout centrée sur un évènement particulier, la consultation sur le projet de loi pour une République numérique (septembre-octobre 2015), en particulier l'article 9 sur « le libre accès aux publications scientifiques de la recherche publique » .
Cette consultation en ligne a donné une envergure nationale et publique aux problématiques d'accès aux savoirs.
En tant qu'épreuve de réalité « équipée » d'un dispositif numérique participatif, elle a été l'occasion d'observer presque « en direct » la défense de différentes conceptions de « ce que devrait être » le régime contemporain des savoirs en France.
M'inscrivant dans une démarche par théorisation ancrée, j'ai constitué progressivement, à propos de ce moment particulier de cristallisation des débats sur l'open en sciences, un corpus de documents reflétant le déploiement des échanges sur des espaces/dispositifs numériques distincts : site web de la consultation, blogs scientifiques, revues académiques, médias « grand public » , rapports.
Les mouvements itératifs de cette enquête, alliant méthodes numériques (réalisation d'une cartographie de similarité des votes) et analyse qualitative du corpus, tout autant que les concepts théoriques mobilisés à la croisée entre sciences de l'information et de la communication et sociologie pragmatique de la critique, ont donné lieu à une modélisation.
Par la suite, en passant de la modélisation à une théorisation transposable à d'autres terrains de recherche, je montre comment, derrière les discours sur l'open, la distinction entre deux logiques (technoindustrielle ou processuelle) peut être pertinente pour analyser les reconfigurations actuelles d'autres agencements sociétaux.
Les stratégies dans l'épreuve employées lors de la consultation illustrent dans ce sens la coexistence de deux conceptions « numériques » de la démocratie (représentative étendue ou contributive), présentes dans le design même de la plateforme consultative.
Dans la dernière partie, je propose d'expliquer les dynamiques de reconfiguration d'un esprit et d'un agencement sociétal dans une interprétation énactive en considérant les couplages permanents entre cognition, actions médiées par les technologies et environnement sociotechnique.
L'expérience même du doctorat narrée tout au long de ce récit constitue aussi l'exemple d'un processus d'énaction sur mes propres conceptions de l'open.
En ce sens, elle ouvre une piste de réflexion sur la nature située et incarnée de toute production de savoirs, qui n'échappe pas aux limites tout autant qu'aux potentialités de la métacognition.
L'étude des usages et pratiques informationnels des usagers en démarche de recherche d'information est un axe majeur de la recherche en sciences de l'information et de la communication, à en juger par les travaux consacrés à cette problématique et disponibles sur Internet.
Si les pays du Nord ont une tradition bien ancrée en la matière, au Sud, par contre, et plus précisément dans les pays francophones d'Afrique au sud du Sahara, peu d'études ont été consacrées à cette thématique.
La présente étude se place dans ce contexte thématique et géographique, et vise comme public cible les doctorants de l'Université de Bamako.
Il s'agira d'abord de définir une typologie de cette communauté d'utilisateurs de l'information (spécialisation, écoles doctorales et laboratoires d'attache, thématique de recherche, etc.), puis d'évaluer ses besoins et pratiques informationnels et enfin d'identifier l'existence de difficultés éventuelles d'accès à l'information et de proposer des pistes de solutions.
Pour la collecte de données une enquête réalisée auprès de doctorants et l'observation de leurs comportements en situation de recherche d'information seront privilégiées.
Les résultats de l'étude permettront une meilleure connaissance des besoins et pratiques informationnels de la communauté des doctorants de l'université de Bamako et pourraient être utilisés par les unités d'information en vue d'améliorer l'offre de services d'information à leur intention.
Notre étude a pour objet les représentations discursives de l'animation japonaise.
En France, le dessin animé japonais, l'anime, fait aujourd'hui partie des habitudes culturelles de plusieurs générations, mais a connu des débuts tumultueux à son arrivée dans le pays au milieu des années 70.
Au Japon d'où il est originaire, l'anime fait depuis longtemps partie de la culture populaire du pays.
Nous avons cherché à savoir comment l'anime, décrit comme étant bien implanté au Japon mais aussi en France, était perçu par les jeunes habitants de ces deux pays.
Nous avons également souhaité savoir si la culture de ces deux pays pouvait exercer une influence sur la façon dont il est dépeint.
Nous avons effectué une enquête auprès d'étudiants français et japonais à qui nous avons soumis un questionnaire dans leur langue respective.
L'analyse discursive et sémantique des réponses des participants français et japonais a permis d'extraire leurs représentations en matière d'anime, mais aussi en ce qui concerne les cultures française et japonaise.
Nous avons ensuite procédé à l'analyse comparative de nos résultats qui a permis de confronter le regard des étudiants français à celui de leurs homologues japonais.
La construction de ressources linguistiques arabes riches en informations syntaxiques constitue un enjeu important pour le développement de nouveaux outils de traitement automatique.
Cette thèse propose une approche pour la création d'un treebank de l'arabe intégrant des informations d'un type nouveau reposant sur le formalisme des Grammaires de Propriétés.
Une propriété syntaxique caractérise une relation pouvant exister entre deux unités d'une certaine structure syntaxique.
Nous avons pu ainsi construire, à l'aide de cette grammaire, d'autres ressources linguistiques arabes.
Une grammaire de propriétés lexicalisée probabiliste fait partie de son modèle d'apprentissage pour pouvoir affecter positivement le résultat d'analyse et caractériser ses structures syntaxiques avec les propriétés de ce modèle.
Nous avons enfin évalué les résultats obtenus en les comparant à celles du Stanford Parser.
Ce travail de thèse se focalise sur une classe de méthodes d'apprentissage profond, probabilistes et non-supervisées qui utilisent l'inférence variationnelle pour créer des modèles évolutifs de grande capacité pour ce type de données.
Nous présentons deux classes d'apprentissage variationnel profond, puis nous les appliquons à deux problèmes spécifiques liés au domaine maritime.
La première application est l'identification de systèmes dynamiques à partir de données bruitées et partiellement observées.
Nous introduisons un cadre qui fusionne l'assimilation de données classique et l'apprentissage profond moderne pour retrouver les équations différentielles qui contrôlent la dynamique du système.
En utilisant une formulation d'espace d'états, le cadre proposé intègre des composantes stochastiques pour tenir compte des variabilités stochastiques, des erreurs de modèle et des incertitudes de reconstruction.
La deuxième application est la surveillance du trafic maritime à l'aide des données AIS.
Nous proposons une architecture d'apprentissage profond probabiliste multitâche pouvant atteindre des performances très prometteuses dans différentes tâches liées à la surveillance du trafic maritime, telles que la reconstruction de trajectoire, l'identification du type de navire et la détection d'anomalie, tout en réduisant considérablement la quantité de données à stocker et le temps de calcul.
Pour la tâche la plus importante-la détection d'anomalie, nous introduisons un détecteur géospatialisé qui utilise l'apprentissage profond variationnel pour construire une représentation probabiliste des trajectoires AIS, puis détecter les anomalies en jugeant la probabilité de cette trajectoire.
The Chronicles of Narnia (1950-1956) est un célèbre recueil de sept romans, traditionnellement reconnus comme des œuvres de littérature de jeunesse et à l'origine du genre fantasy.
L'un des intérêts majeurs de ces livres réside dans leur substrat symbolique, exprimé dans le texte par un double niveau de lecture qui évoque la tradition chrétienne.
Notre thèse consiste en l'analyse d'un corpus incluant les originaux en anglais et les traductions françaises, publiées sous le titre de Le Monde de Narnia (2005)
Parmi ces marqueurs sont notamment analysés les déictiques, la modalité, la transitivité, les choix lexicaux et la prosodie sémantique.
Les parties du discours correspondant à ces marqueurs sont analysées notamment en lien avec l'instance du narrateur, de par son rôle clé pour l'idéologie dans le texte, et de régie dans la focalisation.
Notre analyse porte une attention particulière à la dimension du sacré et aux thèmes de la violence, de la mort et du genre en traduction de littérature de jeunesse.
La littérature de jeunesse, toujours plus ou moins caractérisée par un but éducatif, tout comme les livres qui constituent notre corpus, s'avère un véhicule axiologique puissant, qui reflète les valeurs qu'une société défend et transmet à un moment donné.
Notre travail a montré notamment que les traductions françaises ont tendance à affaiblir le message religieux, éloignant le regard du lecteur ou rendant flous les contours de l'espace.
Dans l'ensemble, l'idéologie du texte cible est caractérisée par un certain nombre d'écarts par rapport au texte source et met en avant d'autres valeurs, pourtant présentes, elles aussi, dans l'original.
Ce travail de recherche montre une méthode permettant d'aborder le texte dans le but d'une meilleure compréhension des enjeux qui sous-tendent la traduction, et en particulier la traduction de l'idéologie et du point de vue dans les livres pour enfants.
Au Mexique, l'un des problèmes technologiques prioritaires est la préservation du patrimoine culturel sous sa forme numérique.
Dans cette recherche, l'intérêt principal est la commande, la gestion et l'identification du patrimoine culturel immatériel en images.
En vision par ordinateur, l'intégration du système visuel humain dans les méthodes d'apprentissage automatique et les classificateurs est devenue un domaine de recherche intensif pour la reconnaissance d'objets et l'extraction de contenu.
Les cartes dites de saillance, sont définies comme une représentation topographique de l'attention visuelle sur une scène, modélisant l'attention instantanément et attribuant un degré d'intérêt à chaque valeur de pixel de l'image.
Les cartes des points saillants se sont avérées très efficaces pour mettre en évidence les régions d'intérêt dans plusieurs tâches de contenu visuel et de sa compréhension.
Dans ce contexte, nous nous concentrons sur l'intégration des modèles d'attention visuelle dans le pipeline de formation des réseaux neuronaux profonds pour la reconnaissance des structures architecturales mexicaines.
Nous considérons que les principales contributions de cette recherche se situent dans les domaines d'intérêt suivants :
i) Ensemble de données à usage spécifique : la collecte de données relatives au sujet est une tâche essentielle pour résoudre le problème de la classification architecturale.
ii) Sélection des données : nous utilisons des méthodes de prédiction des points saillants pour sélectionner et recadrer les régions pertinentes pour le contexte sur les images.
iii) Modélisation de l'attention visuelle : nous annotons les images par une tâche réelle d'observation des images, nous enregistrons les fixations des yeux avec un système de suivi des yeux pour construire des cartes de saillance subjective.
iv) Intégration de l'attention visuelle : nous intégrons l'attention visuelle dans les réseaux neuronaux profonds de deux manières : a) pour filtrer les caractéristiques dans une couche de regroupement basée sur les points saillants et b) avec des mécanismes d'attention.
Dans cette recherche, différentes composantes essentielles à la formation d'un réseau neuronal sont abordées dans le but de reconnaître le contenu culturel mexicain et d'extrapoler ces résultats à des bases de données à grande échelle dans des tâches de classification similaires, comme dans ImageNet.
Enfin, nous montrons que l'intégration de modèles d'attention visuelle « générés par une expérience psycho-visuelle » permet de réduire le temps de formation et d'améliorer les performances en termes de précision.
La pharmacovigilance est une discipline fondamentale pour la sécurité et la confiance dans le médicament.
Cette discipline a évolué au fil du temps et s'est renforcée, mais souffre encore d'imperfection.
Nous nous sommes proposés dans ce travail d'apporter une solution originale d'amélioration.
Dans une première partie, nous décrivons et analysons l'évolution de la pharmacovigilance et son fonctionnement actuel tant du point de vue juridique que du point de vue scientifique, et ce, tant au niveau national qu'au niveau de l'Union Européenne.
Nous avons analysé les insuffisances juridiques et pratiques puis avons fait des propositions pour les combler.
Nous avons donc formulé un certain nombre de possibilités, avant de développer dans la deuxième partie une approche originale : la pédagogie.
En partant du constat que la pharmacovigilance de terrain repose sur les professionnels de santé, nous avons étudié l'offre de formation de ces derniers et proposer d'apporter une formation universitaire plus qualitative et suffisamment quantitative, en pharmacovigilance et iatrogénie, le tout en s'appuyant sur des méthodes pédagogiques et des outils adaptés aux étudiants actuels.
Le paradigme pédagogique proposé s'appuie sur une pédagogie d'explication, qui se base sur la recherche en droit pharmaceutique et sur une pédagogie innovante hybride, alliant cours en présentiel et ressources en e-learning.
Les outils utilisés sont notamment des exposés magistraux complétés par des évaluations formatives sous forme de tests par boîtiers de vote électroniques et des cas cliniques en e-learning disponible sur une plateforme.
Cette pédagogie doit mener les étudiants vers une meilleure compréhension de la pharmacovigilance et une pratique de la gestion des évènements iatrogènes médicamenteux.
En conséquence le travail mené permet de mieux former les professionnels de santé à une gestion du risque médicamenteux avec un objectif final de confortation du système de pharmacovigilance.
Ce dispositif sera complété dans le futur par de la simulation et de jeux de rôle.
L'instrumentation massive des systèmes industriels permet aujourd'hui de générer de gros volumes de données de surveillance et de maintenance qui sont encore actuellement sous-exploitées dans le cadre de la maintenance prévisionnelle.
Un enjeu important pour la valorisation de ces données concerne donc le développement d'algorithmes permettant l'analyse en continu de ces données, complexes et hétérogènes, afin de prédire les défaillances et de planifier les opérations de maintenance.
Les approches actuelles d'apprentissage automatique et notamment l'apprentissage profond et leurs succès et résultats impressionnants dans le domaine de la reconnaissance visuelle et du traitement du langage naturel sont donc à fort potentiel pour la maintenance prévisionnelle.
Des premières approches ont été proposées sur des cas d'application particulier de maintenance.
Cette thèse présente un cadre formel pour l'interaction Homme-robot (HRI), qui reconnaître un important lexique de gestes statiques et dynamiques mesurés par des capteurs portatifs.
Gestes statiques et dynamiques sont classés séparément grâce à un processus de segmentation.
Les tests expérimentaux sur la base de données de gestes UC2017 ont montré une haute précision de classification.
La classification en ligne des gestes permet une classification prédictive avec réussit.
Le réseau propose a atteint une haute précision de rejet de les gestes non entraînés de la base de données UC2018 DualMyo.
Nous avons defini un modele de representation des connaissances contenues dans le discours, que ce soit un texte ou un dialogue homme-machine.
Cette representation est fondee sur des bases linguistiques et notre modele s'appuie sur des elements du fonctionnement cognitif.
Nous proposons un formalisme oriente objet, dont les fondements theoriques sont les systemes logiques de lesniewski : l'ontologie et la mereologie.
Le premier repose sur un foncteur primitif appele "epsilon" interprete comme est un, le second sur la relation partie de appelee " l'ingredience ".
Ces systemes logiques constituent une base theorique plus adaptee que la logique classique des predicats.
Le but de ce travail est d'examiner les propriétés sémantiques et morphosyntaxiques des noms abstraits apparentés à des prédicats verbaux ou adjectivaux.
En nous fondant sur l'hypothèse que le caractère statif commun à ces noms permet une analyse unifiée, nous proposons une étude de leurs différents emplois et montrons notamment qu'outre une acception stative, ces noms peuvent avoir une seconde lecture et dénotent alors des occurrences.
Dans la seconde partie, nous nous intéressons au comportement syntaxique des noms statifs, i.e. le nombre et la détermination, mais aussi la modification adjectivale.
Ceci nous permet de dégager deux comportements morphosyntaxiques distincts, corrélés à la distinction entre les deux lectures mise en évidence dans la première partie.
Dans leur lecture stative, ces noms ont un comportement proche de celui des noms massifs concrets et fonctionnent comme des noms relationnels : ils nécessitent un argument avec lequel ils entrent dans une relation syntaxique de prédication.
Inversement, dans leur lecture d'occurrence, ces noms se comportent comme des noms comptables concrets et ne sont pas intrinsèquement relationnels.
L'analyse des noms statifs que nous proposons tend à montrer que ceux-ci partagent leurs propriétés sémantiques avec certains types de prédicats verbaux et adjectivaux, et leurs propriétés syntaxiques avec diverses classes de noms concrets.
Les travaux de cette thèse explorent les propriétés de procédures d'estimation par agrégation appliquées aux problèmes de régressions en grande dimension.
Cependant, le comportement théorique de l'agrégat avec extit{prior} de Laplace
Le Chapitre 2 explicite une borne du risque de prédiction de cet estimateur.
Le Chapitre 4 introduit des variantes du Lasso pour améliorer les performances de prédiction dans des contextes partiellement labélisés.
Ces dernières années, le besoin de données géographiques de référence a significativement augmenté.
Pour y répondre, il est nécessaire de mettre jour continuellement les données de référence existantes.
Cette tâche est coûteuse tant financièrement que techniquement.
Pour ce qui concerne les réseaux routiers, trois types de voies sont particulièrement complexes à mettre à jour en continu : les chemins piétonniers, les chemins agricoles et les pistes cyclables.
Cette complexité est due à leur nature intermittente (elles disparaissent et réapparaissent régulièrement) et à l'hétérogénéité des terrains sur lesquels elles se situent (forêts, haute montagne, littoral, etc.).En parallèle, le volume de données GPS produites par crowdsourcing et disponibles librement augmente fortement.
Le nombre de gens enregistrant leurs positions, notamment leurs traces GPS, est en augmentation, particulièrement dans le contexte d'activités sportives.
Ces traces sont rendues accessibles sur les réseaux sociaux, les blogs ou les sites d'associations touristiques.
Cependant, leur usage actuel est limité à des mesures et analyses simples telles que la durée totale d'une trace, la vitesse ou l'élévation moyenne, etc.
Une attention particulière est portée aux voies existantes mais absentes du référentiel.
L'approche proposée se compose de trois étapes :
La première consiste à évaluer et augmenter la qualité des traces GPS acquises par la communauté.
Cette qualité a été augmentée en filtrant (1) les points extrêmes à l'aide d'un approche d'apprentissage automatique et (2) les points GPS qui résultent d'une activité humaine secondaire (en dehors de l'itinéraire principal).
Les points restants sont ensuite évalués en termes de précision planimétrique par classification automatique.
La seconde étape permet de détecter de potentielles mises à jour.
Pour cela, nous proposons une solution d'appariement par distance tampon croissante.
Cette distance est adaptée à la précision planimétrique des points GPS classifiés pour prendre en compte la forte hétérogénéité de la précision des traces GPS.
Nous obtenons ainsi les parties des traces n'ayant pas été appariées au réseau de voies des données de référence.
Ces parties sont alors considérées comme de potentielles voies manquantes dans les données de référence.
Finalement nous proposons dans la troisième étape une méthode de décision multicritère visant à accepter ou rejeter ces mises à jour possibles.
Les voies manquantes dans les données de références IGN BDTOPO
Cette thèse s'inscrit dans le cadre du projet PERDIDO dont les objectifs sont l'extraction et la reconstruction d'itinéraires à partir de documents textuels.
Les objectifs de cette thèse sont de concevoir un système automatique permettant d'extraire, dans des récits de voyages ou des descriptions d'itinéraires, des déplacements, puis de les représenter sur une carte.
Nous proposons une approche automatique pour la représentation d'un itinéraire décrit en langage naturel.
Notre approche est composée de deux tâches principales.
La première tâche a pour rôle d'identifier et d'extraire les informations qui décrivent l'itinéraire dans le texte, comme par exemple les entités nommées de lieux et les expressions de déplacement ou de perception.
La seconde tâche a pour objectif la reconstruction de l'itinéraire.
Notre proposition combine l'utilisation d'information extraites grâce au traitement automatique du langage ainsi que des données extraites de ressources géographiques externes (comme des gazetiers).
L'étape d'annotation d'informations spatiales est réalisée par une approche qui combine l'étiquetage morpho-syntaxique et des patrons lexico-syntaxiques (cascade de transducteurs) afin d'annoter des entités nommées spatiales et des expressions de déplacement ou de perception.
Une première contribution au sein de la première tâche est la désambiguïsation des toponymes, qui est un problème encore mal résolu en NER et essentiel en recherche d'information géographique.
Nous proposons un algorithme non-supervisé de géo-référencement basé sur une technique de clustering capable de proposer une solution pour désambiguïser les toponymes trouvés dans les ressources géographiques externes, et dans le même temps proposer une estimation de la localisation des toponymes non référencés.
Nous proposons un modèle de graphe générique pour la reconstruction automatique d'itinéraires, où chaque noeud représente un lieu et chaque segment représente un chemin reliant deux lieux.
Un calcul d'arbre de recouvrement minimal à partir d'un graphe pondéré est utilisé pour obtenir automatiquement un itinéraire sous la forme d'un graphe.
Chaque segment du graphe initial est pondéré en utilisant une méthode d'analyse multi-critère combinant des critères qualitatifs et des critères quantitatifs.
La valeur des critères est déterminée à partir d'informations extraites du texte et d'informations provenant de ressources géographique externes.
Par exemple, nous combinons les informations issues du traitement automatique de la langue comme les relations spatiales décrivant une orientation (ex : se diriger vers le sud) avec les coordonnées géographiques des lieux trouvés dans les ressources pour déterminer la valeur du critère "relation spatiale".
De plus, à partir de la définition du concept d'itinéraire et des informations utilisées dans la langue pour décrire un itinéraire, nous avons modélisé un langage d'annotation d'information spatiale adapté à la description de déplacements, s'appuyant sur les recommendations du consortium TEI (Text Encoding and Interchange).
Enfin, nous avons implémenté et évalué les différentes étapes de notre approche sur un corpus multilingue de descriptions de randonnées (Français, Espagnol et Italien).
La Gestion de Connaissance et de l'innovation sont des thèmes à forte importance dans l'actualité, surtout parce que ces deux sujets sont liés et ils influencent la performance des entreprises.
L'objectif de cette recherche est d'analyser le lien entre gestion de la connaissance et de l'innovation à partir de trois industries de produit simple dans le pôle industriel de Barcarena, état du Pará, au Brésil.
On veut évaluer à la fois la relation de la Gestion de Connaissance dans ces entreprises et leur capacité d'innovation, pour comprendre l'influence des connaissances pour la capacité innovatrice, surtout l'innovation incrémentale.
Pour cela, on se servira de modèles d'analyse qui prennent en compte des facteurs tels quels la culture, le leadership, la technologie, les ressources humaines et les processus.
Notre approche méthodologique est qualitative.
On prit comme base théorique les concepts et la littérature autour de la Gestion de Connaissance et de l'innovation.
L'axe de l'industrie fut choisi par son importance économique dans la région où la recherche a été développée.
De plus, selon les résultats les entreprises qui mieux gèrent les connaissances ont plus de possibiliter d'innover.
La production et la diffusion de musique numérisée ont explosé ces dernières années.
Une telle quantité de données à traiter nécessite des méthodes efficaces et rapides pour l'analyse et la recherche automatique de musique.
Cette thèse s'attache donc à proposer des contributions pour l'analyse sémantique de la musique, et en particulier pour la reconnaissance du genre musical et de l'émotion induite (ressentie par l'auditoire), à l'aide de descripteurs de bas-niveau sémantique mais également de niveau intermédiaire.
Afin d'accéder aux propriétés sémantiques à partir des descripteurs bas-niveau, des modélisations basées sur des algorithmes de types K-means et GMM utilisant des BoW et Gaussian super vectors ont été envisagées pour générer des dictionnaires.
Compte-tenu de la très importante quantité de données à traiter, l'efficacité temporelle ainsi que la précision de la reconnaissance sont des points critiques pour la modélisation des descripteurs de bas-niveau.
Ainsi, notre première contribution concerne l'accélération des méthodes K-means, GMM et UMB-MAP, non seulement sur des machines indépendantes, mais également sur des clusters de machines.
Afin d'atteindre une vitesse d'exécution la plus importante possible sur une machine unique, nous avons montré que les procédures d'apprentissage des dictionnaires peuvent être réécrites sous forme matricielle pouvant être accélérée efficacement grâce à des infrastructures de calcul parallèle hautement performantes telle que les multi-core CPU ou GPU.
En particulier, en s'appuyant sur GPU et un paramétrage adapté, nous avons obtenu une accélération de facteur deux par rapport à une implémentation single thread.
Concernant le problème lié au fait que les données ne peuvent pas être stockées dans la mémoire d'une seul ordinateur, nous avons montré que les procédures d'apprentissage des K-means et GMM pouvaient être divisées par un schéma Map-Reduce pouvant être exécuté sur des clusters Hadoop et Spark.
En utilisant notre format matriciel sur ce type de clusters, une accélération de 5 à 10 fois a pu être obtenue par rapport aux librairies d'accélération de l'état de l'art.
En complément des descripteurs audio bas-niveau, des descripteurs de niveau sémantique intermédiaire tels que l'harmonie de la musique sont également très importants puisqu'ils intègrent des informations d'un niveau d'abstraction supérieur à celles obtenues à partir de la simple forme d'onde.
Ainsi, notre seconde contribution consiste en la modélisation de l'information liée aux notes détectées au sein du signal musical, en utilisant des connaissances sur les propriétés de la musique.
Cette contribution s'appuie sur deux niveaux de connaissance musicale : le son des notes des instruments ainsi que les statistiques de co-occurrence et de transitions entre notes.
Pour le premier niveau, un dictionnaire musical constitué de notes d'instruments a été élaboré à partir du synthétiseur Midi de Logic Pro 9.
Basé sur ce dictionnaire, nous avons proposé un algorithme « Positive Constraint Matching Pursuit » (PCMP) pour réaliser la décomposition de la musique.
L'objectif de cette thèse est d'évaluer l'intérêt d'une nouvelle interface de commande pour fauteuil roulant électrique, un joystick à retour d'effort, destinée à des personnes handicapés moteurs ayant des difficultés à contrôler classiquement leur fauteuil.
Ce joystick devra être implémenté sur un fauteuil «  intelligent  » muni de capteurs télémétriques.
Le retour d'effort est calculé en fonction de la proximité des obstacles et aide l'utilisateur sans le contraindre à se diriger vers la direction libre.
Le premier chapitre du mémoire est une étude bibliographique portant sur les fauteuils «  intelligents  » , sur les modes de commande en téléopération, sur les interfaces haptiques en robotique et sur la modélisation des tâches de pilotage.
Le second chapitre décrit la conception d'un simulateur de pilotage de fauteuil destiné à tester des fonctionnalités nouvelles.
Le troisième et dernier chapitre porte sur un ensemble de résultats expérimentaux visant à conclure sur l'intérêt du retour d'effort pour le pilotage de fauteuils électriques et sur le choix de son algorithme de calcul.
Les paramètres testés sont notamment la configuration de l'environnement (couloir, passage de porte, espace libre, …) et la cinématique du fauteuil (traction avant, traction arrière)
Comment les Travellers anglais, gallois, écossais, irlandais et des Etats-Unis sont-ils représentés à travers quelques vieux mythes gadji qui ont la vie dure.
Ces dernières années, l'apprentissage profond a complètement changé le domaine de vision par ordinateur.
Plus rapide, donnant de meilleurs résultats, et nécessitant une expertise moindre pour être utilisé que les méthodes classiques de vision par ordinateur, l'apprentissage profond est devenu omniprésent dans tous les problèmes d'imagerie, y compris l'imagerie médicale.
Afin de trouver automatiquement des réseaux de neurones adaptés à des tâches spécifiques, nous avons ainsi apporté des contributions à l'optimisation d'hyper-paramètres de réseaux de neurones.
Cette thèse propose une comparaison de certaines méthodes d'optimisation, une amélioration en performance d'une de ces méthodes, l'optimisation bayésienne, et une nouvelle méthode d'optimisation d'hyper-paramètres basé sur la combinaison de deux méthodes existantes : l'optimisation bayésienne et hyperband.
Une fois équipés de ces outils, nous les avons utilisés pour des problèmes d'imagerie médicale : la classification de champs de vue en IRM, et la segmentation du rein en échographie 3D pour deux groupes de patients.
Cette dernière tâche a nécessité le développement d'une nouvelle méthode d'apprentissage par transfert reposant sur la modification du réseau de neurones source par l'ajout de nouvelles couches de transformations géométrique et d'intensité.
En dernière partie, cette thèse revient vers les méthodes classiques de vision par ordinateur, et nous proposons un nouvel algorithme de segmentation qui combine les méthodes de déformations de modèles et l'apprentissage profond.
Cette méthode est validé sur la tâche de la segmentation du rein en échographie 3D.
Dans de nombreux domaines, les graphes constituent une représentation naturelle efficace pour différents types de données.
Des exemples notables existent pour réaliser des analyses comportementales dans les domaines de la cybersécurité ou de l'analyse des réseaux sociaux.
Dans le premier cas, le comportement des utilisateurs sur Internet peut être observé par leurs requêtes DNS, interprétées comme les étapes successives d'un marcheur aléatoire sur un graphe dans lequel les noms de domaine sont les sommets et les arêtes représentent le comportement moyen au niveau de la population.
Il est alors possible d'étudier le comportement des utilisateurs en analysant le sous-graphe induit par des mouvements d'un unique utilisateur.
Dans le cas de l'analyse des réseaux sociaux, les représentations graphiques résultent naturellement des interactions de l'utilisateur.
Les noeuds symbolisent ainsi les utilisateurs, et leurs interactions peuvent être interprétées comme des arêtes (partage d'intérêts ou de messages).
Comprendre et analyser les structures des graphes est donc un outil clé dans de nombreux domaines d'applications réelles.
Il est donc essentiel de trouver des méthodes efficaces et robustes pour la classification ou le regroupement de noeuds ou de graphes.
Dans ce contexte, les réseaux de neurones sur graphes apparaissent comme une technologie clé, mais soulèvent des questions cruciales quant à leur robustesse face aux attaques contradictoires (adversarial attacks) et à la confidentialité des données qu'elles manipulent.
L'objectif de cette thèse est d'explorer la robustesse et la confidentialité des approches basées sur les réseaux de neurones sur graphes, en examinant des solutions combinant des algorithmes à réponses aléatoires et le chiffrement homomorphe afin d'assurer un compromis satisfaisant entre performance, robustesse et confidentialité des données.
L'objectif de cette recherche est de batir un répertoire descriptif generalise des structures interrogatives directes du francais.
Le travail est pluridisciplinaire : linguistique, informatique, documentation.
Dans la partie linguistique nous présentons la définition formelle de la transformation et des types de transformation permis pour le traitement des interrogatives.
Apres avoir explicite les conventions de notation valables pour l'ensemble du travail linguistique, nous proposons les transformations particulières portant sur les dix éléments interrogatifs du francais (qui, quoi, que, pourquoi, combien, ou, quand, comment, quel, lequel) et les inversions (est-il venu ? que fait pierre ? qui est-ce qui est venu ?...).
L'implémentation sous forme d'un répertoire vise ensuite l'incorporation des contraintes apportées par la description linguistique.
Nous expliquons en quoi consiste notre repertoire informatise des structures interrogatives du francais (risif), programme a l'aide du langage fx, et de quelle maniere nous proposons des codages et des diagnostics de séquences interrogatives.
Enfin, nous proposons une consultation de bases de donnees (cbd) a visee linguistique.
La cbd peut etre distincte du repertoire risif ou directement liee a celui-ci.
La visualisation et la comparaison de structures extraites des bases de donnees et celles analysees precedemment (dans risif) sont possibles.
On peut également modifier et creer des BD au sein de l'environnement fx lisp.
Cependant, la construction de telles ressources est coûteuse.
Elles ne sont donc disponibles que pour un nombre restreint de langues et de domaines.
De plus, les choix de formalisation de la sémantique peuvent différer selon les besoins (requêtes SQL, formules logiques,...).
Il est dans ce contexte nécessaire de développer des méthodes qui soient moins dépendantes de ces données de supervision, par exemple en exploitant des ressources linguistiques.
Dans le cadre de cette thèse, nous nous intéresserons à développer des approches jointes pour l'analyse sémantique et la génération contrôlée de textes fondées sur des architectures neuronales de type 'auto-encodeurs' : une phrase est encodée en une représentation latente (par analyse sémantique) puis regénérée à partir de cette dernière (génération contrôlée).
Nous nous focaliserons sur des approches faiblement supervisées en utilisant des connaissances sur la langue, le domaine ainsi que le formalisme visé plutôt que sur l'accumulation de données annotées.
La traduction automatique vise à traduire des documents d'une langue à une autre sans l'intervention humaine.
Avec l'apparition des réseaux de neurones profonds (DNN), la traduction automatique neuronale(NMT) a commencé à dominer le domaine, atteignant l'état de l'art pour de nombreuses langues.
Combiné avec la flexibilité architecturale des DNN, ce cadre a aussi ouvert une piste de recherche sur la multimodalité, ayant pour but d'enrichir les représentations latentes avec d'autres modalités telles que la vision ou la parole, par exemple.
Cette thèse se concentre sur la traduction automatique multimodale(MMT) en intégrant la vision comme une modalité secondaire afin d'obtenir une meilleure compréhension du langage, ancrée de façon visuelle.
J'ai travaillé spécifiquement avec un ensemble de données contenant des images et leurs descriptions traduites, où le contexte visuel peut être utile pour désambiguïser le sens des mots polysémiques, imputer des mots manquants ou déterminer le genre lors de la traduction vers une langue ayant du genre grammatical comme avec l'anglais vers le français.
Je propose deux approches principales pour intégrer la modalité visuelle : (i) un mécanisme d'attention multimodal qui apprend à prendre en compte les représentations latentes des phrases sources ainsi que les caractéristiques visuelles convolutives, (ii) une méthode qui utilise des caractéristiques visuelles globales pour amorcer les encodeurs et les décodeurs récurrents.
Grâce à une évaluation automatique et humaine réalisée sur plusieurs paires de langues, les approches proposées se sont montrées bénéfiques.
Enfin,je montre qu'en supprimant certaines informations linguistiques à travers la dégradation systématique des phrases sources, la véritable force des deux méthodes émerge en imputant avec succès les noms et les couleurs manquants.
Cette thèse est consacrée à la problématique des stratégies d'apprentissage et d'intégration, quelles que soient leurs modalités (visuelles, textuelles), destinées à réaliser efficacement des opérations de détection et d'annotation de concepts visuels, problématique qui est devenue un sujet de recherche très populaire et important ces dernières années en raison de sa large gamme d'applications : indexation et récupération d'images ou de vidéos, systèmes de contrôle d'accès, vidéo-surveillance, etc.
Le cerveau humain est composé d'un grand nombre de réseaux neuraux interconnectés, dont les neurones et les synapses en sont les briques constitutives.
Caractérisé par une faible consommation de puissance, de quelques Watts seulement, le cerveau humain est capable d'accomplir des tâches qui sont inaccessibles aux systèmes de calcul actuels, basés sur une architecture de type Von Neumann.
La conception de systèmes neuromorphiques vise à réaliser une nouvelle génération de systèmes de calcul qui ne soit pas de type Von Neumann.
L'utilisation de mémoire non-volatile innovantes en tant que synapses artificielles, pour application aux systèmes neuromorphiques, est donc étudiée dans cette thèse.
L'utilisation des dispositifs PCM en tant que synapses de type binaire et probabiliste est étudiée pour l'extraction de motifs visuels complexes, en évaluant l'impact des conditions de programmation sur la consommation de puissance au niveau du système.
Une nouvelle stratégie de programmation, qui permet de réduire l'impact du problème de la dérive de la résistance des dispositifs PCM est ensuite proposée.
Il est démontré qu'en utilisant des dispositifs de tailles réduites, il est possible de diminuer la consommation énergétique du système.
La variabilité des dispositifs OxRAM est ensuite évaluée expérimentalement par caractérisation électrique, en utilisant des méthodes statistiques, à la fois sur des dispositifs isolés et dans une matrice complète de mémoire.
Un modèle qui permets de reproduire la variabilité depuis le niveau faiblement résistif jusqu'au niveau hautement résistif est ainsi développé.
Une architecture de réseau de neurones de type convolutionnel est ensuite proposée sur la base de ces travaux éxperimentaux.
Ces travaux de thèse portent sur la reconnaissance automatique du stress chez des humains en interaction dans des situations anxiogènes : prise de parole en public, entretiens et jeux sérieux à partir d'indices
Une partie des travaux portent sur la fusion des informations apportées par les différentes modalités.
L'expression et la gestion du stress sont influencées à la fois par des différences interpersonnelles (traits de personnalité, expériences passées, milieu culturel) et contextuelles (type de stresseur, enjeux de la situation).
Les comparaisons inter-individus, et inter-corpus révèlent la diversité de l'expression du stress.
Une application de ces travaux pourrait être la conception d'outils thérapeutiques pour la maitrise du stress, notamment à destination des populations phobiques.
Le projet GenderedNews vise à proposer de nouvelles méthodes pour mesurer et expliquer le niveau de biais de genre dans les médias en France.
Ces biais peuvent être définis comme le fait que les médias d'information tendent d'une part à surpondérer les hommes par rapport aux femmes en termes de mentions et de citations, et d'autre part à attribuer aux femmes un rôle social spécifique impliquant souvent, entre autres, l'anonymat, une capacité d'action réduite dans la société et la confusion entre cette action et leur état matrimonial ou familial.
De nombreuses études empiriques ont prouvé l'existence de ces biais et ont permis de mieux les comprendre à l'échelle internationale.
Cependant, la recherche sur cette question est souvent basée sur des données limitées en volume et produites par des ONG, des administrations ou des organismes de réglementation des médias.
Ces données sont généralement traitées par des analyses de contenu manuelles qui ne permettent pas de rendre compte systématiquement des évolutions des biais sexistes dans les médias à long terme et sur un grand nombre de sources, ni d'expliquer ces biais en termes de variables telles que le financement des médias, la taille des salles de rédaction ou d'autres variables organisationnelles.
Le projet GenderedNews vise à fournir et à analyser des sources de données importantes et stables dans le temps ainsi qu'à explorer de nouvelles méthodes pour documenter les préjugés sexistes dans les médias.
Il est basé sur un programme de recherche collaboratif entre un sociologue des médias et un informaticien ayant des compétences en études des médias, études de genre, traitement du langage naturel et collecte de données numériques.
Il a également une dimension de partenariat importante dans la mesure où plusieurs médias importants sont associés et fournissent un accès à leurs données.
GenderedNews se concentre sur deux types de biais et deux mesures différentes de ces biais.
Les biais d'échantillonnage se produisent par la sélection d'un échantillon biaisé de personnes mentionnées dans les médias.
Ils peuvent être étudiés en comptant simplement combien d'hommes et de femmes ont accès à la visibilité publique d'un côté et en étudiant les modèles de cadrage des hommes et des femmes représentés de l'autre côté.
Les biais de citation proviennent de la sélection d'un échantillon biaisé de personnes qui, en plus d'être visibles, sont autorisées à exprimer leurs opinions dans les médias.
Ils peuvent également être étudiés en utilisant les deux approches : compter combien d'un côté et analyser comment de l'autre.
Dans le cadre du projet, le/la doctorante contribuera plus spécifiquement à l'étude des biais de citation.
Tout professionnel de la santé est sujet devant un patient à une incertitude inhérente à la pratique médicale.
Dans le cas d'incident médical lors d'un trajet aérien, cette incertitude comporte trois sources additionnelles : (1) variabilité des conditions aéronautiques, (2) variabilité individuelle des conditions du patient, (3) variabilité individuelle des compétences de l'intervenant.
Aujourd'hui les incidents médicaux dans l'avion sont estimés à 350 par jour dans le monde et lorsqu'ils surviennent, ils sont pris en charge dans 95% des cas par des professionnels de la santé passagers qui se portent volontaires.
C'est souvent pour eux une première expérience.
A part l'assistance à distance par télémédecine l'intervenant, souvent seul face à ses doutes et son incertitude, ne dispose d'aucune autre aide à bord.
Par ailleurs l'aviation civile dispose de systèmes de retour d'expérience (RETEX) pour gérer la complexité de tels processus.
Des politiques de recueil et d'analyse des événements sont mises en place à l'échelle internationale, par exemple ECCAIRS (European Co-ordination Centre for Accident and Incident Reporting Systems) et ASRS (Aviation Safety Reporting System).Dans ce travail de thèse, nous proposons tout d'abord une formalisation sémantique basée sur les ontologies pour préciser conceptuellement le vocabulaire des incidents médicaux se produisant durant les vols commerciaux.
Enfin, nous proposons une architecture de Système d'Aide à la Décision Médicale (SADM) qui intègre la gestion des incertitudes présentes tant sur les données récoltées que les niveaux de compétences des professionnels médicaux intervenants.
Afin d'atteindre cet objectif, un système de recherche d'information doit représenter, stocker et organiser l'information, puis fournir à l'utilisateur les éléments correspondant au besoin d'information exprimé par sa requête.
La plupart des systèmes de recherche d'information (SRI) utilisent des termes simples pour indexer et retrouver des documents.
Cependant, cette représentation n'est pas assez précise pour représenter le contenu des documents et des requêtes, du fait de l'ambiguïté des termes isolés de leur contexte.
Une solution à ce problème consiste à utiliser des termes complexes à la place de termes simples isolés.
Cette approche se fonde sur l'hypothèse qu'un terme complexe est moins ambigu qu'un terme simple isolé.
Notre thèse s'inscrit dans le cadre de la recherche d'information dans un domaine de spécialité en langue arabe.
L'objectif de notre travail a été d'une part, d'identifier les termes complexes présents dans les requêtes et les documents.
D'autre part, d'exploiter pleinement la richesse de la langue en combinant plusieurs connaissances linguistiques appartenant aux niveaux morphologique et syntaxique, et de montrer comment l'apport de connaissances morphologiques et syntaxiques permet d'améliorer l'accès à l'information.
En outre, nous avons avons défini linguistiquement les termes complexes en langue arabe et nous avons développé un système d'identification de termes complexes sur corpus qui produit des résultats de bonne qualité en terme de précision, en s'appuyant sur une approche mixte qui combine modèle statistique et données linguistiques
La duplication de code source a de nombreuses origines : copie et adaptation inter-projets ou clonage au sein d'un même projet.
Rechercher des correspondances de code copié permet de le factoriser dans un projet ou de mettre en évidence des situations de plagiat.
Nous étudions des méthodes statiques de recherche de similarité sur du code ayant potentiellement subi des opérations d'édition telle que l'insertion, la suppression, la transposition ainsi que la factorisation et le développement de fonctions.
Des techniques d'identification de similarité génomique sont examinées et adaptées au contexte de la recherche de clones de code source sous forme lexemisée.
Après une discussion sur des procédés d'alignement de lexèmes et de recherche par empreintes de n-grams, est présentée une méthode de factorisation fusionnant les graphes d'appels de fonctions de projets au sein d'un graphe unique avec introduction de fonctions synthétiques exprimant les correspondances imbriquées.
Elle utilise des structures d'indexation de suffixes pour la détermination de facteurs répétés.
Une autre voie d'exploration permettant de manipuler de grandes bases indexées de code par arbre de syntaxe est abordée avec la recherche de sous-arbres similaires par leur hachage et leur indexation selon des profils d'abstraction variables.
En amont et en aval de la recherche de correspondances, des métriques de similarité sont définies afin de préselectionner les zones d'examen, affiner la recherche ou mieux représenter les résultats
Le but de ce projet est d'explorer et d'exploiter les techniques de classification des documents et traitement du langage naturel en mettant en œuvre les systèmes multi-agents afin de répondre à un besoin portant sur l'analyse et de classification des sites web chez Olfeo.
La représentation du temps et de l'espace est une tâche importante dans de nombreux domaines de l'Intelligence Artificielle tels que le traitement du langage naturel, les systèmes d'informations géographiques (GIS), la conception assistée par ordinateur (CAO), la navigation de robots.
De nombreux formalismes qualitatifs ont été proposés pour représenter un ensemble d'entités spatiales ou temporelles et leurs relations.
La plupart de ces formalismes utilisent des réseaux de contraintes qualitatives (RCQ en abrégé) pour représenter l'ensemble des informations d'un système.
Dans certaines applications, en particulier de type multi-agents, plusieurs sources d'informations peuvent chacune fournir un réseau de contraintes qualitatives pour représenter leur connaissance sur l'ensemble des positions relatives d'un ensemble d'objets.
La multiplicité des sources d'informations fournissant les RCQ fait que souvent ces RCQ sont conflictuels, et il est alors utile de mettre en oeuvre une méthode de fusion de ces réseaux pour résoudre les conflits.
De nombreux opérateurs de fusion ont été définis dans le cadre de la logique propositionnelle.
En s'inspirant en partie de ces travaux, nous élaborons des processus de fusion spécifiques aux RCQ et nous en étudions les propriétés logiques.
Cette thèse met l'accent sur l'expression linguistique des sentiments et des émotions dans un corpus jusque là peu interrogé dans ce type d'études, en l'occurrence les formes de communication instantanée rendues possibles par les nouvelles technologies et qui semblent prédisposées à accueillir de nombreux marqueurs expressifs.
Elle déplace le curseur des études sur l'expression linguistique de ces deux catégories affectives du système vers l'emploi, en interrogeant un corpus formé de quatre formes de communication : blogs, forums de discussion, réseau Facebook et plateforme de microblogging Twitter.
Le travail ancre la réflexion au niveau cognitif en cherchant à montrer, dans une perspective dynamique, comment se construit ce type de discours dans l'interaction médiatisée.
Il aborde ainsi les différentes manifestations linguistiques et extralinguistiques qui chargent ces écrits électroniques d'une dimension émotionnelle ouvrant sur une dimension interactive intense.
Il permet une réflexion sur les frontières écrit / oral et sur la naissance d'un nouveau langage expressif propre aux écrits électroniques.
Avec le développement de l'Internet des Objets, la réalisation d'environnements composés de diverses ressources connectées (objets, capteurs, services, données, etc.) devient une réalite tangible.
La création de telles applications va cependant de pair avec le design d'outils supportant les utilisateurs en mobilité, en particulier afin de réaliser la sélection la plus efficace possible des ressources de l'environnement dans lequel l'utilisateur se trouve.
Tandis qu'une telle sélection requiert la définition de modèles permettant de décrire de façon précise les caractéristiques de ces ressources, elle doit également prendre en compte les profils et préférences utilisateurs.
Enfin, l'augmentation du nombre de ressources connectées, potentiellement mobiles, requiert également le développement de processus de sélection qui “passent à l'échelle”.
Des avancées dans ce champ de recherche restent encore à faire, notamment à cause d'une connaissance assez floue concernant les acteurs (ainsi que leurs interactions) définissant (i.e., prenant part à) l'éco-système qu'est un “espace intelligent”.
En outre, la multiplicité de diverses ressources connectées implique des problèmes d'interopérabilité et de scalabilité qu'il est nécessaire d'adresser.
Si le Web Sémantique apporte une réponse à des problèmes d'interopérabilité, il en soulève d'autres liés au passage à l'échelle.
S'appuyant sur mes recherches conduites au sein des Bell Labs, cette dissertation identifie les interactions entre les différents acteurs de cet éco-système et propose des représentations formelles, basées sur une sémantique, permettant de décrire ces acteurs.
Cette dissertation propose également des procédures de recherche, permettant à l'utilisateur (ou ses applications) de trouver des ressources connectées en se basant sur l'analyse de leur description sémantique.
En particulier, ces procédures s'appuient sur une architecture distribuée, également décrite dans cette dissertation, afin de permettre un passage à l'échelle.
Cette thèse porte sur le développement d'une chaîne de traitement complète pour réaliser des tâches de reconnaissance d'écriture manuscrite non contrainte.
Trois difficultés majeures sont à résoudre : l'étape du prétraitement, l'étape de la modélisation optique et l'étape de la modélisation du langage.
Au stade des prétraitements il faut extraire correctement les lignes de texte à partir de l'image du document.
Une méthode de segmentation itérative en lignes utilisant des filtres orientables a été développée à cette fin.
La difficulté dans l'étape de la modélisation optique vient de la diversité stylistique des scripts d'écriture manuscrite.
Les modèles optiques statistiques développés sont des modèles de Markov cachés (HMM-GMM) et les modèles de réseaux de neurones récurrents (BLSTM-CTC).
Les réseaux récurrents permettent d'atteindre les performances de l'état de l'art sur les deux bases de référence RIMES (pour le Français) et IAM (pour l'anglais).
L'étape de modélisation du langage implique l'intégration d'un lexique et d'un modèle de langage statistique afin de rechercher parmi les hypothèses proposées par le modèle optique, la séquence de mots (phrase) la plus probable du point de vue linguistique.
La difficulté à ce stade est liée à l'obtention d'un modèle de couverture lexicale optimale avec un minimum de mots hors vocabulaire (OOV).
Pour cela nous introduisons une modélisation en sous-unités lexicales composée soit de syllabes soit de multigrammes.
Ces modèles couvrent efficacement une partie importante des mots hors vocabulaire.
Elles sont équivalentes aux modèles traditionnels en présence d'un faible taux de mots hors lexique.
Grâce à la taille compacte du modèle de langage reposant sur des unités sous-lexicales, un système de reconnaissance multilingue unifié a été réalisé.
Au cours des dernières années, les réseaux de type (NDN) sont devenus une des architectures réseau les plus prometteuses.
Pour être adopté à l'échelle d'Internet, NDN doit résoudre les problèmes inhérents à l'Internet actuel.
En supposant (i) qu'un ordinateur appartient au réseau d'une entreprise basée sur une architecture NDN, (ii) que l'ordinateur a déjà été compromis par un support malveillant, et (iii) que la société installe un pare-feu, la thèse évalue la situation dans laquelle l'ordinateur infecté tente de divulguer des données à un attaquant externe à l'entreprise.
Les contributions de cette thèse sont au nombre de cinq.
Tout d'abord, cette thèse propose une attaque par fuite d'informations via un paquet donné et un paquet intérêt propres à NDN.
Deuxièmement, afin de remédier à l'attaque fuite d'informations, cette thèse propose un pare-feu basé sur l'utilisation d'une liste blanche et d'une liste noire afin de surveiller et traiter le trafic NDN provenant des consommateurs.
Troisièmement, cette thèse propose un filtre de noms NDN pour classifier un nom dans un paquet d'intérêt comme étant légitime ou non.
Pour prendre en compte le flux de trafic analysé par le pare-feu NDN, cette thèse propose comme quatrième contribution la surveillance du flux NDN à travers le pare-feu.
Enfin, afin de traiter les inconvénients du filtre de noms NDN, cette thèse propose un filtre de flux NDN permettant de classer un flux comme légitime ou non.
L'évaluation des performances montre que le filtre de flux complète de manière tout à fait performante le filtre de nom et réduit considérablement le débit de fuite d'informations
La thèse actuelle présente une étude ERP du traitement du stress métrique en français.
En effet, l'accentuation métrique joue un rôle important dans la compréhension des langues comme l'anglais et le néérlandais, mais son rôle dans le traitement du français n'est pas bien connu.
Le français est une langue traditionnellement décrite sans accent.
Cette thèse remet en question cette vision traditionnelle et s'aligne sur deux modèles métriques d'accentuation français, proposant que l'accent est encodé dans des patrons cognitifs sous-jacents.
Dans notre étude interdisciplinaire en français sur le traitement des contraintes métriques, nous adoptons une approche fonctionnelle.
Nous utilisons la méthode des potentiels évoqués (ERP), qui nous fournit une mesure extrêmement sensible et précise nous permettant de déterminer s'il existe un accent métrique en français et dans quelle mesure l'accent métrique aide l'auditeur à comprendre la parole.
Profitant de la quantité d'information maintenant disponible, la recherche et l'industrie se sont mises en quête de moyens pour analyser automatiquement les opinions exprimées dans les textes.
Pour nos travaux, nous nous plaçons dans un contexte multilingue et multi-domaine afin d'explorer la classification automatique et adaptative de polarité.
Nous proposons dans un premier temps de répondre au manque de ressources lexicales par une méthode de construction automatique de lexiques affectifs multilingues à partir de microblogs.
Pour une meilleure analyse des textes, nous proposons aussi de remplacer le traditionnel modèle n
-gramme par une représentation à base d'arbres de dépendances syntaxiques.
Dans notre modèles, les n-grammes ne sont plus construits à partir des mots mais des triplets constitutifs des dépendances syntaxiques.
Cette manière de procéder permet d'éviter la perte d'information que l'on obtient avec les approches classiques à base de sacs de mots qui supposent que les mots sont indépendants.
Nos propositions ont fait l'objet d'évaluations quantitatives pour différents domaines d'applications (les films, les revues de produits commerciaux, les nouvelles et les blogs) et pour plusieurs langues (anglais, français, russe, espagnol et chinois), avec en particulier une participation officielle à plusieurs campagnes d'évaluation internationales (SemEval 2010, ROMIP 2011, I2B2 2011).
Cette thèse étudie l'adverbe autrement, au travers de ses trois emplois principaux : adverbe de manière, connecteur d'hypothèse négative, et rupteur de topique.
L'accent est mis sur son fonctionnement anaphorique et son rôle dans la structure du discours.
Après avoir passé en revue les théories du discours et la littérature sur l'adverbe, on dégage les propriétés des trois emplois grâce à des énoncés tirés de corpus oraux et écrits, en montrant comment le contexte sert à la récupération de l'antécédent et comment l'adverbe s'appuie sur le discours et le construit en même temps.
Dès l'adverbe de manière, anaphore et portée droite sont essentielles à la construction du sens.
Avec le connecteur, les relations référentielles laissent place aux relations logiques de proposition à proposition, tandis que le rupteur de topique est un emploi métalinguistique portant sur des constituants abstraits du discours.
Un noyau de sens [l'anaphore et la négation] est dégagé, commun aux trois emplois et permettant d'envisager des points de passage entre eux.
Cette étude synchronique est ensuite mise à profit pour reconstruire la grammaticalisation de l'adverbe, le détail des observations présentes contrebalançant la rareté des données historiques.
On montre que c'est à travers la notion de construction, c'est-à-dire l'emploi de l'adverbe dans certains contextes, que l'évolution a pu avoir lieu : en particulier, l'ordre des mots en ancien français a été crucial, permettant à l'adverbe de manière d'occuper la position initiale propice à la réanalyse ;
La Chirurgie Augmentée fait appel à des dispositifs médicaux (Dispositifs de Chirurgie Augmentée ou DCA) permettant au chirurgien de mieux se repérer dans l'espace, et donc d'enrichir son environnement chirurgical en vue de faciliter la réalisation de son geste.
L'essor de ces dispositifs, par leur multiplication et par leur médiatisation, a amené les pouvoirs publics à s'interroger sur la qualité associée aux interventions assistées de ces appareils.
Dans ce travail, nous illustrons la problématique de la Qualité associée aux interventions assistées de DCA par une description historique du premier robot médical actif utilisé pour la pose de prothèses totales de hanche.
Nous abordons ensuite la notion de la qualité en médecine en général puis de la qualité des DCA en particulier.
Nous verrons qu'il n'y a pas de dispositions spécifiques pour ces dispositifs et qu'il n'apparaît pas adéquat de parler de la qualité d'un DCA sans prendre en compte l'environnement dans lequel il est utilisé.
C'est pourquoi il est essentiel de structurer l'usage de ces dispositifs ainsi que l'environnement dans lequel ils sont utilisés.
Une des manières de structurer cet environnement est d'utiliser les ontologies.
En utilisant la fonction d'édition d'ontologies du logiciel ISIS, nous avons modélisé une intervention chirurgicale pour insuffisance ligamentaire du ligament croisé antérieur, avec et sans DCA, ainsi que l'environnement associé.
Cette représentation ontologique est constituée d'un ensemble de 45 Diagrammes Ontologiques (DO) comportant au total 1072 concepts.
Nous décrivons le matériel et la méthode utilisés pour construire l'ensemble de ces diagrammes.
Pour parler de la qualité des DCA, un utilisateur peut créer son système d'information à partir de notre modèle ontologique afin de disposer de ses propres indicateurs.
La validation de notre modèle structurel a été réalisée par un expert à travers un scénario d'une intervention chirurgicale, créé à partir du modèle ontologique.
Nous abordons enfin les perspectives possibles de notre travail.
Un processus ponctuel déterminantal (DPP) génère des configurations aléatoires de points ayant tendance à se repousser.
La notion de répulsion est encodée par les sous-déterminants d'une matrice à noyau, au sens des méthodes à noyau en apprentissage artificiel.
Cette forme algébrique particulière confère aux DPP de nombreux avantages statistiques et computationnels.
Cette thèse porte sur l'échantillonnage des DPP, c'est à dire sur la conception d'algorithmes de simulation pour ce type de processus.
Les motivations pratiques sont l'intégration numérique, les systèmes de recommandation ou encore la génération de résumés de grands corpus de données.
Dans le cadre fini, nous établissons la correspondance entre la simulation de DPP spécifiques, dits de projection, et la résolution d'un problème d'optimisation linéaire dont les contraintes sont randomisées.
Nous en tirons une méthode efficace d'échantillonnage par chaîne de Markov.
Dans le cadre continu, certains DPP classiques peuvent être simulés par le calcul des valeurs propres de matrices tridiagonales aléatoires bien choisies.
Nous en fournissons une nouvelle preuve élémentaire et unificatrice, dont nous tirons également un échantillonneur approché pour des modèles plus généraux.
En dimension supérieure, nous nous concentrons sur une classe de DPP utilisée en intégration numérique.
Nous proposons une implémentation efficace d'un schéma d'échantillonnage exact connu, qui nous permet de comparer les propriétés d'estimateurs Monte Carlo dans de nouveaux régimes.
En vue d'une recherche reproductible, nous développons une boîte à outils open-source, nommée DPPy, regroupant les différents outils d'échantillonnage sur les DPP.
Dans cette thèse, nous proposons une nouvelle methode, Triangular Similarity Metric Learning (TSML), pour spécifier une fonction métrique de données automatiquement.
Le système TSML proposée repose une architecture Siamese qui se compose de deux sous-systèmes identiques partageant le même ensemble de paramètres.
Chaque sous-système traite un seul échantillon de données et donc le système entier reçoit une paire de données en entrée.
Le système TSML comprend une fonction de coût qui définit la relation entre chaque paire de données et une fonction de projection permettant l'apprentissage des formes de haut niveau.
Pour la fonction de coût, nous proposons d'abord la similarité triangulaire (Triangular Similarity), une nouvelle similarité métrique qui équivaut à la similarité cosinus.
Sur la base d'une version simplifiée de la similarité triangulaire, nous proposons la fonction triangulaire (the triangular loss) afin d'effectuer l'apprentissage de métrique, en augmentant la similarité entre deux vecteurs dans la même classe et en diminuant la similarité entre deux vecteurs de classes différentes.
Par rapport aux autres distances ou similarités, la fonction triangulaire et sa fonction gradient nous offrent naturellement une interprétation géométrique intuitive et intéressante qui explicite l'objectif d'apprentissage de métrique.
En ce qui concerne la fonction de projection, nous présentons trois fonctions différentes : une projection linéaire qui est réalisée par une matrice simple, une projection non-linéaire qui est réalisée par Multi-layer Perceptrons (MLP) et une projection non-linéaire profonde qui est réalisée par Convolutional Neural Networks (CNN).
Avec ces fonctions de projection, nous proposons trois systèmes de TSML pour plusieurs applications : la vérification par paires, l'identification d'objet, la réduction de la dimensionnalité et la visualisation de données.
Pour chaque application, nous présentons des expérimentations détaillées sur des ensembles de données de référence afin de démontrer l'efficacité de notre systèmes de TSML.
Cette thèse est consacrée à l'étude des syntagmes nominaux simples en chinois mandarin.
Le deuxième chapitre vise à présenter les caractéristiques les plus saillantes des syntagmes nominaux en mandarin.
Le troisième chapitre se centre sur les propriétés distributionnelles de treize quantificateurs indéfinis.
Le quatrième chapitre met au jour la double fonction du quantificateur indéfini yīdiǎnr 'un peu de'.
Ce travail de recherche se donne pour objectif de formuler des éléments de réflexion indispensables à une initiation à l'écrit de recherche universitaire, pour venir en aide aux étudiants locuteurs non natifs.
Plusieurs questions ont sous-tendu cette étude : quel rôle pourraient avoir les études descriptives des écrits scientifiques dans une familiarisation réussie à l'écrit de recherche ?
Quel est l'intérêt d'une initiation aux fonctions rhétoriques basée sur la phraséologie transdisciplinaire et fondée sur une approche dite par genre ?
Est-il possible de soumettre des éléments d'aide dont pourraient profiter tous les étudiants quelles que soient leurs disciplines ?
Une étude exploratoire autour d'une fonction rhétorique particulière qu'est "le positionnement" a permis de comprendre dans quelle mesure des éléments d'ordre linguistique, en l'occurrence les collocations transdisciplinaires, pourraient aider les étudiants à moins appréhender cette exigence d'un écrit essentiellement polyphonique et argumentatif ou encore à se positionner davantage.
Le crowdsourcing est une technique qui permet de recueillir une large quantité de données d'une manière rapide et peu onéreuse.
Néanmoins, La disparité comportementale et de performances des "workers" d'une part et la variété en termes de contenu et de présentation des tâches par ailleurs influent considérablement sur la qualité des contributions recueillies.
Par conséquent, garder leur légitimité impose aux plateformes de crowdsourcing de se doter de mécanismes permettant l'obtention de réponses fiables et de qualité dans un délai et avec un budget optimisé.
Dans cette thèse, nous proposons CAWS (Context AwareWorker Selection), une méthode de contrôle de la qualité des contributions dans le crowdsourcing visant à optimiser le délai de réponse et le coût des campagnes.
CAWS se compose de deux phases, une phase d'apprentissage opérant hors-ligne et pendant laquelle les tâches de l'historique sont regroupées de manière homogène sous forme de clusters.
Pour chaque cluster, un profil type optimisant la qualité des réponses aux tâches le composant, est inféré ;
la seconde phase permet à l'arrivée d'une nouvelle tâche de sélectionner les meilleurs workers connectés pour y répondre.
Il s'agit des workers dont le profil présente une forte similarité avec le profil type du cluster de tâches, duquel la tâche nouvellement créée est la plus proche.
En outre, CrowdED rend possible la comparaison de méthodes de contrôle de qualité quelle que soient leurs catégories, du fait du respect d'un cahier des charges lors de sa constitution.
Les résultats de l'évaluation de CAWS en utilisant CrowdED comparés aux méthodes concurrentes basées sur la sélection de workers, donnent des résultats meilleurs, surtout en cas de contraintes temporelles et budgétaires fortes.
Les expérimentations réalisées avec un historique structuré en catégories donnent des résultats comparables à des jeux de données où les taches sont volontairement regroupées de manière homogène.
La dernière contribution de la thèse est un outil appelé CREX (CReate Enrich eXtend) dont le rôle est de permettre la création, l'extension ou l'enrichissement de jeux de données destinés à tester des méthodes de crowdsourcing.
Il propose des modules extensibles de vectorisation, de clusterisation et d'échantillonnages et permet une génération automatique d'une campagne de crowdsourcing.
La tâche d'exploration dans des ressources inexploitées mais nouvellement numérisées, afin d'y trouver des informations pertinentes, est complexifiée par la quantité de ressources disponibles.
Grâce au projet ANR CIRESFI, la ressource la plus importante, pour la Comédie-Italienne du XVIIIe siècle, est un ensemble de registres comptables constituée de 28 000 pages.
L'extraction d'informations est un processus long et complexe qui demande une expertise à chaque étape : détection et segmentation, extraction de caractéristiques, reconnaissance d'écriture manuscrite.
Les systèmes à base de réseaux de neurones profonds dominent dans l'ensemble ces approches.
Le problème majeur est qu'ils nécessitent d'avoir une grande quantité de données pour réaliser leur apprentissage.
Cependant, les registres de la Comédie-Italienne ne possèdent pas de vérité terrain.
Pour palier ce manque de données, nous explorons des approches pouvant opérer un apprentissage par transfert de connaissance.
Cela signifie utiliser un ensemble de données déjà étiquetées et disponibles, possédant un minimum de points communs avec nos données pour entraîner les systèmes, pour ensuite les appliquer sur nos données.
L'ensemble de nos expérimentations nous ont montré la difficulté de réaliser cette tâche, chaque choix à chaque étape ayant un impact fort sur la suite du système.
Nous convergeons vers une solution séparant le modèle optique du modèle de langage afin de réaliser un apprentissage indépendant avec différents types de ressources disponibles et se rejoignant grâce à une projection de l'ensemble des informations dans un espace commun nonlatent.
XML est devenu le format standard d'échange de données.
Nous souhaitons construire un environnement multi-système où des systèmes locaux travaillent en harmonie avec un système global, qui est une évolution conservatrice des systèmes locaux.
Dans cet environnement, l'échange de données se fait dans les deux sens.
Nous proposons des outils pour faciliter l'évolution de base de données XML.
Des expériences ont été menées sur des données synthétiques et réelles pour montrer l'efficacité de nos méthodes.
Ce travail évalue les possibilités d'automatisation d'une analyse morphologique des mots de la langue russe.
Cette analyse est soumise à deux contraintes principales :
- elle ne peut s'appuyer que sur la connaissance du mot seul et non du contexte dont il a été tiré.
- aucune racine n'est connue a priori, à l'exception des racines entrant dans la formation : de dérives homonymes, de dérives dont la segmentation ne peut être contrôlée par des règles simples.
Une grande partie de ce travail consiste tout naturellement à définir les facteurs autorisant une réduction sensible de cet ensemble de racines.
L'analyse est conduite à l'aide : 1) de trois ensembles de morphèmes (préfixes, suffixes, désinences) auxquels viennent s'ajouter les deux formes du pronom réfléchi postposé, 2) de règles de reconnaissance des mots n'appartenant pas au fond slave (mots étrangers).
L'analyse doit aboutir à la production d'un grammatème (fiche signalétique) des mots analysés aussi réduit que possible.
La réduction du grammatème résulte de l'intersection des ensembles d'informations liées à chacun des éléments morphémiques entrant dans la composition des mots à analyser.
Cette thèse aborde le défi de la conception des systèmes de mobilité urbaine.
Elle vise à développer un modèle d'expérience-voyageur pour faciliter, dans une démarche de conception, le diagnostic des problèmes de voyage et améliorer la pertinence des modèles de transport pour les voyageurs.
En combinant les points de vue de la conception de l'expérience-utilisateur et du transport, elle contribue à approfondir la compréhension de comment les voyageurs vivent leur voyage et particulièrement des problèmes qu'ils rencontrent.
Le premier axe d'investigation est lié à la modélisation de l'expérience-voyageur pour alimenter un diagnostic pertinent et riche des problèmes de voyage.
Dans un deuxième axe, les voyageurs sont impliqués, par une démarche de théorie ancrée, pour identifier les problèmes qu'ils rencontrent lors de l'utilisation de systèmes de mobilité urbaine au moyen de stimuli appropriés.
Un troisième axe introduit des attributs subjectifs de voyage dans des modèles de transport afin d'améliorer leur précision
Cette recherche utilise la recherche-action comme méthodologie.
Elle combine revue de littérature dans les disciplines de conception et de transport, quatre observations terrain, quinze interviews en profondeurs aves des voyageurs et experts en transport, cinq ateliers de problématisation, et deux expérimentations, dans une amélioration cyclique des résultats.
Les différentes utilisations du modèle ont permis un diagnostic approfondi de trois systèmes de mobilité urbaine (train de banlieue, bus à la demande, navette sur voie dédiée) et la mise au point d'attributs centrés sur le voyageur pour un modèle d'optimisation et une simulation multi-agents qui ont été testé par une enquête de plus de 450 participants.
Dans l'ère de l'information et de la connaissance, la traduction automatique (TA) devient progressivement un outil indispensable pour transposer la signification d'un texte d'une langue source vers une langue cible.
La TA des noms propres (NP), en particulier, joue un rôle crucial dans ce processus,puisqu'elle permet une identification précise des personnes, des lieux, des organisations et des artefacts à travers les langues.
Malgré un grand nombre d'études et des résultats significatifs concernant la reconnaissance d'entités nommées (dont le nom propre fait partie) dans la communauté de TAL dans le monde, il n'existe presque aucune recherche sur la traduction automatique des noms propres (TANP) pour le vietnamien.
En raison des caractéristiques différentes d'écriture de NP, la translittération ou la transcription et la traduction de plusieurs de langues incluant l'anglais, le français, le russe, le chinois, etc. vers levietnamien, le TANP de ces langues vers le vietnamien est stimulant et problématique.
Cette étude se concentre sur les problèmes de TANP d'anglais vers le vietnamien et de français vers le vietnamien résultant du moteurs courants de la TA et présente les solutions de prétraitement de ces problèmes pour améliorer la qualité de la TA.
A travers l'analyse et la classification d'erreurs de la TANP faites sur deux corpus parallèles de textes avec PN (anglais-vietnamien et français-vietnamien), nous proposons les solutions concernant deux problématiques importantes : (1) l'annotation de corpus, afin de préparer des bases de données pour le prétraitement et (2) la création d'un programme pour prétraiter automatiquement les corpus annotés, afin de réduire les erreurs de la TANP et d'améliorer la qualité de traduction des systèmes de TA, tels que Google, Vietgle, Bing et EVTran.
L'efficacité de différentes méthodes d'annotation des corpus avec des NP ainsi que les taux d'erreurs de la TANP avant et après l'application du programme de prétraitement sur les deux corpus annotés est comparés et discutés dans cette thèse.
Ils prouvent que le prétraitement réduit significativement le taux d'erreurs de la TANP et, par la même, contribue à l'amélioration de traduction automatique vers la langue vietnamienne.
L'objectif général des travaux à réaliser est de proposer des méthodes flexibles et robustes pour fusionner les connaissances en domaine ouvert.
L'électronique organique est un domaine de recherche qui vise à développer de nouvelles technologies basées sur des matériaux semi-conducteurs organiques (SCOs).
D'une manière générale, deux approches sont utilisées pour le design moléculaire de SCOs.
La première approche consiste à assembler des fragments moléculaires, connus pour certaines propriétés, afin de synthétiser des matériaux fonctionnels pour une application visée comme les diodes électrophosphorescentes organiques (PhOLEDs).
La seconde approche est plus exploratrice et consiste à développer des nouveaux fragments moléculaires pouvant posséder une ou plusieurs propriétés souhaitées pour une application donnée.
Dans ces travaux de thèse les deux approches ont été développées.
D'un côté, nous avons élaboré des matériaux hôtes pour des PhOLEDs en ajustant leur propriétés (première approche), et, de l'autre côté, nous nous sommes intéressés à une toute nouvelle génération de SCOs : les anneaux moléculaires (seconde approche).
Ses travaux ont permis la fabrication de PhOLEDs rouge, verte et bleue présentant les performances globales les plus élevées de la littérature.
Ces travaux nous ont permis d'incorporer pour la première fois des anneaux moléculaires dans des transistors organiques à effet de champs afin d'en étudier les propriétés de transport.
Cette thèse s'inscrit dans le cadre d'une étude sur le potentiel de la transcription automatique pour l'instrumentation de situations pédagogiques.
Notre contribution porte sur plusieurs axes.
Dans un premier temps, nous décrivons l'enrichissement et l'annotation du corpus COCo que nous avons réalisés dans le cadre du projet ANR PASTEL.
Dans ce cadre multi-thématiques, nous nous sommes ensuite intéressés à la problématique de l'adaptation linguistique des systèmes de reconnaissance automatique de la parole (SRAP).
La proposition d'adaptation des modèles s'appuie à la fois sur les supports de présentation de cours fournis par les enseignants et sur des données spécialisées récoltées automatiquement à partir du web.
Ainsi, nous avons proposé deux protocoles d'évaluation.
Le premier porte sur une évaluation intrinsèque, permettant d'estimer la performance seulement pour des mots spécialisés de chacun des cours (IWER_Average).
D'autre part, nous proposons une évaluation extrinsèque, qui estime la performance pour deux tâches exploitant la transcription : la recherche d'informations et l'indexabilité.
L'adaptation reposant sur une collecte de données issues du web, nous avons cherché à rendre compte de la reproductibilité des résultats sur l'adaptation de modèles de langage en comparant les performances obtenues sur une longue période temporelle.
Nos résultats expérimentaux montrent que même si les données sur le web changent en partie d'une période à l'autre, la variabilité de la performance des systèmes de transcription adaptés est restée non significative à partir d'un nombre minimum de documents collectés.
Enfin, nous avons proposé une approche permettant de structurer la sortie de la transcription automatique en segmentant thématiquement la transcription et en alignant la transcription avec les diapositives des supports de cours.
Pour la segmentation, l'intégration de l'information de changement de diapositives dans l'algorithme TextTiling apporte un gain significatif en termes de F-mesure.
Pour l'alignement, nous avons développé une technique basé sur des représentations TF-IDF en imposant une contrainte pour respecter l'ordre séquentiel des diapositives et des segments de transcription et nous avons vérifié la fiabilité de l'approche utilisée à l'aide d'une mesure de confiance.
Les méthodes d'analyse rythmique existantes se concentrent généralement sur l'un de ces aspects à la fois et n'exploitent pas la richesse de la structure musicale, ce qui compromet la cohérence musicale des estimations automatiques.
Dans ce travail, nous proposons de nouvelles approches tirant parti des informations multi-échelles pour l'analyse automatique du rythme.
Nos modèles prennent en compte des interdépendances intrinsèques aux signaux audio de musique, en permettant ainsi l'interaction entre différentes échelles de temps et en assurant la cohérence musicale entre elles.
Ce système est conçu pour tirer parti des informations de structure musicale (c'est-à-dire des répétitions de sections musicales) dans un cadre unifié.
Nous proposons également un modèle linguistique pour la détection conjointe des temps et du micro-timing dans la musique afro-latino-américaine.
De plus, notre modèle d'estimation conjointe des temps et du microtiming représente une avancée vers des systèmes plus interprétables.
Les méthodes présentées ici offrent des alternatives nouvelles et plus holistiques pour l'analyse numérique du rythme, ouvrant des perspectives vers une analyse automatique plus complète de la musique.
Au cours des dernières années, l'apprentissage profond est devenu l'approche privilégiée pour le développement d'une intelligence artificielle moderne (IA).
L'augmentation importante de la puissance de calcul, ainsi que la quantité sans cesse croissante de données disponibles ont fait des réseaux de neurones profonds la solution la plus performante pour la resolution de problèmes complexes.
Pour résoudre ce problème, les réseaux de neurones basés sur les algèbres des nombres complexes et hypercomplexes ont été développés.
En particulier, les réseaux de neurones de quaternions (QNN) ont été proposés pour traiter les données tridimensionnelles et quadridimensionnelles, sur la base des quaternions représentant des rotations dans notre espace tridimensionnel.
Malheureusement, et contrairement aux réseaux de neurones à valeurs complexes qui sont de nos jours acceptés comme une alternative aux réseaux de neurones réels, les QNNs souffrent de nombreuses lacunes qui sont en partie comblées par les différents travaux détaillés par ce manuscrit.
Ainsi, la thèse se compose de trois parties qui introduisent progressivement les concepts manquants, afin de faire des QNNs une alternative aux réseaux neuronaux à valeurs réelles.
La premiere partie présente et répertorie les précédentes découvertes relatives aux quaternions et aux réseaux de neurones de quaternions, afin de définir une base pour la construction des QNNs modernes.
La deuxième partie introduit des réseaux neuronaux de quaternions état de l'art, afin de permettre une comparaison dans des contextes identiques avec les architectures modernes traditionnelles.
Plus précisément, les QNNs étaient majoritairement limités par leurs architectures trop simples, souvent composées d'une seule couche cachée comportant peu de neurones.
Premièrement, les paradigmes fondamentaux, tels que les autoencodeurs et les réseaux de neurones profonds sont présentés.
Ensuite, les très répandus et étudiés réseaux de neurones convolutionnels et récurrents sont étendus à l'espace des quaternions.
Dans un scénario traditionnel impliquant des QNNs, les caractéristiques d'entrée sont manuellement segmentées en quatre composants, afin de correspondre à la representation induite par les quaternions.
Malheureusement, il est difficile d'assurer qu'une telle segmentation est optimale pour résoudre le problème considéré.
De plus, une segmentation manuelle réduit fondamentalement l'application des QNNs à des tâches naturellement définies dans un espace à au plus quatre dimensions.
De ce fait, la troisième partie de cette thèse introduit un modèle supervisé et un modèle non supervisé permettant l'extraction de caractéristiques d'entrée désentrelacées et significatives dans l'espace des quaternions, à partir de n'importe quel type de signal réel uni-dimentionnel, permettant l'utilisation des QNNs indépendamment de la dimensionnalité des vecteurs d'entrée et de la tâche considérée.
Les expériences menées sur la reconnaissance de la parole et la classification de documents parlés montrent que les approches proposées sont plus performantes que les représentations traditionnelles de quaternions.
Le choix d'un système de gestion de bases de données (SGBD) et de plateforme d'exécution pour le déploiement est une tâche primordiale pour la satisfaction des besoins non-fonctionnels(comme la performance temporelle et la consommation d'énergie).
La difficulté de ce choix explique la multitude de tests pour évaluer la qualité des bases de données (BD) développées.
Cette évaluation se base essentiellement sur l'utilisation des métriques associées aux besoins non fonctionnels.
En effet, une mine de tests existe couvrant toutes les phases de cycle de vie de conception d'une BD.
Les tests et leurs environnements sont généralement publiés dans des articles scientifiques ou dans des sites web dédiés comme le TPC (Transaction Processing Council).
Par conséquent, cette thèse contribue à la capitalisation et l'exploitation des tests effectués afin de diminuer la complexité du processus de choix.
En analysant finement les tests, nous remarquons que chaque test porte sur les jeux de données utilisés, la plateforme d'exécution, les besoins non fonctionnels, les requêtes, etc.
Nous proposons une démarche de conceptualisation et de persistance de toutes.ces dimensions ainsi que les résultats de tests.
Cette thèse a donné lieu aux trois contributions.
(1) Une conceptualisation basée sur des modélisations descriptive,prescriptive et ontologique pour expliciter les différentes dimensions.
(2) Le développement d'un entrepôt de tests multidimensionnel permettant de stocker les environnements de tests et leurs résultats.
(3) Le développement d'une méthodologie de prise de décision basée sur un système de recommandation de SGBD et de plateformes.
La désambiguïsation lexicale automatique fait partie des domaines du Traitement Automatique de la Langue (TAL) les plus productifs.
Deux raisons expliquent cette productivité : d'une part l'intérêt considérable de la désambiguïsation lexicale automatique pour un nombre important d'applications, d'autre part l'absence de consensus sur la manière d'appréhender cette tâche.
Nous proposons un modèle d'un genre nouveau fondé sur la théorie de la construction dynamique du sens.
Parallèlement, nous proposons une méthode de calcul automatique de classes sémantiques pour les éléments lexicaux qui sont rattachés au verbe.
La recherche d'information ainsi que l'aide à la décision nécessitent un accès rapide et eﬃcace aux connaissances contenues dans une collection de documents de santé, ainsi qu'une bonne exploitation des connaissances médicales.
L'indexation (description à l'aide de mots clés) permet de rendre ces connaissances accessibles et utilisables.
Dans le domaine de la santé, le nombre de ressources électroniques disponibles augmente de manière exponentielle ainsi la nécessité de disposer de solutions automatiques pour faciliter l'accès aux connaissances ainsi que l'indexation est omniprésente.
L'objectif de cette thèse a été de développer un outil d'aide à l'indexation automatique multi-terminologique, multi-document et multi-tâche nommé F-MTI (French Multi-terminology Indexer) capable de produire une proposition une indexation pour les documents de santé.
Cet outil a nécessité l'élaboration de méthodes de Traitement Automatique de la Langue Naturelle.
Il a été appliqué à l'indexation documentaire dans le catalogue de santé en ligne CISMeF, à l'indexation des données thérapeutiques pour les médicaments et à l'indexation des diagnostics et des actes médicaux pour les dossiers médicaux éléctroniques.
Dans cette thèse, je me concentrerai principalement sur l'inférence variationnelle et les modèles probabilistes.
En particulier, je couvrirai plusieurs projets sur lesquels j'ai travaillé pendant ma thèse sur l'amélioration de l'efficacité des systèmes AI / ML avec des techniques variationnelles.
La thèse comprend deux parties.
Dans la première partie, l'efficacité des modèles probabilistes graphiques est étudiée.
Dans la deuxième partie, plusieurs problèmes d'apprentissage des réseaux de neurones profonds sont examinés, qui sont liés à l'efficacité énergétique ou à l'efficacité des échantillons
Cette recherche s'intéresse à l'usage de deux variables du français traditionnellement décrites comme phonologiques : la liaison et l'élision du schwa.
Ces variables sont étudiées au cours d'interactions naturelles entre trois enfants et leurs parents respectifs.
Plus précisément, l'objectif de cette thèse est de décrire les particularités du discours adressé à l'enfant (DAE) au niveau de l'usage des variables phonologiques et de mesurer leur impact sur l'émergence de la production de ces mêmes variables chez l'enfant.
Après la présentation du cadre théorique d'analyse et de la méthodologie de recueil, de structuration et d'analyse des données, le travail de recherche s'organise en trois parties.
La première étude basée sur corpus, descriptive, a deux principaux objectifs.
Dans un premier temps, il s'agit de mesurer à quelle variation les jeunes enfants sont exposés au domicile familial.
Ensuite, le but est de confronter les résultats des études précédentes sur l'acquisition de la liaison, principalement obtenus à partir de tâches expérimentales, à des données issues de corpus denses d'interactions parent-enfant.
Cette étude a notamment permis de relever l'influence de facteurs liés à l'usage, comme la fréquence, sur l'emploi des variables phonologiques.
La seconde étude se focalise sur les caractéristiques du DAE.
Les résultats présentés démontrent notamment que l'usage des variables phonologiques est modulé en DAE, et ce essentiellement à un stade précoce.
Cette modulation s'atténue ensuite au cours du développement linguistique des jeunes sujets.
La dernière étude de ce travail de recherche permet de mettre en relation les productions enfantines et parentales.
Il apparaît que le développement de la variation phonologique va dans le sens des hypothèses émises par les modèles basés sur l'usage : la variation phonologique est à un stade précoce mémorisée à l'intérieur de constructions spécifiques, particulièrement fréquentes et saillantes dans le DAE.
L'évolution rapide dans les environnements métier d'aujourd'hui impose de nouveaux défis pour la gestion efficace et rentable des processus métiers.
Dans un tel environnement très dynamique, la conception des processus métiers devient une tâche fastidieuse, source d'erreurs et coûteuse.
Par conséquent, l'adoption d'une approche permettant la réutilisation et l'adaptabilité devient un besoin urgent pour une conception de processus prospère.
Les modèles de processus configurables récemment introduits représentent l'une des solutions recherchées permettant une conception de processus par la réutilisation, tout en offrant la flexibilité.
Un modèle de processus configurable est un modèle générique qui intègre de multiples variantes de procédés d'un même processus métier à travers des points de variation.
Ces points de variation sont appelés éléments configurables et permettent de multiples options de conception dans le modèle de processus.
Un modèle de processus configurable doit être configuré selon une exigence spécifique en sélectionnant une option de conception pour chaque élément configurable.
Depuis lors, la question de la conception et de la configuration des modèles de processus configurables a été étudiée.
Cependant, les approches existantes proposent de recommander des modèles de processus configurables entiers qui sont difficiles à réutiliser, nécessitent un temps complexe de calcul et peuvent confondre le concepteur du processus.
D'autre part, les résultats de la recherche sur la conception des modèles de processus configurables ont mis en évidence la nécessité des moyens de soutien pour configurer le processus.
Par conséquent, de nombreuses approches ont proposé de construire un système de support de configuration pour aider les utilisateurs finaux à sélectionner les choix de configuration souhaitables en fonction de leurs exigences.
Notre objectif est double : (i) assister la conception des processus configurables d'une manière à ne pas confondre les concepteurs par des recommandations complexes et (i) assister la création des systèmes de soutien de configuration afin de libérer les analystes de processus de la charge de les construire manuellement.
Pour atteindre le premier objectif, nous proposons d'apprendre de l'expérience acquise grâce à la modélisation des processus passés afin d'aider les concepteurs de processus avec des fragments de processus configurables.
Les fragments proposés inspirent le concepteur du processus pour compléter la conception du processus en cours.
Pour atteindre le deuxième objectif, nous nous rendons compte que les modèles de processus préalablement conçus et configurés contiennent des connaissances implicites et utiles pour la configuration de processus.
Cette thèse s'intéresse aux formalismes qui permettent de représenter mathématiquement non seulement le sens de phrases indépendantes mais aussi de textes entiers, en incluant les liens de sens que les différentes phrases qui les composent entretiennent les unes avec les autres.
Nous ne nous posons pas seulement la question du sens et de sa représentation, mais aussi celle de la détermination algorithmique de cette représentation à partir des séquences de mots qui composent les énoncés.
Nous nous situons donc à l'interface de trois traditions : l'analyse discursive, la sémantique formelle et la linguistique computationnelle.
La plupart de travaux formels portant sur le discours ne prêtent que peu d'attention aux verbes de dire (affirmer, dire, etc.) et d'attitude propositionnelle (penser, croire, etc.).
Tous ces verbes, que nous regroupons sous l'abréviation « VAP » , ont en commun d'exprimer l'attitude ou la position d'une personne sur une proposition donnée.
Ils sont utilisés fréquemment et introduisent de nombreuses subtilités échappant de fait aux théories actuelles.
Cette thèse a pour objectif principal de mettre à jour les principes d'une grammaire formelle compatible avec l'analyse du discours et prenant en compte les VAP.
Nous commençons donc par présenter de nombreuses données linguistiques illustrant les interactions entre VAP et relations discursives.
Il est souvent considéré que les connecteurs adverbiaux (ensuite, par exemple, etc.) sont anaphoriques.
Cependant, nous pouvons nous demander si, en pratique, un système de linguistique computationnelle ne peut pas gérer cette catégorie particulière d'anaphore comme s'il s'agissait d'un type de dépendance structurelle, étendant d'une certaine manière la syntaxe au-delà de la phrase.
C'est ce que nous nous proposons de faire à l'aide du formalisme D-STAG.
Cela nous amène à développer une approche anaphorique, c'est-à-dire dans laquelle les arguments des relations discursives ne sont plus déterminés uniquement par la structure grammaticale des énoncés.
Cela est possible avec la sémantique par continuation, que nous utilisons en combinaison à la sémantique événementielle.
Nous avançons plusieurs pistes pour y répondre et étudions plus en détail le cas de la négation.
Nous montrons que ces difficultés ont pour origine l'analyse standard de la négation, qui traite phrases positives et phrases négatives de manière fondamentalement différente.
Rejetant cette vue, nous présentons une formalisation nouvelle de la notion d'événement négatif, adaptée à l'analyse de divers phénomènes linguistiques.
De nos jours, avec l'abondance croissante de données de très grande taille, les problèmes de classification de grande dimension ont été mis en évidence comme un challenge dans la communauté d'apprentissage automatique et ont beaucoup attiré l'attention des chercheurs dans le domaine.
Au cours des dernières années, les techniques d'apprentissage avec la parcimonie et l'optimisation stochastique se sont prouvées être efficaces pour ce type de problèmes.
Dans cette thèse, nous nous concentrons sur le développement des méthodes d'optimisation pour résoudre certaines classes de problèmes concernant ces deux sujets.
Nos méthodes sont basées sur la programmation DC (Difference of Convex functions) et DCA (DC Algorithm) étant reconnues comme des outils puissants d'optimisation non convexe.
La thèse est composée de trois parties.
La première partie aborde le problème de la sélection des variables.
La deuxième partie étudie le problème de la sélection de groupes de variables.
La dernière partie de la thèse liée à l'apprentissage stochastique.
Dans la première partie, nous commençons par la sélection des variables dans le problème discriminant de Fisher (Chapitre 2) et le problème de scoring optimal (Chapitre 3), qui sont les deux approches différentes pour la classification supervisée dans l'espace de grande dimension, dans lequel le nombre de variables est beaucoup plus grand que le nombre d'observations.
Poursuivant cette étude, nous étudions la structure du problème d'estimation de matrice de covariance parcimonieuse et fournissons les quatre algorithmes appropriés basés sur la programmation DC et DCA (Chapitre 4).
Deux applications en finance et en classification sont étudiées pour illustrer l'efficacité de nos méthodes.
La deuxième partie étudie la L_p,0régularisation pour la sélection de groupes de variables (Chapitre 5).
En utilisant une approximation DC de la L_p,0norme, nous prouvons que le problème approché, avec des paramètres appropriés, est équivalent au problème original.
Considérant deux reformulations équivalentes du problème approché, nous développons différents algorithmes basés sur la programmation DC et DCA pour les résoudre.
Comme applications, nous mettons en pratique nos méthodes pour la sélection de groupes de variables dans les problèmes de scoring optimal et d'estimation de multiples matrices de covariance.
Dans la troisième partie de la thèse, nous introduisons un DCA stochastique pour des problèmes d'estimation des paramètres à grande échelle (Chapitre 6) dans lesquelles la fonction objectif est la somme d'une grande famille des fonctions non convexes.
Comme une étude de cas, nous proposons un schéma DCA stochastique spécial pour le modèle loglinéaire incorporant des variables latentes
C'est une recherche transverse qui traite à la fois des langues contrôlées et de la traduction automatique français-arabe, deux concepts intimement liés.
Dans une situation de crise où la communication doit jouer pleinement son rôle, et dans une mondialisation croissante où plusieurs langues cohabitent, notre recherche montre que l'association de ces deux concepts est plus que nécessaire.
Nul ne peut contester aujourd'hui la place prépondérante qu'occupe la sécurité dans le quotidien des personnes et les enjeux qu'elle représente au sein des sociétés modernes.
Toutefois, et contrairement à une idée bien ancrée qui tend à associer les risques d'une mauvaise communication à l'oral uniquement, l'usage de la langue écrite peut lui aussi comporter des risques.
En effet des messages mal écrits peuvent conduire à de réelles catastrophes et à des conséquences irréversibles notamment dans des domaines jugés sensibles tels que les domaines à sécurité critique.
C'est dans ce contexte que s'inscrit notre recherche.
Elle apporte en effet des notions nouvelles à travers plusieurs procédés normatifs intervenant non seulement dans le processus de contrôle mais également dans le processus de traduction.
Il introduit de nouveaux concepts notamment celui des macrostructures miroir contrôlées, où la syntaxe et la sémantique des langues source et cible sont représentées au même niveau.
Une interaction sociale désigne toute action réciproque entre deux ou plusieurs individus, au cours de laquelle des informations sont partagées sans "médiation technologique".
Dans le contexte de tests et d'études observationnelles, de multiples mécanismes sont utilisés pour étudier ces interactions tels que les questionnaires, l'observation directe des événements et leur analyse par des opérateurs humains, ou l'observation et l'analyse à posteriori des événements enregistrés par des spécialistes (psychologues, sociologues, médecins, etc.).
Pour faire face aux problèmes susmentionnés, il peut donc s'avérer utile d'automatiser le processus d'analyse de l'interaction sociale.
Il s'agit donc de combler le fossé entre les processus d'analyse des interactions sociales basés sur l'homme et ceux basés sur la machine.
Nous proposons donc une approche holistique qui intègre des signaux hétérogènes multimodaux et des informations contextuelles (données "exogènes" complémentaires) de manière dynamique et optionnelle en fonction de leur disponibilité ou non.
Une telle approche permet l'analyse de plusieurs "signaux" en parallèle (où les humains ne peuvent se concentrer que sur un seul).
Cette analyse peut être encore enrichie à partir de données liées au contexte de la scène (lieu, date, type de musique, description de l'événement, etc.) ou liées aux individus (nom, âge, sexe, données extraites de leurs réseaux sociaux, etc.)
Les informations contextuelles enrichissent la modélisation des métadonnées extraites et leur donnent une dimension plus "sémantique".
La gestion de cette hétérogénéité est une étape essentielle pour la mise en œuvre d'une approche holistique.
Dans cette dissertation, nous proposons des méthodes d'apprentissage automa-tique aptes à bénéficier de la récente explosion des volumes de données digitales.
Premièrement nous considérons l'amélioration de l'efficacité des méthodes derécupération d'image.
Nous proposons une approche d'apprentissage de métriques locales coordonnées (Coordinated Local Metric Learning, CLML) qui apprends des métriques locales de Mahalanobis, puis les intègre dans une représentation globale où la distance l2 peut être utilisée.
Ceci permet de visualiser les données avec une unique représentation 2D, et l'utilisation de méthodes de récupération efficaces basées sur la distance l2.
Notre approche peut être interprétée comme l'apprentissage d'une projection linéaire de descripteurs donnés par une méthode a noyaux de grande dimension définie explictement.
Cette interprétation permet d'appliquer des outils existants pour l'apprentissage de métriques de Mahalanobis à l'apprentissage de métriques locales coordonnées.
Nos expériences montrent que la CLML amé-liore les résultats en matière de récupération de visage obtenues par les approches classiques d'apprentissage de métriques locales et globales.
Nous explorerons différentes stratégies d'apprentissage de métriques locales à partir des couches intermédiaires d'un CNN, afin de faire le rapprochement entre des images de sources différentes.
Dans nos expériences, la profondeur de la couche optimale pour une tâche donnée est positivement corrélée avec le changement entre le domaine source (données d'entraînement du CNN) et le domaine cible.
Les résultats montrent que nous pouvons utiliser des CNN entraînés sur des images du spectre visible pour obtenir des résultats meilleurs que l'état de l'art pour la reconnaissance faciale hétérogène (images et dessins quasi-infrarouges).
Troisièmement, nous présentons les "tissus de neurones convolutionnels" (Convolutional Neural Fabrics) permettant l'exploration de l'espace discret et exponentiellement large des architectures possibles de réseaux neuronaux, de manière efficiente et systématique.
Au lieu de chercher à sélectionner une seule architecture optimale, nous proposons d'utiliser un "tissu" d'architectures combinant un nombre exponentiel d'architectures en une seule.
Le tissu est une représentation 3D connectant les sorties de CNNs à différentes couches, échelles et canaux avec un motif de connectivité locale, homogène et creux.
Les seuls hyper-paramètres du tissu (le nombre de canaux et de couches) ne sont pas critiques pour la performance.
La nature acyclique du tissu nous permet d'utiliser la rétro-propagation du gradient durant la phase d'apprentissage.
De manière automatique, nous pouvons donc configurer le tissu de manière à implémenter l'ensemble de toutes les architectures possibles (un nombre exponentiel) et, plus généralement, des ensembles (combinaisons) de ces modèles.
La complexité de calcul et de taille mémoire du tissu évoluent de manière linéaire alors qu'il permet d'exploiter un nombre exponentiel d'architectures en parallèle, en partageant les paramètres entre architectures.
Nous présentons des résultats à l'état de l'art pour la classification d'images sur le jeu de données MNIST et CIFAR10, et pour la segmentation sémantique sur le jeu de données Part Labels.
Ce dernier est d'ailleurs en général traité en deux temps : tout d'abord, réaliser un prétraitement sur les données hétérogènes, multidimensionnelles et imprécises pour les transformer en un flux d'évènements symbolique, puis utiliser des techniques de reconnaissance de plans sur ces mêmes évènements.
Ceci permet de décrire des étapes de plans symboliques de haut niveau sans avoir à se soucier des spécificités des capteurs bas niveau.
Cependant, cette première étape est destructrice d'information et de ce fait génère une ambigüité supplémentaire dans le processus de reconnaissance.
De plus, séparer les tâches de reconnaissance de comportements est générateur de calculs redondants et rend l'écriture de la bibliothèque de plans plus ardue.
Ainsi, nous proposons d'aborder cette problématique sans séparer en deux le processus de reconnaissance.
Pour y parvenir, nous proposons un nouveau modèle hiérarchique, inspiré de la théorie des langages formels, nous permettant de construire un pont au-dessus du fossé sémantique séparant les mesures des capteurs des intentions des entités.
Grâce à l'aide d'un ensemble d'algorithmes manipulant ce modèle, nous sommes capables, à partir d'observations, de déduire les plausibles futures évolutions de la zone sous surveillance, tout en les justifiant des explications nécessaires.
Il est généralement facile pour les humains de distinguer rapidement différents lieux en se basant uniquement sur leur aspect visuel..
Ces catégories sémantiques peuvent être utilisées comme information contextuelle favorisant la détection et la reconnaissance d'objets.
Des travaux récents en reconnaissance des lieux visent à doter les robots de capacités similaires.
Contrairement aux travaux classiques, portant sur la localisation et la cartographie, cette tâche est généralement traitée comme un problème d'apprentissage supervisé.
La reconnaissance de lieux sémantiques-la capacité à reconnaître la catégorie sémantique à laquelle une scène appartient – peut être considérée comme une condition essentielle en robotique autonome.
Un robot autonome doit en effet pouvoir apprendre facilement l'organisation sémantique de son environnement pour pouvoir fonctionner et interagir avec succès.
Si nous faisons l'hypothèse que les objets sont plus faciles à reconnaître quand la scène dans laquelle ils apparaissent est bien identifiée, la deuxième approche semble plus appropriée.
Elle est cependant fortement dépendante de la nature des descripteurs d'images utilisées qui sont généralement dérivés empiriquement a partir des observations générales sur le codage d'images.
En opposition avec ces propositions, une autre approche de codage des images, basée sur un point de vue plus théorique, a émergé ces dernières années.
Les modèles d'extraction de caractéristiques fondés sur le principe de la minimisation d'une fonction d'énergie en relation avec un modèle statistique génératif expliquant au mieux les données, ont abouti à l'apparition des Machines de Boltzmann Restreintes (Rectricted Boltzmann Machines : RBMs) capables de coder une image comme la superposition d'un nombre limité de caractéristiques extraites à partir d'un plus grand alphabet.
Il a été montré que ce processus peut être répété dans une architecture plus profonde, conduisant à une représentation parcimonieuse et efficace des données initiales dans l'espace des caractéristiques.
Le problème complexe de la classification dans l'espace de début est ainsi remplacé par un problème plus simple dans l'espace des caractéristiques.
Après avoir realisé un codage approprié, une régression softmax dans l'espace de projection est suffisante pour obtenir des résultats de classification prometteurs.
A notre connaissance, cette approche n'a pas encore été proposée pour la reconnaissance de scène en robotique autonome.
Nous avons comparé nos méthodes avec les algorithmes de l'état-de-l'art en utilisant une base de données standard de localisation de robot.
Nous avons étudié l'influence des paramètres du système et comparé les différentes conditions sur la même base de données.
Les expériences réalisées montrent que le modèle que nous proposons, tout en étant très simple, conduit à des résultats comparables à l'état-de-l'art sur une tâche de reconnaissance de lieux sémantiques.
Cette thèse porte sur la question de la sémantique causative.
Elle propose une typologie sémantique pour les verbes causatifs analytiques CAUSE, MAKE, HAVE, GET et LET, fondée sur le modèle de la dynamique des forces.
Le troisième chapitre est un tour d'horizon des idées les plus récurrentes dans la littérature au sujet de la sémantique des verbes causatifs analytiques.
Dans le quatrième chapitre, nous proposons une étude de corpus portant sur les propriétés lexico-sémantiques des verbes CAUSE, MAKE, HAVE, GET et LET.
Sur la base des donnés empiriques de notre étude de corpus, nous présentons, dans le dernier chapitre, une nouvelle typologie sémantique pour les verbes causatifs analytiques anglais
L'abondance de textes dans le domaine biomédical nécessite le recours à des méthodes de traitement automatique pour améliorer la recherche d'informations précises.
L'extraction d'information (EI) vise précisément à extraire de l'information pertinente à partir de données non-structurées.
Une grande partie des méthodes dans ce domaine se concentre sur les approches d'apprentissage automatique, en ayant recours à des traitements linguistiques profonds.
L'analyse syntaxique joue notamment un rôle important, en fournissant une analyse précise des relations entre les éléments de la phrase.
Elle comprend l'évaluation de différents analyseurs ainsi qu'une analyse détaillée des erreurs.
Une fois l'analyseur le plus adapté sélectionné, les différentes étapes de traitement linguistique pour atteindre une EI de haute qualité, fondée sur la syntaxe, sont abordés : ces traitements incluent des étapes de pré-traitement (segmentation en mots) et des traitements linguistiques de plus haut niveau (lié à la sémantique et à l'analyse de la coréférence).
Cette thèse explore également la manière dont les différents niveaux de traitement linguistique peuvent être représentés puis exploités par l'algorithme d'apprentissage.
Les méthodes et les approches décrites sont explorées en utilisant deux corpus biomédicaux différents, montrant comment les résultats d'IE sont utilisés dans des tâches concrètes.
Les environnements confinés, tels que les blocs chirurgicaux ou les salles blanches, hébergent des processus complexes auxquels sont associés de nombreux risques.
Leur conception, leur réalisation et leur exploitation sont complexes, de par les très nombreuses normes les encadrant.
La qualification de ces « environnements normés » , afin d'en garantir la qualité de conception, requiert une expertise fine du métier et souffre du manque d'outil en permettant l'automatisation.
Partant de ce constat, nous proposons une approche unifiée visant à faciliter la qualification des environnements normés.
Celle-ci s'appuie sur une représentation du contexte normatif sous la forme d'un graphe unique, ainsi que sur une modélisation de l'environnement et son objet final par étapes successives permettant une vérification incrémentale de même que la production d'
Cette démarche, illustrée au travers du domaine des environnements confinés médicaux, est générique et peut s'appliquer à l'ensemble des environnements normés.
Le présent manuscrit présente de nouvelles techniques d'extraction des structures : du dialogue de groupe, d'une part;
Déceler la structure de longs textes et de conversations est une étape cruciale afin de reconstruire leur signification sous-jacente.
La difficulté de cette tâche est largement reconnue, sachant que le discours est une description de haut niveau du langage, et que le dialogue de groupe inclut de nombreux phénomènes linguistiques complexes.
Historiquement, la représentation du discours a fortement évolué, partant de relations locales, formant des collections non-structurées, vers des arbres, puis des graphes contraints.
Nos travaux utilisent ce dernier paradigme, via la Théorie de Représentation du Discours Segmenté.
Notre recherche se base sur un corpus annoté de discussions en ligne en anglais, issues du jeu de société
Nous discutons de deux investigations liées à notre corpus.
La première étend la définition de la contrainte de la frontière droite, une formalisation de certains principes de cohérence de la structure du discours, pour l'adapter au dialogue de groupe.
La seconde fait la démonstration d'un processus d'extraction de données permettant à un joueur artificiel des Colons d'obtenir un avantage stratégique en déduisant les possessions de ses adversaires à partir de leurs négociations.
Nous proposons de nouvelles méthodes d'analyse du dialogue, utilisant conjointement apprentissage automatisé, algorithmes de graphes et optimisation linéaire afin de produire des structures riches et expressives, avec une précision supérieure comparée aux efforts existants.
Nous décrivons notre méthode d'analyse du discours par contraintes, d'abord sur des arbres en employant la construction d'un arbre couvrant maximal, puis sur des graphes orientés acycliques en utilisant la programmation linéaire par entiers avec une collection de contraintes originales.
Nous appliquons enfin ces méthodes sur les structures de l'argumentation, avec un corpus de textes en anglais et en allemand, parallèlement annotés avec deux structures du discours et une argumentative.
Nous comparons les trois couches d'annotation et expérimentons sur l'analyse de l'argumentation, obtenant de meilleurs résultats, relativement à des travaux similaires.
Dans le cadre des travaux en cours pour informatiser un grand nombre de langues «  peu dotées  » , en particulier celles de l'espace francophone, nous avons créé un système de traduction automatique français-somali dédié à un sous-langage journalistique, permettant d'obtenir des traductions de qualité, à partir d'un corpus bilingue construit par post-édition des résultats de Google Translate (GT), à destination des populations somalophones et non francophones de la Corne de l'Afrique.
Ce dernier constitue'est un corpus aligné, et de très bonne qualité, car nous l'avons construit en post-éditant les pré-traductions de GT, qui combine pour cela avec une combinaison de lason système de TA français-anglais et système de TA anglais-somali.
Il Ce corpus a également fait l'objet d'une évaluation de la part depar 9 annotateurs bilingues qui ont donné une note score de qualité à chaque segment du corpus, et corrigé éventuellement notre post-édition.
D'autre part, nous avons mis en place une iMAG (passerelle interactive d'accès multilingue) qui permet à des internautes somaliens non francophones du continent d'accéder en somali à l'édition en ligne du journal «  La Nation de Djibouti  » .
Nous avons développé une nouvelle approche pour la morphologie traditionnelle arabe destinés aux traitements automatiques de l'arabe écrit.
Cette approche permet de formaliser plus simplement la morphologie sémitique en utilisant Unitex, une suite logicielle fondée sur des ressources lexicales pour l'analyse de corpus.
Pour les verbes (Neme, 2011), j'ai proposé une taxonomie flexionnelle qui accroît la lisibilité du lexique et facilite l'encodage, la correction et la mise-à-jour par les locuteurs et linguistes arabes.
La grammaire traditionnelle définit les classes verbales par des schèmes et des sous-classes par la nature des lettres de la racine.
Dans ma taxonomie, les classes traditionnelles sont réutilisées, et les sous-classes sont redéfinies plus simplement.
La couverture lexicale de cette ressource pour les verbes dans un corpus test est de 99 %.
Pour les noms et les adjectifs (Neme, 2013) et leurs pluriels brisés, nous sommes allés plus loin dans l'adaptation de la morphologie traditionnelle.
Tout d'abord, bien que cette tradition soit basée sur des règles dérivationnelles
Ensuite, nous avons gardé les concepts de racine et de schème, essentiels au modèle sémitique.
Pourtant, notre innovation réside dans l'inversion du modèle traditionnel de racine-et-schème au modèle schème-et-racine, qui maintient concis et ordonné l'ensemble des classes de modèle et de sous-classes de racine.
Ainsi, nous avons élaboré une taxonomie pour le pluriel brisé contenant 160 classes flexionnelles, ce qui simplifie dix fois l'encodage du pluriel brisé.
Depuis, j'ai élaboré des ressources complètes pour l'arabe écrit.
Ces ressources sont décrites dans Neme et Paumier (2019).
Ainsi, nous avons complété ces taxonomies par des classes suffixées pour les pluriels réguliers, adverbes, et d'autres catégories grammaticales afin de couvrir l'ensemble du lexique.
En tout, nous obtenons environ 1000 classes de flexion implémentées au moyen de transducteurs concatenatifs et non-concatenatifs.
A partir de zéro, j'ai créé 76000 lemmes entièrement voyellisés, et chacun est associé à une classe flexionnelle.
Ces lemmes sont fléchis en utilisant ces 1000 FST, produisant un lexique entièrement fléchi de plus 6 millions de formes.
J'ai étendu cette ressource entièrement fléchie à l'aide de grammaires d'agglutination pour identifier les mots composés jusqu'à 5 segments, agglutinés autour d'un verbe, d'un nom, d'un adjectif ou d'une particule.
Les grammaires d'agglutination étendent la reconnaissance à plus de 500 millions de formes de mots valides, partiellement ou entièrement voyelles.
La taille de fichier texte généré est de 340 mégaoctets (UTF-16).
Il est compressé en 11 mégaoctets avant d'être chargé en mémoire pour la recherche rapide (fast lookup).
La génération, la compression et la minimisation du lexique prennent moins d'une minute sur un MacBook.
Le taux de couverture lexical d'un corpus est supérieur à 99 %.
La vitesse de tagger est de plus de 200 000 mots/s, si les ressources ont été pré-chargées en mémoire RAM.
La précision et la rapidité de nos outils résultent de notre approche linguistique systématique et de l'adoption des meilleurs choix pratiques en matière de méthodes mathématiques et informatiques.
La procédure de recherche est rapide parce que nous utilisons l'algorithme de minimisation d'automate déterministique acyclique (Revuz, 1992) pour comprimer le dictionnaire complet, et parce qu'il n'a que des chaînes constantes.
Le problème de représentation automatique de la signification logique des énoncés ambigus en langage naturel a suscité l'intérêt des chercheurs dans le domaine de la sémantique computationnelle et de la logique.
L'ambiguïté dans le langage naturel peut se manifester au niveau lexical / syntaxique / sémantique de la construction de sens, ou elle peut être causée par d'autres facteurs tels que la grammaticalité et le manque de contexte dans lequel la phrase est effectivement prononcée.
L'approche traditionnelle Montagovienne ainsi que ses extensions modernes ont tenté de capturer ce phénomène en fournissant quelques modèles qui permettent la génération automatique de formules logiques.
Cependant, il existe un axe de recherche qui n'est pas encore profondément étudié : classer les interprétations d'énoncés ambigus en fonction des préférences réelles des utilisateurs de la langue.
Ce manque suggère une nouvelle direction d'étude qui est partiellement explorée dans ce mémoire en modélisant des préférences de sens en alignement avec certaines des théories de performance préférentielles humaines bien étudiées disponibles dans la littérature linguistique et psycholinguistique.
Afin d'atteindre cet objectif, nous suggérons d'utiliser / d'étendre les Grammaires catégorielles pour notre analyse syntaxique et les Réseaux catégoriels de preuve comme notre analyse syntaxique.
Nous utilisons également le Lexique Génératif Montagovien pour dériver une formule logique multi-triée comme notre représentation de signification sémantique.
Nous utilisons un cadre appelé Montagovian Generative
Durant les situations de crise, telles que les catastrophes, le besoin de recherche d'informations (RI) pertinentes partagées dans les microblogs en temps réel est inévitable.
Cependant, le grand volume et la variété des flux d'informations partagées en temps réel dans de telles situations compliquent cette tâche.
Contrairement aux approches existantes de RI basées sur l'analyse du contenu, nous proposons de nous attaquer à ce problème en nous basant sur les approches centrées utilisateurs tout en levant un certain nombre de verrous méthodologiques et technologiques inhérents : 1) à la collection des données partagées par les utilisateurs à évaluer, 2) à la modélisation de leurs comportements, 3) à l'analyse des comportements, et 4) à la prédiction et le suivi des utilisateurs primordiaux en temps réel.
Dans ce contexte, nous détaillons les approches proposées dans cette thèse afin de prédire les utilisateurs primordiaux qui sont susceptibles de partager les informations pertinentes et exclusives ciblées et de permettre aux intervenants d'urgence d'accéder aux informations requises
Nous avons tout d'abord étudié l'efficacité de différentes catégories de mesures issues de la littérature et proposées dans cette thèse pour représenter le comportement des utilisateurs.
En se basant sur cette approche de modélisation, nous entraînons différents modèles de prédiction qui apprennent à différencier les comportements des utilisateurs primordiaux de ceux qui ne le sont pas durant les situations de crise.
La pertinence et l'efficacité des modèles de prédiction appris ont été validées à l'aide des données collectées par notre système multi-agents MASIR durant deux inondations qui ont eu lieu en France et des vérités terrain appropriées à ces collections.
Notre étude est centrée sur les mécanismes elliptiques au sein des anaphores associatives méronymiques.
Nous sommes partie de l'hypothèse que dans ce type d'anaphore, il existe deux structures : une structure profonde et une structure de surface.
La première consiste en la présence des trois éléments : le tout, le prédicat partitif et la partie.
La deuxième, où apparaissent les différents types d'ellipse, fait l'objet de notre travail.
Nous nous sommes attardée sur trois types d'ellipse que nous considérons caractéristiques des anaphores méronymiques : l'ellipse du prédicat partitif, celle du deuxième élément de la structure N DeN et celle de l'antécédent anaphorique.
Traitées séparément, les anaphores nominales, les anaphores verbales et les anaphores adverbiales ont été soumises dans un premier temps à une description syntactico-sémantique et dans un deuxième temps à la théorie des trois fonctions primaires.
Cette théorie nous a permis d'expliquer la possibilité pour certains éléments, d'être élidés au sein de l'anaphore associative méronymique.
Les dispositifs mobiles tels que les smartphones, les montres connectées ou les lunettes électroniques ont révolutionné la façon dont nous interagissons.
Les lunettes électroniques nous intéressent, car elles fournissent aux utilisateurs une vision simultanée des mondes physique et numérique.
Cependant, l'interaction sur les lunettes électroniques n'est pas bien explorée.
L'amélioration de l'interaction sur ce dispositif peut convaincre les utilisateurs à l'utiliser plus dans la vie quotidienne.
Le sujet de thèse est focalisé sur l'étude et le développement de nouvelles techniques d'interaction avec les lunettes électroniques.
En effet, il n'est pas possible d'interagir aussi finement en mobilité ou en situation d'urgence par rapport à une situation stable telle qu'assis devant un bureau.
Notre contexte de travail se situe dans le domaine de la santé et en particulier celui du personnel médical visitant un patient dans un hôpital.
Le personnel médical doit pouvoir accéder aux données du patient déjà collectées, obtenir des données physiologiques en temps réel, préparer son diagnostic et communiquer avec ses collègues.
Le verrou scientifique pour la thèse ici est de trouver des solutions qui permettent au personnel médical de réaliser ces tâches de façon plus précise et moins contraignante.
Le but est de rendre plus efficace le diagnostic et le partage d'informations et de faire d'un dispositif encore non maîtrisé un outil fonctionnel en milieu professionnel de la santé.
Dans cette optique, les travaux de cette thèse présentent des contributions théoriques et applicatives.
Nous avons tout d'abord répertorié les différents travaux effectués dans le cadre des lunettes électroniques pour le domaine de la santé, tout en indiquant le potentiel, les résultats pertinents et les limites.
Nous nous sommes focalisés sur les lunettes électroniques pour afficher et manipuler les dossiers patients.
D'un point de vue conceptuel, nous avons proposé un espace de conception à huit dimensions pour identifier les lacunes dans les systèmes existants et aider à la conception de nouveaux systèmes.
D'un point de vue applicatif, en interaction en sortie, nous avons introduit une technique appelée K-Fisheye sur une interface à tuiles qui permet de parcourir un grand ensemble de données comme celui présent dans le dossier patient.
Nous avons utilisé ensuite l'espace de conception pour porter un système existant sur les lunettes électroniques.
Le prototype obtenu s'appelle mCAREglass.
Ensuite, nous avons conçu une nouvelle technique d'entrée de texte appelé TEXTile qui permet de saisir du texte sur un tissu interactif communiquant avec les lunettes.
De nombreuses applications génèrent et reçoivent des données sous la forme de flux continu, illimité, et très rapide.
Cela pose naturellement des problèmes de stockage, de traitement et d'analyse de données qui commencent juste à être abordés dans le domaine des flux de données.
L'apprentissage de ce modèle est, contrairement à sa version de départ, guidé non seulement par la nouveauté qu'apporte une donnée d'entrée mais également par la donnée elle-même.
De ce fait, le modèle ILoNDF peut acquérir constamment de nouvelles connaissances relatives aux fréquences d'occurrence des données et de leurs variables, ce qui le rend moins sensible au bruit.
Dans un premier temps, notre travail se focalise sur l'étude du comportement du modèle ILoNDF dans le cadre général de la classification à partir d'une seule classe en partant de l'exploitation des données fortement multidimensionnelles et bruitées.
Ce type d'étude nous a permis de mettre en évidence les capacités d'apprentissage pures du modèle ILoNDF vis-à-vis de l'ensemble des méthodes proposées jusqu'à présent.
Dans un deuxième temps, nous nous intéressons plus particulièrement à l'adaptation fine du modèle au cadre précis du filtrage d'informations.
Notre objectif est de mettre en place une stratégie de filtrage orientée-utilisateur plutôt qu'orientée-système, et ceci notamment en suivant deux types de directions.
La première direction concerne la modélisation utilisateur à l'aide du modèle ILoNDF.
Cette modélisation fournit une nouvelle manière de regarder le profil utilisateur en termes de critères de spécificité, d'exhaustivité et de contradiction.
La seconde direction, complémentaire de la première, concerne le raffinement des fonctionnalités du modèle ILoNDF en le dotant d'une capacité à s'adapter à la dérive du besoin de l'utilisateur au cours du temps.
Enfin, nous nous attachons à la généralisation de notre travail antérieur au cas où les données arrivant en flux peuvent être réparties en classes multiples.
Un des enjeux majeurs du marché des synthétiseurs et de la recherche en synthèse sonore aujourd'hui est de proposer une nouvelle forme de synthèse permettant de générer des sons inédits tout en offrant aux utilisateurs de nouveaux contrôles plus intuitifs afin de les aider dans leur recherche de sons.
En effet, les synthétiseurs sont actuellement des outils très puissants qui offrent aux musiciens une large palette de possibilités pour la création de textures sonores, mais également souvent très complexes avec des paramètres de contrôle dont la manipulation nécessite généralement des connaissances expertes.
Cette thèse s'intéresse ainsi au développement et à l'évaluation de nouvelles méthodes d'apprentissage machine pour la synthèse sonore permettant la génération de nouveaux sons de qualité tout en fournissant des paramètres de contrôle pertinents perceptivement.
Le premier challenge que nous avons relevé a donc été de caractériser perceptivement le timbre musical synthétique en mettant en évidence un jeu de descripteurs verbaux utilisés fréquemment et de manière consensuelle par les musiciens.
Deux études perceptives ont été menées : un test de verbalisation libre qui nous a permis de sélectionner huit termes communément utilisés pour décrire des sons de synthétiseurs, et une analyse à échelles sémantiques permettant d'évaluer quantitativement l'utilisation de ces termes pour caractériser un sous-ensemble de sons, ainsi que d'analyser leur "degré de consensualité".
Dans un second temps, nous avons exploré l'utilisation d'algorithmes d'apprentissage machine pour l'extraction d'un espace de représentation haut-niveau avec des propriétés intéressantes d'interpolation et d'extrapolation à partir d'une base de données de sons, le but étant de mettre en relation cet espace avec les dimensions perceptives mises en évidence plus tôt.
S'inspirant de précédentes études sur la synthèse sonore par apprentissage profond, nous nous sommes concentrés sur des modèles du type autoencodeur et avons réalisé une étude comparative approfondie de plusieurs types d'autoencodeurs sur deux jeux de données différents.
Ces expériences, couplées avec une étude qualitative via un prototype non temps-réel développé durant la thèse, nous ont permis de valider les autoencodeurs, et en particulier l'autoencodeur variationnel (VAE), comme des outils bien adaptés à l'extraction d'un espace latent de haut-niveau dans lequel il est possible de se déplacer de manière continue et fluide en créant de tous nouveaux sons.
Cependant, à ce niveau, aucun lien entre cet espace latent et les dimensions perceptives mises en évidence précédemment n'a pu être établi spontanément.
Pour finir, nous avons donc apporté de la supervision au VAE en ajoutant une régularisation perceptive durant la phase d'apprentissage.
En utilisant les échantillons sonores résultant du test perceptif avec échelles sémantiques labellisés suivant les huit dimensions perceptives, il a été possible de contraindre, dans une certaine mesure, certaines dimensions de l'espace latent extrait par le VAE afin qu'elles coïncident avec ces dimensions.
Un test comparatif a été finalement réalisé afin d'évaluer l'efficacité de cette régularisation supplémentaire pour conditionner le modèle et permettre un contrôle perceptif (au moins partiel) de la synthèse sonore.
La non-adhérence médicamenteuse désigne les situations où le patient ne suit pas les directives des autorités médicales concernant la prise d'un médicament.
Il peut s'agir d'une situation où le patient prend trop (sur-usage) ou pas assez (sous-usage) de médicaments, boit de l'alcool alors qu'il y a une contrindication, ou encore commet une tentative de suicide à l'aide de médicaments.
Selon [HAYNES 2002] améliorer l'adhérence pourrait avoir un plus grand impact sur la santé de la population que tout autre amélioration d'un traitement médical spécifique.
Cependant les données sur la non-adhérence sont difficiles à acquérir, puisque les patients en situation de non-adhérence sont peu susceptibles de rapporter leurs actions à leurs médecins.
Dans un premier temps, nous collectons un corpus de messages postés sur des forums médicaux.
Nous construisons des vocabulaires de noms de médicaments et de maladies utilisés par les patients.
Nous utilisons ces vocabulaires pour indexer les médicaments et maladies dans les messages.
Ensuite nous utilisons des méthodes d'apprentissage supervisé et de recherche d'information pour détecter les messages de forum parlant d'une situation de non-adhérence.
Nous identifions 3 motivations : gérer soi-même sa santé, rechercher un effet différent de celui pour lequel le médicament est prescrit, être en situation d'addiction ou d'accoutumance.
La gestion de sa santé recouvre ainsi plusieurs situations : éviter un effet secondaire, moduler l'effet du médicament, sous-utiliser un médicament perçu comme inutile, agir sans avis médical.
Additionnellement, une non-adhérence peut survenir par erreur ou négligence, sans motivation particulière.
À l'issue de notre étude nous produisons : un corpus annoté avec des messages de non-adhérence, un classifieur capable de détecter les messages de non-adhérence, une typologie des situations de non-adhérence et une analyse des causes de la non-adhérence.
Dans cette thèse, nous étudions la segmentation d'un flux audio en parole, musique et parole sur musique (P/M).
Cette étape est fondamentale pour toute application basée sur la transcription automatique de flux radiophoniques et plus généralement multimédias.
L'application visée ici est un système de détection de mots clés dans les émissions radiophoniques.
Les performances de ce système dépendront de la bonne segmentation du signal fournie par le système de discrimination parole/musique.
En effet, une mauvaise classification du signal peut provoquer des omissions de mots clés ou des fausses alarmes.
Afin d'améliorer la discrimination parole/musique, nous proposons une nouvelle méthode de paramétrisation du signal.
Nous utilisons la décomposition en ondelettes qui permet une analyse des signaux non stationnaires dont la musique est un exemple.
Nous calculons différentes énergies sur les coefficients d'ondelettes pour construire nos vecteurs de paramètres.
Cette architecture a été choisie car elle permet de trouver les meilleurs paramètres indépendamment pour chaque tâche P/NP et M/NM.
Une fusion des sorties des classifieurs est alors effectuée pour obtenir la décision finale : parole, musique ou parole sur musique.
Les résultats obtenus sur un corpus réel d'émissions de radio montrent que notre paramétrisation en ondelettes apporte une nette amélioration des performances en discrimination M/NM et P/M par rapport à la paramétrisation de référence fondée sur les coefficients cepstraux.
Cette thèse a pour objectif de contribuer à l'étude de la traduction en Égypte entre 1798 et 1873 et son rôle sur les réformes entreprises par les intellectuels sous l'égide de la politique du Wālī réformiste (Mohamed Ali Pacha), mais aussi à l'étude de l'école al-Alsun et son apport à la traduction, l'expansion lexicale et à l'acquisition de la Nahḍa.
Notre sujet revêt donc une importance particulière parce qu'il traite non seulement le début des relations linguistiques et culturelles franco-égyptiennes qui favorisa l'apparition de la traduction (au sens moderne) et de l'arabe juridique moderne grâce à l'école al-Alsun au Caire en 1834.
Ce n'est pas un hasard si nous avons accordé une attention particulière à cette région du Mašriq al-'arabī, car elle constitue un foyer où la majorité des activités de traduction eurent lieu par rapport aux autres régions du monde arabe.
D'ailleurs, pendant la collection du corpus de notre projet de recherche, nous n'avons pas trouvé un ouvrage consacré à cette école qui, pourtant, joua un rôle primordial sur l'évolution de la société arabe en général et la société égyptienne en particulier.
Toutefois, lorsqu'il s'agit de la « traduction » dans le monde arabe, la dimension historique révèle les grandes aptitudes de cette discipline telles que : la traduction du persan à l'arabe ou du grec à l'arabe à l'époque omeyyade et abbasside, mais aussi la traduction au sens contemporain qui fait l'objet de notre recherche.
L'objectif de cette thèse est d'étudier les sentiments dans les documents comparables.
Premièrement, nous avons recueillis des corpus comparables en anglais, français et arabe de Wikipédia et d'Euronews, et nous avons aligné ces corpus au niveau document.
Nous avons en plus collecté des documents d'informations des agences de presse locales et étrangères dans les langues anglaise et arabe.
Deuxièmement, nous avons présenté une mesure de similarité cross-linguistique des documents dans le but de récupérer et aligner automatiquement les documents comparables.
Ensuite, nous avons proposé une méthode d'annotation cross-linguistique en termes de sentiments, afin d'étiqueter les documents source et cible avec des sentiments.
Enfin, nous avons utilisé des mesures statistiques pour comparer l'accord des sentiments entre les documents comparables source et cible.
Les méthodes présentées dans cette thèse ne dépendent pas d'une paire de langue bien déterminée, elles peuvent être appliquées sur toute autre couple de langue
Ce travail porte sur la synthèse de la parole audio-visuelle.
Dans la littérature disponible dans ce domaine, la plupart des approches traite le problème en le divisant en deux problèmes de synthèse.
Le premier est la synthèse de la parole acoustique et l'autre étant la génération d'animation faciale correspondante.
Mais, cela ne garantit pas une parfaite synchronisation et cohérence de la parole audio-visuelle.
Pour pallier implicitement l'inconvénient ci-dessus, nous avons proposé une approche de synthèse de la parole acoustique-visuelle par la sélection naturelle des unités synchrones bimodales.
La synthèse est basée sur le modèle de sélection d'unité classique.
L'idée principale derrière cette technique de synthèse est de garder l'association naturelle entre la modalité acoustique et visuelle intacte.
Nous décrivons la technique d'acquisition de corpus audio-visuelle et la préparation de la base de données pour notre système.
Nous présentons une vue d'ensemble de notre système et nous détaillons les différents aspects de la sélection d'unités bimodales qui ont besoin d'être optimisées pour une bonne synthèse.
L'objectif principal de ce travail est de synthétiser la dynamique de la parole plutôt qu'une tête parlante complète.
Nous décrivons les caractéristiques visuelles cibles que nous avons conçues.
Nous avons ensuite présenté un algorithme de pondération de la fonction cible.
Cet algorithme que nous avons développé effectue une pondération de la fonction cible et l'élimination de fonctionnalités redondantes de manière itérative.
Elle est basée sur la comparaison des classements de coûts cible et en se basant sur une distance calculée à partir des signaux de parole acoustiques et visuels dans le corpus.
Enfin, nous présentons l'évaluation perceptive et subjective du système de synthèse final.
Les résultats montrent que nous avons atteint l'objectif de synthétiser la dynamique de la parole raisonnablement bien
Les systèmes de Traitement Automatique des Langues Naturelles (TALN) sont de manière récurrente confrontés au problème de la génération et de la propagation d'hypothèses concurrentes et erronées.
Afin d'écarter ces erreurs d'interprétation du processus d'analyse, il apparaît indispensable d'avoir recours à des stratégies spécifiques de contrôle dont l'objectif est de différencier les hypothèses concurrentes selon leur degré de pertinence.
Sur la plupart des cas d'indétermination observés, on constate que cette évaluation de la pertinence relative des hypothèses repose sur l'exploitation de plusieurs sources de connaissances hétérogènes, qui doivent être combinées pour garantir un contrôle robuste et fiable.
À partir de ce constat, nous avons montré que le traitement des indéterminations répondait à une formalisation générique en tant que problème décisionnel basé sur de multiples critères de comparaison.
Par rapport aux méthodes alternatives, cette approche se différentie notamment par l'importance qu'elle accorde aux connaissances et préférences qu'un expert est en mesure d'apporter sur le problème traité.
À partir de cette intersection novatrice entre le TALN et l'AMCD, nos travaux se sont focalisés sur le développement d'un module décisionnel de contrôle multicritère.
L'intégration de ce module au sein d'un système complet de TALN nous a permis d'attester d'une part la faisabilité de notre approche et d'autre part de l'expérimenter sur différents cas concrets d'indétermination.
Cette thèse est le fruit de l'interaction de deux disciplines qui sont la détection de changements dans des images multitemporelles et le raisonnement évidentiel à l'aide de la théorie de Dempster-Shafer (DST).
Aborder le problème de détection et d'analyse de changements par la DST nécessite la détermination d'un cadre de discernement exhaustif et exclusif.
Ce problème s'avère complexe en l'absence des informations a priori sur les images.
L'idée de cet algorithme est la représentation de chaque classe par un nombre varié de centroïdes afin de garantir une meilleure caractérisation de classes.
Afin d'assurer l'exhaustivité du cadre de discernement, un nouvel indice de validité de clustering permettant de déterminer le nombre optimal de classes sémantiques est proposé.
La troisième contribution consiste à exploiter la position du pixel par rapport aux centroïdes des classes et les degrés d'appartenance afin de définir la distribution de masse qui représente les informations.
Nous avons souligné la capacité du conflit évidentiel à indiquer les transformations multi-temporelles.
Nous avons porté notre raisonnement sur la décomposition du conflit global et l'estimation des conflits partiels entre les couples des éléments focaux pour mesurer le conflit causé par le changement.
Cette stratégie permet d'identifier le couple de classes qui participent dans le changement.
Pour quantifier ce conflit, nous avons proposé une nouvelle mesure de changement notée CM.
Finalement, nous avons proposé un algorithme permettant de déduire la carte binaire de changements à partir de la carte de conflits partiels.
Dans le cadre des sciences forensiques, le déguisement de la voix révèle un attrait particulier.
La plupart des criminels essaie de déguiser leur voix avant de réaliser un appel anonyme ou une revendication terroriste.
L'objectif du malfaiteur est de modifier le registre de sa voix afin de changer la perception de son identité ou d'imiter la voix d'un locuteur cible.
Cette thèse envisage deux types d'imposture : la transformation de la voix à partir de techniques délibérées et non électroniques, puis de transformations délibérées et électroniques.
L'analyse de la transformation est tout d'abord réalisée à partir de paramètres acoustiques en vue de mesurer les variations par rapport à une voix non déguisée.
Quatre déguisements parmi les plus commun dans le contexte criminel ont été sélectionnés.
La contrainte d'audibilité et d'intelligibilité a été imposée aux locuteurs dans la construction de la base de données.
L'analyse acoustique a permis de mettre en évidence des variations spécifiques aux déguisements.
L'application de méthodes de classification automatique permet de détecter l'imposture avec un niveau de performance satisfaisant.
Ensuite, des techniques de conversion automatique de la voix ont été mises en oeuvre afin d'imiter automatiquement un locuteur cible d'une part, et d'autre part d'envisager la réversibilité du déguisement.
L'imposture réalisée comme l'application de la conversion à la réversibilité du déguisement offrent des résultats très satisfaisants.
La médecine fondée sur les preuves a permis de formaliser des guides de pratique clinique qui définissent des flux de travail et des recommandations à suivre pour un domaine clinique concis.
Ces guides se sont construits dans le but de standardiser les soins de santé et d'obtenir les meilleurs résultats possibles pour les patients.
Néanmoins, les médecins n'adhèrent pas toujours à ces directives en raison de diverses limitations cliniques et de mise-en-œuvre.
D'une part, les médecins n'ont pas toujours familiarisés ou en accord avec les lignes directrices des guides de pratique clinique, doutant ainsi de leur efficacité et des résultats attendus par rapport aux pratiques antérieures.
D'autre part, maintenir ces guides à jour en incluant les dernières preuves établies requiert une gestion continue d'une documentation établie sur support papier.
Les systèmes d'aide à la décision clinique sont ainsi proposés comme aide durant le processus de prise de décision clinique, par la mise en œuvre informatisée des guides pour promouvoir leur consultation et l'adhésion des médecins.
Bien que ces systèmes aident à améliorer la conformité des guides, il subsiste certains obstacles hérités des guides sur support papier qui ne sont pas résolus avec leur mise en œuvre informatisée, comme le traitement des cas complexes non-définis dans les directives ou le manque de représentation d'autres facteurs externes qui peuvent influer sur les traitements fournis et faire dévier des recommandations des guides (c.-à-d. les préférences du patient).
La présente thèse propose un système avancé d'aide à la décision clinique pour faire face aux limitations du soutien purement basé en guides et aller au-delà des connaissances formalisées en analysant les données cliniques, les résultats et les performances de toutes les décisions prises au fil du temps.
Pour atteindre ces objectifs, une approche de modélisation des connaissances et performances cliniques de manière sémantique validée et informatisée a été présentée, en s'appuyant sur une ontologie et avec la formalisation du concept d'Événement Décisionnel.
De plus, un cadre indépendant du domaine a été mis en place pour faciliter le processus d'informatisation, de mise à jour et de mise en œuvre des guides de pratique clinique au sein d'un système d'aide à la décision clinique afin de fournir un soutien clinique à pour chaque patient interrogé.
Pour répondre aux limites des guides, une méthodologie permettant d'augmenter les connaissances cliniques en utilisant l'expérience a été présentée ainsi qu'une évaluation de la performance clinique et de la qualité au fil du temps, en fonction des différents résultats cliniques étudiés, tels que l'utilisabilité et la fiabilité clinique derrière les connaissances cliniques formalisées.
Enfin, les données du monde réel accumulées ont été explorées pour soutenir les cas futurs, promouvoir l'étude de nouvelles hypothèses cliniques et aider à la détection des tendances et des modèles sur les données à l'aide d'outils d'analyse visuelle.
Les modules présentés ont été développés et mis en œuvre dans leur majorité dans le cadre du projet européen Horizon 2020 DESIREE, dans lequel le cas d'utilisation était axé sur le soutien des unités de soins du sein au cours du processus décisionnel pour la prise en charge des patientes atteintes d'un cancer du sein primaire, en effectuant une validation technique et clinique sur l'architecture présentée, dont les résultats sont présentés dans cette thèse.
Néanmoins, certains des modules ont également été utilisés dans d'autres domaines médicaux tels que le développement des guides de pratique clinique pour le diabète gestationnel, mettant en évidence l'interopérabilité et la flexibilité du travail présenté.
Avec la croissance explosive de la numérisation du patrimoine culturel, de nombreuses patrimoine culturel institutions ont été la conversion des objets physiques du patrimoine culturel dans la représentation numérique ou représentation descriptive.
Toutefois, la conversion a donné lieu à plusieurs questions telles que : 1) les documents sont de nature descriptive, 2) l'ambiguïté et de la brièveté des documents, 3) le vocabulaire spécifique est utilisé dans les documents, et 4), il existe également des variations dans les termes utilisés dans le document.
En outre, l'utilisation de mots-clés inexactes également entraîné problème de requête court.
La plupart du temps, les problèmes sont causés par la faute agrégée en annotant les documents alors que le problème de requête court est causé par l'utilisateur naïf qui a peu de connaissances et d'expérience dans le domaine du patrimoine culturel.
Dans cette recherche, l'objectif principal est de modéliser le système d'accès à l'information pour surmonter partiellement les questions soulevées par le processus de documentation et le fond des utilisateurs du patrimoine culturel numérique.
Par conséquent, trois types d'outils d'accès aux informations sont introduites et établies à savoir l'information système de recherche, la recherche de contexte, et jeu mobile sur le patrimoine culturel qui permettent à l'utilisateur d'accéder, d'apprendre et d'explorer les informations sur le patrimoine culturel.
Fondamentalement, l'idée principale d'information système de recherche et contexte de recherche est d'intégrer la relation de lien entre les termes dans le modèle de la langue par l'extension de Dirichlet lissage pour résoudre les problèmes qui se posent à la fois le processus de documentation et de fond des utilisateurs.
En outre, un modèle de préférence est présenté sur la base de la théorie de la charge d'un condensateur de quantifier le contexte cognitif basé sur le temps et les intégrer dans la longue Dirichlet lissage.
En outre, un jeu mobile est introduite par l'intégration des éléments des jeux de monopole et chasse au trésor pour atténuer les problèmes découlant de l'arrière-plan des utilisateurs en particulier leur comportement décontracté.
Les premier et deuxième approches ont été testées sur le patrimoine culturel dans CLEF (chic) ​​collection qui se compose de questions et de courts documents.
Les résultats montrent que l'approche est efficace et donne une meilleure précision lors de la récupération.
Enfin, une enquête a été menée pour étudier la troisième approche, et le résultat donne à penser que le jeu est en mesure d'aider les participants à explorer et apprendre les informations sur le patrimoine culturel.
En outre, les participants ont également estimé qu'une recherche d'information outil qui est intégré avec le jeu peut fournir plus d'informations à l'utilisateur d'une manière plus pratique tout en jouant le jeu et en visitant les sites du patrimoine dans le match.
En conclusion, les résultats montrent que les solutions proposées sont en mesure de résoudre les problèmes posés par le processus de documentation et le fond des utilisateurs du patrimoine culturel numérique.
Cette recherche présente une méthode d'analyse micro-systémique des mots composés thaïs.
Les points de vue des autres travaux sont étudiés.
Le deuxième chapitre présente les caractéristiques de la langue thaïe qui possède une forme d'écriture typique sans espacement et peut entrainer des difficultés en termes d'ambiguïté dans la traduction.
Certaines divergences entre le thaï et le français sont soulignées à l'aide de la théorie micro-systémique du Centre Tesnière.
Le quatrième chapitre met en évidence un contrôle modélisé des unités lexicales codées syntaxiquement et sémantiquement afin d'en définir des algorithmes efficaces.
Le dernier chapitre conclut sur les résultats des nouveaux algorithmes par leur informatisation.
Sont enfin énoncées les perspectives ouvertes par cette nouvelle recherche.
Cette étude est présentée comme un travail fiable à l'élimination des ambiguïtés.
Fondée sur une méthode hybride, elle nous a permis d'atteindre notre objectif et de trouver ainsi une voie efficace qui nous autorise à traduire automatiquement les mots thaïs vers le français.
Le résultat place cet outil comme l'un des plus accessibles à la recherche internationale où le thaï et le français prennent leurs places de choix
La détection de langage abusif en ligne est un problème de classification comportant des défis cruciaux liés à la compréhension automatique du langage naturel et à la variété des contextes complexes et riches par lesquels le langage naturel se manifeste.
Les réseaux de neurones et les techniques standard d'apprentissage profond se sont montrés efficaces dans la detection de contenu explicitement offensif dans les conversations mais il est plus difficile de détecter automatiquement des formes plus subtiles, mais aussi plus fréquentes, de *toxicité passive* comme le sarcasme, les comportements passifs-aggressifs et les discours de haine formulés poliment, mais incendiaires.
Les modèles d'apprentissage automatique traditionnels et plus récemment les réseaux de neurones convolutifs (CNN) basés sur les mots et sur les caractères et les réseaux de neurones récurrents comme les modèles Long Short-Term Memory (LSTM) et les Gated Recurrent Units (GRUs), couplés à du plongement de mots ont été utilisés pour assister la modération de commentaires et pour detecter les discours de haine.
De plus, des travaux ont cherché à interpréter ces réseaux de neurones detectant l'agressivité verbale.
Dans le cadre de l'initiative de recherche Conversation AI de Google Jigsaw, notre recherche portera sur la façons dont des méthodes novatrices d'apprentissage profond peuvent saisir la toxicité passive.
Conversation AI a de l'expérience dans le développement des modèles théoriques, et des outils pour les utilisateurs et sites web.
La thèse s'articulera autour de deux axes complémentaires : les données annotées et les modèles.
Pour la classification supervisée, nous avons besoin de grandes quantités de commentaires annotés issus de vraies conversations en ligne.
Nous prévoyons de partir des ensembles de données publics fournis par l'équipe Jigsaw / Conversation-AI.
Ils sont soit annotés manuellement par production participative soit annotés à partir de features comme les tags ou étiquettes.
Ces jeux de données ont montré un nombre significatif de commentaires contenant de la toxicité passive mais caractérisés par une probabilité de toxicité intermédiaire.
Nous suivrons l'approche utilisée par l'interface de programmation applicative Perspective, qui consiste à sélectionner un petit nombre de catégories vraissemblables pour annoter les différents types de toxicité passive.
Nous exploiterons et améliorerons les modèles Transformer et BERT, qui s'appuient sur des mécanismes d'attention et nous observerons leur comparaison à d'autres modèles d'apprentissage profond.
Puisque la toxicité passive se base sur l'ambiguité du langage naturel, nous nous concentrerons sur la modélisation de l'impact émotionnel des messages qui, comme Conversation AI l'a découvert, aboutit à de plus forts niveaux d'accords inter-annotateurs (par rapport aux distinctions sémantiques plus complexes que les document de violation de politique tentent d'établir).
Les méthodes d'apprentissage par renforcement (RL) peuvent également se montrer intéressantes dans ce contexte.
l'autre aspect de cette recherche est l'analyse et la compréhension des origines et des mécanismes du comportement passif-aggressif en ligne.
Ceci peut être fait en mesurant et en visualisant son impact sur les conversations.
Du code source ouvert et des publications dans des conférences et journaux sélectifs seront produits au cours de ce doctorat.
Les modèles seront implémentés en Python et avec le cadre de développement TensorFlow.
Les métriques utilisées pour comparer les architectures incluent la justesse, la précision, le rappel, la spécificité, le taux de faux positifs, le score F1, la fonction d'efficacité du récepteur et son aire sous la courbe (ROC-AUC).
Au cours de cette recherche, nous prévoyons d'apporter des éléments de réponse aux différentes questions concernant les conversations sur les réseaux sociaux comme par exemple : comment développer des méthodes d'apprentissage profond pour identifier la toxicité passive dans les conversations en ligne ?
L'apprentissage profond peut-il comprendre la pertinence des contributions dans un débat ?
Qu'est-ce qui déclenche la toxicité passive dans la conversation ?
Que se passe-t-il dans une conversation juste après la détection d'un comportement passif-agressif ?
Est-ce un comportement ponctuel ou récurrent des interlocuteurs ?
Quelles réponses peuvent désamorcer une conversation et l'empêcher de *mal tourner* ?
Dans quelle mesure la détection précoce de toxicité subtile est-elle importante pour garder une conversation en bonne voie ?
Nous espérons que les modèles développés performeront suffisement bien pour être finalement intégrés dans l'interface de programmation applicative Perspective, de sorte qu'ils puissent être utilisés directement dans l'industrie, et soutenir un large éventail d'initiatives de recherche supplémentaires.
Afin d'amener les internautes en dehors de leurs bulles de filtres en ligne, ils doivent pouvoir dialoguer sur des plateformes à l'abri de l'agressivité, de la violence et de l'ostracisme (phénomène des boucs-émissaires).
Les stratégies d'apprentissage automatique, d'apprentissage profond et de traitement automatique du langage naturel peuvent nous aider à *confronter nos opinions divergentes de manière plus constructive* (The Righteous Mind, Jonathan Haidt).
Ainsi, cette recherche s'inscrit dans une optique d'accroissement de le participation, de la qualité, et de l'empathie dans les conversations en ligne à grande échelle, en développant de nouveaux modèles appliqués à de nouveaux jeux de données, suggérant des comportements non-biaisés, guidant et éduquant les utilisateurs vers plus d'équité.
L'utilisation de nouvelles technologie pour permettre un débat plus rationnel et plus éclairé s'inscrit dans un effort de lutte contre le harcèlement en ligne tout en défendant le débat public, la liberté d'expression, la démocratie et le pluralisme.
Aujourd'hui, l'approche « one-size-fits-all » pour les hypermédias n'est plus applicable.
Les Hypermédias Adaptatifs (AH) proposent d'adapter leur comportement aux besoins des utilisateurs.
Cependant, en raison de la complexité de leur processus de création et des différentes compétences requises par les auteurs, peu d'entre eux ont été développés.
Ces dernières années, de nombreux efforts ont été faits pour proposer d'aider les auteurs à créer leurs propres AH.
mais certains problèmes demeurent non résolus.
Nous nous sommes intéressées à deux problèmes particuliers.
Le premier problème concerne l'intégration des ressources des auteurs dans des systèmes existants.
Cela permet aux auteurs de réutiliser directement les adaptations prévues dans un système et de les exécuter sur leurs ressources.
Pour répondre à ce problème, nous proposons un processus semi-automatique de fusion / spécialisation pour intégrer le modèle d'un auteur dans un modèle d'un système existant.
Notre objectif est double : créer un support pour la définition des correspondances entre les éléments d'un modèle existant et ceux du modèle de l'auteur, et aider à créer un modèle cohérent intégrant les deux modèles et les correspondances entre eux.
Le deuxième problème concerne la spécification de l'adaptation, qui est notoirement le processus le plus difficile dans la création des hypermédias adaptatifs.
Nous proposons un framework EAP avec trois contributions principales : un ensemble de 22 patrons d'adaptation élémentaires pour l'adaptation de navigation, une typologie organisant les différents patrons d'adaptation élémentaires et un processus pour générer des stratégies d'adaptation basées sur l'utilisation et la combinaison semi-automatique des patrons d'adaptation élémentaires.
Nos objectifs sont de permettre de définir facilement des stratégies d'adaptation à un niveau d'abstraction élevé en combinant des stratégies d'adaptation simples.
Nous avons comparé l'expressivité du framework EAP à des solutions existantes, identifiant ainsi les avantages et les inconvénients des différentes décisions nécessaires à la définition d'un langage d'adaptation idéal.
Nous proposons aussi une vision unifiée de l'adaptation et des langages d'adaptation, basée sur l'analyse de ces solutions et de notre framework, ainsi que sur l'étude de l'expressivité de l'adaptation et de l'interopérabilité entre les différentes solutions analysées.
La vision unifiée sur l'adaptation n'est pas limitée aux solutions analysées. Elle peut être utilisée pour comparer et étendre d'autres approches.
Outre ces études théoriques qualitatives, cette thèse décrit également des implémentations et des évaluations expérimentales.
Cette thèse concerne la description des constructions possessives en tongugbe, l'un des dialectes de l'éwé, langue parlée au sud-est du Ghana au long du fleuve Volta.
La thèse présente une description détaillée des constructions ; et tente de comprendre la relation qui existe entre les constructions possessives propositionnelles et les constructions locatives et existentielles.
De plus, ce travail présente une première esquisse de la grammaire de tongugbe.
La grammaire présente surtout des résultats préliminaires sur le contraste de durée qui existe au niveau des tons de tongugbe et un système de démonstratifs très riche.
Les constructions possessives peuvent être regroupées dans trois catégories : les constructions attributives, les constructions prédicatives et les constructions à possesseur externe.
Il est montré que les configurations structurelles des constructions possessives attributives sont motivées par des considérations fonctionnelles.
Il est aussi démontré que les variations structurelles des constructions possessives prédicatives et des constructions à possesseur externe correspondent à des différences de sens.
Enfin, il est argumenté que, synchroniquement, les constructions possessives propositionnelles et les constructions locatives et existentielles ne peuvent pas être réduites à une structure unique.
La proposition soutenue est que chaque construction est une correspondance entre une forme et un sens.
Ce travail de thèse propose une nouvelle approche au traitement des langues naturelles.
Plutôt qu'essayer d'estimer directement la probabilité d'une phrase quelconque, nous identifions des structures syntaxiques dans le langage, qui peuvent être utilisées pour modifier et créer de nouvelles phrases à partir d'un échantillon initial.
L'étude des structures syntaxiques est accomplie avec des ensembles de substitution Markoviens, ensembles de chaînes de caractères qui peuvent être échangées sans affecter la distribution.
Ce point de vue décompose l'analyse du langage en deux parties, une phase de sélection de modèle, où les ensembles de substitution sont sélectionnés, et une phase d'estimation des paramètres, où les fréquences pour chaque ensemble sont estimées.
Nous montrons que ces processus constituent des familles exponentielles quand la structure du langage est fixée.
Lorsque la structure du langage est inconnue, nous proposons des méthodes pour identifier des ensembles de substitution à partir d'un échantillon, et pour estimer les paramètres de la distribution.
Les ensembles de substitution ont quelques relations avec les grammaires hors-Contexte, qui peuvent être utilisées pour aider l'analyse.
Nous construisons alors des dynamiques invariantes pour les processus de substitution.
Elles peuvent être utilisées pour calculer l'estimateur du maximum de vraisemblance.
En effet, les processus de substitution peuvent être vus comme la limite thermodynamique de la mesure invariante d'une dynamique de crossing
Le but de cette étude était de considérer la possibilité de la mise en œuvre de l'approche d'apprentissage par problèmes (APP), Problem Based Learning (PBL), comme une méthodologie d'enseignement, épistémologiquement solide, pour enseigner l'anglais de spécialité (ASP), en particulier, dans le domaine académique de la médecine, English for Academic Medical Purposes (EAMP).
Dans un premier temps, l'étude a examiné si PBL est compatible avec l'enseignement des langues et a cherché à déterminer les avantages que cette méthode peut apporter à l'enseignement de l'ASP.
L'étude a également tenté de résoudre des problèmes d'apprentissage en anglais qui ont été identifiés dans les Collèges de Santé de l'Année préparatoire (Branche Féminine) au sein de l'Université de Hail, Arabie Saoudite.
Une analyse des besoins a été menée dans l'institution pour mieux identifier ces problèmes d'apprentissage.
En conséquence, PBL a été mis en œuvre pour déterminer si cette approche est capable de fournir une solution possible à la question, puisque PBL a été initialement mis en œuvre en médecine pour faire face à des problèmes similaires.
Cela a entraîné un changement dans les niveaux macro-méthodologique et micro-méthodologique, comme Demaizière (1996 ; 66) les appelle.
Dans la partie empirique, une étude longitudinale a été menée avec 13 étudiantes qui ont été observées dans une période de 8 semaines au cours de cinq PBL tutoriels, qui a eu lieu pendant quinze séances.
En général, les résultats étaient en faveur de la mise en œuvre de cette approche dans l'enseignement de l'anglais médical.
Ils ont également montré que PBL peut améliorer l'autonomie des apprenants ;
Pour acquérir leur langue maternelle, les bébés doivent a la fois apprendre la forme des mots (par exemple, le mot “chien” en français, “dog” en anglais) et leur sens (la catégorie des chiens).
Ces deux aspects de l'apprentissage de la langue ont été typiquement étudiés indépendamment.
Des découvertes récentes en psychologie du développement et en apprentissage automatique suggèrent, néanmoins, que la forme et le sens pourraient très bien interagir, et ce, dès les premières étapes du développement.
La thèse explore cette piste à travers une étude interdisciplinaire qui combine des outils utilisés dans la technologie de la reconnaissance et la psychologie expérimentale.
Dans un premier temps, j'ai développé un modèle computationnel capable d'apprendre, à partir des données naturelles
Dans un deuxième temps, j'ai testé la plausibilité cognitive du mécanisme sur des sujets adultes.
L'analyse de ces données repose sur l'apprentissage de réseaux de neurones profonds, qui obtiennent des résultats à l'état de l'art dans de nombreux domaines.
En particulier, nous nous concentrons sur la compréhension des intéractions entre les objets ou personnes vivibles dans des images de la vie quotidienne, nommées relations visuelles.
Dans un deuxième temps, nous intégrons des connaissances sémantiques à ces réseaux pendant l'apprentissage.
Ces connaissances permettent d'obtenir des annotations qui correspondent davantage aux relations visibles.
En caractérisant la proximité sémantique entre relations, le modèle apprend ainsi à détecter une relation peu fréquente à partir d'exemples de relations plus largement annotées.
Enfin, après avoir montré que ces améliorations ne sont pas suffisantes si le modèle annote les relations sans en distinguer la pertinence, nous combinons des connaissances aux prédictions du réseau de façon à prioriser les relations les plus pertinentes
Le Transport Optimal est une théorie permettant de définir des notions géométriques de distance entre des distributions de probabilité et de trouver des correspondances, des relations, entre des ensembles de points.
De cette théorie, à la frontière entre les mathématiques et l'optimisation, découle de nombreuses applications en machine learning.
Comment l'adapter lorsque les données sont variées et ne font pas partie d'un même espace métrique ?
Cette thèse propose un ensemble d'outils de Transport Optimal pour ces différents cas.
Plus largement, nous analysons les propriétés mathématiques des différents outils proposés, nous établissons des solutions algorithmiques pour les calculer et nous étudions leur applicabilité dans de nombreux scenarii de machine learning qui couvrent, notamment, la classification, la simplification, le partitionnement de données structurées, ainsi que l'adaptation de domaines hétérogènes.
Les formations à distance en ligne, en particulier les MOOC, voient leurs effectifs augmenter depuis la démocratisation d'Internet.
Malgré leur popularité croissante ces cours manquent encore d'outils permettant aux instructeurs et aux chercheurs de guider et d'analyser finement les apprentissages qui s'y passent.
Des tableaux de bord récapitulant l'activité des étudiants sont régulièrement proposés aux instructeurs, mais ils ne leur permettent pas d'appréhender les activités collectives, or du point vue socio-constructiviste, les échanges et les interactions que les instructeurs cherchent généralement dans les forums sont essentiels pour les apprentissages (Stephens, 2014).
Jusqu'à présent, les études ont analysé les interactions soit sémantiquement mais à petite échelle, soit statistiquement et à grande échelle mais en ignorant la qualité des interactions.
La proposition de cette thèse est une nouvelle approche de détection interactive des activités collectives qui prend en compte à la fois leurs dimensions temporelles, sémantiques et sociales.
Nous cherchons un moyen de permettre aux instructeurs d'intervenir et d'encourager les dynamiques collectives qui sont favorables pour les apprentissages.
Ce que nous entendons par "dynamique collective", c'est l'évolution des interactions à la fois qualitatives et quantitatives, des apprenants dans des forums.
Mais, à la différence des études précédentes, notre approche ne se limite pas à une analyse globale ou centrée sur un individu.
Cette démarche a un gros inconvénient : elle nécessite un investissement lourd qui s'inscrit sur le long terme.
Pour palier à ce problème, il est nécessaire de mettre au point des méthodes et des outils informatiques d'aide à la construction de composants linguistiques fins et directement applicables à des textes.
Nous nous sommes penché sur le problème des grammaires locales qui décrivent des contraintes précises et locales sous la forme de graphes.
Deux questions fondamentales se posent : Comment construire efficacement des grammaires précises, complètes et applicables à des textes ?
Comment gérer leur nombre et leur éparpillement ?
Comme solution au premier problème, nous avons proposé un ensemble de méthodes simples et empiriques.
Nous avons exposé des processus d'analyse linguistique et de représentation à travers deux phénomènes : les expressions de mesure (un immeuble d'une hauteur de 20 mètres) et les adverbes de lieu contenant un nom propre locatif (à l'île de la Réunion), deux points critiques du TAL.
Sur la base de M. Gross (1975), nous avons ramené chaque phénomène à une phrase élémentaire.
Ceci nous a permis de classer sémantiquement certains phénomènes au moyen de critères formels.
Nous avons systématiquement étudié le comportement de ces phrases selon les valeurs lexicales de ses éléments.
Au cours de notre travail, nous avons été confronté à des systèmes relationnels de tables syntaxiques pour lesquels la méthode standard de conversion due à E. Roche (1993) ne fonctionnait plus.
Nous avons donc élaboré une nouvelle méthode adaptée avec des formalismes et des algorithmes permettant de gérer le cas où les informations sur les graphes à construire se trouvent dans plusieurs tables.
En ce qui concerne le deuxième problème, nous avons proposé et implanté un prototype de système de gestion de grammaires locales : une bibliothèque en-ligne de graphes.
Le but à terme est de centraliser et de diffuser les grammaires locales construites au sein du réseau RELEX.
Nous avons conçu un ensemble d'outils permettant à la fois de stocker de nouveaux graphes et de rechercher des graphes suivant différents critères.
Ce mémoire de thèse traite de la modélisation du discours dans le cadre grammatical des Grammaires Catégorielles Abstraites (Abstract Categorial Grammars, ACGs).
Les ACGs offrent un cadre unifié pour la modélisation de la syntaxe et de la sémantique.
Nous nous intéressons en particulier aux formalismes discursifs qui utilisent une approche grammaticale pour rendre compte des régularités des structures discursives.
Nous proposons en particulier un encodage à l'aide des ACGs de deux formalismes discursifs : G-TAG et D
En effet, pour prendre en compte ces connecteurs, G-TAG et D-STAG utilisent une étape extra-grammaticale.
Notre encodage offre au contraire une approche purement grammaticale de la prise en compte de ces connecteurs discursifs.
Ces encodages se font à l'aide d'ACGs de second ordre.
Les grammaires de cette classe ont des propriétés de réversibilité qui nous permettent d'utiliser les mêmes algorithmes polynômiaux aussi bien pour l'analyse discursive que pour la génération de discours.
Un problème central contribuant à la grande difficulté du traitement du langage naturel par des méthodes statistiques est celui de la parcimonie des données, à savoir le fait que dans un corpus d'apprentissage donné, la plupart des évènements linguistiques n'ont qu'un nombre d'occurrences assez faible, et que par ailleurs un nombre infini d'évènements permis par une langue n'apparaitront nulle part dans le corpus.
Les modèles neuronaux ont déjà contribué à partiellement résoudre le problème de la parcimonie en inférant des représentations continues de mots.
Ces représentations continues permettent de structurer le lexique en induisant une notion de similarité sémantique ou syntaxique entre les mots.
Toutefois, les modèles neuronaux actuellement les plus répandus n'offrent qu'une solution partielle au problème de la parcimonie, notamment par le fait que ceux-ci nécessitent une représentation distribuée pour chaque mot du vocabulaire, mais sont incapables d'attribuer une représentation à des mots hors vocabulaire.
Ce problème est particulièrement marqué dans des langues morphologiquement riches, ou des processus de formation de mots complexes mènent à une prolifération des formes de mots possibles, et à une faible coïncidence entre le lexique observé lors de l'entrainement d'un modèle, et le lexique observé lors de son déploiement.
Aujourd'hui, l'anglais n'est plus la langue majoritairement utilisée sur le Web, et concevoir des systèmes de traduction automatique pouvant appréhender des langues dont la morphologie est très éloignée des langues ouest-européennes est un enjeu important.
L'objectif de cette thèse est de développer de nouveaux modèles capables d'inférer de manière non-supervisée les processus de formation de mots sous-jacents au lexique observé, afin de pouvoir de pouvoir produire des analyses morphologiques de nouvelles formes de mots non observées lors de l'entraînement.
La plupart des méthodes statistiques ne sont pas nativement conçues pour fonctionner sur des données incomplètes.
L'étude des données incomplètes n'est pas nouvelle et de nombreux résultats ont été établis pour pallier l'incomplétude en amont de l'étude statistique.
D'autre part, les méthodes de deep learning sont en général appliquées à des données non structurées de type image, texte ou audio, mais peu de travaux s'intéressent au développement de ce type d'approche sur des données tabulaires, et encore moins sur des données incomplètes.
Cette thèse se concentre sur l'utilisation d'algorithmes de machine learning appliqués à des données tabulaires, en présence d'incomplétude et dans un cadre assurantiel.
Au travers des contributions regroupées dans ce document, nous proposons différentes façons de modéliser des phénomènes complexes en présence de schémas d'incomplétude.
Nous montrons que les approches proposées donnent des résultats de meilleure qualité que l'état de l'art
La factorisation des tenseurs est au coeur des méthodes d'analyse des données massives multidimensionnelles dans de nombreux domaines, dont les systèmes de recommandation, les graphes, les données médicales, le traitement du signal, la chimiométrie, et bien d'autres.
Pour toutes ces applications, l'obtention rapide de la décomposition des tenseurs est cruciale pour pouvoir traiter manipuler efficacement les énormes volumes de données en jeu.
L'objectif principal de cette thèse est la conception d'algorithmes pour la décomposition de tenseurs multidimensionnels creux, possédant de plusieurs centaines de millions à quelques milliards de coefficients non-nuls. De tels tenseurs sont omniprésents dans les applications citées plus haut.
Nous poursuivons cet objectif via trois approches.
En premier lieu, nous proposons des algorithmes parallèles à mémoire distribuée, comprenant des schémas de communication point-à-point optimisés, afin de réduire les coûts de communication.
Ces algorithmes sont indépendants du partitionnement des éléments du tenseur et des matrices de faible rang. Cette propriété nous permet de proposer des stratégies de partitionnement visant à minimiser le coût de communication tout en préservant l'équilibrage de charge entre les ressources.
Nous utilisons des techniques d'hypergraphes pour analyser les paramètres de calcul et de communication de ces algorithmes, ainsi que des outils de partitionnement d'hypergraphe pour déterminer des partitions à même d'offrir un meilleur passage à l'échelle.
Deuxièmement, nous étudions la parallélisation sur plate-forme à mémoire partagée de ces algorithmes.
Dans ce contexte, nous déterminons soigneusement les tâches de calcul et leur dépendances, et nous les exprimons en termes d'une structure de données idoine, et dont la manipulation permet de révéler le parallélisme intrinsèque du problème.
Troisièmement, nous présentons un schéma de calcul en forme d'arbre binaire pour représenter les noyaux de calcul les plus coûteux des algorithmes, comme la multiplication du tenseur par un ensemble de vecteurs ou de matrices donnés. L'arbre binaire permet de factoriser certains résultats intermédiaires, et de les ré-utiliser au fil du calcul.
Grâce à ce schéma, nous montrons comment réduire significativement le nombre et le coût des multiplications tenseur-vecteur et tenseur-matrice, rendant ainsi la décomposition du tenseur plus rapide à la fois pour la version séquentielle et la version parallèle des algorithmes.
Enfin, le reste de la thèse décrit deux extensions sur des thèmes similaires.
La première extension consiste à appliquer le schéma d'arbre binaire à la décomposition des tenseurs denses, avec une analyse précise de la complexité du problème et des méthodes pour trouver la structure arborescente qui minimise le coût total.
La seconde extension consiste à adapter les techniques de partitionnement utilisées pour la décomposition des tenseurs creux à la factorisation des matrices non-négatives, problème largement étudié et pour lequel nous obtenons des algorithmes parallèles plus efficaces que les meilleurs actuellement connus.
Tous les résultats théoriques de cette thèse sont accompagnés d'implémentations parallèles,aussi bien en mémoire partagée que distribuée.
Tous les algorithmes proposés, avec leur réalisation sur plate-forme HPC, contribuent ainsi à faire de la décomposition de tenseurs un outil prometteur pour le traitement des masses de données actuelles et à venir.
Cette thèse s'intéresse aux fonctions des marqueurs discursifs 'you know', 'then' et 'ya'ni' (I mean) en tant que structures autonomes du point de vue syntaxique et en tant qu'unités linguistiques opérationnelles et multifonctionnelles du point de vue conversationnel.
Dans une perspective pragmatique, ce travail explore la corrélation entre la position et la fonction en maintenant que la valeur d'un marqueur en position initiale est différente de celle exprimée en position médiane ou en position finale.
Dans le contexte des interactions verbales politiques au sein des émissions télévisées en arabe et en anglais, le but de cette thèse est de contribuer aux analyses conversationnelles pragmatiques.
À travers une recherche empirique, l'analyse montre que la multifonctionnalité de ces items linguistiques est liée à leur flexibilité syntaxique.
Ces marqueurs discursifs assument diverses fonctions contextuelles qui évoluent continuellement sur une échelle pragmatique.
L'évolution pragmatique de 'you know', 'then' et 'ya'ni' engendre des expressions complexes au niveau de leur sémantisme.
Ces unités pragmatiques deviennent plus ambiguës car elles s'éloignent davantage de leur sens de base en acquérant des significations contextuelles.
Ainsi, ces marqueurs évoquent d'autres interprétations et ne peuvent se limiter qu'à leur sens d'origine.
En l'occurrence, on leur attribue des équivalents les plus proches selon le contexte de leur occurrence.
Les résultats révèlent que 'ya'ni' peut être substitué par d'autres marqueurs de différentes catégories grammaticales en anglais.
La variation pragmatique suit le but illocutoire du locuteur, la prise en compte d'autrui et l'organisation de l'interaction verbale.
Dans deux situations socioculturelles différentes, l'anglais et l'arabe confirment que ces items linguistiques sont des unités conversationnelles contextuelles et multifonctionnelles.
Leur rôle est visiblement actif dans une situation sociale où l'interaction entre le locuteur et l'interlocuteur est un facteur saillant ;
Notre recherche doctorale porte sur l'influence des pratiques d'enseignement du code alphabétique sur les progrès des élèves de cours préparatoire.
Elle a pour objectif d'identifier des pratiques pédagogiques efficaces et de contribuer à la réflexion sur la formation professionnelle des enseignants.
Elle constitue l'un des volets d'une enquête collective de grande ampleur dirigée par Roland Goigoux qui visait à évaluer l'influence des pratiques d'enseignement de la lecture et de l'écriture sur la qualité des apprentissages.
La première partie de notre recherche est consacrée à la mise en évidence de relations causales entre les pratiques d'enseignement du code alphabétique et les performances des élèves en décodage et en orthographe.
Nous nous intéressons tout d'abord à la question de la planification de l'enseignement, plus précisément à la vitesse d'étude des correspondances entre les graphèmes et les phonèmes (tempo) et à la part déchiffrable des textes utilisés comme supports d'enseignement de la lecture (rendement effectif).
Nos résultats soulignent l'influence significative de ces deux variables sur la qualité des apprentissages, cette influence s'exerçant de manière différenciée selon le niveau des élèves à l'entrée du cours préparatoire.
En outre, nous proposons une progression de l'étude du code alphabétique fondée sur la fréquence théorique des correspondances graphèmes-phonèmes des textes écrits en français standard pouvant servir de référence aux enseignants.
Nous étudions également les effets du temps d'enseignement de l'encodage sur les acquisitions scolaires, effets qui se révèlent significatifs et positifs mais qui varient selon la nature des tâches proposées et les publics ciblés.
Dans la seconde partie de notre thèse, nous nous attachons à comprendre et à documenter la conduite de l'activité de maitres expérimentés de cours préparatoire à des fins de formation professionnelle.
Nous analysons une situation de référence de l'enseignement du lire-écrire à partir des enregistrements vidéo de trente-six séances de lecture collectives.
Puis, nous décrivons des scénarios pédagogiques prototypiques et nous posons les bases d'une formation destinée à développer les compétences professionnelles des enseignants.
Nous soulevons notamment la problématique de l'articulation de la résolution de tâches de code et de compréhension et celle de l'autonomie de déchiffrage offerte aux élèves.
Nous présentons enfin la plateforme numérique que nous avons élaborée et qui permet de déterminer la part déchiffrable des textes utilisés lors des séances de lecture collectives.
Cette plateforme nommée Anagraph aide les enseignants à planifier l'étude des correspondances graphophonémiques et à choisir des textes adaptés à l'enseignement de la lecture
De nombreuses applications qui font maintenant partie de notre vie telles que des assistants personnels, la traduction automatique, ou encore la reconnaissance vocale, reposent sur ces techniques.
L'apprentissage par imitation propose de réaliser les procédures d'apprentissage et d'inférence de manière approchée afin de pouvoir exploiter pleinement des structures de dépendance plus riches.
Cette thèse explore ce cadre d'apprentissage, en particulier l'algorithme SEARN, à la fois sur le plan théorique ainsi que ses possibilités d'application aux tâches de traitement automatique des langues, notamment aux plus complexes telles que la traduction.
Concernant les aspects théoriques, nous présentons un cadre unifié pour les différentes familles d'apprentissage par imitation, qui permet de redériver de manière simple les propriétés de convergence de ces algorithmes ;
concernant les aspects plus appliqués, nous utilisons l'apprentissage par imitation d'une part pour explorer l'étiquetage de séquences en ordre libre ; d'autre part pour étudier des stratégies de décodage en deux étapes pour la traduction automatique.
Ces dernières peuvent, en particulier, prendre une autre dimension dans le contexte de création d'une écriture.
Elles offrent ainsi des circonstances propices à cette recherche.
L'objectif est de préserver une signification profonde pour le locuteur/scripteur.
Pour répondre à cette question, nous proposons de réinvestir une gestuelle porteuse de sens dans le cadre de la conception d'un environnement technique favorisant la création scripturale.
Dans un premier temps, cette étude pluridisciplinaire explore
En d'autres termes, quels éléments de l'oral peuvent être rapportés à l'écriture.
Dans un second temps, elle envisage l'instrument permettant ce transfert.
Pour cela, nous employons une démarche phénoménologique entendue comme méthodologie descriptive du point de vue en première personne.
Cette méthode s'appuie sur des techniques de verbalisation de l'expérience vécue lors d'entretiens.
La construction de la méthode adaptée à la LS française permet d'accéder à des descriptions fines des locuteurs sourds sur leur gestuelle.
Le corpus de données est ensuite mis en dialogue avec une analyse en troisième personne établie à l'aide d'études linguistiques et kinésiologiques.
Les résultats sur les dimensions du geste sémiotique nous amènent à penser les conditions d'une expérience habilitée, dans une perspective d'appropriation et de création scripturale des LS.
Nous suivons pour cela la démarche de conception du design d'expérience utilisateur, du design d'énaction et de l'approche instrumentale pour l'immersion et l'interaction.
La conception d'un tel dispositif vient non seulement changer le regard des personnes sourdes sur leur langue, mais également de manière plus générale, changer la relation que tout utilisateur a avec sa production gestuelle.
Les algorithmes de Frank-Wolfe sont des méthodes d'optimisation de problèmes sous contraintes.
Elles décomposent un problème non-linéaire en une série de problèmes linéaires.
Cela en fait des méthodes de choix pour l'optimisation en grande dimension et notamment explique leur utilisation dans de nombreux domaines appliqués.
Ici nous proposons de nouveaux algorithmes de Frank-Wolfe qui convergent plus rapidement vers la solution du problème d'optimisation sous certaines hypothèses structurelles assez génériques.
Nous montrons en particulier, que contrairement à d'autres types d'algorithmes, cette famille s'adapte à ces hypothèses sans avoir à spécifier les paramètres qui les contrôlent.
Cette thèse se compose de deux parties indépendantes et la première regroupant deux problématiques distinctes.
Dans la première partie, nous étudions d'abord le problème de Principal-Agent dans des systèmes dégénérés, qui apparaissent naturellement dans des environnements à l'observation partielle où l'Agent et le Principal n'observent qu'une partie du système.
Nous présentons une approche se basant sur le principe du maximum stochastique, dont le but est d'étendre les travaux existants qui utilisent le principe de la programmation dynamique dans des systèmes non-dégénérés.
Ensuite nous utilisons la condition suffisante du problème de l'Agent pour vérifier que le contrat optimal obtenu est bien implémentable.
Une étude parallèle est consacrée à l'existence et l'unicité de la solution d'EDSPRs dépendantes de la trajectoire dans le chapitre IV.
Enfin, nous étudions le problème de hasard moral avec plusieurs Principals.
L'Agent ne peut travailler que pour un seul Principal à la fois et fait donc face à un problème de switching optimal.
En utilisant la méthode de randomisation nous montrons que la fonction valeur de l'Agent et son effort optimal sont donnés par un processus d'Itô.
Cette représentation nous aide à résoudre ensuite le problème du Principal lorsqu'il y a une infinité de Principals en équilibre selon un jeu à champ-moyen.
Nous justifions la formulation à champ-moyen par un argument de propagation de chaos.
La deuxième partie de cette thèse est constituée des chapitres V et VI.
La motivation de ces travaux est de donner un fondement théorique rigoureux pour la convergence des algorithmes du type descente de gradient très souvent utilisés dans la résolution des problème non-convexes comme la calibration d'un réseau de neurones.
Nous montrons que la fonction d'énergie correspondante admet un unique minimiseur qui peut être caractérisé par une condition du premier ordre utilisant la dérivation dans l'espace des mesures au sens de Lions.
Nous présentons ensuite une analyse du comportement à long terme de la dynamique de Langevin à champ-moyen, qui possède une structure de flot de gradient dans la métrique de 2-Wasserstein.
Nous montrons que le flot de la loi marginale induite par la dynamique de Langevin à champ-moyen converge vers une loi stationnaire en utilisant le principe d'invariance de La Salle, qui est le minimiseur de la fonction d'énergie.
Dans le cas des réseaux de neurones profonds, nous les modélisons à l'aide d'un problème de contrôle optimal en temps continu.
Nous donnons d'abord la conditiondu premier ordre à l'aide du principe de Pontryagin, qui nous aidera ensuiteà introduire le système d'équation de Langevin à champ-moyen, dont la mesure invariante correspond au minimiseur du problème de contrôle optimal.
Enfin, avec la méthode de couplage par réflexion nous montrons que la loi marginale du système de Langevin à champ-moyen converge vers la mesure invariante avec une vitesse exponentielle.
Le langage non-littéral (expressions idiomatiques, métaphores, métonymies, etc.) se révèle être très présent dans nos conversations de la vie de tous les jours.
En revanche, très peu de didacticiens se sont interrogés sur les capacités des apprenants à produire du non-littéral dans une langue étrangère à l'exception de Littlemore et al.
Dans le but d'avoir une première idée de la façon dont ces deux types de sujets manient le non-littéral, j'analyse tout d'abord le discours d'une petite fille de nationalité anglaise filmée à intervalles réguliers entre l'âge d'un an et quatre ans, puis j'examine les productions écrites en anglais d'apprenants francophones.
J'observe ensuite les productions non-littérales d'enfants natifs anglophones âgés de 7, 11 et 15 ans, d'apprenants francophones en classe de seconde, première année de licence d'anglais et deuxième année de master d'anglais, et enfin, d'un groupe contrôle d'
Ces formes proviennent principalement d'une carence en ressources lexicales de la langue étrangère et d'expressions figuratives du français que les apprenants ont souhaité transposer en anglais.
Cette thèse propose un ensemble d'implications pédagogiques pour la classe de langue dans le but de remédier à ces difficultés.
Les systèmes embarqués temps réel sont de plus en plus omniprésents dans la vie quotidienne.
Le cycle de développement des systèmes embarqués temps réel critiques peut prendre des mois voire des années.
Par conséquent, la modélisation de ces systèmes doit être analysée à un stade précoce du cycle de développement afin de vérifier si toutes les exigences sont satisfaites, y compris les exigences relatives à la performance temporelle (par exemple, les latences, les délais de bout en bout, etc.).
Cette thèse, qui a été financée dans le cadre d'un projet FUI, propose trois contributions majeures.
La première contribution porte sur le système de tâches monoprocesseur avec des relations de communication multi-périodique déterministe.
Un pattern basé sur les relations de précédence avec sémaphore (SPC) a été étendu afin de supporter les cycles dans le cas d'ordonnancement à priorités dynamiques.
Un graphe de dépliage a été également proposé afin de présenter la cyclicité des systèmes à base de SPC et garantir le non-blocage.
Le pattern SPC étendu ainsi que le test d'ordonnançabilité correspondant ont été implémentés pour le langage standard
AADL.La deuxième contribution de cette thèse consiste en la proposition d'un calcul exact de temps de réponse de bout en bout, en utilisant le formalisme de réseau de Petri temporel, pour les systèmes multiprocesseurs identiques.
Il prend en compte la dépendance entre les tâches : la précédence et l'exclusion mutuelle sans protocole de gestion.
La troisième contribution porte sur la capitalisation des efforts du processus d'analyse.
En effet, de nombreux tests d'analyse ont été proposés, notamment par des chercheurs académiques, basés sur la théorie d'ordonnancement et dédiés aux différentes architectures logicielles et matérielles.
Cependant, l'une des principales difficultés rencontrées par les concepteurs est de choisir le test d'analyse le plus approprié permettant de valider et/ou de dimensionner correctement leurs conceptions.
Cette difficulté se concrétise, dans le milieu industriel, par le peu de tests d'analyse utilisés malgré la multitude de tests proposés.
Cette thèse vise donc à faciliter le processus d'analyse, réduire le fossé sémantique entre le modèle métier et les entrées des tests d'analyse ainsi qu'accélérer le transfert technologique et l'adoption des tests académiques.
La thèse propose un référentiel d'analyse jouant le rôle d'un dictionnaire de tests, leurs contextes pour une utilisation correcte, les outils les implémentant, ainsi qu'un mécanisme pour le choix des tests selon le modèle métier d'entrée.
Dans un processus de conception centré sur l'utilisateur, les artefacts évoluent par cycles itératifs jusqu'à ce qu'ils répondent aux exigences des utilisateurs et deviennent ensuite le produit final.
Chaque cycle donne l'occasion de réviser la conception et d'introduire de nouvelles exigences qui pourraient affecter les artefacts qui ont été définis dans les phases de développement précédentes.
Garder la cohérence des exigences dans tels artefacts tout au long du processus de développement est une activité lourde et longue, surtout si elle est faite manuellement.
Actuellement, certains cadres d'applications implémentent le BDD (Développement dirigé par le comportement) et les récits utilisateur comme un moyen d'automatiser le test des systèmes interactifs en construction.
Les tests automatisés permettent de simuler les actions de l'utilisateur sur l'interface et, par conséquent, de vérifier si le système se comporte correctement et conformément aux exigences de l'utilisateur.
Cependant, les outils actuels supportant BDD requièrent que les tests soient écrits en utilisant des événements de bas niveau et des composants qui n'existent que lorsque le système est déjà implémenté.
En conséquence d'un tel bas niveau d'abstraction, les tests BDD peuvent difficilement être réutilisés avec des artefacts plus abstraits.
Afin d'éviter que les tests doivent être écrits sur chaque type d'artefact, nous avons étudié l'utilisation des ontologies pour spécifier à la fois les exigences et les tests, puis exécuter des tests dans tous les artefacts partageant les concepts ontologiques.
L'ontologie fondée sur le comportement que nous proposons ici vise alors à élever le niveau d'abstraction tout en supportant l'automatisation de tests dans des multiples artefacts.
Cette thèse présente tel ontologie et une approche fondée sur BDD et les récits utilisateur pour soutenir la spécification et l'évaluation automatisée des exigences des utilisateurs dans les artefacts logiciels tout au long du processus de développement des systèmes interactifs.
Deux études de cas sont également présentées pour valider notre approche.
La première étude de cas évalue la compréhensibilité des spécifications des récits utilisateur par une équipe de propriétaires de produit (POs) du département en charge des voyages d'affaires dans notre institut.
À l'aide de cette première étude de cas, nous avons conçu une deuxième étude pour démontrer comment les récits utilisateur rédigés à l'aide de notre ontologie peuvent être utilisées pour évaluer les exigences fonctionnelles exprimées dans des différents artefacts, tels que les modèles de tâche, les prototypes d'interface utilisateur et les interfaces utilisateur à part entière.
Les résultats ont montré que notre approche est capable d'identifier même des incohérences à grain fin dans les artefacts mentionnés, permettant d'établir une compatibilité fiable entre les différents artefacts de conception de l'interface utilisateur.
Dans l'essor d'internet, les contenus générés par les utilisateurs à partir des services de réseaux sociaux deviennent une source géante d'informations qui peuvent être utile aux entreprises sur l'aspect où les utilisateurs sont considérés comme des clients ou des clients potentiels pour les entreprises.
L'exploitation des textes générés par les utilisateurs peut aider à identifier leurs sentiments, leurs intentions, ou réduire l'effort des agents qui sont responsables de recueillir ou de recevoir des informations sur les services de réseaux sociaux.
Dans le cadre de cette thèse, les contenues de textes tels que discours, énoncés, conversations issues de la communication interactive sur les plateformes de réseaux sociaux deviennent l'objet de données principales de notre étude.
Nous proposons une méthode pour l'extraction d'un arbre de GCC à partir de l'arbre dépendante de la phrase, et une architecture générale pour construire un pont de relation entre les syntaxes et les sémantiques des phrases françaises.
Ma dissertation explore comment l'intégration de petites visualisations contextuelles basées sur des données peut complémenter des documents écrits.
Plus spécifiquement, j'identifie et je définis des aspects importants et des directions de recherches pertinentes pour l'intégration de petites visualisations contextuelles basées sur des données textuelles.
Cette intégration devra finalement devenir aussi fluide qu'écrire et aussi utile que lire un texte.
Je définis les visualisations-mots (Word-Scale Visualizations) comme étant de petites visualisations contextuelles basées sur des données intégrées au texte de documents.
Ces visualisations peuvent utiliser de multiples codages visuels incluant les cartes géographiques, les heatmaps, les graphes circulaires, et des visualisations plus complexes.
Les visualisations-mots offrent une grande variété de dimensions toujours proches de l'échelle d'un mot, parfois plus grandes, mais toujours plus petites qu'une phrase ou un paragraphe.
Les visualisations-mots peuvent venir en aide et être utilisées dans plusieurs formes de discours écrits tels les manuels, les notes, les billets de blogs, les rapports, les histoires, ou même les poèmes.
En tant que complément visuel de textes, les visualisations-mots peuvent être utilisées pour accentuer certains éléments d'un document (comme un mot ou une phrase), ou pour apporter de l'information additionnelle.
Par exemple, un petit diagramme de l'évolution du cours de l'action d'une entreprise peut être intégré à côté du nom de celle-ci pour apporter de l'information additionnelle sur la tendance passée du cours de l'action.
Dans un autre exemple, des statistiques de jeux peuvent être incluses à côté du nom d'équipe de football ou de joueur dans les articles concernant le championnat d'Europe de football.
Ces visualisations-mots peuvent notamment aider le lecteur à faire des comparaisons entre le nombre de passes des équipes et des joueurs.
Le bénéfice majeur des visualisations-mots réside dans le fait que le lecteur peut rester concentré sur le texte, vu que les visualisations sont dans le texte et non à côté.
J'étudie différentes options de placement pour les visualisations-mots et je quantifie leurs effets sur la disposition du texte et sa mise en forme.
J'examine aussi comment combiner les visualisations-mots et l'interaction pour soutenir une lecture plus active en proposant des méthodes de collection, d'arrangement et de comparaison de visualisations-mots.
Finalement, je propose des considérations de design pour la conception et la création de visualisations-mots et je conclus avec des exemples d'application.
En résumé cette dissertation contribue à la compréhension de petites visualisations contextuelles basées sur des données intégrées dans le texte et à leur utilité pour la visualisation d'informations.
Analyse de données géométriques, au delà des convolutionsPour modéliser des interactions entre points, une méthode simple est de se reposer sur des sommes pondérées communément appelées "convolutions".
Au cours de la dernière décennie, cette opération est devenue la brique de construction essentielle à la révolution du "deep learning".
Le produit de convolution est, toutefois, loin d'être l'alpha et l'oméga des mathématiques appliquées.
Pour permettre aux chercheurs d'explorer de nouvelles directions, nous présentons des implémentations robustes et efficaces de trois opérations souvent sous
Le transport optimal, qui généralise la notion de "tri" aux espaces de dimension D &gt; 1.3.
Le tir géodésique sur une variété Riemannienne, qui se substitue à l'interpolation linéaire sur des espaces de données où aucune structure vectorielle ne peut être correctement définie.
Nos routines PyTorch/NumPy sont compatibles avec la différentiation automatique, et s'exécutent en quelques secondes sur des nuages de plusieurs millions de points.
Elle sont de 10 à 1,000 fois plus performantes que des implémentations GPU standards et conservent une empreinte mémoire linéaire.
Ces nouveaux outils sont empaquetés dans les bibliothèques "KeOps" et "GeomLoss", avec des applications qui vont de l'apprentissage automatique à l'imagerie médicale.
Notre documentation est accessible aux adresses www.kernel-operations.io/keops et /geomloss.
Alors que nous nous représentons le monde au travers de nos sens, de notre langage et de nos interactions, chacun de ces domaines a été historiquement étudié de manière indépendante en apprentissage automatique.
Heureusement, ce cloisonnement tend à se défaire grâce aux dernières avancées en apprentissage profond, ce qui a conduit à l'uniformisation de l'extraction des données au travers des communautés.
Cependant, les architectures neuronales multimodales n'en sont qu'à leurs premiers balbutiements et l'apprentissage par renforcement profond est encore souvent restreint à des environnements limités.
Idéalement, nous aimerions pourtant développer des modèles multimodaux et interactifs afin qu'ils puissent correctement appréhender la complexité du monde réel.
Dans cet objectif, cette thèse s'attache à la compréhension du langage combiné à la vision pour trois raisons : (i) ce sont deux modalités longuement étudiées aux travers des différentes communautés scientifiques (ii) nous pouvons bénéficier des dernières avancées en apprentissage profond pour les modèles de langues et de vision (iii) l'interaction entre l'apprentissage du langage et notre perception a été validé en science cognitives.
Ainsi, nous avons conçu le jeu GuessWhat?! (KéZaKo) afin d'évaluer la compréhension de langue combiné à la vision de nos modèles : deux joueurs doivent ainsi localiser un objet caché dans une image en posant une série de questions.
Enfin, nous explorons comment l'apprentissage par renforcement permet l'apprentissage de la langue et cimente l'apprentissage des représentations multimodales sous-jacentes.
Nous montrons qu'un tel apprentissage interactif conduit à des stratégies langagières valides mais donne lieu à de nouvelles problématiques de recherche.
Cette thèse définit un modèle sémantique, non probabiliste et prédictif, pour l'analyse décisionnelle de réseaux sociaux professionnels et institutionnels.
Ce modèle, en parallèle à la sociophysique de Galam, intègre des méthodes de traitement sémantique du langage naturel et d'ingénierie des connaissances, des mesures de sociologie statistique et des lois électrodynamiques, appliquées à l'optimisation de la performance économique et du climat social.
Il a été développé et expérimenté dans le cadre du projet Socioprise, financé par le Secrétariat d'´Etat à la prospective et au développement de l'économie numérique.
La sclérose en plaques (SEP) est la maladie neurologique évolutive la plus courante chez les jeunes adultes dans le monde et représente donc un problème de santé publique majeur avec environ 90 000 patients en France et plus de 500 000 personnes atteintes de SEP en Europe.
Afin d'optimiser les traitements, il est essentiel de pouvoir mesurer et suivre les altérations cérébrales chez les patients atteints de SEP.
En fait, la SEP est une maladie aux multiples facettes qui implique différents types d'altérations, telles que les dommages et la réparation de la myéline.
Selon cette observation, la neuroimagerie multimodale est nécessaire pour caractériser pleinement la maladie.
L'imagerie par résonance magnétique (IRM) est devenue un biomarqueur d'imagerie fondamental pour la sclérose en plaques en raison de sa haute sensibilité à révéler des anomalies tissulaires macroscopiques chez les patients atteints de SEP.
L'IRM conventionnelle fournit un moyen direct de détecter les lésions de SEP et leurs changements, et joue un rôle dominant dans les critères diagnostiques de la SEP.
De plus, l'imagerie par tomographie par émission de positons (TEP), une autre modalité d'imagerie, peut fournir des informations fonctionnelles et détecter les changements tissulaires cibles au niveau cellulaire et moléculaire en utilisant divers radiotraceurs.
Cependant, en milieu clinique, toutes les modalités ne sont pas disponibles pour diverses raisons.
Dans cette thèse, nous nous concentrons donc sur l'apprentissage et la prédiction des altérations cérébrales dérivées des modalités manquantes dans la SEP à partir de données de neuroimagerie multimodale.
Cette thèse se propose d'étudier de quelle façon la misogynie peut prendre pour cible des pratiques linguistiques pouvant être perçues comme féminines et prend l'exemple du stéréotype sexiste de la « Valley Girl » .
Ce terme, popularisé dans les années 1980 par le titre éponyme de Frank Zappa, a d'abord fait référence à des adolescentes, prétendument futiles et décérébrées, appartenant à la classe moyenne californienne.
Bien que ces dernières aient été ridiculisées dans la chanson, cette exposition médiatique a paradoxalement eu pour effet de lancer un effet mode dont l'une des manifestations était linguistique : le Valspeak.
Ce dialecte comprend entre autres des marqueurs phonétiques (le California Vowel Shift), prosodiques (l'utilisation d'un contour intonatif montant), lexicaux (fer sure, gag me with a spoon) ou de discours (LIKE).
Bien que certains de ces marqueurs n'aient pas été (uniquement) popularisés par les Valley Girls, ils peuvent néanmoins être perçus comme tels, et y avoir recours peut exposer un locuteur à une perception négative de sa personne.
Ce travail cherche à interroger les liens pouvant exister entre la stigmatisation des marqueurs du Valspeak et la misogynie, phénomène que nous appelons la « misogynie linguistique » du Valspeak.
Dans quelle mesure la potentielle stigmatisation de ces marqueurs peut-elle être due au genre féminin des locutrices prototypiques de ce dialecte ?
Trois éléments de réponse à cette question sont proposés.
Tout d'abord, une étude de perception dialectale quantitative portant sur trois marqueurs du Valspeak (le California Vowel Shift, le contour intonatif montant et LIKE) est menée auprès de locuteurs de l'anglais américain.
Deuxièmement, des entretiens qualitatifs ont pour but d'évaluer quelles idéologies sont associées aux marqueurs linguistiques du Valspeak et à la persona Valley Girl.
Enfin, l'analyse porte sur trois représentations télévisuelles humoristiques de personnages féminins dans Parks and Recreation, Les Griffin et Ew ! (un sketch dans The Tonight Show Starring Jimmy Fallon).
Il est montré que des marqueurs du Valspeak sont utilisés afin d'orchestrer la stigmatisation de personnages féminins sans faire explicitement appel au stéréotype Valley Girl.
L'accroissement rapide des données numériques vidéographiques fait de la compréhension automatique des vidéos un enjeu de plus en plus important.
Comprendre de manière automatique une vidéo recouvre de nombreuses applications, parmi lesquelles l'analyse du contenu vidéo sur le web, les véhicules autonomes,les interfaces homme-machine (eg, Kinect).
Cette thèse présente des contributions dans deux problèmes majeurs pour la compréhension automatique des vidéos : la détection d'actions supervisée par des données web, et la localisation d'actions humaines.
La détection d'actions supervisées par des données web a pour objectif d'apprendre à reconnaître des actions dans des contenus vidéos sur Internet, sans aucune autre supervision.
Nous proposons une approche originale dans ce contexte, qui s'appuie sur la synergie entre les données visuelles (les vidéos) et leur description textuelle associée, et ce dans le but d'apprendre des classifieurs pour les événements sans aucune supervision.
Nous montrons l'importance des deux étapes principales, c'est-à-dire la créations des requêtes et l'étape de suppression des vidéos, par des résutats quantitatifs.
Notre approche est évaluée dans des conditions difficiles, où aucune annotation manuelle n'est disponible, dénotées EK0 dans les challenges TrecVid. Nous obtenons l'état de l'art sur les bases de données MED 2011 et 2013.
Dans la seconde partie de notre thèse, nous nous concentrons sur la localisation des actions humaines, ce qui implique de reconnaître à la fois les actions se déroulant dans la vidéo, comme par exemple "boire" ou "téléphoner", et leur étendues spatio-temporelles.
Nous proposons une nouvelle méthode centrée sur la personne, traquant celle-ci dans les vidéos pour en extraire des tubes encadrant le corps entier, même en cas d'occultations ou dissimulations partielles.
Deux raisons motivent notre approche.
La première est qu'elle permet de gérer les occultations et les changements de points de vue de la caméra durant l'étape de localisation des personnes, car celle-ci estime la position du corps entier à chaque frame.
Notre algorithme de tracking connecte les détections temporellement pour extraire des tubes encadrant le corps entier.
Nous évaluons notre nouvelle méthode d'extraction de tubes sur une base de données difficile, DALY, et atteignons l'état de l'art.
Cette étude présente les caractéristiques, les écritures et la structure de la langue ouïghoure en faisant une étude linguistique et en proposant de nouveaux modèles expérimentaux qui faciliteront le développement des outils informatiques et le traitement automatique de la langue afin de contribuer à l'informatisation de la langue ouïghoure.
Plus précisément, notre étude consiste en quatre parties :
la première partie présente les problématiques d'étude, les caractéristiques de la langue et des écritures, notamment le processus d'unification de l'écriture ouïghoure-latine ;
la deuxième partie expose les notions de base d'extraction d'information et démontre la possibilité d'extraction d'entités nommées en utilisant un outil d'extraction, afin d'expérimenter les conceptions et les théories proposées ;
la troisième partie est consacré à l'étude linguistique notamment sur l'aspect agglutinant de la langue et les règles morphologiques de suffixation qui seront appliquées pendant la réalisation des outils prototypes proposés dans cette thèse ;
enfin la quatrième partie mettre en évidence les problématiques de traitement de la langue ouïghoure dans une situation où les systèmes d'exploitation ne supporte pas la langue ouïghoure.
Dans cette partie, nous décrivons les difficultés existantes et nous proposerons des solutions innovantes afin de les résoudre dans les domaines suivants : Unification des polices ouïghoures et création d'une police ouïghoure basée sur l'Unicode, Implémentation des méthodes d'entrées au niveau système et au niveau navigateur, Création des convertisseurs multiécriture, Réalisation d'un dictionnaire ouïghour – anglais en ligne, Mise en place d'un générateur lexical basé sur les règles morphologiques de suffixation de l'ouïghour, Développement d'un analyseur et explorateur de suffixes, Démonstration d'extraction de l'information Implémentation d'un parseur et un correcteur d'orthographe
Les bénéfices engendrés par les études statistiques sur les données personnelles des individus sont nombreux, que ce soit dans le médical, l'énergie ou la gestion du trafic urbain pour n'en citer que quelques-uns.
Pour retrouver la confiance des individus, il devient nécessaire de proposer dessolutions de user empowerment, c'est-à-dire permettre à chaque utilisateur de contrôler les paramètres de protection des données personnelles les concernant qui sont utilisées pour des calculs.
Cette thèse développe donc un nouveau concept d'anonymisation personnalisé, basé sur la généralisation de données et sur le user empowerment.
De plus, nous utilisons une architecture décentralisée basée sur du matériel sécurisé assurant ainsi les garanties de respect de la vie privée tout au long des opérations d'agrégation.
En deuxième lieu, ce manuscrit étudie la personnalisations des garanties d'anonymat lors de la publication de jeux de données anonymisés.
Nous proposons l'adaptation d'heuristiques existantes ainsi qu'une nouvelle approche basée sur la programmation par contraintes.
Des expérimentations ont été menées pour étudier l'impact d'une telle personnalisation sur la qualité des données.
Les contraintes d'anonymat ont été construites et simulées de façon réaliste en se basant sur des résultats d'études sociologiques.
La présente thèse propose une description et une étude multidimensionnelles (sociolinguistiques, phonologiques et phonétiques) de la variété d'anglais parlée dans le Greater Manchester.
Nous offrons une discussion sur les enjeux méthodologiques et épistémologiques de l'étude du changement linguistique et de l'utilisation des corpus en linguistique.
Notre travail est mené dans le cadre du programme PAC (Phonologie de l'Anglais Contemporain : usages, variétés et structure) et au sein du projet LVTI (Langue, Ville, Travail, Identité) sur la base du corpus PAC-LVTI Manchester, constitué de données authentiques et récentes récoltées sur le terrain.
Notre analyse se concentre notamment sur le phénomène de nivellement dialectal, qui a été l'objet de nombreuses recherches récentes en sociolinguistique anglaise.
Nous nous intéressons en particulier à l'hypothèse de l'expansion d'une variété supralocale dans le nord de l'Angleterre.
Notre étude concerne essentiellement les voyelles du Greater Manchester, et repose sur une analyse phonético-acoustique de la production des locuteurs de notre corpus.
Nous relevons les caractéristiques majeures de la variété mancunienne, telles qu'elles ont pu être décrites dans les quelques travaux publiés jusqu'ici, et étudions leur corrélation avec des facteurs sociolinguistiques classiques comme l'âge, le genre, ou le niveau socio-économique.
Nous explorons également la pertinence des facteurs attitudinaux pour l'étude de nos données.
Nous discutons des évolutions du système à la lumière du phénomène de nivellement dialectal, et nous interrogeons sur la pertinence des facteurs internes et externes pour les expliquer.
Cette thèse de doctorat est le résultat de mes travaux de recherche dans le domaine de l'apprentissage automatique, du traitement d'image et du transport intelligent pour résoudre le problème du système de protection des piétons (PPS) multi-tâches comprenant non seulement la classification, la détection et le suivi des piétons, mais aussi l'action des piétons- classification et prédiction des unités, et enfin estimation du risque piéton.
De plus, notre système PPS utilise des approches originales d'apprentissage en profondeur inter-modalités.
Le but de notre travail de recherche est de développer un composant de protection des piétons intelligent basé uniquement sur un système de vision stéréo unique utilisant une architecture d'apprentissage en profondeur cross-modalité optimale afin de classer l'action piétonne actuelle, de prédire leurs prochaines actions et enfin d'estimer le piéton risque au moment de traverser pour chaque piéton.
Premièrement, nous étudions la composante de classification où nous avons analysé comment les représentations d'apprentissage d'une modalité permettraient de reconnaître d'autres modalités au sein de divers apprentissages profonds, un terme comme apprentissage multimodal.
Troisièmement, nous analysons la prédiction de l'action des piétons et l'estimation du temps à traverser.
Afin de réduire l'intervalle de temps nécessaire entre la publication de l'information sur le Web et sa consultation par les utilisateurs, les sites Web reposent sur le principe de la Syndication Web.
Les fournisseurs d'information diffusent les nouvelles informations à travers des flux RSS auxquels les utilisateurs intéressés peuvent s'abonner.
Nous proposons un index basé sur les mots-clés des requêtes utilisateurs permettant de retrouver ceux-ci dans les items des flux.
Trois structures d'indexation de souscriptions sont présentées.
Un modèle analytique pour estimer le temps de traitement et l'espace mémoire de chaque structure est détaillé.
Nous menons une étude expérimentale approfondie de l'impact de plusieurs paramètres sur ces structures.
Pour les souscriptions jamais notifiées, nous adaptons les index étudiés pour prendre en considération leur satisfaction partielle.
L'émergence de la technologie des médias digitaux durant les dernières décennies a transformé la manière dont les organisations sont évaluées.
Chaque jour, dans de multiples plateformes et sites web, des individus divulguent des informations sur leurs interactions avec des organisations.
En comparaison des critiques professionnels traditionnels, les utilisateurs et les consommateurs digitaux tendent à partager des expériences subjectives et partiales, à être moins enclins à être pondérés, et souvent à donner plus d'importance au contenu émotionnel.
Alors que davantage de consommateurs s'appuient sur cette information pour leurs choix d'achat, les entreprises dans beaucoup de secteurs se trouvent dans une position où il est difficile d'ignorer les opinions exprimées en ligne par les consommateur.
Dans cette thèse, j'étudie la manière dont les stratégies et les comportements des organisations sont influencées par cette « démocratisation » des processus d'évaluation.
Le contexte empirique de mes analyses est celui du secteur de la restauration haut-de-gamme.
Dans le premier chapitre, j'étudie les commentaires en ligne comme source d'information pour les restaurants, qui peuvent avoir l'opportunité d'apprendre des problèmes et d'améliorations potentielles.
J'examine quelles sont les caractéristiques des feedbacks des consommateurs qui ont le plus de chances d'être prises en considération par les restaurants ciblés.
A partir d'une expérimentation en ligne dans le secteur de la restauration haut-de-gamme en France, je trouve que les preneurs de décision allouent leur attention aux feedbacks desquels on attend qu'ils aient le plus fort effet sur la réputation et la performance du restaurant.
Dans le deuxième chapitre, j'analyse les effets de l'interaction entre les évaluations des amateurs et des experts.
En particulier, j'étude l'entrée d'un évaluateur expert (i.e. le guide Michelin) sur le marché, et la manière dont il pousse certaines organisations à faire des choix stratégiques qui signalent leurs aspirations.
En construisant sur la littérature sur le statut organisationnel, je trouve que certains restaurants mieux évalués par le guide Michelin font des changements dans leur offre en visant à s'auto-identifier avec le groupe d'élite.
Ces changements consistent à adopter ou à exclure certaines caractéristiques affichées dans les menus.
De plus, en utilisant les techniques du « topic modeling » appliquées à des commentaires sur Yelp, j'observe que certaines réactions des consommateurs à propos de l'entrée du guide Michelin font que les restaurants apparaissent plus ou moins sensibles aux évaluations de l'expert.
Dans le troisième chapitre, je me concentre sur la manière dont les organisations utilisent des réponses publiques adressées aux consommateurs pour répondre aux critiques en ligne.
Les études récentes n'offrent pas de conclusion nette sur les bénéfices en termes de réputation des réponses publics aux commentaires.
Ces réponses peuvent réduire la probabilité de recevoir des commentaires négatifs dans le futur mais, dans le même temps, elles attirent l'attention sur les problèmes en question.
M'appuyant sur la littérature existante sur la réputation et sur l' « impression management » , je propose que les organisations peuvent résoudre cet arbitrage en utilisant stratégiquement les différents types de réponses verbales (ex : l'excuse).
Bien que les réponses publiques adressées aux consommateurs puissent être contre-productives, adapter le style des réponses publiques aux caractéristiques des commentaires des consommateurs peut être une stratégie optimale pour les organisations.
Dans cette étude, j'analyse les commentaires pour des restaurants situées en France et aux États-Unis en utilisant des modèles économétriques standards appuyés sur des techniques de « supervised learning » .
Cette étude de cas relève de la didactique des langues, et plus particulièrement de l'intégration des outils numériques à la formation pré-professionnelle à la didactique de l'anglais.
Elle a pour objectif de décrire et d'analyser les différents processus qui participent à l'émergence de la dimension sociale de l'autonomie.
Elle repose sur l'idée que ces processus viennent au premier plan lors de la réalisation d'une activité collective, située et médiatisée.
Afin d'étudier ces processus, une tâche-projet basée sur la réalisation de contes multimodaux a été proposée aux apprenant.e.s d'une formation pré-professionnelle au métier d'enseignement de la langue anglaise.
Les supports conçus par les apprenant.e.s de première année de licence ont été ensuite testés comme ressources pédagogiques par des professeur.e.s d'écoles primaires dans le but de conférer une dimension plus réelle, plus située, à la production des contes.
Une modélisation systémique de l'activité permet d'identifier trois axes qui contribuent à l'émergence de l'autonomie sociale.
Le premier est relatif au fonctionnement intra groupal des groupes restreints ayant accompli la tâche-projet.
Le deuxième porte sur la contribution de la communauté des professeur.e.s des écoles primaires qui ont testé les supports.
Le troisième interroge le rapport aux outils numériques sociaux utilisés par les groupes restreints pour collaborer et échanger ensemble.
Les résultats issus des analyses relatives aux trois axes visent à réaliser une modélisation de certaines variables participant à une émergence de l'autonomie sociale.
Les corpus bilingues sont des ressources essentielles pour s'affranchir de la barrière de la langue en traitement automatique des langues (TAL) dans un contexte multilingue.
La plupart des travaux actuels utilisent des corpus parallèles qui sont surtout disponibles pour des langues majeurs et pour des domaines spécifiques.
Les corpus comparables, qui rassemblent des textes comportant des informations corrélées, sont cependant moins coûteux à obtenir en grande quantité.
Plusieurs travaux antérieurs ont montré que l'utilisation des corpus comparables est bénéfique à différentes taches en TAL.
En parallèle à ces travaux, nous proposons dans cette thèse d'améliorer la qualité des corpus comparables dans le but d'améliorer les performances des applications qui les exploitent.
L'idée est avantageuse puisqu'elle peut être utilisée avec n'importe quelle méthode existante reposant sur des corpus comparables.
Nous discuterons en premier la notion de comparabilité inspirée des expériences d'utilisation des corpus bilingues.
Cette notion motive plusieurs implémentations de la mesure de comparabilité dans un cadre probabiliste, ainsi qu'une méthodologie pour évaluer la capacité des mesures de comparabilité à capturer un haut niveau de comparabilité.
Les mesures de comparabilité sont aussi examinées en termes de robustesse aux changements des entrées du dictionnaire.
Les expériences montrent qu'une mesure symétrique s'appuyant sur l'entrelacement du vocabulaire peut être corrélée avec un haut niveau de comparabilité et est robuste aux changements des entrées du dictionnaire.
En s'appuyant sur cette mesure de comparabilité, deux méthodes nommées : greedy approach et clustering approach, sont alors développées afin d'améliorer la qualité d'un corpus comparable donnée.
L'idée générale de ces deux méthodes est de choisir une sous partie du corpus original qui soit de haute qualité, et d'enrichir la sous-partie de qualité moindre avec des ressources externes.
Les expériences montrent que l'on peut améliorer avec ces deux méthodes la qualité en termes de score de comparabilité d'un corpus comparable donnée, avec la méthode clustering approach qui est plus efficace que la method greedy approach.
Le corpus comparable ainsi obtenu, permet d'augmenter la qualité des lexiques bilingues en utilisant l'algorithme d'extraction standard.
Enfin, nous nous penchons sur la tâche d'extraction d'information interlingue (Cross-Language Information Retrieval, CLIR) et l'application des corpus comparables à cette tâche.
Nous développons de nouveaux modèles CLIR en étendant les récents modèles proposés en recherche d'information monolingue.
Le modèle CLIR montre de meilleurs performances globales.
Les lexiques bilingues extraits à partir des corpus comparables sont alors combinés avec le dictionnaire bilingue existant, est utilisé dans les expériences CLIR, ce qui induit une amélioration significative des systèmes CLIR.
Nous visons la constitution d'un cadre théorique et opérationnel général, permettant la modélisation et l'exploration computationnelle d'une variété significative de telles structures.
Nous proposons notamment d'articuler leur analyse autour des trois catégories élémentaires que sont unités, relations et schémas, et envisageons différentes propriétés récurrentes des structures et des mécanismes indiciaires sous-jacents : variabilité du grain, flexibilité, non-linéarité et non-séquentialité potentielles, interactions local/global...
Afin de procéder à la description formelle des phénomènes linguistiques étudiés et à l'opérationalisation de leur analyse sur corpus, nous proposons le formalisme CDML (Contraint-based Discourse Modeling Language), qui permet de modéliser des structures discursives par l'expression de contraintes sur des objets textuels de différentes natures (morphologique, syntaxique, sémantique...
Un analyseur permet de projeter ces contraintes sur corpus pour identifier les structures décrites.
La première porte sur l'hypothèse de l'encadrement du discours de M. Charolles, et la seconde explore les relations de contraste à différentes échelles, entre des objets linguistiques variés.
La présente thèse porte sur l'utilisation et l'interprétation des sujets nuls et pronominaux en portugais brésilien.
Son objectif est de comprendre les facteurs sémantiques et discursifs qui peuvent être pertinents pour le choix entre ces expressions anaphoriques et la façon dont ce choix s'articule avec la théorie générale de la résolution de l'anaphore.
Le point de départ a été la recherche sur les sujets nuls et réalisés sous la perspective de la grammaire générative, en particulier la théorie paramétrique.
Cette thèse démontre que l'analyse proposée dans cette perspective ne peut rendre compte des données observées.
Par exemple, la généralisation sur la « pauvreté » de la morphologie verbale directement liée à l'absence, ou à la fréquence réduite, de sujets nuls est contestée avec les données expérimentales ainsi qu'avec la distribution de la fréquence relative des sujets nuls au sein des personnes discursives dans le corpus.
Une explication alternative présentée dans la littérature, à savoir l'importance des caractéristiques sémantiques des antécédents – l'Animacité et le Specificité – semble mieux expliquer la distribution constatée.
Cette explication n'est cependant pas suffisante pour comprendre le choix des sujets anaphoriques en brésilien, puisque le nombre relatif de sujets nuls animés et spécifiques est relativement plus élevé que dans les langues à expression obligatoire des sujets.
Par conséquent, cette thèse soutient que les facteurs discursifs semblent jouer un rôle crucial dans l'utilisation des sujets nuls et réalisés en brésilien.
Le premier est un facteur standard dans la littérature sur la résolution de l'anaphore (exprimé par différents termes comme l'accessibilité, la familiarité, etc.), qui permet l'hypothèse d'une relation inverse entre le degré de saillance de l'antécédent et degré explicitation nécessaire dans l'expression anaphorique : plus l'antécédent est saillant, moins l'anaphore doit être explicite.
Le second facteur, le contraste, constitue la principale contribution nouvelle de cette thèse : Comme pour d'autres niveaux d'analyse linguistique et d'autres phénomènes dans le langage, le choix de l'expression anaphorique semble être orienté vers l'efficacité.
Plus précisément, lorsque l'information d'arrière-plan (background) et l'information assertée (focalisée) dans un énoncé contrastent, il est plus probable qu'un sujet nul soit utilisé.
Les caractéristiques d'une grammaire permettant de traiter ces diverses caractéristiques est esquissée : on propose une grammaire à plusieurs niveaux dont les contraintes sémantiques et discursives agissent en parallèle à travers un principe de correspondance probabiliste.
Il est ainsi démontré que les sujets nuls sont probables dans certains contextes de co-référence discursifs, puisque dans ces contextes, leurs antécédents sont plus évidents et contrastent plus avec l'information d'arrière-plan.
Une contre-preuve apparente à la proposition esquissée ici est analysée : l'interprétation générique des sujets nuls.
Cependant, on montre que les mêmes contraintes sémantiques appliquées à d'autres constructions génériques dans plusieurs langues peuvent produire des sujets nuls génériques en brésilien, étant donné l'échec de la mise en arrière-plan prédite par l'approche proposée ici.
Enfin, les résultats de trois expériences de mouvements oculaires en lecture, qui étudient l'utilisation et l'interprétation des sujets nuls et pronominaux, sont présentés.
Ces résultats corroborent de façon convaincante l'hypothèse selon laquelle les sujets nuls et réalisés ainsi que leur interprétation peuvent être expliqués par la théorie proposée ici, qui les traite en termes de contraintes d'interprétation plutôt qu'en termes de légitimation syntaxique.
Le figement est un obstacle au traitement automatique des langues naturelles.
Ce travail est une contribution aux recherches en cours et vise à améliorer le traitement automatique des séquences verbales figées (SVF) comme casser sa pipe.
Les notions de degrés de figement et de double structuration guideront notre étude.
Pour déterminer le degré de figement des SVF, nous avons analysé leurs structures interne et externe (en tant que prédicats).
Ceci permet alors de les inclure dans les classes de prédicats du français selon la théorie des classes d'objets de G. GROSS.
Après une présentation des travaux portant sur le figement (verbal), nous identifierons et relèverons les critères de mesure des degrés de figement des SVF par l'étude des séquences à structure [V SN SP].
Puis, nous proposerons des outils formels de reconnaissance automatique de la métaphore.
Enfin, nous décrirons deux classes syntactico-sémantiques de prédicats polylexicaux : celles de <déplacement> et d'<états humains>.
La thèse débute par un rappel des étapes historiques principales du développement de l'ethnométhodologie en tant que discipline, depuis les précurseurs européens des années 30 jusqu'à l'explosion aux Etats Unis puis en Europe à partir de 1967.
La seconde partie de la thèse est consacrée à l'application des principes développés antérieurement au domaine des stratégies d'innovations technologiques mises en oeuvre en France en vue d'accroître le potentiel de recherche et développement dans le secteur du traitement automatique des langues naturelles.
Trois études décrivent successivement les ethnométhodes et les propriétés rationnelles des actions pratiques mises en oeuvre par un groupe de chargés de mission de l'administration, les processus d'élaboration d'une politique d'innovation technologique, les descriptions indexicales du domaine des industries de la langue et de programmes de r et d dans ce secteur.
La conclusion s'efforce de montrer comment la puissance des concepts de l'ethnométhodologie et des outils qui en découlent permettent d'accroître la pertinence des analyses stratégiques et l'efficacité des actions de recherche développement
L'apprentissage collaboratif assisté par ordinateur et les technologies d'e-learning devenant de plus en plus populaires et intégrés dans des contextes éducatifs, le besoin se fait sentir de disposer d'outils d'évaluation automatique et d'aide aux enseignants ou tuteurs pour les deux activités, fortement couplées, de compréhension de textes et collaboration entre pairs.
Bien qu'une analyse de surface de ces activités est aisément réalisable, une compréhension plus profonde et complète du discours en jeu est nécessaire, complétée par une analyse de l'information méta-cognitive disponible par diverses sources, comme par exemples les auto-explications des apprenants.
Plus spécifiquement, nous nous sommes centrés sur la dimension individuelle de l'apprentissage, analysée à partir de l'identification de stratégies de lecture et sur la mise au jour d'un modèle de la complexité textuelle intégrant des facteurs de surface, lexicaux, morphologiques, syntaxiques et sémantiques.
En complément, la dimension collaborative de l'apprentissage est centrée sur l'évaluation de l'implication des participants, ainsi que sur l'évaluation de leur collaboration par deux modèles computationnels : un modèle polyphonique, défini comme l'inter-animation de voix selon de multiples perspectives, un modèle spécifique de construction sociale de connaissances, fondé sur le graphe de cohésion et un mécanisme d'évaluation des tours de parole.
Notre approche met en œuvre des techniques avancées de traitement automatique de la langue et a pour but de formaliser une évaluation qualitative du processus d'apprentissage.
Des validations cognitives de nos différents systèmes d'évaluation automatique ont été réalisées, et nous avons conçu des scénarios d'utilisation de ReaderBench, notre système le plus avancé, dans différents contextes d'enseignement.
L'un des buts principaux de notre modèle est de favoriser la compréhension vue en tant que « médiatrice de l'apprentissage » , en procurant des rétroactions automatiques aux apprenants et enseignants ou tuteurs.
Leur avantage est triple : leur flexibilité, leur extensibilité et, cependant, leur spécificité, car ils couvrent de multiples étapes de l'activité d'apprentissage, de la lecture de matériel d'apprentissage à l'écriture de synthèses de cours en passant par la discussion collaborative de contenus de cours et la verbalisation métacognitive de jugements de compréhension, afin d'obtenir une perspective complète du niveau de compréhension et de générer des rétroactions appropriées sur le processus d'apprentissage collaboratif.
Cette thèse se place à la croisée du traitement automatique du langage, de l'extraction d'information, de la recherche d'information et des sciences cognitives, dans un contexte de géomatique.
Dans ce contexte, nos travaux visent à traiter automatiquement des textes relatant des récits de voyage, avec pour objectif d'interpréter l'information géographique qu'ils contiennent et plus particulièrement les itinéraires qu'ils décrivent.
L'approche que nous proposons est une méthode incrémentale de découverte du sens : partant d'une sémantique du niveau du syntagme nominal simple, et passant par une sémantique du niveau de la proposition, nous atteignons une sémantique du niveau du texte.
Notre contribution réside donc dans la proposition d'un modèle du concept d'itinéraire adapté à des usages ciblés (pour la RI et pour des usages de conception d'activités pédagogiques), la proposition d'une approche pour caractériser l'itinéraire dans le texte, et enfin la proposition d'un mode opératoire automatisé et entièrement implémenté permettant de relier la caractérisation de l'itinéraire dans le texte à son modèle du concept.
Notre démarche étant clairement expérimentale, nous nous sommes appuyé tout au long de nos travaux sur un prototype que nous avons mis au point.
Ce prototype (PIIR pour Prototype pour l'Interprétation d'Itinéraires dans des Récits) nous permet de mettre face au traitement automatique les analyses et les algorithmes que nous proposons.
Depuis les années 90, Internet est au coeur du marché du travail.
D'abord mobilisée sur des métiers spécifiques, son utilisation s'étend à mesure qu'augmente le nombre d'internautes dans la population.
La recherche d'emploi au travers des « bourses à l'emploi électroniques » est devenu une banalité et le e-recrutement quelque chose de courant.
Cette explosion d'informations pose cependant divers problèmes dans leur traitement en raison de la grande quantité d'information difficile à gérer rapidement et efficacement pour les entreprises.
Nous présentons dans ce mémoire, les travaux que nous avons développés dans le cadre du projet E-Gen, qui a pour but la création d'outils pour automatiser les flux d'informations lors d'un processus de recrutement.
Nous nous intéressons en premier lieu à la problématique posée par le routage précis de courriels.
La capacité d'une entreprise à gérer efficacement et à moindre coût ces flux d'informations, devient un enjeu majeur de nos jours pour la satisfaction des clients.
Nous présentons par la suite les travaux qui ont été menés dans le cadre de l'analyse et l'intégration d'une offre d'emploi par Internet.
Le temps étant un facteur déterminant dans ce domaine, nous présentons une solution capable d'intégrer une offre d'emploi d'une manière automatique ou assistée afin de pouvoir la diffuser rapidement.
Basé sur une combinaison de systèmes de classifieurs pilotés par un automate de Markov, le système obtient de très bons résultats.
Nous proposons également les diverses stratégies que nous avons mises en place afin de fournir une première évaluation automatisée des candidatures permettant d'assister les recruteurs.
Nous avons évalué une palette de mesures de similarité afin d'effectuer un classement pertinent des candidatures.
L'utilisation d'un modèle de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.
Nos travaux décrits dans cette thèse portent sur l'apprentissage d'une représentation pour la classification automatique basée sur la découverte de motifs à partir de séries temporelles.
L'information pertinente contenue dans une série temporelle peut être encodée temporellement sous forme de tendances, de formes ou de sous-séquences contenant habituellement des distorsions.
Des approches ont été développées pour résoudre ces problèmes souvent au prix d'une importante complexité calculatoire.
Parmi ces techniques nous pouvons citer les mesures de distance et les représentations de l'information contenue dans les séries temporelles.
Nous nous concentrons sur la représentation de l'information contenue dans les séries temporelles.
Le framework proposé transforme un ensemble de séries temporelles en un espace d'attributs (feature space) à partir de sous-séquences énumérées des séries temporelles, de mesures de distance et de fonctions d'agrégation.
Un cas particulier de ce framework est la méthode notoire des « shapelets » .
L'inconvénient potentiel d'une telle approache est le nombre très important de sous-séquences à énumérer en ce qu'il induit un très grand feature space, accompagné d'une très grande complexité calculatoire.
Nous montrons que la plupart des sous-séquences présentes dans un jeu de données composé de séries temporelles sont redondantes.
De ce fait, un sous-échantillonnage aléatoire peut être utilisé pour générer un petit sous-ensemble de sous-séquences parmi l'ensemble exhaustif, en préservant l'information nécessaire pour la classification et tout en produisant un feature space de taille compatible avec l'utilisation d'algorithmes d'apprentissage automatique de l'état de l'art avec des temps de calculs raisonnable.
On démontre également que le nombre de sous-séquences à tirer n'est pas lié avec le nombre de séries temporelles présent dans l'ensemble d'apprentissage, ce qui garantit le passage à l'échelle de notre approche.
La combinaison de cette découverte dans le contexte de notre framework nous permet de profiter de techniques avancées (telles que des méthodes de sélection d'attributs multivariées) pour découvrir une représentation de séries temporelles plus riche, en prenant par exemple en considération les relations entre sous-séquences.
Ces résultats théoriques ont été largement testés expérimentalement sur une centaine de jeux de données classiques de la littérature, composés de séries temporelles univariées et multivariées.
De plus, nos recherches s'inscrivant dans le cadre d'une convention de recherche industrielle (CIFRE) avec Arcelormittal, nos travaux ont été appliqués à la détection de produits d'acier défectueux à partir des mesures effectuées par les capteurs sur des lignes de production.
Ce travail se penche sur les difficultés auxquelles font face les apprenants nigérians de français, précisément ceux de langue maternelle yoruba, quant à ce qui concerne les temps passés du français : l'imparfait et le passé composé.
Nous sommes parti des productions des apprenants, deux exercices à trous et un écrit long de type rédaction, pour exposer les erreurs de temps commises.
Nous avons découvert, suite à l'analyse des productions, que la plupart de ces erreurs proviennent du système aspectuo-temporel du yoruba, langue ne connaissant pas de conjugaison (désinences verbales) comme le français.
D'autre part, l'analyse des deux tests à trous en plus de celle des copies de rédaction montrent que, le manque de connaissance de certaines notions linguistiques est une autre cause des difficultés rencontrées par les apprenants : la notion de discours / récit et celle de premier / arrière
En somme, nous pensons qu'un enseignement / apprentissage des temps basé sur la notion d'aspect grammatical, et prenant en compte les notions précédemment mentionnées, sera certainement plus productif.
Nos propositions de pistes pour un meilleur enseignement/apprentissage des temps concernés terminent cette recherche.
Nous pensons, par ailleurs, qu'en ajoutant à ce que nous venons de dire, les détails que nous ont révélés les analyses linguistiques des systèmes aspectuo-temporels des deux langues, nous pourrons construire par la suite une méthode d'enseignement et apprentissage des temps verbaux du passé pour l'apprenant nigérian.
De multiples problèmes en apprentissage automatique consistent à minimiser une fonction lisse sur un espace euclidien.
Pour l'apprentissage supervisé, cela inclut les régressions par moindres carrés et logistique.
Dans ce manuscrit, nous considérons le cas particulier de la perte quadratique.
Dans une première partie, nous nous proposons de la minimiser grâce à un oracle stochastique.
Dans une seconde partie, nous considérons deux de ses applications à l'apprentissage automatique : au partitionnement de données et à l'estimation sous contrainte de forme.
Ce nouveau cadre suggère un algorithme alternatif qui combine les aspects positifs du moyennage et de l'accélération.
La deuxième contribution est d'obtenir le taux optimal d'erreur de prédiction pour la régression par moindres carrés en fonction de la dépendance au bruit du problème et à l'oubli des conditions initiales.
Notre nouvel algorithme est issu de la descente de gradient accélérée et moyennée.
La troisième contribution traite de la minimisation de fonctions composites, somme de l'espérance de fonctions quadratiques et d'une régularisation convexe.
Nous étendons les résultats existants pour les moindres carrés à toute régularisation et aux différentes géométries induites par une divergence de Bregman.
Dans une quatrième contribution, nous considérons le problème du partitionnement discriminatif.
Nous proposons sa première analyse théorique, une extension parcimonieuse, son extension au cas multi-labels et un nouvel algorithme ayant une meilleure complexité que les méthodes existantes.
La dernière contribution de cette thèse considère le problème de la sériation.
Nous adoptons une approche statistique où la matrice est observée avec du bruit et nous étudions les taux d'estimation minimax.
Le domaine de l'intelligence artificielle (IA) a connu un essor sans précédent ces dernières années avec des succès spectaculaires très médiatisés.
En réalité, l'IA est utilisée dans de nombreux domaines allant de la vision par ordinateur au traitement automatique des langues.
Parmi les nombreuses techniques de l'intelligence artificielle, l'apprentissage profond à base de réseaux de neurones a montré de très bonnes capacités d'apprentissage et des bonnes performances dans de nombreux domaines.
La conception et le développement de tels réseau est une tâche ardue qui nécessitent des connaissances pointues dans les architectures parallèles modernes afin de pouvoir utiliser au mieux la puissance de calcul de telles machines.
Cet enjeu est d'autant plus important au vu du développement massif de l'IA aussi bien dans le monde académique qu'industriel.
L'objectif de la thèse est de définir un environnement de développement de réseaux de neurones pour l'apprentissage profond capable de prendre en compte les évolutions actuelles des architectures de ces réseaux.
Cet environnement devra comprendre un langage spécifique de domaine permettant à l'utilisateur de décrire l'architecture de son système sans se préoccuper de la manière dont celui-ci sera déployé.
Ce langage de haut-niveau devra reposer sur les principes du parallélisme implicite qui fournit à l'utilisateur des motifs (patterns ou squelettes) permettant de décrire les éléments du réseau de neurones et leur assemblage de manière simple tout en donnant des indications qui permettront de définir une stratégie de parallélisation efficace.
L'environnement devra ensuite être capables de dériver à partir de la description de haut niveau, un programme et un déploiement efficaces du réseau de neurones sur l'architecture cible.
Pour arriver à cette fin il va falloir détecter et décrire les squelettes de programmation pertinents pour les concepteurs de réseaux de neurones et trouver des stratégies de parallélisation efficace pour chacun de ces squelettes.
Par ailleurs des outils d'analyse du réseau d'apprentissage profond permettant de détecter des optimisations en termes de déploiement et de communications seront aussi essentiels pour obtenir des systèmes efficaces.
Les résultats auront pour objectif d'intégrer l'environnement MindSpore de Huawei et de contribuer aux produits et solutions IA de Huawei, afin d'explorer toute la puissance potentielle de sa « Compute Architecture for Neural Networks » (CANN)
Les récents progrès dans la numérisation des collections de documents patrimoniaux ont ravivé de nouveaux défis afin de garantir une conservation durable et de fournir un accès plus large aux documents anciens.
Les efforts se concentrent autant sur le développement d'outils rapides et automatiques de caractérisation et catégorisation des pages d'ouvrages anciens, capables de classer les pages d'un ouvrage numérisé en fonction de plusieurs critères, notamment la structure des mises en page et/ou les caractéristiques typographiques/graphiques du contenu de ces pages.
Ainsi, dans le cadre de cette thèse, nous proposons une approche permettant la caractérisation et la catégorisation automatiques des pages d'un ouvrage ancien.
L'approche proposée se veut indépendante de la structure et du contenu de l'ouvrage analysé.
Le principal avantage de ce travail réside dans le fait que l'approche s'affranchit des connaissances préalables, que ce soit concernant le contenu du document ou sa structure.
Elle est basée sur une analyse des descripteurs de texture et une représentation structurelle en graphe afin de fournir une description riche permettant une catégorisation à partir du contenu graphique (capturé par la texture) et des mises en page (représentées par des graphes).
En effet, cette catégorisation s'appuie sur la caractérisation du contenu de la page numérisée à l'aide d'une analyse des descripteurs de texture, de forme, géométriques et topologiques.
Cette caractérisation est définie à l'aide d'une représentation structurelle.
Dans le détail, l'approche de catégorisation se décompose en deux étapes principales successives.
La première consiste à extraire des régions homogènes.
La seconde vise à proposer une signature structurelle à base de texture, sous la forme d'un graphe, construite à partir des régions homogènes extraites et reflétant la structure de la page analysée.
Cette signature assure la mise en œuvre de nombreuses applications pour gérer efficacement un corpus ou des collections de livres patrimoniaux (par exemple, la recherche d'information dans les bibliothèques numériques en fonction de plusieurs critères, ou la catégorisation des pages d'un même ouvrage).
En comparant les différentes signatures structurelles par le biais de la distance d'édition entre graphes, les similitudes entre les pages d'un même ouvrage en termes de leurs mises en page et/ou contenus peuvent être déduites.
Ainsi de suite, les pages ayant des mises en page et/ou contenus similaires peuvent être catégorisées, et un résumé/une table des matières de l'ouvrage analysé peut être alors généré automatiquement.
Pour illustrer l'efficacité de la signature proposée, une étude expérimentale détaillée a été menée dans ce travail pour évaluer deux applications possibles de catégorisation de pages d'un même ouvrage, la classification non supervisée de pages et la segmentation de flux de pages d'un même ouvrage.
En outre, les différentes étapes de l'approche proposée ont donné lieu à des évaluations par le biais d'expérimentations menées sur un large corpus de documents patrimoniaux.
Le travail de recherche présenté dans cette thèse s'inscrit dans le cadre général des travaux sur les humanités numériques qui cherchent, entre autres, à contribuer à l'amélioration des interactions Homme-Machine.
L'objectif de l'étude est double.
Dans un premier temps, il s'agit d'étudier un corpus de décisions de justice contenues dans la base de données de l'Institut du Droit International des Transports (IDIT) afin de déterminer les contraintes linguistiques du genre judiciaire.
Dans un second temps, il est question de proposer des parcours interprétatifs pouvant aider les utilisateurs dans leur accès à l'information juridique recherchée.
La problématique de l'aide à l'interprétation est appréhendée à travers l'étude des modalités et des scénarios modaux.
Le parti pris de cette recherche est de considérer la pluridisciplinarité comme un atout théorique et méthodologique qui contribue à mieux éclairer un objet d'étude.
De ce fait, plusieurs approches (sémantique des modalités, sémantique textuelle, argumentation rhétorique, textométrie) sont convoquées et articulées pour œuvrer ensemble vers les objectifs fixés.
L'analyse du corpus a été menée à deux niveaux et selon deux approches.
Dans la première partie, l'analyse empirique proposée est quantitative et contrastive.
Elle est menée au niveau micro et mésotextuelle dans la mesure où elle se focalise sur l'étude du lexique.
Aidée de l'outil TXM, cette première investigation a permis une caractérisation linguistique globale du corpus et un premier aperçu de son profil modal.
Elle a également mis en exergue des expressions modales, constructions concessives, routines discursives, etc. qui focalisent sur des moments clés dans le déroulement argumentatif et pouvant donc servir dans le cadre de l'aide à l'interprétation.
Dans la seconde partie, l'étude empirique porte sur des analyses modales menées sur des textes complets.
Elle est donc abordée dans une approche qualitative et au niveau macrotextuelle.
Cette analyse aboutit à la formulation d'un modèle de scénario minutieusement décrit.
Il est décomposable en plusieurs niveaux, selon les modalités qui l'ont construit (modalités de premier plan, modalité d'arrière-plan) et selon qu'il caractérise un texte complet ou une zone spécifique de ce texte.
Par ailleurs, la présentation schématique proposée pour les scénarios a mis en évidence le rôle que représenterait chaque zone modale dans la perspective d'une aide à l'interprétation.
La première partie de cette thèse présente une méthode d'assistance à la transcription automatique de la parole.
Le transcripteur humain dispose de la meilleure hypothèse fournie par le SRAP, et, à chaque correction de sa part, le système propose une nouvelle hypothèse prenant en compte cette correction.
Cette dernière est obtenue à partir d'une réévaluation des réseaux de confusion générés par le SRAP.
Afin de diminuer le taux d'erreur sur les noms propres, une méthode de phonétisation itérative utilisant les données acoustiques à disposition est proposée dans ce manuscrit.
L'utilisation de SMT [Laurent 2009] couplée avec la méthode de phonétisation proposée permet d'observer des gains en terme de taux d'erreur mot (WER) et en terme de taux d'erreur noms propres (PNER).
La linguistique informatique a pour objet de construire un modèle formel des connaissances linguistiques, et d'en tirer des algorithmes permettant le traitement automatique des langues.
Pour ce faire, elle s'appuie fréquemment sur des grammaires dites génératives, construisant des phrases valides par l'application successive de règles de réécriture.
Une approche alternative, basée sur la théorie des modèles, vise à décrire la grammaticalité comme une conjonction de contraintes de bonne formation, en s'appuyant sur des liens profonds entre logique et automates pour produire des analyseurs efficaces.
Notre travail se situe dans ce dernier cadre.
En s'appuyant sur plusieurs résultats existants en informatique théorique, nous proposons un outil de modélisation linguistique expressif, conçu pour faciliter l'ingénierie grammaticale.
Celui-ci considère dans un premier temps la structure abstraite des énoncés, et fournit un langage logique s'appuyant sur les propriétés lexicales des mots pour caractériser avec concision l'ensemble des phrases grammaticalement correctes.
Puis, dans un second temps, le lien entre ces structures abstraites et leurs représentations concrètes (en syntaxe et en sémantique) est établi par le biais de règles de linéarisation qui exploitent la logique et le lambda-calcul.
Par suite, afin de valider cette approche, nous proposons un ensemble de modélisations portant sur des phénomènes linguistiques divers, avec un intérêt particulier pour le traitement des langages présentant des phénomènes d'ordre libre (c'est-à-dire qui autorisent la permutation de certains mots ou groupes de mots dans une phrase sans affecter sa signification), ainsi que pour leur complexité algorithmique.
Cette thèse traite de l'extraction d'unités lexicales en chinois contemporain à partir d'un corpus de textes de spécialité.
Elle aborde la tâche d'extraction de lexique en chinois en utilisant des techniques se basant sur des caractéristiques linguistiques de la langue chinoise.
La thèse traite également de la manière d'évaluer l'extraction de lexique dans un environnement industriel.
La première partie de la thèse est consacrée à la description du contexte de l'étude.
Nous nous attachons dans un premier à temps à décrire les concepts linguistiques d'unité lexicale et de lexique, et nous donnons une description du processus de construction des unités lexicales en chinois contemporain.
Nous faisons ensuite un inventaire des différentes techniques utilisées par la communauté scientifique pour traiter la tâche de l'extraction de lexique en chinois contemporain.
Nous concluons cette partie par une description des pratiques d'extraction de lexique en milieu industriel, et nous proposons une formalisation des critères utilisés par les terminographes d'entreprise pour sélectionner les unités lexicales pertinentes.
La deuxième partie du mémoire porte sur la description d'une méthode d'extraction de lexique en chinois contemporain et sur son évaluation.
Nous introduisons une nouvelle méthode numérique non supervisée s'appuyant sur des caractéristiques structurelles de l'unité lexicale en chinois et sur des particularités syntaxiques du chinois.
La méthode comporte un module optionnel permettant une interaction avec un opérateur (i. E. Semi-automatique).
Dans la section consacrée à l'évaluation, nous évaluons d'abord le potentiel de la méthode en comparant les résultats de l'extraction avec un standard de référence et une méthode de référence.
Nous mettons ensuite en oeuvre une évaluation plus pragmatique de la méthode en mesurant les gains apportés par l'usage de la méthode en comparaison avec l'extraction manuelle de lexique par des terminographes.
Les résultats obtenus par notre méthode sont de bonne qualité et sont meilleurs
L'évaluation pragmatique montre que la méthode n'améliore pas significativement la productivité des terminographes, mais permet d'extraire des unités lexicales différentes de celles obtenue manuellement.
En biomedicine, le domaine du « Big Data » (l'infobésité) pose le problème de l'analyse de gros volumes de données hétérogènes (i.e. vidéo, audio, texte, image).
Les ontologies biomédicales, modèle conceptuel de la réalité, peuvent jouer un rôle important afin d'automatiser le traitement des données, les requêtes et la mise en correspondance des données hétérogènes.
Dans un premier temps, les ontologies ont été construites manuellement.
Au cours de ces dernières années, quelques méthodes semi-automatiques ont été proposées.
Ces techniques semi-automatiques de construction/enrichissement d'ontologies sont principalement induites à partir de textes en utilisant des techniques du traitement du langage naturel (TALN).
Les méthodes de TALN permettent de prendre en compte la complexité lexicale et sémantique des données biomédicales : (1) lexicale pour faire référence aux syntagmes biomédicaux complexes à considérer et (2) sémantique pour traiter l'induction du concept et du contexte de la terminologie.
Dans cette thèse, afin de relever les défis mentionnés précédemment, nous proposons des méthodologies pour l'enrichissement/la construction d'ontologies biomédicales fondées sur deux principales contributions.
La première contribution est liée à l'extraction automatique de termes biomédicaux spécialisés (complexité lexicale) à partir de corpus.
De nouvelles mesures d'extraction et de classement de termes composés d'un ou plusieurs mots ont été proposées et évaluées.
L'application BioTex implémente les mesures définies.
La seconde contribution concerne l'extraction de concepts et le lien sémantique de la terminologie extraite (complexité sémantique).
Les évaluations, quantitatives et qualitatives, menées par des experts et non experts, sur des données réelles soulignent l'intérêt de ces contributions.
Cependant, ces améliorations multiplient la quantité de données à traiter avant transmission du signal par un facteur 8.
En plus de ce nouveau format, les fournisseurs de contenu doivent aussi encoder les vidéos dans des formats et à des débits différents du fait de la grande variété des systèmes et réseaux utilisés par les consommateurs.
Ensuite, dans un second lieu, le design d'une architecture scalable à deux couches, plus conventionnelle, est étudié.
Celle-ci est composée d'un encodeur HEVC standard dans la couche de base pour assurer la compatibilité avec les systèmes existants.
Un algorithme faisant varier la fréquence image est d'abord proposé.
Cet algorithme est capable de détecter localement et de façon dynamique la fréquence image la plus basse n'introduisant pas d'artefacts visibles liés au mouvement.
Les algorithmes de fréquence image variable et de résolution spatiale adaptative sont ensuite combinés afin d'offrir un codage scalable à faible complexité des contenus 4KHFR.
La qualité de l'information géographique volontaire est actuellement un sujet qui questionne autant les consommateurs de données géographiques que les producteurs de données d'autorité voulant exploiter les bienfaits de la démarche collaborative.
Ce phénomène est un travers de la démarche collaborative, et bien qu'il ne concerne qu'une faible portion des contributions, il peut constituer un obstacle à l'utilisation des données cartographiques participatives.
Dans une démarche de qualification de l'information géographique volontaire, ce travail de thèse a plus précisément pour objectif de détecter le vandalisme dans les données collaboratives cartographiques.
Dans un premier temps, il s'agit de formaliser une définition du concept de carto
Enfin, nos expériences explorent la capacité des méthodes d'apprentissage machine (machine learning) à détecter le carto
Cette thèse porte sur l'intégration de ressources lexicales et syntaxiques du français dans deux tâches fondamentales du Traitement Automatique des Langues [TAL] que sont l'étiquetage morpho-syntaxique probabiliste et l'analyse syntaxique probabiliste.
Grâce à des algorithmes d'analyse syntaxique de plus en plus évolués, les performances actuelles des analyseurs sont de plus en plus élevées, et ce pour de nombreuses langues dont le français.
Cependant, il existe plusieurs problèmes inhérents aux formalismes mathématiques permettant de modéliser statistiquement cette tâche (grammaire, modèles discriminants,...).
La dispersion des données est l'un de ces problèmes, et est causée principalement par la faible taille des corpus annotés disponibles pour la langue.
De plus, il est prouvé que la dispersion est en partie un problème lexical, car plus la flexion d'une langue est importante, moins les phénomènes lexicaux sont représentés dans les corpus annotés.
Notre première problématique repose donc sur l'atténuation de l'effet négatif de la dispersion lexicale des données sur les performances des analyseurs.
Dans cette optique, nous nous sommes intéressé à une méthode appelée regroupement lexical, et qui consiste à regrouper les mots du corpus et des textes en classes.
Ces classes réduisent le nombre de mots inconnus et donc le nombre de phénomènes syntaxiques rares ou inconnus, liés au lexique, des textes à analyser.
Notre objectif est donc de proposer des regroupements lexicaux à partir d'informations tirées des lexiques syntaxiques du français, et d'observer leur impact sur les performances d'analyseurs syntaxiques.
Par ailleurs, la plupart des évaluations concernant l'étiquetage morpho-syntaxique probabiliste et l'analyse syntaxique probabiliste ont été réalisées avec une segmentation parfaite du texte, car identique à celle du corpus évalué.
Or, dans les cas réels d'application, la segmentation d'un texte est très rarement disponible et les segmenteurs automatiques actuels sont loin de proposer une segmentation de bonne qualité, et ce, à cause de la présence de nombreuses unités multi-mots (mots composés, entités nommées,...).
Dans ce mémoire, nous nous focalisons sur les unités multi-mots dites continues qui forment des unités lexicales auxquelles on peut associer une étiquette morpho-syntaxique, et que nous appelons mots composés.
Par exemple, cordon bleu est un nom composé, et tout à fait un adverbe composé.
Notre deuxième problématique portera donc sur la segmentation automatique des textes français et son impact sur les performances des processus automatiques.
Pour ce faire, nous nous sommes penché sur une approche consistant à coupler, dans un même modèle probabiliste, la reconnaissance des mots composés et une autre tâche automatique.
La reconnaissance des mots composés est donc réalisée au sein du processus probabiliste et non plus dans une phase préalable.
La planification est un domaine de l'intelligence artificielle qui a pour but de proposer des méthodes permettant d'automatiser la recherche et l'ordonnancement d'ensembles d'actions afin d'atteindre un objectif donné.
Il s'agit de décomposer un problème en plusieurs sous-problèmes (généralement appelés composants) le plus indépendants possibles, et d'assembler des plans pour ces sous-problèmes en un plan pour le problème d'origine.
L'intérêt de cette approche est que, pour certaines classes de problèmes de planification, les composants peuvent être bien plus simples à résoudre que le problème initial.
La principale différence entre cette méthode et la précédente est qu'il s'agit d'une approche distribuée de la planification modulaire.
Dans cette dissertation, nous discutons comment utiliser les données du regard humain pour améliorer la performance du modèle d'apprentissage supervisé faible dans la classification des images.
Le contexte de ce sujet est à l'ère de la technologie de l'information en pleine croissance.
En conséquence, les données à analyser augmentent de façon spectaculaire.
Étant donné que la quantité de données pouvant être annotées par l'humain ne peut pas tenir compte de la quantité de données elle-même, les approches d'apprentissage supervisées bien développées actuelles peuvent faire face aux goulets d'étranglement l'avenir.
Dans ce contexte, l'utilisation de annotations faibles pour les méthodes d'apprentissage à haute performance est digne d'étude.
Plus précisément, nous essayons de résoudre le problème à partir de deux aspects : l'un consiste à proposer une annotation plus longue, un regard de suivi des yeux humains, comme une annotation alternative par rapport à l'annotation traditionnelle longue, par exemple boîte de délimitation.
L'autre consiste à intégrer l'annotation du regard dans un système d'apprentissage faiblement supervisé pour la classification de l'image.
Ce schéma bénéficie de l'annotation du regard pour inférer les régions contenant l'objet cible.
Une propriété utile de notre modèle est qu'elle exploite seulement regardez pour la formation, alors que la phase de test est libre de regard.
Cette propriété réduit encore la demande d'annotations.
Les deux aspects isolés sont liés ensemble dans nos modèles, ce qui permet d'obtenir des résultats expérimentaux compétitifs.
La modélisation sinusoïdale est une des méthodes les plus largement utilisés paramétriques pour la parole et le traitement des signaux audio.
Le eaQHM est montré à surperformer aQHM dans l'analyse et la resynthèse de la parole voisée.
Sur la base de la eaQHM, un système hybride d'analyse / synthèse de la parole est présenté (eaQHNM), et aussi d'une version hybride de l'aHM (aHNM).
En outre, nous présentons la motivation pour une représentation pleine bande de la parole en utilisant le eaQHM, c'est, représentant toutes les parties du discours comme haute résolution des sinusoıdes AM-FM.
Les expériences montrent que l'adaptation et la quasi-harmonicité est suffisante pour fournir une qualité de transparence dans la parole non voisée resynthèse.
La pleine bande analyse eaQHM et système de synthèse est présenté à côté, ce qui surpasse l'état de l'art des systèmes, hybride ou pleine bande, dans la reconstruction de la parole, offrant une qualité transparente confirmé par des évaluations objectives et subjectives.
En ce qui concerne les applications, le eaQHM et l'aHM sont appliquées sur les modifications de la parole (de temps et pas mise à l'échelle).
Les modifications qui en résultent sont de haute qualité, et suivent des règles très simples, par rapport à d'autres systèmes de modification état de l'art.
Les résultats montrent que harmonicité est préféré au quasi-harmonicité de modifications de la parole du fait de la simplicité de la représentation intégrée.
En outre, la pleine bande eaQHM est appliquée sur le problème de la modélisation des signaux audio, et en particulier d'instrument de musique retentit.
Le eaQHM est évaluée et comparée à des systèmes à la pointe de la technologie, et leur est montré surpasser en termes de qualité de resynthèse, représentant avec succès l'attaque, transitoire, et une partie stationnaire d'un son d'instruments de musique.
Enfin, une autre application est suggéré, à savoir l'analyse et la classification des discours émouvant.
Le eaQHM est appliqué sur l'analyse des discours émouvant, offrant à ses paramètres instantanés comme des caractéristiques qui peuvent être utilisés dans la reconnaissance et la quantification vectorielle à base classification du contenu émotionnel de la parole.
Bien que les modèles sinusoidaux sont pas couramment utilisés dans ces tâches, les résultats sont prometteurs.
Grâce à la démocratisation des nouvelles technologies de communications nous disposons d'une quantité croissante de ressources textuelles, faisant du Traitement Automatique du Langage Naturel (TALN) une discipline d'importance cruciale tant scientifiquement qu'industriellement.
Aisément disponibles, ces données offrent des opportunités sans précédent et, de l'analyse d'opinion à la recherche d'information en passant par l'analyse sémantique de textes les applications sont nombreuses.
On ne peut cependant aisément tirer parti de ces données textuelles dans leur état brut et, en vue de mener à bien de telles tâches il semble indispensable de posséder des ressources décrivant les connaissances sémantiques, notamment sous la forme de réseaux lexico-sémantiques comme par exemple celui du projet JeuxDeMots.
La constitution et la maintenance de telles ressources restent cependant des opérations difficiles, de part leur grande taille mais aussi à cause des problèmes de polysémie et d'identification sémantique.
De plus, leur utilisation peut se révéler délicate car une part significative de l'information nécessaire n'est pas directement accessible dans la ressource mais doit être inférée à partir des données du réseau lexico-sémantique.
Nos travaux cherchent à démontrer que les réseaux lexico-sémantiques sont, de par leur nature connexionniste, bien plus qu'une collection de faits bruts et que des structures plus complexes telles que les chemins d'interprétation contiennent davantage d'informations et permettent d'accomplir de multiples opérations d'inférences.
En particulier, nous montrerons comment utiliser une base de connaissance pour fournir des explications à des faits de haut niveau.
Ces explications permettant a minima de valider et de mémoriser de nouvelles informations.
Ce faisant, nous pouvons évaluer la couverture et la pertinence des données de la base ainsi que la consolider.
De même, la recherche de chemins se révèle utile pour des problèmes de classification et de désambiguïsation, car ils sont autant de justifications des résultats calculés.
Dans le cadre de la reconnaissance d'entité nommées, ils permettent aussi bien de typer les entités et de les désambiguïser (l'occurrence du terme Paris est-il une référence à la ville, et laquelle, ou à une starlette ?) en mettant en évidence la densité des connexions entre les entités ambiguës, leur contexte et leur type éventuel.
Enfin nous proposons de tourner à notre avantage la taille importante du réseau JeuxDeMots pour enrichir la base de nouveaux faits à partir d'un grand nombre d'exemples comparables et par un processus d'abduction sur les types de relations sémantiques pouvant connecter deux termes donnés.
Chaque inférence s'accompagne d'explications pouvant être validées ou invalidées offrant ainsi un processus d'apprentissage.
Cette thèse s'inscrit dans ce cadre pour aborder le sujet depuis deux points de vue complémentaires.
D'une part, celui apparent de la fiabilité, de l'efficacité et de l'utilisabilité de ces interfaces.
D'autre part, les aspects de conception et d'implémentation sont étudiés pour apporter des outils de développement aux concepteurs plus ou moins initiés de tels systèmes.
A partir des outils et des évolutions dans le domaine, une plate-forme modulaire de dialogue vocal a été agrégée.
Une méthode simple, basée sur la comparaison des résultats de traitements parallèles a prouvé son efficacité, tout comme ses limites pour une interaction continue avec l'utilisateur.
Les modules de compréhension du langage forment un sous-système interconnecté au sein de la plate-forme.
Le choix de la gestion du dialogue basé sur des modèles de tâches hiérarchiques, comme c'est la cas pour la plate-forme, est argumenté.
Ce formalisme est basé sur une construction humaine et présente, de fait, des obstacles pour concevoir, implémenter, maintenir et faire évoluer les modèles.
L'objet de cette thèse est de préparer un dictionnaire de médecine vétérinaire français-roumain et roumain-français.
A cette fin, dans une première étape, nous avons étudié les méthodes employées au fil du temps par les maîtres de la terminologie.
Nos méthodes de travail ont été offertes par nos collègues plus expérimentés et par les normes ISO de terminologie.
Ainsi, nos recherches se sont constituées dans un essai de modélisation du travail terminologique, au cours de la recherche des meilleures méthodes.
Dans une deuxième étape, nous avons constitué un corpus de recherche, en se basant sur les méthodes de la linguistique de corpus.
Nous avons constitué un corpus de 2193 thèses de doctorat vétérinaires VeThèses des 11 années (1998-2008) des quatre écoles vétérinaires françaises, des thèses disponibles en format numérique, autour du thème des vertébrés domestiques.
L'étape de dépouillement terminologique a compris aussi un essai de systématisation des termes.
Les résultats de ce travail sur le corpus se sont constitués dans une nomenclature et la base de données VeTerm.
Nos travaux de recherche ont pour objectif de résoudre le problème de la géodétection des réseaux enterrés.
Plusieurs méthodes sont utilisées actuellement mais présentent des limites dues à la nature du sol, aux matériaux des canalisations et au produit transporté.
A une époque où l'informatique a envahi tous les aspects de notre vie quotidienne, il est tout à fait normal de voir le domaine informatique participer aux travaux en sciences humaines et sociales, et notamment en linguistique où le besoin de développer des logiciels informatiques se fait de plus en plus pressant avec le volume grandissant des corpus traités.
D'où notre travail de thèse qui consiste en l'élaboration d'un programme EPL qui étudie le parler arabe libanais blanc.
En partant d'un corpus élaboré à partir de deux émissions télévisées enregistrées puis transcrites en lettres arabes, ce programme, élaboré avec le logiciel Access, nous a permis d'extraire les mots et les collocations et de procéder à une analyse linguistique aux niveaux lexical, phonétique, syntaxique et collocationnel.
Le fonctionnement de l'EPL ainsi que le code de son développement sont décrits en détails dans une partie informatique à part.
Des annexes de taille closent la thèse et rassemblent le produit des travaux de toute une équipe de chercheures venant de maintes spécialités.
La vision par ordinateur est un domaine de l'informatique visant à reproduire et à améliorer la capacité de la vision humaine à comprendre son environnement.
Dans cette thèse, nous nous concentrons sur deux domaines de la vision par ordinateur, à savoir la segmentation d'image et l'odométrie visuelle.
Nous montrons l'impact positif qu'apporte l'usage d'applications
La première partie de cette thèse porte sur l'annotation et la segmentation d'images.
Nous définissons dans un premier temps le problème de l'annotation d'images et les défis que cela représente pour des grands ensembles de données.
De nombreuses interactions ont été utilisées dans la littérature pour aider les algorithmes de segmentation.
Les plus courantes consistent à désigner explicitement des contours, dessiner des boîtes englobantes, ou marquer des traits à l'intérieur et à l'extérieur des objets d'intérêt.
Dans un contexte de crowdsourcing, les tâches d'annotation sont déléguées à un public non-expert.
Pour cette raison, nous avons mené une étude utilisateur montrant les avantages d'une interaction que nous appelons entourage par rapport aux autres types d'interactions.
Un tour d'horizon des fonctionnalités et de son architecture est proposé, ainsi qu'un guide pour le déploiement dans des services de microtâches comme Amazon Mechanical Turk.
Cette application est entièrement libre et mise à disposition en ligne.
Dans la seconde partie de cette thèse, nous présentons notre bibliothèque libre d'odométrie visuelle directe.
Nous fournissons une évaluation comparative montrant que notre approche est aussi performante que les alternatives actuellement disponibles.
La formulation du problème d'odométrie visuelle repose sur des outils géométriques et des techniques d'optimisation nécessitant une grosse puissance de calcul pour fonctionner à 25 images par seconde.
Puisque nous aspirons à exécuter ces algorithmes sur le Web, nous passons en revue les technologies passées et courantes fournissant des bonnes performances directement au sein du navigateur Web.
En particulier, nous détaillons comment cibler une nouvelle plateforme appelée WebAssembly à partir des langages de programmation C++ et Rust.
Notre bibliothèque a été implémentée entièrement dans le langage de programmation Rust, ce qui en a facilité le portage vers WebAssembly.
Cette propriété nous a permis de mettre en place une application Web d'odométrie visuelle proposant différents types d'interactions.
Une barre de temps permet une navigation unidimensionnelle le long de la séquence vidéo.
Des paires de points peuvent être sélectionnées sur deux images de la séquence pour réaligner les caméras et corriger l'éventuelle dérive.
Des couleurs sont également utilisées pour identifier des parties sélectionnables du nuage de points 3D pour réinitialiser les positions de la caméra.
La combinaison de ces interactions permet d'apporter des améliorations sur les résultats du suivi et de la reconstruction du nuage de points 3D.
Notre hypothèse de travail est la suivant : la LSF est une langue et, à ce titre, la traduction automatique lui est applicable.
Nous décrivons ensuite les spécifications linguistiques pour le traitement automatique, en fonction des observations mises en avant dans l'état de l'art et des propositions de nos informateurs.
Nous détaillons notre méthodologie et présentons l'avancée de nos travaux autour de la formalisation des données linguistiques à partir des spécificités de la LSF dont certaines (model verbal, modification adjectivale et adverbiale, organisation des substantifs, problématiques de l'accord) ont nécessité un traitement plus approfondi.
Nous présentons le cadre applicatif dans lequel nous avons travaillé : les systèmes de traduction automatique et d'animation de personnage virtuel de France Telecom R&amp;D. Puis, après un rapide état de l'art sur les technologies avatar nous décrivons nos modalités de contrôle du moteur de synthèse de geste grâce au format d'échange mis au point.
Enfin, nous terminons par nos évaluations et perspectives de recherche et de développements qui pourront suivre cette Thèse.
Notre approche a donné ses premiers résultats puisque nous avons atteint notre objectif de faire fonctionner la chaîne complète de traduction : de la saisie d'un énoncé en français jusqu'à la réalisation de l'énoncé correspondant en LSF par un personnage de synthèse.
Il s'agit le plus souvent d'exercices à trous ou de QCM, conçus à l'aide de systèmes auteurs comme Course builder, Hot Potatoes ou Netquizz.
Ce type d'activités pose plusieurs problèmes comme la rigidité des logiciels (les données utilisés sont prédéfinies et elles ne peuvent être ni modifiées ni enrichies) et la non adaptabilité des parcours aux compétences linguistiques des apprenants (le cheminement est indépendant de ses réponses à chaque étape, faute de pouvoir les évaluer).
L'utilisation du TAL, pour la conception de logiciels d'ALAO, représente un moyen faible pour résoudre ces problèmes.
Après plus de deux décennies depuis le début des travaux, l'avancée de recherches dans ce domaine (c'est à dire utilisation de TAL en ALAO) reste insuffisante, à cause de deux facteurs principalement : la méconnaissance du TAL de la part des didacticiens des langues, voire des informaticiens, et le coût des ressources et produits issus du traitement automatique de la langue.
ALAO basé sur le TAL pour la langue arabe sont pratiquement inexistants, malgré une riche bibliographie concernant le traitement automatique de l'arabe.
Outre les facteurs cités précédemment, la carence concernant la langue arabe dans ce domaine est due au fait qu'elle est une langue difficile à traiter automatiquement.
En fonction de cette situation, et désireux d'enrichir les possibilités de création d'activités pédagogiques pour l'arabe, nous avons conçu un étiqueteur, un dérivateur, un conjugueur, un analyseur morphologique ainsi qu'un dictionnaire étiqueté (le plus complet possible) pour la langue arabe.
Ensuite nous avons exploité ces outils pour créer bon nombre d'applications pédagogiques pour l'apprentissage de l'arabe.
Cette thèse présente des recherches linguistiques et phonétiques sur le code-switching Français-Arabe Algérien.
Un corpus de 7h30 de parole (5h de parole spontané et 2h30 de parole lue) a été constitué en enregistrant 20 hommes et femmes parlant le français et l'arabe algérien.
Cette thèse présente également les méthodes de traitement des données orales du code-switching telles que la segmentation de la parole, la segmentation des énoncés de code-switching ainsi que la transcription du français et du dialecte arabe algérien.
Cette thèse présente également des méthodes d'alignement automatique de ces données bilingues ainsi qu'un alignement combiné de deux alignements monolingues.
Nous avons mené des expériences basées sur l'alignement automatique avec des variations qui traitent de la question de l'influence d'un système phonologique d'une langue A sur des productions phonétiques en code-switching du français et de l'arabe algérien.
Nous avons aussi abordé les consonnes emphatiques et l'emphatisation des deux langues.
Enfin, nous avons également travaillé sur les géminées et la gémination dans les productions langagières en code-switching.
Les résultats ont montré que le code-switching FR-AA se caractérise par des changements de langues très courts qui sont un réel défi pour l'identification des langues dans le code-switching.
Le code-switching a un impact sur la variation phonétique des voyelles et des consonnes.
La parole du code-switching permet au locuteur de produire moins de variation de voyelles et de consonnes que la parole monolingue.
Le texte généré automatiquement a été utilisé dans de nombreuses occasions à des buts différents.
Il peut simplement passer des commentaires générés dans une discussion en ligne à une tâche beaucoup plus malveillante, comme manipuler des informations bibliographiques.
Ainsi, cette thèse introduit d'abord différentes méthodes pour générer des textes libres ayant trait à un certain sujet et comment ces textes peuvent être utilisés.
Par conséquent, nous essayons d'aborder plusieurs questions de recherche.
La première question est comment et quelle est la meilleure méthode pour détecter un document entièrement généré.
Ensuite, nous irons un peu plus loin et montrer la possibilité de détecter quelques phrases ou un petit paragraphe de texte généré automatiquement en proposant une nouvelle méthode pour calculer la similarité des phrases en utilisant leur structure grammaticale.
La dernière question est comment détecter un document généré automatiquement sans aucun échantillon, ceci est utilisé pour illustrer le cas d'un nouveau générateur ou d'un générateur dont il est impossible de collecter des échantillons dessus.
Cette thèse étudie également l'aspect industriel du développement.
Un aperçu simple d'un flux de travail de publication d'un éditeur de premier plan est présenté.
À partir de là, une analyse est effectuée afin de pouvoir intégrer au mieux notre méthode de détection dans le flux de production.
En conclusion, cette thèse a fait la lumière sur de multiples questions de recherche importantes concernant la possibilité de détecter des textes générés automatiquement dans différents contextes.
En plus de l'aspect de la recherche, des travaux d'ingénierie importants dans un environnement industriel réel sont également réalisés pour démontrer qu'il est important d'avoir une application réelle pour accompagner une recherche hypothétique.
L'interopérabilité entre les systèmes a été identifiée comme un problème majeur auquel sont confrontées les entreprises lorsqu'ils ont le besoin de collaborer avec d'autres organisations et de participer au sein d'un réseau d'entreprises.
Pour atteindre une qualité d'interopérabilité supérieure et garantir une collaboration efficace, un certain nombre d'Exigences d'Interopérabilité (EI) doivent être satisfaites.
Ainsi, l'interopérabilité doit être vérifiée et continuellement améliorée.
L'Analyse de l'Interopérabilité (ANIN) est une manière de vérifier l'interopérabilité des systèmes.
Il a également été identifié que les interdépendances entre les EI ne sont pas explicitement définies.
En effet, leurs interdépendances doivent être prises en compte car elles peuvent aider à identifier les impacts sur l'ensemble du système.
De plus, la majorité des approches ANIN sont manuelles, ce qui est un processus laborieux et long qui dépend souvent des connaissances « subjectives » des experts.
Science Research » (DSR) a été adoptée pour mener à bien la contribution proposée.
Pour conceptualiser formellement les connaissances sur l'ANIN, en englobant l'ensemble des EI, les problèmes et solutions d'interopérabilité ainsi que leurs relations, nous avons proposé l'Ontologie de l'Analyse de l'Interopérabilité (OAI).
Une approche d'Ingénierie Système basée sur des Modèles a été appliquée pour définir les concepts de l'ontologie.
Un prototype du SAIC utilisant l'OAI comme modèle de connaissance a été développé sur une plate
La contribution proposée a été évaluée grâce à une étude de cas basée sur une véritable entreprise en réseau
La subjectivité des estimations et des perceptions, la complexité de l'environnement, l'interaction entre sous-systèmes, le manque de données précises, les données manquantes, une faible capacité de traitement de l'information, et l'ambiguïté du langage naturel représentent les principales formes d'incertitude auxquelles les décideurs doivent faire face lorsqu'ils prennent des décisions stratégiques à l'aide de systèmes d'intelligence économique.
Cette étude utilise un paradigme de « soft computing » pour identifier et analyser l'incertitude, que nous associons à la notion de facteurs de risque d'information.
Les outils de la logique floue ont été également utilisés au niveau des ontologies ( « FuzzOntology » ).
FuzzyMatch permet de réduire les problèmes de données manquantes.
A l'aide de ces modèles, le processus de décision en intelligence économique bénéficie d'une réduction des risques liés à l'information lors du processus de recherche.
Nous nous intéressons aux conversations écrites en ligne orientées vers la résolution de problèmes.
Nous cherchons à utiliser ces actes pour analyser les conversations écrites en ligne.
Un cadre et des méthodes bien définies permettant une analyse fine de ce type de conversations en termes d'actes de dialogue représenteraient un socle solide sur lequel pourraient reposer différents systèmes liés à l'aide à la résolution des problèmes et à l'analyse des conversations écrites en ligne.
Cependant, les techniques d'identification de la structure des conversations n'ont pas été développées autour des conversations écrites en ligne.
Il est nécessaire d'adapter les ressources existantes pour ces conversations.
Notre objectif est de modéliser les conversations écrites en ligne orientées vers la résolution de problèmes en termes d'actes de dialogue, et de proposer des outils pour la reconnaissance automatique de ces actes.
Dans le cadre de notre thèse, nous avons proposé une approche générique multilingue d'extraction automatique de connaissances.
Nous avons validé l‟approche sur l'extraction des événements de variations des cours pétroliers et l‟extraction des expressions temporelles liées à des référentiels.
Notre approche est basée sur la constitution de plusieurs cartes sémantiques par analyse des données non structurées afin de formaliser les traces linguistiques textuelles exprimées par des catégories d'un point de vue de fouille.
Nous avons mis en place un système expert permettant d‟annoter la présence des catégories en utilisant des groupes de règles.
Deux algorithmes d'annotation AnnotEV et AnnotEC ont été appliqués, dans la plateforme SemanTAS.
Le rappel et précision de notre système d'annotation est autour de 80%.
Nous avons présenté les résultats aussi sous forme des fiches de synthèses.
L'analyse morphologique automatique du slovaque constitue la première étape d'un système d'analyse automatique du contenu des textes scientifiques et techniques slovaques.
Un tel système pourrait être utilisé par des applications, telles que l'indexation automatique des textes, la recherche automatique de la terminologie ou par un système de traduction.
Une description des régularités de la langue par un ensemble de règles ainsi que l'utilisation de tous les éléments au niveau de la forme du mot qui rendent possible son interprétation permettent de réduire d'une manière considérable le volume des dictionnaires.
Notamment s'il s'agit d'une langue à très riche flexion, comme le slovaque.
Les résultats que nous obtenons lors de l'analyse morphologique confirment la faisabilité et la grande fiabilité d'une analyse morphologique basée sur la reconnaissance des formes et ceci pour toutes les catégories lexicales concernées par la flexion.
Ces dernières années, le piratage est devenu une industrie à part entière, augmentant le nombre et la diversité des cyberattaques.
Les menaces qui pèsent sur les réseaux informatiques vont des logiciels malveillants aux attaques par déni de service, en passant par le phishing et l'ingénierie sociale.
Un plan de cybersécurité efficace ne peut plus reposer uniquement sur des antivirus et des pare-feux pour contrer ces menaces : il doit inclure plusieurs niveaux de défense.
Les systèmes de détection d'intrusion (IDS) réseaux sont un moyen complémentaire de renforcer la sécurité, avec la possibilité de surveiller les paquets de la couche 2 (liaison) à la couche 7 (application) du modèle OSI.
Les techniques de détection d'intrusion sont traditionnellement divisées en deux catégories : la détection par signatures et la détection par anomalies.
La plupart des IDS utilisés aujourd'hui reposent sur la détection par signatures ; ils ne peuvent cependant détecter que des attaques connues.
Les IDS utilisant la détection par anomalies sont capables de détecter des attaques inconnues, mais sont malheureusement moins précis, ce qui génère un grand nombre de fausses alertes.
Dans ce contexte, la création d'IDS précis par anomalies est d'un intérêt majeur pour pouvoir identifier des attaques encore inconnues.
Dans cette thèse, les modèles d'apprentissage automatique sont étudiés pour créer des IDS qui peuvent être déployés dans de véritables réseaux informatiques.
Tout d'abord, une méthode d'optimisation en trois étapes est proposée pour améliorer la qualité de la détection : 1/ augmentation des données pour rééquilibrer les jeux de données, 2/ optimisation des paramètres pour améliorer les performances du modèle et 3/ apprentissage ensembliste pour combiner les résultats des meilleurs modèles.
Les flux détectés comme des attaques peuvent être analysés pour générer des signatures afin d'alimenter les bases de données d'IDS basées par signatures.
Toutefois, cette méthode présente l'inconvénient d'exiger des jeux de données étiquetés, qui sont rarement disponibles dans des situations réelles.
L'apprentissage par transfert est donc étudié afin d'entraîner des modèles d'apprentissage automatique sur de grands ensembles de données étiquetés, puis de les affiner sur le trafic normal du réseau à surveiller.
Cette méthode présente également des défauts puisque les modèles apprennent à partir d'attaques déjà connues, et n'effectuent donc pas réellement de détection d'anomalies.
C'est pourquoi une nouvelle solution basée sur l'apprentissage non supervisé est proposée.
Elle utilise l'analyse de l'en-tête des protocoles réseau pour modéliser le comportement normal du trafic.
Les anomalies détectées sont ensuite regroupées en attaques ou ignorées lorsqu'elles sont isolées.
Enfin, la détection la congestion réseau est étudiée.
Le taux d'utilisation de la bande passante entre les différents liens est prédit afin de corriger les problèmes avant qu'ils ne se produisent.
De nos jours, la musique est la plupart du temps diffusée sous forme enregistrée.
Un système capable de transformer automatiquement une interprétation musicale en partition serait donc extrêmement bénéfique pour les interprètes et les musicologues.
Cette tâche, appelée “automatic music transcription” (AMT), comprend plusieurs sous-tâches qui impliquent notamment la transformation de représentations intermédiaires comme le MIDI, quantifié ou non.
Nous d´défendons dans cette thèse l'idée qu'un modèle robuste et bien structure des informations contenues dans une partition musicale aiderait au développement et à l'évaluation de systèmes AMT.
En particulier, nous préconisons une s´séparation claire entre le contenu musical encodé dans la partition et l'ensemble des signes et des règles de notation utilisés pour représenter graphiquement ce contenu.
Le Web des données étend le Web en publiant des données structurées et liées en RDF.
Un jeu de données RDF est un graphe orienté où les ressources peuvent être des sommets étiquetées dans des langues naturelles.
Un des principaux défis est de découvrir les liens entre jeux de données RDF.
Étant donnés deux jeux de données, cela consiste à trouver les ressources équivalentes et les lier avec des liens owl :
Cette thèse étudie l'efficacité des ressources linguistiques pour le liage des données exprimées dans différentes langues.
Les étiquettes des sommets voisins constituent le contexte d'une ressource.
Ceci peut être réalisé à l'aide de la traduction automatique ou de ressources lexicales multilingues.
La similarité entre les documents est prise pour la similarité entre les ressources RDF.Nous évaluons expérimentalement différentes méthodes pour lier les données RDF.
En particulier, deux stratégies sont explorées : l'application de la traduction automatique et l'usage des banques de données terminologiques et lexicales multilingues.
Dans l'ensemble, l'évaluation montre l'efficacité de ce type d'approches.
Les méthodes ont été évaluées sur les ressources en anglais, chinois, français, et allemand.
L'évaluation montre que la méthode basée sur la similarité peut être appliquée avec succès sur les ressources RDF indépendamment de leur type (entités nommées ou concepts de dictionnaires).
Cette thèse traite de l'adaptation automatique des applications sensibles au contexte par l'utilisation d'informations liées à l'environnement social des utilisateurs afin d'enrichir le service rendu par les applications.
Pour cela, notre contribution s'articule autour de la modélisation multidimensionnelle des différents niveaux de contextes sociaux, notamment le poids de la relation entre les acteurs.
Plus spécifiquement, nous synthétisons des contextes sociaux non seulement liés à la familiarité mais aussi liés à la similitude des communautés statiques et dynamiques.
Deux modèles basés respectivement sur les graphes et les ontologies sont proposés afin de satisfaire l'hétérogénéité des réseaux sociaux de la vie réelle.
Nous utilisons les données réelles recueillies sur les réseautages sociaux en ligne pour conduire nos expérimentations et analysons les résultats en vérifiant l'efficacité de ces modèles.
En parallèle nous traitons le point de vue de l'application, et nous présentons deux algorithmes utilisant des contextes sociaux pour améliorer la stratégie de transmission des données dans le réseau opportuniste, et particulièrement la contre-mesure aux nœuds égoïstes.
Les simulations des scénarios réels confirment les avantages liés à l'introduction des contextes sociaux, en termes de taux de succès et de délais de transmission.
Nous effectuons une comparaison avec d'autres algorithmes de transmission traditionnellement décrits dans la littérature pour compléter notre démonstration
Il est communément admis que 70 % des coûts du cycle de vie d'un produit sont engagés dès la phase de spécification.
Pour aider les sous-traitants, nous proposons une méthode outillée de synthèse des exigences, laquelle est supportée par un environnement numérique basé sur les sciences des données.
Des modèles de classification extraient les exigences des documents.
Les exigences sont ensuite analysées au moyen des techniques de traitement du langage naturel afin d'identifier les défauts de qualité qui mettent en péril le reste du cycle de vie.
La validation théorique et empirique de notre proposition corrobore l'hypothèse que les sciences des données est un moyen de synthétiser plusieurs centaines ou milliers d'exigences.
Les maladies cardiovasculaires (MCVs) et les cancers constituent la première et la seconde cause de mortalité et de morbidité en France chez les hommes et les femmes, et leur coût annuel est important.
La prévention de ces maladies chroniques constitue, avec leur dépistage précoce et leur prise en charge rapide et efficace, un moyen possible de réduire ce coût.
Nutrition Santé (PNNS) a été mis en place en France pour aider les Français à avoir une meilleure alimentation, afin de contribuer à réduire l'incidence de ces maladies.
L'objectif de cette thèse est la construction d'un système de suggestions personnalisées fondé sur le profil de l'individu et son risque cardiovasculaire.
Cette approche nécessite la mise en place d'une démarche interdisciplinaire faisant collaborer des chercheurs dans les domaines de l'informatique, de l'épidémiologie et de la nutrition.
L'importance de cette collaboration se justifie par le besoin de produire des suggestions étayées par des recherches attestées dans ces domaines.
Le premier apport de cette thèse est l'intégration des technologies du web sémantique dans une nouvelle approche d'évaluation du risque cardiovasculaire qui prend en compte les interactions entre ces facteurs.
La création de l'outil de visualisation MCVGraphViz a permis de mettre en oeuvre cette stratégie.
Le deuxième apport consiste à proposer une solution pour exploiter les connaissances présentes dans les plans de santé et les recommandations concernant la prévention des maladies cardiovasculaires en France.
Ainsi, nous avons opté pour une approche modulaire intégrée dans l'outil MCVGraphViz qui permet de produire des recommandations (alimentation,activité physique, etc.) fondées sur le risque cardiovasculaire évalué et le profil de l'individu (préférences sensorielles, contraintes allergiques, capacité physique, etc.).
Le troisième apport concerne la qualification nutritionnelle des recettes de cuisine pour un meilleur suivi : l'approche s'appuie sur des techniques de traitement automatique de la langue et des raisonnements ontologiques pour qualifier d'un point de vue nutritionnel des recettes de cuisine.
De nombreuses perspectives sont exposées.
La plupart d'entre elles visent à améliorer les systèmes de recommandation et l'expressivité de la base de connaissances sur les maladies cardiovasculaires.
Cette recherche est ancrée à la fois dans la statistique textuelle, et secondairement dans l'analyse du discours, avec une complémentarité quantitatif-qualitatif.
L'état de la question présente une vue d'ensemble des recherches linguistiques sur la publicité – linguistique descriptive, analyse du discours, sémiologie/sémiotique et rhétorique/stylistique, en prenant pour point de départ le travail pionnier en linguistique descriptive de Leech (TheLanguage of Advertising, 1966) – et révèle plusieurs constats :
1. les mots de la publicité constituent bien un discours spécialisé
2. la publicité ressemble à la fois à la poésie et à la langue courante
3. l'intertextualité et la participation active d'un récepteur ciblé sont deux données clés de la communication publicitaire
4. la langue publicitaire est émotionnelle, ambiguë, indirecte et implicite
5. de façon simpliste et ludique, la publicité nous dépeint un monde de bonheur parfait
6. simplicité et conventionnalisation du discours publicitaire, et invariants lexicaux.
Pour répondre à certaines questions sur le discours, la langue, la communication et la traduction publicitaires, nous avons choisi une approche quantitative et l'approche lexicométrique (ou statistique textuelle, devenue récemment textométrie), développée notamment par le « groupe de Saint-Cloud » et ses collaborateurs, dont André Salem (concepteur de Lexico, le logiciel utilisé pour nos analyses).
L'analyse lexicométrique est complétée par l'analyse factorielle des correspondances (AFC) et la méthode de projection géodésique de Viprey (avec le logiciel Astadiag).
Grâce à un corpus parallèle (bitexte) d'environ 800 000 occurrences en anglais et un million en français (170 brochures publicitaires compilées à partir de sites Web canadiens de 20 marques, 10 constructeurs, 229 véhicules au total), nous avons réussi à déterminer un profil lexical distinct pour chacun des sept segments du marché automobile : AB, CD, EF, MPV (véhicules multifonctions), PK (pickups/camions), SP (voitures de sport) et SUV (VUS : véhicules utilitaires sports) et montré que dans l'ensemble, les publicités sont traduites assez littéralement.
Leur comportements sont adaptés à chaque cas d'usage par des experts.
Permettre au grand public d'enseigner de nouveaux comportements pourrait mener à une meilleure adaptation à moindre coût.
Les domiciles sont des mondes ouverts qui ne peuvent pas être prédéterminés.
Pepper doit donc, en plus d'apprendre de nouveaux comportements, être capable de découvrir son environnement, et de s'y rendre utile ou de divertir : c'est un scénario riche.
L'enseignement de comportements que nous démontrons s'effectue donc dans ces conditions uniques : par le seul langage parlé, dans des scénarios riches et ouverts, et sur un robot Pepper standard.
Grâce à la transcription automatique de la parole et au traitement automatique du langage, notre système reconnaît les enseignements de comportement que nous n'avions pas prédéterminés.
Les nouveaux comportements peuvent solliciter des entités qui auraient été appris dans d'autres contextes, pour les accepter et s'en servir comme paramètres.
Par des expériences de complexité croissante, nous montrons que des conflits entre les comportements apparaissent dans les scénarios riches, et proposons de les résoudre à l'aide de planification de tâche et de règles de priorités.
Le développement de systèmes en traitement automatique des langues (TAL) nécessite de déterminer la qualité de ce qui est produit.
Que ce soit pour comparer plusieurs systèmes entre eux ou identifier les points forts et faibles d'un système isolé, l'évaluation suppose de définir avec précision et pour chaque contexte particulier une méthodologie, un protocole, des ressources linguistiques (les données nécessaires à l'apprentissage et au test des systèmes) ou encore des mesures et métriques d'évaluation.
C'est à cette condition que l'amélioration des systèmes est possible afin d'obtenir des résultats plus fiables et plus exploitables à l'usage.
L'apport de l'évaluation en TAL est important avec la création de nouvelles ressources linguistiques, l'homogénéisation des formats des données utilisées ou la promotion d'une technologie ou d'un système.
Nous avons cherché à réduire et à encadrer ces interventions manuelles.
Pour ce faire, nous appuyons nos travaux sur la conduite ou la participation à des campagnes d'évaluation comparant des systèmes entre eux, ou l'évaluation de systèmes isolés.
Nous avons formalisé la gestion du déroulement de l'évaluation et listé ses différentes phases pour définir un cadre d'évaluation commun, compréhensible par tous.
Le point phare de ces phases d'évaluation concerne la mesure de la qualité via l'utilisation de métriques.
Cela a imposé trois études successives sur les mesures humaines, les mesures automatiques et les moyens d'automatiser le calcul de la qualité et enfin la méta-évaluation des mesures qui permet d'en évaluer la fiabilité.
Dans ce contexte, l'étude des similarités entre les technologies et entre leurs évaluations nous a permis d'observer les points communs et de les hiérarchiser.
Nous avons montré qu'un petit ensemble de mesures permet de couvrir une large palette d'applications à des technologies distinctes.
Notre objectif final était de définir une architecture d'évaluation générique, c'est-à-dire adaptable à tout type de technologie du TAL, et pérenne, c'est-à-dire permettant la réutilisation de ressources linguistiques, mesures ou méthodes au cours du temps.
Notre proposition se fait à partir des conclusions des étapes précédentes afin d'intégrer les phases d'évaluation à notre architecture et d'y incorporer les mesures d'évaluation, sans oublier la place relative à l'utilisation de ressources linguistiques.
La définition de cette architecture s'est effectuée en vue d'automatiser entièrement la gestion des évaluations, que ce soit pour une campagne d'évaluation ou l'évaluation d'un système isolé.
Du point de vue du traitement automatique des langues (TAL), l'extraction des événements dans les textes est la forme la plus complexe des processus d'extraction d'information, qui recouvrent de façon plus générale l'extraction des entités nommées et des relations qui les lient dans les textes.
Le cas des événements est particulièrement ardu car un événement peut être assimilé à une relation n-aire ou à une configuration de relations.
De ce fait, l'adaptation à un nouveau domaine constitue un défi supplémentaire.
Cette thèse présente plusieurs stratégies pour améliorer la performance d'un système d'extraction d'événements en utilisant des approches fondées sur les réseaux de neurones et en exploitant les propriétés morphologiques, syntaxiques et sémantiques des plongements de mots.
Ceux-ci ont en effet l'avantage de ne pas nécessiter une modélisation a priori des connaissances du domaine et de générer automatiquement un ensemble de traits beaucoup plus vaste pour apprendre un modèle.
Nous avons proposé plus spécifiquement différents modèles d'apprentissage profond pour les deux sous-tâches liées à l'extraction d'événements : la détection d'événements et la détection d'arguments.
La détection d'événements est considérée comme une sous-tâche importante de l'extraction d'événements dans la mesure où la détection d'arguments est très directement dépendante de son résultat.
En préalable à l'introduction de nos nouveaux modèles, nous commençons par présenter en détail le modèle de l'état de l'art qui en constitue la base.
Des expériences approfondies sont menées sur l'utilisation de différents types de plongements de mots et sur l'influence des différents hyperparamètres du modèle en nous appuyant sur le cadre d'évaluation ACE 2005, standard d'évaluation pour cette tâche.
Nous proposons ensuite deux nouveaux modèles permettant d'améliorer un système de détection d'événements.
L'un permet d'augmenter le contexte pris en compte lors de la prédiction d'une instance d'événement (déclencheur d'événement) en utilisant un contexte phrastique, tandis que l'autre exploite la structure interne des mots en profitant de connaissances morphologiques en apparence moins nécessaires mais dans les faits importantes.
Nous proposons enfin de reconsidérer la détection des arguments comme une extraction de relation d'ordre supérieur et nous analysons la dépendance de cette détection vis-à-vis de la détection d'événements.
Notre travail concerne les systèmes d'aide à la visite de musée et l'accès au patrimoine culturel.
L'objectif est de concevoir des systèmes de recommandation, implémentés sur dispositifs mobiles, pour améliorer l'expérience du visiteur, en lui recommandant les items les plus pertinents et en l'aidant à personnaliser son parcours.
Nous considérons essentiellement deux terrains d'application : la visite de musées et le tourisme.
Nous proposons une approche de recommandation hybride et sensible au contexte qui utilise trois méthodes différentes : démographique, sémantique et collaborative.
Chaque méthode est adaptée à une étape spécifique de la visite de musée.
L'approche démographique est tout d'abord utilisée afin de résoudre le problème du démarrage à froid.
L'approche sémantique est ensuite activée pour recommander à l'utilisateur des œuvres sémantiquement proches de celles qu'il a appréciées.
Enfin l'approche collaborative est utilisée pour recommander à l'utilisateur des œuvres que les utilisateurs qui lui sont similaires ont aimées.
La prise en compte du contexte de l'utilisateur se fait à l'aide d'un post-filtrage contextuel, qui permet la génération d'un parcours personnalisé dépendant des œuvres qui ont été recommandées et qui prend en compte des informations contextuelles de l'utilisateur à savoir : l'environnement physique, la localisation ainsi que le temps de visite.
Dans le domaine du tourisme, les points d'intérêt à recommander peuvent être de différents types (monument, parc, musée, etc.).
La nature hétérogène de ces points d'intérêt nous a poussé à proposer un système de recommandation composite.
Chaque recommandation est une liste de points d'intérêt, organisés sous forme de packages, pouvant constituer un parcours de l'utilisateur.
L'objectif est alors de recommander les Top-k packages parmi ceux qui satisfont les contraintes de l'utilisateur (temps et coût de visite par exemple).
L'évaluation expérimentale du système que nous avons proposé, en utilisant un data-set réel extrait de Tripadvisor démontre sa qualité et sa capacité à améliorer à la fois la précision et la diversité des recommandations.
L'évolution des technologies de l'information et de la communication (TIC) a permis le développement du travail collaboratif dans quasiment tous les secteurs de l'activité humaine.
Pour assurer la performance du collectif et minimiser le risque d'erreurs, il est crucial que les individus qui collaborent partagent une même représentation de la situation dans laquelle ils sont engagés.
L'avancée de l'étude de la cognition collective, cœur du travail collaboratif, est porteuse d'un potentiel certain qui doit se traduire par des applications concrètes au service de l'optimisation de la gestion et de la réalisation des tâches collectives.
L'évaluation en temps réel de la cognition des individus et des équipes permet d'envisager des outils et des systèmes adaptatifs pour gagner en efficacité, en performance et en agilité.
Face à ces enjeux, notre objectif, sur commande de la DGA, est de trouver des mesures appropriées qui permettraient une évaluation de la dynamique du partage des consciences de situation, dans le contexte très contraignant des salles de commandement et de contrôle, qui nécessite la plus faible instrumentation possible des opérateurs.
Notre contribution au domaine est double.
D'une part nous proposons le concept de synchronie des consciences de situation, pour soutenir le développement théorique de l'étude de la dynamique de partage de conscience de situation.
Ce travail de doctorat se présente comme une mise en avant de l'intérêt et de l'applicabilité de systèmes d'évaluation du partage de cognition en environnement de travail collaboratif réaliste, et s'accompagne de propositions concernant le futur de la recherche sur le C2.
Dans le cadre de la lutte contre le réchauffement climatique, plusieurs pays du monde notamment le Canada et certains pays européens dont la France, ont établi des mesures afin de réduire les nuisances environnementales.
L'un des axes majeurs abordés par les états concerne le secteur du transport et plus particulièrement le développement des systèmes de transport en commun en vue de réduire l'utilisation de la voiture personnelle et les émissions de gaz à effet de serre.
A cette fin, les collectivités concernées visent à mettre en place des systèmes de transports urbains plus accessibles, propres et durables.
Cette thèse s'inscrit dans un contexte général de valorisation des traces numériques et d'essor du domaine de la science des données (e.g., collecte et stockage des données, développement de méthodes d'apprentissage automatique, etc.).
Les travaux comportent trois volets principaux à savoir (i) la prévision long terme de l'affluence des passagers à l'aide de base de données événementielles et de données billettiques, (ii) la prévision court terme de l'affluence des passagers et (iii) la visualisation de l'affluence des passagers dans les transports en commun.
Les recherches se basent principalement sur l'utilisation de données billettiques fournies par les opérateurs de transports et ont été menées sur trois cas d'études réels, le réseau de métro et de bus de la ville de Rennes, le réseau ferré et de tramway du quartier d'affaire de la Défense à Paris en France, et le réseau de métro de Montréal, Québec au Canada
La sélection de caractéristiques acoustiques appropriées est essentielle dans tout système de traitement de la parole.
Pendant près de 40 ans, la parole a été généralement considérée comme une séquence de signaux quasi-stables (voyelles) séparés par des transitions (consonnes).
Bien qu‟un grand nombre d'études documentent clairement l'importance de la coarticulation, et révèlent que les cibles articulatoires et acoustiques ne sont pas indépendantes du contexte, l‟hypothèse que chaque voyelle présente une cible acoustique qui peut être spécifiée d'une manière indépendante du contexte reste très répandue.
Ce point de vue implique des limitations fortes.
Il est bien connu que les fréquences de formants sont des caractéristiques acoustiques qui présentent un lien évident avec la production de la parole, et qui peuvent participer à la distinction perceptive entre les voyelles.
Par conséquent, les voyelles sont généralement décrites avec des configurations articulatoires statiques représentées par des cibles dans l'espace acoustique, généralement par les fréquences des formants correspondants, représentées dans les plans F1-F2 et F2-F3.
Les consonnes occlusives peuvent être décrites en termes de point d'articulation, représentés par locus (ou locus équations) dans le plan acoustique.
Mais les trajectoires des fréquences de formants dans la parole fluide présentent rarement un état d'équilibre pour chaque voyelle.
Elles varient avec le locuteur, l'environnement consonantique (co-articulation) et le débit de parole (relative à un continuum entre hypo et hyper-articulation).
Le contenu généré dans les médias sociaux comme Twitter permet aux utilisateurs d'avoir un aperçu rétrospectif d'évènement et de suivre les nouveaux développements dès qu'ils se produisent.
Cependant, bien que Twitter soit une source d'information importante, il est caractérisé par le volume et la vélocité des informations publiées qui rendent difficile le suivi de l'évolution des évènements.
L'objectif de cette thèse est de faciliter le suivi d'événement, en fournissant des outils de génération de synthèse adaptés à ce vecteur d'information.
Les défis majeurs sous-jacents à notre problématique découlent d'une part du volume, de la vélocité et de la variété des contenus publiés et, d'autre part, de la qualité des tweets qui peut varier d'une manière considérable.
La tâche principale dans la notification prospective est l'identification en temps réel des tweets pertinents et non redondants.
Le système peut choisir de retourner les nouveaux tweets dès leurs détections où bien de différer leur envoi afin de s'assurer de leur qualité.
L'intuition sous-jacente à notre proposition est que la mesure de similarité à base de word embedding est capable de considérer des mots différents ayant la même sémantique ce qui permet de compenser le non-appariement des termes lors du calcul de la pertinence.
Deuxièmement, l'estimation de nouveauté d'un tweet entrant est basée sur la comparaison de ses termes avec les termes des tweets déjà envoyés au lieu d'utiliser la comparaison tweet à tweet.
Cette méthode offre un meilleur passage à l'échelle et permet de réduire le temps d'exécution.
Troisièmement, pour contourner le problème du seuillage de pertinence, nous utilisons un classificateur binaire qui prédit la pertinence.
L'approche proposée est basée sur l'apprentissage supervisé adaptatif dans laquelle les signes sociaux sont combinés avec les autres facteurs de pertinence dépendants de la requête.
De plus, le retour des jugements de pertinence est exploité pour re-entrainer le modèle de classification.
Enfin, nous montrons que l'approche proposée, qui envoie les notifications en temps réel, permet d'obtenir des performances prometteuses en termes de qualité (pertinence et nouveauté) avec une faible latence alors que les approches de l'état de l'art tendent à favoriser la qualité au détriment de la latence.
Cette thèse explore également une nouvelle approche de génération du résumé rétrospectif qui suit un paradigme différent de la majorité des méthodes de l'état de l'art.
Nous proposons de modéliser le processus de génération de synthèse sous forme d'un problème d'optimisation linéaire qui prend en compte la diversité temporelle des tweets.
Les tweets sont filtrés et regroupés d'une manière incrémentale en deux partitions basées respectivement sur la similarité du contenu et le temps de publication.
Nous formulons la génération du résumé comme étant un problème linéaire entier dans lequel les variables inconnues sont binaires, la fonction objective est à maximiser et les contraintes assurent qu'au maximum un tweet par cluster est sélectionné dans la limite de la longueur du résumé fixée préalablement.
Durant les dernières décennies, l'animation 3D s'est largement intégrée à notre vie quotidienne, que cela soit dans le domaine du jeu vidéo, du cinéma ou du divertissement plus généralement.
Malgré son utilisation rependue, la création d'animation reste réservés à des animateurs expérimentés et n'est pas à la portée de novices.
Par ailleurs, le processus de création d'animation doit respecter une pipeline très stricte : après une première phase de storyboarding et de design des personnages, ceux-ci sont ensuite modélisés, riggés et grossièrement positionnés dans l'espace 3D permettant de créer un premier brouillon d'animation.
Les actions et mouvements des personnages sont ensuite décomposés en keyframes interpolées, constituant l'animation finale.
Le processus de keyframing est difficile et nécessite, de la part des animateurs, beaucoup de temps et d'effort, notamment pour la retouche de chaque courbe d'animation liée à un degré de liberté d'un personnage, et ceci pour chaque action.
Ces dernières doivent aussi être correctement séquencées au cours du temps afin de transmettre les intentions des personnages et leur personnalité.
Certaines méthodes telle que la motion capture rendent le processus de création plus aisé en transférant le mouvement de véritables acteurs aux mouvements de personnages virtuels.
Cependant, ces animations transférées manquent souvent d'expressivité et doivent être rectifiées par des animateurs.
Dans cette thèse, nous introduisons une nouvelle méthode permettant de créer aisément des séquences d'animation 3D à partir d'une base de donnée d'animations individuelles comme point de départ.
En particulier, notre travail se concentre dans l'animation de personnages virtuels reproduisant une histoire jouée avec des objets physiques tels que des figurines instrumentées par des capteurs.
Nous présentons un descripteur de mouvement invariant par translation, rotation et passage à l'échelle permettant à notre système de reconnaître des actions exécutées par l'utilisateur ;
Nous introduisons un nouveau modèle d'animation procédural inférant l'expressivité de la qualité de mouvement de la main en tant qu'Effort de la Laban Temps et Poids.
Enfin, nous étendons le système afin de permettre la manipulation de plusieurs personnages en même temps, en détectant et transférant des interactions entre personnages tout en étant fidèle aux qualités de mouvement du narrateur et permettant aux personnages d'agir selon des comportements prédéfinis, laissant l'utilisateur exprimer sa créativité.
Nous concluons avec une discussion sur les futures directions de recherche.
Les réseaux sociaux occupent une place de plus en plus importante dans notre vie quotidienne et représentent une part considérable des activités sur le web.
Ce succès s'explique par la diversité des services/fonctionnalités de chaque site (partage des données souvent multimédias, tagging, blogging, suggestion de contacts, etc.) incitant les utilisateurs à s'inscrire sur différents sites et ainsi à créer plusieurs réseaux sociaux pour diverses raisons (professionnelle, privée, etc.).
Cependant, les outils et les sites existants proposent des fonctionnalités limitées pour identifier et organiser les types de relations ne permettant pas de, entre autres, garantir la confidentialité des utilisateurs et fournir un partage plus fin des données.
Particulièrement, aucun site actuel ne propose une solution permettant d'identifier automatiquement les types de relations en tenant compte de toutes les données personnelles et/ou celles publiées.
Dans cette étude, nous proposons une nouvelle approche permettant d'identifier les types de relations à travers un ou plusieurs réseaux sociaux.
Notre approche est basée sur un framework orientéutilisateur qui utilise plusieurs attributs du profil utilisateur (nom, age, adresse, photos, etc.).
Pour cela, nous utilisons des règles qui s'appliquent à deux niveaux de granularité : 1) au sein d'un même réseau social pour déterminer les relations sociales (collègues, parents, amis, etc.) en exploitant principalement les caractéristiques des photos et leurs métadonnées, et, 2) à travers différents réseaux sociaux pour déterminer les utilisateurs co-référents (même personne sur plusieurs réseaux sociaux) en étant capable de considérer tous les attributs du profil auxquels des poids sont associés selon le profil de l'utilisateur et le contenu du réseau social.
À chaque niveau de granularité, nous appliquons des règles de base et des règles dérivées pour identifier différents types de relations.
Nous mettons en avant deux méthodologies distinctes pour générer les règles de base.
Pour les relations sociales, les règles de base sont créées à partir d'un jeu de données de photos créées en utilisant le crowdsourcing.
Pour les relations de co-référents, en utilisant tous les attributs, les règles de base sont générées à partir des paires de profils ayant des identifiants de mêmes valeurs.
Quant aux règles dérivées, nous utilisons une technique de fouille de données qui prend en compte le contexte de chaque utilisateur en identifiant les règles de base fréquemment utilisées.
Nous présentons notre prototype, intitulé RelTypeFinder, que nous avons implémenté afin de valider notre approche.
Ce prototype permet de découvrir différents types de relations, générer des jeux de données synthétiques, collecter des données du web, et de générer les règles d'extraction.
Nous décrivons les expériementations que nous avons menées sur des jeux de données réelles et syntéthiques.
Les résultats montrent l'efficacité de notre approche à découvrir les types de relations.
Les systèmes collaboratifs à large échelle, où un grand nombre d'utilisateurs collaborent pour réaliser une tâche partagée, attirent beaucoup l'attention des milieux industriels et académiques.
Dans cette thèse, nous étudions le problème de l'évaluation de la confiance et cherchons à concevoir un modèle de confiance informatique dédiés aux systèmes collaboratifs.
Nos travaux s'organisent autour des trois questions de recherche suivantes.
1. Quel est l'effet du déploiement d'un modèle de confiance et de la représentation aux utilisateurs des scores obtenus pour chaque partenaire ?
Nous avons conçu et organisé une expérience utilisateur basée sur le jeu de confiance qui est un protocole d'échange d'argent en environnement contrôlé dans lequel nous avons introduit des notes deconfiance pour les utilisateurs.
L'analyse détaillée du comportement des utilisateurs montre que : (i) la présentation d'un score de confiance aux utilisateurs encourage la collaboration entre eux de manière significative, et ce, à un niveau similaire à celui de l'affichage du surnom des participants, et (ii) les utilisateurs se conforment au score de confiance dans leur prise de décision concernant l'échange monétaire.
Les résultats suggèrent donc qu'un modèle de confiance peut être déployé dans les systèmes collaboratifs afin d'assister les utilisateurs.
2. Comment calculer le score de confiance entre des utilisateurs qui ont déjà collaboré ?
Nous avons conçu un modèle de confiance pour les jeux de confiance répétés qui calcule les scores de confiance des utilisateurs en fonction de leur comportement passé.
Nous avons validé notre modèle de confiance en relativement à : (i) des données simulées, (ii) de l'opinion humaine et (iii) des données expérimentales réelles.
Nous avons appliqué notre modèle de confiance à Wikipédia en utilisant la qualité des articles de Wikipédia comme mesure de contribution.
Nous avons proposé trois algorithmes d'apprentissage automatique pour évaluer la qualité des articles de Wikipédia : l'un est basé sur une forêt d'arbres décisionnels tandis que les deux autres sont basés sur des méthodes d'apprentissage profond.
3. Comment prédire la relation de confiance entre des utilisateurs qui n'ont pas encore interagi ?
Etant donné un réseau dans lequel les liens représentent les relations de confiance/défiance entre utilisateurs, nous cherchons à prévoir les relations futures.
Nous avons proposé un algorithme qui prend en compte les informations temporelles relatives à l'établissement des liens dans le réseau pour prédire la relation future de confiance/défiance des utilisateurs.
L'algorithme proposé surpasse les approches de la littérature pour des jeux de données réels provenant de réseaux sociaux dirigés et signés
Face au nombre croissant d'utilisateurs novices d'applications informatiques, le besoin d'une assistance efficace est devenu crucial.
Nous proposons d'y répondre à l'aide d'un Agent Conversationnel Assistant (ACA), interface permettant l'usage de la langue naturelle (spontanément employée en cas de problème) et fournissant une présence rassurant les utilisateurs.
Une étude préalable décrit la constitution (combinant recueil et utlilisation de thésaurus) d'un corpus de requêtes, dont le besoin est justifié.
Ce corpus de 11626 phrases est contrasté à d'autres, et nous montrons qu'il couvre le domaine de l'assistance et contient en plus des requêtes relevant du contrôle de l'application et de discussion avec l'agent.
En sortie, les requêtes sont exprimées dans un langage formel (DAFI) dont nous donnons la syntaxe et la sémantique.
La chaîne de traitement est évaluée en comparant annotation manuelle et requêtes produites automatiquement, et l'emploi de méthodes d'apprentissage supervisé pour identifier les activités conversationnelles des requêtes est envisagé.
L'approche suivie est validée par l'intégration d'un ACA au sein d'une application Web de conception musicale collaborative.
Enfin, nous nous intéressons à l'architecture requise pour l'ageTIlt rationnel en charge de définir les réactions à partir des requêtes formelles en DAFT et du modèle de l'application, soulignant le besoin pour celui-ci de disposer d'un modèle cognitif
Ce travail montre que toute modélisation est réductrice de sens.
Le moyen employé est la réalisation d'une base de données factuelle et exhaustive d'après des accords d'entreprise.
Dans la première partie de ce texte, les notions de documentation et de modélisation sont discutées.
Ensuite, les moyens informatiques disponibles pour la réalisation d'applications documentaires sont présentés (systèmes de gestion de bases de données relationnelle et objet, langages de marquage, hypertexte, traitement automatique des langues).
Enfin, des applications documentaires actuelles sont présentées.
La deuxième partie de ce texte rend compte des tentatives de modélisation qui ont été réalisées.
La description des cinq modèles mis au point est précédée par une présentation des accords d'entreprise et par une étude des problèmes structurels inhérents à ces documents.
La troisième partie de ce travail décrit les objectifs et la mise en place de la base ACCORD réalisée à l'aide de PostGres et de HTML.
Enfin, des éléments méthodologiques généraux pour le traitement de corpus de documents administratifs ou légaux sont dégagés.
La conclusion souligne l'intérêt d'un travail collaboratif pour la mise en place de telles applications et sur les transformations dans les pratiques documentaires découlant de l'utilisation des réseaux informatiques.
De plus en plus de médias dans le monde disposent de rubriques ou chroniques dédiées au fact-checking.
Elles visent notamment à vérifier la véracité de propos tenus par des responsables politiques.
Cette pratique revisite celle née aux États-Unis dans les années 1920, qui consistait à vérifier de manière exhaustive et systématique les contenus avant parution.
Ce fact-checking « moderne » incarne une stratégie des rédactions web – en dépit des crises structurelles et conjoncturelles – pour renouer avec la diffusion de contenus mieux vérifiés, ainsi que leur capacité à mettre à profit les outils numériques qui facilitent l'accès à l'information.
À travers une trentaine d'entretiens semi-directifs avec des fact-checkeurs français et l'étude de 300 articles et chroniques issus de sept médias différents, ce travail de recherche analyse dans quelle mesure le fact-checking, en tant que genre journalistique, valorise une démarche crédible, mais révèle aussi, en creux, des manquements dans les pratiques professionnelles.
Il examine, enfin, comment la promotion de contenus plus qualitatifs et l'éducation aux médias sont de nature à placer le fact-checking au cœur des stratégies éditoriales, destinées à regagner la confiance des publics.
Depuis plusieurs années, un nouveau phénomène lié aux données numériques émerge : des données de plus en plus volumineuses, variées et véloces, apparaissent et sont désormais disponibles, elles sont souvent qualifiées de données complexes.
Dans cette thèse, nous focalisons sur un type particulier de données complexes : les séquences complexes d'événements, en posant la question suivante : “comment prédire au plus tôt et influencer l'apparition des événements futurs dans une séquence complexe d'événements ?
Nous proposons un algorithme de fouille de règles d'épisode DEER qui a l'originalité de maîtriser l'horizon d'apparition des événements futurs à travers d'une distance imposée au sein de règles extraites.
Dans un deuxième temps, nous focalisons sur la détection de l'émergence dans un flux d'événements.
Nous proposons l'algorithme EER pour la détection au plus tôt de l'émergence de nouvelles règles.
Pour augmenter la fiabilité de nouvelles règles lorsque leur support est très faible, EER s'appuie sur la similarité entre ces règles et les règles déjà connues.
Enfin, nous étudions l'impact porté par des événements sur d'autres dans une séquence d'événements.
Nous proposons l'algorithme IE qui introduit la notion des “événements influenceurs” et étudie l'influence sur le support, la confiance et la distance à travers de trois mesures d'influence proposées.
Ces travaux sont évalués et validés par une étude expérimentale menée sur un corpus de données réelles issues de blogs
Les êtres humains définissent naturellement leur espace quotidien en unités discrètes.
Par exemple, nous sommes capables d'identifier le lieu où nous sommes (e.g.
Les travaux récents en reconnaissance de lieux sémantiques, visent à doter les robots de capacités similaires.
Nous présentons nos travaux dans le domaine de la reconnaissance de lieux sémantiques.
Premièrement, ils combinent la caractérisation globale d'une image, intéressante car elle permet de s'affranchir des variations locales de l'apparence des lieux, et les méthodes basées sur les mots visuels, qui reposent sur la classification non-supervisée de descripteurs locaux.
Deuxièmement, et de manière intimement reliée, ils tirent parti du flux d'images fourni par le robot en utilisant des méthodes bayésiennes d'intégration temporelle.
Dans un premier modèle, nous ne tenons pas compte de l'ordre des images.
Le mécanisme d'intégration est donc particulièrement simple mais montre des difficultés à repérer les changements de lieux.
Une deuxième version enrichit le formalisme classique du filtrage bayésien en utilisant l'ordre local d'apparition des images.
Nous comparons nos méthodes à l'état de l'art sur des tâches de reconnaissance d'instances et de catégorisation, en utilisant plusieurs bases de données.
Nous étudions l'influence des paramètres sur les performances et comparons les différents types de codage employés sur une même base.
Ces expériences montrent que nos méthodes sont supérieures à l'état de l'art, en particulier sur les tâches de catégorisation.
La Variabilité dans le Big Data se réfère aux données dont la signification change de manière continue.
Par exemple, les données des plateformes sociales et les données des applications de surveillance, présentent une grande variabilité.
Afin de réaliser cet objectif, les data scientists ont besoin (a) de mesures de comparaison de données pour différentes dimensions telles que l'âge pour les utilisateurs et le sujet pour le traffic réseau, et (b) d'algorithmes efficaces pour la détection de différences à grande échelle.
Nous proposons des mesures adaptées à la comparaison de distributions de notes attribuées par les utilisateurs, et des algorithmes efficaces qui permettent, à partir d'une opinion donnée, de trouver les segments qui sont d'accord ou pas avec cette opinion.
L'Explication des Différences s'intéresse à fournir une explication succinte de la différence entre deux ensembles de données (ex., les habitudes d'achat de deux ensembles de clients).
Nous proposons des fonctions de scoring permettant d'ordonner les explications, et des algorithmes qui guarantissent de fournir des explications à la fois concises et informatives.
Enfin, l'Evolution des Différences suit l'évolution d'un ensemble de données dans le temps et résume cette évolution à différentes granularités de temps.
Nous proposons une approche basée sur le requêtage qui utilise des mesures de similarité pour comparer des clusters consécutifs dans le temps.
Nos index et algorithmes pour l'Evolution des Différences sont capables de traiter des données qui arrivent à différentes vitesses et des types de changements différents (ex., soudains, incrémentaux).
L'utilité et le passage à l'échelle de tous nos algorithmes reposent sur l'exploitation de la hiérarchie dans les données (ex., temporelle, démographique).Afin de valider l'utilité de nos tâches analytiques et le passage à l'échelle de nos algorithmes, nous réalisons un grand nombre d'expériences aussi bien sur des données synthétiques que réelles.
Nous montrons que l'Exploration des Différences guide les data scientists ainsi que les novices à découvrir l'opinion de plusieurs segments d'internautes à grande échelle.
L'Explication des Différences révèle la nécessité de résumer les différences entre deux ensembles de donnes, de manière parcimonieuse et montre que la parcimonie peut être atteinte en exploitant les relations hiérarchiques dans les données.
Enfin, notre étude sur l'Evolution des Différences fournit des preuves solides qu'une approche basée sur les requêtes est très adaptée à capturer des taux d'arrivée des données variés à plusieurs granularités de temps.
De même, nous montrons que les approches de clustering sont adaptées à différents types de changement.
Les humanités défient les capacités du numérique depuis 60 ans.
Les années 90 marquent une rupture, énonçant l'espoir d'une interprétation qualitative automatique de données interopérables devenues « connaissances » .
Depuis 2010, une vague de désillusion ternit ces perspectives, le foisonnement des humanités numériques s'intensifie.
Cette méthode vise à co-créer des connaissances historiques.
À l'utopie d'une modélisation des connaissances qualitatives de l'historien, nous préférons l'heuristique pragmatique : l'interprétation de quantifications du corpus suscite l'émergence de nouvelles certitudes et hypothèses.
La plupart des méthodes de traitement automatique des langues (TAL) peuvent être formalisées comme des problèmes de prédiction, dans lesquels on cherche à choisir automatiquement l'hypothèse la plus plausible parmi un très grand nombre de candidats.
Dans ce travail, nous nous intéressons à l'importance du design de l'espace de recherche et étudions l'utilisation de contraintes pour en réduire la taille et la complexité.
Une étude de cas sur les modèles exponentiels pour l'analyse morpho-syntaxique montre paradoxalement que cela peut conduire à d'importantes dégradations des résultats, et cela même quand les contraintes associées sont pertinentes.
Parallèlement, nous considérons l'utilisation de ce type de contraintes pour généraliser le problème de l'apprentissage supervisé au cas où l'on ne dispose que d'informations partielles et incomplètes lors de l'apprentissage, qui apparaît par exemple lors du transfert cross-lingue d'annotations.
Nous étudions deux méthodes d'apprentissage faiblement supervisé, que nous formalisons dans le cadre de l'apprentissage ambigu, appliquées à l'analyse morpho-syntaxiques de langues peu dotées en ressources linguistiques.
En effet, il n'est pas possible de considérer l'ensemble factoriel de tous les réordonnancements possibles, et des contraintes sur les permutations s'avèrent nécessaires.
Nous comparons différents jeux de contraintes et explorons l'importance de l'espace de réordonnancement dans les performances globales d'un système de traduction.
Si un meilleur design permet d'obtenir de meilleurs résultats, nous montrons cependant que la marge d'amélioration se situe principalement dans l'évaluation des réordonnancements plutôt que dans la qualité de l'espace de recherche.
Cette thèse vise à la mise en œuvre et à l'évaluation de techniques d'extraction de relations sémantiques à partir d'un corpus multilingue aligné.
D'abord, nos observations porteront sur la comparaison sémantique d'équivalents traductionnels dans des corpus multilingues alignés.
A partir des équivalences, nous tâcherons d'extraire des "cliques", ou sous-graphes maximaux complets connexes, dont toutes les unités sont en interrelation, du fait d'une probable intersection sémantique.
Ces cliques présentent l'intérêt de renseigner à la fois sur la synonymie et la polysémie des unités, et d'apporter une forme de désambiguïsation sémantique.
Ensuite nous tâcherons de relier ces cliques avec un lexique sémantique (de type Wordnet) afin d'évaluer la possibilité de récupérer pour les unités arabes des relations sémantiques définies pour des unités en anglais ou en français.
Ces relations permettraient de construire automatiquement un réseau utile pour certaines applications de traitement de la langue arabe, comme les moteurs de question
Les divers intervenants qui décrivent, étudient et réalisent un système complexe ont besoin de points de vue adaptés à leurs préoccupations.
Cependant, dans le contexte de l'Ingénierie Dirigée par les Modèles, les moyens pour définir et mettre en œuvre ces points de vue sont, soit trop rigides et inadaptées, soit totalement ad hoc.
De plus, ces différents points de vue sont rarement indépendants les uns des autres.
Dès lors, il faut s'attacher à identifier puis décrire les liens/les correspondances qui existent entre les points de vue pour enfin pouvoir vérifier que les réponses apportées par les différents intervenants constituent un tout cohérent.
Les travaux exposés dans cette thèse permettent la définition de langages dédiés basés sur UML pour les points de vue.
Pour cela, une méthode outillée qui analyse la sémantique des descriptions textuelles des concepts du domaine que l'on souhaite projeter sur UML est proposée afin de faciliter la définition de profils UML.
Pour définir des points de vue basés sur des profils UML, cette thèse propose une méthode qui permet au méthodologiste d'expliciter le point de vue voulu.
Un outil génère ensuite l'outillage qui met en œuvre ce point de vue dans un environnement de modélisation ainsi que le langage dédié correspondant là où la pratique actuelle repose sur une mise en œuvre essentiellement manuelle.
Pour assister l'identification des liens entre points de vue, cette thèse propose là aussi d'analyser la sémantique des descriptions textuelles des concepts des langages utilisés par les points de vue.
Utilisée en complément des heuristiques syntaxiques existantes, l'approche proposée permet d'obtenir de bons résultats lorsque les terminologies des langages analysés sont éloignées.
Un cadre théorique basé sur la théorie des catégories est proposé pour expliciter formellement les correspondances.
Pour utiliser ce cadre, une catégorie pour les langages basés sur UML a été proposée.
Afin de pouvoir également expliciter les correspondances entre les modèles issus de ces langages, la catégorie des ontologies OWL est utilisée.
Une solution est proposée pour caractériser des correspondances plus complexes que la simple équivalence.
Ce cadre théorique permet la définition formelle de relations complexes qui permettront de raisonner sur la cohérence de la description de l'architecture.
Une fois la description de l'architecture intégrée en un tout en suivant les correspondances formalisées, la question de la cohérence est abordée.
Les expérimentations faites sur un cas d'étude concret pour vérifier la cohérence à un niveau syntaxique donnent des résultats pratiques satisfaisants.
Les expérimentations menées sur le même cas pour vérifier la cohérence à un niveau sémantique ne donnent pas de résultats pratiques satisfaisants.
Notre recherche porte sur le rôle de la crédibilité des offres dans les sites d'e-commerce et les facteurs qui la déterminent.
294 personnes ont été interrogées à exprimer leur perception sur la crédibilité des offres de livres qui ont été composées en variant la source et les signes de leur réputation.
La recherche s'inscrit dans une approche quasi-expérimentale qui vise à mesurer l'impact de ces facteurs sur la crédibilité perçue mais aussi à mesurer l'influence de cette dernière sur l'intérêt accordé à l'offre et en conséquence sur l'intention d'achat.
Les analyses sont faites en se basant sur un modèle d'équation structurelle (SEM).
Les résultats témoignent le rôle pivot de la crédibilité qui médiatise l'effet des signaux de la qualité de l'offre mis sur l'intérêt qu'elle soulève.
Cet intérêt, à son tour, convoie l'impact de la crédibilité sur l'intention d'achat.
Nous trouvons également l'effet significatif de la confiance dans la source ainsi que l'intérêt à la catégorie sur la crédibilité.
L'effet de la confiance est positif et remarquable tandis que celui de l'intérêt à la catégorie reste négatif.
Enfin, ces résultats nous permettent de conclure l'importance de la crédibilité : soigner la crédibilité de l'offre est le chemin au renforcement de l'intérêt du consommateur ainsi qu'aux performances des ventes.
L'intelligence artificielle est la discipline de recherche d'imitation ou de remplacement de fonctions cognitives humaines.
À ce titre, l'une de ses branches s'inscrit dans l'automatisation progressive du processus de programmation.
Il s'agit alors de transférer de l'intelligence ou, à défaut de définition, de transférer de la charge cognitive depuis l'humain vers le système, qu'il soit autonome ou guidé par l'utilisateur.
Dans le cadre de cette thèse, nous considérons les conditions de l'évolution depuis un système guidé par son utilisateur vers un système autonome, en nous appuyant sur une autre branche de l'intelligence artificielle : l'apprentissage artificiel.
Pour nos travaux, les requêtes sont exprimées en français, et les actions sont désignées par les commandes correspondantes dans un langage de programmation (ici, R ou bash).
L'apprentissage du système est effectué à l'aide d'un ensemble d'exemples constitué par les utilisateurs eux-mêmes lors de leurs interactions.
Ce sont donc ces derniers qui définissent, progressivement, les actions qui sont appropriées pour chaque requête, afin de rendre le système de plus en plus autonome.
Nous avons collecté plusieurs ensembles d'exemples pour l'évaluation des méthodes d'apprentissage, en analysant et réduisant progressivement les biais induits.
Le protocole que nous proposons est fondé sur l'amorçage incrémental des connaissances du système à partir d'un ensemble vide ou très restreint.
Cela présente l'avantage de constituer une base de connaissances très représentative des besoins des utilisateurs, mais aussi l'inconvénient de n'aquérir qu'un nombre très limité d'exemples.
Nous utilisons donc, après examen des performances d'une méthode naïve, une méthode de raisonnement à partir de cas : le raisonnement par analogie formelle.
Nous montrons que cette méthode permet une précision très élevée dans les réponses du système, mais également une couverture relativement faible.
L'extension de la base d'exemples par analogie est explorée afin d'augmenter la couverture des réponses données.
La durée d'exécution de l'approche par analogie, déjà de l'ordre de la seconde, souffre beaucoup de l'extension de la base et de l'approximation.
Enfin, l'assistant opérationnel incrémental fondé sur le raisonnement analogique a été testé en condition incrémentale simulée, afin d'étudier la progression de l'apprentissage du système au cours du temps.
On en retient que le modèle permet d'atteindre un taux de réponse stable après une dizaine d'exemples vus en moyenne pour chaque type de commande.
Bien que la performance effective varie selon le nombre total de commandes considérées, cette propriété ouvre sur des applications intéressantes dans le cadre incrémental du transfert depuis un domaine riche (la langue naturelle) vers un domaine moins riche (le langage de programmation).
Cette thèse présente le développement d'un framework formel pour la représentation des Langues de Signes (LS), les langages des communautés Sourdes, dans le cadre de la construction d'un système de reconnaissance automatique.
Les LS sont de langues naturelles, qui utilisent des gestes et l'espace autour du signeur pour transmettre de l'information.
De plus, lors du discours les signeurs utilisent plusieurs parties de leurs corps (articulateurs) simultanément, ce qui est difficile à capturer avec un système de notation écrite.
Cette situation difficulté leur représentation dans de taches de Traitement Automatique du Langage Naturel (TALN).
Pour représenter les propriétés à vérifier, une logique multi-modale a été choisi : la Logique Propositionnelle Dynamique (PDL).
Cette logique a été originalement crée pour la spécification de programmes.
De manière plus précise, PDL permit d'utilise des opérateurs modales comme [a] et " a ", représentant " nécessité " et " possibilité ", respectivement.
Une variante particulaire a été développée pour les LS : la PDL pour Langue de Signes (PDLSL), qui est interprété sur des STE représentant des corpus.
Avec PDLSL, chaque articulateur du corps (comme les mains et la tête) est vu comme un agent indépendant ; cela veut dire que chacun a ses propres actions et propositions possibles, et qu'il peux les exécuter pour influencer une posture gestuelle.
L'utilisation du framework proposé peut aider à diminuer deux problèmes importantes qui existent dans l'étude linguistique des LS : hétérogénéité des corpus et la manque des systèmes automatiques d'aide à l'annotation.
De ce fait, un chercheur peut rendre exploitables des corpus existants en les transformant vers des STE.
Globalement, le système reçoit des vidéos LS et les transforme dans un STE valide.
Ensuite, un module fait de la vérification formelle sur le STE, en utilisant une base de données de formules crée par un expert en LS.
Le produit de ce processus, est une annotation qui peut être corrigé par des utilisateurs humains, et qui est utilisable dans des domaines d'études tels que la linguistique.
Le lexique est aujourd'hui reconnu comme un composant essentiel de tout système de Traitement Automatique des Langues, et l'utilisation de ressources lexicales est en pleine explosion.
Les travaux dédiés à la résolution des ambiguïtés de rattachement prépositionnel, une des tâches les plus délicates à résoudre en analyse syntaxique automatique, utilisent massivement des informations lexicales acquises à partir de corpus portant sur la langue générale.
Du côté de l'évaluation, l'efficacité des lexiques est en général testé sur un seul corpus, et la question liée à la nécessité d'adapter le lexique au type de corpus demeure peu explorée.
Dans notre étude, nous construisons deux types de lexiques pour le français : l'un est dérivé d'un dictionnaire existant (
Nous faisons émerger des corpus des caractéristiques susceptibles d'éclairer les variations observées dans les résultats de la désambiguïsation.
La nécessaire adaptation des ressources au type de corpus est rendue plus manifeste encore lorsque nous confrontons l'utilité du lexique acquis à partir du corpus journalistique à un lexique contenant des informations spécifiques à chacun des cinq corpus de test.
Cette thèse aborde différents aspects de la modélisation de la microstructure du marché et des problèmes de Market Making, avec un accent particulier du point de vue du praticien.
Nous souhaitons améliorer la connaissance du LOB pour la communauté de la recherche, proposer de nouvelles idées de modélisation et développer des applications pour les Market Makers.
Nous remercions en particuler l'équipe Automated Market Making d'avoir fourni la base de données haute-fréquence de très bonne qualité et une grille de calculs puissante, sans laquelle ces recherches n'auraient pas été possible.
Le Chapitre 1 présente la motivation de cette recherche et reprend les principaux résultats des différents travaux.
Le Chapitre 2 se concentre entièrement sur le LOB et vise à proposer un nouveau modèle qui reproduit mieux certains faits stylisés.
A travers cette recherche, non seulement nous confirmons l'influence des flux d'ordres historiques sur l'arrivée de nouveaux, mais un nouveau modèle est également fourni qui réplique beaucoup mieux la dynamique du LOB, notamment la volatilité réalisée en haute et basse fréquence.
Dans le Chapitre 3, l'objectif est d'étudier les stratégies de Market Making dans un contexte plus réaliste.
La prédiction à haute fréquence avec la méthode d'apprentissage profond est étudiée dans le Chapitre 4.
De nombreux résultats de la prédiction en 1- étape et en plusieurs étapes ont retrouvé la non-linéarité, stationarité et universalité de la relation entre les indicateurs microstructure et le changement du prix, ainsi que la limitation de cette approche en pratique.
La reconnaissance de la langue des signes française (LSF) comme une langue à part entière en 2005 a créé un besoin important de développement d'outils informatiques pour rendre l'information accessible au public sourd.
Dans cette perspective, cette thèse a pour cadre la modélisation linguistique pour un système de génération de la LSF.
Nous présentons dans un premier temps les différentes approches linguistiques ayant pour but la description linguistique des langues des signes (LS).
Nous présentons ensuite les travaux effectués en informatique pour les modéliser.
Dans un deuxième temps, nous proposons une approche permettant de prendre en compte les caractéristiques linguistiques propres au LS tout en respectant les contraintes d'un processus de formalisation.
En étudiant des liens entre des fonctions sémantiques et leurs formes observées dans les corpus LSF, nous avons identifié plusieurs règles de production.
Nous présentons finalement le fonctionnement des règles comme étant un système susceptible de modéliser un énoncé entier en LSF.
Cette thèse de doctorat se place dans le cadre de la reconnaissance de la parole dans des documents audio.
Le but de ce travail est d'adapter les principes de l'identification audio pour la reconnaissance de la parole ainsi que concevoir et développer des techniques d'identification robustes.
Les systèmes d'identification audio par empreinte (audio fingerprinting) sont conçus pour l'indexation d'extraits de musique mais ne traitent pas des spécificités du signal de parole.
Dans un premier temps, différentes méthodes d'identification audio par empreinte sont étudiées ainsi qu'un premier travail d'adaptation à la reconnaissance de la parole.
De nouveaux types de sousempreinte basés sur des paramètres usuels de la parole sont alors proposés.
Dans un second temps, les différents types de variabilité du signal de parole sont décrits ainsi que les principaux paramètres de représentation acoustique du signal de parole.
La robustesse de différents types de sous-empreinte à la variabilité extrinsèque et à la variabilité intrinsèque est évaluée.
En présence de perturbations liées à l'environnement et aux conditions de transmission du signal de parole (CTIMIT), un type de sous-empreinte issu de l'identification audio s'avère alors le plus robuste.
L'aide ambiante à la personne (ambiant assisted living) a pour objectif d'accompagner le vieillissement de la population.
Cela s'instancie notamment par les maisons intelligentes (smart homes), équipées de multiples capteurs connectés, dont un des objectifs est de prolonger le maintien à domicile des personnes âgées.
Le manuscrit s'attache d'abord à introduire la problématique générale des maisons intelligentes, avant de présenter plus avant les trois sous-thématiques qui font plus particulièrement l'objet de la thèse, à savoir la reconnaissance d'activités, la confidentialité et les systèmes de dialogue.
La reconnaissance d'activités consiste à déterminer les activités courantes d'une personne ou d'un groupe de personnes, à partir des données (brutes) des capteurs dont est équipée la maison.
On peut citer comme exemple la détection de la chute d'une personne.
Une maison intelligent repose typiquement sur l'internet des objets (Internet of Things, ou IoT).
De nombreuses données sont produites, pouvant contenir des informations privées ou sensibles.
Une partie de ces données doit être partagée avec l'extérieur, ce qui peut poser des problèmes de confidentialité.
Enfin, pour interragir avec la maison intelligente, un moyen naturel pour l'utilisateur est d'utiliser le dialogue, sujet traité par les systèmes de dialogue.
Ce travail de thèse propose des contributions sur ces trois versants, la plupart basées sur l'apprentissage profond.
Le concept de systèmes informatisés complexes rassemble tous les systèmes constitués d'un grand nombre de composantes inter-connectées et gérées par ordinateur.
La configuration et la gestion de ces systèmes passe par une multitude de tâches critiques à leur bon fonctionnement et leur évolution.
Nous proposons aussi une extension au langage de définition des domaines de planification automatique PDDL afin de modéliser les connaissances des experts du domaine d'application sous forme de méthodes de décomposition des tâches qui serviront à guider l'algorithme de planification HTN.
Enfin, nous proposons des critères d'évaluation pour les systèmes en mixed-initiative qui servent de base à la discussion du système MIP.
La présente étude s'intéresse à la terminologie médicale wolof, envisagée dans le cadre de la sémantique lexicale.
Nous y abordons des unités terminologiques de type collocation.
Ce choix a un lien direct avec notre cadre théorique d'analyse, la Théorie Sens-Texte (TST), qui, à l'heure actuelle, propose l'un des meilleurs outils de description de la collocation avec les Fonctions Lexicales (FL).
Les collocations constituent des indices de spécialisation en plus d'avoir un comportement lexicosyntaxique singulier.
Nous les analysons, sur la base d'un corpus scientifique compilé, afin d'avoir une perception holistique de la cooccurrence lexicale.
Les langues, en Afrique, sont souvent peu dotées du point de vue terminologique.
Il s'agit dans cette recherche de s'appuyer sur le modèle d'analyse du lexique qui prend en compte trois paramètres clés : le sens, la forme et la combinatoire, afin de faire une description du lexique wolof qui, à terme, permet d'établir des principes de terminologisation.
La portée traductive du travail réside dans l'approche interlinguistique que nous adoptons pour élaborer notre liste de termes.
Le versant opératoire de l'étude est la constitution d'un début de corpus médical trilingue (anglais-français
La perspective traductive de la terminologie a permis de relever différents procédés de création et de restitutions de termes médicaux (anglais et français) en wolof.
Au cours des 10 dernières années, les téléphones mobiles ont considérablement évolué~ : l'apparition des écrans tactiles et la disparition des claviers physiques ont changé la façon dont nous interagissons au quotidien avec ces dispositifs.
Pourtant, la saisie de texte demeure toujours une tâche importante avec des activités telles que la prise de notes, l'envoi de messages textuels ou la communication sur les réseaux sociaux.
Cependant, bien que l'utilisation du tactile ait de grands avantages en termes de dynamicité de l'interface et de personnalisation, de tels dispositifs ne sont pas forcément accessibles pour tous.
En effet, pour 39 millions de personnes dans le monde touchés par la cécité, les difficultés sont nombreuses avec ces dispositifs du fait que toutes les interactions se fassent au moyen de l'écran dépourvu de tout repère tactile : les interactions avec le dispositif sont possibles, mais elles sont souvent laborieuses et répétitives, ce qui implique alors une charge cognitive trop importante et des problèmes de précision, de mémorisation, et de fatigue.
Dans ce travail de doctorat, nous nous sommes intéressés à l'accessibilité de la saisie de texte dans le contexte de la déficience visuelle.
Dans un premier temps, nous avons étudié les différentes solutions actuellement existantes pour les utilisateurs en situation de déficience.
La problématique principale de ces recherches était d'améliorer la saisie de texte pour permettre aux utilisateurs d'avoir de meilleures performances de saisie.
Pour cela, nous avons conçu une solution déductive, appelée DUCK.
Cette solution permet aux déficients visuels, de saisir rapidement du texte sans se soucier de la précision de leurs frappes.
Un système à base de connaissances linguistiques permet à chaque fin de mot de déduire le mot que l'utilisateur a voulu saisir.
Ce dispositif a ensuite été testé auprès d'un échantillon de déficients visuels afin de vérifier l'efficacité de notre solution.
La suite des travaux s'est ensuite focalisée sur deux principales optimisations.
La première concerne les listes de mots.
Nous avons étudié et comparé différentes interactions pour permettre à l'utilisateur de naviguer et choisir des mots de façon efficace et simple lorsqu'il est face à une liste de mots proposée par un système de prédiction ou de déduction.
La seconde se focalise sur la saisie des mots couramment utilisés.
Nous avons également mené une étude comparative entre différentes propositions d'interaction permettant de saisir un mot court de façon efficace sans avoir recours au système de déduction, trop coûteux en temps pour ce type de mots.
Enfin, nous terminons ce projet de doctorat par une étude longitudinale qui présente le clavier DUCK avec l'intégration de ces optimisations.
Ce nouveau système a été utilisé par des déficients visuels sur une période de deux semaines afin d'étudier l'efficacité du clavier une fois ce dernier pris en main sur le long terme.
Cette thèse a pour objet d'interroger sur la relation entre les troubles du langage et leur dénomination en orthophonie, comme par exemple en français « dysphasie, troubles spécifiques du langage écrit, aphasie, difficultés du langage écrit, retard de langage » ...
La terminologie employée par les orthophonistes pour décrire les pathologies rencontrées chez leurs patients a intégré celle des courants théoriques en évolution, et ce pour apporter des nuances nécessaires à la précision du diagnostic orthophonique.
Pour décrire l'inconstance de ce lien entre le terme diagnostique et la réalité de la pathologie étiquetée, l'auteure s'est appuyée sur des considérations épistémologiques, lexicologiques et terminologiques.
Le bilan orthophonique permettant l'établissement de ce diagnostic est suivi par convention d'un compte-rendu de bilan orthophonique (CRBO), reflet de la langue de spécialité et révélateur de la représentation de ces troubles.
435 comptes-rendus authentiques ont été explorés au moyen d'une analyse descriptive lexicologique et terminologique semi-automatique grâce à un codage XML, produisant ainsi une « photographie » de l'utilisation des termes concernant l'ensemble des pathologies dont s'occupe l'orthophoniste.
L'analyse a permis de distinguer deux niveaux terminologiques (un traitant de la nature du trouble, et un de sa forme), illustrant les nuances nécessaires aux orthophonistes dans des syntagmes que l'on peut qualifier de collocationnels.
La dernière phase d'analyse de ces données a permis de tisser la trame d'une proposition de classification orthophonique, la COFOP (Classification Orthophonique FOndée sur la Pratique clinique).
Cette thèse contribue à la compréhension d'un phénomène émergent et complexe : la transformation/smartisation du système de service public urbain.
Cela comprend la combinaison de différents secteurs de services tels que les transports, le tourisme,les taxes, etc.
Nous établissons trois piliers pour la Science du Service : Système de service (SS), Innovation de Service (IS) et Logiques Institutionnelles de Service (LIS).
Nous commençons par proposer une méthode basée sur l'analyse sémantique latente (LSA), l'analyse factorielle (FA), le text mining et la théorie enracinée. Il s'agit de mettre en évidence de manière inductive et interdisciplinaire, 30 années d'évolution de la structure intellectuelle de SS, IS et LIS, de 1986 à 2015.
La pensée complexe, comme cadre intégrateur, nous a permis de mettre en évidence le contenu, ou les effets, de la transformation institutionnelle, en considérant le tout et les parties d'un système de service public urbain qui devient smart.
L'architecture de la recherche est basée principalement sur la théorie enracinée, l'observation, les archives, l'étude de cas longitudinale multiniveaux (i.e. locale et nationale) via le modèle dialogique (Parmentier-Cajaiba &amp; Avenier, 2013) et le paradigme épistémologique constructiviste pragmatique (PECP).
Aussi, nous utilisons deux hypothèses ontologiques du travail : l'ontologie relationnelle et l'ontologie du devenir.
Du point de vue théorique, notre recherche contribue au affinement théorique de la littérature sur SS, IS et LIS.
Cette définition est accompagné de trois modèles heuristiques résultants d'une analyse par le processus et par le contenu de la transformation et de la théorie enracinée (Gioia &amp; Chittipeddi, 1991 ; Gioia et al., 2013).
Le premier modèle heuristique contribue à la compréhension du processus de travail institutionnel pour la co-création d'arrangements institutionnels (i.e. standard, normes, les ressources frontières, APIs) entre deux logiques collectives antagonistes et complémentaires (Morin, 2005 ; Smets &amp; Jarzabkowski, 2013 ; Greenwood et al., 2017) : la logique de service dominant du marché (Lusch &amp; Nambisan, 2015 ; Vargo &amp; Lusch, 2016) et la logique du service public (Osborne et al.,2015 ; Osborne, 2017).
Le deuxième modèle heuristique met en évidence les composantes structurales d'un système de service public qui devient smart.
Le troisième modèle heuristique souligne les moteurs et les freins de la transformation institutionnelle et structurelle.
Tout d'abord perçue comme un phénomène marginal, presque un accident en langue, on considère aujourd'hui que la polysémie fait partie intégrante des systèmes linguistiques.
Aussi, à partir de la distinction établie par G. Kleiber (1999), nous considérons deux grands courants selon le rapport établi entre signification, référence et polysémie.
Le premier décrit la polysémie en termes de sens premier référentiel dont sont dérivés des sens secondaires (courant objectiviste).
Le second l'analyse en termes de potentiel sémantique aréférentiel à partir duquel est obtenu l'ensemble des sens du polysème par spécialisation ou enrichissement contextuel(le) (courant constructiviste).
Sur la base des travaux de D. Tuggy (1993), nous déclinons les représentations de la signification des mots à sens multiples le long d'un continuum homonymie-polysémie-multifacialité-indétermination, selon les degrés d'enracinement, de saillance, et les possibilités d'accessibilité et d'activation des différents composants (valeur schématique et élaborations sémantiques).
Et, nous mettons ainsi en avant certaines des régularités organisatrices propres aux représentations sémanticoconceptuelles des polysèmes nominaux, ainsi qu'une typologie des sens polysémiques.
En grammaire cognitive, nous considérons qu'il s'agit d'un processus non modulaire, compositionnel et dynamique.
L'analyse de syntagmes nominaux du type Adj-N et N-Adj révèle en outre certaines régularités dans l'activation des sens polysémiques.
Ces régularités sont liées au contexte (place et fonction de l'adjectif par rapport au substantif recteur) et au contexte extralinguistique.
Dans le domaine de l'apprentissage machine, les réseaux de neurones profonds sont devenus la référence incontournable pour un très grand nombre de problèmes.
Ces systèmes sont constitués par un assemblage de couches, lesquelles réalisent des traitements élémentaires, paramétrés par un grand nombre de variables.
À l'aide de données disponibles pendant une phase d'apprentissage, ces variables sont ajustées de façon à ce que le réseau de neurones réponde à la tâche donnée.
Il est ensuite possible de traiter de nouvelles données.
Si ces méthodes atteignent les performances à l'état de l'art dans bien des cas, ils reposent pour cela sur un très grand nombre de paramètres, et donc des complexités en mémoire et en calculs importantes.
De fait, ils sont souvent peu adaptés à l'implémentation matérielle sur des systèmes contraints en ressources.
Par ailleurs, l'apprentissage requiert de repasser sur les données d'entraînement plusieurs fois, et s'adapte donc difficilement à des scénarios où de nouvelles informations apparaissent au fil de l'eau.
Dans cette thèse, nous nous intéressons dans un premier temps aux méthodes permettant de réduire l'impact en calculs et en mémoire des réseaux de neurones profonds.
Nous proposons dans un second temps des techniques permettant d'effectuer l'apprentissage au fil de l'eau, dans un contexte embarqué.
Entrant dans l'ère de la digitalisation, il est important de transformer les données en connaissances et utiliser celles-ci pour fournir des pistes d'amélioration industrielle.
La transformation de données en connaissances pour optimiser la production est aujourd'hui un défi industriel majeur.
Nous adresserons les problématiques de l'ordonnancement, de la planification et de l'équilibrage de charges sur et entre les lignes de production.
D'un point de vue système, ces aspects sont aujourd'hui considérés comme portés par l'ERP (Enterprise Resource Planning).
Les ERP sont des outils présentant une forte rigidité dans leur structure et dans leur fonctionnement, imposant cette rigidité aux organisations.
Notre partenaire la société iFAKT, experte en équilibrage de charges nous accompagnera dans ce projet de thèse.
Cette thèse aborde deux axes de travail principaux : l'un portant sur l'intégration du Traitement Automatique du Langage Naturel, l'Apprentissage Automatique et l'outil d'équilibrage de charges et l'autre les actions à entreprendre par rapport à cette intégration.
Pour la mise en place de ces deux grandes activités, nous contribuerons à la création de méthodologies couplant les techniques et outils cités ci-dessus dans le cadre de l'industrie 4.0
Ce sujet est à la croisée de plusieurs domaines (interaction Humain-Robot, planification automatique, apprentissage artificiel).
Il s'agit maintenant d'aller au delà de ces premiers résultats obtenus au cours de mon M2R et de trouver un cadre générique pour la programmation de « cobots » (robots collaboratifs) en milieu industriel.
L'approche cobotique consiste à ce qu'un opérateur humain, en tant qu'expert métier directement impliqué dans la réalisation des tâches en ligne, apprenne au robot à effectuer de nouvelles tâches et à utiliser le robot comme assistant « agile » .
Notre thèse se propose d'observer et décrire l'acquisition des marques de la référence en production écrite chez les élèves du CE2 au CM2 (de 9 à 11 ans) dans des textes narratifs.
Cette étude permet de dresser une cartographie des compétences des élèves en matière de continuité référentielle en fonction du niveau de classe.
Beaucoup de travaux en linguistique se sont intéressés à la question de la référence du point de vue de l'analyse et de la réception, mais très peu du point de vue de la production.
En didactique du français langue première, les études qui analysent l'expression référentielle dans les textes d'élèves analysent un échantillon limité de textes.
D'autre part, les études psycholinguistiques qui s'intéressent à la référence d'un point de vue développemental sont plus nombreuses pour l'oral que pour l'écrit et concernent généralement les enfants les plus jeunes (de 0 à 3 ans).
Dans notre thèse, notre étude s'inscrit dans une perspective de progressivité.
Nous considérons la mise en place des compétences rédactionnelles des élèves en étudiant les procédés auxquels ils ont recourt pour introduire et maintenir les référents dans un texte narratif.
Nous nous demandons notamment si les élèves du CE2 au CM2 privilégient des relations anaphoriques (Reichler-Béguelin, 1988) ou s'ils construisent des chaînes de références (Schnedecker, 1997), et si le niveau de classe et la nature des référents influent sur le type de résolution choisie.
Bonnemaison, 2014) contenant des expressions référentielles exigeant l'introduction de référents de diverses natures (humain/non humain, évènement, indication spatio-temporelle).
Le langage XML est devenu un standard de représentation et d'échange de données à travers le web.
Le but de la réplication de données au sein de différents sites est de minimiser le temps d'accès à ces données partagées.
Cependant, différents problèmes sont liés à la sécurisation de ces données.
Le but de cette thèse est de proposer des modèles de contrôles d'accès XML qui prennent en compte les droits de lecture et de mise-à-jour et qui permettent de surmonter les limites des modèles qui existent.
Nous considérons les langages XPath et XQuery Update Facility pour la formalisation des requêtes d'accès et des requêtes de mise-à-jour respectivement.
L'autre partie de cette thèse est consacrée à l'étude pratique de nos propositions.
Nous présentons notre système appelé SVMAX qui met en oeuvre nos solutions, et nous conduisons une étude expérimentale basée sur une DTD réelle pour montrer son efficacité.
Plusieurs systèmes de bases de données natives (systèmes de BDNs) ont été proposés récemment qui permettent une manipulation efficace des données XML en utilisant la plupart des standards du W3C.
Nous montrons que notre système SVMAX peut être intégré facilement et efficacement au sein d'un large ensemble de systèmes de BDNs.
A nos connaissances, SVMAX est le premier système qui permet la sécurisation des données XML conformes à des DTDs arbitraires (récursives ou non) et ceci en moyennant un fragment significatif de XPath et une classe riche d'opérations de mise-à-jour XML
Au cours du premier millénaire avant notre ère, les bibliothèques, qui apparaissent avec le besoin d'organiser la conservation des textes, sont immédiatement confrontées aux difficultés de l'indexation.
Le titre apparaît alors comme une première solution, permettant d'identifier rapidement chaque type d'ouvrage et éventuellement de discerner des ouvrages thématiquement proches.
Alors que dans la Grèce Antique, les titres ont une fonction peu informative, mais ont toujours pour objectif d'identifier le document, l'invention de l'imprimerie à caractères mobiles (Gutenberg, XVème siècle) a entraîné une forte augmentation du nombre de documents, offrant désormais une diffusion à grande échelle.
Mais comment quelques mots peuvent-ils avoir une si grande influence ?
Quelles fonctions les titres doivent-ils remplir en ce début du XXIème siècle ?
Comment générer automatiquement des titres respectant ces fonctions ?
Le titrage automatique de documents textuels est avant tout un des domaines clés de l'accessibilité des pages Web (standards W3C) tel que défini par la norme proposée par les associations sur le handicap.
Côté lecteur, l'objectif est d'augmenter la lisibilité des pages obtenues à partir d'une recherche sur mot-clé(s) et dont la pertinence est souvent faible, décourageant les lecteurs devant fournir de grands efforts cognitifs.
Côté producteur de site Web, l'objectif est d'améliorer l'indexation des pages pour une recherche plus pertinente.
D'autres intérêts motivent cette étude (titrage de pages Web commerciales, titrage pour la génération automatique de sommaires, titrage pour fournir des éléments d'appui pour la tâche de résumé automatique,).
Afin de traiter à grande échelle le titrage automatique de documents textuels, nous employons dans cette étude des méthodes et systèmes de TALN (Traitement Automatique du Langage Naturel).
Alors que de nombreux travaux ont été publiés à propos de l'indexation et du résumé automatique, le titrage automatique demeurait jusqu'alors discret et connaissait quelques difficultés quant à son positionnement dans le domaine du TALN.
Nous soutenons dans cette étude que le titrage automatique doit pourtant être considéré comme une tâche à part entière.
Après avoir défini les problématiques liées au titrage automatique, et après avoir positionné cette tâche parmi les tâches déjà existantes, nous proposons une série de méthodes permettant de produire des titres syntaxiquement corrects selon plusieurs objectifs.
En particulier, nous nous intéressons à la production de titres informatifs, et, pour la première fois dans l'histoire du titrage automatique, de titres accrocheurs.
Notre système TIT', constitué de trois méthodes (POSTIT, NOMIT et CATIT), permet de produire des ensembles de titres informatifs dans 81% des cas et accrocheurs dans 78% des cas.
Nous vivons dans un monde où une grande quantité de données est généré en continu.
Les données sont différentes d'une simple information numérique, mais viennent dans de nombreux format.
Cependant, les données prisent isolément n'ont aucun sens.
Mais quand ces données sont reliées ensemble on peut en extraire de nouvelles informations.
De plus, les données sont sensibles au temps.
La façon la plus précise et efficace de représenter les données est de les exprimer en tant que flux de données.
Si les données les plus récentes ne sont pas traitées rapidement, les résultats obtenus ne sont pas aussi utiles.
Ainsi, un système parallèle et distribué pour traiter de grandes quantités de flux de données en temps réel est un problème de recherche important.
Dans cette thèse nous étudions l'opération de jointure sur des flux de données, de manière parallèle et continue.
Nous séparons ce problème en deux catégories.
Les grammaires locales constituent un formalisme de description de constructions linguistiques et sont communément représentées sous la forme de graphes orientés.
Utilisées pour la recherche et l'extraction de motifs dans un texte, elles trouvent leurs limites dans le traitement de variations non décrites ou fautives ainsi que dans la capacité à accéder à des connaissances exogènes, c'est-à-dire des informations à extraire, au cours de l'analyse, de ressources externes à la grammaire et qui peuvent s'avérer utiles pour normaliser, enrichir, valider ou mettre en relation les motifs reconnus.
Premièrement, en ajoutant des fonctions arbitraires à satisfaire, appelées fonctions étendues, qui ne sont pas prédéfinies à l'avance et qui sont évaluées en dehors de la grammaire.
Le travail présenté se divise en trois parties : dans un premier temps, nous étudions les principes concernant la construction des grammaires locales étendues.
Nous présentons ensuite la mise en œuvre d'un moteur d'analyse textuelle implémentant le formalisme proposé.
Enfin, nous étudions quelques applications pour l'extraction de l'information dans des textes bien formés et des textes bruités.
Nous nous focalisons sur le couplage des ressources externes et des méthodes non-symboliques dans la construction de nos grammaires en montrant la pertinence de cette approche pour dépasser les limites des grammaires locales classiques
La compréhension automatique de vidéos devrait impacter notre vie de tous les jours dans de nombreux domaines comme la conduite autonome, les robots domestiques, la recherche et le filtrage de contenu, les jeux vidéo, la défense ou la sécurité.
Le nombre de vidéos croît plus vite chaque année, notamment sur les plateformes telles que YouTube, Twitter ou Facebook.
L'analyse automatique de ces données est indispensable pour permettre à de nouvelles applications de voir le jour.
L'analyse vidéo, en particulier en environnement non contrôlé, se heurte à plusieurs problèmes comme la variabilité intra-classe (les échantillons d'un même concept paraissent très différents) ou la confusion inter-classe (les exemples provenant de deux activités distinctes se ressemblent).
Bien que ces difficultés puissent être traitées via des algorithmes d'apprentissage supervisé, les méthodes pleinement supervisées sont souvent synonymes d'un coût d'annotation élevé.
Dépendant à la fois de la tâche à effectuer et du niveau de supervision requis, la quantité d'annotations nécessaire peut être prohibitive.
Dans le cas de la localisation d'actions, une approche pleinement supervisée nécessite les boîtes englobantes de l'acteur à chaque image où l'action est effectuée.
Le coût associé à l'obtention d'un telle annotation empêche le passage à l'échelle et limite le nombre d'échantillons d'entraînement.
Cette thèse adresse les problèmes évoqués ci-dessus dans le contexte de deux tâches, la classification et la localisation d'actions humaines.
La classification consiste à reconnaître l'activité effectuée dans une courte vidéo limitée à la durée de l'action.
La localisation a pour but de détecter en temps et dans l'espace des activités effectuées dans de plus longues vidéos.
Notre approche pour la classification d'actions tire parti de l'information contenue dans la posture humaine et l'intègre avec des descripteurs d'apparence et de mouvement afin d'améliorer les performances.
Notre approche pour la localisation d'actions modélise l'évolution temporelle des actions à l'aide d'un réseau récurrent entraîné à partir de suivis de personnes.
Enfin, la troisième méthode étudiée dans cette thèse a pour but de contourner le coût prohibitif des annotations de vidéos et utilise le regroupement discriminatoire pour analyser et combiner différents types de supervision.
Le Traitement Automatique de la Parole (TAP) s'intéresse de plus en plus et progresse techniquement en matière d'étendue de vocabulaire, de gestion de complexité morphosyntaxique, de style et d'esthétique de la parole humaine.
L'Affective Computing tend également à intégrer une dimension « émotionnelle » dans un objectif commun au TAP visant à désambiguïser le langage naturel et augmenter la naturalité de l'interaction personne-machine.
Dans le cadre de la robotique sociale, cette interaction est modélisée dans des systèmes d'interaction, de dialogue, qui tendent à engendrer une dimension d'attachement dont les effets doivent être éthiquement et collectivement contrôlés.
Or la dynamique du langage humain situé met à mal l'efficacité des systèmes automatiques.
L'hypothèse de cette thèse propose dans la dynamique des interactions, il existerait une « glu socio-affective » qui ferait entrer en phases synchroniques deux individus dotés chacun d'un rôle social impliqué dans une situation/contexte d'interaction.
Cette thèse s'intéresse à des dynamiques interactionnelles impliquant spécifiquement des processus altruistes, orthogonale à la dimension de dominance.
Cette glu permettrait ainsi de véhiculer les événements langagiers entre les interlocuteurs, en modifiant constamment leur relation et leur rôle, qui eux même viennent à modifier cette glu, afin d'assurer la continuité de la communication.
Un Magicien d'Oz – EmOz – est utilisé afin de contrôler les primitives vocales comme unique support langagier d'un robot majordome d'un habitat intelligent interagissant avec des personnes âgées en isolement relationnel.
Cet isolement relationnel permet méthodologiquement d'appréhender les dimensions de la glu socio-affective, en introduisant une situation contrastive dégradée de la glu.
Les effets des primitives permettraient alors d'observer les comportements de l'humain à travers des indices multimodaux.
Le système automatisé qui découlera des données et des analyses de cette étude permettrait alors d'entraîner les personnes à solliciter pleinement leurs mécanismes de construction relationnelle, afin de redonner l'envie de communiquer avec leur entourage humain.
Les analyses du corpus EEE recueilli montrent une évolution de la relation à travers différents indices interactionnels, temporellement organisés.
Ces paramètres visent à être intégrés dans une perspective de système de dialogue incrémental – SASI.
Les prémisses de ce système sont proposées dans un prototype de reconnaissance de la parole dont la robustesse ne dépendra pas de l'exactitude du contenu langagier reconnu, mais sur la reconnaissance du degré de glu, soit de l'état relationnel entre les locuteurs.
Ainsi, les erreurs de reconnaissance tendraient à être compensées par l'intelligence socio-affective adaptative de ce système dont pourrait être doté le robot.
Le cancer de la prostate est le plus courant en France et la 4ième cause de mortalité par cancer.
Les méthodes diagnostics de références actuel sont souvent insuffisantes pour détecter et localiser précisément une lésion.
Néanmoins, l'interprétation visuelle des multiples séquences IRM n'est pas aisée.
Dans ces conditions, un fort intérêt s'est porté sur les systèmes d'aide au diagnostic dont le but est d'assister le radiologue dans ses décisions.
Cette thèse présente la conception d'un système d'aide à la détection (CADe) dontl'approche finale est de fournir au radiologue une carte de probabilité du cancer dans la zone périphérique de la prostate.
Ce CADe repose sur une base d'images IRM multi-paramétrique (IRM-mp) 1.5T de types
Cette thèse met l'accent sur la détection des cancers mais aussisur leur caractérisation dans le but de fournir une carte de probabilité corrélée au grade de Gleason des tumeurs.
Nous avons utilisé une méthode d'apprentissage de dictionnaires permettant d'extraire de nouvelles caractéristiques descriptives dont l'objectif est de discriminer chacun des cancers.
Ces dernières sont ensuite utilisées par deux classifieurs : régression logistique et séparateur à vaste marge (SVM), permettant de produire une carte de probabilité du cancer.
Nous avons concentré nos efforts sur la discrimination des cancers agressifs (Gleason&gt;6) et fourni une analyse de la corrélationentre probabilités et scores de Gleason.
Les résultats montrent de très bonnes performances de détection des cancers agressifs et l'analyse des probabilités conclue sur une forte capacité du système à séparer les cancers agressifs du reste des tissus mais ne permet pas aisément de distinguer chacundes grades de cancer
La prise de conscience des conséquences du réchauffement climatique a permis de lancer un mouvement de réduction de l'utilisation d'énergie.
L'électricité utilisée dans les bâtiments représente une part importante de la consommation d'énergie et doit donc être utilisée de manière efficace.
Pour cela, il est nécessaire de pouvoir mesurer et suivre la consommation électrique de chaque appareil au sein d'un bâtiment.
Depuis 30 ans, une méthode de suivi des consommations électriques, Non Intrusive Load Monitoring (NILM), propose à partir d'un unique compteur mesurant la consommation totale du bâtiment, de déterminer la contribution de chaque appareil électrique.
Cette méthode est basée sur un algorithme de désagrégation des consommations électriques et permet de s'affranchir de l'utilisation d'un compteur de mesure pour chaque appareil électrique du bâtiment.
Cette thèse aborde les problèmes algorithmiques que présente le NILM.
Ainsi, les principales difficultés du NILM sont : (i) la standardisation de la formulation, (ii) le caractère mal-posé du problème (perte d'information), (iii) les connaissances insuffisantes sur les signaux et (iv) l'implémentation d'un algorithme d'apprentissage.
L'objectif principale de cette thèse est de traiter le NILM dans le cadre des grands bâtiments (commerciaux, bureaux, industriels) en utilisant des mesures hautes fréquences du courant et de la tension.
Cependant les maisons individuelles et leurs propres types d'appareils électriques ne sont pas exclus de cette étude.
Cette thèse est structurée en deux grandes parties.
Dans une première partie nous abordons le problème du manque de connaissance des signaux de consommation électriques, à la fois ceux des grands bâtiments et ceux des différents appareils utilisés.
La littérature concernant le NILM est principalement orienté sur l'étude des mesures basses fréquences de consommations dans les maisons.
Le manque de données de consommations disponibles est également un frein pour le développement du NILM.
Pour répondre à cela nous développons un modèle génératif permettant de simuler des données hautes fréquences de courant électrique de bâtiments.
A partir d'un nombre limité de données réelles nous réalisons des simulations de bâtiments que nous partageons dans la base de données SHED.
Dans une seconde partie, nous abordons le problème de la séparation de source.
Grâce à nos résultats d'analyse et par manque de données, nous traitons ce problème à l'aide de techniques d'apprentissage non-supervisées.
Pour proposons une nouvelle méthode appartenant à la famille des factorisations de matrice appelée Independent-Variation Matrix Factorization (IVMF), qui permet d'exprimer une matrice d'observation de courant comme le produit de deux matrices : les signatures et les activations.
IVMF est le premier algorithme décrit pour le traitement du NILM dans le cadre de données hautes fréquences et de grands bâtiments.
Enfin, nous montrons que IVMF atteint de meilleurs résultats pour le problème du NILM que des méthodes classiques de séparation de source comme l'Analyse en Composantes Indépendantes ou encore la Factorisation de Matrice Semi Non-négative.
Depuis 2006, les algorithmes d'apprentissage profond qui s'appuient sur des modèles comprenant plusieurs couches de représentations ont pu surpasser l'état de l'art dans plusieurs domaines.
Les modèles profonds peuvent être très efficaces en termes du nombre de paramètres nécessaires pour représenter des opérations complexes.
Bien que l'entraînement des modèles profonds ait été traditionnellement considéré comme un problème difficile, une approche réussie a été d'utiliser une étape de pré-entraînement couche par couche, non supervisée, pour initialiser des modèles profonds supervisés.
Tout d'abord, l'apprentissage non-supervisé présente de nombreux avantages par rapport à la généralisation car il repose uniquement sur des données non étiquetées qu'il est facile de trouver.
Deuxièmement, la possibilité d'apprendre des représentations couche par couche, au lieu de toutes les couches à la fois, améliore encore la généralisation et réduit les temps de calcul.
Cependant, l'apprentissage profond pose encore beaucoup de questions relatives à la consistance de l'apprentissage couche par couche, avec de nombreuses couches, et à la difficulté d'évaluer la performance, de sélectionner les modèles et d'optimiser la performance des couches.
Dans cette thèse, nous examinons d'abord les limites de la justification variationnelle actuelle pour l'apprentissage couche par couche qui ne se généralise pas bien à de nombreuses couches et demandons si une méthode couche par couche peut jamais être vraiment consistante.
Nous constatons que l'apprentissage couche par couche peut en effet être consistant et peut conduire à des modèles génératifs profonds optimaux.
Pour ce faire, nous introduisons la borne supérieure de la meilleure probabilité marginale latente (BLM upper bound), un nouveau critère qui représente la log
Nous prouvons que la maximisation de ce critère pour chaque couche conduit à une architecture profonde optimale, à condition que le reste de l'entraînement se passe bien.
Bien que ce critère ne puisse pas être calculé de manière exacte, nous montrons qu'il peut être maximisé efficacement par des auto-encodeurs quand l'encodeur du modèle est autorisé à être aussi riche que possible.
Cela donne une nouvelle justification pour empiler les modèles entraînés pour reproduire leur entrée et donne de meilleurs résultats que l'approche variationnelle.
En outre, nous donnons une approximation calculable de la BLM upper bound et montrons qu'elle peut être utilisée pour estimer avec précision la log-vraisemblance finale des modèles.
Nous proposons une nouvelle méthode pour la sélection de modèles couche par couche pour les modèles profonds, et un nouveau critère pour déterminer si l'ajout de couches est justifié.
Quant à la difficulté d'entraîner chaque couche, nous étudions aussi l'impact des métriques et de la paramétrisation sur la procédure de descente de gradient couramment utilisée pour la maximisation de la vraisemblance.
Nous montrons que la descente de gradient est implicitement liée à la métrique de l'espace sous-jacent et que la métrique Euclidienne peut souvent être un choix inadapté car elle introduit une dépendance sur la paramétrisation et peut entraîner une violation de la symétrie.
Pour pallier ce problème, nous étudions les avantages du gradient naturel et montrons qu'il peut être utilisé pour restaurer la symétrie, mais avec un coût de calcul élevé.
Nous proposons donc qu'une paramétrisation centrée peut rétablir la symétrie avec une très faible surcharge computationnelle.
Cette thèse porte sur la synthèse de séquences de motion capture avec des modèles statistiques.
Notre point de départ réside dans deux problèmes principaux rencontrés lors de la synthèse de données de motion capture, assurer le réalisme des positions et des mouvements, et la gestion de la grande variabilité dans ces données.
Nous décrivons d'abord une variante de modèles de Markov cachés contextuels pour gérer la variabilité dans les données en conditionnant les paramètres des modèles à une information contextuelle supplémentaire telle que l'émotion avec laquelle un mouvement a été effectué.
Nous proposons ensuite une variante d'une méthode de l'état de l'art utilisée pour réaliser une tâche de synthèse de mouvement spécifique appelée Inverse Kinematics, où nous exploitons les processus gaussiens pour encourager le réalisme de chacune des postures d'un mouvement généré.
Nos résultats montrent un certain potentiel de ces modèles statistiques pour la conception de systèmes de synthèse de mouvement humain.
Pourtant, aucune de ces technologies n'offre la flexibilité apportée par les réseaux de neurones et la récente révolution de l'apprentissage profond et de l'apprentissage Adversarial que nous abordons dans la deuxième partie.
La deuxième partie de la thèse décrit les travaux que nous avons réalisés avec des réseaux de neurones et des architectures profondes.
Nos travaux s'appuient sur la capacité des réseaux neuronaux récurrents à traiter des séquences complexes et sur l'apprentissage Adversarial qui a été introduit très récemment dans la communauté du Deep Learning pour la conception de modèles génératifs performants pour des données complexes, notamment images.
Nous proposons une première architecture simple qui combine l'apprentissage Adversarial et des autoencodeurs de séquences, qui permet de mettre au point des systèmes performants de génération aléatoire de séquences réalistes de motion capture.
A partir de cette architecture de base, nous proposons plusieurs variantes d'architectures neurales conditionnelles qui permettent de concevoir des systèmes de synthèse que l'on peut contrôler dans une certaine mesure en fournissant une information de haut niveau à laquelle la séquence générée doit correspondre, par exemple l'émotion avec laquelle une activité est réalisée.
Le traitement informatique des objets qui nous entourent, naturels ou créés par l'homme, demande toujours de passer par une phase de traduction en entités traitables par des programmes.
Le choix de ces représentations abstraites est toujours crucial pour l'efficacité des traitements et est le terrain d'améliorations constantes.
Mais il est un autre aspect émergeant : le lien entre l'objet à représenter et "sa" représentation n'est pas forcément bijectif !
Ainsi la nature ambiguë de certaines structures discrètes pose problème pour la modélisation ainsi que le traitement et l'analyse à l'aide d'un programme informatique.
Le langage dit ``naturel'', et sous sa forme en particulier de représentation textuelle, en est un exemple.
Le sujet de cette thèse consiste à explorer cette question, que nous étudions à l'aide de méthodes combinatoires et géométriques.
Ces méthodes nous permettent de formaliser le problème d'extraction d'information dans des grands réseaux d'entités ainsi que de construire des représentations géométriques utiles pour le traitement du langage naturel.
Dans un premier temps, nous commençons par démontrer des propriétés combinatoires des graphes de séquences intervenant de manière implicite dans les modèles séquentiels.
Ces propriétés concernent essentiellement le problème inverse de trouver une séquence représentant un graphe donné.
Les algorithmes qui en découlent nous permettent d'effectuer une comparaison expérimentale de différents modèles séquentiels utilisés en modélisation du langage.
Dans un second temps, nous considérons une application pour le problème d'identification d'entités nommées.
A la suite d'une revue de solutions récentes, nous proposons une méthode compétitive basée sur la comparaison de structures de graphes de connaissances et moins coûteuse en annotations d'exemples dédiés au problème.
Nous établissons également une analyse expérimentale d'influence d'entités à partir de relations capitalistiques.
Cette analyse suggère l'élargissement du cadre d'application de l'identification d'entités à des bases de connaissances de natures différentes.
Ces solutions sont aujourd'hui utilisées au sein d'une librairie logicielle dans le secteur bancaire.
Ensuite, nous développons une étude géométrique de représentations de mots récemment proposées, au cours de laquelle nous discutons une conjecture géométrique théoriquement et expérimentalement.
Cette étude suggère que les analogies du langage sont difficilement transposables en propriétés géométriques, et nous amène a considérer le paradigme de la géométrie des distances afin de construire de nouvelles représentations.
Enfin, nous proposons une méthodologie basée sur le paradigme de la géométrie des distances afin de construire de nouvelles représentations de mots ou d'entités.
Nous proposons des algorithmes de résolution de ce problème à grande échelle, qui nous permettent de construire des représentations interprétables et compétitives en performance pour des tâches extrinsèques.
Plus généralement, nous proposons à travers ce paradigme un nouveau cadre et piste d'explorations pour la construction de représentations en apprentissage machine.
Cette thèse consiste à analyser conjointement des signaux de mouvement des yeux et d'électroencéphalogrammes (EEG) multicanaux acquis simultanément avec des participants effectuant une tâche de lecture de recueil d'informations afin de prendre une décision binaire-le texte est-il lié à un sujet ou non ?
La recherche d'informations textuelles n'est pas un processus homogène dans le temps-ni d'un point de vue cognitif, ni en termes de mouvement des yeux.
Au contraire, ce processus implique plusieurs étapes ou phases, telles que la lecture normale, le balayage, la lecture attentive-en termes d'oculométrie-et la création et le rejet d'hypothèses, la confirmation et la décision-en termes cognitifs.
Dans une première contribution, nous discutons d'une méthode d'analyse basée sur des chaînes semi-markoviennes cachées sur les signaux de mouvement des yeux afin de mettre en évidence quatre phases interprétables en termes de stratégie d'acquisition d'informations : lecture normale, lecture rapide, lecture attentive et prise de décision.
Dans une deuxième contribution, nous lions ces phases aux changements caractéristiques des signaux EEG et des informations textuelles.
En utilisant une représentation en ondelettes des EEG, cette analyse révèle des changements de variance et de corrélation des coefficients inter-canaux, en fonction des phases et de la largeur de bande.
En utilisant des méthodes de plongement des mots, nous relions l'évolution de la similarité sémantique au sujet tout au long du texte avec les changements de stratégie.
Dans une troisième contribution, nous présentons un nouveau modèle dans lequel les EEG sont directement intégrés en tant que variables de sortie afin de réduire l'incertitude des états.
Cette nouvelle approche prend également en compte les aspects asynchrones et hétérogènes des données.
L'enseignement de la prosodie en cours de français langue étrangère (FLE) et l'utilisation d'un logiciel de parole, qui permet de visualiser la mélodie des apprenants et de celle d'un modèle francophone, afin de les comparer, font l'objet de cette recherche.
Deux tentatives d'expérimentation de la correction de la prosodie de parole, des années soixante, prenant en compte la visualisation, sont évoquées à titre de rappel.
Je me suis intéressée à la situation actuelle de l'enseignement de la prosodie du FLE et aux possibilités qu'offre l'outil numérique, afin de l'appréhender.
J'ai voulu, entre autres, me rendre compte comment les apprenants réagissaient à un tel outil, et surtout si son utilisation apportait des résultats probants.
Le travail avec un manuel, l'entraînement en autonomie avec un ordinateur, avec des explications collectives de l'enseignante, mais parfois aussi avec des explications individuelles, ont été proposés aux apprenants.
Les deux premières expérimentations (pilote et générale) ont été effectuées auprès d'un public migrant, la troisième a été faite dans un laboratoire de langues en autonomie, et la dernière, avec des explications, en petits groupes.
Les productions du modèle francophone et celles des apprenants ont été analysées.
Les enquêtes concernant l'utilisation de l'outil numérique et la visualisation WinPitch (WP) et WinPitch Language Teaching and Learning (WP LTL) ont montré que la plupart des apprenants ont apprécié le travail avec ces logiciels.
Pour les deux dernières expérimentations, menées dans un contexte universitaire, j'ai constitué deux groupes de travail : l'un expérimental avec WP, et l'autre de contrôle.
Il s'est avéré que les étudiants du groupe expérimental ont eu de meilleurs résultats, en comparaison avec ceux du groupe de contrôle.
Pour la dernière expérimentation, des explications concernant la 'grammaire prosodique', en l'occurrence celle, basée sur le modèle de Ph.
Les résultats obtenus ont permis de valider les hypothèses de travail.
Elles montrent aussi que l'utilisation de visualisation WP, accompagnée des explications de l'enseignant.e, sera bénéfique et justifiée en classe de langue, pour améliorer l'expression orale en français de façon consciente.
Le fil rouge de cette thèse est l'étude des processus de Hawkes.
Ces processus ponctuels décryptent l'inter-causalité qui peut avoir lieu entre plusieurs séries d'événements.
Concrètement, ils déterminent l'influence qu'ont les événements d'une série sur les événements futurs de toutes les autres séries.
Par exemple, dans le contexte des réseaux sociaux, ils décrivent à quel point l'action d'un utilisateur, par exemple un Tweet, sera susceptible de déclencher des réactions de la part des autres.
Le premier chapitre est une brève introduction sur les processus ponctuels suivie par un approfondissement sur les processus de Hawkes et en particulier sur les propriétés de la paramétrisation à noyaux exponentiels, la plus communément utilisée.
Dans le chapitre suivant, nous introduisons une pénalisation adaptative pour modéliser, avec des processus de Hawkes, la propagation de l'information dans les réseaux sociaux.
Cette pénalisation est capable de prendre en compte la connaissance a priori des caractéristiques de ces réseaux, telles que les interactions éparses entre utilisateurs ou la structure de communauté, et de les réfléchir sur le modèle estimé.
Notre technique utilise des pénalités pondérées dont les poids sont déterminés par une analyse fine de l'erreur de généralisation.
Ensuite, nous abordons l'optimisation convexe et les progrès réalisés avec les méthodes stochastiques du premier ordre avec réduction de variance.
Le quatrième chapitre est dédié à l'adaptation de ces techniques pour optimiser le terme d'attache aux données le plus couramment utilisé avec les processus de Hawkes.
De plus, de telles fonctions comportent beaucoup de contraintes linéaires qui sont fréquemment violées par les algorithmes classiques du premier ordre, mais, dans leur version duale ces contraintes sont beaucoup plus aisées à satisfaire.
Ainsi, la robustesse de notre algorithme est d'avantage comparable à celle des méthodes du second ordre dont le coût est prohibitif en grandes dimensions.
Enfin, le dernier chapitre présente une nouvelle bibliothèque d'apprentissage statistique pour Python 3 avec un accent particulier mis sur les modèles temporels.
Appelée tick, cette bibliothèque repose sur une implémentation en C++ et les algorithmes d'optimisation issus de l'état de l'art pour réaliser des estimations très rapides dans un environnement multi-cœurs.
Publiée sur Github, cette bibliothèque a été utilisée tout au long de cette thèse pour effectuer des expériences.
La cohérence est une propriété qui caractérise un texte comme un tout unifié et interprétatif.
Les apprenants chinois de français langue étrangère (FLE) peuvent avoir de la difficulté à produire un texte cohérent en français.
Cette thèse vise donc à analyser la gestion de la cohérence textuelle de ces apprenants, et plus précisément, les emplois des connecteurs ainsi que la résolution des anaphores et des chaînes de référence dans les productions écrites d'étudiants chinois de niveau intermédiaire et avancé de FLE en France.
Les deux cohortes reçoivent les mêmes consignes et réalisent deux tâches de rédaction (narration et argumentation) en utilisant le logiciel de traitement de texte GenoGraphiX qui enregistre et reconstitue le processus d'écriture.
Dans le domaine de la néologie, différentes approches méthodologiques ont été développées pour la détection et l'extraction de néologismes sémantiques. Ces approches utilisent des stratégies telles que la désambiguïsation sémantique et la modélisation thématique, mais il n'existe aucun système complet de détection de néologismes sémantiques.
Ainsi, nous proposons dans cette thèse le développement des algorithmes qui permettent d'identifier et d'extraire les néologismes sémantiques au moyen de méthodes statistiques,d'extraction d'information et d'apprentissage automatique.
La méthodologie proposée est basée sur le traitement du processus de détection et d'extraction en tant que problème de classification. Il consiste à analyser la proximité des thèmes entre le champ sémantique de la signification principale d'un terme et son contexte.
Pour la construction du système nous avons étudié cinq méthodes de classification automatique supervisée et trois modèles pour la génération de représentations vectorielles de mots par apprentissage profonde.
Le corpus d'analyse est composé de néologismes sémantiques du domaine informatique appartenant à la base de données de l'Observatoire de Néologie de l'Université Pompeu Fabra, enregistrés de 1989 à 2015.
Nous utilisons ce corpus pour évaluer les différentes méthodes mises en oeuvre par le système : classification automatique, extraction de mots à partir de contextes courts et génération de listes de mots similaires.
Cette première approche méthodologique cherche à établir un cadre de référence en termes de détection et d'extraction de néologismes sémantiques.
Les systèmes stochastiques à information partielle permettent de représenter de nombreux systèmes dont les paramètres sont inconnus et dont le fonctionnement dépend de facteurs en dehors de notre contrôle.
Dans cette thèse, nous étudions plusieurs problèmes liés à ces systèmes.
Le premier est la diagnosticabilité, c'est-à-dire la capacité de décider si un évènement particulier s'est produit.
Le second est la classification qui est la capacité de décider à partir d'une trace d'une exécution quel système l'a produite.
Enfin, nous nous intéressons aux garanties que l'on peut avoir quand on apprend les probabilités de transition d'un système stochastique.
A travers les siècles, les institutions financières ont façonné le paysage financier et influencé l'activité économique.
L'objectif de cette thèse est de mettre en évidence, d'un point de vue théorique, les limites fondamentales des institutions modernes et d'en déduire les implications concernant le futur rôle de ces institutions.
Le premier chapitre propose un analyse des Chambres de Compensation.
A la suite de la crise financière de 2008, les autorités financières à travers le monde ont mis en place des réglementations imposant la compensation centrale sur la plupart des produits dérivés.
Nous montrons que la compensation centrale nécessite un plus grand niveau de liquidité que la compensation bilatérale.
Le second chapitre présente un modèle d'apprentissage en temps continu censé
Le dernier chapitre introduit et analyse le Lightning Network qui est un réseau de paiement basé sur la Blockchain.
Il permet aux utilisateurs de transférer de la valeur de façon instantanée sans avoir recours à un tiers de confiance.
Nous discutons des implications à propos de la structure de ce réseau de paiement ainsi que de sa capacité à prendre une place importante dans le paysage financier.
Le trafic maritime est le principal contributeur des bruits sous-marins anthropique : depuis les années 1970, l'augmentation du trafic maritime hauturier a provoqué dans certaines zones une augmentation du bruit ambiant de plus de 10 dB.
En réponse à cette préoccupation, la Directive Cadre pour la Stratégie pour le Milieu Marin (DCSMM) recommande un suivi acoustique.
Peu d'études s'intéressent à l'activité côtière et aux bruits rayonnés par les petites embarcations ainsi qu'à leurs conséquences sur la faune marine alors que ces environnements côtiers sont les pourvoyeurs de 41.7 % des services écosystémiques produits par les océans.
A mi-chemin entre le monde académique et le monde industriel, le travail présenté aux différents questions scientifiques et industrielles sur la thématique du trafic côtier, en termes de l'étude de son influence dans le paysage acoustique et de capacité à détecter et classifier les embarcations côtières.
En l'absence d'information sur le trafic maritime côtier, un protocole d'identification visuelle par traitement d'images GoPro ® produisant les mêmes données que l'AIS (position, vitesse, taille et type d'embarcation) est proposé et permet la création de carte du trafic maritime sur un disque de 1.6km de rayon.
D'un point de vue acoustique, le trafic est caractérisé par deux descripteurs acoustiques, le SPL lié à la distance du bateau le plus proche et l'ANL caractérisant le nombre de bateaux dans un disque de 500 m de rayon.
Le suivi spatio-temporel de ces descripteurs permet d'identifier l'impact du trafic maritime dans le paysage acoustique des environnements côtiers.
La détection et la classification sont réalisées après caractérisation individuelle du bruit par un ensemble de paramètres acoustiques et par l'utilisation d'algorithmes d'apprentissage supervisé.
Un protocole spécifique pour la création de l'arborescence de classification est proposé par comparaison des données acoustiques aux caractéristiques physiques et contextuelle de chaque bateau.
Les travaux présentés sont illustrés sur la flottille d'embarcations côtières présente dans la baie de Calvi (Corse) durant la saison estivale.
Aujourd'hui, la communauté scientifique a l'opportunité de partager des connaissances et d'accéder à de nouvelles informations à travers les documents publiés et stockés dans les bases en ligne du web.
Dans ce contexte, la valorisation des données disponibles reste un défi majeur pour permettre aux experts de les réutiliser et les analyser afin de produire de la connaissance du domaine.
Pour être valorisées, les données pertinentes doivent être extraites des documents puis structurées.
Nos travaux s'inscrivent dans la problématique de la capitalisation des données expérimentales issues des articles scientifiques, sélectionnés dans des bases en ligne, afin de les réutiliser dans des outils d'aide à la décision.
Les mesures expérimentales (par exemple, la perméabilité à l'oxygène d'un emballage ou le broyage d'une biomasse) réalisées sur différents objets d'études (par exemple, emballage ou procédé de bioraffinerie) sont représentées sous forme de relations n-aires dans une Ressource Termino-Ontologique (RTO).
La RTO est modélisée pour représenter les relations n-aires en associant une partie terminologique et/ou linguistique aux ontologies afin d'établir une distinction claire entre la manifestation linguistique (le terme) et la notion qu'elle dénote (le concept).
La thèse a pour objectif de proposer une contribution méthodologique d'extraction automatique ou semi-automatique d'arguments de relations n-aires provenant de documents textuels afin de peupler la RTO avec de nouvelles instances.
De manière précise, nous cherchons, dans un premier temps, à extraire des termes, dénotant les concepts d'unités de mesure, réputés difficiles à identifier du fait de leur forte variation typographique dans les textes.
Après la localisation de ces derniers par des méthodes de classification automatique, les variants d'unités sont identifiés en utilisant des mesures d'édition originales.
L'hyperlien est une ressource qui permet de relier entre eux des documents textuels et sonores ou des images fixes ou animées, publiés sur le World Wide Web.
Il est ainsi devenu le moyen standard par lequel les internautes, les éditeurs de contenus ou les moteurs de recherche élaborent et diffusent des informations.
De prime abord, le rapport entre l'hyperlien et le droit de l'Union européenne peut paraître diffus.
Pourtant, l'hyperlien soulève une série d'enjeux juridiques qui justifie de poser la question de son appréhension en le droit de l'UE.
On peut donc s'attendre à ce que le droit de l'UE favorise, voire protège la création d'hyperliens.
D'un autre côté, certains usages de l'hyperlien sont décriés.
L'hyperlien participe également activement à la dissémination sur Internet de contenus illicites ou préjudiciables, en opposition avec le droit de l'UE.
Au vu de ces enjeux, l'ambition de cette étude est d'évaluer en quoi, et dans quelle mesure, le droit de l'UE permet de délimiter de manière cohérente la liberté de lier et ses limites.
Afin de le découvrir, notre thèse s'emploie dans un premier temps à démontrer l'émergence d'une liberté de lier au regard du droit européen de la propriété intellectuelle.
L'appréhension de l'hyperlien par le droit de l'UE achoppe toutefois sur le terrain de la cohérence.
Afin d'y remédier, des propositions normatives visant à rétablir la stabilité du droit jalonnent cette étude.
Cette thèse fournit ainsi un socle de compréhension et de réflexion, commun à l'Union européenne, quant à l'application du droit à l'hyperlien.
Développer des ressources lexico-sémantiques pour le Traitement Automatique des Langues Naturelles est un enjeu majeur du domaine.
Ces ressources explicitant notamment des connaissances que seuls les humains possèdent, ont pour but de permettre aux applications de TALNune compréhension de texte assez fine et complète.
De nouvelles approches populaires de construction de ces dernières impliquant l'externalisation ouverte (crowdsourcing) émergent en TALN.
Dans ce travail de recherche, nous prenons comme exemple d'étude le réseau lexico
Ce système se base principalement sur l'enrichissement du réseau par l'inférence et l'annotation de nouvelles relations à partir de celles existantes, ainsi que l'extraction de règles d'inférence permettant de (re)générer une grande partie du réseau.
L'objectif de cette thèse, menée dans un cadre industriel, est d'apparier des contenus textuels médiatiques.
Plus précisément, il s'agit d'apparier à des articles de presse en ligne des vidéos pertinentes, pour lesquelles nous disposons d'une description textuelle.
Notre problématique relève donc exclusivement de l'analyse de matériaux textuels, et ne fait intervenir aucune analyse d'image ni de langue orale.
Surviennent alors des questions relatives à la façon de comparer des objets textuels, ainsi qu'aux critères mobilisés pour estimer leur degré de similarité.
L'un de ces éléments est selon nous la similarité thématique de leurs contenus, autrement dit le fait que deux documents doivent relater le même sujet pour former une paire pertinente.
Ces problématiques relèvent du domaine de la recherche d'information (ri), dans lequel nous nous ancrons principalement.
Le système d'appariement développé dans cette thèse distingue donc différentes étapes qui se complètent.
Dans un premier temps, l'indexation des contenus fait appel à des méthodes de traitement automatique des langues (tal) pour dépasser la représentation classique des textes en sac de mots.
Ensuite, deux scores sont calculés pour rendre compte du degré de similarité entre deux contenus : l'un relatif à leur similarité thématique, basé sur un modèle vectoriel de ri;
L'évaluation des performances du système a elle aussi fait l'objet de questionnements dans ces travaux de thèse.
Les contraintes imposées par les données traitées et le besoin particulier de l'entreprise partenaire nous ont en effet contraints à adopter une alternative au protocole classique d'évaluation en ri, le paradigme de Cranfield.
Depuis peu, émerge une réelle dynamique de constitution et de diffusion de corpus d'écrits scolaires, notamment francophones.
Ces corpus, qui appuient les travaux en didactique de l'écriture, sont souvent de taille restreinte et peu diffusés.
Des corpus longitudinaux, c'est-à-dire réalisant le suivi d'une cohorte d'élèves et permettant de s'intéresser à la progressivité des apprentissages, n'existent pas à ce jour pour le français.
Par ailleurs, bien que le traitement automatique des langues (TAL) ait outillé des corpus de natures très diverses, peu de travaux se sont intéressés aux écrits scolaires.
Ce nouveau champ d'application représente un défi pour le TAL en raison des spécificités des écrits scolaires, et particulièrement les nombreux écarts à la norme qui les caractérisent.
Les outils proposés à l'heure actuelle ne conviennent donc pas à l'exploitation de ces corpus.
Il y a donc un enjeu pour le TAL à développer des méthodes spécifiques.
Cette thèse présente deux apports principaux.
D'une part, ce travail a permis la constitution d'un corpus d'écrits scolaires longitudinal (CP-CM2), de grande taille et numérisé, le corpus Scoledit.
Par « constitution » , nous entendons le recueil, la numérisation et la transcription des productions, l'annotation des données linguistiques et la diffusion de la ressource ainsi constituée.
D'autre part, ce travail a donné lieu à l'élaboration d'une méthode d'exploitation de ce corpus, appelée approche par comparaison, qui s'appuie sur la comparaison entre la transcription des productions et une version normalisée de ces productions pour produire des analyses.
Cette méthode a nécessité le développement d'un aligneur de formes, appelé AliScol, qui permet de mettre en correspondance les formes produites par l'élève et les formes normalisées.
Cet outil représente un premier niveau d'alignement à partir duquel différentes analyses linguistiques ont été menées (lexicales, morphographiques, graphémiques).
La conception d'un aligneur en graphèmes, appelé AliScol_Graph, a été nécessaire pour conduire une étude sur les graphèmes.
La mémoire de travail peut être définie comme la capacité à stocker temporairement et à manipuler des informations de toute nature.
Par exemple, imaginez que l'on vous demande d'additionner mentalement une série de nombres.
Afin de réaliser cette tâche, vous devez garder une trace de la somme partielle qui doit être mise à jour à chaque fois qu'un nouveau nombre est donné.
La mémoire de travail est précisément ce qui permettrait de maintenir (i.e. stocker temporairement) la somme partielle et de la mettre à jour (i.e. manipuler).
Dans cette thèse, nous proposons d'explorer les implémentations neuronales de cette mémoire de travail en utilisant un nombre restreint d'hypothèses.
Pour ce faire, nous nous plaçons dans le contexte général des réseaux de neurones récurrents et nous proposons d'utiliser en particulier le paradigme du reservoir computing.
Ce type de modèle très simple permet néanmoins de produire des dynamiques dont l'apprentissage peut tirer parti pour résoudre une tâche donnée.
Dans ce travail, la tâche à réaliser est une mémoire de travail à porte (gated working memory).
Le modèle reçoit en entrée un signal qui contrôle la mise à jour de la mémoire.
Lorsque la porte est fermée, le modèle doit maintenir son état de mémoire actuel, alors que lorsqu'elle est ouverte, il doit la mettre à jour en fonction d'une entrée.
Dans notre approche, cette entrée supplémentaire est présente à tout instant, même lorsqu'il n'y a pas de mise à jour à faire.
En d'autres termes, nous exigeons que notre modèle soit un système ouvert, i.e. un système qui est toujours perturbé par ses entrées mais qui doit néanmoins apprendre à conserver une mémoire stable.
Dans la première partie de ce travail, nous présentons l'architecture du modèle et ses propriétés, puis nous montrons sa robustesse au travers d'une étude de sensibilité aux paramètres.
Celle-ci montre que le modèle est extrêmement robuste pour une large gamme de paramètres.
Peu ou prou, toute population aléatoire de neurones peut être utilisée pour effectuer le gating.
Par ailleurs, après apprentissage, nous mettons en évidence une propriété intéressante du modèle, à savoir qu'une information peut être maintenue de manière entièrement distribuée, i.e. sans être corrélée à aucun des neurones mais seulement à la dynamique du groupe.
Plus précisément, la mémoire de travail n'est pas corrélée avec l'activité soutenue des neurones ce qui a pourtant longtemps été observé dans la littérature et remis en cause récemment de façon expérimentale.
Ce modèle vient confirmer ces résultats au niveau théorique.
Dans la deuxième partie de ce travail, nous montrons comment ces modèles obtenus par apprentissage peuvent être étendus afin de manipuler l'information qui se trouve dans l'espace latent.
Nous proposons pour cela de considérer les conceptors qui peuvent être conceptualisé comme un jeu de poids synaptiques venant contraindre la dynamique du réservoir et la diriger vers des sous-espaces particuliers ; par exemple des sous-espaces correspondants au maintien d'une valeur particulière.
Plus généralement, nous montrons que ces conceptors peuvent non seulement maintenir des informations, ils peuvent aussi maintenir des fonctions.
Dans le cas du calcul mental évoqué précédemment, ces conceptors permettent alors de se rappeler et d'appliquer l'opération à effectuer sur les différentes entrées données au système.
Ces conceptors permettent donc d'instancier une mémoire de type procédural en complément de la mémoire de travail de type déclaratif.
Nous concluons ce travail en remettant en perspective ce modèle théorique vis à vis de la biologie et des neurosciences.
La segmentation et le regroupement en locuteurs (SRL) impliquent la détection des locuteurs dans un flux audio et les intervalles pendant lesquels chaque locuteur est actif, c'est-à-dire la détermination de 'qui parle quand'.
La première partie des travaux présentés dans cette thèse exploite une approche de modélisation du locuteur utilisant des clés binaires (BKs) comme solution à la SRL.
La modélisation BK est efficace et fonctionne sans données d'entraînement externes, car elle utilise uniquement des données de test.
Les contributions présentées incluent l'extraction des BKs basée sur l'analyse spectrale multi-résolution, la détection explicite des changements de locuteurs utilisant les BKs, ainsi que les techniques de fusion SRL qui combinent les avantages des BKs et des solutions basées sur un apprentissage approfondi.
La tâche de la SRL est étroitement liée à celle de la reconnaissance ou de la détection du locuteur, qui consiste à comparer deux segments de parole et à déterminer s'ils ont été prononcés par le même locuteur ou non.
Même si de nombreuses applications pratiques nécessitent leur combinaison, les deux tâches sont traditionnellement exécutées indépendamment l'une de l'autre.
La deuxième partie de cette thèse porte sur une application où les solutions de SRL et de reconnaissance des locuteurs sont réunies.
La nouvelle tâche, appelée détection de locuteurs à faible latence, consiste à détecter rapidement les locuteurs connus dans des flux audio à locuteurs multiples.
Il s'agit de repenser la SRL en ligne et la manière dont les sous-systèmes de SRL et de détection devraient être combinés au mieux.
Les objets connectés ont aujourd'hui pénétré les foyers et, poussés par une société tournée de plus en plus vers le bien-être, ces capteurs mesurent et proposent dorénavant une grande variété de données physiologiques.
L'arrivée à maturité des technologies de la réalité virtuelle, couplée avec l'avènement des objets connectés, permet et favorise dès lors de nouvelles perspectives dans la proposition d'
Ceux-ci se basent principalement sur du matériel médical, qui possède des contraintes d'utilisation forte, reste souvent encombrant et limite de fait la mobilité des utilisateurs.
Nous étudierons en particulier l'influence d'un biofeedback cardiaque, via des capteurs connectés grand public, sur l'engagement utilisateur et le sentiment d'agentivité.
Nous avons ainsi mené deux expérimentations nous permettant d'étudier l'impact des différentes modalités de biofeedback sur l'expérience utilisateur.
Notre première expérimentation met en place un biofeedback cardiaque dans un jeu d'horreur en réalité virtuelle, permettant d'augmenter le sentiment de peur.
Les résultats de cette expérimentationconfortent l'intérêt de l'utilisation de capteurs connectés comme moyen de captation physiologique dans des expériences de réalité virtuelle immersive.
Ils mettent également en avant l'impact positif de ce biofeedback sur la dimension d'engagement de l'expérience utilisateur.
La deuxième expérience porte sur l'utilisation de l'activité cardiaque
Les résultats de cette expérience démontrent la possibilité d'utiliser la dite mécanique pour des expériences virtuelles immersives et indiquent un impact positif sur le sentiment d'agentivité, lié au niveau de compétence des participants.
Sur un plan théorique, cette thèse propose une synthèse des modèles de l'expérience utilisateur en environnement virtuel et soumet par ailleurs les bases d'un modèle que nous nommons « l'immersion physiologique » .
Les classificateurs ga (cl.16), ku (cl.17) et mu (cl.18) marquent respectivement les valeurs de «  contact  » , de «  distance  » et d' « intériorité  »   : a) lorsqu'ils sont préfixés à la base ‑úma /endroit/  ; b) lorsqu'ils apparaissent dans le verbe conjugué ou préfixés aux thèmes des déterminants  ; c) lorsqu'ils sont suivis d'un nom en isolation ; d) ou lorsqu'ils ils sont suivis d'un verbe.
Bu (cl.14) marque en outre une valeur « abstraite  » lorsqu'il se combine avec des bases lexicales  ; il exprime le temps, la comparaison et la cause, lorsqu'il est préfixé aux thèmes des déterminants  ; il sert en troisième lieu à marquer l'hypothèse lorsqu'il est employé sous la forme d'un morphème libre.
Depuis les années 2000, un progrès significatif est enregistré dans les travaux de recherche qui proposent l'apprentissage de détecteurs d'objets sur des grandes bases de données étiquetées manuellement et disponibles publiquement.
Cependant, lorsqu'un détecteur générique d'objets est appliqué sur des images issues d'une scène spécifique les performances de détection diminuent considérablement.
Cette diminution peut être expliquée par les différences entre les échantillons de test et ceux d'apprentissage au niveau des points de vues prises par la(les) caméra(s), de la résolution, de l'éclairage et du fond des images.
De plus, l'évolution de la capacité de stockage des systèmes informatiques, la démocratisation de la "vidéo-surveillance" et le développement d'outils d'analyse automatique des données vidéos encouragent la recherche dans le domaine du trafic routier.
Les buts ultimes sont l'évaluation des demandes de gestion du trafic actuelles et futures, le développement des infrastructures routières en se basant sur les besoins réels, l'intervention pour une maintenance à temps et la surveillance des routes en continu.
Par ailleurs, l'analyse de trafic est une problématique dans laquelle plusieurs verrous scientifiques restent à lever.
Ces derniers sont dus à une grande variété dans la fluidité de trafic, aux différents types d'usagers, ainsi qu'aux multiples conditions météorologiques et lumineuses.
Ainsi le développement d'outils automatiques et temps réel pour l'analyse vidéo de trafic routier est devenu indispensable.
Ces outils doivent permettre la récupération d'informations riches sur le trafic à partir de la séquence vidéo et doivent être précis et faciles à utiliser.
C'est dans ce contexte que s'insèrent nos travaux de thèse qui proposent d'utiliser les connaissances antérieurement acquises et de les combiner avec des informations provenant de la nouvelle scène pour spécialiser un détecteur d'objet aux nouvelles situations de la scène cible.
Dans cette thèse, nous proposons de spécialiser automatiquement un classifieur/détecteur générique d'objets à une scène de trafic routier surveillée par une caméra fixe.
Nous présentons principalement deux contributions.
Cette formalisation approxime itérativement la distribution cible inconnue au départ, comme étant un ensemble d'échantillons de la base spécialisée à la scène cible.
Les échantillons de cette dernière sont sélectionnés à la fois à partir de la base source et de la scène cible moyennant une pondération qui utilise certaines informations a priori sur la scène.
La base spécialisée obtenue permet d'entraîner un classifieur spécialisé à la scène cible sans intervention humaine.
La deuxième contribution consiste à proposer deux stratégies d'observation pour l'étape mise à jour du filtre SMC.
Elles sont utilisées pour la pondération des échantillons cibles.
Les différentes expérimentations réalisées ont montré que l'approche de spécialisation proposée est performante et générique.
Nous avons pu y intégrer de multiples stratégies d'observation.
Elle peut être aussi appliquée à tout type de classifieur.
De plus, nous avons implémenté dans le logiciel OD SOFT de Logiroad les possibilités de chargement et d'utilisation d'un détecteur fourni par notre approche.
Nous avons montré également les avantages des détecteurs spécialisés en comparant leurs résultats avec celui de la méthode Vu-mètre de Logiroad.
Le discours produit lors d'une visite touristique naît de différentes modalités de communication dont la visite assistée par un dispositif socio-technique et la visite-conférence dirigée par un médiateur.
Plusieurs interrogations se posent dont celle d'une taxonomie des genres de discours liés au domaine spécialisé étudié, celle d'une unité de segmentation textuelle s'affranchissant du caractère scriptural ou oral du mode de production du texte, et celles liées à la catégorisation d'un texte dans un genre discursif.
En effet, les valeurs des paramètres de caractérisation doivent permettre l'introduction d'un prototype indispensable à la catégorisation et à l'indexation textuelle du genre étudié.
En outre, le traitement quantitatif d'une compilation de textes sélectionnés trouve ses fondements au sein même de l'analyse de discours et de la linguistique de corpus.
La méthode suivie, qui introduit les règles de segmentation textuelle dont l'annotation manuelle qualitative, et l'analyse quantitative permettent de proposer un modèle d'organisation de chaque genre considéré.
Cette thèse porte sur l'intégration de données brutes provenant de sources hétérogènes sur le Web.
L'objectif global est de fournir une architecture générique et modulable capable de combiner, de façon sémantique et intelligente, ces données hétérogènes dans le but de les rendre réutilisables.
Dans ce rapport, nous proposons de nouveaux modèles et techniques permettant d'adapter le processus de combinaison et d'intégration à la diversité des sources de données impliquées.
Les problématiques sont une gestion transparente et dynamique des sources de données, passage à l'échelle et responsivité par rapport au nombre de sources, adaptabilité au caractéristiques de sources, et finalement, consistance des données produites(données cohérentes, sans erreurs ni doublons).
Pour répondre à ces problématiques, nous proposons un méta-modèle pour représenter ces sources selon leurs caractéristiques, liées à l'accès (URI) ou à l'extraction (format) des données, mais aussi au capacités physiques des sources (latence, volume).
En s'appuyant sur cette formalisation, nous proposent différentes stratégies d'accès aux données, afin d'adapter les traitements aux spécificités des sources.
En se basant sur ces modèles et stratégies, nous proposons une architecture orientée ressource, ou tout les composants sont accessibles par HTTP via leurs URI.
Afin d'améliorer la qualité des données produites par notre approches, l'accent est mis sur l'incertitude qui peut apparaître dans les données sur le Web.
Nous proposons un modèle, permettant de représenter cette incertitude, au travers du concept de ressource Web incertaines, basé sur un modèle probabiliste ou chaque ressource peut avoir plusieurs représentation possibles, avec une certaine probabilité.
Cette approche sera à l'origine d'une nouvelle optimisation de l'architecture pour permettre de prendre en compte l'incertitude pendant la combinaison des données
La France connaît un vieillissement de sa population sans précédent.
La part des séniors s'accroît et notre société se doit de repenser son organisation pour tenir compte de ce changement et mieux connaître cette population.
De nombreuses cohortes de personnes âgées existent déjà à travers le monde dont quatre en France et, bien que la part de cette population vivant dans des structures d'hébergement collectif (EHPAD, cliniques de soins de suite) augmente, la connaissance de ces seniors reste lacunaire.
Aujourd'hui les groupes privés de maisons de retraite et d'établissements sanitaires comme Korian ou Orpéa s'équipent de grandes bases de données relationnelles permettant d'avoir de l'information en temps réel sur leurs patients/résidents.
Depuis 2010 les dossiers de tous les résidents Korian sont dématérialisés et accessibles par requêtes.
Ils comprennent à la fois des données médico-sociales structurées décrivant les résidents et leurs traitements et pathologies, mais aussi des données textuelles explicitant leur prise en charge au quotidien et saisies par le personnel soignant.
Au fil du temps et alors que le dossier résident informatisé (DRI) avait surtout été conçu comme une application de gestion de base de données, il est apparu comme une nécessité d'exploiter cette mine d'informations et de construire un outil d'aide à la décision destiné à améliorer l'efficacité des soins.
L'Institut du Bien Vieillir IBV devenu entretemps la Fondation Korian pour le Bien Vieillir a alors choisi, dans le cadre d'un partenariat Public/Privé de financer un travail de recherche destiné à mieux comprendre le potentiel informatif de ces données, d'évaluer leur fiabilité et leur capacité à apporter des réponses en santé publique.
Ce travail de recherche et plus particulièrement cette thèse a alors été pensée en plusieurs étapes.
- D'abord l'analyse de contenu du data warehouse DRI, l'objectif étant de construire une base de données recherche, avec un versant social et un autre de santé.
Ce fut le sujet du premier article.
- Ensuite, par extraction directe des informations socio-démographiques des résidents dès leur entrée, de leurs hospitalisations et décès puis, par un processus itératif d'extractions d'informations textuelles de la table des transmissions et l'utilisation de la méthode Delphi, nous avons généré vingt-quatre syndromes, ajouté les hospitalisations et les décès et construit une base de données syndromique, la Base du Bien Vieillir (BBV).
Ce système d'informations d'un nouveau type a permis la constitution d'une cohorte de santé publique à partir de la population des résidents de la BBV et l'organisation d'un suivi longitudinal syndromique de celle-ci.
La BBV a également été évaluée scientifiquement dans un cadre de surveillance et de recherche en santé publique au travers d'une analyse de l'existant : contenu, périodicité, qualité des données.
La cohorte construite a ainsi permis la constitution d'un outil de surveillance. Cet échantillon de population a été suivi en temps réel au moyen des fréquences quotidiennes d'apparitions des 26 syndromes des résidents.
La méthodologie d'évaluation était celle des systèmes de surveillance sanitaire proposée par le CDC d'Atlanta et a été utilisée pour les syndromes grippaux et les gastro entérites aiguës. Ce fut l'objet du second article.
- Enfin la construction d'un nouvel outil de santé publique : la distribution de chacun des syndromes dans le temps (dates de transmissions) et l'espace (les EHPAD de transmissions) a ouvert le champ de la recherche à de nouvelles méthodes d'exploration des données et permis d'étudier plusieurs problématiques liées à la personne âgée : chutes répétées, cancer, vaccinations et fin de vie.
Les systèmes dynamiques présentent des comportements temporels qui peuvent être exprimés sous diverses formes séquentielles telles que des signaux, des ondes, des séries chronologiques et des suites d'événements.
Détecter des motifs sur de tels comportements temporels est une tâche fondamentale pour comprendre et évaluer ces systèmes.
Étant donné que de nombreux comportements du système impliquent certaines caractéristiques temporelles, le besoin de spécifier et de détecter des motifs de comportements qui implique des exigences de synchronisation, appelées motifs temporisés, est évidente.
Cependant, il s'agit d'une tâche non triviale due à un certain nombre de raisons, notamment la concomitance des sous-systèmes et la densité de temps.
La contribution principale de cette thèse est l'introduction et le développement du filtrage par motif temporisé, c'est-à-dire l'identification des segments d'un comportement donné qui satisfont un motif temporisé.
Nous proposons des expressions rationnelles temporisées (TRE) et la logique de la boussole métrique (MCL) comme langages de spécification pour motifs temporisés.
Nous développons d'abord un nouveau cadre qui abstraite le calcul des aspects liés au temps appelé l'algèbre des relations temporisées.
Ensuite, nous fournissons des algorithmes du filtrage hors ligne pour TRE et MCL sur des comportements à temps dense à valeurs discrètes en utilisant ce cadre et étudions quelques extensions pratiques.
Il est nécessaire pour certains domaines d'application tels que le contrôle réactif que le filtrage par motif doit être effectué pendant l'exécution réelle du système.
Pour cela, nous fournissons un algorithme du filtrage en ligne pour TREs basé sur la technique classique des dérivées d'expressions rationnelles.
Nous croyons que la technique sous-jacente qui combine les dérivées et les relations temporisées constitue une autre contribution conceptuelle majeure pour la recherche sur les systèmes temporisés.
Nous présentons un logiciel libre Montre qui implémente nos idées et algorithmes.
Nous explorons diverses applications du filtrage par motif temporisé par l'intermédiaire de plusieurs études de cas.
Enfin, nous discutons des orientations futures et plusieurs questions ouvertes qui ont émergé à la suite de cette thèse.
Cette thèse porte sur les différents facteurs impliqués dans le traitement des relatives, notamment le traitement des relatives sujet et objet ainsi que celui de l'attachement des relatives sujet.
Je propose alors dans cette thèse de dépasser les notions de langue avec un avantage pour la relative sujet ou objet, ou avec une préférence avec attachement haut ou bas.
Cette description permet de mieux appréhender leur rôle dans le traitement, notamment pour comprendre les différences constatées dans les expériences entre les langues.
Je poursuis par une réflexion sur l'approche multifactorielle en tenant compte des facteurs liés aux domaines sémantique et pragmatique.
Enfin, pour élargir les facteurs en jeu dans le traitement des relatives au-delà du domaine linguistique, je montre l'influence d'un domaine non linguistique (amorçage mathématique) sur le domaine linguistique (attachement) en français avec des expériences de choix restreints, et de mouvements oculaires avec le paradigme monde visuel.
Dans cette thèse, nous nous sommes intéressés à la gestion dynamique des processus métiers.
D'autre part l'objectif est aussi de gérer les processus en prenant en compte les compétences nécessaires à leurs exécutions.
Ce travail de thèse s'appuie sur l'approche BPM et plus précisément à sa phase d'exécution.
Dans un monde de travail turbulent et en constante évolution, on parle souvent de modèles adaptables ou adaptatifs, de modèles qui s'enrichissent à chaque exécution et ne suivent pas un modèle structuré et prédéfini tel que le cas du BPM (Business Process Management) classique.
Ainsi, les acteurs impliqués ne sont pas seulement supportés, ils sont forcés à effectuer les tâches dans des séquences spécifiées.
De surcroit, un processus flexible c'est un processus capable de changer que les parties qui ont besoin d'être changées tout en gardant la stabilité de ses autres parties.
Dans ce contexte, parmi les approche prometteuses, l'approche orientée services offre aux entreprises une modularité permettant de remplacer facilement un composant par un autre, de le réutiliser et d'étendre son objectif en lui ajoutant un autre composant.
De ce fait, dans le but de supporter l'agilité des processus métiers, nous proposons une approche combinant les trois approches suivantes : l'approche BPM, la gestion des compétences et l'approche orientée service dans un environnement social supportant le travail collaboratif.
L'utilisation de termes équivalents ou sémantiquement proches est nécessaire pour augmenter la couverture et la sensibilité d'une application comme la recherche et l'extraction d'information ou l'annotation sémantique de documents.
Dans le contexte de l'identification d'effets indésirables susceptibles d'être dûs à un médicament, la sensibilité est aussi recherchée afin de détecter plus exhaustivement les déclarations spontanées et de mieux surveiller le risque médicamenteux.
C'est la raison qui motive notre travail.
Dans notre travail de thèse, nous cherchons ainsi à détecter des termes sémantiquement proches et à les regrouper en utilisant plusieurs méthodes : des algorithmes de clustering non supervisés, des ressources terminologiques exploitées avec le raisonnement terminologique et des méthodes de Traitement Automatique de la Langue, comme la structuration de terminologies, où nous visons la détection de relations hiérarchiques et synonymiques.
Nous avons réalisé de nombreuses expériences et évaluations des clusters générés, qui montrent que les méthodes proposées peuvent contribuer efficacement à la tâche visée.
Ce travail aura pour but d'explorer l'interface sémantique-syntaxe des « constructions » avec les prédicats secondaires, résultatifs et dépictifs.
Une attention particulière sera donnée au problème du choix des sujets (ou des hôtes) de prédication pour ces deux types de prédicats, ainsi qu'à la classe aspectuelle lexicale du verbe à la base de la construction.
Dans la première partie, nous introduisons divers patterns de la construction résultative et expliquons le principe de base qui régit la syntaxe de ces constructions, à savoir la Restriction sur l'objet direct.
Après avoir écarté un nombre de prétendus contre-exemples à la Restriction DOR, nous réaffirmerons sa validité, notamment en tant que diagnostic de l'inaccusativité en anglais.
La deuxième partie sera consacré aux prédicats dépictifs, notamment aux contraintes qui pèsent sur le choix du contrôleur pour ce type de prédication secondaire, ainsi qu'aux propriétés des adjectifs dépictifs en comparaison avec d'autres types d'adjoints participant-oriented.
Nous étudions la distribution des adjectifs formes longues et formes courtes en russe, conditionnée par les propriétés d'accord qui les distinguent et esquissons un processus historique à l'origine de leur distribution dans la langue d'aujourd'hui.
Dans le domaine de la cartographie cérébrale, nous avons identifié le besoin d'un outil fondé sur la connaissance détaillée des sulci.
Dans cette thèse, nous développons un nouvel outil de cartographie cérébrale appelé NeuroLang, qui s'appuie sur la géométrie spatiale du cerveau.
Nous avons abordé cette question avec deux approches : premièrement, nous avons fermement fondé notre théorie sur la neuroanatomie classique.
Deuxièmement, nous avons conçu et implémenté des méthodes pour les requêtes spécifiques au sulcus dans le langage spécifique au domaine, NeuroLang.
Nous avons testé notre méthode sur 52 sujets et évalué les performances de NeuroLang pour des cartographie du cortex spécifiques à la population et au sujet.
Ensuite, nous proposons une nouvelle organisation hiérarchique basée sur les données de la stabilité sulcal appuyée sur ces données.
Pour conclure, nous avons résumé l'implication de notre méthode dans le domaine actuel, ainsi que notre contribution globale au domaine de la cartographie cérébrale.
L'acquisition du lexique précoce est très importante dans le développement du langage dans la mesure où les mots sont constitutifs des énoncés signifiants de l'enfant mais également car leur développement préfigure dans une certaine mesure les habiletés langagières ultérieures.
Il est aujourd'hui admis que l'acquisition du lexique se fait sur la base d'étapes communes mais au sein desquelles il existe de fortes variations inter-individuelles, qui selon les chercheurs seraient d'ordre linguistiques, sociales ou idiosyncrasiques.
Cependant, il reste encore des zones d'ombre, notamment sur l'influence possible des méthodes d'évaluation sur les résultats ;
et malgré le fait que certains chercheurs conseillent l'utilisation conjointe de plusieurs méthodes de collecte pour éviter cette influence liée à la méthodologie, cette préconisation est peu suivie.
Cette thèse vise à étudier les trajectoires développementales du lexique en production et leurs variations selon les enfants ;
plus spécifiquement, il s'agit de montrer l'apport de méthodes complémentaires et l'importance de l'exploration du contexte de production des mots lors des observations spontanées en milieu naturel pour mieux interpréter les différences inter-individuelles.
Globalement, le développement et la composition du vocabulaire des 10 enfants évalués par l'IFDC suivent les tendances observées dans la littérature.
Nous nous sommes ensuite focalisés sur 4 de ces enfants pour les stades linguistiques 15-25 ; 50 ; 70-120 mots (corpus CIBLÉ).
L'utilisation des deux méthodes – questionnaires parentaux et données spontanées – a permis d'évaluer le développement lexical de manière plus fiable et complète, les avantages d'une méthode permettant de combler les limites de l'autre.
Afin de mieux comprendre les divergences de certains résultats entre ces deux méthodes, nous avons poursuivi nos investigations sur les données spontanées des 4 enfants en examinant les contextes situationnels et interactionnels.
Nous avons défini et catégorisé les situations présentes dans les enregistrements du corpus TOTAL.
Par exemple, il est apparu que les deux enfants dont les effectifs de mots sont les moins élevés au niveau des données spontanées ont été davantage filmés en situation ludique solitaire ; situation où les analyses révèlent que le nombre d'unités lexicales produites est le plus faible.
Ensuite, un autre travail a consisté à décrire le contexte interactionnel et plus précisément à comprendre les implications des enfants dans les échanges interactionnels.
Beaucoup de différences inter-individuelles sont apparues, dont certaines nous permettent de clarifier les données des enfants.
Ainsi, chaque analyse apporte des informations complémentaires – du vocabulaire estimé des questionnaires parentaux, au vocabulaire en usage enregistré en milieu naturel.
En dépit du nombre restreint d'enfants qui composent cet échantillon, ces résultats encouragent l'utilisation de méthodes complémentaires.
L'analyse des contextes situationnels et interactionnels nous semble aussi cruciale pour comprendre les mesures lexicales des enfants et mieux interpréter les différences intra et inter-individuelles.
L'analyse du domaine vise à identifier et organiser les caractéristiques communes et variables dans un domaine.
La contribution générale de cette thèse consiste à adopter et exploiter des techniques de traitement automatique du langage naturel et d'exploration de données pour automatiquement extraire et modéliser les connaissances relatives à la variabilité à partir de documents informels.
Nous étudions l'applicabilité de notre idée à travers deux études de cas pris dans deux contextes différents : (1) la rétro-ingénierie des Modèles de Features (FMs) à partir des exigences réglementaires de sûreté dans le domaine de l'industrie nucléaire civil et (2) l'extraction de Matrices de Comparaison de Produits (PCMs) à partir de descriptions informelles de produits.
Dans la première étude de cas, nous adoptons des techniques basées sur l'analyse sémantique, le regroupement (clustering) des exigences et les règles d'association.
L'évaluation de cette approche montre que 69% de clusters sont corrects sans aucune intervention de l'utilisateur.
Les dépendances entre features montrent une capacité prédictive élevée : 95% des relations obligatoires et 60% des relations optionnelles sont identifiées, et la totalité des relations d'implication et d'exclusion sont extraites.
Dans la deuxième étude de cas, notre approche repose sur la technologie d'analyse contrastive pour identifier les termes spécifiques au domaine à partir du texte, l'extraction des informations pour chaque produit, le regroupement des termes et le regroupement des informations.
Notre étude empirique montre que les PCMs obtenus sont compacts et contiennent de nombreuses informations quantitatives qui permettent leur comparaison.
L'expérience utilisateur montre des résultats prometteurs et que notre méthode automatique est capable d'identifier 43% de features correctes et 68% de valeurs correctes dans des descriptions totalement informelles et ce, sans aucune intervention de l'utilisateur.
Nous montrons qu'il existe un potentiel pour compléter ou même raffiner les caractéristiques techniques des produits.
La principale leçon à tirer de ces deux études de cas, est que l'extraction et l'exploitation de la connaissance relative à la variabilité dépendent du contexte, de la nature de la variabilité et de la nature du texte.
Avec le développement et la multiplication des appareils connectés dans tous les domaines, de nouvelles solutions pour le traitement de flux de données ont vu le jour.
Cette thèse s'inscrit dans ce contexte : elle a été réalisée dans le cadre du projet FUI Waves, une plateforme de traitement de flux distribués.
Le cas d'usage pour le développement a été la gestion des données provenant d'un réseau de distribution d'eau potable, plus précisément la détection d'anomalie dans les mesures de qualité et leur contextualisation par rapport à des données extérieures.
Plusieurs contributions ont été réalisées et intégrées à différentes étapes du projet, leur évaluation et les publications liées témoignant de leur pertinence.
Celles-ci se basent sur une ontologie que j'ai spécifiée depuis des échanges avec les experts du domaine travaillant dans chez le partenaire métier du projet.
L'utilisation de données géographiques a permis de réaliser un système de profilage visant à améliorer le processus de contextualisation des erreurs.
Un encodage de l'ontologie adapté au traitement de flux de données RDF a été développé pour supporter les inférences de RDFS enrichis de owl : sameAs.
Conjointement, un formalisme compressé de représentation des flux (PatBin) a été conçu et implanté dans la plateforme.
Il se base sur la régularité des motifs des flux entrants.
Enfin, un langage de requêtage a été développé à partir de ce formalisme.
Il intègre une stratégie de raisonnement se basant sur la matérialisation et la réécriture de requêtes.
Enfin, à partir de déductions provenant d'un d'apprentissage automatique, un outil de génération de requêtes a été implanté.
Ces différentes contributions ont été évaluées sur des jeux de données concrets du domaine ainsi que sur des jeux d'essais synthétiques.
Cette thèse étudie les méthodes de regroupement basées sur le principe des plus proches voisins partagés (SNN).
Comme la plupart des autres approches de clustering à base de graphe, les méthodes SNN sont effectivement bien adaptées à surmonter la complexité des données, l'hétérogénéité et la haute dimensionnalité.
La première contribution de la thèse est de revisiter une méthode existante basée sur les voisins partagés en deux points.
Nous présentons d'abord un formalisme basé sur la la théorie de décision à contrario.
Cela nous permet de tirer des scores de connectivité plus fiable des groupes et une interprétation plus intuitive des voisinage selectionnés optimalement.
Nous proposons également un nouveau algorithme de factorisation pour accélérer le calcul intensif nécessaire des matrices des voisins partagés.
La deuxième contribution de cette thèse est une généralisation de la classification SNNau cas multi-source.
La principale originalité de notre approche est que nous introduisons une étape de sélection des sources d'information optimales dans le calcul de scores de groupes candidats.
Chaque groupe est alors associé à son propre sous-ensemble optimal des modalités.
Comme le montre le expériences, cette étape de sélection de source rend notre approche largement robuste à la présence de sources locales aberrantes.
Cette nouvelle méthode est appliquée à un large éventail de problèmes, y compris la structuration multimodale des collections d'images et dans le regroupement dans des sous-espaces basés sur les projections aléatoires.
La troisième contribution de la thèse est une tentative pour étendre les méthodes SNNdans le contexte des graphes biparites.
Nous introduisons de nouvelles mesures de pertinence SNNrevisitées pour ce contexte asymétrique et nous montrons qu'elles peuvent être utiliséespour sélectionner localement des voisinages optimales.
En conséquence, nous proposons un nouveau algorithme de clustering bipartite SNN qui est appliqué à la découverte d'objets visuels.
Les expériences montrent que cette nouvelle méthode est meilleure par rapport aux méthodes de l'état de l'art.
Basé sur les objets découverts, nous introduisons également un paradigme de recherche visuelle, c.-à-d les objet basés sur la suggestion de requêtes visuel les.
Depuis plusieurs années, la notion de Big Data s'est largement développée.
Afin d'analyser et explorer toutes ces données, il a été nécessaire de concevoir de nouvelles méthodes et de nouvelles technologies.
Aujourd'hui, le Big Data existe également dans le domaine de la santé.
Les hôpitaux en particulier, participent à la production de données grâce à l'adoption du dossier patient électronique.
L'objectif de cette thèse a été de développer des méthodes statistiques réutilisant ces données afin de participer à la surveillance syndromique et d'apporter une aide à la décision.
Cette étude comporte 4 axes majeurs.
Tout d'abord, nous avons montré que les données massives hospitalières étaient très corrélées aux signaux des réseaux de surveillance traditionnels.
Dans un second temps, nous avons établi que les données hospitalières permettaient d'obtenir des estimations en temps réel plus précises que les données du web, et que les modèles SVM et Elastic Net avaient des performances comparables.
Puis, nous avons appliqué des méthodes développées aux Etats-Unis réutilisant les données hospitalières, les données du web (Google et Twitter) et les données climatiques afin de prévoir à 2 semaines les taux d'incidence grippaux de toutes les régions françaises.
Enfin, les méthodes développées ont été appliquées à la prévision à 3 semaines des cas de gastro-entérite au niveau national, régional, et hospitalier.
Dans un premier temps, nous considérons le problème de la classification avec un grand nombre de classes.
Afin de valider cette méthode, nous fournissons une analyse théorique et expérimentale détaillée.
Dans la seconde partie, nous approchons le problème de l'apprentissage sur données distribuées en introduisant un cadre asynchrone pour le traitement des données.
Nous appliquons ce cadre à deux applications phares : la factorisation de matrice pour les systèmes de recommandation en grande dimension et la classification binaire.
Notre recherche s'inscrit dans le cadre de la phraséologie berbère abordant le figement lexical, notamment l'étude des séquences figées.
Elle s'inscrit également dans la continuité, d'une part, de nos deux mémoires de Master 1 « étude thématique des expressions idiomatiques berbères liées au corps humain, parler Ayt Ḥmad Uɛisa, Moyen Atlas » , et de Master 2 consacré à l'étude de la typologie sémantico-syntaxique des expressions idiomatiques relatives au corps humain et, d'autre part, d'autres travaux effectués dans l'aire berbère en général.
Il vise à mettre l'accent sur trois différents aspects des expressions idiomatiques en menant une étude thématique et sémantico-syntaxique, et en se focalisant sur la typologie syntaxique et sémantique - étude des comportements syntaxiques et sémantiques de chaque syntagme dans l'expression, les procédés de construction du sens - ainsi que sur le rapport entre le sémantisme des constituants et leur figement syntaxique.
L'analyse portera sur un corpus oral que nous comptons collecter à l'aide d'entretiens semi-directs réalisés avec les locuteurs natifs du parler Ayt Ḥmad Uɛisa.
Nous analyserons ensuite les unités du corpus en nous référant au cadre théorique général de la phraséologie internationale ainsi que aux travaux effectués dans le domaine de la phraséologie berbère en particulier.
L'importance de ce projet réside dans le fait qu'il fournira des éléments linguistiques expliquant le fonctionnement syntaxique et sémantique des expressions idiomatiques berbères, et aidant à leur compréhension, et qu'il contribuera sans doute à conserver ces formes langagières qui constituent une partie importante de la langue berbère par la collecte du corpus et la constitution d'une base de données pouvant être exploitées dans d'autres perspectives, telles que la lexicographie, la phraséodidactique, la traduction et le traitement automatique des langues.
L'objectif de ce travail est de coupler le processus de prétraitement des documents textuels, les techniques de machine learning et les méthodes de visualisation afin de décrire les parcours de soins de cohortes de patients et de mettre en évidence des clusters en fonction de certaines caractéristiques médicales, sociales ou d'organisation territoriale.
La modélisation des séquences des états se fera par example par méthode de clustering, Les méthodes de détection des séquences anormales (parcours avec durées anormales, réactions et résultats innatendus sur le traitement, déviations obvservées lors du suivi, etc...).
La modélisation des séquences des états sera effectuée par des réseaux de neurones
Des méthodes d'apprentissage et de prédictions seront associées aux réseaux de neurones pour optimiser l'ensemble du système.
La présente thèse propose une analyse morphologique automatique des séquences de kanji dans des textes japonais, généraux ou spécialisés.
Cette analyse s'appuie sur les particularités graphémiques, morphologiques et syntaxiques du japonais.
La première partie décrit le système d'écriture japonais et son codage informatique.
La deuxième partie décrit les parties du discours japonais, en particulier les verbes, qualificatifs, particules et suffixes flexionnels, leurs caractéristiques morphosyntaxiques étant essentielles pour l'analyse morphologique.
La troisième partie décrit le module d'analyse : identification et formalisation des données pour l'analyse, algorithme de l'analyse et des pré-traitements, formalisation de modèles d'objets pour la manipulation informatique du japonais.
Cette thèse présente une modélisation des principaux aspects syntaxiques de la coordination dans les grammaires d'interaction de Guy Perrier.
Les grammaires d'interaction permettent d'expliciter la valence des groupes conjoints.
C'est précisément sur cette notion qu'est fondée notre modélisation.
Nous présentons également tous les travaux autour de cette modélisation qui nous ont permis d'aboutir à une implantation réaliste : le développement du logiciel XMG et son utilisation pour l'écriture de grammaires lexicalisées, le filtrage lexical par intersection d'automates et l'analyse syntaxique.
Dans cette thèse, nous présentons une nouvelle méthode de cartographie visuelle hybride qui exploite des informations métriques, topologiques et sémantiques.
Notre but est de réduire le coût calculatoire par rapport à des techniques de cartographie purement métriques.
Comparé à de la cartographie topologique, nous voulons plus de précision ainsi que la possibilité d'utiliser la carte pour le guidage de robots.
Cette méthode hybride de construction de carte comprend deux étapes.
Ces cartes sont ensuite complétées avec des données métriques aux nœuds correspondant à des sous-séquences d'images acquises quand le robot revenait dans des zones préalablement visitées.
La deuxième étape augmente ce modèle en ajoutant des informations sémantiques.
Une classification est effectuée sur la base des informations métriques en utilisant des champs de Markov conditionnels (CRF) pour donner un label sémantique à la trajectoire locale du robot (la route dans notre cas) qui peut être "doit", "virage" ou "intersection".
La fermeture de boucle n'est réalisée que dans les intersections ce qui accroît l'efficacité du calcul et la précision de la carte.
En intégrant tous ces nouveaux algorithmes, cette méthode hybride est robuste et peut être étendue à des environnements de grande taille.
Elle peut être utilisée pour la navigation d'un robot mobile ou d'un véhicule autonome en environnement urbain.
Nous présentons des résultats expérimentaux obtenus sur des jeux de données publics acquis en milieu urbain pour démontrer l'efficacité de l'approche proposée.
Cette thèse, organisée en deux parties indépendantes, a pour objet la sémantique distributionnelle et la sélection de variables.
Dans la première partie, nous introduisons une nouvelle méthode pour l'apprentissage de représentations de mots à partir de grandes quantités de texte brut.
Cette méthode repose sur un modèle probabiliste de la phrase, utilisant modèle de Markov caché et arbre de dépendance.
Nous évaluons les modèles obtenus sur des taches intrinsèques, telles que prédire des jugements de similarité humains ou catégoriser des mots et deux taches extrinsèques~ : la reconnaissance d'entités nommées et l'étiquetage en supersens.
Dans la seconde partie, nous introduisons, dans le contexte des modèles linéaires, une nouvelle pénalité pour la sélection de variables en présence de prédicteurs fortement corrélés.
Cette pénalité, appelée trace Lasso, utilise la norm trace des prédicteurs sélectionnés, qui est une relaxation convexe de leur rang, comme critère de complexité.
En particulier, lorsque tous les prédicteurs sont orthogonaux, il est égal à la norme ℓ 1, tandis que lorsque tous les prédicteurs sont égaux, il est égal à la norme ℓ 2.
Nous proposons deux algorithmes pour calculer la solution du problème de régression aux moindres carrés régularisé par le trace Lasso et réalisons des expériences sur des données synthétiques.
Les architectures d'ordinateur classiques sont optimisées pour le traitement déterministe d'informations pré-formatées et ont donc des difficultés avec des données naturelles bruitées (images, sons, etc.).
Comme celles-ci deviennent nombreuses, de nouveaux circuits neuromorphiques (inspirés par le cerveau) tels que les réseaux de neurones émergent.
Dans ce travail, nous étudions des memristors basés sur des jonctions tunnel ferroélectriques qui sont composées d'une couche ferroélectrique ultramince entre deux électrodes métalliques.
Nous montrons que le renversement de la polarisation de BiFeO3 induit des changements de résistance de quatre ordres de grandeurs et établissons un lien direct entre les états de domaines mixtes et les niveaux de résistance intermédiaires.
En alternant les matériaux des électrodes, nous révélons leur influence sur la barrière électrostatique et les propriétés dynamiques des memristors.
La combinaison de leur analyse avec de l'imagerie par microscopie à force piézoélectrique nous permet d'établir un modèle dynamique du memristor.
Suite à la démonstration de la spike-timing-dependent plasticity, une règle d'apprentissage importante, nous pouvons prédire le comportement de notre synapse artificielle.
Ceci représente une avance majeure vers la réalisation de réseaux de neurones sur puce dotés d'un auto-apprentissage non-supervisé.
De nombreuses sources géohistoriques sont aujourd'hui mises à la disposition des chercheurs : plans anciens, bottins, etc.
L'objectif de cette thèse est d'apporter une solution à ce verrou par la production de données anciennes de référence.
En nous focalisant sur le réseau des rues de Paris entre la fin du XVIIIe et la fin du XIXe siècles, nous proposons plus précisément un modèle multi-représentations de données agrégées permettant, par confrontation d'observations homologues dans le temps, de créer de nouvelles connaissances sur les imperfections des données utilisées et de les corriger.
De manière générale, un réseau reflète les interactions entre les nombreuses entités d'un système.
Ces interactions peuvent être de différentes natures, un lien social ou un lien d'amitié dans un réseau social constitué de personnes, un câble dans un réseau de routeurs, une réaction chimique dans un réseau biologique de protéines, un hyperlien dans un réseau de pages Web, etc. Plus encore, la rapide démocratisation du numérique dans nos sociétés, avec Internet notamment, a pour conséquence de produire de nouveaux systèmes qui peuvent être représentés sous forme de réseaux.
Aujourd'hui, la science des réseaux est un domaine de recherche à part entière dont l'enjeu principal est de parvenir à décrire et modéliser ces réseaux avec précision afin de révéler leurs caractéristiques générales et de mieux comprendre leurs mécanismes.
La plupart des travaux dans ce domaine utilisent le formalisme des graphes qui fournit un ensemble d'outils mathématiques particulièrement adaptés à l'analyse topologique et structurelle des réseaux.
Il existe de nombreuses applications dans ce domaine, par exemple des applications concernant la propagation d'épidémie ou de virus informatique, la fragilité du réseau en cas de panne, sa résilience en cas d'attaque, l'étude de la dynamique pour prédire l'apparition de nouveaux liens, la recommandation, etc.
La grande majorité des réseaux réels sont caractérisés par des niveaux d'organisation dans leur structure mésoscopique.
Du fait de la faible densité globale des réseaux réels couplée à la forte densité locale, on observe la présence de groupes de nœuds fortement liés entre eux et plus faiblement liés avec le reste du réseau, que l'on appelle communautés.
Les dossiers patients électroniques contiennent des informations importantes pour la santé publique.
La majeure partie de ces informations est contenue dans des documents rédigés en langue naturelle.
Bien que le texte texte soit pertinent pour décrire des concepts médicaux complexes, il est difficile d'utiliser cette source de données pour l'aide à la décision, la recherche clinique ou l'analyse statistique.
Parmi toutes les informations cliniques intéressantes présentes dans ces dossiers, la chronologie médicale du patient est l'une des plus importantes.
Être capable d'extraire automatiquement cette chronologie permettrait d'acquérir une meilleure connaissance de certains phénomènes cliniques tels que la progression des maladies et les effets à long-terme des médicaments.
Dans notre thèse, nous nous concentrons sur la création de ces chronologies médicales en abordant deux questions connexes en traitement automatique des langues : l'extraction d'informations temporelles et la résolution de la coréférence dans des documents cliniques.
Concernant l'extraction d'informations temporelles, nous présentons une approche générique pour l'extraction de relations temporelles basée sur des traits catégoriels.
Cette approche peut être appliquée sur des documents écrits en anglais ou en français.
Puis, nous décrivons une approche neuronale pour l'extraction d'informations temporelles qui inclut des traits catégoriels.
Nous décrivons une approche neuronale pour la résolution de la coréférence dans les documents cliniques.
Nous menons une étude empirique visant à mesurer l'effet de différents composants neuronaux, tels que les mécanismes d'attention ou les représentations au niveau des caractères, sur la performance de notre approche.
Il est important pour les robots de pouvoir reconnaître les objets rencontrés dans la vie quotidienne afin d'assurer leur autonomie.
De nos jours, les robots sont équipés de capteurs sophistiqués permettant d'imiter le sens humain du toucher.
Dans cette thèse, notre but est d'exploiter les données haptiques issues de l'interaction robot-objet afin de reconnaître les objets de la vie quotidienne, et cela en utilisant les algorithmes d'apprentissage automatique.
Le problème qui se pose est la difficulté de collecter suffisamment de données haptiques afin d'entraîner les algorithmes d'apprentissage supervisé sur tous les objets que le robot doit reconnaître.
En effet, les objets de la vie quotidienne sont nombreux et l'interaction physique entre le robot et chaque objet pour la collection des données prend beaucoup de temps et d'efforts.
Pour traiter ce problème, nous développons un système de reconnaissance haptique permettant de reconnaître des objets à partir d'aucune, de une seule, ou de plusieurs données d'entraînement.
Enfin, nous intégrons la vision afin d'améliorer la reconnaissance d'objets lorsque le robot est équipé de caméras.
Cette étude a pour objectif la description syntaxique et sémantique des constructions transitives non locatives à un complément d'objet direct en grec moderne : N0 V N1.
Nous nous sommes appuyée sur le cadre théorique de la grammaire transformationnelle de Zellig
À partir de 16 560 entrées verbales morphologiques, nous procédons à la classification des constructions transitives non locatives, à partir de 24 classes distinctes, sur la base de critères formels posés.
Un inventaire de 2 934 emplois verbaux à construction transitive non locative à un complément d'objet direct a été ainsi produit et scindé en neuf classes.
En outre, la transformation passive est largement interdite pour les emplois verbaux recensés dans la table 32GNM, alors que les tables 32GCV et 32GRA regroupent des verbes acceptant une transformation à verbe support.
Afin d'espérer développer une philosophie politique qui reconnaisse d'emblée notre interdépendance, nous travaillons dans une première partie à établir des hypothèses sur ce qu'on entend par réalité et sur notre accès à cette dernière.
Une ontologie événementielle paraît compatible avec l'ontogénèse narrative qui nous constitue individuellement en constituant un « nous » .
Il faut pour cela imaginer chacun s'imaginant le monde et apprenant au travers d'histoires, dans une logique inductive qui peut réconcilier la phénoménologie herméneutique d'une part et l'apprentissage statistique de l'autre.
De ces histoires chacun tire des universaux, interprétables comme des composantes principales d'une analyse statistique factorielle de ces histoires qui nous constituent.
Le temps joue un rôle clef dans la dynamique de cette constitution autant que dans celle des événements rassemblés dans ces histoires.
L'enjeu est finalement de partager ces universaux dans une histoire commune, ou, à l'inverse, dans une rupture temporelle qui permet peut-être de mieux accéder à un monde commun.
Nous travaillons alors dans une seconde partie la question du vivre ensemble avec les idées républicaines de liberté, d'égalité et de fraternité, et avec celles de pluralité et de confins.
L'écologie politique que l'on aperçoit alors est aussi républicaine que libertaire.
Dans ce cadre, la justice s'exprime par la justesse, la fidélité, la sensibilité et par une juste démesure.
L'impératif catégorique s'y décline dans la nécessité de rendre les autres beaux, libres, et puissants et d'apprendre ensemble.
Le Droit apparaît comme s'élaborant dynamiquement dans le temps même où s'élabore la Cité.
La possibilité du radicalement nouveau travaillée dans la première partie autorise d'articuler la liberté et les institutions.
La logique d'un code d'honneur permet in fine de ne pas s'abandonner à la Raison toute puissante sans pour autant renoncer aux Lumières.
Dans un contexte collaboratif, le tutorat et les outils « d'awareness » constituent des solutions admises pour faire face à l'isolement qui très souvent, mène à l'abandon de l'apprenant.
Ainsi, du fait des difficultés rencontrées par le tuteur pour assurer un encadrement et un suivi appropriés à partir des traces de communication (en quantités conséquentes) laissées par les apprenants, nous proposons une approche multi-agents pour analyser les conversations textuelles asynchrones entre apprenants.
Ces indicateurs seront déduits à partir des grands volumes d'échanges textuels entre apprenants.
Bluecime a mis au point un système de vidéosurveillance à l'embarquement de télésièges qui a pour but d'améliorer la sécurité des passagers.
Ce système est déjà performant, mais il n'utilise pas de techniques d'apprentissage automatique et nécessite une phase de configuration chronophage.
L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui traite de l'étude et de la conception d'algorithmes pouvant apprendre et acquérir des connaissances à partir d'exemples pour une tâche donnée.
Une telle tâche pourrait consister à classer les situations sûres ou dangereuses dans les télésièges à partir d'exemples d'images déjà étiquetées dans ces deux catégories, appelés exemples d'entraînement.
L'algorithme d'apprentissage automatique apprend un modèle capable de prédire la catégories de nouveaux cas.
Depuis 2012, il a été démontré que les modèles d'apprentissage profond sont les modèles d'apprentissage machine les mieux adaptés pour traiter les problèmes de classification d'images lorsque de nombreuses données d'entraînement sont disponibles.
Dans ce contexte, cette thèse, financée par Bluecime, vise à améliorer à la fois le coût et l'efficacité du système actuel de Bluecime grâce à l'apprentissage profond.
Entre 2 et 5% des enfants de 6 mois à 5 ans présentent au moins un épisode de Crise d'Épilepsie en contexte Fébrile (CEF).
Bien que généralement bénignes, ces crises sont associées à un risque d'urgences neurologiques graves et curables dont l'élimination requiert la réalisation d'examens complémentaires douloureux et/ou irradiants.
Actuellement, ce risque est évalué en fonction de trois facteurs : l'âge de l'enfant, le caractère simple ou complexe de la crise, et l'examen clinique.
Cette thèse avait pour objectif de tester l'hypothèse que parmi les enfants consultant pour une CEF, seuls ceux avec un examen clinique anormal présentent un risque d'urgence neurologique grave et urgent.
Pour ce faire, nous avons créé un outil informatique permettant une recherche exhaustive de cas parmi un million de dossiers médicaux informatisés dans sept services d'urgences pédiatriques entre 2007 et 2011.
Nous avons alors identifié : les visites d'enfants présentant une CEF.
Nous avons ensuite évalué le risque d'urgence neurologique grave et curable associé à ces visites, notamment lorsque l'examen clinique au décours était normal.
Nous n'avons retrouvé aucune urgence neurologique grave et curable parmi les enfants consultant pour une CEF avec un examen clinique normal au décours, quels que soient l'âge et les caractéristiques de la crise.
Ce travail de thèse associé aux données de la littérature confirme notre hypothèse et souligne la nécessité de recommandations quant à la prise en charge de ces enfants.
Enfin, cette thèse constitue l'occasion de mener une réflexion méthodologique quant à l'utilisation de dossiers médicaux informatisés pour la recherche clinique.
Le concept de “Business Rule Management System ” (BRMS) a été introduit pour faciliter la création, la vérification, le déploiement et l'exécution des politiques commerciales propres à chaque compagnie.
Basée sur une approche symbolique, l'idée générale est de permettre aux utilisateurs métier de gérer les changements des règles métier dans le système sans avoir besoin de recourir à des compétences techniques.
Il s'agit donc de fournir à ces derniers la possibilité de formuler des politiques commerciales et d'automatiser leur traitement tout en restant proche du langage naturel.
De nos jours, avec l'expansion de ce type de systèmes, il faut faire face à des logiques de décision de plus en plus complexes et à de larges volumes de données.
Il n'est pas toujours facile d'identifier les causes conduisant à une décision.
On constate ainsi un besoin grandissant de justifier et d'optimiser les décisions dans de courts délais qui induit l'intégration à ses systèmes d'une composante d'explication évoluée.
Le principal enjeu de ces recherches est de fournir une approche industrialisable de l'explication des processus de décision d'un BRMS et plus largement d'un système à base de règles.
Cette approche devra être en mesure d'apporter les informations nécessaires à la compréhension générale de la décision, de faire office de justification auprès d'entités internes et externes ainsi que de permettre l'amélioration des moteurs de règles existants.
La réflexion se portera tant sur la génération des explications en elles-mêmes que sur la manière et la forme sous lesquelles elles seront délivrées.
Dans cette thèse, nous abordons le problème de prédiction de liens dans des tenseurs binaires d'ordre trois et quatre contenant des observations positives uniquement.
Ce type de tenseur apparaît dans les problèmes de recommandations sur le web, en bio-informatique pour compléter des bases d'interactions entre protéines, ou plus généralement pour la complétion bases de connaissances.
Ces dernières nous permettent d'évaluer nos méthodes de complétion à grande échelle et sur des types de graphes relationnels variés.
Notre approche est parallèle à celle de la complétion de matrice.
Nous résolvons de manière non-convexe un problème de minimisation empirique régularisé sur des tenseurs de faible rangs.
Dans un premier temps, nous validons empiriquement notre approche en obtenant des performances supérieures à l'état de l'art sur de nombreux jeux de données.
Ces performances ne peuvent être atteintes que pour des rangs trop élevés pour que cette méthode soit applicable à l'échelle de bases de connaissances complètes.
Nous nous intéressons dans un second temps à la décomposition Tucker, plus expressive que la décomposition Canonique, mais plus difficile à optimiser.
Ces méthodes nous permettent d'améliorer les performances en complétion pour une quantité faible de paramètres par entités.
Finalement, nous étudions le cas de base de connaissances temporelles, dans lesquels les prédicats ne sont valides que sur certains intervalles de temps.
Nous proposons une formulation faible rang et une régularisation adaptée à la structure du problème, qui nous permet d'obtenir des performances supérieures à l'état de l'art.
En effet,afin de rester compétitives, les entreprises concevant des systèmes cherchent à leur rajouter de plus en plusde fonctionnalités.
Cette compétitivité introduit aussi une demande de réactivité lors de la conception desystèmes, pour que le système puisse évoluer lors de sa conception et suivre les demandes du marché.
De plus, la langue naturelle est difficile à traiter automatiquement, par exemple, onpeut difficilement déterminer avec un programme informatique que deux exigences en langue naturelle secontredisent.
Cependant, la langue naturelle reste indispensable dans les spécifications que nous étudions,car elle reste un moyen de communication pratique et très répandu.
Nous cherchons à compléter ces exigences en langue naturelle avec des éléments permettant à la fois de lesrendre moins ambiguës et de faciliter les traitements automatiques.
Ces éléments peuvent faire partie demodèles (d'architecture par exemple) et permettent de définir le lexique et la syntaxe utilisés dans lesexigences.
Nous avons testé les principes proposés sur des spécifications industrielles réelles et développéun prototype logiciel permettant de réaliser des tests sur une spécification dotée de ces éléments de syntaxeet de lexique.
L'objectif de ce projet est de développer des outils pour analyser et traiter automatiquement la parole enregistrée de patients atteints de la maladie de Huntington, maladie génétique neurodégénérative grave, pour en tirer un marqueur permettant leur suivi clinique.
Ce projet portera sur la modélisation conjointe des déficits cognitifs et émotionnels, en utilisant une variété de représentations des signaux vocaux.
Nous allons examiner trois niveaux de représentations sur les différents protocoles : Intelligibilité et mesures articulatoires / Représentations acoustiques et prosodique du signal vocal / Représentations linguistiques.
En particulier, nous utiliserons des techniques d'apprentissage automatique non supervisées pour détecter les motifs répétés dans le signal, mais aussi quantifier les trajectoires individuelles évolutives des patients dans les différentes dimensions de la maladie.
La diversité des aptitudes testées dans la parole des patients nous permettra de rendre compte de l'hétérogénéité de leurs symptômes observée en clinique (troubles cognitifs, comportementaux, émotionnels et moteurs).
Ce projet pourra offrir aux cliniciens et aux chercheurs des marqueurs composites reflétant fidèlement l'évolution de la maladie propre à chaque patient.
Ce travail de recherche relève du domaine de la didactique de l'espagnol et porte sur les représentations sociales des étudiants du bassin du Département de Saint-Denis (lle de France) et leurs attitudes envers cette langue.
Les effets du dispositif sur les acquisitions langagières ont été mesurés d'une part par le biais d'un pré-test et d'un post-test sommatif pour évaluer les compétences linguistiques en CO, CE et EE courte inspirées des critères du Cecrl, et d'autre part sur une analyse d'un échantillon des productions des étudiants.
Les utilisations et les modes d'appropriation de la plate-forme par les étudiants sont mesurés par un traçage sur la plateforme, par les commentaires effectués lors des validations semestrielles et par un questionnaire d'attitude.
Nos résultats ont mis en évidence des acquisitions langagières et un changement d'attitude pour deux tiers des étudiants et une progression en fluidité et en précision des énoncés, selon les niveaux de départ et visant le niveau B1-B2 de la passation Gies 1 et 2.
Si la majorité des étudiants interrogés évaluent positivement le dispositif et le travail en groupe auxquels ils ont été confrontés, un tiers cependant dit avoir eu des difficultés à assumer la charge de travail imposée et a préféré le travail en situation d'autonomie.
L'investissement coordonné des différents acteurs qu'exige ce mode d'apprentissage est une des solutions de partage réclamée par les enseignants de L2 dans l'accompagnement hybride faute de quoi les initiatives dans le domaine de l'accompagnement numérique en enseignement/apprentissage des langues risquent fort de demeurer individuelles et isolées.
La construction montre un retard de productivité par rapport à l'industrie.
Pour y remédier, le monde de la construction met en place le processus BIM (Building Information Modeling).
Celui-ci repose sur l'utilisation d'une maquette numérique 3D, reproduction du bâtiment, contenant toutes les informations nécessaires à sa réalisation.
Cependant, les ingénieurs ont remonté des difficultés dans l'utilisation du BIM.
La maquette contient énormément d'éléments provenant de plusieurs intervenants.
Le processus BIM propose la même méthode de travail aux différents types de métiers de la construction.
Le but est d'offrir un environnement adapté à tout type de métier du bâtiment tout en respectant des procédés de conception actuelle.
Ce travail propose une étude sur la visualisation des connaissances métiers ainsi qu'une interaction avec des dispositifs immersifs pour la construction.
Dans un premier temps, ce manuscrit propose d'étudier la classification des divers éléments d'une maquette BIM en utilisant l'apprentissage automatique, assister la visualisation des données en utilisant les modèles de saillance et une interface reposant sur la collaboration.
Dans un second temps, deux salles de réalité virtuelle dédiées à la construction seront décrites.
Le travail effectué dans cette thèse présente une évaluation des techniques de transformation de voix à base de GMM.
Ces techniques de transformation linéaires malgré leurs qualités obtenues, elles ne manquent pas de quelques défauts, on peut noter le sur-lissage, le problème de distorsion spectrale et le sur-apprentissage.
Dans un premier volet, nous avons pris en compte ces questions pour adapter la stratégie d'apprentissage des fonctions de conversion.
La première c'est la réduction du nombre des paramètres libres de la fonction de conversion.
La deuxième considère que les solutions par transformation linéaire sont instables face au peu de données d'apprentissage, d'où le recours aux modèles de transformation non-linéaire de type RBF.
Dans un deuxième volet, pour aligner les données non-parallèles des locuteurs source et cible, une solution consiste à correspondre ces données via une représentation récursive d'un arbre binaire.
Les dispositifs matériels mobiles proposent des capacités de mesure à l'aide de capteurs soit embarqués, soit connectés.
Ils présentent un caractère critique dans le sens où ces informations doivent être fiables, car potentiellement utilisées dans un contexte exigeant.
Malgré une grande demande, peu d'applications proposent d'assister les utilisateurs lors de relevés exploitant ces capacités.
Idéalement, ces applications devraient proposer des méthodes de visualisation, de calcul, des procédures de mesure et des fonctions de communications permettant la prise en charge de capteurs connectés ou encore la génération de rapports.
La rareté de ces applications se justifie par les connaissances nécessaires pour permettre la définition de procédures de mesure correctes.
Ces éléments sont apportés par la métrologie et la théorie de la mesure et sont rarement présents dans les équipes de développement logiciel.
Ce postulat apporte la question de recherche à laquelle les travaux présentés répondent : Comment proposer une approche pour la conception d'applications adaptées à des procédures de mesures spécifiques.
Les procédures de mesure pouvant être configurées par un utilisateur final
La réponse développée est une "plateforme" de conception d'applications d'assistance à la mesure.
Elle permet d'assurer la conformité des procédures de mesures sans l'intervention d'expert de la métrologie.
Cette expertise comprend des termes et des règles assurant l'intégrité et la cohérence d'une procédure de mesure.
Un modèle conceptuel du domaine de la métrologie est proposé.
Ce modèle conceptuel est ensuite intégré au processus de développement d'une application.
Il permet, la vérification du respect des contraintes inhérentes à la métrologie dans une procédure de mesure.
Cette vérification est réalisée en confrontant les procédures de mesures au schéma sous forme de requêtes.
Ces requêtes sont décrites à l'aide d'un langage proposé par le schéma.
Pour cela, un éditeur d'application est proposé.
Ce langage est construit à partir des concepts, formalismes et outils proposés par l'environnement de métamodélisation Diagrammatic Predicate Framework (DPF).
L'influence de l'anglais est évidente sur les langues du monde entier.
L'anglais est considéré comme une langue de communication mondiale et est utilisé par un grand nombre de locuteurs du monde entier dans leurs interactions.
Il est clair que l'anglais domine dans de nombreux aspects de la vie quotidienne tels que la technologie, la science, les médias et Internet.
Toutes les influences observées sur les langues du monde qui sont dues à l'influence de l'anglais relèvent de la notion d'anglicisation, qui couvre tous les niveaux de l'analyse linguistique.
Dans ma thèse, j'étudie l'influence de l'anglais sur le grec moderne, qui a été particulièrement forte au cours des deux à trois dernières décennies.
J'examine le phénomène de l'anglicisation du grec moderne en tenant compte de l'influence de l'anglais à tous les niveaux de l'analyse linguistique, et particulièrement au niveau lexical, phraséologique et morphosyntaxique.
Pour la collecte et l'analyse de mes données, j'utilise des dictionnaires et des grammaires du grec moderne, ainsi que des corpus de textes comme le Trésor National de la Langue Grecque (Hellenic National Corpus), le Corpus des Textes Grecs (Corpus of Greek Texts) et les corpus de textes disponibles via la plateforme Sketch Engine, ainsi qu'un corpus de textes personnalisé que j'ai construit exclusivement pour mon étude via Sketch Engine.
De plus, j'étudie les facteurs responsables de l'utilisation des formes non translittérées des emprunts anglais en examinant leur apparition dans des vocabulaires spécialisés du grec moderne tels que le vocabulaire du sport et de la technologie.
En ce qui concerne les unités phraséologiques et les structures morphosyntaxiques calquées, je compare la fréquence d'apparition de la structure calquée à la fréquence d'apparition de la structure équivalente en grec moderne.
De plus, j'essaie de déterminer la chronologie de l'insertion des emprunts anglais en grec moderne, et, enfin, je tire quelques conclusions générales concernant l'anglicisation du grec moderne à partir des résultats de ma recherche.
Le déploiement massif de capteurs couplé à leur exploitation dans de nombreux secteurs génère une masse considérable de données multivariées qui se sont révélées clés pour la recherche scientifique, les activités des entreprises et la définition de politiques publiques.
Plus spécifiquement, les données multivariées qui intègrent une évolution temporelle, c'est-à-dire des séries temporelles, ont reçu une attention toute particulière ces dernières années, notamment grâce à des applications critiques de monitoring (e.g.
Cependant, les explications du modèle de remplacement ne peuvent pas être parfaitement exactes au regard du modèle original, ce qui constitue un prérequis pour de nombreuses applications.
L'exactitude est cruciale car elle correspond au niveau de confiance que l'utilisateur peut porter aux explications relatives aux prédictions du modèle, c'est-à-dire à quel point les explications reflètent ce que le modèle calcule.
Cette thèse propose de nouvelles approches pour améliorer la performance et l'explicabilité des méthodes d'apprentissage automatique de séries temporelles multivariées, et établit de nouvelles connaissances concernant deux applications réelles.
Aujourd'hui, l'apprentissage des langues assisté par ordinateur est de plus en plus répandu, dans les institutions publiques et privées.
Cependant, il est encore loin des attentes des enseignants et des apprenants et ne répond pas encore à leurs besoins.
Les systèmes d'apprentissage des langues assisté par ordinateur (ALAO) actuels sont plutôt des environnements de tests des connaissances de l'apprenant et ressemblent plus à un support d'apprentissage traditionnel.
De plus, le feedback proposé par ces systèmes reste basique et ne peut pas être adapté pour un apprentissage autonome, car, il devrait être en mesure de diagnostiquer les problèmes d'un apprenant avec l'orthographe, la grammaire, la conjugaison,etc., puis générer intelligemment un feedback adéquat selon la situation de l'apprentissage.
Cette recherche expose les capacités des outils TAL à apporter des solutions aux limitations des systèmes d'ALAO dans le but d'élaborer un système d'ALAO complet et autonome.
Nous présentons une architecture complète d'un système multilingue pour l'apprentissage des langues assisté par ordinateur destiné aux apprenants des langues étrangères, français et arabe.
Ce système pourrait être utilisé pour l'apprentissage des langues par les apprenants de la langue en tant que langue seconde ou étrangère.
La première partie de nos travaux porte sur l'adaptation des outils et des ressources issues du TAL pour qu'ils soient utilisés dans un environnement d'apprentissage des langues assisté par ordinateur.
Parmi ces outils et ressources, il y a les analyseurs morphologiques pour l'arabe et le français, corpus, dictionnaires électroniques, etc.
Ensuite, dans la deuxième section, nous présentons la reconnaissance de l'écriture manuscrite en ligne.
Dans cette optique, nous exposons une approche statistique basée sur le réseau de neurones, puis, nous présentons la conception de l'architecture du système de reconnaissance ainsi que l'implémentation de l'algorithme de la reconnaissance.
La deuxième partie de notre exposé porte sur l'élaboration, l'intégration et l'exploitation des outils TAL utilisés (analyseurs morphologiques, système de reconnaissance de l'écriture, dictionnaires, etc.) dans notre système d'apprentissage des langues assisté par ordinateur.
Nous y présentons aussi les modules ajoutés à la plate-forme pour avoir une architecture complète d'un système d'ALAO.
Parmi ces modules, figure le générateur de feedback qui permet de corriger les fautes des apprenants et générer un feedback pédagogique pertinent qui permet à l'apprenant de cerner et ses fautes.
Enfin, nous décrivons l'outil de génération automatique des activités pédagogiques variées et automatisées.
Le deuxième chapitre (en collaboration avec Xavier Lambin) étudie l'impact de la réputation digitale sur la discrimination ethnique.
Le troisième chapitre (rédigé en collaboration avec Rossi Abi Rafeh) développe et estime un modèle de dynamique industrielle.
Avec plusieurs entrants en cours, le titulaire du brevet exploite ces externalités en proposant des accords de licence à certains entrants ou en poursuivant des contentieux afin de diminuer le coût des contrats dilatoires offerts aux autres.
Le nombre des entrants tardifs augmente en fonction de la solidité du brevet.
Le deuxième chapitre montre que les systèmes de réputation peuvent atténuer la discrimination ethnique en permettant les vendeurs appartenant à des minorités ethniques de construire une bonne réputation rapidement, ce qui entraîne les acheteurs d'actualiser leurs convictions.
En utilisant une base de données collectée sur une plateforme de covoiturage, nous trouvons qu'en absence d'avis, les conducteurs membres des minorités ethniques gagnent 12% moins de revenue par rapport aux conducteurs non membres des minorités.
Pour comprendre le mécanisme derrière ce processus, nous concevons un modèle de 'career concerns' des vendeurs discriminés en présence d'un système de réputation.
Les estimations du modèle montrent que les conducteurs appartenant à des minorités ethniques, qui viennent d'entrer dans la plateforme, font face à des convictions trop pessimistes quant à la qualité de leur service.
Pour changer ces convictions, ils exercent de grands efforts et proposent des bas prix de lancement, pour renforcer rapidement leur réputation.
Des simulations contrefactuelles révèlent que le coût des croyances antérieures erronées est élevé et que le système de réputation bénéficie strictement aux conducteurs des minorités ethniques.
Le dernier chapitre étudie l'entrée sur un marché avec un système de réputation et les décisions de formation de prix des vendeurs.
Nous proposons un modèle d'oligopole dynamique avec une hétérogénéité des coûts marginaux et des coûts d'opportunité, et avec la réputation individuelle comme une variable d'état.
Nous montrons que les nouveaux vendeurs sont généralement moins susceptibles de rejoindre la plateforme par rapport aux anciens, et les vendeurs avec une faible chance de rejoindre dans les périodes suivantes mettent des prix moyens plus élevés.
Le mécanisme derrière ces résultats est la sélection selon les coûts marginaux.
Notre modèle s'appuie sur une base de données de vendeurs sur une grande plateforme d'un marché de covoiturage.
Nous constatons une corrélation négative des utilisateurs expérimentés de la plateforme, mesurée par le nombre d'avis et les prix déterminés par les conducteurs.
Cependant, après avoir pris en compte les caractéristiques non observées des conducteurs, dont lesquelles on interprète comme coûts marginaux, nous trouvons une relation positive.
Par ailleurs, en étudiant les décisions d'une nouvelle entrée sur la plateforme, nous démontrons la sélection des non-observables.
Finalement, nous découvrons la distribution des coûts d'opportunité.
L'objectif général de ce projet doctoral est de proposer des nouvelles approches automatiques non- ou faiblement supervisées visant à caractériser une conversation orale à partir de son enregistrement audio et sa transcription (automatique ou manuelle quand elle est disponible).
Dans un premier temps, on s'intéressera à l'identification nommée des différents locuteurs.
Sans aucune connaissance a priori sur la qualité et le nombre des participants à la conversation, ce problème peut se découper en deux sous-problèmes : la constitution automatique de la liste des participants à la conversation (grâce à des outils de détection d'entités nommées et d'entity linking, par exemple) suivi de l'attribution automatique de leurs tours de parole respectives (en combinant compréhension de dialogue et reconnaissance du locuteur, par exemple).
Dans un second temps, on s'interessera à la caractérisation des conversations selon une typologie permettant d'identifier la nature des échanges (argumentation, débat, altercation, etc.).
Cette caractérisation pourra se faire en utilisant des techniques de traitement automatique des langues enrichies d'informations acoustiques (informations prosodiques par exemple) afin d'améliorer les performances.
Ces nouvelles approches seront appliquées sur un corpus composé de films (Anna et ses soeurs, les films Harry Potter) et de séries télévisées de différente nature (Lost, Friends, The Big Bang Theory, Game of Thrones, etc.).
L'objectif de cette thèse est de développer un outil qui fédère l'ensemble des recherches lancées à l'IDHN dans le cadre de l'analyse du discours politique numérique par le biais de plateformes, afin de les agréger dans une même interface, et de les faire fonctionner ensemble, dans l'optique d'une analyse globale du phénomène de l'influence.
Notre choix se porte sur viky.ai qui est une plateforme open source développée par la société Pertimm, pour créer et partager des agents linguistiques qui analysent des textes dans toutes les langues.
La création d'agents d'analyse modifiables et réutilisables, qui peuvent être partagés en collaboration par la communauté, peut contribuer à simplifier considérablement la gestion de l'analyse textuelle sur des problèmes que l'on retrouve de façon similaire dans de nombreux domaines.
viky.ai offre une interface utilisateur unique et sans code.
C'est une nouvelle façon de créer des modules sémantiques qui sont faciles à intégrer dans n'importe quel système par le biais d'une API.
Les briques sémantiques, appelées « agents » de viky.ai, sont des assistants multilingues pour trouver des données pertinentes dans les textes.
Ces briques sont constituées d'entités et d'interprétations et peuvent s'assembler à l'infini pour construire des modèles dans tous les domaines de l'analyse sémantique.
Beaucoup d'experts du traitement du langage s'accordent à penser que les technologies de TAL alliant à la fois des règles et de l'apprentissage, seront la combinaison gagnante.
Plus concrètement, la thèse visera à développer des agents dans deux domaines de recherche déjà esquissés : détection et caractérisation des idéologies, et liens avec le discours émotionnel ; détection des changements de thèmes dans les discours politiques.
Cette thèse en informatique se situe dans le domaine des Environnements Informatiques pour l'Apprentissage Humain (EIAH), et plus particulièrement au sein du projet AGATE (an Approach for Genericity in Assistance
To complEx tasks) qui vise à proposer des modèles génériques et des outils unifiés pour permettre la mise en place de systèmes d'assistance dans des applications existantes.
Ce système d'assistance peut ensuite être exécuté par le moteur d'assistance de SEPIA pour fournir de l'assistance aux utilisateurs finaux sur les applications
Des ingénieurs pédagogiques endossent donc le rôle de concepteurs d'assistance, alors que les apprenants sont les utilisateurs finaux des applications assistées.
Nous avons abordé cette problématique de recherche en deux étapes : tout d'abord l'étude d'assistances existantes au sein d'applications utilisées en contexte éducatif, puis l'exploitation et l'enrichissement des modèles et outils du projet AGATE pour les adapter au contexte éducatif.
Dans un premier temps, nous avons étudié des applications variées utilisées par des enseignants au sein de leurs cours, ainsi que des travaux existants qui proposent des systèmes d'assistance.
Dans un second temps, nous avons confronté les modèles et outils proposés précédemment dans le projet AGATE aux caractéristiques de l'assistance ainsi identifiées dans le contexte éducatif.
Les limites des modèles et outils précédents nous ont amené à proposer deux contributions au langage aLDEAS et au système SEPIA pour les adapter au contexte éducatif.
Que ce soit dans un contexte éducatif ou non, il est important de pouvoir définir facilement et de manière explicite plusieurs modes d'articulation entre les différents éléments d'un système d'assistance.
Nous avons donc proposé un modèle d'articulation entre les règles aLDEAS explicitant le déroulement d'une assistance et permettant de définir des systèmes d'assistance comprenant des éléments qui se déroulent de manière successive, interactive, simultanée, progressive, indépendante.
Ce modèle et ce processus ont été implémentés dans SEPIA
Elle concernait la complexité à définir un guidage pédagogique proposant un parcours entre différentes activités au sein d'une application existante.
Nous avons tout d'abord proposé un modèle d'activité permettant de délimiter les activités au sein des applications...
C'est presque une banalité que de dire qu'une des caractéristiques principales de la parole est sa variabilité : variabilité inter-sexe, inter-locuteur, mais aussi variabilité d'un contexte à un autre ou d'une répétition à une autre pour un même sujet.
C'est cette variabilité qui fait à la fois la beauté de la parole mais aussi la complexité de son traitement par les technologies vocales, et la difficulté pour en comprendre les mécanismes.
Dans cette thèse nous étudions certains aspects de cette variabilité, avec comme point de départ la variabilité observée chez un locuteur dans la répétition d'un même son dans les mêmes conditions, que nous appelons variabilité intrinsèque.
Les modèles de contrôle moteur de la parole abordent principalement la variabilité contextuelle de la parole mais prennent rarement en compte sa variabilité intrinsèque, alors même que l'on sait que c'est cette variabilité qui donne à la parole tout son caractère naturel.
L'objectif principal de cette thèse est d'aborder la variabilité intrinsèque et contextuelle de la production de la parole dans un cadre formel intégrateur.
Pour cela nous faisons l'hypothèse que la variabilité intrinsèque n'est pas que le résultat d'un bruit d'exécution, mais qu'elle résulte aussi d'une stratégie de contrôle où la variabilité inter-répétition fait partie intégrante de la représentation de la tâche.
Méthodologie :
Nous formalisons cette idée dans un cadre computationnel probabiliste, la modélisation Bayésienne, où l'abondance de réalisations possibles d'un même item de parole est représentée naturellement sous la forme d'incertitudes, et où la variabilité est donc manipulée formellement.
Nous illustrons la pertinence de cette approche à travers trois contributions.
Résultats :
Dans un premier temps, nous reformulons un modèle existant de contrôle optimal de la parole, le modèle GEPPETO, dans le formalisme probabiliste et démontrons que le modèle Bayésien contient GEPPETO comme un cas particulier.
Cette étape a nécessité l'élaboration d'hypothèses sur l'intégration des retours sensoriels dans la planification, dont nous avons cherché à évaluer la pertinence en concevant une expérience originale de production
Cela est rendu possible grâce à la représentation unifiée des connaissances dans le modèle, qui permet d'intégrer la production et la perception dans un cadre formel unique.
L'ensemble de ces travaux illustre la capacité du formalisme Bayésien à proposer une démarche systématique et structurée pour la construction des modèles.
Cette démarche facilite le développement des modèles et leur complexification progressive en précisant et explicitant les hypothèses formulées.
Le projet de thèse s'inscrit globalement dans la thématique de l'outillage de la linguistique de terrain.
Dans un contexte où la diversité linguistique est menacée par la disparition programmée de près de la moitié des langues qui sont aujourd'hui parlées sur la terre, il devient crucial de doter les spécialistes de linguistique de terrain d'outils automatiques ou semi-automatiques pour recueillir des données linguistiques, les annoter, les enrichir et les archiver, et tenter par là de préserver une partie du patrimoine culturel de l'humanité.
Ces problématiques rencontrent un intérêt croissant au sein de la communauté du traitement automatique des langues, dans le cadre de collaboration soutenues avec des équipes de linguistes.
Cette thèse se déroulera ainsi dans le cadre d'une collaboration internationale impliquant des équipes de linguistes en France et en Allemagne, avec le soutien de l'Agence Nationale de la Recherche.
Du point de vue méthodologique, la première tâche peut être abordée avec des outils de la modélisation statistique, tels que les modèles bayésiens non-paramétriques, dont on étudiera ici l'applicabilité dans un contexte où les données à annoter sont de petite taille, mais où il existe des ressources complémentaires potentiellement mobilisables (lexiques, éléments de description morphologique, etc).
Le développement de ces divers types de modèles statistiques sera validée sur les langues du projet au travers d'expérimentations impliquant une collaboration étroite avec des utilisateurs de ces outils.
La compétitivité toujours plus importante et la mondialisation ont mis l'industrie manufacturière au défi de rationaliser les différentes façons de mettre sur le marché de nouveaux produits dans un délai court, avec des prix compétitifs tout en assurant des niveaux de qualité élevés.
Le PDP moderne exige simultanément la collaboration de plusieurs groupes de travail qui assurent la création et l'échange d'information avec des points de vue multiples dans et à travers les frontières institutionnelles.
Dans ce contexte, des problèmes d'interopérabilité sémantique ont été identifiés en raison de l'hétérogénéité des informations liées à des points de vue différents et leurs relations pour le développement de produits.
Le travail présenté dans ce mémoire propose un cadre conceptuel d'interopération pour la conception et la fabrication de produits.
Un système expérimental a été proposé à l'aide de l'outil Protégé pour modéliser des ontologies de base et d'une plateforme Java intégrée à Jena pour développer l'interface avec l'utilisateur.
Le concept et la mise en œuvre de cette recherche ont été testés par des expériences en utilisant des produits tournants en plastiques.
Les résultats ont montré que l'information et ses relations rigoureusement définies peuvent assurer l'efficacité de la conception et la fabrication du produit dans un processus de développement de produits moderne et collaboratif
Cette thèse comprend trois expériences et deux prétests (N = 1135) dans lesquelles sont étudiés trois aspects fondamentaux du design statique des messages sur internet : son format (infographie, audio ou texte), sa couleur et sa typographie, sur la thématique du recyclage des déchets électroniques (études 1 et 2) puis à propos de la migration humaine (étude 3).L'étude des aspects graphiques est pertinente si l'on veut augmenter la force persuasive d'un message.
Le format joue un rôle prépondérant (étude 1a), permettant à la fois de changer les attitudes, mais aussi d'ancrer ce changement dans le temps.
Les couleurs, par contre, ne semblent pas faire varier la force persuasive du message ou amener les lecteurs à agir en faveur du recyclage (étude 1b).
La typographie ne semble pas non plus jouer de rôle dans la dynamique persuasive, qu'elle soit jugée lisible ou peu lisible (étude 2).
Des pistes théoriques concernant la personnalité des typographies et leur cohérence avec le contexte sont développées.
L'analyse des composantes de l'ELM a révélé, dans chaque étude, le fort lien entre l'attitude des individus et leur sentiment de responsabilité personnelle envers la thématique abordée ainsi que leurs connaissances a priori.
Nous avons également vu que les leviers de persuasion ne sont pas systématiquement les mêmes selon le besoin de cognition.
Globalement, nous suggérons que les messages persuasifs doivent adopter un format permettant une analyse centrale à faible coût cognitif, utilisant une couleur principale et une typographie toutes deux lisibles et cohérentes avec la thématique développée, avec un argumentaire qui renforce le sentiment de responsabilité des lecteurs.
Le développement d'un environnement collaboratif est un processus complexe.
La complexité réside dans le fait que ce développement implique beaucoup de prise de décisions.
De multiples compromis doivent être faits pour répondre aux exigences actuelles et futures d'utilisateurs aux profils variés.
La prise en compte de cette complexité pose des problèmes aux chercheurs, développeurs et utilisateurs.
Les informations et données requises pour prendre des décisions adéquates de conception et évaluer rigoureusement ces décisions sont nombreuses, parfois indéterminées et en constante évolution.
Nous en déduisons trois modèles : SyCoW (travail collaboratif synchrone), SyCoE (environnement collaboratif synchrone) et SyCoEE (évaluation environnement collaboratif synchrone).
Dans la partie II de cette thèse, nous proposons un processus pour la sélection / développement d'un environnement collaboratif, où nous démontrons comment les modèles SyCoW, SyCoE et SyCoEE structurent ce processus.
Les résultats de l'évaluation ont confirmé la convivialité de MT-DT et fournissent des éléments de validation des choix que nous avons faits au cours du développement de MT-DT.
L'apprentissage par transfert de réseaux profonds réduit considérablement les coûts en temps de calcul et en données du processus d'entraînement des réseaux et améliore largement les performances de la tâche cible par rapport à l'apprentissage à partir de zéro.
Cependant, l'apprentissage par transfert d'un réseau profond peut provoquer un oubli des connaissances acquises lors de l'apprentissage de la tâche source.
Puisque l'efficacité de l'apprentissage par transfert vient des connaissances acquises sur la tâche source, ces connaissances doivent être préservées pendant le transfert.
Cette thèse résout ce problème d'oubli en proposant deux schémas de régularisation préservant les connaissances pendant l'apprentissage par transfert.
Nous examinons d'abord plusieurs formes de régularisation des paramètres qui favorisent toutes explicitement la similarité de la solution finale avec le modèle initial, par exemple, L1, L2, et Group-Lasso.
Nous proposons également les variantes qui utilisent l'information de Fisher comme métrique pour mesurer l'importance des paramètres.
Le second schéma de régularisation est basé sur la théorie du transport optimal qui permet d'estimer la dissimilarité entre deux distributions.
Nous nous appuyons sur la théorie du transport optimal pour pénaliser les déviations des représentations de haut niveau entre la tâche source et la tâche cible, avec le même objectif de préserver les connaissances pendant l'apprentissage par transfert.
Au prix d'une légère augmentation du temps de calcul pendant l'apprentissage, cette nouvelle approche de régularisation améliore les performances des tâches cibles et offre une plus grande précision dans les tâches de classification d'images par rapport aux approches de régularisation des paramètres.
Dans le monde d'aujourd'hui de multiples acteurs de la technologie numérique produisent des quantités infinies de données.
Capteurs, réseaux sociaux ou e-commerce, ils génèrent tous de l'information qui s'incrémente en temps-réel selon les 3 V de Gartner : en Volume, en Vitesse et en Variabilité.
L'objectif premier de cette étude est de pouvoir établir au moyen de ces approches une vision intégratrice du cycle de vie des données qui s'établit selon 3 étapes, (1) la synthèse des données via la sélection des valeurs-clés des micro-données acquises par les différents opérateurs au niveau de la source, (2) la fusion en faisant le tri des valeurs-clés sélectionnées et les dupliquant suivant un aspect de dé-normalisation afin d'obtenir un traitement plus rapide des données et (3) la transformation en un format particulier de carte de cartes de cartes, via Hadoop dans le processus classique de MapReduce afin d'obtenir un graphe défini dans la couche applicative.
Cette réflexion est en outre soutenue par un prototype logiciel mettant en oeuvre les opérateurs de modélisation sus-décrits et aboutissant à une boîte à outils de modélisation comparable à un AGL et, permettant une mise en place assistée d'un ou plusieurs traitements sur BigData
Avec la mise en place d'entrepôts de données cliniques, de plus en plus de données de santé sont disponibles pour la recherche.
Si une partie importante de ces données existe sous forme structurée, une grande partie des informations contenues dans les dossiers patients informatisés est disponible sous la forme de texte libre qui peut être exploité pour de nombreuses tâches.
Dans ce manuscrit, deux tâches sont explorées~ : la classification multi-étiquette de textes cliniques et la détection de la négation et de l'incertitude.
La première est étudiée en coopération avec le centre hospitalier universitaire de Rennes, propriétaire des textes cliniques que nous exploitons, tandis que, pour la seconde, nous exploitons des textes biomédicaux librement accessibles que nous annotons et diffusons gratuitement.
Afin de résoudre ces tâches, nous proposons différentes approches reposant principalement sur des algorithmes d'apprentissage profond, utilisés en situations d'apprentissage supervisé et non-supervisé.
Notre recherche porte sur la recommandation de nouvelles offres d'emploi venant d'être postées et n'ayant pas d'historique d'interactions (démarrage à froid).
Nous adaptons les systèmes de recommandations bien connus dans le domaine du commerce électronique à cet objectif, en exploitant les traces d'usage de l'ensemble des demandeurs d'emploi sur les offres antérieures.
Une des spécificités du travail présenté est d'avoir considéré des données réelles, et de s'être attaqué aux défis de l'hétérogénéité et du bruit des documents textuels.
La contribution présentée intègre l'information des données collaboratives pour apprendre une nouvelle représentation des documents textes, requise pour effectuer la recommandation dite à froid d'une offre nouvelle.
Cette représentation dite latente vise essentiellement à construire une bonne métrique.
L'espace de recherche considéré est celui des réseaux neuronaux.
Les réseaux neuronaux sont entraînés en définissant deux fonctions de perte.
La première cherche à préserver la structure locale des informations collaboratives, en s'inspirant des approches de réduction de dimension non linéaires.
La seconde s'inspire des réseaux siamois pour reproduire les similarités issues de la matrice collaborative.
Le passage à l'échelle de l'approche et ses performances reposent sur l'échantillonnage des paires d'offres considérées comme similaires.
L'intérêt de l'approche proposée est démontrée empiriquement sur les données réelles et propriétaires ainsi que sur le benchmark publique CiteULike.
Enfin, l'intérêt de la démarche suivie est attesté par notre participation dans un bon rang au challenge international RecSys 2017 (15/100 ; un million d'utilisateurs pour un million d'offres).
Durant ces dernières années, les architectures de réseaux de neurones (RN) ont été appliquées avec succès à de nombreuses applications en Traitement Automatique de Langues (TAL), comme par exemple en Reconnaissance Automatique de la Parole (RAP) ainsi qu'en Traduction Automatique (TA).
Pour la tâche de modélisation statique de la langue, ces modèles considèrent les unités linguistiques (c'est-à-dire des mots et des segments) à travers leurs projections dans un espace continu (multi-dimensionnel), et la distribution de probabilité à estimer est une fonction de ces projections.
Ainsi connus sous le nom de "modèles continus" (MC), la particularité de ces derniers se trouve dans l'exploitation de la représentation continue qui peut être considérée comme une solution au problème de données creuses rencontré lors de l'utilisation des modèles discrets conventionnels.
Dans le cadre de la TA, ces techniques ont été appliquées dans les modèles de langue neuronaux (MLN) utilisés dans les systèmes de TA, et dans les modèles continus de traduction (MCT).
L'utilisation de ces modèles se sont traduit par d'importantes et significatives améliorations des performances des systèmes de TA. Ils sont néanmoins très coûteux lors des phrases d'apprentissage et d'inférence, notamment pour les systèmes ayant un grand vocabulaire.
Afin de surmonter ce problème, l'architecture SOUL (pour "Structured Output Layer" en anglais) et l'algorithme NCE (pour "Noise Contrastive Estimation", ou l'estimation contrastive bruitée) ont été proposés : le premier modifie la structure standard de la couche de sortie, alors que le second cherche à approximer l'estimation du maximum de vraisemblance (MV) par une méthode d'échantillonnage.
Toutes ces approches partagent le même critère d'estimation qui est la log-vraisemblance ; pourtant son utilisation mène à une incohérence entre la fonction objectif définie pour l'estimation des modèles, et la manière dont ces modèles seront utilisés dans les systèmes de TA.
Cette dissertation vise à concevoir de nouvelles procédures d'entraînement des MC, afin de surmonter ces problèmes.
Les contributions principales se trouvent dans l'investigation et l'évaluation des méthodes d'entraînement efficaces pour MC qui visent à : (i) réduire le temps total de l'entraînement, et (ii) améliorer l'efficacité de ces modèles lors de leur utilisation dans les systèmes de TA.
D'un côté, le coût d'entraînement et d'inférence peut être réduit (en utilisant l'architecture SOUL ou l'algorithme NCE), ou la convergence peut être accélérée.
La dissertation présente une analyse empirique de ces approches pour des tâches de traduction automatique à grande échelle.
D'un autre côté, nous proposons un cadre d'apprentissage discriminant qui optimise la performance du système entier ayant incorporé un modèle continu.
Les résultats expérimentaux montrent que ce cadre d'entraînement est efficace pour l'apprentissage ainsi que pour l'adaptation des MC au sein des systèmes de TA, ce qui ouvre de nouvelles perspectives prometteuses.
Le concept du web des objets (WOT - web of things) est devenu une réalité avec le développement d'internet, des réseaux, des technologies matérielles et des objets communicants.
De nos jours, il existe un nombre croissant d'objets susceptibles d'être utilisés dans des applications spécifiques.
Le Monde est ainsi plus étroitement connecté, différents objets pouvant maintenant partager leurs informations et être ainsi utilisés à travers une structure similaire à celle du Web classique.
Cependant, même si des objets hétérogènes ont la possibilité de se connecter au Web, ils ne peuvent pas être utilisés dans différentes applications à moins de posséder un modèle de représentation et d'interrogation commun capable de prendre en compte leur hétérogénéité.
Dans cette thèse, notre objectif est d'offrir un modèle commun pour décrire les objets hétérogènes et pouvoir ensuite les utiliser pour accéder aux requêtes des utilisateurs.
Ceux-ci peuvent avoir différentes demandes, que ce soit pour trouver un objet particulier ou pour réaliser certaines tâches.
Nous mettons en évidence deux directions de recherche.
Dans un premier temps, nous étudions d'abord les technologies, les applications et les domaines existants où le WOT peut être appliqué.
Nous comparons les modèles de description existants dans ce domaine et nous mettons en évidence leurs insuffisances lors d'applications relatives au WOT.
Cette thèse de doctorat est centrée sur l'apprentissage supervisé.
Il faut imposer des contraintes sur la variance du problème comme une condition de Bernstein ou de marge.
On dit qu'un estimateur est robuste si ce dernier présente certaines garanties théoriques, sous le moins d'hypothèses possibles.
Cette problématique de robustesse devient de plus en plus populaire.
Ainsi, construire des estimateurs fiables dans cette situation est essentiel.
Dans cette thèse nous montrons que le fameux minimiseur du risque empirique (regularisé) associé à une fonction de perte Lipschitz est robuste à des bruits à queues lourde ainsi qu'a des outliers dans les labels.
En revanche si la classe de prédicteurs est à queue lourde, cet estimateur n'est pas fiable.
Dans ce cas, nous construisons des estimateurs appelé estimateur minmax-MOM, optimal lorsque les données sont à queues lourdes et possiblement corrompues.
En grande dimension, certains estimateurs interpolant les données peuvent être bons.
En particulier, cette thèse nous étudions le modèle linéaire Gaussien en grande dimension et montrons que l'estimateur interpolant les données de plus petite norme est consistant et atteint même des vitesses rapides.
L'analyse chimique de la composante odorante repose sur une stratégie séparative qui permet d'identifier les différents odorants présents dans l'aliment.
Cependant, la perception des odorants en mélange induit des interactions au niveau perceptif qui ne sont pas prises en compte dans les techniques séparatives.
Les mécanismes sous-jacents aux interactions perceptives sont mal connus, ce qui limite les possibilités de prédiction de l'odeur d'un aliment sur la base de sa composition chimique.
Cependant, les études concernent des odorants seuls et non leurs mélanges.
La seconde repose sur la recombinaison d'odorants en mélange après l'étape d'analyse séparative, mais le choix des odorants à associer est essentiellement empirique.
Ainsi, deux questions se posent : Comment prédire l'odeur de mélanges de molécules d'après la structure moléculaire des odorants ?
Comment améliorer l'analyse de la flaveur dans le but de prédire l'odeur d'aliments complexes composés de plusieurs dizaine d'odorants en mélanges ?
Ces deux questions ont été abordées dans cette thèse dont les travaux sont décrits dans ce manuscrit selon deux axes principaux.
Le premier axe décrit l'utilisation et le développement d'un modèle basé sur le concept des distances angulaires calculées à partir de la structure moléculaire des odorants avec pour objectif de prédire la similarité perceptive de mélanges plus ou moins complexes d'odorants.
Les résultats soulignent l'importance de prendre en compte la dimension d'intensité des odorants afin d'améliorer la qualité de la prédiction.
Des perspectives d'amélioration du modèle sont dégagées pour permettre de dépasser la dimension de similarité et prédire des dimensions qualitatives de l'odeur.
Le deuxième axe présente une démarche originale d'intégration de connaissances liées à l'expertise dans la procédure d'analyse de la flaveur.
Ainsi, trois types de données hétérogènes sont agrégés dans un modèle mathématique global : des données chimiques, des données sensorielles et des connaissances d'experts aromaticiens.
L'expertise est intégrée à travers la création d'une ontologie qui est ensuite associée à une approche de logique floue optimisée par algorithme évolutionnaire.
Le modèle développé permet de prédire le profil odorant de seize vins rouges sur la base de leur composition en odorants.
Au final, l'ensemble des travaux menés dans cette thèse apporte des résultats originaux permettant une meilleure compréhension de la construction des odeurs des aliments et permet d'élaborer des hypothèses quant aux relations sous-jacentes de l'espace perceptif des odeurs en mélanges complexes.
Le comptage des foules est un sujet de recherche important.
De nos jours, la population est de plus en plus préoccupée par les problèmes de sécurité.
Lorsque la densité de population atteint des pics élevés, les systèmes de comptage se mettent en route et analyse les foules, afin de réorienter le surplus de personnes lorsque le seuil normal est dépassé.
Avec ce genre de système, le piétinement du nouvel an de Shanghai ne se reproduirait plus.
L'aspect le plus critique pour cette analyse est l'impossibilité d'installer un système de vidéosurveillance intelligent dans certains lieux publics.
Dans ces conditions, comment pourrions-nous estimer la densité de population dans ces zones afin d'éviter de futurs accidents ?
Face à ces défis, nous proposons la mise en œuvre d'une architecture embarquée reconfigurable en temps réel pour le comptage des personnes dans les zones de regroupement.
Premièrement, notre travail intègre les fonctionnalités de HOG et LBP, qui non seulement combinent les informations d'identifications de multiples caractéristiques, mais également la plupart des informations redondantes, réalisant ainsi une compression efficace des informations, économisant ainsi de l'espace mémoire pour le stockage des données.
Pour le comptage de personnes dans une foule, nous utilisons plusieurs sources d'informations, à savoir HOG, LBP et le filtrage de CANNY.
Ces sources fournissent des estimations distinctes du nombre de personnes comptées et d'autres mesures statistiques de classification, par le biais du vecteur de support machine SVM.
Dans le même temps, afin de résoudre efficacement le problème d'extraction des fonctionnalités liées à l'échelle dans le comptage de foules, nous proposons un nouveau environnement M-MCNN basé sur MCNN utilisé pour le comptage de foules sur une seule image.
M-MCNN contient non seulement les trois colonnes originales des réseaux de neurones convolutionnels avec différentes tailles de filtres, mais aussi remplace les couches entièrement connectées par une couche convolutionnelle de filtre 1*1, de sorte que l'image d'entrée du modèle peut être de n'importe quelle taille.
De plus, pour un échantillon individuel, nous améliorons considérablement l'apprentissage des caractéristiques de l'échantillon en extrayant les caractéristiques de texture d'une seule tête humaine et mieux l'utiliser pour les jeux de données.
Enfin, nous implémentons notre nouveau framework M-MCNN sur un FPGA et l'installons sur un drone pour estimer et prévoir la zone de foule à haute densité en temps réel.
Notre modèle a obtenu de bons résultats en comptage de personnes dans une foule.
Ce travail s'inscrit dans le projet « Personnalisation des EIAH » étudiant l'utilisation des traces d'activité dans l'évaluation et la personnalisation des situations d' apprentissage médiatisées.
L'analyse des traces se fait généralement par des outils d'analyse réalisés par les chercheurs et spécifiques à leurs besoins.
Les résultats publiés ne peuvent généralement pas être vérifiés ni comparés, ceci étant dû aux difficultés de partage des corpus et des outils d'analyse.
L'objectif de ce travail est de proposer aux chercheurs utilisant les EIAH, une plateforme pour le partage, d'une part des corpus de traces d'interaction contextualisées et les analyses réalisées sur ces corpus, et des outils d'analyse et de visualisation des traces d'autre part.
L'hétérogénéité des traces produites par les EIAH, due à la variété des domaines d'apprentissage et des besoins d'analyse, fait que la proposition d'une représentation commune ne peut répondre aux différents besoins de chercheurs pluridisciplinaires.
Nous proposons l'approche par « proxy » , une solution participative et incrémentale basée sur une ontologie qui définit trois modèles : un modèle de corpus définissant la structure et les métadonnées de description du corpus et son contenu, un modèle définissant les concepts génériques pouvant être retrouvés dans les corpus, et un modèle opérationnel définissant des opérations assurant l'interopérabilité entre un corpus et un outil d'analyse partagé.
En nous basant sur cette approche, nous proposons une architecture de plateforme de partage de corpus de traces et d'outils d'analyse permettant aux chercheurs de partager leurs corpus,d'accéder aux corpus partagés, et de les analyser.
Depuis les débuts du défi DARPA, la conception de voitures autonomes suscite un intérêt croissant.
Cet intérêt grandit encore plus avec les récents succès des algorithmes d'apprentissage automatique dans les tâches de perception.
Bien que la précision de ces algorithmes soit irremplaçable, il est très difficile d'exploiter leur potentiel.
Les contraintes en temps réel ainsi que les problèmes de fiabilité alourdissent le fardeau de la conception de plateformes matériels efficaces.
Nous discutons les différentes implémentations et techniques d'optimisation de ces plateformes dans ce travail.
Nous abordons le problème de ces accélérateurs sous deux perspectives : performances et fiabilité.
Nous proposons deux techniques d'accélération qui optimisent l'utilisation du temps et des ressources.
Sur le vol et fiabilité, nous étudions la résilience des algorithmes de Machine Learning face aux fautes matérielles.
Nous proposons un outil qui indique si ces algorithmes sont suffisamment fiables pour être employés dans des systèmes critiques avec de fortes critères sécuritaire ou non.
Un accélérateur sur processeur associatif résistif est présenté. Cet accélérateur atteint des performances élevées en raison de sa conception en mémoire qui remédie au goulot d'étranglement de la mémoire présent dans la plupart des algorithmes d'apprentissage automatique.
Quant à l'approche de multiplication constante, nous avons ouvertla porte à une nouvelle catégorie d'optimisations en concevant des accélérateurs spécifiques aux instances.
Les résultats obtenus surpassent les techniques les plus récentes en termes de temps d'exécution et d'utilisation des ressources.
Combinés à l'étude de fiabilité que nous avons menée, les systèmes ou la sécurité est de priorité peuvent profiter de ces accélérateurs sans compromettre cette dernière.
L'apprentissage profond a abouti à des développements spectaculaires dans le domaine de l'image et du langage naturel au cours des dernières années.
Pourtant, dans de nombreux domaines, les données d'observations ne sont ni des images ni du texte mais des séries temporelles qui représentent l'évolution de grandeurs mesurées ou calculées.
Dans cette thèse, nous étudions et proposons différentes représentations de séries temporelles à partir de modèles d'apprentissage profond.
Dans un premier temps, dans le domaine du contrôle de véhicules autonomes, nous montrons que l'analyse d'une fenêtre temporelle par un réseau de neurones permet d'obtenir de meilleurs résultats que les méthodes classiques qui n'utilisent pas de réseaux de neurones.
Dans un troisième temps, dans un but de génération de mouvements humains, nous proposons des réseaux de neurones génératifs convolutifs 2D où les dimensions temporelles et spatiales sont convoluées de manière jointe.
Enfin, dans un dernier temps, nous proposons un plongement où des représentations spatiales de poses humaines sont (ré)organisées dans un espace latent en fonction de leurs relations temporelles.
Les équipements domestiques qui supportent ces protocoles peuvent être détectés automatiquement, configurés et invoqués pour une tâche donnée.
Actuellement, plusieurs protocoles coexistent dans la maison, mais les interactions entre les dispositifs ne peuvent pas être mises en action à moins que les appareils supportent le même protocole.
En plus, les applications qui orchestrent ces dispositifs doivent connaître à l'avance les noms des services et dispositifs.
Or, chaque protocole définit un profil standard par type d'appareil.
Par conséquent, deux appareils ayant le même type et les mêmes fonctions mais qui supportent un protocole différent publient des interfaces qui sont souvent sémantiquement équivalentes mais syntaxiquement différentes.
Ceci limite alors les applications à interagir avec un service similaire.
Dans ce travail, nous présentons une méthode qui se base sur l'alignement d'ontologie et la génération automatique de mandataire pour parvenir à une adaptation dynamique de services.
Malgré les différences typologiques entre le français et le turc, nous posons l'hypothèse que des rapprochements entre ces deux langues peuvent être effectués au niveau des significations et des procédés de construction de significations.
Cette approche, fondée sur l'établissement d'un répertoire de significations pourrait être élargie vers d'autres langues ne disposant pas d'outils semblables.
La vision par ordinateur est un domaine qui inclut des méthodes d'acquisition, de traitement, d'analyse et de compréhension des images afin de produire de l'information numérique ou symbolique.
Un axe de recherche contribuant au développement de ce domaine consiste à reproduire les capacités de la vision humaine par voie électronique afin de percevoir et de comprendre une image.
Nos travaux de thèse s'inscrivent dans cet axe de recherche.
Nous proposons plusieurs contributions originales s'inscrivant dans le cadre de résolution des problèmes de la reconnaissance et de la localisation des symboles graphiques en contexte.
L'originalité des approches proposées réside dans la proposition d'une alliance intéressante entre l'Analyse Formelle de Concepts et la vision par ordinateur.
Pour ce faire, nous nous sommes confrontés à l'étude du domaine de l'AFC et plus précisément l'adaptation de la structure du treillis de concepts et son utilisation comme étant l'outil majeur de nos travaux.
La principale particularité de notre travail réside dans son aspect générique vu que les méthodes proposées peuvent être alliées à divers outils autre que le treillis de concepts en gardant les mêmes stratégies adoptées et en suivant une procédure semblable.
Le principal avantage du treillis de concepts est l'aspect symbolique qu'il offre.
Il présente un espace de recherche concis, précis et souple facilitant ainsi la prise de décision.
Nos contributions sont inscrites dans le cadre de la reconnaissance et de localisation de symboles dans les documents graphiques.
Nous proposons des chaînes de traitement s'inscrivant dans le domaine de la vision par ordinateur
Des ressources telles que les terminologies ou les ontologies sont utilisees dans differentes applications, notamment dans la description documentaire et la recherche d'information.
Differentes methodologies ont ete proposees pour construire ce type de ressources, que ce soit a partir d'entrevues d'experts du domaine ou a partir de corpus textuels.
Nous nous interessons dans ce memoire a l'utilisation de methodologies existantes dans le domaine du traitement automatique des langues, destinees a la construction d'ontologies a partir de corpus textuels, pour la construction du type de ressource particulier que sont les ontologies differentielles.
L'extraction de catalogues de sources fiables à partir des images est cruciale pour un large éventail de recherches en astronomie.
Cependant, l'efficacité des méthodes de détection de source actuelles est sérieusement limitée dans les champs encombrés, ou lorsque les images sont contaminées par des défauts optiques, électroniques et environnementaux.
Les performances en termes de fiabilité et de complétude sont aujourd'hui souvent insuffisantes au regard des exigences scientifiques des grands relevés d'imagerie.
Dans cette thèse, nous développons de nouvelles méthodes pour produire des catalogues sources plus robustes et fiables.
Nous tirons parti des progrès récents en apprentissage supervisé profond pour concevoir des modèles génériques et fiables basés sur des réseaux de neurones à convolutions (CNNs).Nous présentons MaxiMask et MaxiTrack, deux réseaux de neurones à convolutions que nous avons entrainés pour identifier automatiquement 13 types différents de défauts d'image dans des expositions astronomiques.
Nous présentons également un prototype de détecteur de sources multi-échelle et robuste vis-à-vis des défauts d'image, dont nous montrons qu'il surpasse largement les algorithmes existants en terme de performances.
Nous discutons des limites actuelles et des améliorations potentielles de notre approche dans le cadre des prochains grands relevés tels que Euclid.
Associé à des images préconçues et pétrit de préjugés, le roman sentimental de type sériel est un produit de consommation qui représente un miroir social et culturel.
Présent en France depuis plusieurs dizaines d'années, il a conquis un public hétérogène et fidèle.
Quels sont les ressorts de ces romans qu'on dit tous similaires ?
Que représentent-ils pour le lectorat ?
Comment sont-ils perçus par ceux qui les lisent ?
Comment sont-ils lu, appréhendés, interprétés ?
Autant de questions auxquelles nous nous efforçons de répondre dans ce travail pluridisciplinaire.
Cette recherche a permis de mettre au jour les mécanismes à l'œuvre dans l'évolution du roman sentimental sériel depuis la seconde guerre mondiale, elle a également permis d'observer différents modes de réception et de lecture
Depuis une vingtaine d'années, l'accès et l'utilisation des données médicales sont devenus des enjeux majeurs pour les professionnels de santé comme pour le grand public.
Dans ce contexte, plusieurs terminologies médicales spécialisées ont été créées.
Ces terminologies ont pour la plupart des formats de représentation et visées différentes : la nomenclature SNOMED 3. 5 pour le codage d'informations cliniques, les classifications CIM10 et CCAM pour le codage épidémiologique puis médico-économique, le thésaurus MeSH pour la bibliographie...
Devant ce constat et la nécessité grandissante de permettre la coopération de différents acteurs de la santé et des systèmes d'information associés, il apparaît nécessaire de rendre les terminologies "interopérables".
De plus, nous utilisons nos différents algorithmes conjointement avec le métathésaurus UMLS afin d'apporter une plus grande couverture au niveau des relations entre les terminologies.
Nous bénéficions, notamment, dans le cadre de cette thèse, d'une expérience riche dans le domaine du Traitement Automatique de la Langue (TAL) issue des précédents travaux de recherche dans les équipes CISMeF et LERTIM.
La recherche en Sciences humaines et sociales repose souvent sur de grandes masses de données textuelles, qu'il serait impossible de lire en détail.
Le Traitement automatique des langues (TAL) peut identifier des concepts et des acteurs importants mentionnés dans un corpus, ainsi que les relations entre eux.
Ces informations peuvent fournir un aperçu du corpus qui peut être utile pour les experts d'un domaine et les aider à identifier les zones du corpus pertinentes pour leurs questions de recherche.
Pour annoter automatiquement des corpus d'intérêt en Humanités numériques, les technologies TAL que nous avons appliquées sont, en premier lieu, le liage d'entités (plus connu sous le nom de Entity Linking), pour identifier les acteurs et concepts du corpus ;
deuxièmement, les relations entre les acteurs et les concepts ont été déterminées sur la base d'une chaîne de traitements TAL, qui effectue un étiquetage des rôles sémantiques et des dépendances syntaxiques, entre autres analyses linguistiques.
La partie I de la thèse décrit l'état de l'art sur ces technologies, en soulignant en même temps leur emploi en Humanités numériques.
Des outils TAL génériques ont été utilisés.
Comme l'efficacité des méthodes de TAL dépend du corpus d'application, des développements ont été effectués, décrits dans la partie II, afin de mieux adapter les méthodes d'analyse aux corpus dans nos études de cas.
La partie II montre également une évaluation intrinsèque de la technologie développée, avec des résultats satisfaisants.
Les technologies ont été appliquées à trois corpus très différents, comme décrit dans la partie III.
Deuxièmement, le corpus PoliInformatics, qui contient des matériaux hétérogènes sur la crise financière américaine de 2007--2008.
Enfin, le Bulletin des Négociations de la Terre (ENB dans son acronyme anglais), qui couvre des sommets internationaux sur la politique climatique depuis 1995, où des traités comme le Protocole de Kyoto ou les Accords de Paris ont été négociés.
Pour chaque corpus, des interfaces de navigation ont été développées.
Ces interfaces utilisateur combinent les réseaux, la recherche en texte intégral et la recherche structurée basée sur des annotations TAL.
À titre d'exemple, dans l'interface pour le corpus ENB, qui couvre des négociations en politique climatique, des recherches peuvent être effectuées sur la base d'informations relationnelles identifiées dans le corpus : les acteurs de la négociation ayant discuté un sujet concret en exprimant leur soutien ou leur opposition peuvent être recherchés.
Le type de la relation entre acteurs et concepts est exploité, au-delà de la simple co-occurrence entre les termes du corpus.
Les interfaces ont été évaluées qualitativement avec des experts de domaine, afin d'estimer leur utilité potentielle pour la recherche dans leurs domaines respectifs.
Tout d'abord, il a été vérifié si les représentations générées pour le contenu des corpus sont en accord avec les connaissances des experts du domaine, pour déceler des erreurs d'annotation.
Ensuite, nous avons essayé de déterminer si les experts pourraient être en mesure d'avoir une meilleure compréhension du corpus grâce à avoir utilisé les applications, par exemple, s'ils ont trouvé de l'évidence nouvelle pour leurs questions de recherche existantes, ou s'ils ont trouvé de nouvelles questions de recherche.
On a pu mettre au jour des exemples où un gain de compréhension sur le corpus est observé grâce à l'interface dédiée au Bulletin des Négociations de la Terre, ce qui constitue une bonne validation du travail effectué dans la thèse.
En conclusion, les points forts et faiblesses des applications développées ont été soulignés, en indiquant de possibles pistes d'amélioration en tant que travail futur.
Dans cette thèse, nous décrivons la création du French FrameNet (FFN), une ressource de type FrameNet pour le français créée à partir du FrameNet de l'anglais (Baker et al., 1998) et de deux corpus arborés : le French Treebank (Abeillé et al., 2003) et le Sequoia Treebank (Candito et Seddah, 2012).
La ressource séminale, le FrameNet de l'anglais, constitue un modèle d'annotation sémantique de situations prototypiques et de leurs participants.
Elle propose à la fois : a) un ensemble structuré de situations prototypiques, appelées cadres, associées à des caractérisations sémantiques des participants impliqués (les rôles) ; b) un lexique de déclencheurs, les lexèmes évoquant ces cadres ; c) un ensemble d'annotations en cadres pour l'anglais.
Pour créer le FFN, nous avons suivi une approche « par domaine notionnel » : nous avons défini quatre « domaines » centrés chacun autour d'une notion (cause, communication langagière, position cognitive ou transaction commerciale), que nous avons travaillé à couvrir exhaustivement à la fois pour la définition des cadres sémantiques, la définition du lexique, et l'annotation en corpus.
Cette stratégie permet de garantir une plus grande cohérence dans la structuration en cadres sémantiques, tout en abordant la polysémie au sein d'un domaine et entre les domaines.
De plus, nous avons annoté les cadres de nos domaines sur du texte continu, sans sélection d'occurrences : nous préservons ainsi la distribution des caractéristiques lexicales et syntaxiques de l'évocation des cadres dans notre corpus.
à l'heure actuelle, le FFN comporte 105 cadres et 873 déclencheurs distincts, qui donnent lieu à 1109 paires déclencheur-cadre distinctes, c'est-à-dire 1109 sens.
Le corpus annoté compte au total 16167 annotations de cadres de nos domaines et de leurs rôles.
La thèse commence par resituer le modèle FrameNet dans un contexte théorique plus large.
Nous justifions ensuite le choix de nous appuyer sur cette ressource et motivons notre méthodologie en domaines notionnels.
Nous explicitons pour le FFN certaines notions définies pour le FrameNet de l'anglais que nous avons jugées trop floues pour être appliquées de manière cohérente.
Nous introduisons en particulier des critères plus directement syntaxiques pour la définition du périmètre lexical d'un cadre, ainsi que pour la distinction entre rôles noyaux et non-noyaux.
Nous décrivons ensuite la création du FFN : d'abord, la délimitation de la structure de cadres utilisée pour le FFN, et la création de leur lexique.
Nous présentons alors de manière approfondie le domaine notionnel des positions cognitives, qui englobe les cadres portant sur le degré de certitude d'un être doué de conscience sur une proposition.
Puis, nous présentons notre méthodologie d'annotation du corpus en cadres et en rôles. à cette occasion, nous passons en revue certains phénomènes linguistiques qu'il nous a fallu traiter pour obtenir une annotation cohérente ;
Enfin, nous présentons des données quantitatives sur le FFN tel qu'il est à ce jour et sur son évaluation.
Nous terminons sur des perspectives de travaux d'amélioration et d'exploitation de la ressource créée.
Cette thèse se penche sur l'analyse et les contraintes d'élaboration des composants permettant aux systèmes de répondre à des questions en domaine ouvert.
Les systèmes de questions réponses (SQR) sont adaptés à cette tâche, car ils associent une question factuelle à une réponse précise.
Il est proposé d'adapter les SQR à une tâche d'interaction afin de leur définir un cadre d'adaptation générique aux systèmes de dialogue.
Sont étudiés, les techniques existantes pour les SQR en domaine ouvert, les différents modèles de dialogue homme-machine ainsi que des systèmes à la limite entre le dialogue et la recherche d'informations.
Ensuite, est présenté la construction d'un modèle de structure permettant de décrire les interactions entre des questions.
Du cadre formel, est déduis une méthode de calcul qui est évalué.
Afin de discuter cette évaluation et de voir comment utiliser notre modèle, il est réalisé une analyse des moteurs de recherche pour les systèmes de questions réponses.
Dans une dernière partie, il est présenté une utilisation des données calculées pour l'amélioration des résultats de réponses aux questions.
La langue arabe a une morphologie particulière fondée sur des racines et des schèmes.
L'Abjad n'est pas seulement un problème culturel, mais aussi un problème informatique.
Le rire est une vocalisation universelle à travers les cultures et les langues.
Il est omniprésent dans nos dialogues et utilisé pour un large éventail de fonctions.
Le rire a été étudié sous plusieurs angles, mais les classifications proposées sont difficiles à intégrer dans un même système.
Malgré le fait qu'il soit crucial dans nos interactions quotidiennes, le rire en conversation a reçu peu d'attention et les études sur la pragmatique du rire en interaction, ses corrélats neuronaux perceptuels et son développement chez l'enfant sont rares.
Dans cette thèse, est proposé un nouveau cadre pour l'analyse du rire, fondé sur l'hypothèse cruciale que le rire a un contenu propositionnel, plaidant pour la nécessité de distinguer différentes couches d'analyse, tout comme dans l'étude de la parole : forme, positionnement, sémantique et pragmatique.
Des études préliminaires sont menées sur la viabilité d'un mappage forme-fonction du rire basée sur ses caractéristiques acoustiques, ainsi que sur les corrélats neuronaux impliqués dans la perception du rire qui servent différentes fonctions dans un dialogue naturel.
Nos résultats donnent lieu à de nouvelles généralisations sur le placement, l'alignement, la sémantique et les fonctions du rire, soulignant le haute niveau des compétences pragmatiques impliquées dans sa production et sa perception.
Le développement de l'utilisation sémantique et pragmatique du rire est observé dans une étude de corpus longitudinale de 4 dyades mère-enfant de l'age de 12 à 36 mois, locuteurs d'anglais américain.
Les résultats montrent que l'utilisation du rire subit un développement important à chaque niveau analysé et que le rire peut être un indicateur précoce du développement cognitif, communicatif et social.
Nous avons mené différentes études pour étudier le phénomène entre des paires d'inconnus, d'amis de longue date, puis entre des personnes provenant de la même famille.
On s'attend à ce que l'amplitude de la convergence soit liée à la distance sociale entre les deux interlocuteurs.
On retrouve bien ce résultat.
Nous avons ensuite étudié l'impact de la connaissance de la cible linguistique sur l'adaptation.
Pour caractériser la convergence phonétique, nous avons développé deux méthodes : la première basée sur une analyse discriminante linéaire entre les coefficients MFCC de chaque locuteur
Finalement, nous avons caractérisé la convergence phonétique à l'aide d'une mesure subjective en utilisant un nouveau test de perception basé sur la détection « en ligne » d'un changement de locuteur.
Le test a été réalisé à l'aide signaux extraits des interactions mais également avec des signaux obtenus avec une synthèse adaptative basé sur la modélisation HNM.
Les acteurs d'une entreprise doivent collaborer efficacement pour atteindre les objectifs de l'entreprise, et ce dans un environnement évoluant en permanence, alors que les systèmes informatiques actuels sont pour la plupart construits sur la base d'analyses de l'organisation de l'entreprise et d'un recueil des besoins des utilisateurs à un moment donné.
Or les médiations par lesquelles les acteurs d'un processus de conception vont échanger des informations, spécifier leurs tâches, se coordonner, évaluer l'activité sont les véritables objets qui doivent former la base des outils informatiques de support des activités coopératives de conception.
Ces ajustements passent presque exclusivement par la communication et souvent suivant des circuits qui ne peuvent être définis à l'avance.
Les méthodes et outils d'organisation du travail peuvent bénéficier des nouvelles technologies de la communication pour rendre ces adaptations plus efficaces.
Un nouveau type d'outillage informatique devient alors nécessaire pour désambigui͏̈ser, faciliter, structurer, améliorer, augmenter, et permettre des recherches évoluées dans les échanges survenant entre les acteurs (humains et informatiques) d'un système sociotechnique coopératif.
Les systèmes multi-agents, dynamiquement reconfigurables, peuvent suivre ces évolutions et prendre en compte les spécificités de chacun des acteurs.
Le but de cette thèse est de développer un outil de traitement automatique des langues naturelles visant à analyser et reformuler les problèmes arithmétiques des élèves du 3e cycle de l'école primaire.
Plus précisément, cet outil devrait permettre  :
De détecter les éléments de difficultés linguistiques dans les problèmes mathématiques additifs.
De classifier des problèmes en fonction des difficultés linguistiques qu'ils présentent.
De proposer une reformulation de ces problèmes afin d'en faciliter la compréhension.
D'évaluer la reformulation proposée, ce qui peut revenir à comparer deux versions d'un même problème.
Cette thèse s'intéresse à la détection et la reconnaissance du texte arabe incrusté dans les vidéos.
Dans ce contexte, nous proposons différents prototypes de détection et d'OCR vidéo (Optical Character Recognition) qui sont robustes à la complexité du texte arabe (différentes échelles, tailles, polices, etc.)
Nous introduisons différents détecteurs de texte arabe qui se basent sur l'apprentissage artificiel sans aucun prétraitement.
Les détecteurs se basent sur des Réseaux de Neurones à Convolution (ConvNet) ainsi que sur des schémas de boosting pour apprendre la sélection des caractéristiques textuelles manuellement conçus.
Nous utilisons différents modèles d'apprentissage profond, regroupant des Auto-Encodeurs, des ConvNets et un modèle d'apprentissage non-supervisé, qui génèrent automatiquement ces caractéristiques.
Chaque modèle résulte en un système d'OCR bien spécifique.
Le processus de reconnaissance se base sur une approche connexionniste récurrente pour l'apprentissage de l'étiquetage des séquences de caractéristiques sans aucune segmentation préalable.
Nos modèles d'OCR proposés sont comparés à d'autres modèles qui se basent sur des caractéristiques manuellement conçues.
Nous proposons un schéma de décodage conjoint qui intègre les inférences du LM en parallèle avec celles de l'OCR tout en introduisant un ensemble d'hyper-paramètres afin d'améliorer la reconnaissance et réduire le temps de réponse.
Afin de surpasser le manque de corpus textuels arabes issus de contenus multimédia, nous mettons au point de nouveaux corpus manuellement annotés à partir des flux TV arabes.
Le corpus conçu pour l'OCR, nommé ALIF et composée de 6,532 images de texte annotées
Nos systèmes ont été développés et évalués sur ces corpus.
L'étude des résultats a permis de valider nos approches et de montrer leurs efficacité et généricité avec plus de 97% en taux de détection, 88.63% en taux de reconnaissance mots sur le corpus ALIF dépassant ainsi un des systèmes d'OCR commerciaux les mieux connus par 36 points.
Cette thèse de doctorat porte sur le processus de développement d'un logiciel de sous-titrage adapté au contexte de l'enseignement supérieur à partir des pratiques d'une professionnelle de ce secteur.
En premier lieu, une révision des fondements théoriques de la Traduction Audiovisuelle et du sous-titrage permet de définir les caractéristiques linguistiques et sémiotiques du texte audiovisuel et de sa traduction.
Ce travail se poursuit par l'étude de normes extratextuelles explicites et par l'identification des fonctionnalités qui rendent possible leur application afin que le sous-titreur puisse adapter son projet aux attentes de la culture-cible.
Cette thèse aborde également l'aspect professionnel de cette pratique via l'analyse de l'impact des avancées technologiques sur le processus de sous-titrage et des outils dont disposent les sous-titreurs.
Elle présente également l'utilisation de matériel audiovisuel sous-titré dans l'enseignement supérieur au moyen d'un exemple d'environnement virtuel d'apprentissage multilingue et multiculturel.
L'ensemble de ces analyses a permis de concevoir un nouveau logiciel de sous-titrage, Miro Translate, plateforme hybride en ligne spécialement élaborée pour le sous-titrage de vidéos pédagogiques.
Enfin, la qualité de cet outil est étudiée par le biais d'un test d'utilisabilité qui mesure la satisfaction des utilisateurs, son efficience et son efficacité afin d'identifier les actions nécessaires pour son amélioration.
1. Etudes économétriques de mouvements de protestation sur les réseaux sociaux
2. Développement de nouvelles méthodes d'analyse de données liées aux réseaux sociaux
3. Développement de nouvelles méthodes d'analyse de texte appliquées à l'économie
Dans la première partie de la thèse, nous proposons un formalisme d'optimisation général, commun à l'ensemble des méthodes d'apprentissage semi-supervisé et en particulier aux Laplacien Standard, Laplacien Normalisé et PageRank.
En utilisant la théorie des marches aléatoires, nous caractérisons les différences majeures entre méthodes d'apprentissage semi-supervisé et nous définissons des critères opérationnels pour guider le choix des paramètres du noyau ainsi que des points étiquetés.
Cette application montre de façon édifiante que la famille de méthodes proposée passe parfaitement à l'échelle.
Plus précisément, nous proposons des algorithmes randomisés pour la détection rapide des nœuds de grands degrés et des nœuds avec de grandes valeurs de PageRank personnalisé.
A la fin de la thèse, nous proposons une nouvelle mesure de centralité, qui généralise à la fois la centralité d'intermédiarité et PageRank.
Cette nouvelle mesure est particulièrement bien adaptée pour la détection de la vulnérabilité de réseau.
Considéré comme un historien qui sacrifie rigueur et exactitude à son souci de la rhétorique, Quinte-Curce jouit, et avec lui son histoire « romancée » , d'une réputation en demi-teinte.
L'historien se livre alors à une véritable entreprise de démystification qui touche la nature même de cet Orient merveilleux, la fortune providentielle dont se réclame le Macédonien et même le langage.
Sont ainsi condamnés la quête effrénée de gloire que poursuit le roi, et son rêve de divinisation : l'Orient est synonyme de renversement généralisé des normes et des valeurs, la fortune une illusion conduisant à un sentiment d'impunité.
En filigrane, il propose aussi un idéal du pouvoir qui repose essentiellement sur l'équilibre et sur la responsabilité du prince.
Par là, il interroge, au regard des réalités politiques de son temps, la pertinence d'un mythe central dans l'imaginaire politique romain et dont l'ombre plane sur tous les ambitieux, à commencer par les empereurs ou les candidats à l'Empire.
Son récit bien mené incite donc à une réflexion réelle sur l'exercice du pouvoir, ses enjeux et ses limites.
L'extraction de la sémantique d'une image est un processus qui nécessite une analyse profonde du contenu de l'image.
Elle se réfère à leur interprétation à partir d'un point de vuehumain.
Elle consiste à extraire une sémantique simple ou multiple de l'image afin de faciliter sa récupération.
Ces objectifs indiquent clairement que l'extraction de la sémantique n'est pas un nouveau domaine de recherche.
Cette thèse traite d'une approche d'annotation collaborative et de recherche d'images baséesur les sémantiques émergentes.
Il aborde d'une part, la façon dont les annotateurs pourraient décrire et représenter le contenu des images en se basant sur les informations visuelles, et d'autre part comment la recherche des images pourrait être considérablement améliorée grâce aux récentes techniques, notamment le clustering et la recommandation.
Pour atteindre ces objectifs, l'exploitation des outils de description implicite du contenu des images, des interactions des annotateurs qui décrivent la sémantique des images et celles des utilisateurs qui utilisent la sémantique produite pour rechercher les images seraient indispensables.
Dans cette thèse, nous nous sommes penchés vers les outils duWeb Sémantique, notamment les ontologies pour décrire les images de façon structurée.
L'ontologie permet de représenter les objets présents dans une image ainsi que les relations entre ces objets (les scènes d'image).
Autrement dit, elle permet de représenter de façon formelle les différents types d'objets et leurs relations.
L'ontologie code la structure relationnelle des concepts que l'on peut utiliser pour décrire et raisonner.
Cela la rend éminemment adaptée à de nombreux problèmes comme la description sémantique des images qui nécessite une connaissance préalable et une capacité descriptive et normative.
La contribution de cette thèse est focalisée sur trois points essentiels : La représentationsémantique, l'annotation sémantique collaborative et la recherche sémantique des images.
La représentation sémantique permet de proposer un outil capable de représenter la sémantique des images.
Pour capturer la sémantique des images, nous avons proposé une ontologie d'application dérivée d'
La recherche sémantique permet de rechercher les images avec les sémantiques fournies par l'annotation sémantique collaborative.
Elle est basée sur deux techniques : le clustering et la recommandation.
Le clustering permet de regrouper les images similaires à la requête d'utilisateur et la recommandation a pour objectif de proposer des sémantiques aux utilisateurs en se basant sur leurs profils statiques et dynamiques.
Elle est composée de trois étapes à savoir : la formation de la communauté des utilisateurs, l'acquisition des profils d'utilisateurs et la classification des profils d'utilisateurs avec l'algèbre de Galois.
Des expérimentations ont été menées pour valider les différentes approches proposées dans ce travail.
Les services cloud offrent des coûts réduits, une élasticité et un espace de stockage illimité qui attirent de nombreux utilisateurs.
Le partage de fichiers, les plates-formes collaboratives, les plateformes de courrier électroniques, les serveurs de sauvegarde et le stockage de fichiers sont parmi les services qui font du cloud un outil essentiel pour une utilisation quotidienne.
Actuellement, la plupart des systèmes d'exploitation proposent des applications de stockage externalisées intégrées, par conception, telles que One Drive et iCloud, en tant que substituts naturels succédant au stockage local.
Cependant, de nombreux utilisateurs, même ceux qui sont disposés à utiliser les services susmentionnés, restent réticents à adopter pleinement le stockage et les services sous-traités dans le cloud.
Les préoccupations liées à la confidentialité des données augmentent l'incertitude pour les utilisateurs qui conservent des informations sensibles.
Il existe de nombreuses violations récurrentes de données à l'échelle mondiale qui ont conduit à la divulgation d'informations sensibles par les utilisateurs.
Pour en citer quelques-uns : une violation de Yahoo fin 2014 et annoncé publiquement en Septembre 2016, connue comme la plus grande fuite de données de l'histoire d'Internet, a conduit à la divulgation de plus de 500 millions de comptes utilisateur ; une infraction aux assureurs-maladie, Anthem en février 2015 et Premera BlueCross BlueShield en mars 2015, qui a permis la divulgation de renseignements sur les cartes de crédit, les renseignements bancaires, les numéros de sécurité sociale, pour des millions de clients et d'utilisateurs.
Une contre-mesure traditionnelle pour de telles attaques dévastatrices consiste à chiffrer les données des utilisateurs afin que même si une violation de sécurité se produit, les attaquants ne peuvent obtenir aucune information à partir des données.
Malheureusement, cette solution empêche la plupart des services du cloud, et en particulier, la réalisation des recherches sur les données externalisées.
Les chercheurs se sont donc intéressés à la question suivante : comment effectuer des recherches sur des données chiffrées externalisées tout en préservant une communication, un temps de calcul et un stockage acceptables ?
Cette question avait plusieurs solutions, reposant principalement sur des primitives cryptographiques, offrant de nombreuses garanties de sécurité et d'efficacité.
Bien que ce problème ait été explicitement identifié pendant plus d'une décennie, de nombreuses dimensions de recherche demeurent non résolues.
Dans ce contexte, le but principal de cette thèse est de proposer des constructions pratiques qui sont (1) adaptées aux déploiements dans les applications réelles en vérifiant les exigences d'efficacité nécessaires, mais aussi, (2) en fournissant de bonnes assurances de sécurité.
Tout au long de notre recherche, nous avons identifié le chiffrement cherchable (SSE) et la RMA inconsciente (ORAM) comme des deux potentielles et principales primitives cryptographiques candidates aux paramètres des applications réelles.
Nous avons identifié plusieurs défis et enjeux inhérents à ces constructions et fourni plusieurs contributions qui améliorent significativement l'état de l'art.
Premièrement, nous avons contribué à rendre les schémas SSE plus expressifs en permettant des requêtes booléennes, sémantiques et de sous-chaînes.
Cependant, les praticiens doivent faire très attention à préserver l'équilibre entre la fuite d'information et le degré d'expressivité souhaité.
Deuxièmement, nous améliorons la bande passante de l'ORAM en introduisant une nouvelle structure récursive de données et une nouvelle procédure d'éviction pour la classe d'ORAM ; nous introduisons également le concept de redimensionnabilibté dans l'ORAM qui est une caractéristique requise pour l'élasticité de stockage dans le cloud.
Dans cette thèse, nous nous intéressons au problème de la détection d'objets faiblement supervisée. Le but est de reconnaître et de localiser des objets dans les images, n'ayant à notre disposition durant la phase d'apprentissage que des images partiellement annotées au niveau des objets.
Pour cela, nous avons proposé deux méthodes basées sur des modèles différents.
Pour la première méthode, nous avons proposé une amélioration de l'approche " Deformable Part-based Models " (DPM) faiblement supervisée, en insistant sur l'importance de la position et de la taille du filtre racine initial spécifique à la classe.
Tout d'abord, un ensemble de candidats est calculé, ceux-ci représentant les positions possibles de l'objet pour le filtre racine initial, en se basant sur une mesure générique d'objectness (par region proposals) pour combiner les régions les plus saillantes et potentiellement de bonne qualité.
Ensuite, nous avons proposé l'apprentissage du label des classes latentes de chaque candidat comme un problème de classification binaire, en entrainant des classifieurs spécifiques pour chaque catégorie afin de prédire si les candidats sont potentiellement des objets cible ou non.
De plus, nous avons amélioré la détection en incorporant l'information contextuelle à partir des scores de classification de l'image.
Enfin, nous avons élaboré une procédure de post-traitement permettant d'élargir et de contracter les régions fournies par le DPM afin de les adapter efficacement à la taille de l'objet, augmentant ainsi la précision finale de la détection.
Pour la seconde approche, nous avons étudié dans quelle mesure l'information tirée des objets similaires d'un point de vue visuel et sémantique pouvait être utilisée pour transformer un classifieur d'images en détecteur d'objets d'une manière semi-supervisée sur un large ensemble de données, pour lequel seul un sous-ensemble des catégories d'objets est annoté avec des boîtes englobantes nécessaires pour l'apprentissage des détecteurs.
Nous avons proposé de transformer des classifieurs d'images basés sur des réseaux convolutionnels profonds (Deep CNN) en détecteurs d'objets en modélisant les différences entre les deux en considérant des catégories disposant à la fois de l'annotation au niveau de l'image globale et l'annotation au niveau des boîtes englobantes.
Nos approches ont été évaluées sur plusieurs jeux de données tels que PASCAL VOC, ImageNet ILSVRC et Microsoft COCO.
Ces expérimentations ont démontré que nos approches permettent d'obtenir des résultats comparables à ceux de l'état de l'art et qu'une amélioration significative a pu être obtenue par rapport à des méthodes récentes de détection d'objets faiblement supervisées.
Le domaine de l'apprentissage automatique a récemment été considérablement bouleversé par l'apprentissage profond.
Les réseaux neuronaux profonds sont maintenant à la base de l'état de l'art en matière de vision par ordinateur, de reconnaissance vocale, de traitement du langage naturel et de nombreux autres domaines.
Bien que très efficaces, ces modèles sont coûteux en termes de calcul et nécessitent de grandes quantités de données pour estimer avec précision leurs nombreux paramètres.
Les statistiques bayésiennes offrent un cadre théoriquement bien fondé pour raisonner sur l'incertitude et constituent l'une des pierres angulaires de l'apprentissage automatique moderne.
Bien qu'apparemment assez éloigné, il existe un grand potentiel de fertilisation croisée entre l'apprentissage en profondeur et les statistiques bayésiennes.
Pourtant, l'interaction entre ces deux paradigmes d'apprentissage est relativement peu exploré jusqu'à présent.
Le but de cette thèse est d'apporter théorie et techniques pratiques qui se situent à l'interface de ces deux domaines.
Voir version anglais pour sujet de thèse en détail.
Cette thèse examine le phénomène universel de la poly-divergence dans le chinois archaïque avec cinq sous-types : La polygrammaticalisation avec le cas de huò 或 ; la polylexicalisation et l'hybridation de ces deux avec le cas de rán 然 ; la poly-divergence sous l'influence des culte-culture-philosophique avec le cas d'yī 一 ; la poly-divergence de la structure multifonctionnelle avec le cas de « Reference+Comment » ; la poly-divergence de la locution avec le cas de « chiffre+mois » .
Ces divergences ont pour effet de réduire et éviter l'opacité causée par la pratique langagière de l'emploi non-obligatoire d'item fonctionnel ou le manque d'item fonctionnel exclusif.
Et ces types de poly-divergence en chinois se réalisent via des mécanismes qui sont distincts de ceux des langues occidentales.
Enfin, on peut en conclure que, le modèle de la poly-divergence en branches devient le modèle essentiel d'évolution grammaticale dans le chinois archaïque.
Nos travaux ont démontré les performances des systèmes d'intelligence épidémiologique en matière de détection précoce des évènements infectieux au niveau mondial, la valeur ajoutée spécifique de chaque système, la plus grande sensibilité intrinsèque des systèmes modérés et la variabilité du type de source d'information utilisé.
La création d'un système virtuel combiné intégrant le meilleur résultat des sept systèmes a démontré les gains en termes de sensibilité et de réactivité, qui résulterait de l'intégration de ces systèmes individuels dans un supra-système.
Ils ont illustrés les limites de ces outils et en particulier la faible valeur prédictive positive des signaux bruts détectés, la variabilité les capacités de détection pour une même pathologie, mais également l'influence significative jouée par le type de pathologie, la langue et la région de survenue sur les capacités de détection des évènements infectieux.
Ils ont établis la grande diversité des stratégies d'intelligence épidémiologique mises en œuvre par les institutions de santé publique pour répondre à leurs besoins spécifiques et l'impact de ces stratégies sur la nature, l'origine géographique et le nombre des évènements rapportés.
Ils ont également montré que dans des conditions proches de la routine, l'intelligence épidémiologique permettait la détection d'évènements infectieux en moyenne une à deux semaines avant leur notification officielle, permettant ainsi d'alerter les autorités sanitaires et d'anticiper la mise en œuvre d'éventuelles mesures de contrôle.
Nos travaux ouvrent de nouveaux champs d'investigations dont les applications pourraient être importantes pour les utilisateurs comme pour les systèmes.
Le système Pan-Européen de radio mobile numérique (DMR) cellulaire qui devrait entrer en fonction au début des années '90 utiliserait un codage de vitesse d'environ 16 kbit/s.
On a établi au moins trois phases d'expérimentation : (1) épreuves de présélection nationale, (2) épreuves de sélection européenne, et (3) épreuves de caractérisation et de vérification finale avec codage de canal associé.
Les premières 2 phases devaient utiliser seulement des épreuves d'écoute qui ne considéraient pas les effets de retard d'élaboration des codeurs/décodeurs.
On a fixé comme objectif, un retard initial maximum de 65 ms.
La phase finale emploiera les épreuves de conversation pour évaluer les effets de qualité, retard et écho ; ces épreuves seront complétées par des épreuves d'écoute à employer aussi pour la caractérisation des codeurs/décodeurs.
Le mémoire décrit des méthodologies d'épreuves subjectives adoptées pour choisir les codeurs/décodeurs candidats appropriés qui peuvent être employés dans le système DMR proposé.
Chaque administration participant aux tests doit convainere le Comité Européen (Conférence Européenne des Administrations des Postes et Télécommunications, CEPT) que la qualité de son propre codeur/décodeur candidat est meilleure ou au moins égale à la qualité des systèmes analogiques FM à 900 MHz maintenant employés en Europe.
On a mesuré la variabilité de la fréquence fondamentale (F 0) dans les formes isolées des tons lexicaux.
Le thai est une langue à 5 tons lexicaux : moyen, bas, descendant, haut et montant.
Vingt-et-un sujets ont participé à l'expérience : dix jeunes locuteurs masculins et dix locuteurs âgés, cinq hommes et cinq femmes.
Des enregistrements de haute qualité ont été réalisés pour la production d'une série minimum de 5 mots monosyllabiques par chaque sujet.
Les contours de F 0 ont été obtenus par une analyse cepstrale.
On a comparé la variabilité inter- et intra-locuteur en ce qui concerne la production des cinq tons.
Les résultats de l'analyse de la variance ont indique que le degré de la variabilité intersujets était plus élevé que la variabilité intra-locuteur au travers des cinq tons ; les locuteurs jeunes et âgés ont produit le même modèle de variabilité ; la variabilité dans la production du ton différait selon le ton lexical.
Les tons descendants et montants ont produit de plus petits degrés de variabilité que les tons moyens, bas ou hauts.
Les résultats sont interprétés pour souligner la nature de la variabilité de F 0, sa relation avec la quantité du mouvement de F 0 et les différences interlinguistiques de cette variabilité en fonction de la structure prosodique.
Des recherches précédentes ont montré que la charge de travail peut avoir un effet négatif sur l'utilisation des systèmes de reconnaissance de la parole.
Dans cet article, on discute de la relation entre charge de travail et parole et l'on fournit les résultats de deux études.
La première étude concerne le stress dû à la durée.
La seconde étude étudie les performances pour une tâche double.
Les deux études montrent que la charge de travail réduit les taux de reconnaissance et la performance de l'utilisateur.
La nature de la détérioration semble dépendre des individus et du type de charge de travail.
De plus, il apparaît que la charge de travail affecte la sélection des mots à utiliser, l'articulation des mots et la relation entre la tâche de parler à un système de reconnaissance automatique de la parole (RAP) et la réalisation d'autres travaux.
On suggère que le fait de parler à un système de RAP est une tâche contraignante en elle-même, et que, lorsque la charge de travail augmente, la capacité de réalisation de cette tâche, dans les limites imposées par le système de reconnaissance, décroît.
Les Synonyma d'Isidore de Séville, oeuvre écrite dans un style synonymique mais de contenu moral, pouvaient être utilisés à la fois comme livre de grammaire et comme livre de morale.
C'est cependant cette seconde lecture qui a clairement dominé au Moyen Âge, comme le prouve l'étude du paratexte et du contexte manuscrit, des inventaires médiévaux, des centons ou de la postérité littéraire.
Cette publication présente deux nouvelles approches de conception de réseaux d'antennes imprimées.
La première est basée sur la technique des algorithmes génétiques inspirée des processus de l'évolution des espèces et de la génétique naturelle et la deuxième sur l'analogie entre la résolution des problèmes d'optimisation combinatoire et le recuit des solides.
Ces deux approches permettent de rechercher simultanément la loi d'alimentation optimale et la répartition spatiale des éléments rayonnants pour que le diagramme de directivité du réseau soit aussi proche que possible d'un diagramme désiré optimal spécifié à partir d'une fonction ou d'un gabarit.
La mise en correspondance d'objets 3D est un problème important dans le domaine du traitement d'image.
Il apparaît lorsque des données acquises par différents capteurs, à des moments ou/et des instants différents doivent être fusionnées.
Si l'on suppose que les objets à mettre en correspondance sont rigides, nous avons à retrouver les paramètres d'une transformation rigide.
Cet article présente une méthode itérative générale pour la mise en correspondance d'objets 3D.
Son originalité réside dans ses fondements mécaniques : plutôt que de minimiser une énergie potentielle par rapport aux paramètres de la transformation rigide, qui est l'approche classique, nous étudions le mouvement d'un objet rigide, c'est-à-dire un solide, dans un champ de potentiel.
Cette approche particulière prend en compte l'énergie cinétique du solide, ce qui permet de « sauter » certains maxima locaux de l'énergie potentielle et donc d'en éviter certains minima locaux.
L'article est illustré par l'application de la méthode au recalage d'images médicales réelles, où nous utilisons la totalité du volume segmenté.
Différents modes de prescription sont décelables dans les listes de termes recommandés dans le cadre du circuit de terminologie officielle en France, tel qu'il fonctionne depuis plus de vingt ans (suite au décret Juppé en 1996).
Diverses dynamiques sont observées dans ces listes officielles, où sont proposées de précieuses remarques.
On tente de considérer ces éléments de prescription officielle comme on le ferait dans le cadre d'une analyse combinée métalexicographique et terminologique.
Cet article présente tout d'abord le web sémantique, qui vise à développer un niveau de connaissances indépendant de sa réalisation en langue.
Nous montrons que ce projet est illusoire si on n'établit pas un lien entre les connaissances formalisées à travers des ontologies ou des réseaux sémantiques et les textes présents sur le web.
L'article présente différentes approches permettant de faire ce lien et revient sur la question du rapport entre sources de connaissances générales et connaissances acquises à partir de textes.
Le contact entre les communautés vietnamienne et française durant l'occupation française au Viêt Nam a entraîné l'adaptation phonologique d'un nombre important d'emprunts au français.
En vietnamien, ces emprunts se sont vu attribuer des tons.
La littérature scientifique sur cette question (ainsi que sur celle du contact linguistique français-vietnamien) est jusqu'ici limitée et se fonde sur un nombre restreint d'emprunts.
L'analyse phonologique et statistique de 600 mots vietnamiens d'origine française nous permet d'étudier les mécanismes d'attribution de tons à des syllabes qui en étaient dépourvues.
Nous dénombrons l'occurrence de chaque ton, et dégageons quelques-uns des principes régissant l'intégration tonale des emprunts au français.
Dans cette étude, nous nous intéressons aux unités sonores, vocalises séparées par 2 silences, qui composent ces chants, à leurs récurrences, et à leurs structurations.
Cependant, tous ces paramètres dépendent de l'année et du lieu d'enregistrement.
Cet article propose un codage parcimonieux des chants afin de déterminer leurs composantes stables de celles qui varient, pour différentes échelles de temps.
Une définition de la complexité du code est également proposée afin de séparer les composantes du chant du bruit mer.
Cette étude montre statistiquement que les codes les plus courts sont les plus stables et surviennent avec une fréquence similaire sur deux années consécutives, tandis que les plus longues unités sont clairement différentes.
Le filtre à long-terme d'un codeur de parole CELP à bas-débit a une influence notable sur la qualité de la parole reconstruite.
Dans cet article, nous proposons un pseudo-filtre de prédiction à long terme à plusieurs coefficients qui possède moins de degrés de liberté que le nombre de coefficients de prédiction, mais donne un meilleur gain en prédiction à long-terme et une meilleure réponse en fréquence qu'un filtre de prédiction conventionnel à un seul coefficient.
Le gain de prédiction de ce filtre est comparé à celui des filtres classiques à un seul coefficient et à trois coefficients, avec des décalages entiers et fractionnaires.
Elle donne de meilleurs résultats qu'un test de stabilité stricte.
Pour finir, nous avons incorporé notre pseudo-filtre de prédiction à long terme à plusieurs coefficients dans un codeur CELP à 4.8 kbit/s.
Tant le rapport objectif signal à bruit que la qualité subjective ont été améliorés par rapport à ceux mesurés avec un filtre de prédiction à long terme conventionnel à un seul coefficient.
L'algorithme SELP (“Stochastically Excited Linear Prediction”) offre de bonnes performances à un débit aussi faible que 4.8 kbit/s.
La procédure de la prédiction linéaire supprime la corrélation à court terme.
La boucle de mélodie enléve la corrélation à long terme, produisant un résidu proche du bruit qui est soumis à une quantification vectorielle.
L'information décrivant les coefficients du filtre LPC, la prédiction à long terme et la quantification vectorielle est transmise.
Dans cet article, nous décrivons des améliorations apportées à l'algorithme SELP qui aboutissent à une meilleure qualité de parole et à uapidité accrue de calcul.
A boucle fermeé, le ciuit de mélodie peut être interpréte une quantification vectorielle du signal d'excitation désiré à l'aide d'un dictionnaire adaptatif composé de séquences antérieures d'excitations.
Pour améliorer la modélisation de la non-stationnarité du signal de parole, nous enrichissons ce code avec un ensemble de candidats-vecteurs qui sont des transformées d'autres entrées du dictionnaire.
La deuxième quantification vectorielle est exécutée avec un dictionnaire stochastique fixe.
Dans sa forme originale, l'algorithme SELP nécessite un effort excessif de calcul.
Nous utilisons un nouvel algorithme récursif de consultaton rapide du dictionnaire adaptatif.
De cette manière, nous modifions le critère d'erreur et nous exploitons les symétries résultantes.
La même quantification vectorielle rapide est appliquée au dictionnaire.
Ce papier présente une revue et une analyse des résultats obtenus par les auteurs et par leurs collaborateurs en ce qui concerne l'effet de la transition de phase dans le problème de l'appariement.
Ce qui a été mis en évidence est que la recherche d'hypothèses, guidée par les heuristiques plus connues (gain d'information ou simplicité), finissent inévitablement dans la région TP : donc, on ne peut pas éviter cette région à haute complexité.
Les conclusions qu'on peut en tirer pour l'apprentissage relationnel est qu'il semble difficile de pouvoir approcher des problèmes plus importants que ceux que l'on a actuellement.
Le choix d'une mesure de distance est très important pour la quantification vectorielle (QV) à la fois pour la construction du dictionnaire et pour la recherche de la valeur quantifié.
La distance spectrale, qui est la plus significative, est peu souvent utilisé à cause de sa complexité calculatoire.
La distance Euclidienne quadratique pondérée est mathématiquement plus attractive et par conséquent plus souvent utilisée.
Selon la distance utilisée, on peut trouver des différences significatives en terme de performances.
Dans cet article, une comparaison entre les différentes distances Euclidienne quadratique pondérée est étudiée.
Une mesure de distance est proposée dans le cas de la QV des paires de raies spectrales (LSP) ou Cosinus des LSP (CLSP).
Bien que des systèmes de transcription de la prosodie existent depuis longtemps, la nécessité de représenter l'information prosodique dans de larges bases de données de parole impose à ces systémes des exigences nouvelles.
Le système ToBI récemment élaboré aux Etats-Unis présente de nombreuses différences intéressantes par rapport au système “Standard britannique” utilisé depuis plusieurs dizaines d'années.
Cet article analyse ces différences dans le contexte des recherches actuelles sur un corpus d'anglais parlé et examine la possibilité de conversion automatique entre les deux types de transcription.
Deux larynx humains excisés ont été utilisés pour étudier les effets du changement de l'acoustique supraglottique sur la fréquence fondamentale (F 0) d'oscillation des cordes vocales.
Un tube artificiel supraglottique a été relié au larynx.
Deux pistons cylindriques ont été introduits séparément dans le tube et déplacés à l'intérieur de celui-ci afin de représenter des voyelles neutres, avant et arrière.
Quand le piston le plus petit a été introduit, on a pu mesurer, en général, une augmentation de la F 0.
Les changements de la F 0 liés à l'utilisation du piston le plus grand n'ont pas montré de variations systématiques.
Dans plusiers cas, on a pu mesurer une diminution de la F 0.
La résultats sont interprétés en termes de rétroaction acoustico-mécanique dans l'interaction de la source et du conduit vocal.
En conclusion, nous constatons que la F 0 intrinsèque des voyelles ne peut pas être expliquée à partir de bases acoustiques.
Les conditions acoustiques pour le petit piston ont été simulées à l'aide d'un modéle théorique.
Nous décrivons un nouvel algorithme pour la détection robuste des fins de mots isolés.
L'algorithme utilise des mesures simples basées sur l'énergie et le taux de passages à zéro afin de discriminer entre parole et silence.
Au lieu du modèle conventionnel à deux états, un modèle à trois temps comprenant une phase de transition est introduit.
Le taux de passages à zéro est traité de manière spéciale lors de la phase de transition afin d'améliorer la précision de la détection des fins de mots.
L'algorithme est basé sur une classification en contexte et utilise quelques heuristiques sur base de connaissances afin de corriger des détections erronées.
L'approche est celle utilisée lors de la détection visuelle de formes d'onde noyées dans du bruit.
Aucune connaissance préalable de la nature du bruit n'est exigée et l'algorithme produit des résultats fiables même dans les cas où le signal débute par des bruits buccaux parasites.
Un aspect important de l'algorithme est sa facilité d'implantation pour le traitement en temps réel.
L'algorithme est adaptif et peut s'accommoder d'environnements dotés de rapports signal-sur-bruit variables.
L'algorithme a été initialement développé sur la parole enregistrée dans une chambre anéchoïque.
Les modifications nécessaires pour une application à la parole de qualité téléphonique sont décrites, ainsi que les résultats de tests préliminaires.
Une étude par analyse/synthèse de la pharyngalisation est réalisée dans le cadre du développement d'une synthèse par règles de l'Arabe.
Dans ce travail nous utilisons un synthétiseur à formants pour la paramétrisation acoustique des consonnes pharyngalisées de l'Arabe/ /, / /, / /, / /, / /.
L'étude acoustique montre que le système vocalique arabe traditionnellement décrit comme un ensemble de 3 voyelles brèves et de leurs 3 opposées longues est inadéquat.
Un système de 12 voyelles dont 6 pharyngalisées est défini en compatibilité avec les faits acoustiques et phonologiques.
Les règles régissant l'extension de la pharyngalisation au sein de la chaîne sonore sont établies et validées par synthèse.
Nous commençons par considérer les travaux d'histoire institutionnelle très récents, travaux qui nous conduisent à mieux comprendre une face cachée jusqu'alors de l'histoire de la norme en France : celle de l'exclusion du Parlement de Paris comme modèle de bon usage.
Dans la seconde moitié de l'article nous abordons une question incontournable : celle de la portée des injonctions normatives sur l'usage, en examinant trois études de cas dans les domaines de la morphologie et du lexique.
Nous présentons les principaux travaux menés dans le projet Sample Orchestrator, destiné au développement de fonctions innovantes de manipulation d'échantillons sonores.
Celles-ci se fondent sur des études consacrées à la description des sons, c 'est-à-dire à la formalisation de structures de données pertinentes pour caractériser le contenu et l'organisation des sons.
Ces travaux ont été appliqués à l'indexation automatique des sons, ainsi qu 'à la réalisation d'applications inédites pour la création musicale - synthèse sonore interactive par corpus et aide informatisée à l'orchestration.
Le projet a aussi comporté un important volet consacré au traitement de haute qualité des sons, à travers plusieurs perfectionnements du modèle de vocodeur de phase - traitement par modèle sinusoïdal dans le domaine spectral et calcul automatique des paramètres d'analyse.
La fonction de correction de pôles supérieurs (HPC) dans un modéle tout-pôle de production de parole, analogique ou numérique, est analysée en comparant celui-ci à une ligne de transmission (TL).
La validité du modèle TL, qui sert de référence dans l'étude, est testée en comparant sa fonction de transfert aux mesures acoustiques effectuées sur un modéle physique du conduit vocal.
La variation de la longueur efficace du conduit vocal s'est avérée être un paramètre important dans la formulation de la HPC.
Même si les réponses en fréquencies de la HPC different dans les cas analogique et numérique. les changements relatifs dans la correction, qui sont influencés par les variations de la longueur efficace du conduit vocal, sont exactement les mêmes dans les deux cas.
Nous formons ainsi un nouveau type de modèle de production de parole comportant des pôles et des zéros.
Celui-ci est aussi mis en relation avec le modèle PARCAS [Laine, 1982].
On emploie dans cet article les réseaux connexionnistes pour deux problèmes concernant les mouvements articulatoires.
Le premier est l'estimation du mouvement articulatoire à partir du signal de parole ; le second est la génération du mouvement articulatoire étant donnée une séquence de symboles phonémiques.
Les réseaux connexionnistes sont un outil adapté au premier problème, puisque l'estimation des paramètres articulatoires est connue pour être une application non linéaire entre les paramètres accoustiques et les param`etres articulatoires.
Pour le second, on construit efficacement un système de commande non linéaire pour produire le mouvement articulatoire, en combinant plusieurs réseaux connexionnistes.
Les variations temporelles du profil de la langue enregitrées dans des images cinéradiographiques sont décrites par les mouvements de quatre paramètres articulatoires.
La variation temporelle de chaque paramètre (mouvement) est supposée être la sortie d'un filtre auto-régressif invariant dans le temps.
Chaque filtre est excité par une séquence d'impulsions qui représente la commande articulatoire.
Les coefficients du filtre, la position et l'amplitude des impulsions sont déterminés par la méthode de la MLPC.
Le nombre minimum d'impulsions pour chaque paramètre articulatoire est déterminé en utilisant un critère acoustique.
Il dépend du nombre des traits phonétiques dans la phrase dont la réalisation dépend d'un paramètre particulier.
Cet article concerne l'étude de l'influence des contours intonatifs, des durées segmentales et des caractéristiques spectrales sur l'identification perceptive de deux styles d'élocution.
Deux locuteurs ont parlé “spontanément” à un interlocuteur et ont ensuite lu à haute voix la transcription littérale de leurs énoncés spontanés.
On a ensuite sélectionné des paires d'énoncés qui étaient à la fois identiques dans les versions “spontanées” et lues, et “fluides” dans les deux versions (pas de reprise, pas d'hésitation, etc.).
On a construit 5 conditions de test en manipulant les énoncés comme suit : (1) aucune manipulation ; (2) échange des durées segmentales entre les deux styles d'élocution ; (3) échange des contours intonatifs entre les deux styles d'élocution ; (4) application d'un contour intonatif monotone ; (5) combinaison des caractéristiques spectrales originales avec les contours prosodiques du style d'élocution opposé.
La tâche des 32 auditeurs était de classer chaque stimulus dans l'une des deux catégories “lue” ou “spontanée”.
Il apparaît que toutes les manipulations des énoncés avaient une influence sur cette classification.
Diverses mesures acoustiques ont également été effectuées sur les énoncés originaux.
Globalement, la parole lue semble avoir, par rapport à la parole spontanée : une vitesse d'articulation plus faible, des variations de F 0 plus amples, une déclinaison de F 0 plus marquée, moins de vibrato, et moins de réduction vocalique.
Toutefois, aucun de ces traits acoustiques ne suffit à lui seul pour discriminer entre les deux styles d'élocution et l'on note une très grande variabilité dans les réalisations des locuteurs et les performances des auditeurs.
Ce document propose une structure des données sous forme de tableau, enrichie d'une grille de mots et d'un système d'analyse grammaticale exploitables dans le domaine de la reconnaissance de la parole.
Ce tableau enrichi et l'algorithme d'analyse grammaticale qui y est associé peuvent représenter et analyser grammaticalement de façon très efficace une grille d'hypothèses de mots émis dans le domaine de la reconnaissance de la parole avec un niveau d'ambiguïté lexicale élevé, sans pour autant changer les principes fondamentaux de l'analyse grammaticale sous forme étendue.
Chaque grille de mots peut être entrée avec ordre et cohérence dans le tableau enrichi parmi des hypothèses de mots bien mémorisées dans ledit tableau.
Un point de rupture est déterminé par rapport à des points de liaison que représentent des hypothèses de mots séparés dans le texte mais qu'il est en réalité possible de relier.
Les résultats expérimentaux préliminaires nous montrent que l'enrichissement de l'analyse grammaticale sous forme de diagramme étendu permet l'interprétation de tous les composants possibles à l'entrée du réseau lexical et élimine la nécessité de créer chaque composant plus d'une fois.
Cela réduit la complexité des calculs de manière significative surtout en cas d'ambiguïté lexicale importante dans l'entrée de la grille lexicale comme cela arrive dans de nombreux problèmes de reconnaissance de la parole.
Cet enrichissement de l'analyse grammaticale sous forme de tableau représente une approche très utile et très efficace en ce qui concerne les problèmes de traitement de l'information dans le domaine de la reconnaissance de la parole.
Les difficultés soulevées lors de la comparaison de signatures vibro-acoustiques d'équipements de commutation de haute puissance nous ont amené à développer un nouvel algorithme de recalage temporel.
Cette comparaison de signatures est requise pour réaliser la surveillance de ces équipements.
Or, ces signatures comportent une suite de transitoires générée par une séquence d'événements électromécaniques qui apparaissent avec une trame temporelle légèrement différente, d'une commutation à l'autre, selon, entre autres, la température et la charge.
Cette distorsion temporelle génère une divergence significative entre les amplitudes instantanées des signatures.
De plus, la grandeur de la déviation temporelle, entre la dernière signature et une référence, a une utilité diagnostique.
L'algorithme de recalage temporel proposé permet de trouver la relation temporelle entre les événements de deux signatures, même en présence de discontinuités de la trame temporelle.
Or, nous démontrons que ces discontinuités ne sont pas traitées adéquatement par l'algorithme de recalage temporel DTW (Dynamic Time Warping) utilisé couramment dans le traitement de la parole.
Nous expliquons donc le fonctionnement de ces deux algorithmes et nous présentons des résultats démontrant la plus grande acuité de la corrélation multiéchelle.
C'est en partie l'interpolation de la trace warp qui permet d'atteindre cette acuité.
Cette trace warp est une fonction décrivant la déviation temporelle entre signatures, rapportant même la présence d'inversions dans l'ordre d'apparition de transitoires.
De nombreux domaines d'application font appel à des signaux à plus d'une dimension (ND) et le développement des moyens de calcul permet de mettre en œuvre des traitements adaptés à ces signaux.
Après avoir précisé les limites du domaine couvert et donné quelques éléments de terminologie, nous présentons les modèles de signaux ND.
Nous donnons ensuite une approche synthétique des méthodes d'estimation (ou de mesure) des caractéristiques des signaux ND.
Notre exposé est complété par la présentation et l'étude des principaux opérateurs (filtres) utilisés dans le traitement des signaux ND en liaison avec les opérateurs déjà explorés dans le traitement des signaux 1D.
Les réseaux de neurones sont actuellement d'usage courant en traitement du signal et de l'image.
Nous proposons un nouveau modèle de neurone qui utilise un codage particulier de sa sortie que nous nommerons « Représentation Scalaire Distribuée » (RSD).
Cette représentation repose sur l'idée de représenter la sortie d'un neurone par une fonction et non par un scalaire.
Nous montrons que la RSD induit un comportement non linéaire des connexions entre neurones.
La RSD est décrite dans toute sa généralité, puis particularisée pour sa mise en œuvre pratique.
Nous considérons notamment la mise en œuvre de la RSD dans un réseau de neurones de type Perceptron Multi-Couches, et nous proposons un algorithme d'apprentissage.
Enfin, nous validons le modèle sur deux applications : la réduction de dimensionnalité et la prédiction.
Dans les deux cas, un gain important par rapport au modèle classique est obtenu.
L'optimisation d'un système de détection distribuée parallèle comprenant N capteurs aboutit toujours à un système de 2 N + N équations non linéaires couplées, qui n'est résolu pour l'instant que pour des cas particuliers (en supposant par exemple l'indépendance des observations locales) et pour des systèmes comportant peu de capteurs.
Le nombre d'équations à résoudre simultanément augmente très rapidement avec le nombre de capteurs.
Les calculs nécessaires à la résolution de ces équations deviennent alors très vite inextricables.
Dans cette contribution, une procédure de sélection de capteurs pertinents pour le processus de décision basée sur l'utilisation de l'entropie conditionnelle de Shannon est développée.
Puis, ces systèmes sont optimisés via une méthode entropique.
Celle-ci détermine les seuils locaux et construit un arbre de décision (qui représente l'opérateur de fusion) permettant de minimiser la probabilité d'erreur de décision.
Les performances des systèmes distribués parallèles étant moins bonnes que celles des systèmes centralisés, les techniques d'optimisation précédentes seront étendues au problème de la quantification répartie afin d'obtenir un compromis entre la quantité d'information à envoyer à l'opérateur de fusion et les performances souhaitées du système de décision.
Un système permettant à un signal vocal d'être isolé de bruits de fond ou bien d'autres signaux vocaux non-désirés ainsi que d'autres interférences sous conditions réelles a été développé.
En utilisant deux micros, placés à 25 cm 1'un de 1'autre, le système exploite des repères directionels et harmoniques dans un algorithme hybride qui profite de ces deux techniques.
Le signal de sortie de l'algorithme a été testé de façon subjective par des auditeurs humains et de façon objective par un système de reconnaissance de voices.
Les résultats démontrent une amélioration de l'intelligibilité du signal vocal désiré.
Par exemple, dans une série de tests, les performances d'un système de reconnaissance de voies après ségrégation est comparable à une proportion signal/bruit de 6 dB comparé à une proportion signal/bruit de 20 dB sans ségrégation
Estimer une fonction échantillonnée irrégulièrement à partir d'un ensemble de points constitue un problème de régression classique.
Des solutions basées sur les méthodes à noyaux existent mais leur mise en œuvre conduit à deux problèmes récurrents de sélection de modèles : l'optimisation des paramètres du noyau et le réglage du compromis biais-variance.
Cet article présente une méthode novatrice pour estimer une fonction à partir d'un ensemble de points bruités dans le contexte des espaces de Hilbert à noyau reproduisant.
Nous avons conçu l'algorithme du Kernel Basis Pursuit pour construire une solution reposant sur des noyaux multiples et une régularisation L\.
Notre idée est de décomposer la fonction à apprendre dans l'espace engendré par un dictionnaire de fonctions explicatives, à la manière des statégies de poursuite.
La mise en œuvre repose sur la formulation du LASSO (Least Absolute Shrinkage and Selection Operator) et nous avons utilisé l'algorithme Least Angle Regression Stepwise pour la résolution.
Le calcul du chemin complet de régularisation nous permet d'utiliser des nouveaux critères pour déterminer automatiquement le compromis biais-variance optimal.
L'article propose un nouveau modèle de Prony à pôles dépendant du temps pou r modéliser des signaux non stationnaires.
Ce modèle est basé sur une combinaison linéaire d'exponentielles complexes à coefficients variant avec le temps.
Il est une extension des techniques d'estimation spectrale appliquée dans le cas stationnaire : l'amplitude et la phase du signal varient avec le temps.
Nous présentons et justifions une méthode avec l'algorithme correspondant pour estimer complètement les paramètres du modèle.
Le calcul des paramètres dépendant du temps nécessite cinq étapes : l'estimation des paramètres autorégressifs (AR) variant avec le temps, l'estimation des pôles à droite, la modélisation de ces pôles, le calcul des nouveaux pôles et l'estimation avec une méthode des moindres carrés des facteurs d'amplitudes.
Pour valider le modèle, une simulation est effectuée sur un signal à deux composantes de loi en fréquence variant linéairement avec le temps.
De nombreux travaux présentent une combinaison d'apprentissage par démonstration et d'amélioration locale de politiques pour apprendre des contrôleurs pour des robots le long d'une trajectoire.
Il manque à ces travaux une capacité de généralisation permettant d'apprendre sur tout l'espace atteignable par le robot.
Dans cet article, nous présentons une méthode qui consiste à apprendre un tel contrôleur réactif en feedback et quasi optimal en deux étapes.
Ensuite, le contrôleur en feedback est optimisé par des méthodes de recherche directe sur les politiques.
Nous obtenons alors un contrôleur quasi optimal qui s'exécute 20 000 fois plus vite que l'original, pour une performance proche.
Ce travail est réalisé en simulation.
Cet article présente le système dyd (Dial-Your-Disc/Choisissez votre disque), qui comprend entre autres la fonction de recherche dans une grande base de données d'information musicale, et celle de génération d'un monologue sous forme vocale, une fois que l'oeuvre musicale a été sélectionnée.
L'article présente en détail la génération des monologues, avec un accent particulier sur la nécessité de recourir au contexte linguistique de l'énoncé pour effectuer différemment la génération en fonction de la position courante dans le monologue.
La plus grande partie des méthodes de classification de textures existantes consiste à alimenter un classifieur par un ensemble de paramètres caractéristiques calculés localement sur l'image texturée.
La mise en œuvre de ces méthodes dans le cadre d'applications opérationnelles suppose, la prise en compte d'un élément important : le risque de confusion de classes dans l'espace paramétrique.
Nous montrons qu'un classifieur connexionniste est capable d'exploiter efficacement ces paramètres.
Un système de reconnaissance automatique de consonnes sonorantes extraites à partir de la parole continue est décrit.
Le système est basé sur des régles de la logique floue.
Aprés avoir opéré une classification précatégorielle, dans laquelle l'extraction de traits est réalisée par des modules organisés en une hiérarche de niveaux, le message parlé est segmenté en noyaux pseudo-syllabiques et des hypothèse sur les voyelles et les consonnes sont émises.
Les régles qui améliorent la classification des consonnes liquides et nasales de la langue italienne ont été inférées à partir d'expériences et tiennent compte des effets de coarticulation.
Les résultats obtenus pour quatre locuteurs masculins et deux locuteurs féminins sont présentés de concert avec une motivation acoustico-phonétique de l'approche utilisée.
Ces résultats montrent que cette méthode aboutit à des performances nettement supérieures à celles des approches antérieures.
Les modèles de reconnaissance de mots diffèrent en ce qui concerne la localisation des effets du contexte sémantique des phrases.
En utilisant une technique de facilitation intermodale cette recherche examine la disponibilité des entrées lexicales en fonction de l'information présente dans le stimulus et des contraintes du contexte.
Pour rechercher le locus exact des effets des contextes de phrase, des mots-sondes, reliés associativement à des mots contextuellement appropriés ou inappropriés ont été présentés à différentes positions avant ou en même temps que le mot parlé.
Les résultats montrent que les contextes de phrase ne préselectionnent pas un ensemble de mots contextuellement approprié avant que ne soit disponible une information sensorielle concernant le mot parlé.
En outre, durant l'accès lexical, défini ici comme le contact initial avec les entrées lexicales et leurs propriétés sémantiques et syntaxiques, des mots à la fois contextuellement appropriés et inappropriés sont activés.
Les effets contextuels se produisent aprés l'accés lexical, à un stade où l'entrée sensorielle elle-même demeure insuffisamment informative pour choisir entrte les entrées activitées.
Ceci suggère que les contextes sémantiques de phrase produisent leurs effets au cours du processus de sélection d'une des hypothèses de reconnaissance activées.
Les codes malveillants, tels que les virus et les vers, sont rarement écrits de zéro ; en conséquence, il existe des relations de nature évolutive entre ces différents codes.
Etablir ces relations et construire une phylogénie précise permet d'espérer une meilleure capacité d'analyse de nouveaux codes malveillants et de disposer d'une méthode de fait de nommage de ces codes.
La concordance de permutations de code avec des parties de codes malveillants sont susceptibles d'être très intéressante dans l'établissement d'une phylogénie, dans la mesure où les étapes évolutives réalisées par les auteurs de codes malveillants ne conservent généralement pas l'ordre des instructions présentes dans le code commun.
Nous décrivons ici une famille de générateurs phylogénétiques réalisant des regroupements à l'aide de caractéristiques extraites d'arbres PQ.
Une expérience a été réalisée, dans laquelle l'arbre produit par ces générateurs est évalué d'une part en le comparant avec les classificiations de références utilisées par les antivirus par scannage, et d'autre part en le comparant aux phylogénies produites à l'aide de polygrammes de taille n (n-grammes), pondérés.
Les résultats démontrent l'intérêt de l'approche utilisant les permutations dans la génération phylogénétique des codes malveillants.
Nous présentons un modèle d'interlocuteur pour un agent virtuel.
Il génère des comportements d'écoute (rétroactions) en fonction des comportements verbaux et non verbaux de l'utilisateur.
La personnalité de l'agent virtuel doit influencer le choix de ses comportements d'écoute.
Nous supposons que les agents extravertis ont tendance à montrer plus de rétroactions que les introvertis et que la stabilité émotionnelle est liée à la tendance des agents à imiter l'utilisateur.
Des études perceptives sur internet ont été réalisées pour évaluer notre modèle dans une situation d'interaction agent-utilisateur.
Nous présentons une base de données de grande envergure pour la langue japonaise.
Elle consiste en une base de données de mots et de parole continue ; elle est relative à un grand nombre de locuteurs et à la synthèse de la parole.
De nombreuses transcriptions ont été réalisées à cinq niveaux allant d'une simple description phonémique à une description acoustico-phonétique détaillée.
La base de données a été utilisée pour le développement d'algorithmes de reconnaissance de la parole, pour des études relatives à la synthèse et pour rechercher des marques acoustiques, phonétiques et linguistiques utiles aux technologies de la parole.
Le but de cet article est d'attirer l'attention sur le rôle que joue la structure prosodique dans la reconnaissance des mots.
Nous commençons par soutenir que la notion de mot écrit a eu une influence bien trop grande sur les modéles de la reconnaissance des mots parlés.
Nous discutons ensuite plusieurs propriétés de la structure prosodique qui sont importantes pour les questions de reconnaissance.
Nous présentons ensuite une conception de la reconnaissance des mots “en continu” qui tient compte de l'alternance entre syllabes fortes et faibles dans le flux de la parole.
L'Accès lexical est guidé par les syllabes fortes, tandis que les syllabes faibles sont identifiées par une analyse globale de leur distribution et par l'utilisation de règles phonotactiques et morphonémiques.
Nous concluons par une discussion de la controverse sur les différences d'accès aux mots à contenu et aux mots à fonction à la lumière de notre conception.
La rétro-propagation a été utilisée pour entraîner un petit réseau à prédire la durée syllabique dans un système texte-parole.
L'entrée et la sortie se présentent sous la forme de valeurs analogiques et le réseau effectue une fonction de régression multiple.
Des études récentes ont montré que les techniques de reconnaissance de la parole en sous-bandes peuvent améliorer les performances des systèmes classiques larges bandes dans des conditions de bruit additif à bande étroite.
Un aspect important de l'implémentation de l'approche en sous-bandes est le choix de la mèthode de recombinaison des observations dans chaque bande.
Ce papier présente une nouvelle méthode, appelée modèle d'union probabiliste, pour effectuer cette recombinaison.
Ce nouveau modèle est basé sur la théorie de l'union des événements aléatoires, et représente une nouvelle méthode pour la modélisation d'observations partiellement bruitées sans (ou presque) connaissance a priori de la nature du bruit.
Le nouveau modèle a été intégré aux modèles de Markov cachés (HMM) et testé dans une táche de reconnaissance indépendante du locuteur de parole corrompue par divers types de bruits additifs.
Les résultats montrent que le nouveau modèle offre une bonne robustesse aux bruits bandes étroites, tout en ne nécessitant pas ou presque pas de connaissance des propriétés statistiques du bruit.
Nous présentons un nouvel algorithme réalisant un recalage multimodal d'images planes (2D, ARX) et tomographiques (3D, ARM).
Le recalage 2D/3D est défini par la recherche de la meilleure transformation rigide permettant de replacer un ensemble de données multimodales dans un espace tridimensionnel commun.
L'intérêt de la méthode proposée réside principalement en deux points.
Tout d'abord, l'exploitation de données anatomiques offre la possibilité d'un recalage sans référentiel externe, donc la possibilité d'un examen minimalement invasif plus confortable pour le patient.
Ensuite, après la sélection manuelle d'une structure anatomique de référence, la phase de recalage peut être réalisée indépendamment de l'opérateur, puisque l'initialisation est automatique.
En premier lieu, les données de l'imagerie tomographique sont exploitées pour reconstruire une structure tridimensionnelle.
Puis, la position optimale de cette structure dans le référentiel de l'ARX est recherchée au moyen d'une analyse multi-résolution et d'une procédure d'optimisation.
Finalement, la position de la structure tridimensionnelle étant connue en ARX et ARM, il est possible d'avoir une correspondance tridimensionnelle entre les modalités, à la condition de disposer d'au moins deux incidences pour les images planes.
Le CV est un document textuel singulier : faible structure, informations éparses, contenu fortement symbolique, etc. d'où la difficulté de traitement de ces documents.
Nous utilisons cet espace ainsi défini pour modéliser le ciblage par apprentissage supervisé.
En utilisant des méthodes d'arbres d'induction et analyse discriminante, nous obtenons des résultats intéressants en apprentissage (86% de rappel et 88% de précision).
Même si les résultats en validation peuvent paraître décevants (55% de rappel et 60% de précision), cette approche ouvre des perspectives intéressantes d'exploitation automatique de CV basée sur le contenu informationnel et non à partir de simples mots clefs.
Cet article propose deux méthodes de construction d'un modèle indépendant du phonème et du locuteur qui réduisent considérablement les calculs nécessaires à la normalisation de la similarité (ou de la vraisemblance) pour la vérification du locuteur.
Pour chaque élocution, ces méthodes demandent de calculer la similarité avec un modèle unique, et non pas avec tous ceux des locuteurs de référence comme on le fait classiquement.
De plus, ces nouvelles méthodes présentent des performances égales ou supérieures à celles des méthodes classiques.
Le test de vérification du locuteur se fait en utilisant des groupes séparés d'usagers et d'imposteurs, ce qui permet d'évaluer sa performance pratique de fonctionnement.
Les taux d'erreur sur les locuteurs (et sur les textes) sont dans ce cas approximativement une fois et demi plus grands que quand on utilise la même population pour les usagers et les imposteurs.
Avec 15 usagers et un groupe séparé de 15 imposteurs, le taux d'erreur en vérification indépendante du texte est de 1.8% et celui obtenu en vérification du locuteur avec un texte imposé est de 1.1%. Ces deux chiffres sont obtenus grâce à la normalisation;
le second est environ deux fois plus élevé si on utilise la méthode sans normalisation.
Dans cet article, on présente un bilan de l'état de l'art en modélisation statistique pour l'élaboration de systèmes de dialogue oral.
Ce bilan traite en particulier de la modélisation acoustique des unités de parole pour la reconnaissance de parole et de la modélisation du langage pour le traitement du langage naturel.
On mentionne certaines des techniques émergentes pour la modélisation statistique et l'on montre la similarité qui existe entre la modélisation du langage et la modélisation acoustique.
Enfin, on discute des problèmes de recherche et de décision liés à l'intégration de sources de connaissances en reconnaissance de la parole et traitement du langage naturel.
Cet article s'intéresse au problème de l'explication des résultats fournis par un arbre de décision utilisé en tant que système d'aide à la décision.
On cherche à apporter une information supplémentaire à la classe prédite pour chaque vecteur particulier de données d'entrée.
Actuellement on dispose surtout de la trace du classement (le chemin parcouru dans l'arbre), et d'une estimation du taux d'erreur ou d'un risque associé à un mauvais classement.
Nous proposons ici deux nouvelles méthodes de qualification du résultat, basées sur une étude géométrique de la frontière de l'image inverse des différentes classes (la surface de décision).
La première méthode consiste à identifier les séparateurs les plus déterminants pour expliquer le résultat, en effectuant une analyse de sensibilité, par projection des données initiales sur la surface de décision.
Dans cette époque de systèmes et d'interfaces multi-modales, de nombreuses équipes de recherche tentent de trouver pour quels usages l'utilisation de nouvelles combinaisons de modalités pourraient être nécessaires.
Basées sur l'étude d'applications particulières, les recherches empiriques sur la fonctionnalité de la parole concernent des points d'un vaste espace multi-dimensionnel.
Au mieux, des résultats solides permettent des généralisations de bas-niveau qui ne peuvent être utilisées que pour la mise au point d'applications quasi-identiques.
De plus, l'appareil conceptuel et théorique nécessaire pour décrire ces résultats sur la base de principes, manque.
Cet article défend la position suivant laquelle seul un changement de perspective peut permettre de traiter les questions de choix de modalités d'un point de vue tant scientifique que pratique.
Au lieu de se focaliser empiriquement sur des fragments d'une combinatoire virtuellement infinie de tâches, d'environnements, de paramètres de performance, de groupes d'utilisateurs, de propriétés cognitives, etc., le problème de la fonctionnalité des modalités est traité comme un problème de choix parmi diverses modalités qui ont diverses propriétés par rapport à la représentation et à l'échange d'information entre l'utilisateur et le système.
Sur la base de l'étude de 120 demandes d'usage de la fonctionnalité `parole' tirées de la littérature, on montre qu'un ensemble réduit de propriétés de cette modalité permet, avec une efficacité surprenante, de justifier, soutenir et corriger l'ensemble des demandes.
Cet article analyse pourquoi les propriétés des modalités peuvent être utilisées pour ces usages et défend l'idée que leur efficacité peut être mise à la disposition des développeurs de systèmes et d'interfaces qui ont à faire un choix entre diverses modalités lors des premières phases d'élaboration d'interfaces et de systèmes à composante vocale.
Avec l'hypertexte, nous illustrons comment cette efficacité peut être gérée pour permettre de soutenir de façon prédictive le choix de la modalité `parole' dans les premières phase de développement de systèmes et d'interfaces.
Des études antérieures ont montré que la discriminabilité entre une paire d'occlusives est fonction du couple proprement dit et de sa position dans une syllable CVC.
Dans deux expériences, on a testé l'hypothèse selon laquelle les différences du pouvoir discriminatif sont liées à des différences dans la représentation auditive des stimuli telle qu'elle résulte de l'analyse en fréquence par le système auditif périphérique.
La représentation auditive de chaque consonne en position initiale et finale a été ensuite mesurée par une technique de masquage simultané.
Les deux indices qui identitient une consonne sont (a) le spectre instantané de la transition formantique au début ou à la fin de la syllable et (b) l'allure de la transition formantique.
La discriminabilité de six paires de consonnes (trois initiales et trois finales) a été corrélée avec la différence entre leurs pattern de masquage au début ou à la fin (ϱ = 0.89) et avec les différences entre la représentation auditive des transitions (ϱ = 0.94).
L'importance de ces corrélations indique que les variations du pouvoir discriminatif entre consonnes peuvent été predite a partir de la connaissance de leut représentation auditive au niveau périphérique avec une faible contribution de facteurs phonétiques, linguistiques ou cognitifs.
Dans cet article nous proposons de coupler la géométrie riemannienne avec les techniques d'apprentissage pour une biométrie faciale efficace et robuste aux changements d'expressions faciales.
Nous représentons localement la forme des surfaces faciales par des collections de courbes 3D.
Nous appliquons des techniques d'apprentissage afin de déterminer les courbes les plus pertinentes à la reconnaissance d'identité des personnes.
Le taux de reconnaissance de l'ordre de 98, 02 % sur le benchmark de référence FRGC v2 confirme l'efficacité de coupler l'analyse géométrique de la forme avec des techniques d'apprentissage.
Une classification des différentes méthodes pour l'évaluation des systèmes de synthèse à partir du texte, selon les exigences imposées à l'auditeur, est suggérée et discutée.
La classification est faite selon les quatre niveaux traditionnels d'évaluation : les niveaux Nominal, Ordinal, Intervalle et Relatif.
Un cinquième niveau, le Supra-Nominal, qui comprend les processus de mémorisation, est proposé.
Les méthodes sont divisées en méthodes qualitatives non métriques et en méthodes quantitatives métriques.
Il en résulte que le niveau métrique d'évaluation le plus élevé (Relatif) n'est pas nécessairement le niveau qui impose les plus grandes exigences à l'auditeur.
Tout au contraire, le niveau Nominal, qui se base sur une approche qualitative non métrique, impose à l'auditeur des exigences bien plus élevées.
Cette article s'intéresse à la fusion d'images sériées, et ce pour la restauration de l'image 2D d'un objet.
Pour réaliser cette fusion, nous avons besoin d'estimer la fonction de transfert entre les courbes de sensibilités de ces films.
Dans notre contexte, cette estimation est difficile car l'une des images est perturbée par des taches lumineuses.
Dans ce cadre, nous montrons que l'on peut s'affranchir de la présence de ces taches afin de réaliser la restauration.
Il existe de nombreuses situations où le rehaussement de la parole ne nécessite pas d'être fait en temps-réel.
Pour ce type d'applications, l'emploi de toutes les connaissances disponibles a priori peut permettre d'aboutir à des solutions plus efficaces.
Dans cette étude, on a développé un nouvel algorithme de rehaussement de parole, dépendant du texte, pour des applications de ce type.
Dans notre approche, le texte du dialogue concerné est utilisé pour répartir la parole bruitée en classes correspondant à des classifications phonétiques larges.
Les classes considérées concernent les plosives, les fricatives, les affriquées, les nasales, les voyelles, les semi-voyelles, les diphtongues et le silence.
Ces partitions sont ensuite utilisées pour piloter un nouveau procédé de rehaussement basé sur une quantification vectorielle dans lequel des contraintes liées aux classes phonétiques sont utilisées pour améliorer la qualité de la parole.
L'algorithme proposé a été, évalué par des méthodes d'evaluation objectives et subjectives.
On montre que cette approche, dépendante du texte, améliore la qualité de la parole dégradée pour un grand nombre de sources de bruit (par exemple bruit blanc de canal de transmission, bruit de cockpit d'avion, vrombissement d'hélicoptère et bruit d'autoroute).
Dans chaque cas, la méthode proposée fournit une qualité de parole notablement supérieure à celle obtenue par soustraction spectrale linéaire ou généralisée, ou par la méthode Auto-LSP de rehaussement itératif contraint utilisant la mesure d'Itakura-Saito et un corpus d'évaluation de 100 phrases.
Les évaluations de qualité subjective ont été menées sous la forme d'un test de comparaison A-B.
Les résultats de ces évaluations montrent que, pour les distorsions dues à des bruits large-bande, l'algorithme proposé est préferé dans un rapport 2 à 1 par rapport à la parole bruitée non-traitée et qu'il est préféré dans un rapport 3 à 1 par rapport à la soustraction spectrale.
Pour la reconnaissance de la parole avec des méthodes probabilistes, il est souvent souhaitable de pouvoir estimer la contribution du modèle de langage et celle du modèle acoustique.
Nous proposons une approche basée sur la théorie de l'information qui considère l'interaction entre ces deux sources d'informations.
On présente des résultats expérimentaux concernant le prototype de reconnaissance d'IBM pour la langue italienne qui est capable de fonctionner en temps réel avec un dictionnaire de 20.000 mots.
Dans le cadre d'un modèle phonosyntaxique de l'intonation de la phrase [1–4], la structure prosodique de la phrase est indiquée par des contours mélodiques placés sur les voyelles accentuées.
La description phonologique de ces contours utilise les traits [±Extreme], [±Montant] et [±Ample].
L'analyse acoustique de phrases lues montre que, à l'exception du contour final, l'intensite et la durée ne jouent pas de rôle significatif dans la différenciation des contours, qui contrastenteffectivement par leur pente montante ou descendante, et par l'amplitude relative de variation mélodique.
Ce papier propose une approche multi-agent locale, coopérative et temps réel pour la création de profils adaptatifs et incrémentaux dans laquelle un usager est supposé être représenté par un ensemble de documents textuels.
Ces documents sont analysés séquentiellement conduisant alors à la création d'un réseau terminologique temporaire (RTT).
Les résultats préliminaires obtenus ainsi que les perspectives associées sont enfin présentés.
Les consonnes du Néerlandais énoncées dans les listes de logatomes de deux syllabes du type CVCVC, et insérées dans des courtes phrase porteuses, ont été identifées ar des sujets sous diverses conditions de déformations acoustiques.
Les 28 conditions étaient un mélange de quatre temps de réverbération, cinq rapports signal/bruit et cinq spectres de bruit différents.
Les résultats d'indentification ont été sommés pour les six locuteurs et les cinq sujets.
De cette manère, nous avons obtenu 28 matrices de confusion par position de la consonne (initiale, médiane, finale).
Ces ensembles de matrices ont été traités par des programmes d'échelonnement multidimensionnel de différences individuelles, et, plus spécifiquement par TUCKALS (Kroonenberg et de Leeuw, [9]).
La configuration de stimulus tridimensional qui en résulte pour les consonnes initiales est très stable et peut être représentée comme un tétraèdre avec /z, s/, /m, n/, /p, t, k, b, d/, et /f, v, χ/ aux quatre coins et /l, r, w, j, h/ au centre.
Cette configuration de consonnes est discutée dans sa relation à langue néerlandaise, compte tenu des conditions expérimentales.
La représentation des 28 conditions devient presqu'entièrement uni-dimensionelle en dépit des trois aspects différents (temps de réverbération, niveau de bruit, spectre de bruit) des déformations de l'environnement acoustique.
Nous présentons deux expériences concernant la perception de la distinction entre /ba/ et /pa/ et dont l'object est de cerner les indices perceptifs signalant le début du voisement.
Ni le début de l'excitation périodique, ni le changement de balance spectrale ne paraissent constituer l'indice dominant pour le début du voisement.
Dans chaque expérience, le niveau d'intensité globale fournit la meilleure métrique pour expliquer la position des frontiéres perceptives.
On se représente souvent la seconde moitié du XVII e siècle, en raison de l'importance de ses propositions prescriptives, comme un moment où les discours tenus ont été acceptés et suivis d'effet.
C'est au volet « limites » que nous nous intéressons dans cette contribution.
Après avoir apporté quelques précisions terminologiques sur ce que l'on peut appeler normes et prescriptions, nous revenons sur les fondements de l' « anti-prescriptivisme » au XVII e siècle, notamment dans l'espace des « remarques sur la langue » , et nous montrons comment ce courant de résistance aux prescriptions a été repris par la culture mondaine de la fin du siècle.
Au total, nous considérons que, loin de pouvoir être considérée seulement comme une période de « réglage » , la seconde moitié du XVII e siècle peut être vue au contraire comme un moment conflictuel et polémique, caractérisé par une altération de la normativité traditionnelle de la grammaire et par l'installation de régimes sociaux d'opposition qui rendent difficile tout établissement d'une norme unique adossée à une prescription claire.
Les situations, les interprétations sémantiques du contexte, fournissent une meilleure base pour sélectionner des comportements adaptatifs que le contexte lui-même.
La définition des situations repose typiquement sur la capacité de définir des expressions logiques et des méthodes d'inférences pour identifier des situations particulières.
Dans ce papier, nous étendons cette approche pour fournir une organisation et une sélection efficaces à des systèmes avec un très grand nombre de situations entretenant des relations structurées entre elles.
Nous appliquons les treillis de Gallois pour définir une relation de spécialisation sur les situations, et nous montrons comment le résultat peut être utilisé pour améliorer l'identification de situations utilisant les opérateurs du treillis et le raisonnement incertain.
La technique présentée est finalement validée sur un ensemble de données de taille réelle.
Essai de définition du statut et de la valeur du manuscrit d'auteur en moyen français à partir de l'étude archéologique et philologique des mss originaux de quatre textes de Christine de Pizan (Cent Ballades, Debat de deux amans, Mutacion de Fortune, Advision).
Trois étapes dans le processus de publication d'un texte sont envisagés (établissement du modèle ; transcription d'une copie d'envoi ; retour sur la transcription) afin de souligner que la transcription autographe, certes privilégiée, importe moins que le geste premier d'autorisation de la copie de la mise au net, rédigée et fréquemment relue par la main de l'auteur, et le geste final de visite du ms., visible dans les émendations du texte, les corrections rédactionnelles et l'ajout d'éléments de personnalisation du codex.
Enfin, l'article pose les enjeux méthodologiques et éditoriaux d'une telle étude (intention de l'auteur, choix du ms. de base et pertinence d'une étude esthétique et génétique de ces documents).
L'objectif de cet article est de présenter le calculateur embarqué de traitement d'image SYMPHONIE, ainsi que la méthodologie mise en oeuvre pour sa réalisation.
Afin de tenir compte des contraintes de volume et de consommation, un ASIC d'un million de portes a été développé.
Pour réussir cette réalisation un modèle VHDL a été écrit permettant des simulations de l'ensemble du système.
Par ailleurs, ce modèle a permis d'aborder la réalisation du circuit ASIC en faisant appel aux outils de synthèse VHDL.
Nous terminerons cette présentation par quelques performances dans différents domaines d'applications.
La plupart des théories de l'acquisition du langage supposent que celui qui apprend le langage peut segmenter la parole en propositions.
Dans l'étude que nous présentons, une procédure de préférence a été utilisée pour étudier si les enfants de 7–10 mois sont sensibles aux correlats acoustiques des propositions en anglais.
Les enfants s'orientent plus longuement vers les exemples segmentés aux frontières des propositions.
Une seconde expérience confirme que cette préférence dépend des endroits où sont insérées les pauses.
Ces résultats ont des implications importantes pour comprendre comment le langage est appris.
L'enfant prelinguistique possède apparemment les moyens de détecter des unités importantes telles que les propositions à l'intérieur desquelles les règles grammaticales s'appliquent.
Nous essayons de formuler une esquisse des propriétés de la voix humaine en parole continue.
Six niveaux différents sont pris en compte : (1) les données de référence pour un locuteur donné ; (2) les valeurs segmentales et les interactions source/conduit ; (3) la coarticulation des gestes glottiques et l'interpolation aux frontières ; (4) les dépendances de base sur F 0 ; (5) l'influence du stress, de l'accentuation et de l'intensité de la voix ; (6) l'enveloppe syntagmatique des variations de la source.
La paramétrisation des données de la source est fondée sur le modèle LF transformé et sur des correspondances dans le domaine fréquentiel (Fant, 1995), ce qui permet une puissance de spécification maximale avec un nombre limité de paramètres.
Cet article décrit une technique d'apprentissage automatique pour l'adaptation au locuteur et au canal téléphonique, basée sur la séparation des sources de variation spectrale de la parole, dans le contexte de reconnaissance de parole continue, indépendante du locuteur.
On propose des méthodes statistiques pour éliminer les biais spectraux au niveau acoustique et pour adapter les paramètres des mélanges de densités gaussiennes au niveau des segments phonétiques.
Le biais spectral est estimé en deux passes, en utilisant l'estimation, non-supervisée, du maximum de vraisemblance : on considère d'abord que la distribution de probabilité du contenu spectral est uniforme pour les canaux présentant une forte distorsion, puis le biais spectral est réestimé en utilisant les modèles gaussiens des segments.
La taille du vocabulaire testéétait de 853 mots ; la grammaire avait une perplexité de 105 ; les données de parole de test ont été enregistrées dans diverses conditions, chaque séquence de test contenant 198 phrases.
La réduction en deux passes du biais spectral a permis d'obtenir une réduction relative du taux d'erreur (RER) de 3 à 11% (suivant les locuteurs et les canaux) par rapport aux techniques conventionnelles de compensation cepstrale moyenne. La technique USPA donne une RER de 12 à 26% après la suppression en deux passes du biais ; la technique IPA fournit une RER supplémentaire de 8 à 19% après application de l'USPA.
La structure d'un système de synthèse vocale basée sur la proéminence comme paramètre central est présentée.
Une définition de la proéminence, adaptée à cette application, est donnée.
Comme validation empirique, la concordance entre les jugements de proéminence de différents auditeurs est déterminée.
Ces jugements sont mis en relation avec les données acoustiques de F 0 et durée.
Une relation linéaire entre valeurs de proéminence et données acoustiques est constatée.
Deux algorithmes, conçus à transformer les valeurs de proéminence en paramètres prosodiques sont décrits ainsi que leur évaluation.
L'application du paramètre proéminence à la synthèse des accents de focus est démontrée.
Ces résultats démontrent la validité de l'approche basée sur la proéminence comme interface entre linguistique et acoustique.
Dans le cadre de la simulation multi-agent, la dynamique globale d'un système, au niveau macroscopique, est considérée comme étant le fruit de la dynamique issue des interactions qui se déroulent entre les entités du niveau microscopique.
La manière dont un modèle de simulation multi-agent articule la dynamique de ces deux niveaux est donc fondamentale.
Dans cet article, nous revenons en détail sur l'intérêt du modèle influence/réaction de Ferber et Müller pour traiter cette problématique.
Nous montrons en quoi ce modèle constitue une solution intéressante et nous en proposons une variante mieux adaptée à la simulation : le modèle IRM4S.
Ce modèle explicite clairement le principe influence/réaction et met en exergue sa capacité à articuler la modélisation du niveau micro avec celle du niveau macro.
Les spécialistes du monde routier et des travaux public s'intéressent depuis longtemps aux aspérités présentes sur les chaussées.
Ces défauts de la route par rapport à une surface idéalement plane constituent ce que l'on appelle son uni.
L'analyseur de profil en long, l'APL, a été conçu par le Laboratoire Central des Ponts et Chaussées de Nantes afin de mesurer cet uni.
Le signal que délivre cet appareil peut être considéré comme la sortie d'un système linéaire dont l'entrée est le profil inconnu de la route.
Le signal que délivre cet appareil peut être considéré comme la sortie d'un système linéaire dont l'entrée est le profil de la route.
Il se pose donc le problème de la déconvolution du signal APL.
Dans un premier temps on modélise l'APL par une fonction de transfert du 5ème ordre construite à partir d'une description des différents organes mécaniques et électroniques qui le constituent et d'une analyse harmonique expérimentale.
La technique du double filtrage permet alors d'éliminer les distorsions de phases du signal APL et d'obtenir ainsi un pseudo profil qui ne diffère du profil exact que par des atténuations pour des fréquences extérieures à la bande passante de l'analyseur.
La deuxième approche utilise un modèle de l'APL obtenu par identification paramétrique (méthode du maximum de vraisemblance) et un modèle du profil du type signal de Wiener.
Après élimination des composantes polynomiales et basses fréquences le signal reconstruit suit fidèlement les variations de l'uni de la chaussée.
Des résultats, obtenus à partir d'enregistrements effectués au banc d'essais et sur une piste expérimentale, sont présentés.
Dans cet article, nous généralisons les relations entre les signaux de parole bruitée et non bruitée en utilisant une extension des séries de vecteurs de Taylor (VTS) pour la reconnaissance de la parole robuste au bruit.
Nous utilisons cette méthode tant pour la compensation des données bruitées que pour l'adaptation des paramètres des modèles de Markov cachés (HMM), et nous l'appliquons directement dans le domaine cepstral, alors que Moreno l'avait utilisée pour estimer les paramètres spectraux logarithmiques.
Nous développons également une procédure détaillée pour estimer les variables environnementales dans le domaine cepstral en utilisant les algorithmes de prédiction et de maximisation (EM) basés sur une idée de maximum de vraisemblance.
Des expériences de reconnaissance de mots isolés et de parole continue, en mode indépendant du locuteur, ont été menées pour évaluer la méthode.
Comme sources de bruit, on a utilisé des bruits blancs gaussiens et des bruits de voiture ajoutés à la parole propre à différents rapports signal sur bruit.
Des améliorations notables des performances ont été atteintes en utilisant des statistiques de bruit obtenues à partir de seulement trois échantillons de silence et de parole bruitée à reconnaı̂tre.
En particulier, l'adaptation des paramètres HMM avec la VTS est plus efficace que la combinaison de modèles parallèles (PCM) basée sur l'hypothèse log-normale.
Un égaliseur aveugle nouveau (HOCFLN) basé sur le cepstre d'ordre supérieur hybride (HOC) et un réseau à liens fonctionnels (FLN) est présenté.
Le système utilise initialement le cepstre complexe de la tranche 1-D des cumulants d'ordre quatre du signal reçu inconnu pour estimer partiellement les coefficients de l'égaliseur, puis il commute sur un égaliseur adaptatif FLN opérant en mode de conduite par décision (DDM) pour améliorer encore la convergence en erreur quadratique moyenne (MSE).
Dans ce système deux non-linéarités sont utilisées : l'une sur les données d'entrée pour lesquelles le HOC est employé et l'autre dans le filtre d'égalisation FLN.
Il est montré que dans le système nouveau HOCFLN des non-linéarités multiples produisent des améliorations de performances significatives pour une complexité de calcul réduite, particulièrement pour des distortions de canal importantes, vis-à-vis des algorithmes d'égalisation conventionnels.
Cette méthode peut prendre en compte aussi bien des canaux MA que ARMA à phase non minimale.
Des résultats pour des canaux donnant lieu à des changements abrupts de caractéristiques sont également présentés.
La description d'objets tridimensionnels indépendamment de leur position et de leur orientation est un problème important et difficile de l'analyse de formes.
Dans cet article, nous traitons ce problème à l'aide d'une pseudo-transformation de Fourier sur le groupe M(3) des déplacements de l'espace euclidien à trois dimensions.
Celle-ci nous permet de définir des descripteurs de volumes à niveaux de gris stables et invariants par rapport à M(3).
Cette méthode est appliquée à la classification et la description automatiques d'os humains.
On a essayé de réduire la qualité phonétique de voyelles à la position des pics dans leur spectre tonotipique par rapport aux autres pics, simultanés ou précédents.
Des voyelles synthétiques à deux formants ont été identifiées par des locuteurs dans les langues desquelles les distinctions entre voyelles hautes sont relativement riches (Suédois, Turc).
Les paramètres F 1 (204–801 Hz) et F′2 (509–3702 Hz) ont été modifiés de manitore systématique par intervalles de 0.75 bande critique.
La f0 a été maintenue légèrement au-dessous du F 1 dans toutes les voyelles.
Celles-ci ont été présentées sous deux ordres differents d'après le caractére ascendant ou descendant du F′2.
La plupart des sujets ont entendu des voyelles fermées de manière prédominante.
On propose d'attribuer ce second point de référence à une représentation par défaut de la position du troisième formant ou équivalent.
Nous définissions un indice acoustique lié au rendement vocal et nous étudions ses performances de discrimination sur un ensemble de locuteurs dysphoniques et normaux.
Cet indice de rendement vocal (IRV) est calculé à partir de la fonction d'autocorrélation d'une portion stationnaire de la voyelle du français /a/.
Au moyen d'une simulation nous monorons que l'indice en question est sensible à des variations du taux de charge et de la forme de l'onde glottique.
La simulation indique que l'IRV est d'autant plus faible que le quotient de fermeture de la glotte est petit et que la forme de l'onde est symétrique.
Nous calculons ensuite l'IRV à partir d'un ensemble d'échantillons de signaux dysphoniques et normales.
II permet de discriminer entre locuteurs normaux et dysphoniques et la classification qui en résulte peut être interprétée dans le cadre d'un modèle théorique [20].
Analyse d'une recherche centrée sur l'apprentissage du langage humain, développant des modéles mécanistes précis susceptibles, en principe, d'acquérir le langage à partir d'une exposition aux données linguistiques.
Une telle recherche comporte des théorémes (empruntés à la linguistique mathématique) des modéles informatiques pour l'acquisition du langage (empruntés à la simulation cognitive et à l'intelligence artificielle) des modéles d'acquisition de la grammaire transformationnelle (empruntés à la linguistique théorique).
On soutient que cette recherche repose étroitement sur les thèmes principaux de la psycholinguistique de développement et en particulier sur l'opposition nativisme-empirisme, sur le rôle des facteurs sémantiques et pragmatiques dans l'apprentissage du langage, sur le développement cognitif et l'importance du discours simplifié que les parents adressent aux enfants.
Cet article a pour projet de préciser la théorie du malentendu en la confrontant á l'organisation conversationnelle d'un malentendu apparu dans une séquence extraite d'un dialogue tutoriel.
L'analyse interlocutoire (Trognon et Brassac, 1992) de la séquence soutenant le malentendu, révéle qu'une organisation conversationnelle d'un malentendu peut prendre des formes complexes.
Dans la littérature l'organisation conversationnelle du malentendu repose sur une structure articulant trois éléments : T1, le contenu du tour de parole qui supporte le malentendu, T2, le contenu du tour de parole révélant le malentendu, et T3, le contenu du tour de parole résolvant le malentendu.
Dans notre séquence, ces trois moments (T1, T2 et T3) ne se succédent pas directement mais s'agencent dans une structure hiérarchique complexe.
Ces trois moments, plus précisément T2 et T3 se reproduisent plusieurs fois de suite.
Aussi notre séquence a pour singularité d'illustrer une résolution de malentendu sans travail sur l'intersubjectivité.
Trois vocabulaires (mots de contrôle d'un cockpit, chiffres, et consonnes et initiales) ont été comparés en vue d'évaluer des systèmes de reconnaissance de la parole.
Le but de cette étude est de comparer plusieurs méthodes d'évaluation orientées vers des applications pour contrôler des situations de laboratoire.
Nous avons observé que la discrimination entre plusieurs conditions de reconnaissance est améliorée par l'utilisation de vocabulaires difficiles.
Les confusions entre les stimuli et les réponses des mots de tests peuvent être utilisées comme un outil de diagnostic pour prédire les performances et les développements.
Dans cet article nous présentons des expériences réalisées au LIMSI-CNRS sur le développement et sur la portabilité d'un module de compréhension utilisant une méthode stochastique à d'autres langues humaines et d'autres applications.
Le travail de recherche concerne l'application ATIS (Air Travel Information Services) en anglais ainsi que l'application française MASK (Multimodal-Multimedia Automated Service Kiosk).
L'étude montre pour des applications limitées que, comparée à un module conventionnel de compréhension de la parole spontanée, une méthode stochastique fournit de meilleurs résultats.
De plus nous montrons que l'avantage d'une telle méthode réside dans le fait que l'effort humain se limite à l'étiquetage des données, ce qui est plus aisé que la conception, la maintenance et l'extension des règles de grammaire.
La méthode est donc relativement flexible et robuste.
Lorsqu'on trouve des problèmes dans les conversations, ce sont tout d'abord des problèmes collectifs et les participants doivent les co-gérer.
Les participants à une conversation disposent de trois sortes de stratégies pour les gérer.
(1) Ils essayent de prévenir les problèmes qui sont prévisibles mais évitables.
(2) Ils avertissent leurs associés contre les problèmes prévisibles mais inévitables.
(3) Ils réparent les problèmes qui se sont déjà révelés.
Alors, les interlocuteurs et leurs destinaires agissent en commun à trois niveaux de la parole. Ils coordonnent (1) l'articulation de l'interlocuteur à l'attention des destinaires. Ils coordonnent (2) la présentation d'une expression par l'interlocuteur à l'identification de cette expression par les destinaires. Et ils coordonnent (3) la signification signalée par l'interlocuteur à la compréhension de cette signification par les destinaires.
En somme, les participants à une conversation travaillent ensemble afin de prévenir, d'avertir et de réparer les problèmes à chacun de ces niveaux.
En plus, toutes choses égales par ailleurs, ils préfèrent les préventions aux avertissements, et les avertissements aux réparations.
Nos travaux concernent la qualité de la représentation et la détection des points atypiques en apprentissage supervisé.
Dans le cas où la variable à prédire est numérique - on parle alors d'apprentissage par la régression - nous avons proposé d'évaluer la qualité de ta représentation associée à un graphe de voisinage issu des prédicteurs à partir d'un coefficient d'autocorrélation de voisinage. Ce coefficient est construit sur le modèle du coefficient d'autocorrélation spatiale de Moran.
Poursuivant l'analogie avec l'analyse spatiale, nous proposons dans ce papier de décompo.ser ce coefficient en une somme des coefficients locaux associés à chaque exemple et de tracer le diagramme de dispersion de Moran afin de repérer les exemples pour lesquels la valeur de la variable à prédire est atypique.
L'expérimentation conduite sur diverses bases du site UCl Machine Learning a donné des résultats satisfaisants.
Le but de cet article est de donner un aperçu des progrès récents obtenus dans le domaine de la reconnaissance automatique de la parole.
Il traite essentiellement de la reconnaissance vocale, mais mentionne également les progrès réalisés dans d'autres domaines du Traitement Automatique de la Parole (Reconnaissance du Locuteur, Synthèse de Parole. Analyse et Codage), qui utilisent des méthodes voisines.
Ces méthodes ont conduit à des progrès obtenus concurremment suivant plusieurs axes, à des performances meilleures sur les vocabulaires difficiles, ou à des systèmes plus robustes.
Quelques matériels spécialisés sont également décrits, ainsi que les efforts qui ont été consentis dans le but d'évaluer la qualité des systèmes de reconnaissance.
Au cours des siècles, les Zaydites ont été amenés à répondre à une série de remises en cause – du dedans comme du dehors – dirigées contre la cohésion interne de leur tradition.
Remarquant que les Zaydites ne suivent généralement pas les opinions légales de leur ancêtre éponyme Zayd b. ʿAlī (m. 112/740), les critiques sunnites ont mis ceux-là au défi de justifier leur adoption du label « zaydite » .
La réponse classique fut avancée par l'imam yéménite al-Manṣūr ʿAbd Allāh b. Ḥamza (m. 614/1217), lequel expliqua l'affiliation zaydite en termes théologiques et politiques.
Au sein même du zaydisme, toutefois, des divergences opposant les principaux imams sur des questions légales causèrent des dissensions parmi leurs adeptes.
Afin de contrer cette menace à l'unité dans leurs rangs, les jurists zaydites adoptèrent largement la théorie selon laquelle tous les experts légalement qualifiés (muğtahids) – ce qui incluait les imams zaydites – étaient égaux dans leurs avis.
Plus techniques s'avérèrent les questions entourant le caractère de l'école juridique (maḏhab) devenue dominante chez les Zaydites yéménites.
Elles touchaient tant la source des opinions légales qui avaient façonné la doctrine de l'école que la question liée de la structure de l'autorité de l'école.
Ces questions, d'ordre historique et théorique, acquirent une acuité particulière à partir du xi e/xvii e siècle pour être popularisées grâce à la circulation du court poème d'Isḥāq b. Yūsuf (m. 1173/1760), ʿUqūd al-taškīk, lequel appelait les Zaydites yéménites à clarifier leur identité « légale » .
Mais c'est également la structure d'autorité de l'ensemble des maḏhabs sunnites qui se trouva interrogée, si bien que certains points soulevés ici relèvent de la loi islamique de manière plus générale.
Ce poème évoquait une palette de réponses, en prose et en vers, incluant un court traité de l'auteur du poème, al-Tafkīk li-ʿUqūd al-taškīk.
Alors que nombre de défendeurs s'efforcèrent d'affirmer la viabilité de l'école juridique, d'autres, notamment Ibn al-Amīr al-Ṣanʿānī (m. 1182/1769) et Muḥammad b. ʿAlī l-Šawkānī (d. 1250/1834), considé­rèrent qu'elle ne pouvait être sauvée.
Leurs objections à l'autorité légale traditionnelle (taqlīd), dans le cadre du zaydisme, furent largement reprises par les réformateurs musulmans des xix e et xx e siècles qui ambitionnaient de miner les maḏhabs sunnites et continuent, encore aujourd'hui, à jouir d'un réel crédit.
L'une des tâches majeures de l'auditeur lors du processus de compréhension de la parole est la segmentation du signal en mots.
Quand les conditions d'écoute sont mauvaises, les locuteurs peuvent aider ceux qui les écoutent en articulant clairement.
Nous avons examiné, au cours de quatre expériences, la production des frontières de mots dans la parole délibérément claire.
Des mesures de la durée des syllabes qui précèdent les frontières de mots, ainsi que celle de pauses éventuelles ont été faites dans une série de phrases prononcées normalement et dans ces mêmes phrases prononcées de manière particulièrement claire.
Nous avons trouvé que quand les locuteurs articulent très soigneusement, ils essaient effectivement de souligner les frontières de mots ; de plus, la manière dont ils s'y prennent suggère qu'ils sont sensibles aux besoins de ceux qui les écoutent.
En effet, des études précédentes ont suggéré que les Anglais emploient une stratégie de segmentation telle qu'ils repèrent plus aisément les frontières de mots précédant les syllabes accentuées.
Or, dans des conditions d'écoute difficiles, les locuteurs s'appliquent à souligner les frontières de mots précédant les syllabes faibles, c'est-à-dire celles-là mêmes quis seraient particulièrement difficiles à percevoir.
Les médianes pondérées avec des coefficients entiers utilisent largement les opérateurs non linéaires en traitement des signaux.
Les filtres à médianes pondérées appartiennent à la classe des filtres en pile.
Alors qu'un filtre en pile est uniquement spécifié par sa function positive et booléenne correspondante, cette dernière peut être utilisée pour analyser les caractéristiques du filtre.
D'un autre côté, les médianes pondérées sont habituellement représentées par un ensemble donné de poids.
Dans cet article, les médianes pondérées sont tout d'abord généralisées pour inclure des opérateurs utilisant des poids réels et positifs.
Il est montré que tout poids réel est équivalent à une valeur de poids entière.
En conséquence, une analyse précise peut toujours être réalisée pour représenter des poids médians réels avec une précision de poids médians finie.
Ce résultat s'avère très utile car les valeurs adaptatives et optimales des médianes pondérés sont souvent réelles.
Des algorithmes de conversion sont présentés pour trouver la fonction positive et booléenne représentant une médiane pondérée donnée et un ensemble de poids d'une médiane pondérée représentant une fonction donnée positive et booléenne si cela est possible.
L'algorithme d'itérations sur les valeurs avec approximations (IVA) permet de résoudre des problèmes de décision markoviens en grande dimension en approchant la fonction valeur optimale par une séquence de représentations V n calculées itérativement selon $ {V}_{n+1}=\mathcal{AT}{V}_n$ où $ \mathcal{T}$ est l'opérateur de Bellman et $ \mathcal{A}$ un opérateur d'approximation, ce dernier pouvant s'implémenter selon un algorithme d'apprentissage supervisé (AS).
Les résultats usuels établissent des bornes sur la performance de IVA en fonction de la norme L ∞ des erreurs d'approximation induites par l'algorithme d'AS.
Cependant, un algorithme d'AS résout généralement un problème de régression en minimisation une norme L p (p ≥ 1), rendant les majorations d'erreur en L ∞ inadéquates.
Dans cet article, nous étendons ces résultats de majoration à des normes L p pondérées.
Ceci permet d'exprimer les performances de l'algorithme IVA en fonction de la puissance d'approximation de l'algorithme d'AS, ce qui garantit la finesse et l'intérêt applicatif de ces bornes.
Nous illustrons numériquement la qualité des majorations obtenues pour un problème de remplacement optimal.
La sélection de variables et la régularisation sont deux classes de méthodes couramment employées pour améliorer les capacités de régularisation des réseaux connexionnistes.
Dans cet article, nous verrons comment l'utilisation d'une pénalisation sur la norme L1 des paramètres en tant que régulariseur permet de réaliser de façon simultanée une sélection des paramètres importants.
Cette méthode formellement à mi-chemin entre la sélection de variables classique et la régularisation type weight-decay permet en fait de bénéficier des avantages des deux techniques.
On obtient ainsi des modèles parcimonieux aux paramètres bien maîtrisés.
Nous illustrons tout d'abord brièvement quelques propriétés de ce régulariseur par une étude analytique d'un modèle simple à un seul paramètre.
On présente ensuite les résultats obtenus sur des réseaux de neurones pour la prédiction de séries chronologiques.
Enfin, on discute des aspects pratiques comme l'estimation des paramètres ainsi que des liens avec une méthode similaire utilisée en statistique pour la régression linéaire.
Le décodage acoustico-phonétique constitue une étape importante en reconnaissance de la parole continue.
Cet article rappelle d'abord les difficultés du problème et les principales méthodes qui ont été proposées pour le résoudre.
Il présente ensuite les diverses approches complémentaires adoptées par notre équipe : système expert fondé sur l'activité de lecture de spectrogrammes, reconnaissance par triplets phonétiques, modèle connexionniste de colonne corticale et reconnaissance par méthode stochastique sans segmentation.
Dans cet article, nous présentons quelques résultats sur le développement d'un système de mosaïquage de visages panoramiques.
Notre objectif est d'étudier la faisabilité de construction de visages panoramiques en temps réel.
Ceci nous a conduit tout d'abord à concevoir un système d'acquisition très simple, composé de 5 caméras standards qui réalise la prise de 5 vues simultanément sous différents angles.
Cet algorithme est basé sur des transformations linéaires successives, pour composer un visage panoramique de 150° à partir de ces 5 vues.
La méthode a été testée sur une centaine de visages.
Nous avons aussi effectué une étude préliminaire sur la reconnaissance de visages panoramiques dans le but de valider notre système de mosaïquage de visages.
Nous pensons aussi utiliser notre système de mosaïquage dans d'autres applications comme la reconstruction rapide de visages 3D et la catégorisation des expressions basée sur le mouvement.
Dans cet article, nous présentons une application de la théorie des fonctions de croyance pour la classification d'états physiologiques dans un bioprocédé.
Nous nous intéressons surtout à la pertinence des sources d'informations qui sont ici des paramètres biochimiques mesurés durant le procédé.
La théorie des fonctions de croyance, et plus particulièrement la notion de conflit est utilisée pour évaluer la pertinence de chaque source d'information.
Une autre mesure du conflit, basée sur une distance, est utilisée comme alternative, et fournit dans certains cas, des résultats plus cohérents qu'avec le conflit défini dans la théorie de Demspter et Shafer.
Les résultats concernant deux types de bioprocédés (procédé batch correspondant à une classification supervisée, et procédé fed-batch correspondant à une classification non supervisée) sont présentés.
Une nouvelle approche en vue de la transcription phonémique d'un énoncé oral est décrite.
Dans un premier temps, grâce à la “carte topologique auto-organisatrice de Kohonen”, nous quantifions vectoriellement le signal de parole pour obtenir une séquence de labels phonétiques tous les centièmes de seconde.
Cette séquence de codes est convertie en une séquence de phonèmes en utilisant un réseau multi-couche entraîné par rétropropagation de l'erreur.
Le réseau, une fois entraîné, agit comme un filtre qui enlève de la séquence de codes les transitions non désirées et les effets de coarticulation.
Ceci rend quasiment trivial la conversion de la séquence de codes en une séquence de phonèmes.
Dès lors, la nécessité d'utiliser des modèles statistiques de la parole tels que les modèles markoviens, disparaît.
La nouvelle approche est comparée à une méthode déjà utilisée dans un système de reconnaissance de la parole et qui se base sur de simples règles de durée.
Grâce à notre méthode, la précision de la transcription phonémique obteneu est de 4,8% supérieure à celle des méthodes habituelles (88,4% vs 83,6%).
Lorsqu'employée en reconnaissance des formes, la modélisation des mouvements humains vise, entre autres, à procurer certaines assises théoriques au traitement en ligne de l'écriture manuscrite et à fournir des connaissances fondamentales pouvant servir de balises dans la conception de systèmes automatiques.
À ce jour, plusieurs approches ont été proposées pour modéliser la production des mouvements en général et de l'écriture en particulier : des modèles dynamiques, psychophysiques, cinématiques, à base de réseaux de neurones ou reposant sur des principes de minimisation.
Parmi les modèles dits analytiques, la Théorie Cinématique et son équation delta-lognormale se sont avérées des plus prometteuses.
Mais, bien qu'il ait été démontré que ce paradigme permettait de prendre en compte la majorité des phénomènes couramment observés en motricité fine, plusieurs problèmes théoriques et techniques ont retardé l'intégration directe ou indirecte de cette théorie dans la conception de systèmes.
Dans cet article, nous faisons le point sur ces différentes difficultés et nous dévoilons les résultats de récents travaux que notre équipe a réalisés pour les surmonter.
Dans un premier temps, dans une perspective de généralisation, nous présentons les principaux modèles de types log-normaux.
Ensuite, du point de vue pratique, nous décrivons deux nouveaux algorithmes d'extraction de paramètres.
Nous montrons également comment la nouvelle représentation qui en résulte peut être employée pour améliorer le traitement des signaux électromyographiques, ouvrant ainsi la porte à de nouvelles applications en génie biomédical.
Nous concluons en présentant brièvement d'autres applications potentielles qui sont présentement en cours de développement dans notre laboratoire ou le seront dans un avenir rapproché.
Cet article étudie les possibilités offertes par l'utilisation du réseau neuronal de Kohonen pour la quantification vectorielle d'images.
Quelques résultats théoriques sur la convergence du processus d'apprentissage du réseau précéderont les essais réalisés sur des images.
On comparera d'abord, à l'aide d'une image test, les performances obtenues pour des dictionnaires de taille et de dimension différentes.
On poursuivra alors les essais avec les meilleures combinaisons, en utilisant finalement plusieurs images pour élaborer les dictionnaires.
Cette étude se particularise par l'emploi de cinq réseaux en parallèle, ainsi que par la variation de plusieurs paramètres, longueur de la séquence d'entraînement, nombre de classes de vecteurs, dimension des vecteurs, et nombre de vecteurs utilisés pour le codage.
Cet article décrit l'architecture et le fonctionnement du système de reconnaissance de la parole DIRA (DIRA : Dialogue Intégré et Reconnaissance Automatique) dans son état actuel.
Ce système est un système multi-experts supervisé.
Le superviseur organise les tâches de ses experts qui sont attachés aux diverses sources de connaissances : acoustico- phonétiques, lexicales, syntaxico-sémantiques, prosodiques et pragmatiques.
Le tableau noir sert de boîte à lettre pour la communication de messages entre les divers modules ainsi que de mémoire à long terme où toutes les hypothèses en cours de construction sont consignées.
Le superviseur est un planificateur opportuniste : il raisonne sur les données présentes dans le tableau noir et « calcule » la stratégie la meilleure pour activer les experts.
Les experts sont également décrits dans cet article : les DAP (décodages acoustico-phonétiques) avec leurs bases de connaissance représentées sous forme de règles qui contrôlent les transitions d'un ATN (Augmented Transition Network), les analyseurs linguistiques utilisant aussi le concept d'ATN compilé et la notion de grammaire lexicale fonctionnelle, la compréhension fondée essentiellement sur le phénomène d'amorçage sémantique et enfin l'analyseur prosodique à base de règles.
La mise en œuvre de ce système est commentée à travers des exemples et les résultats de reconnaissance sont discutés.
Nous présentons nos travaux sur la conception, l'ajustement et le recalage d'un modèle 3D articulé de la main pour son suivi dans des séquences d'images monoscopiques, sans recourir à des marqueurs.
Après un état de l'art, nous décrivons notre modèle 3D générique de la main et son ajustement à la morphologie de l'opérateur sur une image de sa main ouverte.
Nous comparons ensuite différentes fonctionnelles et méthodes d'optimisation pour le recalage.
Enfin, des résultats sur des séquences d'images sont présentés.
A terme, un tel système d'acquisition du geste par la vision artificielle pourrait être utilisé pour des interfaces homme-machine par le geste, pour l'animation d'humains virtuels, le codage à très bas débit du geste pour la téléprésence, ou la reconnaissance de la langue des signes.
Disposer de descriptions phonétiques précises de bases de données de parole est essentiel pour la recherche en traitement automatique de la parole : toutefois, la segmentation et l'étiquetage manuels sont des tâches longues et sujettes à erreurs.
Dans cet article, nous présentons une méthode de segmentation automatique de la parole. Etant donnée une transcription phonétique ou une transcription orthographique, notre système fournit la segmentation phonétique correspondante.
Cette méthode est basée sur l'utilisation d'un système de reconnaissance de parole utilisant les chaînes de Markov cachées (Hidden Markov Models, HMM) pour modéliser les unités acoustico-phonétiques. La base de données américaines DARPA-TIMIT a été utilisée pour l'apprentissage et le test du système.
Pour évaluer notre système,d es expériences de segmentation et de transcription phonétique ont été effectuées dans différentes conditions.
Des résultats satisfaisants ont été obtenus, en particulier lorsque le système est entrainé avec des signaux préalablement segmentés manuellement.
La taille de ce corpus d'apprentissage joue un rôle important : les performances du système ont été évaluées en fonction de ce paramètre.
Lorsque l'apprentissage est effectué avec seulement 256 phonétiquement équilibrées, le taux de frontières phonétiques correctement positionnées, avec une marge de 20 ms, est de 88.3%.
On suggère une nouvelle caractérisation qui s'appuie sur des données d'aphasiques agrammatiques parlant hébreux et sur le réexamen de données de patients russes et italiens.
Cette caractérisation est en relation avec les niveaux linguistiques de représentation.
On montre ensuite que la performance agrammatique dans des taˆches variées (y compris la compréhension) est une conséquence naturelle de cette condition.
Nous présentons, dans le cadre de la reconnaissance de la parole basée sur les phonèmes utilisant des modèles de Markov cachés (HMM, Hidden Markov Models) en conjonction avec des dictionnaires entraînés par quantification vectorielle supervisée (LVQ, Learning Vector Quantization), une nouvelle manière de modéliser les dépendances contextuelles.
Nous utilisons les LVQ pour transformer les données acoustiques contextuelles en une forme phonémique indépendante du contexte.
Cette transformation élimine le besoin d'utiliser des modèles de Markov dépendants du contexte, ainsi que les difficultés associées.
Au contraire, des modèles simplifiés indépendants du contexte des observations discrètes sont suffisants.
Nous avons obtenu par cette méthode d'excellents résultats pour une tâche dépendante du locuteur en Finnois.
Face à la nécessité de mieux comprende la variabilité du signal de parole, l'analyse des stratégies individuelles liées au processus d'encodage et de décodage de messages humains est fondamentale.
L'étude phonologique présentée ici, concerne la langue française.
Deux modules de règles de transformation graphème-phonème avec variantes du français ont été développés.
Les tests de ces modules ainsi que la typologie des erreurs rencontrées sont présentés.
L'étude des bases de données utilisées pour la création et les tests des règles a mis en lumière certaines exceptions à ces règles générales.
L'analyse de ces exceptions nous a permis de dégager une certaine régularité dans leur occurrence, de rechercher les causes qui peuvent leur être associées, et de prévoir des liens entre différents phénomènes phonologiques.
La parole est un code naturel de correction d'erreurs.
Le signal de la parole est riche en redondances contextuelles à de nombreux niveaux de représentation, qu'il s'agisse des variations allophoniques, de la phonotactique, de la structure syllabique, des domaines d'accentuation, de la morphologie, de la syntaxe, de la sémantique et de la pragmatique.
Les recherches psycho-linguistiques ont eu tendance à insister sur des contraintes à des niveaux élevés comme la sémantique et la pragmatique et ont en général laissé de côté l'utilité de contraintes de niveau inférieur comme la variation allophonique.
Il a même été dit que la variation allophonique est une source de confusion ou une sorte de bruit statistique qui rend la reconnaissance de la parole bien plus difficile qu'elle ne l'est déjà.
Par opposition à ces idées, je soutiendrai que l'aspiration, le “flapping”, la palatalisation et d'autres indices qui varient de manière systématique avec le contexte syllabique peuvent être utilisés pour analyser les syllabes et les domaines d'accentuation.
Ces constituants peuvent ensuite limiter le processus d'identification lexicale, de sorte que l'identification de la bonne entrée lexicale demandera bien moins de travail.
Je proposerai donc que la structure syllabique et les domaines d'accentuation constituent un niveau intermédiaire de représentation entre la description phonétique et le lexique.
Ma discussion est avant tout computationnelle et inclut la présentation d'un prototype de parseur phonétique qui a été implémenté en utilisant des mécanismes de parsing connus.
Aucun résultat expérimental ne sera présenté.
Cet article s'intéresse à la synthèse automatique d'agents en environnement incertain, se plaçant dans le cadre de l'apprentissage par renforcement, et plus précisément des processus de décision markoviens partiellement observables.
Les agents (dénués de modèle de leur environnement et de mémoire à court terme) sont confrontés à de multiples motivations/objectifs simultanés, problématique qui s'inscrit dans le domaine de la sélection d'action.
Nous proposons et évaluons différentes architectures de sélection d'action.
Elles ont en commun de combiner de manière adaptative des comportements de base déjà connus, en apprenant les réglages de la combinaison afin de maximiser les gains de l'agent.
La suite logique de ces travaux est d'automatiser la sélection et la conception des comportements de base eux-mêmes.
La visualisation de données multidimensionnelles est un problème important.
Nous proposons dans cet article d'utiliser l'image couleur pour obtenir une visualisation immédiate et synthétique des données initiales.
L'apport de la couleur permet d'exhiber les principales structures de ces données complexes.
Après avoir réduit la dimension du problème, notre méthode génére des pixels couleur en utilisant une transformation non triviale inspirée des travaux d'Ohta et al.
Une dernière étape de tri et d'arrangement de ces pixels dans une image nous permet alors de visualiser nos données multidimensionnelles sur une image couleur.
L'hypothèse due à Chistovich et au groupe de Léningrad selon laquelle il existerait dans le système auditif une intégration spectrale à large bande (3–3.5 Bark) n'est supportée par aucune preuve réelle, puisque toutes les données expérimentales peuvent être expliquées sans cette théorie.
Il nous semble en fait que l'existence d'un tel mécanisme ne peut être démontrée par un test d'ajustement, car si on demande à un sujet de remplacer un signal “complique” par un signal “plus simple”, il le fera, quel que soit le moyen qu'il utilisera.
Pour mettre ce mécanisme “simplificateur” en évidence, il faut essayer de trouver des “traces” de son existence à l'aide d'une expérience où on ne demande pas explicitement au sujet une telle simplification.
Nous sommes partis à la recherche d'une telle “trace de F 2 en mémoire” en soumettant nos sujets à la fois à un test d'identification et à un test de discrimination sur deux ensembles de voyelles synthétiques à 4 formants (antour des frontiéres phonétiques [i]-[y] et [e]-[ø] respectivement).
Nous avons obtenu les principaux résultats suivants :
• - les différences significantives observées dans les scores de discrimination ne peuvet être expliquées sans supposer l'existence d'une représentation in intégrée à large bande;
• - cette représentation intégrée fournit une trace en mémoire plus persistante que le classique representation en bandes critiques (1-Bark).
L'argument crucial repose sur le fait que des stimuli associés à des valeurs ambiguës de F 2 dans tests d'adjustement sont aussi associés à des traces ambiguës en mémoire — mises en évidence par un taux fortement accru de fausses alarmes - dans une expérience de discrimination.
Ces résultats nous ont fourni les “traces de F'2 en mémoire” que nous recherchions.
Dans cette approche, le MLP est entrainé pour la classification des phonèmes.
Les valeurs de sortie du MLP sont ensuite utilisées comme pondérations dépendantes des états, ce qui améliore les performances par rapport à des HMMs sans pondération dépendante des états.
Pour améliorer encore la discrimination entre classes concurrentes, l'apprentissage discriminant des pondérations dépendantes des états est effectué en calculant le gradient du critère d'optimisation pour les HMM pondérés par rapport aux paramètres des MLPs.
Pour la reconnaissance de parole continue en mode indépendant du locuteur, l'algorithme proposé réduit notablement le taux d'erreur par rapport à celui des HMMs conventionnels.
Cet article propose un modèle segmentai probabiliste reposant sur une représentation polynomiale du signal de parole.
A la différence du modèle probabiliste classique, opérant au niveau de la trame, ce modèle de segments regroupe les trames consécutives dont les caractéristiques sont semblables et représente l'ensemble du segment sur une base de polynômes orthogonaux.
Un algorithme itératif d'estimation des paramètres du modèle est proposé.
Le modèle segmentai est appliqué à la vérification du locuteur indépendante du texte.
Les tests ont été réalisés sur une base de données de 20 locuteurs.
La meilleure version du modèle permet d'obtenir un taux d'égale erreur de 0.59%, pour des énoncés de test composés de 10 chiffres.
Ceci correspond à une réduction relative du taux d'erreur de plus de 50%, par rapport au modèle probabiliste conventionnel, opérant au niveau de la trame.
Afin de classer les objets présents dans leur environnement, les robots mobiles sous-marins peuvent exploiter des informations sensorielles acquises séquentiellement (sonar).
Elles sont généralement qualifiées d'imparfaites, c'est-à-dire qu'elles sont imprécises, incertaines et incomplètes.
L'incomplétude est vue ici comme l'indisponibilité d'un jeu de paramètres rendant impossible le calcul des critères de classification qui en dépendent, retardant ainsi la prise de décision.
L'article propose de modéliser les informations dans le cadre de la théorie des possibilités, et d'appliquer le calcul flou afin d'évaluer des critères même en présence d'incomplétude.
Les résultats ainsi obtenus sont fusionnés séquentiellement par un processus de combinaison dissymétrique.
Les différentes lois de fusion dissymétrique sont passées en revue et une loi spécifique au traitement de l'incomplétude est proposée.
Cet article présente une étude comparée de différentes transformations utilisées pour calculer la section du conduit vocal à partir de la distance sagittale.
Les distances sagittales et les sections du conduit vocal ont été mesurées sur des coupes obtenues par Résonance Magnétique pour les voyelles orales du Français prononcées par deux locuteurs.
La section mesurée peut ainsi être comparée aux sections calculées au moyen des différentes transformations.
L'évaluation est réalisée au moyen d'une technique de “jackknife” : les paramètres de la transformation sont estimés pour une région du conduit vocal à partir de l'ensemble des données sauf une, qui permet ensuite d'évaluer la transformation.
Cette procédure permet d'étudier à la fois les performances des transformations et la stabilité des paramètres des transformations pour chaque région du conduit vocal.
Trois formes différentes de transformation ont été comparées : linéaire, polynomiale et exponentielle.
Les performances de quatre transformations existantes sont également présentées.
L'objet de cet article est de renouveler la compréhension du rôle joué par la théorie, comme par exemple la théorie du choix rationnel, dans la fabrication des décisions organisationnelles.
Nous approchons la prise de décision comme une praxis performative, c'est-à-dire un ensemble d'activités par lesquelles les acteurs produisant des décisions transforment des théories en réalité sociale.
Nous généralisons l'approche en termes de praxis performative, qui articule les processus de conventionnalisation, d'ingénierie et de marchandisation, pour rendre compte des formes de mobilisation directes et indirectes des représentations théoriques par les acteurs dans la fabrique de la décision organisationnelle.
Ce papier concerne le codage en sous-bandes par filtrage Pseudo-QMF des signaux d'images de télévision couleur.
Les méthodes proposées sont basées sur des codages prédictifs et des quantifications scalaires, la qualité visée des images reconstruites devant être excellente.
D'abord nous proposons et comparons un ensemble de trois fonctions de prédiction adaptatives.
Ensuite nous développons trois stratégies de quantification présentant des niveaux d'adaptativité différents.
Le niveau faible des programmes de go n'est pas seulement dû à la complexité combinatoire du go, mais aussi à la difficulté de construire une fonction d'évaluation adéquate et complète d'une position.
Cet article présente les nombreux concepts spatiaux que fait intervenir une fonction d'évaluation au go ; les principaux sont le regroupement, le fractionnement, l'encerclement, l'agrégation.
La programmation go est une excellente illustration des théories du raisonnement spatial en raison de cette richesse conceptuelle.
Pour chaque concept spatial, les outils mathématiques utilisés pour les simuler sur machine (morphologie mathématique, topologie, distance de Hausdorff, raisonnement spatial qualitatif) sont présentés.
Cet article s'appuie sur une démarche expérimentale, basée sur une validation informatique, qui a produit le programme Indigo classé sur l'échelle internationale des programmes de go.
Dans cet article, nous décrivons une méthode efficace d'obtention des classes de mots pour des modèles de langage.
Cette méthode emploie un algorithme d'échange qui utilise le critère d'amélioration de la perplexité.
Les contributions nouvelles apportées par ce travail concernent l'extension aux trigrammes du critère de perplexité de bigrammes de classes, la description d'une implémentation efficace pour accélérer le processus de regroupement, l'analyse détaillée de la complexité calculatoire, et, finalement, des résultats expérimentaux sur de grands corpus de textes de 1, 4, 39 et 241 millions de mots, incluant des exemples de classes de mots produites, de perplexités de corpus de test comparées aux modèles de langage de mots, et des résultats de reconnaissance de parole.
Dans cet article, nous présentons un réseau adaptatif de microphones avec des contraintes linéaires sur l'adaptation, destiné à la suppression des bruits cohérents et incohérents présents sur le signal de parole.
Nous utilisons une structure de type “generalized sidelobe canceller” (GSC) opérant dans le domaine fréquentiel ; cette structure permet d'obtenir séparément la suppression des bruits incohérents par ajustement adaptatif de la réponse dans la direction de visée et la suppression des bruits cohérents par ajustement des filtres adaptatifs du GSC.
La fonction de transfert dans la direction de visée est celle d'un filtre de Wiener adaptatif, estimé au moyen de la transformée de Fourier à court terme, suivant la méthode de Nutall-Carter pour l'estimation spectrale.
Les résultats expérimentaux montrent que la méthode proposée fonctionne bien dans une large gamme de temps de réverbération, et qu'elle est par conséquent capable d'opérer indépendamment des propriétés de corrélation du champ sonore.
Réaliser une aide visuelle pour l'apprentissage de la parole chez les mal entendants, impose de résoudre des problèmes très variés ayant trait à des domaines aussi différents que la phonétique, la reconnaissance automatique de la parole, la perception et la production de la parole les théories de l'apprentissage, l'ergonomie ou le développement et la programmation de systèmes informatiques.
Le succès d'une aide visuelle dépendra de la cohérence de la solution proposée.
La première partie de cet article décrit les aspects formels du problème et présente un relevé des diffucultés rencontrées lors des étapes successives de l'éboration du système d'aide.
La seconde partie décriot, en tenant compte des remarques formulées antérieurement, le Visual Speech Apparatus lui-même.
Les méthodes de modification décrites ici combinent des caractéristiques des méthodes basées sur le principe PSOLA et des algorithmes pour resynthétiser la parole basés uniquement sur le module de sa transformée de Fourier à court-terme.
Le point de départ est une représentation de Fourier à court terme du signal.
Pour effectuer des modifications de durée, des portions voisées correspondant à des périodes complètes sont supprimées ou insérées.
En ce qui concerne les modifications de fréquence fondamentale, des périodes sont raccourcies ou ralongées et un certain nombre de périodes respectivement supprimées ou insérées.
Comme il s'agit d'un outil important pour les modifications de durée et de fréquence fondamentale, la méthode de resynthèse de Griffin et Lim (1984) et Griffin et al. (1984), basée uniquement sur le module de la transformée de Fourier à court terme, est revue et adaptée.
Des méthodes de modification de durée et de fréquence fondamentale sont présentées ainsi que leurs résultats.
Lors de la numérisation de la transmission des radiocommunication vocoles, les conditions exigées pour les différentes composantes du système sont assez rigoureuses.
En particulier, il faut préter beaucoup d'attention au numérisateur de parole.
Le debit binaire applique à la transmission vocale influence essentiellement la largeur des bandes de fréquences utilisées pour les voies de transmission vocale et, de ce fait, l'économie de fréquence réalisée pour le système dans son ensemble.
Le débit binaire doit done être aussi faible que possible.
De plus, les influences pertubatrices du canal radioélectrique sur l'intelligibilité doivent être compensées par des codages appropries en vue d'assurer une protection contre les erreurs.
Ces deux exigences entraînent des structures de codeurs complexes, composées de blocs fonctionnels diférents qui se complètent.
La structure ici discutée réunit un algorithme de codage de source optimal donné par la Quantification de Vecteurs (VQ), avec un procédé multi-impulsionnel modifié.
Il est montré dans quels eas la VQ est utilisée pour la quantification de différents paramètres.
En particulier, on discute l'application de la VQ à des fonctions d'aire divisées en groupes.
Dans cet article nous présentons une revue de certains effets d'écoulement intervenant lors de la production des plosives bilabiales.
Comme support pour une analyse ultérieure, nous décrivons tout d'abord un dispositif expérimental destiné à la mesure in vivo.
L'ordre de grandeur de paramètres aérodynamiques ou géométriques est alors présenté.
Différents modèles théoriques d'écoulement sont ensuite discutés et évalués par rapport aux mesures obtenues sur des maquettes de lèvres ou par simulation numérique.
Cet article s'intéresse à la cartographie automatique des fonds marins en imagerie sonar haute résolution.
De nombreuses méthodes d'analyse de textures ont été développées jusqu'à présent, utilisant des approches statistiques, géométriques ou spectrales [14, 45, 7, 44].
Cependant, peu d'entre elles fournissent des attributs caractéristiques robustes vis-à-vis des rotations d'images.
Cette propriété est pourtant essentielle dans le cadre de l'étude proposée : elle a pour objectif de faciliter et d'améliorer l'apprentissage du classifieur.
Nous présentons dans cet article cinq méthodes de caractérisation, robustes vis-à-vis des rotations d'images.
La première méthode est une version étendue de la modélisation AutoRégressive (AR) circulaire initialement proposée par Kashyap et Khotanzad [19], en vue d'en extraire directement un nombre restreint de paramètres caractéristiques significatifs invariants en rotation.
Les quatre autres méthodes résultent d'une approche originale qui consiste à appliquer une méthode de traitement d'images à un ensemble de paramètres décrivant une texture, afin de le rendre robuste vis-à-vis des rotations d'images.
Les deux premières de ces méthodes consistent à appliquer la Transformation Log-Polaire respectivement aux paramètres issus d'une modélisation autorégressive 2D non- causale et aux paramètres de corrélation associés (nommés paramètres COR).
Quant aux deux dernières méthodes, elles consistent en l'application de la méthode des moments de Zernike respectivement à ces deux ensembles de descripteurs.
Des résultats de classification expérimentaux obtenus sur images réelles et sur images tournées artificiellement, ainsi que sur des textures issues de l'album de Brodatz, sont fournis pour souligner les performances de chacune des méthodes.
Nous présentons une architecture d'Hidden Control Neural Network (HCNN-CDF) dépendant du contexte pour la reconnaissance de la parole continue, basée sur les phonémes et les mots fonctionnels.
Le système peut être considéré comme une large extension du vocabulaire du système HCNN basé sur les mots, proposé par Levin.
Initialement, nous avons analysé les principes de modélisation de HCNN sous une forme indépendante du contexte, dans le cadre du système de reconnaissance de la parole Linked Predictive Neural Networks (LPNN) et avons trouvé qu'il aboutit à une augmentation de 6% dans la précision de reconnaissance de la parole à un degré de perplexité 402.
Comparé à LPNN, nous avons pu obtenir des réductions significatives dans les exigences de ressources et les charges computationnelles grâce à notre implémentation HCNN.
Dans des expériences de reconnaissance dépendant du locuteur, avec un degré de perplexité 111, les versions actuelles des systèmes LPNN et HCNN-CDF obtiennent respectivement une précision de reconnaissance de mots de 60% et 75%.
Entre la fin de la chaine des méthodes de codage de forme d'onde qui s'étend de la PCM linéaire jusqu'au codage adaptif et prédictif (APC) et celle des “vocoders” (en particulier : LPC), il existe un “trou de codage” de 32-2.4 kbit/s.
Cet espace définit actuellement le domaine des codeurs à “vitesse moyenne”.
Il est démontré, comment leuts faiblesses pourraient étre surmontées par des méthodes prédictives améliorées comme
Les idées et les problèmes fondamentaux de la quantification vectorielle (VQ) sont résumés brièvement.
Des combinaisons de méthodes VQ et d'autres ainsi que diverses combinaisons sont également discutées.
On sait que l'amplitude de crête négative est en forte corrélation avec le niveau de pression accoustique (SPL) de la parole.
Ainsi, la fonction entre d peak et SPL a été conventionnellement modelée en une seule ligne.
Dans cette étude, la linéarité de la fonction entre d peak et SPL est reverifiée en analysant les flux glottaux obtenus par filtrage inverse à partir de sons de parole d'une large gamme de différentes intensités.
On peut voir que les courbes d peak et SPL sont obtenues de manière bien plus précises en utilisant deux fonctions linéaires, la première pour les modèles de phonation douce, la seconde représentant les sons de parole normale et forte.
Pour toutes les courbes d peak et SPL analysées, l'inclinaison de la ligne modelée correspondant aux phonations douces est plus prononcée que celle de la ligne des sons de parole normale et forte.
Ce résultat suuggère que l'intensité vocale n'est pas seulement affectée par la valeur du domaine d'amplitude de la source vocale, d peak, mais aussi par la forme du flux glottal différentié à l'approche de la crête négative.
Cet article est consacré à l'emphase particle ou “prominence” de segments de la phrase japonaise.
Quatre groupes de 48 phrases lues par deux locuteurs et qui renferment différents types de prominences sont analysés.
Cette analyse, qui porte ainsi sur un total de 172 phrases, montre que dans 88% des phrases la “prominence” est générée par l'élévation de F 0 et l'augmentation de l'énergie.
Sauf dans certains cas exceptionnels, aucun exemple d'allongement des phonèmes n'a été observé sur les parties prominentes des phrases.
Une exception concerne l'allongement accompagné d'une pause insérée comme marque de prominence, une autre concerne le ralentissement du débit global.
Les caractéristiques prosodiques du discours lu naturellement sont ensuite utilisées pour élaborer des règles à partir desquelles les phrases de référence seront modifiées afin de générer un effet de prominence pour une synthèse de discours basé sur des règles.
D'après les résultats des tests d'écoute sur 10 sujets, la différence d'expressibilité entre une prominence synthétisée par des règl;es (pourcentage d'expressions correctes : 76,9%) et une prominence de discours naturel (79,9%) est non significative.
Pour augmenter l'expressibilité de prominence, on utilise des tests d'écoute sur 10 sujets afin de mettre en évidence dans quelles conditions l'expression de prominence est à son niveau optimal.
Ces tests ont montré que les paramètres de contrôle prosodique accroissent l'expressibilité d'environ 20%.
Finalement, les caractéristiques prosodiques du discours conversationnel spontané sont analysées et comparées à celle du discours de phrases lues.
Il apparait que sur les parties qui renferment une prominence, le ralentissement du débit est d'avantage évidente dans le cas d'un discours conversationnel spontané.
La variabilité interlocuteurs dans la coarticulation des voyelles /a,i,u/ a été examinée dans des mots artificiels contenant les consonnes /p,t,k,d,s,m,n,r/.
Ces mots ont été lus en isolation par 15 locuteurs néerlandais (m).
Les formants F 1–3 (en Barks) ont été extraits de la partie stable de chaque voyelle /a,i,u/.
La coarticulation dans chacune des 1200 réalisations des voyelles a été mesurée en F 1–3 en fonction du contexte consonantique, par une mesure statistique, nommée COART.
L'effect de coarticulation le plus grand a été trouvé sur /u/ : les consonnes nasales et alvéolaires en position C 1 y ont l'effect le plus grand sur les formants, plus spécifiquement sur F 2.
La coarticulation sur /a,u/ s'avère être spécifique du locuteur.
Pour ces voyelles, la variabilité par locuteur de la mesure COART était, généralement, la plus grande pour des valeurs de COART plus grandes.
Dans le contexte d'une tâche d'identification de locuteurs, COART n'améliore les résultats d'identification que si trois conditions sont combinées : (a) COART est additionné aux mesures des F 1–3 ; (b) les valeurs de COART pour la voyelle sont grandes ; (c) tous les contextes de voyelles sont combinés dans une seule analyse.
Les deux conclusions de cette expérience sont que le phénomène de coarticulation ne peut pas être étudié indépendamment de la variabilité interlocuteurs, et que COART peut contribuer à l'identification de locuteurs, mais seulement dans des conditions très restreintes.
Dans cet article, on s'intéresse au problème d'extraction de règles non redondantes sur un fond de sémantique basée sur la fermeture de connexion de Galois.
Cette approche présente un fort intérêt pour un système de visualisation de règles.
Les résultats des expérimentations menées sur des bases réelles montrent l'utilité de l'approche ainsi que de taux de réduction de la redondance fort intéressants.
Le bruit d'environnement dégrade de façon significative les performances de la plupart des systèmes actuels de reconnaissance automatique de la parole.
Cette dégradation provient principalement des différences entre les environnements d'apprentissage et d'utilisation d'un système.
Ces dernières années, de nombreux travaux ont porté sur la réduction de ces différences.
Une synthèse des résultats de ces recherches est présentée dans cet article, selon trois grandes catégories : les paramétrages résistant au bruit et les mesures de similarité, le débruitage de la parole, et la compensation des modèles en présence de bruit.
L'article met en évidence les points essentiels en reconnaissance de parole bruitée, à savoir l'utilisation des corrélations en temps et en fréquence du signal, l'augmentation de l'importance des portions du signal ayant un rapport S/B élevé lors de la décision, la prise en compte de connaissances spécifiques à la tâche sur le signal et sur le bruit, la mise en oeuvre de traitements dépendant des classes d'événements de la parole, et enfin l'utilisation des modèles auditifs.
On propose dans cet article une unité rythmique alternative à la syllabe : le groupe interperceptual-center (GIPC).
Ce groupe est délimité par des événements qui peuvent être caractérisés par des corrélats purement acoustiques parallèlement au décodage acoustico-phonétique (Pompino-Marschall, 1989).
Les patrons rythmiques du français sont ainsi décrits : la réalisation de l'accent est graduelle au long du groupe accentuel et cet allongement progressif de la durée est nécessaire à la perception.
Dans une série d'expériences d'identification, les effets de phase et d'amplitude sur la perception de consonnes plosives en environnement vocalique ont été étudiés.
Pour la première expérience ont été produits, à partir de stimuli VCV (voyelle, consonne, voyelle), trois types de stimuli : (1) les “Swapped” stimuli, qui ont le spectre d'amplitude d'un signal VCV et le spectre de phase d'un autre signal VCV ; (2) les “Phase-only” stimuli ; (3) les “Amplitude-only” stimuli.
Les expériences révèlent que la perception de consonnes plosives, en environnement vocalique, passe de la dominance d'amplitude à la dominance de la phase quand la taille de la fenêtre de l'analyse Fourier croı̂t.
Le passage de l'une à l'autre apparait pour des longueurs de fenêtres situées entre 192ms et 256ms.
Dans la deuxième expérience, on a examiné l'effet de phase pour des tailles de fenêtre plus petites.
Cette expérience montre qu'à partir d'un même spectre d'amplitude, mais avec spectres de phase différents, on peut construire différents sons. Donc le spectre d'amplitude obtenu avec une fenêtre d'analyse très petite ne permet pas toujours l'identification d'une consonne plosive.
Selon les deux expériences, la perception d'une plosive sonore semble dépendre fortement de l'information de phase, celle du lieu d'articulation semble dépendre surtout de l'information d'amplitude.
On présente un système de transmission de la parole à faible débit (environ 1000 bit/s), fondé sur le principe suivant;
le signal est analysé par prédiction linéaire, et on code séparément pour chaque fenêtre d'une part l'entrée (énergie, pitch, détection voisé/non-voisé), d'autre part les coefficients du filtre.
C'est sur ce dernier point que les études ont porte : on a comparé plusieurs espaces de représentation de ces coefficients (autocorrélation, cepstre, avant et aprés préaccentuation) afin de choisir celui qui se prête le mieux à une quantification vectorielle.
Celle-ci a été ensuite réalise par un algorithme simple, que nous avons comparé à la méthode “LBG”.
Nous montrons que dans les conditions expérimentales décrites, la simplicité de notre algorithme n'est pas pénalisante et qu'il présente de grands avantages pour constituer le dictionnaire de codage.
Une seconde partie de l'étude porte sur l'utilisation du dictionnaire ainsi obtenu;
il est calculé sur plusieurs locuteurs, pour un total d'environ 30000 fenêtres de parole (soit environ 6 minutes), et comporte plus de 1500 éléments.
Nous avons étudié une technique permettant d'obtenir un codage rapide d'un vecteur inconnu, en perdant l'optimalité du codage par rapport au dictionnaire.
Cependant, nous montrons que l'on augmente très peu la distorsion moyenne en utilisant une technique d'accès hiérarchique (obtenue par classification automatique descendante) : ceci permet de coder très rapidement un vecteur inconnu.
En conclusion, une méthode simple de construction du dictionnaire (associée à un bon choix de l'espace des vecteurs à quantifier), et une technique d'accès rapide à ce dictionnaire conduisent à un système aux performances peu dégradées par rapport aux références classiques en codage du spectre par quantification vectorielle.
Dans ce papier nous proposons une nouvelle technique de codage dans un espace transformé appelée “Transform Trellis Coded Quantization” (TTCQ).
Cette technique se base sur la quantification en codage par treillis proposée récemment où les treillis de modulation d'amplitude d'Ungerboeck et les idées de partition d'ensembles sont utilisés pour effectuer le codage de source.
Le gestionnaire d'appel (`CallManager') de Bellcore est un prototype expérimental de filtrage d'appels et de transfert d'appels ou numéro unique (`follow-me anywhere') qui utilise un gestionnaire de calendrier personnel et un répertoire téléphonique personnel (qui peuvent être accédés à partir de la ligne téléphonique de l'abonné ou de sa ligne radiotéléphonique ou cellulaire) pour déterminer où se trouve l'abonné au moment de l'accès et l'importance de chaque appelant pour l'abonné.
C'est un exemple d'une classe future de services centralisés qui utilisent des traitements de la parole avancés et des données spécifiques à l'abonné pour améliorer la valeur du réseau téléphonique auprès des abonnés.
De tels services permettent que les appels importants atteignent l'usager tout en filtrant les appels non désirés ou non importants.
Ces services peuvent potentiellement donner proactivement des informations à l'abonné `telles qu'elles arrivent' quelque soit l'endroit où l'abonné se trouve.
L'article s'intéresse à la façon dont Jean Lemaire de Belges développe une réflexion sur les enjeux de la pratique poétique et sur le rôle de l'auteur, à une période qui impose de prendre position face à l'héritage médiéval, qu'il s'agisse de celui de la poésie courtoise ou de l'ensemble des connaissances transmises par le Moyen Âge depuis l'Antiquité.
Dans les Épîtres de l'Amant vert, le rapport de dépendance qui lie le poète et sa protectrice le porte à aborder le poids de ces traditions sous la forme parodique de l'autodérision.
La portée politique et morale des Illustrations de Gaule et singularitez de Troye donne lieu à un questionnement sur la légitimité du discours poétique et de sa rhétorique affectés par leurs attaches avec la thématique amoureuse.
La Concorde des deux langages ouvre sur l'hypothèse d'une conciliation possible dans la persona de l'auteur, entre le poète, armé de la séduction de son éloquence, et le lettré soumis aux exigences du « labeur historien » .
Nous décrivons un modèle du processus de codage des mouvements de la membrane basilaire en activité nerveuse dans les fibres du nerf auditif.
Le modèle est constitué de deux parties : un modèle unidimensionnel des mouvements de la membrane basilaire et un modèle des récepteurs sensitifs.
La première partie est un modèle fonctionnel linéaire qui donne une image électrique des déplacements de la membrane basilaire.
La seconde partie est un modèle physique non linéaire qui repose essentiellement sur l'hypothése d'interaction électrique entre les informations issues des deux types de récepteurs sensitifs (cellules celiées).
On montre que le modéle est capable de reproduire l'excitation des fibres isolées du nerf auditif en réponse à des stimulations simples (entree mono- et bi-tonale) ou à des déplacements trapézoïdaux de la membrane basilaire.
Une deuxième série d'expériences est décrite pour montrer l'intérêt d'un tel modèle dans l'étude du codage des sons complexes.
Nous présentons des résultats pour un signal d'entrée de type parole, à un formant, avec une attention toute particulière au codage de la fréquence fondamentale dans les réponses de nos fibres simulées.
Nous présentons dans ce papier une illustration d'une tendance forte du traitement d'image actuel, qui, comptetenu des puissances de calculs disponibles, tend d'une part à privilégier les approches non-paramétriques sur les approches paramétriques, pour proposer des modèles moins contraints, plus souples, plus expressifs et, d'autre part, s'éloignant des descriptions linéarisées, essaie de replacer les objets d'études dans leurs structures géométriques naturelles.
Nous montrons comment la géométrisation du problème de l'appariement comme recherche d'un chemin optimal de déformation d'un objet source vers un objet but, permet de lier le problème de l'estimation d'un appariement à celui d'une géodésique dans un espace de formes.
Cet article présente les recherches récentes sur la reconnaissance de la parole aux Laboratoires de NTT (Nippon Telegraph and Telephone Corporation), depuis le traitement acoustique jusqu'au traitement linguistique.
Elles comprennent la proposition de paramètres hièrarchiques Δcepstraux et de paramètres ΔLSP, une nouvelle méthode d'utilisation de l'information apportée par la fréquence fondamentale, des techniques d'adaptation automatique au locuteur, de robustes modèles markoviens au niveau du phonème, de nouveaux algorithmes d'apprentissage pour réseaux neuronaux, un traitement linguistique utilisant des connaissances syntactiques et sémantiques, l'implémentation de systèmes prototypes de reconnaissance de la parole continue et un algorithme de reconnaissance du locuteur efficace et indépendant du texte.
Nous présentons une méthode géométrique utilisant l'orientation du gradient pour la détection de la signalisation verticale dans des images fixes, indépendamment de leur position et de leur orientation.
La détection est réalisée par une transformation de type accumulateur de Hough bivariée, fondée sur l'utilisation de paires de points avec des contraintes sur leurs gradients.
Les panneaux circulaires et polygonaux (non triangulaires) sont détectés par la transformation chinoise bilatérale TCB.
Cette transformation est rapide et ne fait pas de distinction entre les cercles et les polygones 4 côtés ou plus.
Les performances de la TCB et de la TSB sont estimées sur plusieurs bases d'images de scènes urbaines.
La plate-forme utilise simultanément un ensemble de méthodes de visualisation et d'apprentissage automatique permettant de découvrir de manière intuitive, interactive et rétroactive des connaissances.
La tâche de fouille est exprimée sous la forme d'un flot graphique de données où une méthode est représentée par un composant graphique de JavaBeans, la communication entre les méthodes se base sur un bus de données.
L'utilisateur peut créer et contrôler totalement le processus d'extraction de connaissances sans aucune programmation.
Notre plate-forme de programmation visuelle permet d'une part de réduire la complexité d'une tâche de fouille de données et, d'autre part, d'améliorer la qualité et la compréhensibilité des résultats obtenus.
Ce papier décrit une méthode à base de règles, guidée par les données, qui est destinée à modéliser les variations de prononciation en reconnaissance automatique de la parole (RAP).
Les différentes phases de cette méthode guidée par les données sont les suivantes.
Premièrement, les éventuelles variantes de prononciation sont générées en considérant que chaque phone de la transcription canonique peut être omis.
Ensuite, nous effectuons une reconnaissance forcée afin de déterminer quelle variante correspond la mieux au signal acoustique.
Enfin, les régles sont dérivées en alignant la meilleure variante avec la transcription canonique qui lui correspond.
Une analyse des erreurs commises est effectuée afin de mieux comprendre le processus de modèlisation de prononciation.
Cette analyse montre que, bien que cette modélisation apporte des améliorations, elle introduit également des détériations.
Une forte corrélation entre le nombre d'améliorations et le nombre de détériations par régle a été trouveé.
Ce résultat indique qu'il n'est pas possible d'améliorer les performances de la RAP en élimant les règles qui détériorent la reconnaissance pour certains candidats, étant donné que ces mêmes règles améliorent la reconnaissance de beaucoup d'autres candidats.
Enfin, nous avons comparé trois critères visant à sélectionner les meilleures règles.
La “fréquence absolute d'application d'une règle (F abs)” est apparue comme le meilleur critère.
Pour la meilleure condition de test, nous avons obtenu une réduction du taux d'erreur par mot statistiquement significative de 1.4% (taux absolu) ou de 8% (taux relatif).
Les Contemplations sur les sept heures de la Passion, rédigées par Jean Miélot en 1454 pour Philippe le Bon, duc de Bourgogne, sont conservées dans un manuscrit unique (Paris, BnF, fr. 12441) ;
la source principale de Miélot est la Meditatio de passione Christi per septem diei horas d'un auteur anonyme. La comparaison du texte français et de sa source permet de reconnaître les modifications que Miélot a introduites en adaptant son modèle pour un lecteur laïc.
Nous présentons, dans cet article, la conception et le développement du workbench MATE, un programme pour l'annotation de la parole et des textes écrits.
Il facilite la visualisation et l'édition flexibles de telles annotations, et permet des requêtes complexes sur un corpus résultant.
Le workbench offre une approche plus souple que la plupart des outils d'annotation, qui ont souvent été conçus pour un jeu d'étiquettes spécifique.
Le workbench MATE permet le traitement de tout jeu d'étiquettes pourvu que ce dernier puisse s'exprimer en format XML (rattaché au signal vocal, si disponible, en utilisant certaines conventions).
Le workbench utilise un langage de transformation pour définir les éditeurs spécialisés qui sont optimalisés pour des tâches d'annotation particulières avec des visualisations appropriées et des actions permises conçues en fonction de la tâche.
Le workbench est écrit dans le langage Java, ce qui le permet d'être indépendant de la plateforme.
Cet article décrit l'architecture du logiciel du workbench et le compare à d'autres outils d'annotation.
Une experience en deux parties sur les mouvements de protrusion de la lèvre inférieure pour la voyelle [u] a été menée afin d'étudier : (1) la production de 'creux' (diminution de la protrusion labiale devant la consome intervocalique dans des énoncés [uCu] ; (2) la fixation temporelle (début de mouvement à un intervalle fixe avant le début du voisement de [u] quel que soit le nombre de consonnes neutres dans des énoncés [Vnon arrondieC n u]).
Deux locuteurs français, un locuteur espagnol et deux locuteurs anglais ont produit des ensembles similaires d'énoncés dépourvus de sens.
Un bloc dentaire a été utilisé pour éliminer les mouvements mandibulaires, sources des mouvements de la lèvre inférieure.
Un examen détaillé a été conduit sur les relations etre les données à la fois sur le plan du mouvement et sur celui du signal pour tous les énoncés-tests.
Les résultats suggérent que la production des creux peut être due partiellement à des cibles articulatoires spécifiques pour les consonnes intervocaliques [s] et [t].
Des cibles consonantiques spécifiques empêchent l'investigation directe de la fixation temporelle.
Des limitations expérimentales préviennent des conclusions fermes mais la nature des résultats suggère qu'un ensemble de facteurs, incluant la coproduction des segments adjacents et des ajustements de cibles articulatoires dépendant du contexte, peuvent sous-tendre les variations observées dans le comportement articulatoire.
Cet article rend compte d'une série d'études sur quelques traits phonétiques de l'anglais américain représentés dans la base de données TIMIT.
D'abord nous décrivons certaines caractéristiques pertinentes de TIMIT, et comment nous utilisons les fichiers d'étiquetage du CD TIMIT avec une base de données commerciale.
Puis, nous décrivons deux études : l'une n'utilise que les informations non-audio de TIMIT (les transcriptions et durées segmentales et l'information sur les locuteurs), l'autre utilise le signal audio pour une analyse acoustique.
Les résultats de ce type d'étude sont utiles non seulement pour la phonétique linguistique mais aussi pour l'élaboration de lexiques pour la reconnaissance de la parole et la synthèse à partir du texte des systèmes de traduction texte-parole.
Nous présentons dans cet article un système de reconnaissance de caractères multifontes utilisant plusieurs classifieurs.
Chaque classifieur fournit une réponse puis le résultat final est obtenu par vote majoritaire.
Les classifieurs sont de deux types : stochastique et plus proche voisin.
Les classifieurs stochastiques sont des modèles de Markov cachés du premier et du second ordre.
La reconnaissance des caractères est suivie d'un module de vérification lexicale qui utilise un modèle de Markov caché pour les mots dont les paramètres sont déterminés à partir de statistiques sur la langue et d'un dictionnaire.
Dans cet exposé sur la modification de la structure temporelle du signal de parole, nous opterons pour l'utilisation des représentations temps–fréquence du signal, plutôt que pour des représentations par modèles.
Nous examinerons une méthode itérative permettant de reconstruire une fonction de phase pour spectrogrammes d'amplitude modifiés.
La recherche d'une bonne condition initiale pour démarrer l'itération nous ammènera aux méthodes de recouvrement-addition synchronisées et notamment à WSOLA, une technique basée sur un critère de ressemblance entre formes d'ondes.
Les émotions complexes dans des contextes réels ont encore été peu étudiées.
Dans ce papier, nous explorons comment représenter et automatiquement détecter le comportement émotionnel de sujets dans le contexte d'interactions orales Homme-Homme.
Par rapport aux nombreuses études précédentes conduites sur des données artificielles, ce papier montre les défis auxquels on doit faire face lorsqu'on étudie des émotions non basiques présentes dans des données réelles.
Des dialogues enregistrés dans des centres d'appels ont révélé la présence de nombreuses émotions mixtes.
Un vecteur d'émotions pondérées sert à représenter les émotions.
Cette représentation permet d'obtenir une annotation plus fiable et de sélectionner la partie du corpus sans émotions mixtes conflictuelles pour l'apprentissage des modèles.
Un taux de détection correcte de 80 % entre émotions Négative et Neutre a été obtenu avec des modèles SVMs et des arbres de décision, en utilisant des indices paralinguistiques sur un corpus de 20 heures enregistrées dans un centre d'appel médical.
Cet article décrit une méthode de gestion de pare-feux à partir de mise en œuvre de règles.
On analyse d'abord l'architecture de réseautique à base de règles (pbn) proposée par le groupe « Policy Framework » de l'ietf qui comporte des protocoles de communication, des langages de spécification de politique et la modélisation de l'information nécessaire.
On présente ensuite un état de l'art de l'application des langages de spécification de règles à l'architecturepbn en détaillant particulièrement la spécification des règles de sécurité avec le langagespsl.
Le protocolecops et sa variantecops-pr utilisés pour transporter l'information sur les règles sont également présentés.
La dernière partie de l'article est consacrée à l'application de l'architecturepbn à la gestion de pare-feux.
L'architecture proposée est alors analysée au travers de quelques exemples.
L'article se conclut en évaluant l'approche à base de règles dans la gestion des pare-feux.
Pour améliorer la perception des consonnes, on propose dans cet article une technique de traitement de la parole qui amplifie certaines parties du signal contenant une forte concentration d'indices acoustiques responsables de contrastes phonémiques.
La deuxième série de tests porte des phrases dépourvues de sens et traitées de la même façon.
Les énoncés (mots et phrases) ont été présentés aux sujets avec du bruit de fond.
Ces résultats montrent qu'il est avantageux d'exploiter la connaissance des indices acoustiques dans le développement de techniques de traitement de la parole visant à améliorer la perception de la parole dans le bruit.
Cette étude met en lumière un trope de la littérature apocalyptique musulmane resté jusque-là négligé, selon lequel, dans une région connue sous le nom d'al-Ṭālaqān, un grand trésor attendrait le futur Mahdī, trésor grâce auquel il s'assurerait une armée puissante qui l'aiderait à mener la bataille finale contre le Mal.
En faisant remonter ce trope à l'apocalyptisme zoroastrien et à sa dissémination ultérieure dans un large éventail de traditions apocalyptiques musulmanes, cet article soutient que ce trope apocalyptique fut finalement intégré au sein de l'apocalyptisme musulman, en particulier dans sa variante chiite, au cours d'une révolte zaydite menée par le Ḥasanide Yaḥyā b. ʿAbd Allāh contre les Abbassides en 176/792.
Nous explorons alors comment ladite révolte forgea la fonction du trope des « trésors d'al-Ṭālaqān » dans l'apocalyptisme musulman et comment la personnalité de Yaḥyā ainsi que la révolte qu'il inspira laissèrent une empreinte indélébile sur l'apocalyptisme imamite par la suite.
Le développement des objets datifs chez les enfants apprenant l'hébreu a été étudié.
Un grand corpus constitué des informations présentes dans le discours maternel a été examiné, et la catégorie du verbe datif s'est révélée extrêmement bancale par sa fréquence et variée du point de vue sémantique.
A travers l'analyse de 30 observations longitudinales le développement des enfants a été exploré.
Les tout premiers verbes produits avec un complément au datif étaient en effet des verbes noyaux.
La facilitation sémantique a démontré un apprentissage fondé sur le transfert.
Les effets de fréquence d'entrée des données et leur caractère obligatoire étaient beaucoup plus faibles pour la deuxième vague de verbes appris.
Les résultats montrent que deux processus ordonnés de façon séquentielle interviennent dans le développement d'une catégorie syntaxique qui recrée immédiatement sa structure : l'établissement du noyau et l'apprentissage- transfert de la périphérie basé sur la similarité.
Dans de nombreuses applications, où les données X E A3 à analyser représentent des objets auxquels sont affectés aléatoirement des labels binaires Y E {-1, +1}, l'enjeu de l'apprentissage statistique ne se borne pas à déterminer comment affecter le label le plus probable à un objet donné mais consiste à ranger l'ensemble des objets x E A dans le même ordre que celui induit par la probabilité a posteriori y (x) = P {Y = +1 | X = x}, les règles d'ordonnancement étant évaluées à l'aune de la courbe COR.
Contrairement à la plupart des approches utilisées en pratique, reposant sur une estimation préalable de la fonction y (x), les résultats récents décrits dans cet article permettent d'étendre la notion d'arbre de décision au problème de l'ordonnancement statistique et d'ajuster/combiner des règles de ce type de façon à optimiser la courbe COR directement.
Combinant les propriétés de la théorie des probabilités et de la théorie des graphes, les réseaux bayésiens ont acquis une popularité certaine durant la dernière décennie.
La détermination de la structure d'un modèle à partir d'une base de cas demeure cependant un problème délicat.
Nous avons développé un algorithme génétique déterminant une structure tout en s'affranchissant des limitations fréquemment imposées (nombre limité de parents par variable, connaissance d'un ordre total sur les variables).
L'algorithme parcourt l'espace des graphes orientés sans circuit et repère un ensemble d'op- tima locaux pour renvoyer le meilleur optimum local trouvé.
Cet article présente une nouvelle approche de l'analyse et de la synthèse paramétriques de la parole basée sur l'élaboration d'un modèle du résidu du filtre inverse de la prédiction linéaire dans le domaine fréquentiel.
Ce modèle fait intervenir la recherche d'une structure harmonique et de la période fondamentale, l'analyse en sous-bandes du signal pour la détermination de « proportions de voisement » de sorte à rendre la décision voisé-non voisé progressive et fonction de la fréquence.
Cette représentation paramétrique du signal résiduel est associée aux paramètres LPC habituels pour la transmission ou la mémorisation.
Le système ainsi créé présente la souplesse des systèmes paramétriques et est bien adapté aux problèmes de connexion entre trames intervenant dans les applications de synthèse à partir du texte.
Dans cet article, on décrit et on analyse des algorithmes pour la téléphonie mains-libres, qui utilisent un annuleur d'écho acoustique combiné avec un filtre RIF complémentaire dans la voie d'émission.
Nous décrivons deux méthodes différentes pour adapter le filtre complémentaire (appelé “filtre de mise en forme de l'écho”) ; ces deux méthodes sont très efficaces et faciles à mettre en oeuvre.
Nous montrons que les algorithmes proposés permettent de réduire significativement l'ordre de l'annuleur d'écho tout en apportant une atténuation élevée de l'écho et une faible distorsion de la parole locale pendant les phases de double parole.
Cet article présente les enjeux de l'ingénierie des connaissances dans le domaine des sciences du vivant et, à titre d'illustration, un système d'intégration de données thématiques ouvert sur le web, appelé ONDINE (Ontology based Data INtEgration).
Ce système propose un processus complet d'acquisition, d'annotation sémantique et d'interrogation de données à partir de tableaux trouvés dans des documents scientifiques issus du web.
Nous présentons le modèle de la RTO, la méthode d'annotation semi-automatique de tableaux de données guidée par cette RTO, puis le logiciel @Web (Annotating Tables from the Web) d'annotation sémantique de tableaux.
SIGNAL est un langage en cours de définition à l'IRISA dans le cadre d'un projet d'aide à l'implantation d'algorithmes en traitement temps réel du signal.
Les caractéristiques générales du langage sont inspirées des principes flots de données ; un programme SIGNAL décrit un réseau de processus (opérateurs arithmétiques ou temporels) interconnectés à travers des ports orientés, sur lesquels circulent des signaux. Ces réseaux sont construits progressivement à l'aide d'opérations permettant d'établir des liaisons entre les ports.
SIGNAL permet la description et l'exécution de tout algorithme temps réel synchrone mettant en œuvre éventuellement plusieurs horloges pour rythmer différents signaux.
La présentation est illustrée par la description d'un algorithme de prédiction linéaire en treillis.
Dans un témoignage unique, Apollonius Dyscole soutient que certains mots simples seraient formés à partir de mots composés privatifs, par suppression de l'ἀ privatif (ἀέκητι → ἕκητι).
L'examen de ce procédé par Apollonius s'avère paradoxal.
En effet, le grammairien assimile cette formation à un pathos (une altération formelle) ;
or une forme altérée, de l'aveu même d'Apollonius, doit toujours garder le sens de la forme de base, tandis qu'un mot simple signifie forcément le contraire de son composé privatif.
Cet article vise à montrer ce que cette position inattendue d'Apollonius, qui remet en cause le principe même de la pathologie, nous apprend du statut particulier du composé privatif et de sa place dans la théorie grammaticale ancienne.
La plupart d'entre eux ne fonctionnent bien que lorsque les séquences de parole de référence et de test sont enregistrées dans des conditions identiques d'ambiance non bruyante.
Le but de cette étude est de développer un système de vérification du locuteur basé sur un modèle de prédiction linéaire orthogonal, suivant une approche approche et n'utilisant qu'un jeu de paramètres de référence sans se préoccuper de savoir si le signal à tester est de haute qualité ou bruité.
Des seuils sont ensuite calculés pour déterminer les paramètres orthogonaux à prendre en compte dans le calcul de la distance et l'ordre du modèle pour un rapport S/B donné.
Notre travail a eu pour but d'étudier les effets de longuer de registre fini dans un système de reconnaissance de la parole par modèle caché de Markov (HMM).
Un modèle statistique a été employé pour la distribution approximative des différences entre des scores.
L'étendue des erreurs de reconnaissance attribuables à des erreurs de quantification des paramèters du HMM a été calculée au moyen de ce modèle statistique.
La relation entre le taux de reconnaissance et l'erreur de quantification est ensuite calculée.
Ceci détermine la longueur de registre nécessaire pour la réalisation matérielle du système de reconnaissance par HMM.
Dans cet article nous présentons un nouveau système de reconnaissance de textes manuscrits scripts en mode omni-scripteur.
Ce système utilise un lexique français de très grande taille (200 000 mots), qui couvre de nombreux champs d'application.
Le processus de reconnaissance repose sur le modèle d'activation- vérification proposé en psychologie perceptive.
Un ensemble d'experts code le signal d'entrée et extrait des informations probabilistes à différents niveaux d'abstraction (géométrique, morphologique).
Nous avons expérimenté plusieurs stratégies d'adaptation non supervisée au scripteur.
La meilleure, appelée « adaptation non-supervisée dynamique » agit en continu sur les paramètres du système.
Elle permet d'atteindre des performances proches de l'une adaptation supervisée.
Les performances, évaluées sur une base de données comportant 90 textes (5400 mots) écrits par 38 utilisateurs différents, sont très encourageantes car elles atteignent un taux de reconnaissance de 90%.
Ce papier décrit les résultats récents en modélisation de l'onde glottique pour la synthèse de parole.
Nous décrivons plus spécifiquement deux méthodes de modélisation de l'onde glottique, que nous appliquons á des problèmes de conversion de voix.
Un des modèles utilise une modélisation polynomiale (pour modéliser la dérivée de l'onde de débit à l'intérieur d'une période fondamentale).
Les coefficients de ce polynome forment un vecteur qui est ensuite utilisé pour apprendre un dictionnaire de formes de références d'excitation glottiques, comprenant 32 entrées (pour les segments voisés).
Ce dictionnaire est appris sur deux phrases, prononcées par des locuteurs différents.
L'excitation dans les segments non-voisés est représentée à l'aide d'un dictionnaire de formes d'onde de bruit comprenant 256 références.
Nous décrivons dans cette contribution les techniques d'analyse pour apprendre les deux dictionnaires.
Le synthétiseur GELP permet d'obtenir une très bonne qualité de parole. Elle permet au chercheur de disposer d'un système simple, aux qualités et aux défauts universellement reconnus, capables de reproduire tous les sons du signal de parole, et disposant d'une excitation paramétrique directement reliée á la dérivée de l'onde de débit glottique et à l'intégrale du résidu de prédiction.
Nous proposons aussi une autre approche fondée sur l'utilisation du modèle de Liljencrants-Fant (LF) d'onde glottique pour modéliser les caractéristiques de différents modes de phonation (tendu, chuchotté,…).
Nous proposons de transformer le mode de phonation en utilisant le synthétiseur GELP, en altérant l'onde de débit glottique sans modifier les caractéristiques du filtre de synthèse.
L'authentification automatique du locuteur a été le sujet d'actives recherches durant de nombreuses années, et malgrés ces efforts et des résultats prometteurs en laboratoire, le niveau de performance sur le réseau téléphonique reste inférieur au niveau requis pour de nombreuses applications.
L'étude expérimentale, dont les principaux résultats sont présentés dans cette article, avait pour objectif de quantifier en dehors de toute application l'influence de facteurs plus ou moins reconnus pour leur effet sur les performances des systèmes d'authentification du locuteur.
Les questions addressées sont : le choix du modèle (mélange de gaussiennes ou modèle phonétique) ; la connaissance ou non du texte prononcé par le locuteur ; l'importance de la quantité et de la nature des données d'apprentissage et d'authentification, en particulier l'influence du contenu linguistique des énoncés sur le niveau de performance pour des textes lus et de la parole spontanée ; la dégradation des résultats due au vieillisssement des modèles et la manière de le compenser avec des techniques d'adaptation.
Les résultats experimentaux ont été obtenus sur un corpus téléphonique conçu et enregistré pour cette étude qui comprend plus de 250 heures de parole pour un total de 100 locuteurs abonnés et 1000 imposteurs.
Sur ces données le taux d'égale erreur est de l'ordre de 1% dans le mode dépendant du texte lorsque deux essais sont autorisés par tentative d'authentification avec une durée minimale de 1,5 s par essai.
Les réseaux bayésiens de modélisation de la croyance constituent un outil efficace pour combiner différentes sources d'information ayant différents degrés d'incertitude. Ils sont rigoureux mathématiquement et peuvent être implémentés de façon efficace.
Ils sont malheureusement peu utilisés en traitement de la parole.
Cet article montre comment ils peuvent être appliqués à ce domaine.
Nous donnons aussi un algorithme, inspiré de l'algorithme EM, qui permet l'apprentissage de matrices de liens à partir d'exemples.
A l'aide de ces extensions, nous construisons un modèle de langage à base “context-free” pour la reconnaissance de la parole, dans lequel les phrases sont analysées localement, en segments judicieusement choisis, au lieu d'être analysées dans leur totalité.
Ce modèle a été évalué sur une base de données textuelles.
Sa performance, mesurée par l'entropie de l'ensemble de test, est au minimum égale à celle des modèles bi-ou tri-grammes ; nous avons aussi constaté une bonne capacité de généralisation des données d'apprentissage vers les données de test.
Nous proposons dans cet article une approche de fusion probabiliste reposant sur des critères entropiques dont le but est de réduire l'espace de combinaison en représentant explicitement les notions de redondance et de complémentarité des sources d'information.
Ce type de modélisation est en particulier intéressant pour optimiser le choix des mesures, issues des sources d'information, à combiner dans un système de fusion.
Il est en accord avec le souci de rapidité de traitement et de minimisation des ressources matérielles qui se pose en fusion d'informations.
Pour repondre à cela, nous avons réalisé une étude de la parallélisation de l'algorithme de fusion entropique développé en vue de son implantation parallèle dans le cadre d'une application en robotique mobile.
La spécification de l'algorithme faisant apparaître du parallélisme potentiel est ensuite implantée sur un réseau de stations de travail fonctionnant en mode MIMD-NORMA à l'aide des environnements de programmation SynDEx, qui supporte la méthodologie AA-A, et PVM, qui est de type CSP de Hoare.
Inspirés par des observations neurobiologiques et psychoacoustiques, nous avons construit un réseau de neurones pour la reconnaissance de mots isolés.
Un prétraitement du signal de parole biologiquement motivé transforme la fréquence en bandes critiques et la puissance en sonie, contraste les spectrogrammes et extrait les indices temporels et spectraux.
Nous montrons que les différentes étapes du prétraitement augmentent le taux de reconnaissance d'une manière significative et sont essentielles pour réaliser une reconnaissance sans erreurs d'un petit vocabulaire.
De plus, le réseau est capable, sans aucune modification de l'architecture du système, de reconnaître des mots prononcés simultanément.
Ceci représente une nouvelle approche pour résoudre l'un des problèmes les plus difficiles, celui de la séparation du signal de parole du fond sans utiliser des techniques conventionnelles telles la séparation directionelle de la parole enregistrée en stéréophonie ou le suivi de la fréquence fondamentale.
Dans ce papier, nous présentons la problématique de la reconnaissance d'images détériorées et plus particulièrement l'étape de sélection de primitives au sein d'un traitement de classification supervisée.
Cette étape de sélection a lieu après que la segmentation et l'extraction des descripteurs statistiques sur des images documentaires aient été réalisées.
Nous exposons en détail l'utilisation d'un arbre de décision, afin de l'harmoniser puis la comparer avec une approche moins étudiée utilisant un treillis de Galois.
Cet article décrit l'approche d'IBM pour la transcription des nouvelles du journal télévisé.
Les problèmes caractéristiques de cette tâche sont la segmentation, la classification automatique, la modélisation acoustique, l'estimation de modèles de langage et l'adaptation des modèles acoustiques.
Cet article présente de nouveaux algorithmes pour chacun de ces problèmes.
Parmi les idées clés, on peut citer le critère d'information Bayesien (pour la segmentation, la classification automatique et la modélisation acoustique) et l'apprentissage adapté pour chaque locuteur/classe de locuteurs.
On a enseigné à des enfants aphasiques ayant des déficits de langage sérieux, à communiquer par l'intermédiare du système de symboles visuels mis au point par D. Premack pour les chimpanzés.
Les sujets, qui ne possedaient pas de langage normal, ont rapidement pu de cette façon exprimer plusieurs fonctions du langage (mot, phrase, concept de classe, intérrogation, négation).
On peut s'intérroger sur le statut linguistique du 'système Premack' dont on se rend sans doute mieux compte comme d'un système de communication.
En conséquence, il est possible alors de penser que les enfants aphasiques manquent d'une capacité spécifiquement linguistique.
Dans cet article, nous appliquons une méthode générale d'apprentissage par renforcement pour la mise au point automatique de comportements de personnages non joueurs d'un jeu vidéo de tir à la première personne, Counter-Strike©.
Le résultat de l'apprentissage est un ensemble d'arbres de décision représentant de façon lisible un modèle du problème et la politique de décision des personnages.
Enfin, nous discutons de la portée de notre méthode pour la réalisation d'architectures de décision pour les personnages non joueurs de jeux vidéo.
Quand il rencontre un “mot inconnu”, c'est-à-dire absent de son répertoire, un système traditionnel de synthèse de parole à partir du texte active des règles de transcription orthographigue-phonétiques contextuelles pour produire une prononciation.
Une hypothese proposée par les sciences psychologiques est que le locuteur humain prononce les mots inconnus sans utiliser de règles explicites, mais plutôt par analogie avec des correspondances lettres-phonèmes dans des mots qu'ils connaît déjà.
Dans cet article, on présente un système de synthèse par analogie, qui est donc également un modèle de la prononciation des mots inconnus par un locuteur humain.
L'usage de l'analogie est proposé aussi bien au niveau orthographique que phonétique, et s'applique ici à la prononciation de mots inconnus en Anglais et en Allemand.
Le développement du système a permis de rencontrer certains points de détail auxquels la théorie de l'analogie ne s'est pas révélée pour le moment capable de répondre.
C'est pourquoi une grande partie de ce travail traite des conséquences des choix d'implémentation sur les performances, c'est-à-dire la capacité du système à produire des prononciations conformes à celles données par les humains.
Il faut aussi prendre en compte la taille et le contenu de la base de donnée lexicale sur laquelle tout système par analogie doit s'appuyer.
Les meilleures des implémentations ont fourni des résultats utiles pour les deux langues citées.
Mais les meilleurs résultats ont été obtenu pour chaque langue avec des implémentations assez différentes.
Dans cet article, nous analysons la possibilité de réduire le nombre de niveaux de lissage d'un filtre LUM (lower-upper-middle) adaptatif 3-D basé sur un contrôle par seuils fixes (FTC = fixed threshold control).
Outre son excellente capacité d'atténuation du bruit tout en assurant la conservation des détails, le filtre LUM FTC avec une fenêtre de taille N, est caractérisé par une structure relativement complexe, où l'estimation de la valeur de sortie est faite en fonction de (N + 1)/2 règles de décision.
Ceci peut entraver l'implémentation matérielle de tels filtres dans des applications vidéo temps réel.
Afin de simplifier la complexité du filtre tout en gardant ses excellentes performances, nous proposons deux approches qui sont la réduction linéaire du nombre de niveaux de lissage et la réduction optimale basée sur un algorithme génétique.
Bien que la plupart des paramètres dans un système de reconnaissance de la parole soient estimés à partie des données en utilisant une fonction objective, l'inventaire des unités acoustiques et le lexique sont généralement créés à la main, et donc susceptibles de ne pas être optimeux.
Cette étude propose une solution conjointe aux problèmes interdépendants que sont l'apprentissage à partir des données d'un inventaire des unités acoustiques et du lexique correspondant.
Nous avons testé l'algorithme proposé sur des échantillons lus, en reconnaissance indépendantes du locuteur avec un vocabulaire de 1k : il surpassé les systèmes phonétiques en faible ou forte complexité.
Nous proposons une nouvelle méthode formulée au niveau région pour la segmentation texturale d'images sonar haute résolution.
Nous caractérisons les différents types de fonds marins par des descripteurs de texture sous forme de distributions de leurs réponses à un ensemble de filtres, estimées sur la globalité des régions et nous définissons une nouvelle mesure de similarité adaptée à la discrimination entre fonds marins dans l'espace de ces descripteurs.
Notre mesure de similarité est une somme doublement pondérée de divergences de Kullback-Leibler entre les descripteurs de textures : la première pondération permet la sélection des filtres les plus pertinents pour la discrimination entre textures et la deuxième pondération est angulaire et elle permet de tenir compte de la variation des descripteurs de texture en fonction des angles d'incidence.
Le premier est un terme qui évalue l'homogénéité des régions selon la mesure de similarité pondérée entre les statistiques estimées sur les différentes régions de l'image et les prototypes relatifs aux différents types de fonds marins.
Le deuxième terme contraint la régularité des frontières entre régions.
La minimisation de la fonctionnelle est effectuée par descente du gradient et exploite les outils de dérivation de forme et la méthode est implantée selon la technique des ensembles de niveaux.
Un modèle de Markov caché (HMM) à maximum de vraisemblance pour la reconnaissance de mots isolés peut être traité comme un réseau neuronal récurrent.
Les unités dans la boucle de récurrence sont linéaires mais les observations entrent dans la boucle via une multiplication.
L'apprentissage peut utiliser la rétro-propagation des dérivées partielles pour maximiser une mesure de la discrimination entre les mots.
La rétro-propagation a exactement la même forme que la 'Backward Pass' de l'algorithme de Baum-Welch (EM) pour l'apprentissage des HMM à maximum de vraisemblance.
Il est intéressant d'observer que l'utilisation d'un critère particulier d'erreur basé sur l'entropie relative (équivalent au critère d'information mutuelle utilisé pour l'apprentissage discriminant des HMM) peut mener à des dérivées qui sont liées aux réestimations selon l'algorithme de Baum-Welch et à l'apprentissage correctif.
Nous proposons une méthode morphologique de segmentation d'images couleur de cytologie.
Cette méthode est basée sur la ligne de partage des eaux utilisant une fonction de potentiel couleur combinant informations locale et globale.
Cette méthode de segmentation utilise des informations a priori pour élaborer l'utilisation de la méthode.
L'article s'articule autour de trois parties.
Dans une première partie, nous rappellerons tout d'abord la structure d'une segmentation morphologique couleur.
Dans une dernière partie nous verrons une illustration de la méthode de segmentation sur des images de la cytologie des séreuses.
Le domaine d'application du système de reconnaissance de parole et de dialogue EVAR est celui des renseignements sur les horaires de train.
Nous avons constaté que dans les dialogues réels de personne à personne, la personne qui cherche une information interrompt souvent l'agent lorsque celui-ci communique l'information.
La plupart de ces interruptions sont des répétitions des horaires indiqués par l'agent.
Le rôle fonctionnel de ces interruptions n'est souvent déterminé que par des indices prosodiques.
Un résultat essentiel des expériences réalisées avec des personnes naïves est qu'il leur est difficile de suivre les informations horaires données par EVAR lorsque celles-ci sont délivrées en synthèse de parole.
Il est alors encore plus important que l'usager puisse intervenir lors de la réponse.
Nous avons donc élargi le module de dialogue pour donner à l'usager la possibilité de répéter les horaires et nous avons ajouté un module prosodique qui commande la poursuite du dialogue en analysant l'intonation de cet énoncé de l'usager.
Les modèles markoviens (HMM) sont devenus l'approche prédominante dans les systèmes de reconnaissance de la parole.
SPHINX est un exemple d'un tel système ; développé à CMU, il offre les caractéristiques suivantes : vaste vocabulaire, indépendance du locuteur et parole continue.
Dans cet article, nous présentons les techniques de modélisation des HMM, nous analysons les raisons de leurs succès et nous décrivons quelques améliorations par rapport aux HMM standards utilisés dans SPHINX.
Nous avons analysé des phrases produites spontanément au cours d'interviews semi-standardisées par dix patients souffrant d'une démence sénile modérée de type Alzheimer, cinq aphasiques de Wernicke et cinq sujets contrôles âgés mais sans atteinte cérébrale.
L'analyse a révélé chez les deux groupes de patients une réduction de la longueur des phrases, mais l'absence de symptomes paragrammaticaux systématiques chez les déments séniles.
Dans les productions des deux types de patients, l'usage des noms était dénoncée de façon relativement sélective, alors que la capacité de trouver les mots était étonnamment préservée chez les Alzheimer.
Les deux groupes ont montré des déficits marqués, mais des comportements pathologiques différents, dans leurs réponses aux questions de l'examinateur.
Les résultats sont interprétés dans le cadre d'un modèle neurolinguistique de production du langage.
Nous pensons que le processus de formulation peut être préservé chez les déments séniles, mais qu'il est atteint dans l'aphasie.
Les désordres du langage observés chez les déments séniles résulteraient de troubles pré-linguistiques dans la formation de la structure conceptuelle de l'acte de parole.
Les modèles articulatoires de la production de la parole sont, en général, contrôles par des paramètres cinématiques qui gouvernent les positions et les mouvements des tissues et de l'air.
Nous presentons afin de complèter cette approche un modèle de la glotte à quatre paramètres d'un type similaire.
Les paramètres sont des quotients sans dimension qui contrôlent la forme statique et dynamique de la glotte.
Le modèle peut simuler simultanément avec les mêmes paramétres, la vitesse volumique, l'aire glottique et la surface de contact entre les cordes vocales.
Les propriétés de ces formes d'ondes sont diccutées en termes de configuration glottique sous-jacente à leur production.
En particulier, une description détaillée de la forme d'onde de la surface de contact est donnée.
Le modéle est destiné à être appliqué à l'imagerie du mouvement des cordes vocales reposant sur des signaux captés de maniére non invasive à la surface du corps.
Dans le contexte de la synthèse, le modèle constitute une alternative à la modélisation du bébit parce qu'il inclut l'interaction entre la source et le conduit sans réclamer beaucoup plus de calcul.
Les psycholinguistes s'efforcent de construire un modèle du traitement de langage humain en général.
Mais ceci n'implique pas qu'ils doivent limiter leurs recherches aux aspects universels de la structure linguistique et éviter les recherches sur les phénomènes spécifiques à telle ou telle langue.
Tout d'abord, mêmes les caractéristiques universelles de la structure du langage ne peuvent être observées de façon précise qu'en étudiant plusieurs langues.
Ce point est illustré dans cet article par des travaux sur le rôle de la syllabe dans la reconnaissance de mots parlés, sur le traitement perceptif des voyelles par rapport aux consonnes et sur la contribution des phénomènes d'assimilation phonétique à l'identification des phonèmes.
Dans chaque cas, ce n'est qu'en étudiant les phénomènes de façon comparative entre les langues que l'on peut en comprendre le principe général.
D'autre part, les traitements spécifiques à chaque langue peuvent certainement aider à identifier un modèle universel de la compréhension du langage.
On illustre ici ce second point par des études sur l'exploitation de l'harmonie vocalique dans le segmentation lexicale en Finois, sur la reconnaissance de mots en Néerlandais avec et sans épenthèse et sur la contribution des différents types de structure prosodique lexicale (ton, accent intonatif, accent lexical) à 1'activation initiale des mots candidats dans l'accès au lexique.
Dans chaque cas, les aspects du modèle universel de traitement sont révélés par l'analyse d'effets spécifiques à chaque langue.
En résumé, l'étude du traitement du langage parlé par l'être humain requiert des comparaisons inter-langues.
Cette recherche a évalué deux explications alternatives des déficits de la compréhension syntaxique chez les aphasiques agrammatiques : l'explication par une perte syntaxique (Grodzinsky 1986) et l'hypothèse d'une déterioration de la mémoire de travail (Kolk et van Grunsven 1985).
Quatre patients aphasiques présentant différentes formes de déterioration morphologique et structurelle de la production ont été examinés.
Des tâchcs de compréhension ont comparé la performance sur des phrases passives entiéres ou tronquées.
L'hypothèse de perte syntaxique prédisait un moins bon résultant avec les phrases passives tronquées qu'avec les passives entières alors que l'hypothèse du déficit de mémoire active prédisait l'inverse.
Aucune des deux hypothéses ne fut vérifée car les résultats obtenus par les patients étaient les mêmes sur ces deux types de phrases passives.
En outre, il y avait peu de rapport entre les indicateurs de production des patients et leur niveau de compréhension.
Ces résultats vont à l'encontre de toute théorie globale de l'agrammatisme qui tenterait d'attribuer la même origine au discours agrammatique et aux déficits de compréhension qui peuvent l'accompagner.
Dans cet article nous présentons une étude des mesures objectives de qualité pour une vaste gamme de systèmes de codage.
Ces mesures prennent en compte les distorsions linéaires et non linéaires des codeurs.
Une analyse de corrélation a été effectuée afin de repérer les mesures qui prédisent le mieux les indices perceptifs de la qualité de la parole.
Finalement, nous décrivons le signal test que nous avons utilisé dans notre étude.
Cet article rentre dans le cadre de la recherche d'information (RI) flexible.
Une technique de reformulation de requête par réinjection de pertinence graduelle est présentée.
L'utilisateur donne des préférences graduelles aux documents préalablement sélectionnés par le système.
Ainsi il pourra formuler son jugement par « tel document est moyennement pertinent » ou « très pertinent » .
Lors de la conception de systèmes interactifs multimodaux le concepteur se trouve confronté au problème du choix des modalités pour favoriser l'utilisabilité du système.
En s'appuyant sur une modélisation à un haut niveau d'abstraction, nous proposons une évaluation en phase de conception sur un principe multicritère.
L'évaluation s'intéresse simultanément à plusieurs points de vue, pondérés en fonction de l'environnement et de la nature de la tâche.
Elle repose sur des mesures estimant des adéquations entre les éléments impliqués dans l'interaction.
Ces mesures sont rendues possibles notamment par une décomposition fine des modalités d'interaction.
Nous discutons de la normalisation du ton et de son application à des données sur la fréquence fondamentale de sept locuteurs parlant une variété du chinois Wu.
Nous dérivons une représentation phonétique en vue d'une comparaison avec d'autres variétés du chinois.
L'annulation d'écho acoustique utilisant un filtre adaptatif transverse et l'algorithme de l'erreur carrée moyenne minimale (LMS) est la plus efficace pour réduire les échos acoustiques d'un téléphone mains libres.
Cependant, il est nécessaire d'utiliser un filtre d'ordre très élevé pour chaque microphone, ce qui induit des problèmes de convergence et d'implantation matérielle.
Nous étudions dans cet article les performances des filtres adaptatifs de longueur finie.
Nous établissons une formule reliant l'annulation d'écho à la taille du filtre.
Une analyse détaillée montre que les performances d'un filtre de longueur finie sont meilleures sur de la parole que sur du bruit blanc.
L'emploi des méthodes du deuxième ordre de traitement du signal pour l'analyse spatiale est limité par la corrélation existante entre les sources.
Celles-ci doivent rayonner des signaux indépendants afin de satisfaire les exigences de ces méthodes et permettre un haut pouvoir résolvant.
La méthode proposée ici, parvient à augmenter le rang d'une matrice contenant les vecteurs susceptibles de reformer la base du sous-espace formé par les sources.
Ceci agit comme une décorrélation des sources.
L'avantage de cette méthode est qu'elle continue à séparer les sources totalement corrélées avec un haut pouvoir résolvant contrairement à la populaire diversité d'espace.
Cet article montre comment l'algorithme DEESE se sert de l'invariance en translation puis, de l'invariance en direction dans le but de récupérer une base orthonormale du sous-espace source.
Des simulations réalistes sont aussi présentées pour confirmer la bonne estimation de la position des sources et la conservation du pouvoir résolvant.
Cet article rend compte d'une comparaison des performances de deux techniques récentes de codage de la parole : le codeur excité par impulsions régulièrement espacées et à boucle de prédiction à long terme (RPE-LTP) et le codeur à prédiction linéare excité par code (CELP) dans le cas d'un canal de transmission bruité, dans des conditions d'erreurs aléatoires et en rafales.
En simulation, le flux de bits généré par chaque codeur est altéré par des suites d''erreurs provenant d'un modèle d'erreurs en rafales.
Le modèle d'erreurs est composé de deux parties : un générateur de l'envelope des évanouissements (fading) de Rayleigh et un modèle de réceptuer à modulation de phase différentielle à M états dans lequel le niveau du signal varie suivant l'enveloppe de fading de Rayleigh.
Les performances de chaque codeur sont évaluées à l'aide de tests objectifs et subjectifs.
Le résultat des simulations indique que le codeur RPE-LTP offre des performances plus stables dans un environnement de transmission en rafales.
Quatre techniques de codage du canal spécifique au codeur CELP sont également comparées.
Il n'y a pas de contradiction majeure entre le grand plaidoyer d'Henri Ey pour un « corps psychique » ou un « devenir conscient » et ce que peuvent déployer comme efforts, en ce moment, les théoriciens cognitivistes.
L'organodynamisme pourrait aussi bien parrainer ces tentatives que ces dernières s'en réclament.
Cela peut présenter des avantages et enrichir les deux domaines, en maintenant tendu le fil de l'histoire.
Des données précises sur les caractéristiques de la source glottique sont importantes pour analyser et synthétiser la parole.
On a utilisé ici une méthode temporelle de filtrage inverse pour analyser les voix de 4 hommes et de 4 femmes.
14 mono-syllabes ont été prononcées par chacun des locuteurs, (10 avec une voix normale, 2 avec une voix soufflée, 2 avec une voix laryngalisée) puis filtrées par un ensemble de zéros correspondant aux formants.
On a effectué des mesures temporelles et fréquentielles de l'onde de débit différenciée ainsi obtenue : près du début du voisement, du centre de la voyelle et de la fin du voisement.
Bien qu'on ait constaté de la variabilité parmi les sujets du même sexe, les femmes ont des quotients de fermeture plus courts et des périodes plus longues entre le maximum de l'excitation et le minimum du débit.
Dans le domaine spectral, on a trouvé moins d'énergie pour les femmes que pour les hommes dans les hautes fréquences près du centre de la voyelle.
Ces différences hommes-femmes sont comparables à celle s observées pour les différences entre voix laryngalisées et normales et entre voix soufflées et normales.
Ces différences qualitatives peuvent être liées à une combinaison de facteurs biologiques, sociaux et acoustiques.
L'article présente un type de réseaux neuro-mimétiques appelé « réseaux d'yprels » .
Après avoir rappelé quelques caractéristiques essentielles de ces réseaux, on précise la méthode d'apprentissage incrémental permettant d'améliorer les performances par reprise des erreurs de classification.
Les résultats obtenus pour un problème de reconnaissance de caractères sont alors présentés.
Dans la volumineuse correspondance de Timothée I, Catholicos de l'Église orientale, deux lettres renvoient à sa collaboration à la traduction des Topiques d'Aristote en syriaque et en arabe, commandée par le Calife al-Mahdī.
On trouvera ici la traduction annotée en anglais de ces deux lettres.
Dans cet article nous présentons un modèle permettant de caractériser les gestes à l'aide d'une série de paramètres d'expressivité, puis de générer des gestes expressifs pour les Agents Conversationnels Animés, l'objectif étant d'augmenter la crédibilité des agents et le naturel de leurs comportements.
Nous présentons ensuite deux tests perceptifs destinés à évaluer expérimentalement notre modèle.
De plus, il apparaît nécessaire d'étudier de manière plus approfondie les effets d'interaction entre paramètres.
L'objet de cet article est de proposer une synthèse des applications musicales du traitement de signal, des problématiques de recherche qui leur sont liées et des directions prospectives qui se dégagent sur la base de travaux récents dans ce domaine.
Après l'exposé de notions préliminaires, relatives au système technique musical et à l'analyse des différentes représentations numériques des informations musicales, cette synthèse se concentre sur trois types de fonctions principales : la synthèse et le traitement des sons musicaux, la spatialisation sonore et les technologies d'indexation et d'accès.
Le Perceval imprimé en 1530, qui réunit sous un titre unique la réfection en prose de cinq œuvres rattachées au Conte du Graal de Chrétien de Troyes, est fondé sur un manuscrit aujourd'hui perdu.
Le but de cet article est de vérifier, à partir d'un sondage mené sur la totalité du texte, les rapports avec l'un ou l'autre des manuscrits conservés des romans en vers.
Si aucun de ceux-ci ne constitue « la » source du prosateur, l'adaptation de 1530 peut apporter des lumières sur la tradition des poèmes, voire offrir des leçons qui pourraient remonter aux « originaux » .
Cet article propose de dresser un panorama des outils et techniques mis en place pour la reconnaissance d'images de documents.
L'expérience a démontré que la mise en oeuvre d'une gestion efficace des connaissances passe par la mise en place d'une cartographie des connaissances.
Ainsi, la cartographie des connaissances est un moyen de navigation « cognitif » pour accéder aux ressources d'un patrimoine de connaissances d'une organisation, qu'il soit implicite ou explicite.
De plus, elle permet d'avoir une compréhension fine, par une analyse de criticité, des domaines de connaissances sur lesquels des efforts doivent être faits en terme de capitalisation, partage ou innovation.
Nous présentons dans cet article une méthodologie et des outils mis en œuvre pour bâtir une telle cartographie.
Cet article décrit une procédure de segmentation automatique de bases de données de mots isolés, du type de celles utilisées classiquement pour la synthèse de la parole.
Les unités phonémiques issues de la transcription sont représentées par des modèles de Markov spécifiques et la segmentation est obtenue en alignant le signal de parole avec la séquence des modèles de Markov représentant la transcription phonétique des mots.
L'avantage spécifique de la méthode présentée ici est qu'elle ne doit pas recourir à une base de donnée segmentée manuellement pour initialiser l'apprentissage des modèles de Markov.
Dès lors, cette procédure peut être considérée comme une amélioration d'une technique connue pour la segmentation automatique.
Le problème de l'initialisation des modèles de Markov en l'absence de matériel acoustique préalablement segmenté à la main est résolu ici par une approche hiérarchique en trois étapes.
La première réalise une segmentation en grandes classes phonétiques, de façon à obtenir des points d'ancrage pour la deuxième étape qui consiste en une quantification vectorielle avec contrainte de séquence.
Cette deuxième étape poursuit le travail en segmentant chaque grande classe phonétique suivant les phonèmes qui la composent.
Le résultat est une segmentation phonétique grossière qui est à son tour utilisée pour initialiser les modèles de Markov.
Le réglage fin des modèles de Markov est achevé au moyen d'estimations Baum-Welch.
La segmentation définitive est obtenue au moyen d'un alignement Viterbi des signaux de parole avec les modèles de Markov.
La représentation générique d'un ensemble de données que nous utilisons ici est un treillis de Galois, c'est-à-dire un treillis correspondant aupartitionnement des termes d'un langage en classes d'équivalence relativement à leur extension (l'extension d'un terme est la partie d'un ensemble d'instances qui satisfait ce terme).
Pour réduire la taille du treillis, nous proposons ici de simplifier la représentation des données, tout en conservant la structure formelle de treillis de Galois.
Pour cela nous utilisons une partition préliminaire des données correspondant à l'association d'un type à chaque instance.
En redéfinissant la notion d'extension d'un terme de manière à tenir compte, à un certain degré a, de cette partition, nous aboutissons à des treillis de Galois particuliers appelés treillis de Galois Alpha.
Dans cet article, nous présentons une solution au problème de l'estimation spectrale nonlinéaire pour la restauration de la qualité d'un signal vocal bruité.
La méthode est fondée sur un simple modèle statistique pour l'estimation spectrale court-terme de la parole et du bruit.
La génération des données empiriques et l'emploi de méthodes d'approximation de courbes permettent de dériver des expressions (explicites et simples) de la relation entre le niveau d'entrée de l'estimateur MMSE et les paramètres du modèle pour chaque fréquence.
Le principal avantage de notre technique est qu'elle a une bonne fondation théorique, qu'elle est générale par le choix de ses paramètres et qu'elle est presque aussi simple que la soustraction spectrale classique.
De plus, l'emploi d'un réseau de neurones pour l'approximation de fonctions (qui s'avère être la meilleure méthode pour notre problème d'approximation de courbes) permet de réaliser d'autres estimateurs MMSE basés sur des modèles statistiques avec l'approche proposée.
Nous évaluons ici les performances d'un estimateur de la L2-mesure de dépendance probabiliste en vue de la réduction de dimension bidimensionnelle linéaire dans le cas multiclasse.
Nous comparons l'algorithme proposé d'une part, à l'analyse discriminante linéaire introduite par Fisher et, d'autre part, à une version généralisée au cas multiclasse se basant sur l'extracteur linéaire récursif de la L2-mesure de dépendance probabiliste.
Dans le cas non gaussien, cette évaluation sera faite au sens de l'erreur des к plus proches voisins.
L'estimateur à noyau des densités de probabilité est calculé dans le contexte du paramètre de lissage optimisé au sens de la moyenne quadratique intégrée.
Ce dernier servira à l'estimation de la probabilité d'erreur de classification des mélanges de vecteurs gaussiens.
Nous montrons sur un exemple de bases d'images de visages l'intérêt du réducteur de dimension proposé relativement aux méthodes conventionnelles.
La plupart des biologistes et certains cognitivistes sont arrivés indépendamment à la conclusion que l'apprentissage, au sens instructiviste et traditionnel du terme, n'existait pas.
Cette thèse peut sembler extrémiste, mais je la défend ici à la lumière de données et de théories provenant d'une part de la biologie, en particulier de la théorie de l'évolution et de l'immunologie, et d'autre part, de la grammaire générative moderne.
Je souligne également que la chute de l'apprentissage est incontestée dans les sciences biologiques, alors qu'un consensus similaire n'a pas encore été atteint en psychologie ni en linguistique.
Puisque de nombreux arguments offerts à l'heure actuelle en faveur de l'apprentissage et d'une capacité d' “intelligence générale” s'appuient souvent sur une image déformée de l'évolution humaine, je dévouc quelques sections de cet article à une critique de l' “adaptationnisme”, en donnant également les éléments d'une meilleure théorie de l'évolution (fondée sur i' “exaptation”).
De plus, certains arguments en psychologie et en intelligence artificielle présentés aujourd'hui comme indubitables sont en fait une réplique exacte des anciens arguments en faveur de l'instruction et contre la sélection en biologie ; je m'appuie sur ces erreurs du passé en tirant des leçons pour le présent et le futur.
Dans cet article, nous proposons deux formes d'hybridation entre une métaheuristique (Optimisation par Colonie de Fourmis ou Algorithme Génétique) et une méthode exacte (Programmation Linéaire en Nombres Entiers) pour la résolution du problème d'ordonnancement de voitures.
Nous examinons dans quelles mesures ce type d'hybridation peut améliorer la qualité des solutions obtenues par rapport à ce que l'on pourrait obtenir avec une métaheuristique utilisant une procédure de recherche locale.
Les résultats obtenus montrent que l'utilisation d'approches hybrides apporte une amélioration significative de la performance et que l'efficacité de l'heuristique ou de la métaheuristique sur laquelle se base l'hybridation influence aussi grandement la qualité des solutions.
Nous concluons que les mécanismes de diversification d'une métaheuristique à base de populations jouent un rôle important dans l'hybridation.
La description phonético-acoustique de la diphtongue s'appuie encore de nos jours (1) sur les deux plus basses fréquences de résonance (ou formants F 1 et F 2) du conduit vocal, considérées individuellement ou conjointement dans le plan F 1−F 2, et (2) sur une représentation très éparse du contour de ces fréquences.
S'il est vrai que cette approche de longue date a facilité la caractérisation des voyelles initiale et finale de la diphtongue, l'état de la recherche semble avoir très peu évolué au-delà de l'espace planaire F 1−F 2, en tant que cadre paramétrique permettant l'élucidation de la nature dynamique de la transition vocalique.
En revanche, nous avons pu obtenir, à l'aide d'un échantillonnage temporel détaillé des trois formants F 1, F 2 et F 3, une description spectro-temporelle plus précise d'un sous-ensemble des diphtongues de l'anglais australien ().
Il en ressort surtout la mise en évidence de certains traits de non-linéarité du contour de F 3, qui semblent avoir été jusqu'ici inconnus ou considérés sans conséquence pour la spécification de la diphtongue.
Cette découverte permet de dégager, dans l'espace tridimensionel F 1−F 2−F 3, une perspective nouvelle sur la nature acoustique de la transition vocalique propre à la diphtongue.
Dans l'histoire des théories énonciatives du XX e siècle, Charles Bally est l'auteur de la première « théorie générale de l'énonciation » , bien que son nom ait été rarement évoqué par les théoriciens de ce champ.
Dans le cadre de la réflexion sur l'articulation entre représentations et opérations chez plusieurs linguistes de l'énonciation, dans un rapport plus ou moins étroit avec une image saussurienne, nous envisageons d'éclairer les points les plus révélateurs qui ont conduit Bally de la stylistique à la théorie de l'énonciation, en poursuivant un fil qui va de la distinction entre « impressions » et « idées pures » (Précis 1905) à l'opposition affectif/intellectuel (Traité 1909), puis au traitement de l'expressivité linguistique en tant que mécanisme (Le langage et la vie 1926) pour arriver à la théorie de l'énonciation, où il définit la modalité.
Dans ce parcours, nous évoquons son interprétation originale de concepts et de conceptualités saussuriennes, ainsi que la position de Saussure à l'égard du statut de l'affectif dans la linguistique.
On rend compte de l'évolution de l'accentuation des mots dissyllabiques en anglais - accentuation de type trochaïque (fort-faible) en ce qui concerne les noms et de type iambique (faible-fort) et en ce qui concerne les verbes.
Deux théses sous-tendent l'explication proposée :
(1) Les utilisateurs du langage, dans leur choix d'un type d'accentuation, cherchentáalterner les accents forts et faibles (principe d'alternance rythmique) ; (2) les noms et les verbes tendentáapparaître dans des contextes rythmiques différents, de sorte que les verbes ont plus de chance que les noms de recevoir une accentuation iambique.
Il est apparu,ál'analyse d'échantillons d'anglais parléetécrit, que les verbes dissyllabiques ont plus de chances que les noms dissyllabiques de recevoir une inflection qui ajoute une syllabe au mot.
De telles syllabesétant faiblement accentuées, il y a alternance rythmique si le mot dissyllabique reçoit l'accent sur la seconde syllabe (par ex. “suggesting”) plutoˆt que sur la premiére (“promising”).
Deux expériences ont montréque l'accentuation de pseudomots dépend de la nature syllabique des inflexions ajoutées au mot.
En outre, les analyses de texte et les expériences peuvent rendre compte de sous-types d'accentuation particuliersál'intérieur de l'asymétrie générale noms/verbes, aussi bien que de cette asymétrie générale elle-meˆme.
La discussion porte à la fois sur les implications de ces faits pour les théories de l'accent et, de façon plus générale, sur la possibilitéde comprendre le changement linguistique en le rapportant aux opérations cognitives du locuteur/auditeur individuel.
Un codeur de parole à 16 kbit/s avec une faible complexité et un faible retard de signaux est présenté, qui est une version spéciale de l'algorithme d'excitation à impulsions régulières et de prédiction LPC (RPE-LPC).
Cette proposition forme la base du standard de codeurs qui sera utilisé dans le futur système de téléphone mobile à couverture européenne.
Un modèle expérimental est décrit.
Une façon de réduire le débit des codeurs CELP consiste à rallonger la taille des trames d'analyse de l'excitation.
Pour améliorer la qualité de la parole codée, il est souhaitable de restituer l'excitation CELP avec des pics plus marqués.
Sur la base de cette observation, on propose une nouvelle source adaptative dans laquelle un prédicteur de fréquence fondamentale à deux coefficients permet d'attribuer aux échantillons différents gains suivant leur amplitude.
Les résultats de simulation montrent que les impulsions pointues de début de voisement sont clairement reconstruites, ainsi que l'explosion des plosives.
On détermine un algorithme récursif (sur l'ordre du modèle) d'estimation des coefficients d'un modèle autorégressif au sens du maximum de vraisemblance.
Les coefficients ainsi estimés servent alors à estimer les densités spectrales des sources par la méthode du maximum d'entropie.
Enfin, on présente des résultats de simulation permettant de comparer cet algorithme avec l'algorithme de Durbin-Levinson.
Jusqu'ici, l'estimation des fractions d'abondance a toujours été réalisée dans un second temps, par résolution d'un problème inverse généralement.
Dans cet article, nous montrons que les techniques géométriques d'extraction des composants purs de la littérature permettent d'estimer conjointement les fractions d'abondance, pour un coût calculatoire supplémentaire négligeable.
Pour ce faire, un socle commun d'interprétations géométriques du problème est proposé, que l'on peut décliner pour mieux l'adapter à la technique d'extraction de composants purs retenue.
Le caractère géométrique de l'approche lui confère une flexibilité très appréciable dans le cadre de techniques de démélange géométrique, illustrée ici avec N- Findr, SGA, VCA, OSP et ICE.
Une extension non linéaire est proposée, en utilisant des techniques de réduction de dimensionnalité par apprentissage de variétés, illustrée avec les algorithmes MDS, LLE et ISOMAP.
Une telle approche permet de maintenir inchangés les algorithmes géométriques d'identification des composants purs et d'estimation de la proportion de ces derniers dans le mélange.
La pertinence de cette approche est illustrée par des expérimentations sur des données synthétisées et réelles
Cet article présente deux stratégies d'échantillonnage dans le contexte de l'apprentissage par renforcement en mode “batch”.
La première stratégie repose sur l'idée que les expériences susceptibles de mener à une modification de la politique de décision courante sont particulièrement informatives.
Etant donné a priori un algorithme d'inférence de politiques de décision ainsi qu'un modèle prédictif du système, une expérience est réalisée si, étant donné le modèle prédictif, cette expérience mène à l'apprentissage d'une politique de décision différente.
La deuxième stratégie exploite des résultats récemment publiés pour calculer des bornes sur le retour des politiques de décision de manière à sélectionner des expériences améliorant la précision des bornes afin de discriminer les politiques non-optimales.
Ces deux stratégies sont illustrées sur des problèmes élémentaires et les résultats obtenus sont prometteurs.
Une technique TDNN à deux niveaux a été développée pour reconnaître les finales (voyelles) de syllabes en Chinois Mandarin.
Le premier niveau discrimine le groupe “voyelle”, basé sur (a, e, i, o, u, v) et le groupe “nasal”, basé sur des terminaisons nasales (-n, -ng, -autres).
Le discriminateur de groupe “nasal” est ensuite utilisé pour partitionner de nouveau le large groupe “voyelle” produit par le discriminateur de groupe “voyelle”.
Les 2 regroupements du premier niveau produisent 8 sous-groupes au deuxième niveau.
Des discriminations supplémentaires au deuxième niveau permettent d'identifier chacune des 35 finales du Chinois Mandarin.
Cette technique a été testée de façon approfondie en utilisant 8 séries de 1265 syllabes isolées en Pinyin Hanyu, 6 séries étant utilisées pour l'apprentissage et 2 pour le test.
Les résultats globaux montrent que l'on peut atteindre un taux de reconnaissance de 99.4% sur les données d'apprentissage et de 95.6% sur les données de test.
Le taux de bonne reconnaissance parmi les 4 meilleurs candidats atteint 99.1% sur la base de données de test.
Dans cette article nous développons plusieurs modèles autour du paradigme “multi-stream” de la RAP (Reconnaissance Automatique de la Parole) robuste, et nous discutons de leurs relations avec la perception naturelle de parole.
Fortement inspirée par la règle “produit des erreurs” de Fletcher, issue de la psychoacoustique, la reconnaissance “multi-bande” se veut être robuste à l'inadéquation des données par rapport aux conditions d'apprentissage, en exploitant la redondance spectrale, tout en faisant un minimum d'hypothèses sur la nature du bruit.
Des études précédentes en RAP ont montré que le traitement indépendant des sous bandes peut diminuer le taux de reconnaissance en parole propre.
Nous avons surmonté ce problème en considérant toutes les combinaisons de sous bandes comme des flux indépendants de données.
Après un état de l'art sur la RAP multi-bandes, nous formalisons cette approche “full-combination” dans le contexte de la RAP HMM/ANN, en introduisant une variable latente qui spécifie à chaque trame de signal la combinaison de sous bandes la plus adéquate.
Ceci nous permet de décomposer la probabilité a posteriori pour chaque phonème en une somme pondérée, sur toutes les positions possibles des données propres.
Cette approche est prometteuse pour l'adaptation aux bruits imprévisibles et variant rapidement.
Quand plusieurs caractéristiques acoustiques différentes contribuent à la perception d'une distinction phonétique, on peut montrer l'existence d'une relation compensatoire lors de l'identification de stimuli de parole lorsque ceux-ci sont phonétiquement ambigus.
Les expériences présentes essayent d'observer si ces relations compensatoires existent également avec des stimuli non-ambigus, au moyen d'une tâche de discrimination AX avec des stimuli, soit proches de la limite entre deux catégories phonétiques, soit à l'intérieur d'une catégorie.
Les résultats suggèrent que parmi les cinq relations compensatoires, quatre sont liées à la perception des contrastes phonétiques ; elles disparaissent ou s'inversent à l'intérieur des catégories.
L'unique exception prévue représente une relation compensatoire supposée tirer son origine du niveau psychoacoustique.
Ces données limitent strictement les explications psychoacoustiques pour les relations compensatoires et suggérent également que la discrimination à l'intérieur des catégories n'est pas réalisée dans un mode phonétique de perception, affirmant ainsi que le discrimination de la parole peut être traitée de deux façons ; phonétique et psychoacoustique.
Le Perceptron MultiCouche (PMC) est un des réseaux de neurones les plus utilisés actuellement, pour la classification supervisée notamment.
On fait dans un premier temps une synthèse des résultats acquis en matière de capacités de représentation dont jouit potentiellement l'architecture PMC, indépendamment de tout algorithme d'apprentissage.
Puis on montre pourquoi la minimisation d'une erreur quadratique sur la base d'apprentissage semble être un critère mal approprié, bien que certaines propriétés asymptotiques soient aussi exhibées.
Dans un second temps, l'approche bayesienne est analysée lorsqu'on ne dispose que d'une base d'apprentissage de taille finie.
A l'aide de certains estimateurs de densité, dont les propriétés remarquables sont résumées, il est possible de construire un réseau de neurones stratifié réalisant la classification bayesienne.
Cette technique de discrimination directe semble être supérieure au PMC classique à tous points de vue en dépit de la similarité des architectures.
Le travail de recherche dans le domaine du traitement de la parole date de plus de soixante ans, mais c'est seulement depuis quelques années que l'on commence à s'apercevoir de l'impact de ces années de travail dans les systèmes de télécommunication modernes.
Pratiquement tous les domaines issus du traitement de la parole, qui comprennent en particulier le codage de la parole, la synthèse de la parole, la reconnaissance de la parole, et même, dans une moindre mesure, la vérification d'identité par la parole, ont quitté le laboratoire de recherche et apparaissent aujourd'hui dans des produits et des services qui sont utilisés journellement sur le marché, souvent par des millions d'utilisateurs.
Cette révolution du traitement de la parole pour les télécommunications est attisée par les progrès des algorithmes (qui améliorent la qualité des systèmes de traitement de la parole), par les progrès des matériels (qui permettent une grande capacité de calcul et de mémoire à bas prix), ainsi que par les progrès des réseaux (qui fournissent des accès à haute capacité vers les habitations, les lieux de travail, et à travers l'ensemble du réseau de télécommunications).
Dans cet article, nous illustrons l'impact du traitement de la parole sur les télécommunications modernes en montrant comment le codage de la parole, la synthèse de la parole, la reconnaissance de la parole, et la vérification d'identité par la parole se sont intégrés dans de nouveaux produits et services.
Cet article présente un dispositif qui définit des moyens opérationnels pour repérer, acquérir et valoriser des connaissances liées au traitement d'images de document.
Notre démarche pragmatique, a consisté à utiliser une approche terminologique basée sur des expertises pour extraire des unités de connaissance.
Cette analyse linguistique nous a mené à l'élaboration d'un modèle consensuel de scénario et au développement d'une interface.
Ensuite nous proposons une interaction dynamique, pour faire l'acquisition et l'historisation des connaissances des traiteurs d'images dans une base de scénarios.
Enfin, une exploitation des scénarios déjà joués, est faite par les naïfs qui naviguent dans cette base de façon intuitive, et extraient les connaissances qui sont nécessaires à la résolution d'un problème.
La navigation suivant différents points de vue se fait grâce à un graphe hyperbolique.
La modélisation de la sortie du conduit vocal nécessite que soient connues les relations qu'entretiennent les dimensions de la géométrie des lèvres : • - de face : l'écartement, la séparation, l'aire ; • - de profil : les protrusions supérieure, inférieure, du coin ; l'aperture du pavillon et sa profondeur.
Le corpus utilisé explore l'espace maximum de manoeuvre sur les lévres du française (5 locuteurs), avec des consonnes qui requièrent naturellement un contrôle précis de la mandibule (contextes [s] et [∫], et qui, par leur influence fermante, maximisent les mouvements propres aux lèvres.
Ainsi les propositions tendant à calculer l'aire avec une seule de ces mesures ne peuvent être retenues.
De profil, les protrusions sont naturellement très corrélées ; et il en va de même pour la profondeur du pavillon avec la protrusion du coin de la fente labiale.
Nous pouvons donc choisir ce coin comme un pivot de la modélisation de profil.
Mais le plus crucial pour la modélisation de tout le pavillon est sans doute la corrélation inverse que nous avons pu observer entre l'écartement et la protrusion (à condition de bien choisir la protrusion du coin).
Cet article donne une vue d'ensemble d'un système à large vocabulaire et basé sur le phonème pour la reconnaisssance de la parole continue.
Il constitue le module de reconnaissance, dépendant du locuteur, du système spicos conçu pour reconnaître, compredre et répondre à des questions d'une bangue de données formulées en allemand.
Il s'ensuit que le problème de la reconnaissance se transforme en une recherche efficiente dans un très grand espace des phases de manière à ce que les décisions purement locales puissent être évitées au profit de décisions globalement optimales.
La taille de cet espace des phases dépend principalement du modèle de langage employé.
Trois types de modèles sont étudiés ici : un modèle sans aucune contrainte, un réseau à nombre d'états fini et un modéle stochastique trigramme basé sur les probabilités des différentes catégories de mots.
Pour chacun de ces trois modèles, des expériences de reconnaissance ont été menées pour 4 locuteurs sur un vocabulaire de 917 mots.
Pour chaque locuteur, 200 phrases totalisant 1391 mots devaient être reconnues.
Nous présentons dans cet article les avancées réalisées au LIMSI sur la reconnaissance de parole continue de grand vocabulaire, indépendante du locuteur dans une application de dictée de textes.
Le système utilise des modèles de Markov cachés à densités continues au niveau acoustique, et des modèles de langage n-grammes au niveau syntaxique.
La modélisation acoustique repose sur une analyse cepstrale du signal vocal, des modèles de phones en contexte (inter-et intramot) dépendant du genre du locuteur, et des modèles de durée phonémique.
Nous avons utilisé, pour la langue anglaise, le corpus de parole continue ARPA-WSJ contenant des enregistrements de textes lus extraits du Wall Street Journal, et, pour la langue française, le corpus BREF contenant des enregistrements de textes lus extraits du journal Le Monde.
Les performances du système de reconnaissance, mesurées au niveau phonétique et au niveau mot sont données sur ces deux corpus pour des vocabulaires contenant jusqu'à 20.000 mots.
Nous donnons également pour référence les résultats obtenus sur le corpus ARPA-RM qui a été très largement utilisé pour évaluer et comparer des systèmes de reconnaissance de parole.
Pour obtenir le volume désiré dans un vocodeur à Prédiction Linéaire (LP) on utilise souvent la méthode de l'énergie du signal d'erreur.
De grandes erreurs de gain se présentent tout de même, quand un formant à basse fréquence est en résonance avec les impulsions d'excitation voísée.
Dans cet article, on donne une évaluation des erreurs et on discute leurs causes fondamentales.
On démontre que les grandes erreurs sont dues aussi bien à la suppression excessive d'une raie spectrale du signal LP d'erreurs, qu'à l'incompatibilité des fenêtres d'analyse et l'excitation continue pendant la synthèse.
Pour terminer, une amélioration réalisée est discutée dans ce contexte.
Nous présentons une nouvelle méthode d'analyse de séquences d'images adaptée à l'extraction automatique en temps réel de mouvements localisés dans des scènes naturelles.
Nous montrons comment extraire ces mouvements sous la forme de voisinages de points formés dans un espace de très grande dimension par le plongement temporel des variations de niveaux de gris des pixels d'une même enveloppe.
Nous présentons tout d'abord notre méthode d'extraction rapide des voisinages dans cet espace multidimensionnel.
L'application est un détecteur des feux de forêts capable de faire la distinction entre des enveloppes causées par une source de fumée ou par tout autre phénomène dynamique pouvant apparaître localement dans un paysage.
Nous généralisons ensuite les perspectives d'utilisation de la méthode du plongement fractal en envisageant d'autres types d'applications par l'extraction de caractéristiques autres que des mouvements.
Cet article tente de dégager la structure spatio-temporelle des informations acoustiques le long de la cochlée et, corrélativement, l'importance de l'effet de la phase dans l'oreille.
On s'appuie sur les résultats obtenus par un modèle de l'audition périphérique.
Ce modèle est décomposé fonctionnellement en parties indépendantes : de l'oreille externe aux fibres nerveuses.
Il est décrit par un ensemble d'équations portant sur la propagation des ondes acoustiques, la vibration mécanique de la membrane basilaire et la transduction mécano-électrique des cellules ciliées et des fibres nerveuses.
La représentation des informations circulant dans la cochlée est un probléme important quand on cherche à extraire des indices acoustiques pertinents en vue de l'analyse de la parole.
De plus, en synthèse de la parole, on s'aperçoit que la phase joue un rôle non négligeable pour une meilleure qualité perceptive.
Il est donc nécessaire de visualiser ces phénomènes échantillon par échantillon, tout en gardant cependant l'intérêt du sonagramme, c'est-à-dire une relation avec l'évolution de l'intensité.
La morphologie mathématique repose sur la notion d'ordonnancement.
Pour le traitement d'images couleur, l'écriture d'une relation d'ordre valide nécessite l'utilisation de distances couleur normalisées issues des espaces CIELAB ou CIELUV.
Depuis les premières recommandations de la CIE (Commission internationale de l'éclairage), plusieurs distances couleur ont été proposées.
Le but de cet article est d'étudier l'impact de ces formules de distance couleur dans le contexte de la morphologie mathématique couleur.
Les résultats sont développés pour une nouvelle construction des opérateurs morphologiques couleur basée sur la distance dans l'espace CIELAB.
Un critère de comparaison des méthodes d'ordonnancement couleur est ensuite proposé pour comparer les principales approches en morphologie mathématique avec celles basées sur une fonction de distance.
Les analyseurs syntaxiques à ilôts de confiance ont des applications potentielles intéressantes en Reconnaissance Automatique de la Parole.
La plupart des systèmes récemment développés sont fondés sur l'association d'un Module Acoustique et d'un Module Linguistique.
Le premier calcule la probabilité a priori des données acoustiques, étant donnée une interprétation linguistique.
Le second calcule la probabilité de cette interprétation linguistique.
Cet article décrit des travaux de généralisation des analyseurs syntaxiques à ilôts de confiance aux grammaires hors-contextes stochastiques.
Celles-ci pourraient alors être utilisées comme modèles par le module linguistique pour calculer la probabilité d'une interprétation linguistique.
L'intelligibilité de consonnes initiales et finales dans des suites monosyllabes CVC a été mesurée pour une parole synthétisée par règles (concaténation dyadique).
Afin d'évaluer les résultats d'intelligibilité, deux conditions supplémentaires ont été testées : une parole en codage PCM (12 bits, échantillonnage à 10 kHz) et une parole resynthétisée par LPC (12 coefficients).
L'ensemble des stimuli se compose de toutes les consonnes initiales et finales possibles dans neuf contextes CVC différents, c'est-à-dire avec trois voyelles /i, u, O/ et trois consonnes non-voisines /p, t, k/.
Les sujets devaient identifier la consonne initiale ou finale de chaque mot et pouvaient choisir n'importe quelle consonne comme réponse.
Les scores généraux de pourcentages de bonnes réponses pour 33 sujets, pour les consonnes initiales et finales étaient de 93% et de 92.8% pour la parole codée PCM, de 86.7% et de 85.4% pour la synthèse LPC, et de 58.2% et de 73.5% pour la synthèse par règles.
Ces résultats mis à part, les confusions notées sont discutées en détail.
Plus importante que les taux d'intelligibilité dans ce test particulier de la synthèse par règles améliorée, est l'expérience acquise dans l'évaluation et l'amélioration de tels systèmes.
Ce papier présente un nouvel algorithme qui génére des trajectoires tri-dimensionnelles de points de visage à partir d'un fichier de parole, avec ou sans son texte.
Ce dictionnaire contient les caractéristiques acoustiques et visuelles.
L'acoustique est représentée par les coefficients LSF (line spectral frequencies), et les points du visage sont représentées par leurs composantes principales (PC).
Dans l'étape de synthèse, l'entrée parole est comparée aux entrées du dictionnaire.
Sur base de cette similarité, on assigne un coefficient de pondération à chaque entrée du dictionnaire.
Si l'information phonétique de la phrase test est disponible, celle-ci est utilisée pour limiter la recherche dans le dictionnaire aux entrées qui sont les plus proches du phonème courant (en utilisant une matrice de similarité).
Ces pondérations sont alors utilisées pour synthétiser les composantes principales de la trajectoire des points du visage.
Les performances de l'algorithme sont testés sur des données séparées, et on montre que la trajectoire synthétique a une corrélation de 0.73 avec la trajectoire réelle.
Nous proposons dans cet article une voie à suivre pour tenter d'apporter une solution au problème complexe qu'est la définition d'un facteur de forme adapté au problème de la vérification automatique des signatures manuscrites.
Le codage de la signature obtenu de la projection locale du tracé sur les segments d'un motif M($ {\gamma }$) est un compromis entre les approches globales où la silhouette de la signature est considérée comme un tout, et les approches locales où des mesures sont effectuées sur des portions spécifiques du tracé.
Inspiré de ces deux familles d'approches, l'ESC est en fait une approche mixte qui permet d'effectuer des mesures locales sur la forme sans la segmenter en primitives élémentaires, une tâche très difficile en pratique.
Ce travail porte principalement sur l'étude de l'influence de la résolution des motifs utilisés pour le codage de la signature (par la projection locale du tracé), et sur la définition d'un système de type multi- classifieurs pour tenter de rendre plus robuste la performance des systèmes de vérification de signatures.
Dans cet article nous proposons une structure multibande à plusieurs capteurs qui utilise un mécanisme d'adaptation intermittente.
La convergence de cette méthode est comparée avec celle de la méthode dés moindres carrés appliquée dans le domaine temporel et fréquentiel.
Des résultats préliminaires concernant l'influence de l'ordre des filtres dans chaque bande sont aussi présentés.
L'analyse des paramètres acoustiques pour la caractérisation et l'identification des phonèmes de la langue à étudier représente la première étape du décodage acoustique-phonétique, indépendamment de la méthode utilisée.
On trouve malgré tout très peu d'analyses acoustico-phonétiques de la langue espagnole.
Le travail présenté ici traite de la discrimination des occlusives espagnoles.
Rechercher un ensemble réduit de paramètres robustes pour identifier le lieu d'articulation des occlusives sourdes a constitué notre premier objectif.
A partir de cet ensemble de paramètres, on a ensuite élaboré et évalué deux algorithmes de classification et de reconnaissance automatique.
Après une étape de localisation automatique de l'explosion (“burst”), seules les caractéristiques acoustiques de ce segment sont prises en compte pour l'estimation des paramètres.
On a mesuré aussi bien les caractéristiques temporelles que fréquentielles sur un corpus de syllables CV prononcées par 6 locuteurs.
On a conçu, dans le premier cas, un système procédural à base de règles, pour reconnaître le lieu d'articulation.
Pour le traitement des paramètres fréquentiels, on a mis au point un classifieur statistique, développé à partir d'une analyse discriminante préalable des paramètres d'entrée.
Un corpus de syllables CV prononcées par 40 nouveaux locuteurs, non utilisé pour la définition et l'analyse des paramètres, a permis d'évaluer les deux systèmes qui ont tous deux conduit à de bons résultats d'identification.
L'algorithme calcule le taux de vraisemblance brut pour chaque mot du lexique en utilisant les probabilités d'observation des spectres et de l'information sur les durées des unités de reconnaissance.
Avec cette méthode, nous avons pu réduire l'effort de calcul de 74% en tolérant une légère dégradation du taux de reconnaissance dans un système à 1160 mots qui utilise la modélisation de phonèmes par des modèles de Markov cachés.
Aussi, nous avons observé que le calcul du taux de vraisemblance brut est un bon estimateur du même taux calculé par l'algorithme de Viterbi.
Les microperturbations de la période fondamentale permettent de discriminer entre locuteurs normaux et dysphoniques.
Deux études comparées ont été menées à bien.
Les différences inter-voyelles significatives ont pu être reliées au comportement idiosyncratique des différentes méthodes de prétraitement du signal par rapport à la qualité phonétique de la voyelle.
Les différences inter-voyelles intrinsèques sont compensées par une normalisation des microperturbations à l'aide de la moyenne de la période fondamentale.
En règle générale, le prétraitement a été d`autant plus la distance en fréquence entre F 0 et F 1 était élevée.
Lors d`une deuxième expérience, des obteunus à partir de phrases isolées n`ont pas mieux séparé locuteurs normaux et dysphoniques que des mesures basées sur des voyelles soutenues.
En ce qui concerne nos corpus, aucune supériorité intrinsèque de la parole continue n`a pu ětre mise en évidence au niveau des performances de discrimination.
Par contre, lors des transition entre segments et lors de l`établissement et de l`extinction du voisement les valeurs des microperturbations sont appareus plus importantes en valeur absolue.
Cet article présente une approche de l'apprentissage et de l'adaptation du modèle acoustique, appelée normalisation des environnements, appliquée au modèle stochastique des mélanges de trajectoires.
L'approche proposée étend la technique connue de normalisation des environnements — utilisée pour l'adaptation des HMM — aux modèles fondés sur les segments.
De plus, l'approche proposée donne une nouvelle méthode de représentation et de combinaison des différentes sources de la variabilité de la parole.
Dans notre approche, la normalisation et l'adaptation sont effectuées en utilisant des transformations linéaires.
Les résultats des expériences de l'adaptation au locuteur montrent que l'approche proposée conduit à une amélioration du taux de reconnaissance jusqu'à 34% par rapport au modèle non adapté.
Les résultats des expériences de l'adaptation au bruit montrent que pour certaines configurations de test la technique proposée donne même de meilleurs résultats que le modèle dépendant de l'environnement.
Nous avons également observé que l'apprentissage par la normalisation des environnements et l'adaptation sont plus performants que l'apprentissage classique et l'adaptation par régression linéaire (MLLR).
En commande des systèmes, l'anticipation pure apparaît comme la meilleure solution mathématique au problème de l'asservissement des sorties aux consignes, que celles-ci soient connues a priori (cas des systèmes de régulation) ou pas (cas des systèmes de poursuite de trajectoire).
D'un point de vue formel, cela est obtenu en mettant le système à asservir en cascade avec son système inverse.
Cette solution n'est toutefois pas réalisable physiquement, car le modèle du système n'est que rarement complet.
De plus, le système inverse est souvent instable.
Enfin, les perturbations observées, soit en sortie, soit en entrée, rendent difficile l'accès à une solution satisfaisante.
Dernier aspect : pour être exploitable, l'anticipation doit se faire en temps réel.
La méthode présentée ici tente d'apporter une solution rapide et robuste au problème de prédiction.
Elle utilise à la fois les propriétés géométriques du signal à prédire à l'instant considéré (procédure locale), ainsi qu'une base d'apprentissage constituée d'observations passées (procédure globale).
Les performances du prédicteur sont évaluées par quelques exemples significatifs et comparées aux performances d'autres prédicteurs.
Dans ce papier, nous évaluons en détail l'influence du canal de transmission téléphonique sur les performances de systèmes de reconnaissance automatique de la parole (RAP).
Un simulateur temps-réel est décrit et mis en œuvre, permettant une génération flexible et contrôlée des différentes perturbations habituellement rencontrées dans les réseaux téléphoniques, aussi bien traditionnels (fixes) que mobiles et IP.
Le modèle utilisé est basé sur un ensemble de paramètres d'entrée qui sont connus des concepteurs de réseaux ; il est donc applicable sans devoir mesurer explicitement les caractéristiques du réseau.
Ce modèle peut donc être utilisé pour mesurer analytiquement l'impact des perturbations du réseau sur les performances de la RAP, pour produire des données d'entraı̂nement correspondant à des caractéristiques de transmission déterminées, ou encore pour tester des systèmes vocaux interactifs dans des conditions de réseau réalistes et multiples.
Dans ce papier, nous nous focalisons avant tout sur le premier point.
La robustesse de deux systèmes de RAP, intégrés dans une application de recherche d'informations basée sur un dialogue vocal, est évaluée en fonction de la dégradation (contrôlée) du canal de transmission.
Les dégradations résultantes du système de RAP sont ensuite comparées à celles observées dans le cas de la communication homme-homme.
Il est alors intéressant de noter que les conclusions dépendent fortement du type de perturbations (résultant souvent en différents comportements).
Ces conclusions sont pertinentes aussi bien dans le cadre de la conception des réseaux que lors du développement de technologies vocales.
Des résultats sont présentés qui indiquent que les performances d'une technique à fréquence de trames variable et basée sur des triphones peuvent être meilleures que celles obteneus en utilisant des triphones et la fréquence de trames maximale.
La technique à fréquence variable requiert un temps de calcul très inférieur.
Un problème inverse du conduit vocal est analysé du point de vue de la théorie des problèmes incorrects.
On propose la méthode, utilisant la régularisation variée avec des restrictions pour surmonter des difficultés liés à la dissemblance et à l'instabilité.
Le fonctionnement des organes d'articulation est utilisé comme un régulateur et comme un critère d'optimum pour une résolution approximative recherchée.
Des paramètres acoustiques mesurés servent des restrictions extérieures tandis que la géométrie du conduit vocal, les mécanismes d'articulation et les particularités phonétiques de la langue servent des restrictions intérieures.
Une méthode effective numérique de la réalisation d'un approche proposé est basée sur une approximation linéaire d'une image “articulation – acoustique”.
Une méthode euristique nommée “une méthode des courbes calibrées” a été utilisée pour évaluer la précision des résolutions approximatives obtenues.
On fait voire que dans les certains cas une faute dans la résolution d'une tâche inverse ne dépend que très faiblement des fautes des mesures des fréquences des formantes.
Formes du conduit vocal évaluées à l'aide d'une méthode proposée ressemblent beaucoup aux formes mesurées au cours des expériences radiographiques.
Le cliquetis dans les moteurs à allumage commandé demeure un problème pour les motoristes.
La détection du cliquetis doit en effet permettre de réaliser un compromis entre l'optimisation du rendement du moteur, la consommation de la voiture et le respect des normes en matière de dépollution.
Souvent l'allumage est réglé avec une marge de sécurité qui garantit l'absence de cliquetis même en cas de variations de la qualité du carburant.
L'enjeu de l'élaboration d'une méthode de détection de cliquetis est de se rapprocher le plus possible des conditions limites de cliquetis tout en évitant son apparition.
L'objectif de l'étude présentée consiste à évaluer l'intensité du cliquetis produit dans la chambre de combustion à partir d'un enregistrement fourni par un accéléromètre placé sur le bloc moteur.
Son but est de reconnaître trois types de cliquetis : l'absence de cliquetis, le cliquetis naissant et le cliquetis violent.
L'approche envisagée pour mener à bien cette détection fait appel aux techniques du diagnostic par reconnaissance des formes floue.
La méthode, mise au point à l'aide d'un ensemble d'apprentissage, conduit à la réalisation de plusieurs processus de diagnostic qui coopèrent.
La présente contribution examine les impacts de la standardisation et de la véhicularisation sur la diversité linguistique.
Nous démontrons à la lumière du tɔŋúgbe, parlé dans le sudest du Ghana, et du peul véhiculaire dans le nord du Cameroun que les langues sont des objets sociaux dynamiques pouvant être institutionnellement et / ou socialement soumis à une promotion ou à une rétrogradation.
Sur une base de données empirique et authentique, nous montrons par exemple que le tɔŋúgbe, l'un des dialectes de la langue éwé, comporte des propriétés linguistiques exceptionnelles qui ont échappé à la documentation de l'éwé standard.
Pourtant ces propriétés, à l'instar de celle de la détermination du syntagme nominal par l'article défini, sont essentielles à la compréhension et à l'étude non seulement de l'éwé, mais également de l'ensemble du groupe Gbe.
Dans la deuxième partie, nous soutenons que la véhicularisation, comme pour le cas du peul Adamawa, se révèle être une arme à double tranchant qui, d'une part, promeut une variété de peul vis à vis d'autres et, d'autre part, implique le recul d'autres langues et variétés minoritaires.
Le codage optimal d'un synthétïseur à formants a été déterminé.
L'optimisation des paramètres de commande est basée sur des critères statistiques et subjectifs.
Le synthétiseur est du type parallèle et est capable de faire une synthèse de très haute qualité de sons voisés ou non voisés.
Les phrases choisies pour l'expérimentation sont des logatomes CVCV représentant des groupes français licites.
Le premier groupe comprend les occlusives /b, d, g/ et les voyelles /a, u, i/ alors que le second utilise des consonnes fricatives /З, z/ et les mm̂mes voyelles.
La procédure comprend 4 étapes.
La première consiste en une étude statistique sur les paramètres de commande afin de déterminer pour chacun d'eux la gamme de leur dynamique effective.
Dans un deuxième temps, on détermine le nombre minimum de bits nécessaires pour la quantification de chacun des paramètres indépendamment (sans dégradation perceptive notable).
La troisième étape permet de trouver le nombre minimum de bits nécessaires lorsque tous les paramètres sont quantifiés simultanément.
On opère par regroupement de paramètres en sous-groupes et recherche du nombre minimum de bits de quantification pour tous les paramètres d'un sous-groupe.
Puis nous regroupons les sous-groupes pour trouver le nombre optimal de bits pour tous les paramètres simultanés.
L'étape finale détermine la période d'échantillonnage de chacun des intervalles d'un logatome.
Un échantillonnage variable est proposé selon la nature des événements sonores.
Les applications embarquées dans les appareils portables tels que téléphones ou PDAs et qui interagissent avec leur utilisateur, doivent prendre en compte la disponibilité de celui-ci.
Une approche de livraison de messages, dynamique et dépendante du contexte de l'utilisateur, est ici proposée.
Cette approche est illustrée en utilisant Scatterbox, une application pervasive qui permet la fusion d'information afin de déterminer le contexte dans lequel se trouve l'utilisateur.
En fonction du niveau de disponibilité de celui-ci, Scatterbox rend prioritaires certains messages et les envoie sur son téléphone portable.
Nous tirons ces conclusions après une évaluation préliminaire du système.
Cet article passe en revue plusieurs domaines d'applications du critère de classification « K-produits » introduit dans [10].
Ce critère est appliqué à l'estimation de mélange de lois, à l'extraction de lignes droites dans des images binaires et à l'estimation aveugle de canal en radiocommunications.
Plusieurs algorithmes d'optimisation du critère sont présentés et des comparaisons avec d'autres approches sont conduites.
Parmi un certain nombre de représentations paramétriques équivalentes de la prédection linéaire, la représentation par coefficients du cepstre est reconnue comme fournissant les meilleures performances en reconaissance de la parole.
Comme les coefficients cepstraux d'un filtre tout pôle sont inversement proportionnels à leurs quéfréquences, ces coefficients sont multipliés par leur quéfréquence respective.
Les coefficients cepstraux pondérés en quéfréquence sont étudiés en fonction de leur eficacité dans une expérience de reconnaissance de voyelles.
Notre projet de recherche vise à élaborer des Webs Sémantiques d'Organisation (WSOs) hybrides, réalisant un couplage fort entre une Base de Connaissances (BC) et une Base de Documents (BDoc).
Les connaissances capitalisées sont ainsi réparties à la fois dans la BC et la BDoc.
L'intérêt de modéliser des connaissances est de permettre au WSO de raisonner sur ces connaissances pour assister les utilisateurs dans leurs tâches de gestion des connaissances.
En contrepartie, la distribution des connaissances dans des sources hétérogènes complique leur accès.
Pour pallier ces difficultés, nous proposons, d'une part, d'introduire un modèle de l'information contenue dans le WSO, en faisant abstraction de son mode de spécification et, d'autre part, de coupler ce modèle à un mécanisme de génération dynamique de présentations d'informations ciblées dont le contenu est adapté à l'utilisateur.
Dans cet article, nousprésentons une vue d'ensemble de cette approche.
On a réalisé des simulations aérodynamiques de séquences /aCa/ à l'aide d'un modèle basse-fréquence pour l'écoulement de l'air dans la partie supérieure du conduit vocal et d'un modèle à deux masses pour la source.
Ces simulations nous ont permis de mieux comprendre les résultats d'une étude empirique sur l'écoulement lors de la parole continue.
On a examiné la contribution des diverses sources d'écoulement, y compris la compliance des parois, à l'écoulement buccal global.
Le modèle à deux masses a été modifié pour permettre un écoulement glottique plus naturel pendant l'abduction et l'adduction.
Malgré ces modifications, le modèle à deux masses s'est avéré insuffisant pour modéliser les variations de source en parole continue.
Par une technique de diversité d'espace consistant à utiliser plusieurs sous-réseaux identiques (par exemple appartenant à un seul réseau rectiligne de capteurs équidistants), il est possible d'extraire le propagateur à partir d'une seule épreuve de courte durée et, par suite, de procéder à la goniométrie de sources lointaines.
De la sorte peuvent être traités le cas de situations non stationnaires ou celui de fronts d'onde incidents totalement cohérents, puisqu'on exploite seulement le vecteur des signaux reçus et non pas leur matrice interspectrale.
La méthode exige un rapport signal à bruit par source au moins égal à OdB;
toutefois, s'il existe une certaine cohérence temporelle des sources sur une durée correspondant à trois ou quatre dizaines d'épreuves, la robustesse vis-à-vis du bruit peut être grandement accrue en employant une technique de mémoire consistant à appliquer un facteur d'oubli au vecteur des signaux reçus.
Il s'agit d'une méthode superdirective qui a la propriété de fournir la phase des fronts d'onde incident, ce qui permet éventuellement des mesures de Doppler.
La base de données et les tests NOISEX-92 sont décrits et commentés.
NOISEX-92 spécifie des tests sur des données bruitées artificiellement en évaluant les performances dans le cadre de la reconnaissance de chiffres et une gamme relativement large de rapports signal sur bruit.
Des examples de scores de reconnaissances sont présentés.
Les recherches sur les langues parlées ont montréque la durée des pauses silencieuses d'une phrase est fortement liéeàla structure syntaxique de cette phrase.
Une analyse du meˆme type sur un passage de la Langue des Signes Américaine permet de voir que les suites de signes sontégalement entrecoupées par des pauses (arreˆts entre les signes) de longueurs variables : les pauses longues semblent indiquer la fin des phrases, les pauses courtes marquent la frontière entre des phrases coordonnées et les pauses très courtes indiquent les frontières de constituants internes.
L'analyse des pauses est un guide pour la segmentation des phrases dans la Langue des Signes Américaine.
Dans cet article nous décrivons une technique permettant d'améliorer la qualité de signaux de parole dégradés par du bruit additif non stationnaire.
Cette technique est évaluée dans le cadre d'un système de reconnaissance de chiffres connectés perturbés par des bruits représentatifs d'environnements militaires.
L'algorithme développé utilise une estimation de l'amplitude spectrale du signal de parole étant donnée une représentation paramétrique par état des modèles de la parole et du bruit.
L'analyse spectrale est réalisée par un banc de filtres avec interpolation fréquentielle dont les paramètres sont sélectionnés en fonction de la nature du bruit.
Les modèles sont des modèles ergodiques de Markov cachés appris sur du bruit et de la parole qui ont des distributions gaussiennes multivariées.
Ce dispositif permet de transformer certains paramètres de la voix d'un locuteur source de façon à approcher les caractéristiques acoustiques de la voix d'un locuteur cible.
Nous nous intéressons ici particulièrement à la transformation du spectre à court-terme.
Dans cette contribution, nous utilisons comme paramètre les formants, connus pour représenter de façon satisfaisante les caractéristiques acoustiques du conduit vocal.
Notre méthode de transformation comprend deux phases distinctes : une phase d'analyse, dans laquelle nous extrayons les paramètres formantiques, et une phase d'apprentissage dans laquelle nous apprenons les transformations à l'aide d'un réseau de neurones.
Les formants transformés sont ensuite utilisés, lors de la synthèse, dans un synthétiseur à formants.
Cette contribution décrit l'adaptation à l'analyse de l'intonation en allemand du modèle quantitatif proposé par Fujisaki et son application à la synthèse de F 0 par règles.
Les valeurs paramétriques du modèle sont déterminées par une approximation automatique des contours de F 0 produits en langage naturel.
Les sources potentielles de la variation des valeurs paramétriques sont examinées à l'aide de méthodes statistiques.
Sur la base de ces analyses est formulé un ensemble de reègles qui exprime des traits linguistiques aussi bien que relatifs aux locuteurs individuels.
Les règles produisent des contours d'intonation artificiels qui peuvent être interprétés relativement à des traits linguistiques, comme par exemple la modalité de phrase ou l'accent du mot.
L'acceptabilité des patrons intonatifs produits sur la base des règles ainsi que le modèle adéquat des traits prosodiques sont évalués perceptuellement par des auditeurs professionnls (phonéticiens) et des auditeurs “naïfs”.
En général, les énoncés resynthétisés avec des contours de F 0 produits par les règles sont jugés très acceptables et naturels par les deux groupes d'auditeurs.
La représentation en fréquence sur base d'un “line spectrum pair” a été proposée en tant que variante de la représentation paramétrique par prédiction linéaire.
Dans le contexte du codage du signal de parole, la structure LSP possède de meilleures propriétes de quantification que les représentations LPC plus classiques.
Dans cet article, la présentation LSP est étudiée dans le contexte de la reconnaissance automatique de la parole.
Plusieurs mesures de distance sur base d'un modèle LSP sont étudiées dans le cadre d'une tâche de reconnaissance de voyelles soutenues.
La mesure LSP pondérée s'avère être la meilleure.
Elle a été comparée à d'autres mesures de distances mieux connues (c'est-à-dire les distances d'Itakura, cepstrale, cepstrale pondérée, de racine-puissance-somme, du rapport logarithmique d'aires et des coefficients de réflexion).
La distance LSP pondérée permet un meilleur taux de reconnaissance que les autres mesures indiquées.
Les méthodes de classification non supervisée sont des outils de fouille de données qui visent à identifier des groupes d'objets similaires par rapport aux valeurs qu'ils prennent sur différentes variables.
Les méthodes dites conceptuelles adjoignent à la partition une interprétation des classes en fonction des valeurs des variables présentes dans chacune des classes.
Une telle structuration peut être décrite par un couple de partitions liées, appelé bipartition, constitué d'une partition des objets et d'une partition des modalités de variables.
Cet article présente une étude de l'utilisation de méthodes de recherche locale pour la construction de bipartitions maximisant un critère de qualité défini par Goodman et Kruskal [GOO 54, GOO 59].
Nous proposons un algorithme basé sur le parcours stochastique d'une structure de voisinage sur l'espace de recherche des bipartitions possibles.
Le critère d'arrêt est redéfini de manière statistique, ce qui permet de borner statistiquement la qualité de la solution obtenue.
Enfin, une étude variationnelle de la fonction à optimiser permet de réduire la complexité du calcul.
Cette contribution vise à évaluer l'utilisation des variantes de prononciation pour différentes configurations de système, différentes langues et différents types d'élocution.
Pour évaluer le besoin de variantes nous avons défini le taux de variant2+ qui correspond au pourcentage de mots du corpus qui ne sont pas alignés avec la meilleure transcription phonémique.
Ce taux peut être considéré comme indicatif d'un éventuel besoin de variantes de prononciation dans le système de reconnaissance.
Différents lexiques de prononciation ont été créés automatiquement générant différents types et quantités de variantes (avec surgénération).
En particulier des lexiques avec des variantes parallèles et séquentielles ont été distingués afin d'évaluer la précision de la modélisation spectrale et temporelle.
Dans une première étape nous avons montré le lien entre le besoin de variantes de prononciation et la qualité des modèles acoustiques.
Nous avons ensuite comparé différents phénomènes de variantes pour l'anglais et le français sur des grands corpus de parole lue (WSJ et BREF).
Une comparaison entre parole spontanée et parole lue est présentée.
Cette étude montre que le besoin de variantes diminue avec la précision des modèles acoustiques.
Pour le français, elle permet de révéler l'importance des variantes séquentielles, en particulier du e-muet.
L'auditeur réussit assez bien à identifier des paires de voyelles, et cela même lorsque les deux voyelles commencent et se terminent en même temps, sont présentées en monaurale, ont la même fréquence fondamentale (f 0) et ont approximativement la même intersité.
La sensation décrite par l'auditeur est la suivante : une voyelle dominante “teintée” d'une seconde non-dominante.
Une petite différence dans la f 0 améliore la performance et procure au sujet une sensation typique résultant de l'utilisation de deux sources de voix plutôt que d'une seule teintée d'une autre.
La dominance pourrait refléter une stratégie de “décision” cognitive s'ajoutant au masquage spectral au niveau auditif périphérique.
Nous proposons, dans ce papier, un algorithme d'estimation automatique du rapport signal à bruit (signal-to-noise ratio – SNR) dans le cas d'un signal de parole bruité.
Le choix de la technique d'extration des paramètres acoustiques est motivée par des propriétés neurophysiologiques du processus de modulation d'amplitude aux niveaux supérieurs du système auditif des mammifières.
Celui-ci semblant analyser l'information à la fois en sous-bandes de fréquences et en terme de modulation de l'amplitude du signal entrant.
Cette information peut-être représentée par un spectrogramme de modulation d'amplitude (amplitude modulation spectrogram – AMS).
Un réseau de neurones est alors entraı̂né sur un grand nombre d'exemples d'AMS générés à partir de signaux de parole bruités.
Une fois entraı̂né, le réseau de neurones est capable de produire une estimation du SNR lorsque des AMS de sources sonores inconnues sont présentées à son entrée.
Des expériences de classification démontrent une estimation relativement précise du SNR dans des trames d'analyse indépentantes de 32 ms.
L'écriture manuscrite, comme la plupart des productions de l'activité humaine, présente une étonnante variabilité.
Nous étudions cette variabilité afin d'obtenir, préalablement à sa reconnaissance, un premier degré de caractérisation de l'écriture manuscrite.
Sur des écrits possédant peu de mots, ce premier degré de caractérisation peut être déterminé, au niveau de chaque mot, par un petit nombre d'observations largement indépendantes de leur contenu sémantique, et, par suite, propres à la main du scripteur.
Une étude statistique portant sur 980 montants littéraux de chèques montre que ces observations sont peu corrélées entre elles : elles définissent un espace de variabilité inégalement dense, laissant ainsi apparaître la possibilité d'un regroupement des écritures en familles.
Cet article présente une nouvelle variante des Modèles de Markov cachés (HMM) “à Quantification Vectorielle Multiple” (MVQHMM).
Sa caractéristique principale est d'utiliser un dictionnaire différent pour chaque modèle.
On décrit les algorithmes d'apprentissage et de reconnaissance pour ces modèles.
La procédure de reconnaissance combine le calcul de l'erreur de quantification de la séquence de vecteurs avec celui de la probabilité de sa génération par le HMM.
De plus, les MVQHMM apparaissent plus robustes à la variabilité interlocuteur que les modèles discrets et semi-continus.
Nous proposons dans cet article une méthode originale pour rechercher, dans des séquences vidéo, des sous-séquences similaires.
En introduisant de la flexibilité temporelle dans la caractérisation des sous-séquences, cette méthode permet d'éviter l'utilisation de mesures de distance flexibles (telles que le Dynamic Time Warping) qui ont l'inconvénient d'être lentes.
La méthode proposée permet donc de rechercher, en temps réel, des sous-séquences vidéo similaires parmi plusieurs centaines de milliers d'exemples.
La méthode proposée est adaptative ; un algorithme d'apprentissage rapide est présenté.
Les performances ont été évaluées avec succès sur un ensemble de 1 707 clips vidéo (> 800 000 sous-séquences).
A terme, notre objectif est de proposer un système de génération d'alertes et/ou de préconisations, en temps réel, dans le cadre de l'aide à la chirurgie sous contrôle vidéo.
La connaissance du langage repose-t-elle sur la représentation mentale de règles ?
Rumelhart et McClelland ont développé un modèle connectioniste (parallel distributed processing, PDP) de l'acquisition du passé anglais qui parvient à produire la forme passé d'un certain nombre de verbes, à la fois réguliers (walk/walked) et irréguliers (go/went), à partir de leurs racines, et qui semble commettre certaines des erreurs et passer par certains des étapes de développement des enfants qui apprennent le passé anglais.
Pourtant, le modèle ne contient pas de règles explicites ; il est exclusivement constitué d'un ensemble d'unités qui représentent des trigrammes de traits phonétiques de la racine, d'un ensemble d'unités qui représentent des trigrammes de traits phonétiques de la forme passée de la racine, et d'un réseau de connections entre les deux ensembles d'unités, connections dont la force varie en fonction de l'apprentissage.
La conclusion de Rumelhart & McClelland est que les règles linguistiques ne sont peut-eˆtre en fait que des approximations pratiques et que les processus causaux réels de l'utilisation et de l'acquisition du langage doivent eˆtre caractérisés en termes de transfert de niveaux d'activation entre unités et de modification du poids de leurs connections.
Nous avons analysé en détail les hypothèses linguistiques et de développement qui sous-tendent leur modèle et avons découvert que (1) le modèle ne peut pas représenter certains mots, (2) il ne peut pas apprendre beaucoup de règles, (3) il peut apprendre des règles que l'on ne rencontre dans aucune langue humaine, (4) il ne peut pas expliquer certaines régularités morphologiques et phonologiques, (5) il ne peut pas expliquer les différences entre formes régulières et irrégulières, (6) il ne parvient pas à accomplir la taˆche qui lui a été assigné, à savoir apprendre le passé anglais, (7) il explique incorrectement deux phénomènes de développement : les étapes de sur-régularisation de formes irrégulières comme bringed, et l'apparition de formes doublement marquées comme ated, enfin, (8) il donne une explication de deux autres phénomènes (la surrégularisation peu fréquente des verbes qui se terminent en t/d, et l'ordre d'acquisition des différentes sous-classes irrégulières) qui est indifférenciable de celle fournie par des théories utilisant des règles.
En outre, nous montrons que c'est l'architecture connectioniste du modèle qui est responsable de ses nombreux défauts.
Notre conclusion est que les affirmations des connectionistes quant à l'inutilité des règles dans les explications doivent eˆtre rejetées et que, bien au contraire, toutes les données militent en faveur de l'existence de telles règles.
L'A. critique plusieurs contributions grammaticales récentes qui tentent d'expliquer la fonction syntaxique de la forme wayyiqtol (dit » Impf. conséc. « ), en tant que marqueur de la narration : B. K. Waltke/M. O'Connor et P. Joüon/T. Muraoka, d'une part, et A. Niccacci, d'autre part).
Il considère en particulier les paradigmes/modèles linguistiques qui sous-tendent selon lui ces contributions ( » philologie comparative « et » linguistique textuelle « ).
Il critique l'explication étymologique de Waltke/O'Connor, mais modifie en même temps le modèle de linguistique textuelle proposé par Niccacci.
En conclusion, l'A. signale l'erreur qui consiste(rait) à utiliser en priorité la construction paratactique des récits-LXX pour les traductions modernes de la Bible.
Il faudrait par conséquent rendre, le plus souvent possible, les séries » monotones « de propositions débutant par wayyiqtol par la même disposition des phrases dans les langues-cibles modernes.
Cet article porte sur l'extension de la problématique classique de la découverte de règles de type « si a alors presque b » à la recherche de règles généralisées de type R^> R' où les prémisses R et les conclusions R' peuvent être elles-mêmes des règles.
Dans le cadre de l'analyse statistique implicative développée initialement par Gras [GRA 79], [GRA 96], une première formalisation basée sur la notion de « hiérarchie orientée » a été récemment proposée [GRA 01].
Inspirée fortement par la classification ascendante hiérarchique, la démarche consiste à « agréger » des règles entre elles selon un mécanisme incrémental.
Nous proposons ici une nouvelle formalisation du modèle qui met plus nettement en évidence les structures en jeu.
Et, nous justifions l'emploi du terme hiérarchie, jusqu'alors utilisé métaphoriquement, en montrant que la mesure construite pour l'indicer vérifie les propriétés d'une ultramétrique.
La démarche est illustrée sur un corpus de données réelles issu d'une enquête auprès d'enseignants de mathématiques du secondaire sur les objectifs assignés à leur enseignement.
Cet article décrit un système de récupération de documents oraux (Spoken Document Retrieval, SDR) pour des Emissions d'Informations (télévisées ou radio) britanniques et nord-américaines.
Le système est basé sur un reconnaisseur de la parole à large vocabulaire connecté à un système de recherche probabiliste.
Nous discutons le développement d'un reconnaisseur vocal en temps réel des Emissions d'Informations télévisées ou radio ainsi que son intégration au sein du système SDR.
Pour cela, nous avons améliore deux approches a ce problème : la segmentation automatique et l'expansion statistique de la récupération des données en utilisant un corpus secondaire.
L'une des étapes les plus importantes de la compréhension du langage parlé est la segmentation du signal de parole en mots.
Quand les conditions d'écoute sont mauvaises, les locuteurs peuvent aider ceux qui les écoutent en articulant bien, c'est-à-dire en produisant une “parole claire”.
Nous avons examiné, au cours de quatre expériences, la production des frontières de mots dans la parole claire.
Un rapport précédent a montré que les locuteurs qui articulent très soigneusement essaient effectivement de souligner les frontières de mots en effectuant une pause aux frontières et en allongeant les syllables qui les précèdent. De plus, ils appliquent particulièrement ces stratégies aux frontières qui précèdent les syllabes faibles.
Les anglophonesemploient en effet une stratégie de segmentation telle qu'ils repèrent plus aisément les frontières de mots précédant les syllabes fortes ou accentuées ; le soulignement des frontières de mots précédant les syllabes faibles dans la parole claire permet de marquer les frontières qui seraient difficiles à percevoir.
Le présent article décrit des données supplémentaires : les analyses prosodiques des syllabes qui suivent une frontière critique.
Dans la parole claire, la durée est plus allongée et l'intensité davantage renforcée dans les syllabes faibles que dans les syllabes fortes.
La fréquence fondamentale moyenne augmente aussi davantage dans les syllabes faibles que dans les syllabes fortes.
Au contraire, le mouvement du ton augmente plus dans les syllabes fortes que dans les syllabes faibles.
Ces effets sont toutefois très petits comparés aux allongements, observés auparavant, de la durée des syllabes précédent les frontières de mots et de celle des pauses à la frontière.
Au cours d' une conversation, les interlocuteurs travaillent ensemble pour construire une référence définie.
Dans le modèle proposè, le locuteur initie le processus en présentant un syntagme nominal.
Avant de passer à la contribution suivante, les participants, si cela est nécessaire, corrigent, développent ou remplacent ce syntagme nominal au cours d'un processus itératif jusqu'a ce que soit atteinte une version que tout deux acceptent.
En faisant cela ils essaient de minimiser l'effort conjoint.
La procedure préférée consiste pour le locuteur à présenter un syntagme nominal simple et pour l'allocuteur d'accepter ce syntagme en donnant le feu vert pour l'échange suivant.
Nous décrivons une tache de communication an cours de laquelle deux personnes discutent l'agencement de figures complexes et nous montrons comment le modele proposé rend compte de nombreux traits des références produites.
Le modéle découle, selon notre suggestion, de la responsabilité mutuelle que les participants prennent pour que soit compris chaque énoncé durant la conversation.
Quand ils parlent à des systèmes interactifs, les utilisateurs “sur-articulent” parfois, ou bien adoptent un type d'élocution, se voulant didactique, qui a été associé à une augmentation des erreurs de reconnaissance.
Les buts de cette étude étaient les suivants (1) établir une méthode de simulation flexible pour étudier les réactions des utilisateurs aux erreurs des systèmes, (2) analyser le type et l'ampleur des adaptations linguistiques lors des résolutions d'erreurs entre l'homme et la machine, (3) fournir un modèle théorique unifié pour prédire et interpréter les adaptations de la parole de l'utilisateur au cours du traitement des erreurs de la machine, et (4) souligner les implications pour le développement de systèmes interactifs plus robustes.
Une méthode de simulation semi-automatique permettant de générer de nouvelles erreurs a été développée pour comparer la parole de l'utilisateur juste avant et juste après l'apparition d'erreurs de reconnaissance de la part du système, et pour divers taux d'erreur de base.
Les paires de phrases “originale versus répétée” ont ensuite été analysées du point de vue du type et de l'ampleur de l'adaptation linguistique.
Il est apparu que, quand ils veulent corriger des erreurs de la machine, les utilisateurs modifient activement leur parole sur un axe d'hyper-articulation, réaction prédictible à leur identification de la machine avec un auditeur “à risques”.
Tant pour des taux d'erreurs bas que élevés, les modifications de durée étaient manifestes et concernaient aussi bien l'allongement des segments de parole qu'une augmentation relative importante du nombre et de la durée des silences.
En cas de taux d'erreurs élevés, la parole étaient également modifiée, incluant plus de traits phonologiques “hyper-clairs”, moins d'hésitations, et une fréquence fondamentale modifiée.
Un modèle à deux niveaux, appelé CHAM (Computer-elicited Hyperarticulate Adaptation Model : modèle automatique d'adaptation hyper-articulée) est proposé pour rendre compte de ces modifications de la parole de l'utilisateur lors de résolution interactive d'erreurs.
Dans le cadre de la théorie de l'évidence ou théorie de Dempster-Shafer, la fusion de données est basée sur la construction d'une masse de croyance unique résultant de la combinaison de plusieurs fonctions de masse issues de sources d'information distinctes.
Cette combinaison, appelée règle de combinaison de Dempster, ou somme orthogonale, possède différentes propriétés mathématiques intéressantes telle que la commutativité ou l'associativité.
Cependant, cette combinaison, à cause de l'étape de normalisation, gère mal le conflit existant entre différentes sources d'information.
La gestion du conflit n'est pas mineure, particulièrement lorsqu'il s'agit de fusionner de nombreuses sources d'information.
En effet, le conflit a tendance à croître avec le nombre de sources d'information à fusionner.
C'est pourquoi une stratégie de redistribution de ce conflit est indispensable.
L'idée de cet article est de définir un formalisme permettant de décrire une famille d'opérateurs de combinaison.
Pour cela, nous proposons un cadre générique afin d'unifier plusieurs opérateurs.
Nous présentons, au sein de ce cadre de travail, les opérateurs de combinaison classiques utilisés dans le cadre de la théorie de l'évidence.
Nous proposons ensuite d'autres opérateurs permettant une redistribution moins arbitraire de la masse conflictuelle sur les propositions.
Ces opérateurs seront testés et comparés aux opérateurs classiques sur des fonctions de croyance synthétiques et des données réelles.
L'article étudie l'émergence des qualifications langue morte et langue vivante en France au XVII e siècle.
Il s'appuie sur un dépouillement de sources métalinguistiques (grammaires, dictionnaires, recueils de remarques, traités divers).
Une première partie traite de l'évolution des termes pour qualifier le latin et montre comment on est passé du motif de l'altération et de la corruption, présent depuis le XVI e siècle, vers celui de langue morte.
Une deuxième partie montre comment le développement des théories de l'usage est concomitant de la valorisation nouvelle des langues vivantes.
Une troisième partie montre comment les dictionnaires de la fin du siècle enregistrent l'opposition langue vivante / langue morte, ouvrant la voie à une manière d'organiser la représentation des langues qui se diffusera dans le contexte scolaire.
Au travers de l'étude de ce paradigme émergent, c'est la question de la représentation des langues comme dotées de grammaires réglées ou comme vecteurs changeants de l'expression humaine qui est abordée.
Des mesures locales de distortions spectrales sont souvent utilisées pour évaluer la similitude (ou la distance) entre deux spectres à court terme.
Dans cette étude, nous comparons différences mesures de distortion spectrale, entre autres la mesure de distorsion d'Itakura-Saito (IS), celle par quotient de vraisemblance logarithmique, la mesure de distortion par quotient de vraisemblance (LR), la mesure de distorsion cepstrale (CEP) et deux mesures de distortion basées sur la perception—le quotient de vraisemblance pondéré (WLR) et la métrique à pente pondérée (WSM). On souhaite déterminer leur effet sur les performances d'un système de reconnaissance de mots isolés par programmation dynamique (DTW).
Deux modifications de la version de base de chaque mesure ont été également examinées—une distortion en fréquences selon une échelle en Bark et l'incorporation d'une information suprasegmentale—.
Toutes ces mesures et leurs mldifications ont été testées sur une base de données multi-locuteurs (4) enregistrés par téléphone.
Les résultats peuvent être résumés ainsi :
(1) toutes les mesures de distorsion sur base LPC ont fourni des résultats satisfaisants.
Les mesures de distorsion par quotient de vraisemblance logarithmique et par métrique à pente pondérée ont donné lieu aux meilleures performances de reconnaissance, tandis que la mesure d'Itakura-Saito a réalisé les performances les plus faibles;
(2) l'utilisation d'une information suprasegmentale a amélioré la reconnaissance, tandis que l'utilisation du grain et de la force sonore a dégradé les performances;
(3) la distorsion spectrale sur une échelle en Bark s'est révélée moins performante que son équivalent libre de toute distorsion, du moins sur notre base de données caractérisée par une largeur de bande limitée;
(4) la puissance du quotient de vraisemblance logarithmique pondérée est apparue réduite par rapport à son équivalent non pondéré.
La protéomique offre une approche puissante et complémentaire à la génomique.
Elle permet de répertorier et caractériser les protéines, de comparer leur niveau d'expression entre un état physiologique sain et malade par exemple.
En effet, le bruit du détecteur, le bruit électronique et chimique, la présence de peu de matériel protéique et enfin le bruit de la réduction des spectres (mauvais filtrage et/ou seuillage), tous ces bruits peuvent induire des Pics de Masses Parasites (PMP) et/ou supprimer des Pics de Masses Utiles (PMU) de faible intensité.
La conséquence immédiate est que la présence des PMP et l'absence des PMU seront utilisées au dépens de la qualité d'identification de la protéine.
Dans cet article, nous proposons un algorithme original éliminant les PMP, détectant et amplifiant ceux utiles.
Le principe du pré-traitement utilise une Analyse Multirésolution (AM) couplée à un seuillage basé sur la logique floue (seuillage flou multi-échelle), une amplification locale des PMU, et enfin une correction adaptative de la Ligne de Base (LB).
Le principe consiste à découper la bande passante fréquentielle de chaque spectre de masses en deux sous-bandes, une Basse Fréquence (BF), l'autre Haute Fréquence (HF), ensuite chaque sous-bande est à son tour découpée en deux sous-bandes etc.
Les sous-bandes HF sont seuillées selon le critère de minimisation de l'entropie floue de Shannon et amplifiées localement, la ligne de base est calculée automatiquement et soustraite du spectre reconstruit.
Pour évaluer la qualité de cet algorithme, nous présentons une comparaison des résultats obtenus par notre algorithme, et ceux fournis par le spectromètre MALDI-TOF (Matrix Assisted Laser Desorption/Ionisation-Time Of Flight), qui utilise le logiciel « DataExplorer » comme logiciel de réduction.
Les énoncés d'un discours peuvent varier selon deux dimensions essentielles : (i) leur position vis-à-vis de la hiérarchie des unités du discours, (ii) leur contenu, en termes d'actes du discours.
L'amélioration des systèmes de dialogue oral peut donc être substantielle s'il est convenablement tenu compte des caractéristiques structurelles du discours et de l'intention d'un énoncé.
Une question importante concerne donc la détection de ces éléments structurels.
L'analyse de monologues et de dialogues homme-homme montre que la prosodie est un indicateur fidèle de ces facteurs.
Cet article étudie dans une application de renseignements pour voyages touristiques ou d'affaires si les locuteurs utilisent la prosodie, pour souligner la structure du discours.
Plus particulièrement, il explore si les locuteurs indiquent (i) un changement de thème en marquant prosodiquement le premier énoncé d'une partie du discours, et (ii) si un énoncé donné exprime une demande d'information ou une correction dans un sous-dialogue.
Cette étude montre qu'en situation de dialogue homme-machine, la segmentation du discours et l'intention d'un énoncé peuvent être spécifiquement corrélées à des indices prosodiques, bien que les locuteurs puissent également avoir recours au choix d'une phraséologie particulière.
L'intégration d'indices prosodiques dans les systèmes de dialogue oral semble donc être une issue prometteuse.
Dans cet article deux méthodes qui permettent d'obtenir des critères pour l'évaluation perceptive de détecteurs de voisement ont été testées et comparées.
La première expérience consiste dans une tâche de “scanning perceptif” ; les sujets ont été priés de catégoriser les segments de parole de 30 ms comme “voisés”.
Les jugements obtenus dans cette tâche s'avèrent très fiables, bien que la confiance des jugements sur la fin des segments soit moins grande que celle des jugements sur le début.
Dans la seconde expérience, des versions différentes de parole resynthétisée ont été présentés à des auditeurs, qui ont été priés d'en apprécier la qualité relative;
les versions ne diffèrent que dans la position des transitions de voisement.
La qualité de la parole dans laquelle les segments voisés ont été déterminés par le détecteur de mélodie SIFT, mais raccourcis de 20 millisecondes des deux côtés, est considérée comme la plus acceptable.
On a constaté que l'acceptabilité des versions dans lesquelles les transitions de voisement ont été déterminées par les auditeurs à l'aide du “scanning perceptif” était à peine inférieure à celle dy “SIFT raccourci”.
Au début des années 90, la disponibilité du corpus TIMIT de transcription phonétique de parole dictée conduisit les chercheurs de AT&T à travailler sur l'inférence automatique des variations de prononciation.
Ce travail, brièvement résumé ici, utilisa des arbres de décision construit à partir de traits phonétiques et linguistiques, et fut appliqué à la reconaissance de la parole dictée dans le domaine du DARPA North American Business News.
Plus récemment, le corpus ICSI de transcription phonétique de parole spontanée fut mis au point à l'occasion des ateliers de travails d'été à l'université Johns Hopkins (WS) en 1996 et 1997.
Un groupe de travail du WS97 mit l'accent sur l'inférence de la prononciation à partir de ce corpus afin de l'utiliser dans le domaine de la parole spontanée téléphonique DoD Switchboard.
Nous décrivons plusieurs approches suivies à cette occasion.
Ces approches incluent (1) une approche analogue à celle de AT&T, (2) une autre approche inspirée par le travail accompli à WS96 et à CMU qui nécessite l'addition de variantes de prononciation d'une séquence d'un ou plusieurs mots du corpus (`multimots', avec des probabilités extraites de ce corpus) au dictionnaire de prononciation du système de reconnaissance de la parole, et (1+2) une approche hybride où un model d'arbres de décision est utilisé pour déterminer automatiquement la transcription phonétique d'un corpus bien plus long que celui d'ICSI, et où l'approche multimots est utilisée pour construire un dictionnaire de prononciation pour la reconnaissance automatique de la parole.
Nous présentons ici un problème de conception de circuit à demande maximale, qui met en jeu des demandes de routages élastiques aux temps de parcours et qui est dérivé d'un problème général de synthèse de réseaux de transports publics.
Ce problème implique la gestion sur un domaine de structure très irrégulière, d'une quantité induisant des temps de calcul très lourds.
Afin de gérer à la fois la problématique classique des optima locaux et celle particulière de ces coûts de calcul, nous proposons un schéma métaheuristique dit de poursuite, fondé sur l'utilisation d'un processus de réécriture polymorphe du problème initial.
Nous discutons diverses interprétations de ce schéma et présentons des expérimentations numériques.
Une analyse théorique du contrôle des composantes segmentales de la parole est proposée, qui s'appuie sur un certain nombre de résultats expérimentaux.
La mise en évidence de phénomènes d'équivalence motrice entre deux constrictions, dont les variations se compensent pour préserver la même fonction de transfert du conduit vocal, est interprétée comme une première corroboration de l'hypothèse selon laquelle le contrôle segmental est orienté vers des objectifs acoustiques ou audio-perceptifs.
Nous proposons que ces objectifs soient partiellement déterminés par des relations quantiques (appelées “effets de saturation”) entre les commandes motrices et les mouvements articulatoires, et entre l'articulation et le son.
Considérant que les temps de traitement qui seraient nécessaires à l'exploitation en boucle fermée du feedback auditif seraient trop longs, nous proposons que la réalisation des objectifs acoustiques soit assurée via un contrôle mettant en jeu un “modèle interne” robuste, qui serait appris par le locuteur au cours de la phase d'apprentissage de la parole, et qui rendrait compte des relations entre l'articulation et le son.
Des études menées sur des patients équipés d'implants cochléaires ou souffrant de neurinomes acoustiques bilatéraux ont fourni un certain nombre d'évidences en faveur d'un double rôle du feedback auditif chez l'adulte : (1) préserver et actualiser le modèle interne ; (2) prendre en compte l'environnement acoustique pour contribuer à assurer l'intelligibilité de la parole, en guidant des ajustements rapides des paramètres posturaux agissant sur le niveau sonore moyen de la parole, le débit d'élocution, et les inflexions de F0 et de niveau sonore associées à des facteurs prosodiques.
Dans cet article, nous proposons une nouvelle méthode de lissage des paramètres dans le cadre d'une architecture hybride pour la reconnaissance de la parole associant un réseau neuronal à délai TDNN et un modéle de Markov HMM.
Dans cette architecture, le réseau TDNN et le module HMM sont combinés en utilisant les activations fournies par la deuxième couche cachée du TDNN comme entrées du quantificateur vectoriel flou (FVQ).
L'algorithme HMM est adapté pour pouvoir traiter ces sorties FVQ.
Pour améliorer les performances de l'architecture hybride, une nouvelle méthode de lissage des paramètres est proposée.
Les valeurs moyennes des vecteurs d'activation de la deuxième couche cachée du TDNN modulaire sont utilisées pour générer une matrice de lissage dont sont extraites les probabilités p pondérées d'observation des symboles.
Avec cette approche, les résultats de simulation sur une base de données de mots isolés en Coréen, en mode indépendant du locuteur, montrent une réduction du taux d'erreur de 44.9% par rapport à une Méthode de lissage classique.
Cet article décrit une méthode dont le but est d'adapter un système de reconnaissance basé sur des HMM à densité continue (appris sur des paramètres cepstraux représentant de la parole normale) pour rendre le système plus robuste en présence de bruit.
Cette méthode, fondée sur la combinaison de modèles parallèles, permet de combiner les états appariés de bruit et de parole pour fournier un ensemble de paramères compensés.
Ceci est une amélioration par rapport à des méthodes de compensation de la moyenne cepstrale car cette méthode permet aussi d'adapter les variances, ce qui permit de prendre en compte des rapports signal sur bruit beaucoup plus faibles.
Nous montrons qu'elle donne de bons résultats pour un rapport signal sur bruit de 0 dB ou inférieur dans le cadre de bruits stationnaires et non-stationnaires.
De plus, pour des conditions de bruit relativement constantes, cette méthode n'ajoute aucun temps de calcul en phase de test.
Dans cet article, nous présentons une méthode générique d'extraction et de reconnaissance de champs numériques (numéro de téléphone, code postal, etc.) dans des courriers manuscrits non contraints.
La méthode d'extraction exploite la syntaxe des champs comme information a priori pour les localiser.
Un analyseur syntaxique à base de modèles de Markov filtre les séquences de composantes qui respectent la syntaxe d'un type de champ connu du système.
Nous montrons l'efficacité de la méthode sur une base de courriers manuscrits réels de type courrier entrant.
Dans cet article, nous présentons une étude exhaustive de la reconnaissance de la parole continue en espagnol.
Nous avons dévéloppé un modèle semi-continu et contextuel dependant des classes des phones.
En utilisant quatre phones, nous avons obtenu des réductions du taux des erreurs de reconnaissance qui sont approximativement équivalent à l'augmentation par pourcentage du nombre des paramètres, comparé avec le modèle semi-continu et contextuel qui sert de point de comparaison.
Nous montrons également que l'utilisation de la pause dans le système d'entraı̂nement et les prononciations multiples dans le vocabulaire contribuent à améliorer le taux de reconnaissance considérablement.
Les pauses actuelles des phrases d'entraı̂nement et le considération des effets d'assimilation améliorent la transcription dans des unitées qui dependent du contexte.
Des multiples possibilités de prononciation sont générées en utilisant des règles générales qui s'appliquent facilement à n'importe quel vocabulaire espagnol.
À l'aide de ces idées, nous avons réduit le taux des erreurs du système de base en plus du 30% dans un travail qui est parallèle à DARPA-RM, traduit à l'espagnol avec un vocabulaire de 979 mots.
Notre base de données contient quatre locuteurs avec 600 phrases d'entraı̂nement et 100 phrases de test par locuteur.
Toutes les expériences ont été réalisées avec une perplexité de 979 et même avec une perplexité un peu plus haute dans le cas des prononciations multiples pour pouvoir étudier la force du modèle en acoustique des systèmes sans contraintes grammaticales.
La reconnaissance de langue non-maternelle est un sujet qui prend de plus en plus d'importance dans les systèmes de reconnaissance automatique de la parole.
La majorité des systèmes existants sont spécialisés dans la reconnaissance de langue maternelle et par conséquent les résultats de reconnaissance sont insuffisants.
Une possibilité pour approcher ce problème est d'adapter les models acoustiques au nouvel orateur.
Un autre moyen important est de prendre en considération les variations de prononciation non-maternelle dans le dictionnaire du système.
La difficulté ici est la génération de variations de prononciation étrangère, plus particulièrement dans le cas oú plusieurs différents accents seraient à prendre en considération.
Les approches traditionnelles pour modeler la prononciation ont besoin soit de connaissance phonétique soit de banques de données de parole volumineuses en langue non-maternelle.
Ces deux méthodes sont trop coteuses particulièrement si une modélisation flexible de plusieurs accents est désirée.
Nous proposons donc l'utilisation de banque de données de parole en langue maternelle uniquement pour la génération de variations de prononciation en langue non-maternelle.
Nous avons combiné cette méthode avec la méthode d'adaptation de l'orateur MLLR.
Nos expériences ont montré que l'utilisation d'un dictionnaire étendu par les variantes produites par cette méthode peut améliorer les resultats de 5.2% word error rate.
En combinaison nous avons atteinds une amélioration de 18.2% en comparaison avec le dictionnaire normal.
Les terminologies métier sont au cœur de nombreuses applications : mémoire d'entreprise, gestion des savoir-faire et des compétences, veille technologique, recherche documentaire.
La construction de terminologies est une tâche difficile pour laquelle il n 'existe pas de méthode générale unanimement acceptée.
Nous verrons dans le cadre de cet article que le modèle ontologique pour la représentation et la signification des termes permet de garantir de telles propriétés.
Elle définie une « Terminologie Ontologique » entre la « Terminologie Textuelle » et la « Terminologie Conceptuelle » .
Nous présentons un cadre unificateur pour l'inférence exacte et approchée dans les réseaux bayésiens.
Ce cadre est utilisé dans “ProBT”, un moteur d'inférence généraliste permettant d'automatiser le raisonnement probabiliste et de faciliter la construction incrémentale des modèles.
Cet article n 'a pas pour objectif de présenter ProBT, mais de décrire ses algorithmes sous-jacents et principalement l'algorithme “Successive Restrictions Algorithm ” (SRA) pour l'inférence exacte et l'algorithme “Monte Carlo Simultaneous Estimation and Maximization ” (MCSEM) pour l'inférence approchée.
Ces expressions peuvent être “exactes ” ou bien “approchées ” et sont utilisées comme briques de base pour la construction incrémentale de modèles probabi- listes plus complexes.
Nous décrivons dans cet article une nouvelle méthode de classification d'images de télédétection.
Ces images contiennent des données volumineuses et complexes : les informations sont parfois bruitées, parfois erronées.
Avec notre approche, des règles de classification sont découvertes par un algorithme évolutif au lieu d'appliquer un algorithme de classification choisi a priori.
Durant le processus évolutif de découverte, les règles de classification sont créées en utilisant des images de télédétection brutes, une expertise contenue dans des images classifiées, et des statistiques sur les concepts thématiques à découvrir.
Les règles découvertes sont simples à interpréter, efficaces, robustes et résistantes au bruit.
Nous détaillons dans l'article l'algorithme évolutif conçu, ainsi que des validations sur des images de télédétection qui couvrent non seulement la zone urbaine de Strasbourg, mais aussi une zone de végétation dans la lagune de Venise.
La qualité segmentale reste un des domaines les plus importants à étudier et à améliorer et nous avons évalué régulièrement cet aspect.
Un test basé sur des VCV (voyelle-conspnne-voyelle) avec la consonne dans un contexte vocalique symétrique a été choisi.
Toutes les consonnes du suédois peuvent se trouver dans cette position.
Comme nous utilisons une réponse ouverte et des mots sans signification, toutes les confusions sont possibles.
Dans le système le plus récent, de nouvelles stratégies de contrôle des paramètres ont été explorées.
Auparavant, la plupart des paramètres de notre système avaient été spécifiés par des fonctions échelons lissées.
Dans le nouveau système, l'algorithme général de lissage a ét'e remplacé par un algorithme d'interpolation linéaire.
Le développement du nouveau système a nécessité une révision importante de l'ancien ensemble de règles.
Le taux d'erreurs a progressivement diminué : 41,7% (1983), 26,1% (1987) et 12,8% (1989).
Néanmoins, un effort important sur certaines consonnes est encore nécessaire pour améliorer davantage le système.
L'introduction d'une modélisation statistique par champs de Markov a récemment permis des avancées importantes dans nombre de problèmes classiques en analyse d'images.
Ces modèles sont généralement associés à des algorithmes d'optimisation globale par relaxation qui restent coûteux en temps de calcul dans certaines applications.
Or les techniques multigrilles, par ailleurs classiques en analyse numérique, peuvent conduire à des gains importants sur ce point.
Pour l'heure il n'existe cependant pas réellement de support théorique permettant d'associer de façon simple et efficace stratégie multigrille et modélisation markovienne.
Nous présentons dans cet article une approche multiéchelle de la modélisation markovienne qui est à la fois mathématiquement cohérente et facilement implantable.
Nous détaillons son application sur deux exemples d'analyse du mouvement dans une séquence d'images : la détection du mouvement et la mesure du mouvement.
Cela permet de mettre en évidence les apports de l'approche : accélération de la convergence et amélioration de l'estimée par rapport aux techniques markoviennes multirésolutions classiques.
Une méthode de recalage d'images multimodales 2D/3D entre Imagerie par Résonance Magnétique (3D) et à l'angiographie par rayons X (2D) est appliquée à la planification dosimétrique en radiochirurgie.
Cependant, l'utilisation d'algorithmes de recalage dans la phase de planification permet de simplifier les procédures d'imagerie en diminuant l'usage du cadre sans contraindre la planification.
Nous proposons ici les résultats préliminaires de l'application du recalage dans un contexte radiochirurgical par comparaison avec la méthode basée sur un repérage stéréotaxique qui constitue le gold standard.
Les résultats préliminaires obtenus lors de cette première phase de validation permettent de conclure sur la compatibilité de certaines séquences d'images IRM avec le recalage d'images tomographique et de projection.
Au jour d'aujourd'hui, il est possible de développer des systèmes de reconnaissance de la parole exhibant de très bonnes performances pour différentes tâches et langues.
Ceci résulte surtout de l'utilisation de puissantes approches statistiques de reconnaissance de formes, couplées avec la disponibilité de très grandes bases de données contenant des exemples de parole et de grammaire spécifiques à la tâche étudiée.
Il est cependant également connu que ces bonnes performances de reconnaissance ne peuvent être préservées lorsque les données de test ne “ressemblent” pas aux données d'entraı̂nement.
Une déformation du signal de parole apparait généralement comme une combinaison de variations acoustiques diverses, mais la forme exacte de cette distorsion est souvent inconnue et difficile à modéliser.
Une façon de réduire ces différences acoustiques consiste à ajuster les paramètres caractéristiques du signal de parole selon des modéles de la distorsion.
Une autre solution consiste à adapter les paramètres des modèles statistiques, par exemple les modèles de Markov cachès, de façon à ce que les modèles modifiés caractérisent mieux les caractéristiques du signal perturbé.
Dépendant des connaissances utilisées, cette famille de méthodes de compensation des vecteurs caractéristiques et des paramètres des modèles peut se subdiviser en trois classes, à savoir : (1) compensation basée sur 1′ entraı̂nement, (2) compensation aveugle, et (3) compensation basée sur la structure.
Cet article présente un aperçu des possibilités et limitations des approches de compensation et illustre leurs similarités et différences.
La relation entre adaptation et compensation sera également discutée.
La gamme des produits à base terminologique nécessaires pour répondre aux nouveaux besoins en matière de gestion de connaissances et de documents s'élargit considérablement.
Dans cet article, nous défendons l'idée que chaque type d'application passe par la mise au point de produits spécifiques à partir de textes et autres ressources du domaine.
Nous proposons alors un cadre méthodologique unificateur et un ensemble de logiciels dont l'utilisation ciblée facilite le recueil de connaissances et la modélisation de ressources adaptées.
Nous évoquons en parallèle les problèmes fondamentaux qui se posent et, lorsqu'elles existent, les solutions, techniques ou théoriques qui peuvent être envisagées.
Nous nous appuyons pour cela sur trois études de cas de construction de ressources terminologiques à partir de textes pour des applications très différentes.
Dans ce contexte, les noyaux sur graphes fournissent une approche intéressante en combinant les méthodes d'apprentissage automatique et la représentation naturelle des molécules par graphes.
Parmi les méthodes basées sur les noyaux sur graphes, la décomposition du graphe en sous-structures représente une importante famille de noyau.
Dans cet article, nous présentons deux extensions d'un noyau précédemment basé sur les sous-structures non étiquetées à l'énumération de sous structures étiquetées et à la prise en compte de l'information cyclique des molécules.
Nous proposons également des méthodes de sélection de variables permettant de pondérer un ensemble de sous-structures afin d'améliorer la précision de la prédiction.
On rend compte d'expériences de vérification du locuteur sur des chiffres isolés, de qualité téléphonique, utilisant des modèles de Markov cachés, discrets et semi-continus.
Une combinaison linéaire finie de variables aléatoires indépendantes de même loi non gaussienne ne peut être gaussienne.
Dans le cas d'une combinaison linéaire infinie, il est d'usage de conclure à la normalité par application systématique du théorème de la limite centrale.
La sortie des filtres Autorégressifs (AR) ou Autorégressifs à Moyenne Ajustée (ARMA) s'exprime sous la forme d'une somme infinie d'échantillons de l'entrée du modèle.
Nous étudions dans cet article la loi de la sortie de ces filtres et plus particulièrement leur « proximité » avec la loi gaussienne.
Cet article traite du problème de l'évaluation des systèmes de vérification du locuteur.
Son objectif est de présenter une façon concise et pertinente de caractériser ces systèmes.
Il suggère l'emploi d'un “profil de performance” prenant en compte les aspects importants d'un système en test : le taux réel de vérification, la capacité mémoire demandée, la ressemblance des locuteurs dans l'ensemble de test, la qualité de la parole et la durée des données de parole en phase d'apprentissage et de test.
Il présente des résultats montrant comment cet ensemble de mesures peut s'utiliser pour donner une représentation significative d'un système donné.
Il ne s'agit pas dans cet article d'imposer une méthode particulière d'évaluation, mais de souligner quelles sont les solutions avancées.
Il est en général nécessaire de préciser strictement les définitions de certains termes d'emploi courant avant de pouvoir comparer deux systèmes de vérification du locuteur de manière rigoureuse.
Un consensus sur l'adoption d'un ensemble de mesures standardisées, dans la ligne de celui proposé ici, permettrait d'augmenter significativement la validité de telles comparaisons.
Jean Miélot joue un rôle important dans le renouvellement de la connaissance de l'Antiquité à la fin du Moyen Âge, en relayant à la cour de Bourgogne plusieurs textes de l'humanisme italien et en traduisant Cicéron.
Il semblait donc opportun de s'intéresser tout spécialement au vocabulaire qu'il utilise pour traiter de l'ancienne Rome.
Conformément à une conception utilitaire de la traduction, moins le texte est explicitement historique, plus les transpositions sont nombreuses.
Le corpus ne dévoile aucune intention de créer ou d'améliorer le vocabulaire français en usage.
Tout au contraire, Miélot s'inscrit dans la tradition d'illustres prédécesseurs comme Simon de Hesdin et Nicolas de Gonesse.
Parmi les données utilisées en identification juridique les empreintes digitales et les données génétiques semblent présenter un degré de fiabilité élevé.
Il n'en n'est rien : un enregistrement de parole n'est pas une trace laissée sur une surface au contact d'une partie du corps d'un individu, ni un prélèvement direct opéré sur celui-ci.
on comprend tout l'intérêt que pourraient offrir des techniques fiables d'identification du locuteur.
En France il n'existe pas d'experts auprès de tribunaux répertoriés comme spécialistes d'identification vocale.
Et pourtant certains “experts” prétendent identifier de façon certaine la voix d'un suspect et certains magistrats accordent, en France, beaucoup d'importance à ces analyses.
L'auteur précise les conditions dans lesquelles sont faites, en France, les expertises vocales dans le cadre d'une procédure pénale ; tente de cerner les limites de ce protocole, les difficultés (et impossibilités) d'une évaluation probabiliste;
présente un rappel historique des discussions et prises de position de la communauté parole française depuis 1990;
avance enfin des éléments de réflexion et des propositions qui pourraient être discutées par les spécialistes de parole en collaboration avec la police, la gendarmerie et la magistrature, au niveau national, européen (et international) pour faire avancer la recherche de preuve dans un cadre scientifique et aboutir à des protocoles bien balisés.
Cet article présente l'approche suivie dans le projet ANR StaRAC et en résume les résultats principaux.
L'objectif était de reconsidérer le concept de stationnarité dans le but de lui donner une forme opérationnelle, se prêtant à une interprétation relative à une échelle d'observation et permettant de le tester dans un sens statistique précis grâce à l'emploi de substituts temps-fréquence, ainsi que d'en fournir diverses extensions, en particulier au-delà de l'invariance en translation.
Les besoins en codage de parole à débit variable existent pour un nombre sans cesse croissant d'applications telles que la visioconférence, l'audioconférence, les systèmes à multiplication de paquets (PCMS) et les communications avec les mobiles.
Cet article étend le concept des codes imbriqués au codage CELP et VSELP à débit variable.
Les gains orthogonalisés résultant permettent de déduire un nouvel algorithme d'optimisation des dictionnaires successifs dans le cas du codage CELP/VSELP multi-étages à codes imbriqués.
Finalement les résultats subjectifs présentés montrent que les codeurs CELP/VSELP à codes imbriqués fournissent, aux débits de 24 et 32 kbit/s, une parole codée en bande élargie équivalente à celle du SB/ADPCM G722 à codes imbriqués fonctionnant à 56 et 64 kbit/s.
Cet article passe en revue et évalue trois modèles récents de l'apprentissage de la lecture par stades successifs (Marsh, Friedman, Welch, & Desberg ; Frith : Seymour). Il discute également des liens entre la “conscience phonologique” et la lecture, en particulier de la direction d'un éventuel lien causal.
Les données d'une étude longitudinale de l'acquisition de la lecture sont exposées.
Cette étude comprend des évaluations des capacités phonologiques des enfants avant qu'ils apprennent à lire.
Les résultats suggèrent que (a) même si on se représente l'apprentissage de la lecture comme des stades successifs, tous les enfants ne passent pas par la même succession de stades ; (b) la “conscience phonologique” et l'apprentissage de la lecture entretiennent des relations causales réciproques et interactives, et non par une relation unidirectionnelle ; et (c) chez les enfants qui possèdent les meilleures capacités phonologiques. ces dernières peuvent jouer un rôle dans le tout premier stade de l'acquisition de la lecture.
En conséquence, il est d'avancer que le premier stade dans l'apprentissage de la lecture implique toujours des procédures non-phonologiques telles que le traitement dit “logographique”.
Le but de ce travail est de définir un cadre commun incluant un ensemble de critères d'apprentissage discriminant et de méthodes d'optimisation pour la reconnaissance de la parole continue.
Nous introduisons un critère discriminant fondé sur le rapport entre la vraissemblance des modèles corrects et concurrents.
Ce critère général conduit à définir des critères spécifiques par le choix des séquences de mots en concurrence et par celui de la méthode de lissage.
Des comparaisons analytiques et expérimentales sont menées pour les critères d'information mutuelle maximale (MMI) et d'erreur de classification minimum (MCE) ainsi que pour leur optimisation par la déscente de gradient (GD) et l'algorithme Baum étendu (EB).
Une méthode de reconnaissance restrictive fondée sur une recherche arborescente est proposée pour réduire la complexité de l'apprentissage discriminant pour les grands vocabulaires.
De plus une méthode efficace a été introduite dans l'apprentissage MCE, utilisant des graphes de mots pour le calcul des statistiques discriminantes.
Des expériences de reconnaissance de parole continue ont été menées sur le corpus ARPA Wall Street Journal (WSJ) (vocabulaire de 5k mots) ainsi que pour la reconnaissance de chiffres connectés sur les corpus TI digit string (anglais américain) et Sie Till (allemand par téléphone).
Les résultats analytiques et expérimentaux n'ont pas mis en évidence des différences significatives entre les méthodes d'optimisation EB et GD pour le critère MMI.
Pour des modèles acoustiques de faible complexité, l'apprentissage MCE a fourni des résultats significativement meilleurs que l'apprentissage MMI.
Les résultats de reconnaissance pour l'apprentissage MMI avec un grand vocabulaire sur le corpus WSJ montrent une forte dépendance à la taille du contexte pour le modèle de langage utilisé pendant l'apprentissage.
Les meilleurs résultats ont été obtenus pour un modèle de langage unigramme avec l'apprentissage MMI.
Aucune corrélation significative n'a été observée entre le choix du modèle de langage pour l'apprentissage et celui pour la reconnaissance.
Dans ce but, des demi-syllables ont été utilisées comme unités de segmentation et les suites de consonnes contenues dans celles-ci comme unités de décision pour la classification.
Comparée au grandd nombre de demi-syllabes différences, l'utilisation de suites de consonnes réduit considérablement l'inventaire des classes.
Trois expériences utilisant des mots allemands prononcés isolément ont été réalisées pour tester cette méthode.
Dans la troisiéme expérience, un systéme complet de reconnaissance de 1000 mots a été développé, lequel réalise la segmentation, la classification de suites de consonnes et de voyelles ainsi que la correction d'erreurs de reconnaissance à l'aide d'un dictionnaire phonétique.
La segmentation en demi-syllables s'est avérée appropriée en particulier pour le traitment d'un dictionnaire d'un grand vocabulaire.
De nombreux problèmes de traitement automatique des langues (TAL) se ramènent à des problèmes de classification.
La complexité de la langue naturelle fait qu'il est difficile d'isoler les attributs discriminants pour une tâche donnée, que la fiabilité des valeurs associées à ces attributs est souvent faible et varie sur des corpus de domaines différents.
Cet article défend l'idée que le formalisme des réseaux bayésiens est adapté à la classification de données décrites par de tels attributs.
Nous avons estimé le bénéfice apporté par un classifieur bayésien sur une application réelle du TAL : la reconnaissance des pronoms anglais 'it' impersonnels et anaphoriques.
Ce texte, effectuant une synthèse de divers travaux de l'auteur, présente une méthodologie de modélisation des signaux non stationnaires, au moyen d'une classe de modèles autorégressifs à moyenne ajustée (ARMA).
La non-stationnarité du signal y est caractérisée par l'évolution temporelle des coefficients du modèle : ceux-ci s'expriment comme combinaisons linéaires d'une famille de fonctions connues, suivant en cela les idées dues à Rao, Mendel, puis Liporace.
Il sera montré que cette hypothèse conduit à un ensemble d'estimateurs dont on détaillera plusieurs variantes, pour les modèles autorégressifs, puis les modèles à moyenne ajustée, enfin pour les filtres en treillis, et leur paramétrisation en terme de fonctions d'aire logarithmiques.
La synthèse de parole est une des applications où de tels modèles s'avèrent performants et une description de cette application concluera l'article.
Ce papier décrit le serveur téléphonique interactif à commande vocale MAIRIEVOX, développé par le CNET.
Enfin, les développements industriels français dérivés de ce système sont rapidement décrits.
Nous présentons les définitions et synthèses de processus stochastiques respectant des lois d'échelles voilées, qui s'écartent de façon contrôlée d'un comportement en loi de puissance.
Nous définissons des bruit, mouvement et marche aléatoire issus de cascades infiniment divisibles (IDC) voilées.
Nous étudions analytiquement le comportement des moments des accroissements de ces processus à travers les échelles.
Ces résultats théoriques sont illustrés sur l'exemple d'une cascade log-Normale voilée.
Les algorithmes de synthèse et les fonctions Matlab utilisés sont disponibles sur nos pages web.
Les formes participiales des temps composés font l'objet d'un traitement particulier chez les grammairiens des différentes langues romanes depuis la Renaissance.
Les formes participiales posent en effet un problème assez spécifique, dès lors que dans ces vernaculaires elles présentent des propriétés incompatibles avec la classe du participe telle que la définit la tradition latine.
Certains proposent de recatégoriser ces formes en leur affectant une désignation ou une nouvelle classe avec des propriétés plus adaptées.
La mise en série des options théoriques relevées dans un corpus étendu (XV e-XVIII e s.) tend à souligner l'importance de cette manière d'appréhender les données qui mettent à l'épreuve le modèle descriptif latin.
Par ailleurs, la récurrence et la commensurabilité des solutions théoriques dans diverses traditions montrent l'intérêt de sortir du cadre des histoires nationales.
Nous présentons une évaluation de la durée des simulations de la machine de Boltzmann synchrone en phase de relaxation sur des stations de travail usuelles et des supercalculateurs vectoriels.
Nous étudions l'impact des paramètres les plus importants : la topologie du réseau, la nature des connexions entre couches, le codage des états, la précision des calculs et l'architecture des machines utilisées.
Nous proposons une méthode pour prédire l'ordre de grandeur des performances des différentes machines.
Enfin nous montrons que ces performances permettent d'envisager à court terme l'utilisation d'une machine de Boltzmann de taille moyenne dans des applications pratiques.
Leséchecs de compréhension des aphasiques agrammatiques ainsi que leurs difficultésáconstruire des phrases ontétéattribuésáun déficit sous-jacent impliquant la recherche de la structure syntaxique.
Dans cetteétude les performances de quatre sujets agrammatiques dans une taˆche de jugements grammaticaux montrent que ces sujets font preuve d'une remarquable sensibilitépour l'information structurell.
Ces résultats indiquent que la connaissance syntaxique n'est pas atteinte chez l'agrammatique et suggérent que les troubles de compréhension de phrase ne reflétent pas une perte dans la capacitéàretrouver la structure syntaxique.
L'interprétation selon laquelle les déficits seraient dusàunéchec dans l'utilisation de l'information transmise par la classe fermée (mots foncteurs) du vocabulaire est remise en cause.
D'autres interprétations sont proposées pour rendre compte des problémes de comprehension des agrammatiques.
Cette contribution présente quatre tests visant à évaluer le module de génération de parole utilisé dans le système de dialogue vocal INSPIRE.
Ce système a pour objectif le contrôle vocal des appareils domestiques d'une maison “intelligente”, soit sur place, par un ensemble de microphones, soit à distance, par téléphone.
Le but des expériences est de quantifier l'impact de trois classes de facteurs sur la qualité du système : la première classe caractérise le comportement du système (sa voix et sa “personnalité”), la deuxième porte sur l'environnement physique (interface acoustique, bruit de fond, voie de transmission) et la troisième est liée à la tâche effectuée par l'utilisateur (situation d'écoute ou d'interaction, effet des tâches parallèles).
Les résultats obtenus montrent que l'impact des deux premières classes de facteurs est important, mais pas celui de la troisième.
Les raisons de cette observation sont discutées et prises en compte pour la finalisation du prototype du système.
Le langage de programmation Prolog III est une extension de Prolog au niveau de ce qu'il a de plus fondamental, le mécanisme d'unification.
Il intègre dans ce mécanisme un traitement fin des arbres et des listes, un traitement numérique et un traitement du calcul propositionnel complet.
Puis il illustre les possibilités accrues du langage à travers des exemples variés.
Cet article propose une présentation des systèmes de classeurs qui sont des outils destinés à l'apprentissage d'interactions.
Il en existe de nombreuses variantes qui sont présentées par le biais d'une approche chronologique introduisant les nouveaux défis ou concepts abordés par chaque modèle.
Nous décrivons les concepts de base que sont les algorithmes génétiques et l'apprentissage par renforcement.
L'article présente ensuite les systèmes de base tels que le ZCS ou le XCS, les systèmes à anticipation, ainsi que les classeurs hiérarchiques ou hétérogènes.
Dans cet article, nous suggérons que des modèles de production de la parole fortement structurés pourront contribuer significativement à la réussite future des modèles de reconnaissance automatique de la parole, limités en ce moment par les faiblesses de la base théorique de la technologie actuelle.
Nous suggérons en conclusion que l'interaction entre les domaines de la production et de la reconnaissance de la parole peut être particulièrement efficace si l'on intègre les modèles de production dans la stratégie d'analyse-synthèse probabiliste, utilisée déjà depuis longtemps en reconnaissance de la parole.
Cette recherche porte sur les changements temporels survenant dans la chaîne parlée lorsqu'un lecteur réalise un accent d'insistance (accent didactique) sur des mots-cibles inclus dans le texte, et analyse les formes prosodiques structurées qui en résultent.
Les trois expériences présentées tentent de répondre aux questions suivantes :
(1) Les changements temporels sont-ils limités à l'entourage immédiat des cibles (dernière syllabe et pause précédant la cible, durée d'énonciation de la cible, pause suivant la cible) ?
(2) Ces divers changements temporels sont-ils corrélés ?
(3) La nature même des marques typographiques permettant de repérer les cibles produit-elle un effet sur les modifications temporelles réalisées ?
(4) L'accent didactique produit varie-t-il avec l'importance relative de la cible dans la signification du texte ?
Deux locuteurs ont lu 16 textes différents présentés chacun deux fois, avec et sans mots-cibles signalés par un changement de typographie.
Les valeurs des indices mesurés sur et avant la cible sont inter-corrélés, mais la valeur de la pause suivant la cible est indépendante.
Les deux locuteurs sont insensibles aux déterminants typographiques et sémantiques.
Les résultats témoignent des contraintes cognitives qui pèsent sur les décisions du locuteur lors de la lecture continue de textes.
L'objet de cet article consiste en un état de l'art des réseaux de neurones temporels et d'une comparaison de trois réseaux de neurones récurrents les plus représentatifs pour des applications de surveillance dynamique et de pronostic.
Les critères de sélection de ces réseaux se situent à deux niveaux : temporel et architectural.
Suite à l'application de ces critères, trois réseaux récurrents se distinguent : le RRBF, le R2BF et le DGNN.
Des tests utilisant un benchmark de surveillance dynamique et un benchmark de pronostic nous permettent d'évaluer les performances des trois réseaux temporels en termes de temps de calcul et de capacité de traitement.
Différents niveaux d'affects sont exprimés dans différents niveaux du traitement de la parole : les expressions des émotions, relevant d'un contrôle déclenché involontairement, les expressions des attitudes et des intentions du locuteur et les stratégies expressives métalinguistiques.
C-Clone (Communicative Clone) est présenté comme une architecture cognitive interactive de la communication expressive.
Un sous-ensemble auto-annoté d'expressions émotionnelles a permis de valider la modélisation de la prosodie affective en contours gradients.
Des expériences perceptives indiquent en outre qu 'aucune dimension acoustique prosodique n 'est spécifique à une valeur d'émotion.
Ce papier présente un formalisme relationnel pour la classification topographique de données qualitatives (ou catégorielles), se présentant sous forme d'une matrice binaire ou d'une somme de matrices binaires.
L'algorithme de classification relationnelle topographique proposé s'inspire du modèle de Kohonen (conservation de l'ordre topologique) et utilise le formalisme de l'analyse relationnelle en optimisant un critère défini à partir du critère de Condorcet.
Il s'agit d'un algorithme hybride « Batch-CRT » , qui se comporte linéairement par rapport à la taille des données, et permet simultanément une classification des données et une visualisation des classes découvertes sur une grille bidimensionnelle en préservant l'ordre topologique a priori des données.
L'approche proposée a été validée sur plusieurs bases de données et les résultats expérimentaux ont montré des performances très prometteuses.
Le traitement dynamique de la parole dans le système auditif s'effectue apparemment par analyse de la forme spectrale et détection en parallèle du changement spectral.
Cette formulation inspirée de Chistovich et al. (1982), constitue une hypothèse de base à partir de laquelle nous examinons le rôle auditif des diphtongues en tant qu'exemple de changement temporel du spectre.
Il y a un désaccord très net dans la littérature en ce qui concerne les poids perceptifs relatifs des différents éléments d'une diphtongue.
L'auditeur prête-t-il attention aux points extrêmes d'une diphtongue ou à son taux de changement spectral ?
Cet article fournit des données supplémentaires pour essayer de résoudre ce conflit.
Nos résultats montrent qu'un simple modéle cumulatif du traitement auditif du signal de parole est inadéquat.
Il en va de même pour les modèles qui utilisent la vitesse de changement spectral comme déclencheur du processus de comparaison spectrale.
Par contre, le rôle du changement spectral dans les diphtongues semble consister en (1) un drapeau qui commande un accrïssement supplémentaire de pondération perceptive en vertu du fait précisément qu'un certain changement intervient (2) un repère pour localiser les régions temporelles adjacentes qui sont importantes et qui devraient être échantillonnées de manière plus dense.
Cet article décrit un modèle pôle-zéro (ARMA) de la parole utilisant un algorithme de filtrage transversal rapide (FTF) basé sur les moindres carrés récursifs (RLS).
Cet algorithme ARMA FTF permet d'estimer un signal d'excitation inconnu, et utilise celui-ci pour déterminer les paramètres du modèle pôle-zéro.
L'algorithme est obtenu au moyen de projections géométriques.
Cette approche permet d'accéder au fonctionnement de l'algorithme et d'interpréter utilement les différents filtres qui le composent.
Nous donnons une évaluation des performances de l'algorithme en l'appliquant à l'estimation du spectre de signaux de parole synthétiques et naturels.
L'algorithme est capable de représenter avec précision les pics et les vallées du spectre du signal de parole et nécessite moins de temps de calcul que les filtres maillés RLS et que l'algorithme ARMA FTF développé par Ardalan et Faber (1988).
De plus, l'algorithme peut également être utilisé pour d'autres problèmes de traitement de signal pour lesquels le signal d'entrée est inconnu.
DragonDictate est un système de reconnaissance de la parole développé par Dragon Systems. Pour l'entraînement d'un tel système, une approche efficace consiste à utiliser des “phonèmes-en-contexte”, c.à.d. des triphones accompagnés d'un code concernant leur allongement éventuel devant une pause (PIC).
A son tour, chaque PIC est représenté comme une suite de 1 à 6 élément phonétiques (PEL).
Pour chaque phonème, il peut y avoir des milliers de PIC différent, mais les PEL sont tout au plus au nombre de 63.
Initialement, tous les PIC et PEL sont entraînés à partir d'une base de données d'environ 16.000 mots enregistrés.
Quand un nouveau locuteur utilise le système de reconnaissance, chaque mot reconnu sert immédiatement à adapter les PEL dans sa chaîne de Markov cachée.
Après la reconnaissance d'environ 1.000 mots, la plupart des PEL se trouvent adaptés au nouveau locuteur. Ainsi, même les modèles de mots qu'il n'a jamais prononcés sont adaptés au locuteur.
Nous avons essayé le système de reconnaissance avec 2 textes, qui diffèrent beaucoup sur le plan du vocabulaire et du style.
Ils ont été lus par 3 locuteurs : un locuteur de référence, un nouveau locuteur masculin et un nouveau locuteur féminin.
Après une phase d'adaptation d'approximativement 1.500 mots, le rendement pour les trois était meilleur que celui obtenu par le locuteur de référence avec des modèles non adaptés.
Avec un vocabulaire actif de 25.000 mots, 86% des mots étaient reconnus correctement ; en plus 8% des mots figurant sur une liste de choix de 8 mots.
Dans cet article, des outils issus de la théorie de l'estimation sont proposés pour permettre l'évaluation des performances et le dimensionnement de systèmes expérimentaux.
Ceux-ci ont été appliqués au système WACS (Whales Anti Collision System), système de localisation passive des cachalots vocalisants ou non afin de contribuer à réduire les collisions avec les navires.
Basée sur des outils théoriques, l'approche proposée s'attache à se rapprocher au mieux de la réalité par les hypothèses effectuées.
Sans monitorage de l'environnement acoustique, il est démontré que WACS est un bon outil pour le biologiste mais pourraît ne pas être assez précis pour être inséré au sein d'un réseau anti-collision.
Dans cet article une nouvelle approche de commande prédictive non linéaire est proposée pour un robot marcheur bipède à cinq segments sous actionné.
La caractéristique principale dans la stratégie proposée est d'utiliser l'optimisation en-ligne pour mettre à jour les trajectoires à poursuivre sur les variables complètement commandables (coordonnées actionnées) dans le but d'améliorer le comportement et la stabilité des variables indirectement commandées (coordonnées non actionnés).
L'approche proposée est illustrée à travers différents scénarios de simulations.
La robustesse, quant à elle, est analysée par rapport à des incertitudes dans le modèle du robot, et des irrégularités dans le sol.
Nous passons en revue l'utilisation de l'algorithme “Time-Domain Pitch Synchronous OverLap-Add (TD-PSOLA)” dans le cadre de la synthèse à partir du texte.
Nous en établissons les inconvénients et en déduisons trois conditions sur la base de données de parole.
Afin de mieux y satisfaire, nous développons plus avant une technique de re-synthèse de haute qualité qui tire parti du modèle “Multi-Band Excited (MBE)”.
Un des effets importants de cette opération est de rendre automatique l'operation de marquage de pitch.
Elle permet également d'introduire un bloc d'interpolation temporelle dans la chaîne de traitement PSOLA.
La technique de synthèse finale, appelée “Multi-Band Re-synthesis Pitch Synchronous OverLap Add (MBR-PSOLA)”, est ainsi pourvue de réelles possibilités de concaténation entre segments, sans que cela ne nuise à la simplicité initiale de l'algorithme TD-PSOLA.
Cet article présente le développement de modèles de Markov cachés à densités continues partagées (SC-HMM) précis pour la modélisation acoustique d'un système de reconnaissance de parole continue de grand vocabulaire, indépendante du locuteur.
Deux méthodes sont décrites afin d'améliorer substantiellement l'efficacité du calcul des probabilités d'émission pour les SC-HMM.
En premier lieu, on a créé des SC-HMM réduits, pour lesquels chaque état ne partage pas toutes les densités gaussiennes, mais seulement celles qui sont importantes pour cet état.
Il se trouve que le nombre moyen de gaussiennes utilisées par état peut être réduit jusqu'a 70, sur un ensemble de 10 000 gaussiennes.
Ensuite, un nouvel algorithme de sélection scalaire est présenté, qui réduit – sans détérioration des performances de reconnaissance – à 5% le nombre de gaussiennes qui doivent être calculées sur l'ensemble des 10 000 gaussiennes.
Par ailleurs, le concept de modélisation dépendante du contexte avec regroupement d'état basé sur des arbres de décision phonétiques est adapté pour les SC-HMM.
Plus précisément, un critère de division de nœud compatible avec les SC-HMM est introduit qui est basé sur une mesure de distance entre les mélanges de fonctions pdfs gaussiennes utilisés pour la modélisation des états des SC-HMM.
Ceci contraste avec d'autres critères proposés dans la littérature qui se basent sur des fonctions pdfs simplifiées pour résoudre la complexité des calculs.
Pour la tâche ARPA Resource Management, l'utilisation de notre critère offre une réduction relative des taux d'erreur de 8% par rapport à deux critères basés sur des fonctions pdfs simplifiées.
Les réseaux neuronaux sont de bons instruments pour la reconnaissance automatique de la parole non seulement parce qu'ils fournissent un mode de reconnaissance non linéaire mais aussi parce que leur architecture permet d'incorporer et d'exploiter notre connaissance actuelle de la parole.
Dans la lère partie, nous soulignons que la définition du problème de la reconnaissance de la parole implique que la connaissance préalable de méthodes d'analyse linguistique est essentielle à sa résolution et suggère que la faible exploitation actuelle de cette connaissance est une conséquence des architectures contemporaines de reconnaissance des formes.
Nous critiquons l'accent mis sur les algorithmes de reconnaissance des formes syntaxiques opérant au niveau du segment phonétique.
La 2e partie démontre qu'une architecture en réseau pour le lexique fournit un mécanisme qui incorpore et exploite une gamme d'analyses phonologiques.
De plus, par une séparation explicite entre les représentations phonologiques et phonétiques, il est possible de construire une conmposante phonétique d'entrée sur la base de principes relevant purement de la reconnaissance des formes.
Par la normalisation du locuteur et de l'environnement, la composante phonétique peut être interfacée au réseau lexical pour aboutir à une architecture complète de reconnaissance qui évite les compromis lors de l'exploitation de la connaissance sur la parole.
Cet article présente la méthodologie sous-jacente à la conception de la procédure de vérification en virgule flottante du codeur de parole prédictif excité par codes à faible retard (LD-CELP), récemment retenu par le CCITT (recommandation G.728).
Cette procédure est basée sur une spécification nun définie au bit près, ce qui diffère des procédures de vérification utilisées jusqu'ici pour les codeurs de parole au CCITT.
Cette approche autorise one plus grande liberté pour implanter l'algorithme, et permettra des réalisations plus efficaces sur des matériels variés.
Cependant, cette flexibilité entra^ine aussi que des implantations différentes produiront des réponses légèrement différentes aux séquences de test.
Pour remédier à cet inconvénient, des mesures explicites et objectives de ces variations sont utilisées dann la procédure de vérification.
Ces mesures sont de simples rapports signal à bruit (RSB), pondérés ou non.
En plus de ces mesures objectives, un certain nombre de restrictions ont dû ^etre introduites dann la conception de séquences de test.
Cependant, en dépit de ces restrictions sur ces entrées, un ensemble de séquences de test assurant one couverture satisfaisante de l'algorithme LD-CELP et de l'espace des états associé a pu être trouvé par des résultats expérimentaux.
Des expériences de validation décrites ici montrent que ces séquences possèdent one capacité satisfaisante de détection des erreurs.
La discussion finale conclut que la procédure de vérification proposée constitue un outil réaliste pour l'implanteur.
La caractérisation est une tâche supervisée de fouille de données qui permet de résumer de manière succincte et concise un ensemble de données.
Cette tâche est intéressante dans la mesure où elle ne nécessite pas de contre exemples.
Nous proposons un cadre général pour la caractérisation d'un ensemble d'objets, appelé ensemble "cible", en nous basant non seulement sur leurs propriétés propres mais aussi sur les propriétés des objets qui leur sont liés.
Selon le type des objets considérés, différents liens peuvent être envisagés.
Dans le cas de bases de données géographiques, ce sont les relations spatiales qui expriment des liens entre objets géoréférencés.
Nous proposons des algorithmes d'extraction de règles de caractérisation et nous montrons comment nous les avons appliqués à des données géographiques réelles fournies par le BRGM.
Un court siècle a transformé la France urbaine.
Un nouveau genre de spectacles, la revue théâtrale locale, a accompagné cette mutation d'environ 1855 à 1930.
D'anciens citadins ne reconnaissaient plus un cadre de vie que découvraient simultanément de jeunes ruraux.
Le développement d'une industrie du spectacle facilita sans doute l'intégration culturelle.
Rire - ou sourire - en ville contribua à une coexistence qui devint identité, sans pour autant faire oublier les différences sociales.
Nous proposons un modèle de représentation statistique de la structure conceptuelle d'un sous-ensemble parlé du langage naturel.
Le modèle est utilisé pour segmenter une phrase en propositions et pour étiqueter celles-ci avec des relations de concepts (ou cas).
Le modèle est entraîné au moyen d'un corpus de phrases dont la transcription est annotée.
Un système de compréhension est construit sur base de ce modèle permettant une entrée vocale sans contraintes dans le cadre d'une tâche de consultation de banque de données.
Le but de cet article est de donner des détails et des résultats concernant le nouveau modèle de représentation du langage.
Dans ce but, le modèle a été implémenté et testé avec un texte en entrée.
Les paramètres du modèle ayant été estimés par l'intermédiaire de 547 phrases d'entraînement, les résultats d'un test effectué sur 148 phrases ont montré que presque 97% des concepts ont été correctement détectés et étiquetés par la procédure automatique d'étiquetage de concepts ; finalement, 65% des phrases ont été correctement comprises.
Plusieurs études ont récemment été menées sur l'attribution de compétences cognitives et de caractéristiques psychologiques à des agents artificiels.
Cependant ces études reposent sur des approches procédurales, difficiles à analyser, et elles se focalisent sur des phénomènes particuliers au lieu de couvrir une partie significative du domaine psychologique des humains.
Nous présentons ici une approche systématique de l'implémentation du principe stipulant que les traits de personnalité ont une influence potentielle et effective sur le processus de décision rationnelle d'agents cognitifs.
L'étude de la parole et de l'émotion, partie du stade de la recherche exploratrice, en arrive maintenant au stade qui est celui d'applications importantes, notamment dans l'interaction homme–machine.
Le progrès en ce domaine dépend étroitment du développement de bases de données appropriées.
Cet article aborde quatre points principaux qui méritent notre attention à ce sujet : l'étendue, l'authenticité, le contexte et les termes de description.
L'article montre comment trois récents projets importants (celui de Reading–Leeds, celui de Belfast, et celui de CREST–ESP) ont relevé le défi posé par la construction de bases de données appropriées.
A partir de ces trois projets, ainsi que d'autres travaux, les auteurs présentment un bilan des outils et méthodes utilisés, identifient les problèmes qui y sont associés, et indiquent la direction dans laquelle devraient s'orienter les recherches à venir.
Cet article traite du problème de la reconnaissance en temps-réel de la parole en milieu bruyant.
Les nombreux travaux qui ont été effectués dans ce domaine n'ont pour l'instant abouti qu'à des succès limités.
La raison essentielle en est que les performances des algorithmes de reconnaissance sont prédites à partir d'hypothèses sur les conditions d'environnement pour lesquelles les algorithmes sont conçus et implémentés.
Ce système prend en compte à la fois les effets du bruit additif sur le signal de parole émis et les effets sur le système de production de parole lui-même.
Les évaluations montrent une amélioration moyenne du taux de reconnaissance de +17.28% dans 11 conditions d'émission de parole en milieu bruyant.
En reconnaissance de la parole, le signal est habituellement représenté par un ensemble de séquences temporelles de paramètres spectraux (TSSPs) qui modélisent l'évolution temporelle, trame par trame, de l'enveloppe spectrale.
Ces séquences sont ensuite filtrées soit pour les rendre plus robustes aux conditions de l'environnement ou pour calculer des paramètres différentiels (indices dynamiques) qui améliorent la discrimination.
Dans cet article, nous appliquons une analyse fréquentielle aux TSSPs afin de fournir un cadre d'interprétation pour les divers types de filtres de paramètres utilisés jusqu'à présent.
Ainsi, l'analyse du spectre moyen à long-terme des séquences correctement filtrées révèle un effet combiné de l'égalisation et de la sélection de bande qui fournit des informations intéressantes sur le filtrage TSSP.
Nous montrons également que, quand des paramètres différentiels supplémentaires ne sont pas utilisés, le taux de reconnaissance peut être amélioré même pour de la parole non bruitée, juste en filtrant les TSSPs de manière appropriée.
Pour confirmer cette assertion, un certain nombre de résultats expérimentaux sont fournis, en utilisant tant des modèles de mots que des modèles phonétiques.
Les filtres empiriquement optimaux atténuent la bande des basses fréquences et accentuent celle des hautes fréquences, de sorte que le pic du spectre moyen à long-terme de la sortie de ces filtres se situe aux alentours de la vitesse syllabique moyenne de la base de données utilisée (3 Hz, environ).
À ces deux types d'erreurs peuvent être associés deux types de rejet : le rejet d'ambiguïté et le rejet d'ignorance.
Par contre, les approches qui agissent par modélisation sont par nature mieux adaptées à ce second type de rejet, mais ne s'avèrent que peu discriminantes.
Ainsi, nous proposons de combiner les deux types d'approche au sein d'un système de classification à deux niveaux de décision.
Au premier niveau, une approche par modélisation sera utilisée pour rejeter les données aberrantes et pré-estimer les probabilités a posteriori.
En outre, cette combinaison présente l'avantage de réduire la complexité de calcul associée à la prise de décision des SVM.
Ainsi, les résultats obtenus sur un problème classique de reconnaissance d'images de chiffres manuscrits isolés ont montré qu'il est possible de maintenir les performances associées aux SVM, tout en réduisant la complexité d'un facteur 8.7 et en permettant de filtrer efficacement les données aberrantes.
On propose une méthode utilisant des techniques de traitement optique pour l'analyse et la reconnaissance de la parole.
Elle est mise en oeuvre sous la forme d'un processeur optique comportant un laser HeNe, des lentilles optiques, des plaques photographiques et des diffuseurs.
Un calculateur personnel permet de traiter les données expérimentales produites par l'ensemble du système de traitement optique.
Cette méthode de traitement optique a l'avantage intrinsèque de permettre un traitement parallèle et à haute vitesse de signaux bi-dimensionnels : par suite, les évolutions temps-fréquence d'un signal mono-dimensionnel peuvent être obtenues sans déplacer de fenêtre le long de l'axe temporel et la comparaison de formes pour la reconnaissance de parole peut être réalisée sur les périodes courtes.
Des productions de voyelles et de syllabes ont été analysées à l'aide de ce processeur : les résultats montrent une concordance forte avec ceux obtenus par simulation informatique.
On montre que la déformation nonlinéaire de l'axe temporel, indispensable en reconnaissance de mots, est réalisée par le contrôle de la fonction de transfert de la plaque de fenêtrage.
La comparaison de formes sur les voyelles donne une reconnaissance correcte des cinq voyelles.
Ces résultats montrent la validité du processeur optique.
L'utilisation de cristaux liquides est aussi proposée pour la plaque de fenêtrage : une expérience a été menée concernant l'analyse des voyelles.
Une comparaison de formes, quasi temps-réel et avec déformation temporelle nonlinéaire est rendue possible par le contrôle électrique de la plaque à cristaux liquides en utilisant, comme signal de rétro-action, le résultat de la comparaison précédente.
Pour différents signaux de parole voisée, l'instant de la fermeture de la glotte a été déterminé en utilisant la pseudo-distribution de Wigner-Ville lissée et la méthode de Wong.
Les deux méthodes donnent des résultats équivalents quand cette détermination est facile.
Quand la détermination devient difficile, la pseudo-distribution de Wigner-Ville lissée peut être encore employée dans certains cas où la méthode de Wong cesse d'être utilisable.
Nous nous intéressons à l'utilisation de dispositifs mobiles pour l'apprentissage informel en musée.
De nombreux travaux s'appuient sur une représentation sémantique des œuvres, pour la présentation d'informations en mobilité.
Cependant, ces travaux prennent peu en compte le caractère situé de la visite.
Nous proposons dans cet article le modèle CALM (ContextuAlized Learning through Mobility) qui associe à la représentation sémantique des œuvres des informations contextuelles sur l'utilisateur et sur sa situation.
Les scores retournés par les séparateurs à vaste marge sont souvent utilisés comme mesures de confiance pour la classification de nouveaux exemples.
Cependant, il n'y a pas de fondement théorique à cette pratique.
C'est pourquoi, lorsque l'incertitude de classification doit être estimée, il est plus sûr de recourir à des classifieurs qui estiment les probabilités conditionnelles des classes.
Ici, nous nous concentrons sur l'ambiguïté à proximité de la frontière de décision.
Nous proposons une adaptation de l'estimation par maximum de vraisemblance.
Le critère proposé vise à estimer les probabilités conditionnelles, de manière précise à l'intérieur d'un intervalle défini par l'utilisateur, et moins précise ailleurs.
Le modèle est aussi parcimonieux, dans le sens où peu d'exemples contribuent à la solution.
Nous appliquons ce critère à la régression logistique.
Ce modèle de régression logistique parcimonieuse sera ensuite validé par le jeu de données Forest Covertype de l'UCI.
L'acquisition des savoir-faire de parole par l'être humain comprend l'apprentissage “simultané” de sa perception et de sa production, dans un environnement de locuteurs ayant déjà assimilé ces savoir-faire.
En revanche, quand la parole est traitée par des machines, la reconnaissance et la synthèse de la parole sont étudiées et implémentées séparément (et l'on a développé des méthodes différentes pour chacune).
Le présent article propose une structure d'acquisition de la parole par des machines dans laquelle l'apprentissage de la reconnaissance et de la synthèse se fait simultanément à partir de parole naturelle.
La structure consiste en une chaîne de synthèse dans laquelle un synthétiseur est contrôlé par un réseau de neurones artificiels à partir d'un vecteur d'état de synthèse, et d'une chaîne de reconnaissance comprenant un réseau de neurones de reconnaissance qui produit un vecteur d'état de reconnaissance
Le système de reconnaissance reçoit successivement de la parole naturelle pour l'apprentissage et de la parole synthétique.
Une minimisation couplée est mise en œuvre pour entraîner le système de reconnaissance à classifier ou à reconnaître de la parole naturelle, et le synthétiseur à produire de la parole synthétique qui peut être reconnue comme appartenant à la même classe que la parole naturelle.
Une démonstration de l'algorithme est faite pour l'acquisition de voyelles et de mots simples isolés.
Cet article aborde le problème de la prise de décision décentralisée dans les colonies de robots autonomes et coopératifs.
Toutefois, ils ne proposent qu'une modélisation restreinte du temps et des actions et ne permettent pas la prise en compte de certaines propriétés des colonies de robots autonomes.
Afin d'étendre l'applicabilité des DEC-MDPs à la robotique collective, nous proposons un modèle, nommé OC-DEC-MDP, basé sur les DEC-MDPs qui permette une modélisation plus adéquate du temps et une gestion des contraintes sur l'exécution des tâches.
Nous décrivons également un algorithme de faible complexité permettant une résolution approchée des problèmes formalisés sous forme d'OC-DEC-MDP.
L'approche présentée permet ainsi de résoudre les problèmes de décision rencontrés dans les colonies de robots autonomes.
Le cliquetis dans les moteurs à allumage commandé demeure un problème pour les motoristes.
La détection du cliquetis doit en effet permettre de réaliser un compromis entre l'optimisation du rendement du moteur, la consommation de la voiture et le respect des normes en matière de dépollution.
Souvent l'allumage est réglé avec une marge de sécurité qui garantit l'absence de cliquetis même en cas de variations de la qualité du carburant.
L'enjeu de l'élaboration d'une méthode de détection de cliquetis est de se rapprocher le plus possible des conditions limites de cliquetis tout en évitant son apparition.
L'objectif de l'étude présentée consiste à évaluer l'intensité du cliquetis produit dans la chambre de combustion à partir d'un enregistrement fourni par un accéléromètre placé sur le bloc moteur.
Son but est de reconnaître trois types de cliquetis : l'absence de cliquetis, le cliquetis naissant et le cliquetis violent.
L'approche envisagée pour mener à bien cette détection fait appel aux techniques du diagnostic par reconnaissance des formes floue.
La méthode, mise au point à l'aide d'un ensemble d'apprentissage, conduit à la réalisation de plusieurs processus de diagnostic qui coopèrent.
Le présent article introduit le système informatique ALTO conçu afin de favoriser le développement et l'expérimentation d'algorithmes de génération de tournées pour des véhicules de transport.
Ce système repose principalement sur une “heuristique générale”, soit un ensemble de modèles qui sont instancies par un utilisateur expert à l'aide de ses propres formules afin de créer des algorithmes spécifiques.
Il devient dès lors possible de reproduire de nombreux algorithmes classiques décrits dans la littérature ou encore de concevoir de nouvelles approches de résolution face aux problèmes rencontrés.
Une application concrète dans le domaine de la collecte du courrier est d'ailleurs présentée à la fin de l'article afin de souligner l'intéret d'un tel système.
Cet article présente une adaptation du boosting à l'inférence grammaticale.
Notre but est d'améliorer les performances d'un algorithme à base de fusion d'états, en présence de données bruitées.
Cette information est une évaluation de la confiance en l'étiquette d'un exemple.
Nous montrons que la règle de mise à jour des poids conserve les propriétés théoriques du boosting.
Nous décrivons enfin une étude expérimentale sur l'algorithme à base de fusions d'états RPNI*, dont les performances sont significativement améliorées.
Nous proposons une reformalisation de l'analyse spectrale autorégressive régularisée dans le cadre de l'approche varia- tionnelle en considérant le polynôme autorégressif comme une transformation du cercle complexe unité en une courbe paramétrique fermée et orientée dans le plan complexe (théorie globale des courbes planes fermées : classe d'équivalences d'immersions du cercle complexe unité dans le plan Euclidien).
Nous montrons que l'Equation d'Euler-Lagrange associée nous ramène à la solution par moindres carrés régularisés classique.
Nous posons ensuite le problème sous une forme géométrique intrinsèque pour laquelle la solution est définie comme une géodésique minimale particulière dont la métrique dépend explicitement du terme d'adéquation aux données.
Le calcul des variations alors, en redéfinissant la notion de courbure d'une fonction complexe, une équation aux dérivées partielles (EDP) de type « flot de courbure moyenne » .
La discrétisation du problème via la transformée en Z aboutit à une EDP agissant sur le vecteur des paramètres autorégressifs.
Cette seconde approche permet de s'affranchir de l'optimisation de l'hyperparamètre de régularisation intervenant dans l'approche de Tikhonov classique, en stoppant l'EDP dès que sa vitesse d'évolution est ralentie.
Le second avantage réside dans la formalisation EDP qui permet naturellement l'estimation continue, en ligne, du spectre au rythme du flot des données.
L'extension de cette formalisation au Cepstre, dont la distance induite ainsi que celle du retard de groupe sont très utilisées en signal, fait apparaître le cepstre différentiel comme la transformation de Hopf-Cole du polynôme autorégressif et induit donc une évolution associée selon l'équation de Burgers conditionnellement aux données.
Nous concluons en utilisant l'interprétation de l'intégration complexe par Polya en terme de flux et de travail d'un champ de vecteurs pour montrer que la régularisation tend à rendre non-divergent et irrotationnel le champ de vecteurs autorégressifs conjugués sur le cercle complexe unité.
Cette étude vise à déterminer l'effet de l'orientation du cahier des charges comme une aide aux concepteurs pour réaliser des sites web plus faciles d'utilisation.
En effet, aussi bien les professionnels que les débutants confrontés au CCU prennent en compte oralement et respectent, dans leurs maquettes, un nombre important de contraintes liées à l'utilisateur sans pour autant que cela nuise à la prise en compte de contraintes liées au commanditaire.
Cela n 'est pas le cas pour les concepteurs confrontés au CCM.
Bien que les résultats soient encourageants, les maquettes réalisées comportent encore un nombre important de problèmes ergonomiques.
Nous concluons par la présentation d'études et de pistes de recherche pour aider les concepteurs à réaliser des sites plus simples d'utilisation.
Dans cette contribution, nous décrivons une méthode de transformation du locuteur par interpolation de formes de références à l'aide de réseaux de fonctions à symétries radiales (Radial Basis Function ou RBF).
L'intérêt principal de cette méthode d'interpolation réside dans le fait qu'elle reste exploitable même lorsque le nombre de données disponibles pour apprendre la transformation est limité.
Les fonctions d'interpolation de différents ordres sont pondérées et additionnées, en utilisant les coefficients de pondération donnés par le réseau RBF.
Un certain nombre d'expériences ont été conduites dans une tâche prototype de transformation (4 locuteurs).
Dans une deuxième expérience, l'ensemble d'apprentissage est composé de 10 mots ; la transformation par interpolation de fonctions multiples permet d'atteindre 48% de réduction.
Cet article présente une étude en apprentissage automatique semi-supervisé asymétrique, où seules des données positives et non étiquetées sont disponibles, ainsi qu 'une application à un problème bio-informatique.
Nous montrons que sous des hypothèses faibles, le classi- fieur naïf de Bayes peut être identifié à partir de données positives et non étiquetées.
Nous en déduisons des algorithmes que nous étudions sur des données artificielles.
Enfin, nous présentons une application de ces travaux au problème de l'extraction d'affinités locales dans les protéines pour la prédiction des ponts disulfures.
Nous nous plaçons dans le cadre de la classification automatique.
Nous abordons le problème de l'estimation du nombre de classes et des paramètres qui leurs sont associés.
Nous proposons une méthode utilisant l'hypothèse contextuelle inhérente aux images pour discriminer les différentes classes.
Cette méthode est validée à la fois sur le plan théorique et sur des images de synthèse et des images réelles.
Nous montrons, en outre, que la méthode proposée a un domaine de validité plus étendu que les méthodes fondées sur une analyse des modes de ('histogramme.
Nous discutons ensuite de la forme du potentiel d'attache aux données dérivé de cette classification dans le cadre d'une segmentation markovienne.
Les résultats sont obtenus avec deux modèles a priori différents : le modèle de Potts et le chien-modèle.
Cet article présente un ensemble de mesures pour la reconnaissance du locuteur.
Ces mesures reposent sur des tests statistiques du second ordre, et peuvent être exprimées sous un formalisme commun.
Différentes expressions de ces mesures sont proposées et leurs propriétés mathématiques sont étudiées.
Dans leur forme la plus simple, ces mesures ne sont pas symétriques, mais elles peuvent être symétrisées de différentes façons.
Toutes les mesures sont testées dans le cadre de l'identification du locuteur indépendante du texte en ensemble fermé, sur 3 versions de la base de données TIMIT (630 locuteurs) : TIMIT (parole de très bonne qualité), FTIMIT (version filtrée de TIMIT) et NTIMIT (qualité téléphonique).
Des performances remarquables sont obtenues sur TIMIT, mais les résultats se dégradent naturellement avec FTIMIT et NTIMIT.
La symétrisation apparaît comme un facteur d'amélioration, plus particulièrement lorsque l'on dispose de peu de parole.
Il est finalement suggéré, comme conclusion à ce travail, d'utiliser certaines mesures proposées comme méthodes de référence pour évaluer la complexité intrinsèque d'une base de données quelconque, sous un protocole donné.
Nous présentons dans cet article une application complète des « Support Vector Machine » au contrôle qualité par vision artificielle de pièces à géométrie complexe.
Nous précisons le cadre pratique dans lequel s'effectuent les opérations, la nature des défauts à détecter ainsi que les techniques d'extraction des paramètres discriminants.
Nous présentons ensuite les trois méthodes de classification utilisées.
Nous définissons le protocole d'apprentissage, ainsi que la méthode de recherche des paramètres optimum du classifieur.
Nous comparons les résultats obtenus à partir d'un espace de description défini a priori ainsi que ceux issus d'une sélection de paramètres via un algorithme séquentiel.
Cette contribution aborde la façon dont Bar Hebræus a emprunté au grammairien arabe Zamaḫšarī la notion de transitivité et comment il l'a reformulée dans le cadre de sa grammaire du syriaque.
Je procède en traduisant et commentant son texte et en comparant avec celui de Zamaḫšarī.
Son chapitre s'organise en quatre sections :
1. Première section : à propos d'exemples de verbes intransitifs et transitifs ;
2. Deuxième section : des causes de la transitivité ;
3. Troisième section : à propos de l'échec des causes de la transitivité ;
4. Quatrième section : à propos des verbes qui sont à la fois transitifs et intransitifs.
C'est dans ces deux dernières sections que se manifeste le mieux la différence entre les deux grammairiens ; et il apparaît que si Bar Hebræus a emprunté le concept de transitivité à Zamaḫšarī, il en a donné un traitement qui dépasse largement sa source.
En effet, la seule préoccupation du grammairien arabe est d'assurer que tous les compléments sont bien à l'accusatif et d'identifier les causes de la transitivité.
Enfin Bar Hebræus étudie en détail les verbes labiles qui sont à la fois transitifs et intransitifs.
Cet article examine les formes d'écoulement d'air lors des transitions voyelle-consonne et consonne-voyelle.
L'écoulement d'air buccal a été enregistré chez 6 locuteurs anglais-américains répétant des énoncés.
Un filtrage inverse du signal a été effectué pour obtenir une estimation des impulsions glottiques.
On a mesuré les écoulements maximum et minimum, le quotient d'ouverture, la zone des impulsions et la fréquence fondamentale.
Les résultats montrent de grandes variations dans les caractéristiques des impulsions à la transition entre voyelles et consonnes sourdes.
En particulier, la source est caractérisée par un mode de phonation rauque.
Cette raucité est associée à de larges variations de l'écoulement et à une valeur du quotient d'ouverture proche de 1.
Les variations observées peuvent être dues aux ajustement laryngés effectués pour les consonnes sourdes, en particulier le mouvement d'ouverture de la glotte et sa relation de phase avec les événements articulatoires buccaux.
Les différences individuelles observées suggèrent que les locuteurs diffèrent dans leur façon d'utiliser la tension longitudinale des cordes vocales pour contrôler le non-voisement.
L'article présente une nouvelle formulation matricielle des concepts qui sont à la base des modèles markoviens discrets (HMM — Hidden Markov Models).
En utilisant cette formulation, nous montrons que les probabilités d'états et de symbolés sont des fonctions exponentielles de la matrice de transition du modèle.
Ensuite, nous dérivons une expression fermée entre les valeurs propres de la matrice de transition et les probabilités des symboles à des instants différents.
La formulation matricielle fournit un outil qui est utile à l'interprétation physique du processus d'apprentissage et de décision à l'aide d'HMM.
Elle apporte une aide à la compréhension et elle fournit des outils pour un modèle qui possède des caractéristiques d'apprentissage améliorées.
D'autre part, nous utilisons un ensemble de connaissances linguistiques sous forme de modèles réduits issues de modèles linguistiques de textes.
Dans ce contexte, nous cherchons à évaluer si l'utilisation de connaissances et de traitements linguistiques peut améliorer les performances d'un système de filtrage.
En effet, nous utilisons, au-delà des caractéristiques lexicales, un ensemble d'indicateurs sur le message portant sur la structure et le contenu.
Ces connaissances sont indépendantes du domaine d'application et la fiabilité repose sur l'opération d'apprentissage.
Pour tenter de statuer sur la faisabilité de notre approche et d'évaluer son efficacité, nous l'avons expérimenté sur un corpus de 1 200 messages.
Nous présentons les résultats d'un ensemble d'expériences d'évaluation.
Cet article présente quelques développements dans l'expansion de requête et la représentation des documents de notre système de recherche documentaire et montre comment les diverses techniques de recherche affectent la performance pour différents ensembles de transcriptions dérivées d'une source de parole commune.
Des modifications de la représentation des documents sont effectuées, qui combinent plusieurs techniques pour l'expansion de requête, fondées sur des connaissances d'une part et sur des statistiques d'autre part.
Utilisées conjointement, ces techniques peuvent améliorer la Précision Moyenne de plus de 19%, relativement à un système semblable à celui que nous avons présenté à TREC-7.
Ces nouvelles expérimentations ont également confirmé que la dégradation de la Précision Moyenne due à un Taux d'Erreur de Mot (WER) de 25% est vraiment faible (3,7% relatif) et peut être réduite à une quantité négligeable (0,2% relatif).
L'amélioration globale du système de recherche documentaire peut aussi être observée pour sept ensembles différents de transcriptions provenant de différents systèmes de reconnaissance ayant un WER variant de 24,8% à 61,5%.
Nous espérons reproduire ces expérimentations, lorsque de plus grandes collections de documents parlés seront disponibles, afin d'évaluer le comportement de ces techniques sur de plus gros volumes de données.
Cet article porte sur le traitement de l'information visuelle apre`s la cre´ation des premie`res repre´sentations.
La capacite´de de´terminer visuellement les proprie´te´s formelles absraites et les relations spatiales esl un pre´requisa`ce niveau.
Cette capacite´joue un role majeur dans la reconnaissance d'objet, dans les manipulations guide´es par la vision ainsi que dans la pense´e visuelle plus abstraite.
Pour le syste`me visuel humain, la perception des proprie´te´s spatiales et des relations complexes au point de vue calcui apparait trompeusement imme´diate et facile.
La perception des propriete´s de forme abstraite et des relations spatiales soule`ve des difficulte´es fondamentales avec des conse´quences importantes pour le traitement ge´ne´ral de l'information visuelle.
Les auteurs de´fendent l'ide´e que le calcul des relations spatiales se´pare l'analyse de l'infformation visuelle en deux stades principaux.
Au cours du premier se cre´ent, de bas en haut, certaines repre´sentations de l'environnement visible.
Au cours du second des processus dits 'routines visuelles' s'appliquent aux repre´sentations issues du premier stade.
Ces routines peuvent reve`ler des proprie´te´s et des relations qui n'e´taient pas repre´sente´es de façon explicite dans les repre´sentations initiales.
Les routines visuelles sont compose´es de se´quences d'ope´rationse´le´mentaires conjointes pour les diffe´rentes proprie´te´s et relations.
En utilisant une se´rie fixe d'ope´rations de base, le syste`me visuel peut assembler diffe´rentes routines pour extraire une suite illimite´e de proprie´te´s de forme et de relations spatiales.
Les auteurs posent le proble`me de l'assemblage de ces ope´rationse´le´mentaires en routines visuelles signifiantes.
L'état actuel de la recherche sur l'effet des émotions d'un locuteur sur la voix et la parole est décrit et des approches prometteuses pour le futur identifiées.
En particulier, le modèle de perception de Brunswik (dit “de la lentille” est proposé) comme paradigme pour la recherche sur la communication vocale des émotions.
Ce modèle permet la modélisation du processus complet, de l'encodage (expression) par la transmission au décodage (impression).
La conceptualisation et l'opérationalization des éléments centraux du modèle (l'état émotionnel du locuteur, l'inférence de cet état par l'auditeur, et les indices auditifs) sont discuté en détail.
De plus, en analysant des exemples de la recherche dans le domaine, les avantages et désavantages de différentes méthodes pour l'induction et l'observation de l'expression émotionnelle dans la voix et la parole et pour la manipulation expérimentale de différents indices vocaux sont évoqués.
Il propose un formalisme original, l'interac-DEC-POMDP inspiré des modèles markoviens au sein duquel les agents peuvent interagir directement et localement entre eux.
A partir de ce formalisme, cet article propose un algorithme d'apprentissage décentralisé fondé sur une répartition heuristique des gains des agents au cours des interactions.
Une démarche expérimentale valide sa capacité à produire automatiquement des comportements collectifs.
Les techniques présentées pourraient alors constituer des moyens permettant aux agents de décider automatiquement et de manière décentralisée comment s'organiser avec les autres pour résoudre un problème donné.
L'estimation du mouvement à partir de séquences d'images bidimensionnelles s'appuie sur deux hypothèses de base : l'hypothèse de conservation de la luminance des objets au cours de leurs mouvements et l'hypothèse de continuité spatiale, temporelle ou spatio-temporelle, du champ de vitesses apparentes.
Cette dernière hypothèse est valable localement, « à l'intérieur » des objets, mais elle provoque un lissage indésirable au voisinage des frontières entre les projections, dans le plan image, des objets animés de mouvements différents.
Ces frontières sont appelées discontinuités de mouvement.
Le sujet principal de cet article est la revue des techniques visant à estimer le champ de vitesses apparentes en évitant de lisser les discontinuités de mouvement.
La première partie de l'article présente les méthodes s'appuyant sur l'hypothèse selon laquelle les discontinuités de mouvement coïncident spatialement avec certaines frontières photométriques.
La deuxième partie rapporte les méthodes de segmentation du champ estimé courant en régions homogènes au sens du mouvement.
Cette inhibition peut être obtenue par introduction d'un « processus de ligne » binaire, grâce à l'utilisation d'un « estimateur robuste » , ou dans un schéma de diffusion anisotrope.
La dernière partie est consacrée à la gestion des occultations.
Les estimations obtenues dans les zones d'occultation sont erronées à cause de la violation des deux hypothèses de base : conservation des propriétés photométriques et continuité.
L'intérêt de séparer les impulsions en fonction de l'émetteur est de procéder ensuite à l'analyse des lois qui régissent les paramètres décrivant ces impulsions.
Cependant, en raison d'innombrables perturbations et de la complexité des lois d'évolution des paramètres des impulsions, la tâche s'avère fort délicate.
Nous montrons que les méthodes de Monte-Carlo séquentielles sont susceptibles d'apporter une réponse adaptée à ce problème d'extraction, que l'on peut également formuler comme une problématique d'association de données.
La fusion des différents paramètres, à condition que cette dernière s'effectue en tenant compte des spécificités du modèle, permet d'étendre le domaine d'application de l'algorithme à des densités d'impulsions importantes.
La voix soufflée est utilisée pour former des contrastes linguistiques dans certaines langues, mais elle peut également caractériser les locuteurs en tant qu'individus et, dans une certaine mesure, le sexe.
Henton et Bladon (1985) prétendaient que la présence de souffle dans les voix diminue l'intelligibilité.
Dans les expériences décrites dans cet article, de la parole synthétique a été employée pour déterminer les effets de l'ajout d'une source de bruit à une source de voix modale ainsi que l'effet des différents corrélats acoustiques de la voix soufflée sur l'intelligibilité de mots isolés.
Aucun effet significatif n'a été trouvé.
Cette communication présente les principaux problèmes liés à la recherche d'information dans la blogosphère.
Recourant au modèle vectoriel tf idf, ainsi qu'à trois approches probabilistes et un modèle de langue, cet article évalue leur performance sur un corpus TREC extrait de la blogosphère et comprenant 100 requêtes.
Basés sur deux mesures de performance, nous démontrons que l'absence d'enracineur s'avère plus efficace que d'autres approches (enracineur léger ou celui de Porter).
Imposer la présence côte à côte de deux mots recherchés dans la réponse fournie permet d'accroître significativement la performance obtenue.
Cet article, basé sur les trois exposés effectués en 1998 lors de la conférence RLA2C à Avignon, présente les méthodes de métrologie de la reconnaissance du locuteur.
Pour cela nous utiliserons des detection error trade-off (DET) curves.
Ces courbes ont l'avantage de montrer le compromis entre erreur et fausse alarme et l'effet sur les performances des conditions d'entrainement, de la durée des segments de tests, du sexe du locuteur ou du type de combiné.
Nous avons découvert que plusieurs facteurs influençaient fortement les performances des systèmes, comme par exemple la tonalité de la voix, le type de combiné utilisé ou le bruit ambiant.
Nous concluerons avec un historique des techniques de reconnaissance du locuteur et avec quelques projections de ce domaine dans l'avenir
Dans cet article, nous définissons un critère de fidélité pour quantifier le degré de distorsion introduite par un codeur de parole.
Un signal vocal original et sa version codée sont transformés du domaine temporel dans le domaine perceptuel utilisant un modèle auditif (cochléaire).
Cette représentation dans le domaine perceptuel fournit une information liée aux probabilités de décharges dans les canaux neuronaux.
La mesure de discrimination cochléaire introduite ici compare ces probabilités de décharges au sens de la théorie de l'information.
En essence, elle évalue l'entropie croisée des décharges neuronales pour la parole codée par rapport à celle de la parole originale.
La performance de cette mesure objective est comparée à des résultats d'évaluation subjective.
Finalement, nous fournissons une analyse débit-distorsion en calculant la fonction débit-distorsion pour le codage de la parole en utilisant l'algorithme de Blahut.
Quatre codeurs de parole performants avec des débits allant de 4.8 kbit/s (CELP) à 32 kbit/s (ADPCM) sont étudiés du point de vue leurs performances (telles qu'évaluées par la mesure de discrimination cochléaire) par rapport aux limites débit-distorsion.
Cinquante ans se sont écoulés depuis que Stuart Piggott a fouillé le complexe préhistorique de Cairnpapple.
A cette époque-là on n' avait exploré que peu de sites parallèles en Ecosse, et, inévitablement, leur interprétation reposait essentiellement sur les sites déblayés dans le sud de la Grande-Bretagne.
Beaucoup plus de données appropriées à la région sont désormais disponibles et la séquence de Cairnpapple peut maintenant être réévaluée dans son contexte régional.
Piggott avait identifié cinq périodes, avec au commencement un agencement de pierres, 'un cromlech', et un cimetière à incinération datant de la fin du néolithique, vers environ 2500 av. J.-C.
La période II consistait en un monument avec enceinte, comprenant un 'cercle' de pierres dressées avec inhumations cérémoniales associées, entouré par un fossé accompagné d'un talus du côté extérieur – 'de l'époque des peuples à vase, probablement vers 1700 av. J.-C.'.
La période III comprenait le cairn primaire, contenant deux inhumations avec dalles de pierre 'datant du milieu de l'âge du bronze, probablement vers 1500 av. J.-C.
Pendant la période IV la taille du cairn a doublé, avec deux crémations dans des urnes à incinérations retournées.
De la période finale de l'âge du bronze moyen ou de la fin de l'âge du bronze régional, probablement autour de 1000 av. J.-C.
La période V comprenait quatre tombes 'mais peut être du début de l'âge du fer, au cours des deux premiers siècles ap. J.-C.
La présente étude, utilisant des matériaux comparables provenant d'autres endroits en Ecosse, argumente en faveur d' une révision des différentes phases :
la phase 1 comprend un dépôt de tessons de bols non décorés et de fragments de têtes de haches du début du néolithique associé à une suite de foyers.
On peut comparer ceci à des dépôts structurés répertoriés sur d'autres sites de cette période.
La phase 2 comprend la construction de l'enceinte – un ensemble de 24 verticaux –probablement en bois plutôt qu'en pierre, probablement suivi par la construction du fossé et du talus tout autour.
Le 'cromlech' est analysé dans le contexte de structures comparables en Ecosse.
La phase 3 a vu la construction d'une série de tombes, y compris la monumentale North Grave, tombe nordique, qui était probablement enchâssée dans un cairn.
Le cairn qui correspond à la période III de Piggott fut alors construit, suivi par le cairn de la 'période IV'.
Il semble probable que les inhumations à urnes aient été introduites dans la surface de ce tertre, qui recouvrait peut-être une inhumation (perturbée depuis) en haut du tertre de la 'période III', ou en était peut-être une monumentalisation délibérée.
Il semble plus probable que les quatre tombes identifiées par Piggott comme appartenant à l'âge du fer datent en réalité du début de l'ère chrétienne.
La réévaluation du rapport de Piggott insiste sur la valeur que représente un compte-rendu dont l'écriture est claire et le contenu suffisamment détaillé.
Tandis qu'aucun rapport ne peut être totalement objectif, on constate que les efforts d'objectivité fournis par Piggott l'ont amené à écrire un article dont la valeur subsiste.
Nous proposons dans cet article une approche de fusion probabiliste reposant sur des critères entropiques dont le but est de réduire l'espace de combinaison en représentant explicitement les notions de redondance et de complémentarité des sources d'information.
Ce type de modélisation est en particulier intéressant pour optimiser le choix des mesures, issues des sources d'information, à combiner dans un système de fusion.
Il est en accord avec le souci de rapidité de traitement et de minimisation des ressources matérielles qui se pose en fusion d'informations.
Pour repondre à cela, nous avons réalisé une étude de la parallélisation de l'algorithme de fusion entropique développé en vue de son implantation parallèle dans le cadre d'une application en robotique mobile.
La spécification de l'algorithme faisant apparaître du parallélisme potentiel est ensuite implantée sur un réseau de stations de travail fonctionnant en mode MIMD-NORMA à l'aide des environnements de programmation SynDEx, qui supporte la méthodologie AA-A, et PVM, qui est de type CSP de Hoare.
Cet article étudie dans quelle mesure la reconnaissance des mots est influencée par le contexte lexical, syntaxique et sémantique, cela afin de comparer les prédictions faites par des théories modulaires et interactives de l'architecture du systéme de compréhension du langage.
Notre conclusion est que les données montrent clairement qu'il existe des effets du contexte lexical, moins clairement qu'il existe des effets du contexte sémantique, et qu'il y a peu de données en faveur d'effets du contexte syntaxique.
Selon nous, les effets de feedback “de haut en bas” dans la compréhension apparaissent essentiellement dans des situations où il existe une relation partie-ensemble claire entre les deux niveaux, et où l'ensemble des unités de niveau inférieur qui peuvent recevoir un feedback du niveau supérieur est restreint.
Nous présentons un système de classification de phonèmes indépendant du locuteur et appliqué aux voyelles.
L'architecture du classificateur de voyelles est basée surun modèle d'oreille suivi d'un ensemble de réseaux neuronaux à plusieurs couches (MLNN).
Les MLNNs apprennent à reconnaître les traits articulatoires, par exemple le lieu et le mode d'articulation en relation avec la position de la langue.
Des expériences ont été effectuées sur 10 voyelles anglaises et montrent un taux de reconnaissance supérieur à 95% sur de nouveaux locuteurs.
Lorsque les traits sont utilisés pour la reconnaissance, des résultats comparables sont obtenus pour des voyelles et des dihthongues qui n'ont pas été utilisées lors de l'apprentissage et prononcées par de nouveaux locuteurs.
Ceci suggère que, pour des données calculées par un modèle d'oreille, les MLNNs présentent un bon pouvoir de généralisation pour de nouveaux locuteurs et de nouveaux sons.
Annoter des images selon un nombre fini de concepts fixés a priori est une tâche fondamentale permettant la recherche d'images par le contenu.
En pratique, plusieurs modalités (visuelle, textuelle...) fournissent des informations sur le contenu des images.
Notre intérêt porte ici sur les tags associés aux images, généralement issus d'une indexation personnelle, fournissant une information imparfaite et partiellement pertinente.
Nous proposons deux modèles de tags pour pallier de telles imperfections, l'un modélisant le phénomène au moyen d'un seuil établi automatiquement selon la similarité sémantique avec les concepts visuels, et l'autre améliorant le schéma de codage des sacs-de-mots en s'inspirant de récents travaux effectués en classification d'images.
L'ensemble de ces travaux est validé sur plusieurs bases d'images publiques et couramment utilisées dans le domaine de l'annotation d'images.
Les résultats expérimentaux montrent que les méthodes proposées dépassent l'état de l'art tout en restant moins coûteuses en calculs que les travaux récents dans le domaine.
Cet article présente une méthode d'approximation pour les POMDP qui est basée sur une recherche en profondeur pour la planification dans un environnement temps-réel dynamique.
L'idée de base de notre approche, appelée RTBSS, est d'éviter de calculer des politiques complètes pour des POMDP.
Cette approche est spécialement utile pour des environnements temps-réel où l'espace d'états est trop grand pour que l'on puisse considérer les algorithmes de résolution hors lignes des POMDP.
A cet effet, nous proposons une approche en ligne pour calculer à chaque cycle, l'action qui maximise l'utilité espérée de l'agent.
Nous commençons par présenter tout le formalisme à la base de notre méthode.
Par la suite, nous présentons les résultats expérimentaux obtenus sur trois environnements.
Mentionnons par ailleurs que cette approche a été implémentée avec succès pour la compétition mondiale de la RoboCupRescue en 2004 où nous nous sommes classés en deuxième position.
Dans cet article, un système de reconnaissance de la parole basé sur la gńération synthétique de prototypes de références est décrit.
Le vocabulaire et la grammaire sont décrits dans un réseau de phonèmes à l'état fini.
Dans les transformations des représentations symboliques en représentations spectrales, des règles de réduction modifient les valeurs cibles du phonème initial et un modèle de la coarticulation insère des états de transition interpolés aux frontières de phonèmes.
Les gabarits de phonèmes sont spécifiés en terme de paramètres de contrôle d'un synthétiseur à formants en série.
A chaque état, une section spectrale est calculée à partir des paramètres de synthèse.
Le processus de reconnaissance utilise une technique de programmation dynamique et synchronisée pour trouver le chemin dans le réseau qui minimise la distance spectrale cumulée par rapport à la phrase de départ.
Une méthode dynamique d'adaptation au spectre de la source vocale du locuteur est employée pendant la phase de reconnaissance.
Sans adaptation, le taux de reconnaissance moyen est de 88% pour 10 locuteurs masculins dans une tâche de reconnaissance de mots isolés utilisant un vocabulaire de 26 mots.
L'ajout d'une adaptation à la source vocale fait monter la performance jusqu'à 96%.
Sur un vocabulaire de séquences de 3 chiffres connectés, le tauc de reconnaissance pour six locuteurs masculins augmente de 88.7%, à 92.8% grâce à la technique d'adaptation.
L'amélioration est plus grande pour les sujets qui ont u taux de reconnaissance initial faible, ce qui montre le bénéfice de la technique d'adaptation à la source vocale pour certaines voix.
En changeant le modèle de source vocale et en optimisant l'adaptation, la constante de temps fait monter le taux de reconnaissance jusqu'à plus de 96,1%.
Les travaux en cours étudient l'adaptation du locutuer aux paramètres phonèmiques et la modélisation de la variabilité de la dynamique des paramètres aux frontières de phonème.
Les effets du VOT, de la tenue voisée (la période entre la fermeture orale et l'arrèt de voix), de la durée et de l'étendue de la transition des formants et de l'intensité du bruit de friction sur la perception de voisement dans des séquences de deux obstruentes néerlandaises (C 1 C 2) ont ét'e testés dans quatre expériences séparées à l'aide de stimuli synthétiques
Les résultats montrent que le VOT et la tenue voisée sont des indices importants pour la perception de voisement dans C 1 C 2, et que l'intensité du bruit de friction constitue un indice plus faible.
Les transitions des formants n'ont pas eu d'effet significatif.
En outre, les données concernant le VOT et la tenue voisée montrent que des indices qui proviennent de parties du signal acoustique qui se trouvent à des distances temporelles assez éloignées, sont intégrés dans une unité perceptive, et qu'un indice particulier a un effet semblable sur le caractère de voisement perçu des deux consonnes qui constituent la séquence.
Cet article décrit deux expériences portant d'une part sur le rappel stimulé de phrases présentées dans un contexte particulier, et d'autre part le rappel non-stimulé de ces mêmes phrases.
Certains mots clef de ces phrases ont été introduits de façon à posséder le même sens supérficiel tout en appartenant à des catégories syntactiques et des fonctions sémantiques indépendantes.
Pour les phrases dans lesquelles la fonction sémantique d'acteur et de recepteur coincidaient respectivement avec la fonction syntactique de sujet et d'objet profond, le rappel stimulé était meilleur que pour les phrases qui ne manifestaient pas cette coincidence normale sémantico/syntactique.
Le rappel non-stimulé était identique dans les deux cas.
Les deux genres d'information normalement utilisés dans le langage courant apparaitraient donc indispensables dans le traitement de phrases modèle.
Dans cette communication, nous appliquons un modèle de recherche d'information pour la tâche d'identification du scripteur.
Les requêtes sont des images de documents qui sont tout d'abord projetées dans un espace de caractéristiques.
La base de documents manuscrits est indexée selon le principe du modèle vectoriel de recherche d'information textuelle.
L'approche exploite donc à la fois la représentation mixte image et textuelle spécifique d'un document manuscrit.
Les documents identifiés à l'issue de cette étape font ensuite l'objet d'une analyse complémentaire pour vérifier les hypothèses émises.
Nous proposons d'utiliser un critère d'information mutuelle pour vérifier que chacun des documents identifiés peut avoir été produit par le scripteur de la requête.
Nous utilisons un test d'hypothèse à cet effet.
L'approche est testée sur deux bases d'écritures différentes et montre une grande robustesse aux différentes écritures.
L'approche semble donc très intéressante pour des applications à plus grande échelle nécessitant d'interroger des bases de documents manuscrits.
Nous proposons une nouvelle méthode de calcul d'une politique approchée d'un Dec-POMDP qui surpasse les approches de l'état de l'art dont PBDP et MBDP.
Notre approche est fondée sur une estimation de la distribution de probabilité des croyances atteignables pour un horizon donné.
Cette estimation est faite en simulant l'exécution d'une politique heuristique du Dec-POMDP considéré.
Cette distribution de probabilité des croyances est ensuite utilisée pour choisir les arbres de politique candidats à l'horizon considéré grâce à un critère simple qui cherche à minimiser l'erreur induite par l'élagage.
Un nouvel algorithme de modélisation acoustique est proposé qui génère des unités de type Modèles de Markov Cachés (MMCs) non-uniformes pour prendre en compte les variations spectrales de la parole continue.
L'algorithme est conçu pour la génération automatique et itérative d'unités de longue durée dans le cadre d'une modélisation non-uniforme.
Cet algorithme est fondé sur un critère de réduction d'entropie, qui utilise des données textuelles ainsi qu'un critère de probabilité maximum qui utilise des données orales.
L'efficacité des modelès d'unités non-unifomes est confirmée en comparant des probabilités entre les unités MMCs de longue durée et les unités MMCs basées sur le modèle conventionnel des unités phonémiques.
Dans les descriptions de la grammaire du tibétain, il est courant de traiter -las et -nas comme des marques casuelles similaires, en signalant simplement que -las peut former des comparaisons alors que -nas ne le peut pas.
De même, la plupart des descriptions n'opèrent aucune distinction entre les suffixes -bas et -las en ce qui concerne la comparaison.
Nous montrons à travers divers exemples illustrant l'emploi de ces trois morphèmes qu'ils ont des fonctions syntactiques distinctes et présentent également des différences sémantiques.
Cet article décrit un système de dialogue oral en cours de développement, TARSAN, pour guider des personnes en voyager.
TARSAN utilise comme source de connaissance des guides commercialisés sous la forme de CD-ROMs qui comportent une grande quantité d'informations touristiques.
Pour manipuler une telle quantité d'informations, le système de reconnaissance doit pouvoir accepter un très large vocabulaire sans réduire ses performances.
Dans ce but, nous proposons deux étages de contrôle des mots actifs/non-actifs : (1) une stratégie de prédiction des mots/de la grammaire, et (2) un algorithme de ré-estimation des mots inconnus.
La stratégie de prédiction mot/grammaire modifie de façon dynamique le réseau de reconnaissance en fonction de l'état de la conversation en utilisant les résultats retrouvés à partir des CD-ROMs.
Cette stratégie permet aux utilisateurs d'accéder à quasiment toutes les informations du CD-ROM en utilisant un système de reconnaissance de petit vocabulaire.
Cet algorithme améliore l'efficacité de la prédiction mot/grammaire.
Dans les expériences sans modèles poubelle, 80.9% des énoncés étaient correctement reconnus.
Dans l'expérience utilisant les modèles poubelles pour la ré-estimation des mots inconnus, 86.4% étaient correctement ré-estimés, pour un taux de fausse-alarme de 5%.
Nous combinons pour l'exploration Monte-Carlo d'arbres de l'apprentissage artificiel à 4 échelles de temps : regret en ligne, via l'utilisation d'algorithmes de bandit et d'estimateurs Monte-Carlo ; de l'apprentissage transient, via l'utilisation d'estimateurs rapides de Q-fonction (RAVE, pour Rapid Action Value Estimate) qui sont appris en ligne et utilisés pour accélérer l'exploration mais sont ensuite peu à peu laissés de côté à mesure que des informations plus fines sont disponibles ; apprentissage hors-ligne, par fouille de données de jeux ; utilisation de connaissances expertes comme information a priori.
L'algorithme obtenu est plus fort que chaque élément séparément.
Nous mettons en évidence par ailleurs un dilemme exploration-exploitation dans l'exploration Monte-Carlo d'arbres et obtenons une très forte amélioration par calage des paramètres correspondants.
La recherche dans le domaine de la reconnaissance de grands vocabulaires s'est développée de façon intensive, au niveau international ces dernières années, stimulées par les progrés dans les domaines de l'algorithmique, des architectures et des matériels.
Aux Etats-Unis, la communauté DARPA a orienté ses efforts sur l'étude de diverses tâches de reconnaissance de parole continue, dont la gestion de ressources navales (Ressource Management : une tâche de 991 mots), un système d'information sur les transports aériens (ATIS, une tâche de compréhension de parole avec un vocabulaire ouvert (en pratique, plusieurs milliers de mots) et une composante de traitement de langage naturel) et le Wall Street Journal (WSJ : une tâche de dictée vocale avec un vocabulaire de l'ordre de 20 000 mots).
Bien que nous ayons beaucoup appris sur la façon de construire et d'implémenter efficacement des systèmes de reconnaissance de grands vocabulaires, il reste toutes une série de questions fondamentales pour lesquelles nous n'avons pas de réponses définitives.
Des stimuli consistant en des trains alternés d'impulsions à un et deux formants, et des stimuli à deux formants à rapport A 1/A 2 variable, on été utilisés pour des expériences d'identification vocalique.
Deux faits essentiels ont été relevés : 1) l'augmentation en proportion d'impulsion à un formant dans le train d'impulsions et l'augmentation en amplitude du formant correspondant dans le stimulus à deux formants affectent l'identification exactement de la même manière. 2) de fortes variations dans la différence d'amplitude entre les impulsions à un et à deux formants dans le train d'impulsions n'ont pas d'effet sur l'identification.
Ni l'hypothèse de classification phonémique courante. ni l'hypotèse de spectre moyen ne sont compatibles avec l'ensemble de ces deux faits.
Il se pourrait que des patterns simples “à une crête” et “à deux crêtes” soient utilisés comme composantes pour l'analyse de la forme spectrale.
Au cours d'une serie d'expériences les sujets décrivent des scènes visuelles simples soit avec des phrases soit avec des mots.
Des données appuient les positions suivantes sur les processus de lexicalisation (recherche de mots);
1) les mots utilisés pour dénomer dans des phrases sont selectionnés suivant deux processus séquentiels. le premier travaille sur les items prephonologiques abstraits (items L1), Le second ajoute la forme phonologique (items L2).
2) La sélection des items L1 dans un énoncé de plusieurs mots peut être simultanée.
3) Un dispositif de contrôle (moniteur) vérifie à la sortie de la lexicalisation L1 l'accord avec les contraintes sur le format de l'énoncé.
4) La recherche de l'item L2 correspondant à un item L1 donné ne commence qu'après la vérification de L1 par le moniteur et qu'après que tous les L1 nécessaires à la construction de l'énoncé soient disponibles.
Une image cohérente des processus de lexicalisation commence à emerger lorsqu'on réunit ces points et les résultats expérimentaux obtenus avec les travaux sur la dénomination et le production de phrases, e.g., temps de réaction à la dénomination d'images (Seymour, 1979), erreurs (Garrett, 1980) ou préférences sur l'ordre des mots (Bock, 1982).
Le diagnostic précoce est le moyen le plus efficace de lutte contre le cancer.
Parmi toutes les techniques possibles, les méthodes optiques (photodiagnostic du proche UV au proche IR) présentent des caractéristiques importantes recherchées par les médecins : grande sensibilité, radiations non ionisantes et mesures atraumatiques.
Elles sont particulièrement bien adaptées à la détection des cancers des organes creux, par nature superficiels et difficilement décelables en endoscopie classique.
Cet article décrit une approche méthodologique fondée sur l'exploitation de l'autofluorescence tissulaire, applicable en endoscopie clinique, et conduisant à l'élaboration d'indicateurs diagnostiques issus des paramètres spectraux.
Après un état de l'art sur les méthodes spectroscopiques (LIFS) et d'imagerie endoscopique d'autofluorescence, nous montrons l'efficacité de la LIFS fibrée en terme de sensibilité et de spécificité pour le diagnostic de lésions cancéreuses de l'œsophage (étude clinique sur 25 patients).
Nous présentons ensuite les caractéristiques technologiques et le calibrage du prototype d'imageur endoscopique d'autofluorescence développé.
Une seconde partie traite du pré-traitement, du recalage et du mosaïquage des images endoscopiques appliqués à la construction automatique d'une image panoramique (cartographie) à partir de séquences vidéos des zones explorées de l'organe.
Finalement, en exploitant les informations de fluorescence fournies par l'imageur, la faisabilité d'une superposition des informations spatiale et spectrale est validée sur fantôme.
On présente ici un ensemble de règles permettant de fournir à des systèmes de synthèse du français les durèes des phonèmes en contexte.
Les règles utilisent, pour chaque phonème, des durées intrinsèques qui sont considèrées comme des données indépendantes du locuteur.
Ainsi, cet ensemble de régles permet de prédire des durées segmentales différentes pour imiter au mieux des locuteurs varés.
La validité du modele a été testd sur 2 locuteurs.
Sur les corpus de test utilisés, les écarts moyens entre durées prédites et mesurées sont inférieurs à 18 ms.
L'ancienne écriture zhuang - un terme qui désigne actuellement les langues tai parlées au Guangxi, dans le sud de la Chine - est empruntée au chinois.
La typologie présentée ici a des implications considérables pour l'étude des systèmes d'écriture et de l'écriture chinoise en particulier.
Dans la conception des algorithmes de codage de la parole à bas-debit la variabilité de langue est souvent considérée avec une importance secondaire en comparaison avec les autres facteurs opérationnels, tels que la variabilité d'interlocuteur et le bruit.
Etant donné que les langues sont largement différentes au niveau de composition de l'enveloppe spectrale et que l'enveloppe spectrale de la parole représente une partie importante de la répartition des bits dans le codage de la parole, il est surprenant de trouver qu'aucune étude complète n'a jamais été effectuée sûr le rôle des langues en la quantification spectrale.
Dans cette étude, nous adressons ce déficit a travers d'une série d'enquêtes sur la performance de quantification spectrale effectué pour un ensemble de familles de langage typique de la telecommunication mobile planétaire.
Dans cette étude, nous considérons certains facteurs de conception de la quantification tel que la taille et structure des dictionnaires de code, et la quantité des donnes monolingue utilisées dans la conception des dictionnaires de code.
Cette évaluation indique que la distorsion de quantification n'est pas uniforme à travers des différents langages.
Il est montré qu'une différence importante existe dans le comportement de la quantification spectrale à travers des langages, en particulier celle des points écarte de haute distorsion.
Une analyse détaille des donnes de la distorsion spectrale sur un niveau phonétique a révélé que la nature de la distribution de l'énergie spectrale des phonèmes influence le comportement des dictionnaires de code monolingue.
Nous présentons quelques explications pour la performance des dictionnaires de code ainsi qu'un ensemble de recommandations pour la conception des dictionnaires de code destine aux environnants multilingues.
L'oeuvre d'Othon de Grandson, qui fait aujourd'hui l'objet d'une réévaluation globale, met à contribution différents types de mythologies.
L'évaluation de la qualité de la parole codée et synthétisée est étroitement liée à la perception auditive des sons complexes.
Une compréhension de la perception des sons complexes est donc nécessaire pour améliorer la qualité des sons après traitement.
L'étude perceptuelle des sons de parole est abordée dans ce papier sous l'aspect du masquage auditif.
Contrairement à la plupart des autres travaux analogues, nous avons pris comme cibles des signaux de bruit à bande étroite et comme masqueurs des sons harmoniques complexes à large bande.
Nous montrons que la détection des cibles à basses fréquences est surtout fonction des propriétés spectrales des masqueurs.
Pour les hautes fréquences, la détection des cibles est préférentiellement déterminée par le comportement temporel des masqueurs.
Les contributions relatives des analyses spectrale et temporelle dépendent fortement de la fréquence fondamentale du masqueur.
Une meilleure résolution temporelle correspond à un plus haut niveau de masquage.
Une prothèse auditive a été développée qui permet d'estimer en temps réel la fréquence et l'amplitude des formants.
Ces paramètres peuvent être utilisés pour améliorer le signal de sortie en renforçant les pics formantiques et en ajustant les amplitudes des formants à la dynamique d'amplitude de l'audition pour chaque fréquence ou en resynthétisant le signal de parole qui convient aux caractéristiques auditives de l'auditeur.
Cette aide auditive peut aussi être utilisée dans un mode “adaption à la courbe de réponse en fréquence”, similaire à celui utilisé dans les aides auditives conventionnelles.
Les premières évaluations du fonctionnement en mode “renforcement des pics spectraux” font apparaitre de légères améliorations de la perception de parole pour 3 groupes de sujets :
(a) Cinq déficients auditifs (de “sévère” à “profond”) ont donné des scores améliorés de 7% en utilisant la prothèse “à formant” combinée avec un implant cochléaire multi-électrodes, par rapport à la combinaison de leur implant et de leur prothèse conventionnelle;
(b) un déficient auditif présentant une perte auditive importante fournit un score amélioré de 11% avec le signal “à pics spectraux renforcés” par rapport à l'utilisation de sa prothèse auditive conventionelle;
(c) 4 auditeurs bien-entendants montrent, en présence de bruit, une amélioration moyenne de 19% de leur perception des voyelles et une diminution de 5% de celle des consonnes quand le signal est pré-traité par la nouvelle prothèse.
Ces résultats préliminaires illustrent certains effets potentiels d'un traitement formantique.
Cet article présente une nouvelle approche du codage sinusoïdal de la parole ne nécessitant pas de détecteur de voisement.
Le modèle propose de représenter le signal de parole comme une somme de sinusoïdes et de signaux aléatoires passe-bande ; il est appelé “modèle hybride harmonique”.
L'utilisation d'ensembles variés de fonctions de base permet d'augmenter la robustesse du modèle, puisqu'il n'est plus nécessaire d'alterner des méthodes adaptées à une classe particulière de signaux.
Prendre pour base des fonctions sinusoïdales aux fréquences réglées harmoniquement autorise une représentation précise de la structure quasi-pérodique de la parole voisée, mais n'est pas adéquat pour celle des sons non-voisés.
Par contre, les signaux aléatoires passe-bande permettent une représentation excellente des sons de parole non voisés car leur largeur de bande est supérieure à celle d'une sinusoïde.
On réalise, simultanément, l'estimation des amplitudes de ces deux ensembles de fonctions de base par un algorithme des moindres carrés ; le signal de parole résultant est synthétisé dans le domaine temporel par la superposition de toutes les fonctions de base, en proportion de leur amplitude.
Les tests expérimentaux confirment que le modèle hybride, pour le codage de signaux de parole bruités, surclasse les modèles sinusoïdaux classiques qui sont sensibles à la décision voisé/non-voisé.
Pour finir, on décrit l'implémentation et le test de l'intégralité de ce codeur hybride à 4.8 Kbit/s.
Cet article propose une méthode d'extraction de l'impact émotionnel des images à partir de descripteurs bas niveau.
Nous avons émis l'hypothèse que la précision de ces derniers encoderait des informations haut niveau intéressantes voire discriminantes pour les émotions.
Plus ce temps est long plus l'interprétation sémantique de l'image prend le dessus sur l'émotion « primaire » .
A ces descripteurs nous avons associé deux classifieurs performants, particulièrement adaptés à des discriminations d'informations complexes.
Nous avons, à cet effet, effectué nos tests sur une base diversifiée de 350 images, construite à partir d'images à faible contenu sémantique.
Nous avons défini trois classes d'émotions.
La reconnaissance d'un mot parlé consiste en deux étapes principales.
Lors de l'étape pré-lexicale, l'information présente dans le signal de parole est analysée, et la représentation abstraite de l'énoncé ainsi générée est utilisée lors de l'accès au lexique mental.
Cet article présente des données indiquant que les informations segmentales et suprasegmentales présentes dans le signal de parole peuvent jouer un rôle positif ou négatif lors de ce processus de compétition selon que ces informations sont compatibles ou incompatibles avec la représentation lexicale des candidats.
De plus, nous discutons l'existence d'un “feedback” du lexique vers le niveau pré-lexicale qui s'ajouterait ainsi au flux d'information nécessaire entre le niveau pré-lexicale et le lexique.
Lors de deux experiences de catégorisation phonétique, conduites en néerlandais, les auditeurs avaient pour tâche de décider de l'identité d'une fricative ambiguë présente au début ou à la fin d'une séquence qui correspondait, selon l'identité attribuée à cette fricative, soit à un mot, soit ou un nonmot (e.g., une fricative ambiguë entre [f] et [s] placée dans une séquence mot–nonmot, maf–mas, ou dans une séquence nonmot–mot, jaf–jas).
Ce biais lexical était, cependant, plus faible lorsque les temps de réaction lors de la catégorisation étaient lents, même lorsque les auditeurs étaient encouragés à répondre le plus vite possible.
Ces résultats sont problématiques pour les modèles dans lesquels l'analyse pré-lexicale du signal sonore est modulée par un “feedback” opérant systématiquement, toutes les fois où un mot est perçu.
L'amincissement est assurément un descripteur de forme majeur pour l'analyse d'image et la reconnaissance de forme.
Le rehaussement est aussi un outil essentiel pour faciliter l'interprétation visuelle et la compréhension des images de documents notamment celles qui sont bruitées et floues.
Nous décrivons dans cet article une méthode d'amincissement et de rehaussement utilisant un filtre de chocs dérivant de celui introduit par Remaki et Cheriet pour le rehaussement.
Ce nouveau filtre utilise un champ de diffusion spécifique initial.
L'utilisation de tels champs apporte un nouveau degré de liberté aux filtres de chocs, puisque ceux-ci sont spécifiques aux applications (amincissement, rehaussement) et permettent ainsi au même filtre d'être utilisé pour différentes applications.
Nous illustrons la performance de notre méthode par des résultats probants obtenus sur des images manuscrites.
L'association chez Saussure et ses prédécesseurs immédiats d'un héritage empiriste et d'une approche combinatoire des faits linguistiques a rendu difficile l'édification d'une théorie satisfaisante de la signification (Bedeutung), le plus souvent comprise comme résultant d'un empilement de représentations mentales (Vorstellungen).
Une difficulté face à laquelle le signe saussurien fait plutôt figure d'échappatoire.
Certains contemporains ont toutefois privilégié une autre approche, centrée sur la Darstellung (soit la « représentation » , entendue cette fois comme technique de figuration et non plus comme phénomène cognitif).
Si différents qu'aient été les objets grammaticaux ainsi traités, ces tentatives ont pour point commun de postuler l'existence de propriétés organisationnelles spécifiques au représentant lui-même, et d'argumenter par là en faveur de la motivation du signe.
Elles partagent toutefois avec les théories centrées sur la Vorstellung une orientation essentiellement sémantique, ce qui circonscrit de facto leur sémiotique.
Ce travail s'inscrit par le cadre du développement méthodologique d'une architecture multi spécialistes, appelée MESSIE, pour l'interprétation d'images.
Nous présentons, dans cet article, les spécifications du système pour l'interprétation d'objets, en particulier ceux faits de main d'homme dans l'imagerie aérienne.
La principale difficulté de ce problème est l'expression des connaissances nécessaires à l'interprétation : les connaissances liées aux Objets, à la Stratégie et à la Scène.
L'architecture MESSIE (1), de type tableau noir, est organisée sur ces quatre types de bases de connaissances, qui schématiquement se décomposent en deux niveaux.
Le premier niveau correspond à la connaissance liée à la Scène et à la Stratégie et le deuxième correspond aux spécialistes (un par type d'objet).
Chaque niveau n'accède qu'à certains des points de vue.
Les erreurs rencontrées dans le discours des enfants montrent que certains enfants formulent d'aboard la transformation affixe et l'inversion sujet-auxiliaire comme copie sans effacement.
D'autres erreurs suggèrent que certains enfants peuvent formuler d'autres règles de mouvement comme l'effacement sans copie.
En se fondant sur l'analyse de ces erreurs on fait une proposition sur le mécanisme de l'acquisition du langage : ce mécanisme d'acquisition du langage formule les hypothèses sur les transformations en terme d'opérations fondamentales.
Melyador de Jean Froissart s'inscrit dans la lignée du roman de chevalerie courtoise.
En digne héritier, il met en scène quatre grands tournois, autour desquels s'articulent l'architecture de l'œuvre et les intrigues amoureuses de l'histoire.
L'examen de ces morceaux convenus permet de voir qu'ils reproduisent et assument la part de stéréotypie inhérente au thème, mais non sans la transcender et en faire un mode de renouvellement esthétique.
La stéréotypie de l'épisode de tournoi se trouve en effet à la fois généralisée et ramenée à son essence, tout en étant très stylisée au point de conférer une dimension lyrique et une circularité à l'épisode.
Dans Melyador, le temps du spectacle chevaleresque rejoint ainsi, de manière originale, la dimension poétique de l'œuvre.
Les réseaux de neurones ont été utilisés dans une large gamme d'applications.
En particulier, des résultats satisfaisants ont été observés dans le domaine de la reconnaissance de formes.
Toutefois, dans le contexte de la reconnaissance de la parole, l'utilisation des réseaux de neurones est difficile vue l'absence du paramètre temps dans leur structure.
D'un autre point de vue, dans une application de reconnaissance de la parole, un mot peut être reconnu et bien classé ou reconnu et mal classé, il est donc impératif de pouvoir expliquer le raisonnement qui a conduit à cette décision.
Dans cet article, nous considérons deux insuffisances des réseaux de neurones artificiels : le manque de connaissances explicites du domaine d'application et l'absence de l'aspect temps dans la structure des réseaux.
À cet effet, nous proposons un modèle de neurone temporel et spécialisé, que nous appelons STN (pour specilized temporal neuron).
Ce modèle est ensuite utilisé comme élément de base dans un réseau neuro-symbolique pour la reconnaissance de la parole.
L'utilisation des processeurs de signaux n'est pas encore très courante, en dépit des avantages qu'ils procurent.
Nous présentons ici un certain nombre de réflexions menées dans le sens d'une plus grande facilité d'utilisation de ces processeurs.
Ces réflexions ont été concrétisées dans l'élaboration d'une architecture de processeur, à la fois optimisée et d'utilisation aisée.
Le travail présenté dans cette correspondance s'appuie sur l'idée que le développement d'algorithmes pour la reconnaissance automatique de la parole nécessite des connaissances variées qui doivent coopérer.
Afin d'aider au développement de tels algorithmes, appelés algorithmes hybrides, et de façon plus générale pour le traitement automatique de la parole, des architectures avancées et des environnements intégrés sont souhaitables.
Il est aussi souvent nécessaire de représenter les connaissances du domaine parole étudié par l'intermédiaire de structures abstraites, de manipuler des bases de données importantes, et de modéliser, simuler, et évaluer des systèmes de reconnaissance automatique de la parole.
Afin de prendre en compte ces différents aspects, nous proposons dans cette correspondance le nouveau concept de laboratoire artificiel pour le traitement automatique de la parole.
Un tel système simule un laboratoire réel et permet de manipuler des données.
Il fournit aussi à l'utilisateur un ensemble de fonctions préprogrammées qui facilitent la modélisation et la simulation d'algorithmes de traitement automatique de la parole.
Dans cette correspondance nous décrivons brièvement les principaux concepts du prototype de laboratoire artificiel en cours de développement.
Les méthodes d'ensemble ont été utilisées avec succès comme schéma de classification.
Les algorithmes d'élagage d'ensembles de classifieurs sont apparus afin de réduire la complexité de ce paradigme populaire d'apprentissage.
Cet article présente une nouvelle méthode efficace d'élagage d'ensembles qui, non seulement réduit de manière significative la complexité des méthodes d'ensemble, mais permet aussi une meilleure précision de classification que la version sans élagage.
Cet algorithme consiste à ordonner tous les classifieurs de base par rapport à leur entropie qui exploite une nouvelle version de la marge des méthodes d'ensemble.
La confrontation de cette méthode avec l'approche naïve d'élagage aléatoire des classifieurs de base et avec un autre algorithme d'élagage par ordonnancement a permis de montrer sa supériorité à travers une analyse empirique conséquente.
Dans des expériences précédentes, il a été montré que la perception du voisement dans les séquences consonantiques de deux obstruentes en Néerlandais (C1C2) était affectée par les paramètres suivants : la durée de la fermeture de la deuxième consonne dans la séquence, le VOT, la tenue voisée (VTT - l'intervalle entre la fermeture orale et l'arrêt de voix), la durée de la résonance de la voyelle précédente, la position de l'accent et l'intensité du bruit de friction.
L'objectif de l'expérience présentée ici vise à déterminer si les effets des six indices sont additifs ou interactifs, et à établir l'importance relative des six indices dans la perception.
Les résultats (mesurés en termes de réponses voisée-voisée, non voisee-voisée, non voisée-non voisée et voisée-non voisée) montrent que les effets des paramètres sont additifs et que — bien que la présence/absence de périodicité (VOT et VTT) soit le facteur primordial dans la perception de “voix” — la perception est influencée de façon considérable par la durée de la “C2” et la durée de la “voyelle précédente”
Les langues moyen-indiennes appartiennent à la même famille linguistique que le sanskrit,
mais leurs grammaires présentent une situation contrastée : les prakrits littéraires sont décrits par des grammairiens qui utilisent le sanskrit, modèle normatif par excellence qu'elles étendent donc ;
en revanche, le pali (langue des écritures du bouddhisme Theravāda), est décrit au moyen du pali.
Cet article examine les raisons susceptibles d'expliquer cette différence surprenante, alors même que prakrits et pali présentent de nombreux points communs dans leurs évolutions phonétiques ou morphologiques.
Le choix d'une langue distincte, le pali, est-il plus qu'une différence superficielle ?
Inversement, le choix du sanskrit est-il un obstacle à prendre en compte les réalités des langues décrites ?
On tente de répondre à ces questions à partir d'un cas exemplaire : le fonctionnement de la description du verbe et le traitement de la notion de racine.
Comment les grammairiens négocient-ils entre le modèle sanskrit omnipotent et la réalité de conjugaisons qui, tendant à prendre pour forme de base le thème du présent, développent des paradigmes réguliers ?
Nous discutons les résultats de l'application à un problème réel simplifié de quatre techniques de classification - les plus proches voisins, méthode des mélanges, perceptron multi-couches et les réseaux auto-organisateurs de Kohonen.
Le problème choisi est l'identification des mots anglais “yes” et “no” prononcés par plusieurs locuteurs et représentés par de simple vecteurs.
Les résultats montrent que le perceptron multi-couches est supérieur aux autres méthodes et qu'il est d'une faible complexité à l'exécution.
Ce texte présente deux systèmes performants d'identification et de vérification du locuteur fondés sur la modélisation par mélange de gaussiennes, une caractérisation statistique robuste de l'identité d'un locuteur.
La méthode d'identification est un classificateur à maximum de vraisemblance ; celle de vérification est un test de rapport de vraisemblance appuyé sur une normalisation des locuteurs.
Les systèmes ont été évalués sur quatre bases de données publiques de parole : TIMIT, NTIMIT, Switchboard et YOHO.
On y trouve une variabilité et des différences de qualité permettant d'évaluer les systèmes selon différents points de vue.
Les contraintes y varient de l'élocution par mots isolés à la parole spontanée ; la qualité sonore va de la quasi-perfection à l'enregistrement téléphonique.
L'identification dans un ensemble fermé de 630 locuteurs, pour TIMIT et NTIMIT, a atteint les taux respectifs de 99.5% et de 60.7%.
Pour une population de 113 locuteurs extraits de Switchboard, le taux d'identification a été de 82.8%.
Les taux globaux d'erreurs (à seuil égal) de 0.24%, 7.19%, 5.15% et 0.51% ont été obtenus dans les expériences de vérification sur les bases de données TIMIT, NTIMIT, Switchboard et YOHO.
Ce papier présente une nouvelle technique de réduction du nombre de canaux spectraux pour aider à la classification des images multispectrales en mode d'occupation du sol.
Cette technique, basée sur des réseaux de neurones multicouches, propose une règle d'apprentissage de ces réseaux qui adapte le gradient conjugué à la méthode de rétropropagation ; permettant ainsi une convergence rapide au réseau.
Dans cet article, nous développons un modèle d'image qui fait appel aux champs aléatoires markoviens de Pickard dans le but de modéliser des notions contextuelles aussi vagues et imprécises que « l'uniformité d'une région » ou « la continuité du bord d'un objet » .
Nous décrivons une méthode d'estimation par maximum de vraisemblance a posteriori obtenue par une généralisation simple d'une méthode largement utilisée dans le contexte unidimensionel de la reconnaissance de la parole.
Des liens sont nécessaires pour surmonter le fossé entre l 'analyse de la parole en tant qu'ensemble d'unités linguistiques discrétes, ordonnées mais atemporelles et les analyses de signaux acoustiques continúment variables dans le temps.
Les dispositifs actuels de reconnaissance et de synthése usent mal de la structure imposée par les processus de production de la parole lors de l'application d'une séquence allophonique sur la pluralité des signaux de parole y associés.
Nous avons développé en tant qu'aide à la description phonétique un cadre de réference articulatoire à la fois quantitatif et souple.
Nous proposons des unités articulatoires pour quelques allophones de phonèmes anglais ainsi que des méthodes de liaison avec des allophones adjacents.
Des spécifications préliminaires pour un sous-ensemble d'allophones sont proposèes qui sont basées sur des résultats déjà publiés.
Les schèmes articulatoires sont organisés par rapport à des événements saillants E.
Ces derniers peuvent concerner la coordination inter-articulateurs entre deux articulateurs différents et quasi autonomes ou concerner un seul articulateur maintenu dans un ètat statique.
La coordination entre deux événements est exprimée à travers la durée D de l'intervalle temporel qui les sépare.
Nous donnons six exemples de construction d'un plan articulatoire complet pour une séquence d'anglais.
Ce plan forme le premier module d'un modèle numérique des processus articulatoires, aérodynamiques et acoustiques de la production de la parole.
Le signal ainsi synthétisé est acoustiquement modifié afin de simuler les variations observables en parole naturelle et dues aux options des locuteurs, y compris les modifications du débit d'élocution.
Ceci est obtenu en altérant une ou plusieurs valeurs D dans le plan temporel et en négligeant quelques actions optionnelles.
La variabilité observée lors de multiples répétitions par un locuteur réel peut être simulée en perturbant les valeurs D.
Le modéle réclame d'être confronté à des locuteurs réels afin d'évaluer le réalisme de sa simulation de la variation et de la variabilité des traits acoustiques parole naturelle ainsi que son degré de prédictabilité des covariations.
Cet article explore dans quelle mesure la qualité vocale d'un locuteur, définie comme le timbre perçu de la parole d'un individu, évolue en fonction de la variation mélodique.
Les analyses sont fondées sur plusieurs productions de la voyelle `a' correspondant à différents patrons intonatifs.
I1 apparaı̂t qu'en général la fréquence fondamentale covarie avec la `relation de force' entre les deux premiers harmoniques (H1–H2).
Cette relation détermine dans une certaine mesure la qualité vocale et elle est souvent avancée comme le reflet du quotient d'ouverture.
Cependant, la corrélation de la mesure de H1–H2 avec les paramétres du modéle LF ŕevéle que le quotient d'ouverture et l'asymétrie de l'impulsion glottique ont tous deux un impact sur la partie basse du spectre harmonique
Nous avons réalisé trois expériences dans le but d'examiner l'effet d'intégration temporelle sur la sonie dans des syllables CV.
La voyelle du stimulus de référence [ta] utilisé dans les tests de perception a une durée de 250 ms et une intesité de 70 dB SPL. Les durés retenues pour la voyelle du stimulus variable [ta] sont : 75, 100, 150, 200 et 250 ms.
L'intervalle entre les voyelles a été fixé à 200 ms, dans l'expérience I et à 600 ms, dans l'experience II.
Pour ces deux expériences, nous avons employé une méthode d'adjustment pilotée par ordinateur.
le niveau de différence mesuré en dB est fonction décroissante de la durée de la variable. Il est inférieur à 2 dB pour la durée la plus bréve et égal à 0.2 dB pour la durée la plus longue.
Dans l'expérience III, nous avons employé la méthode 'Up-Down-Transformed-Response' et les mêmes stimulus que dans l'expérience II.
Les valeurs du niveau de différence obtenues dans cette derniére expérience sont légèrement supérieures à celles mesurées dans les expériences I et II, mais elles définissent une courbe de même allure.
Un nouvel algorithme est décrit pour l'extraction de la fréquence fondamentale.
La méthode est basée sur l'utilisation ittérative d'un filtre linéaire de phase nulle et réponse fréquencielle décroissante en fonction de la fréquence.
Les résultats montrent que cette méthode est efficace et résistante à la présence de bruit.
Dans cet article, nous présentons une nouvelle approche dans le domaine de la fouille textuelle, dans le but de faciliter le développement d'interfaces intelligentes pour les utilisateurs de systèmes de recommandation et de recherche documentaire.
En recherche documentaire, le principal problème pour les utilisateurs est de disposer de connaissances quasiment parfaites du domaine d'application et de sa terminologie.
Notre approche vient atténuer cette nécessité en permettant d'encoder les connaissances de domaines d'application de manière à ce que les systèmes de recherche documentaire puissent transformer la terminologie (relative aux domaines) des utilisateurs en celle des experts des domaines respectifs.
Cette transformation effectuée, les systèmes peuvent consulter leurs bases de données pour trouver les informations recherchées.
La faisabilité de notre approche est démontrée par l'étude de cas d'un système de recommandation d'émissions de télévision.
L'engouement du grand public pour les applications multimédia sans fil ne cesse de croître depuis le développement d'Internet.
Des contraintes d'hétérogénéité de canaux de transmission, de fiabilité, de qualité et de délai sont généralement exigées pour satisfaire les nouveaux besoins applicatifs entraînant ainsi des enjeux économiques importants.
C'est dans ce cadre que s'inscrit le panorama présenté ici.
Cet article présente d'une part un état de l'art sur les principales techniques de codage et de décodage conjoint développées dans la littérature pour des applications multimédia de type téléchargement et diffusion de contenu sur lien mobile IP.
Sont tout d'abord rappelées des notions fondamentales des communications numériques à savoir le codage de source, le codage de canal ainsi que les théorèmes de Shannon et leurs principales limitations.
Les techniques de codage décodage conjoint présentées dans cet article concernent essentiellement celles développées pour des schémas de codage de source faisant intervenir des codes à longueur variable (CLV) notamment les codes d'Huffman, arithmétiques et les codes entropiques universels de type Lempel-Ziv (LZ).
La biométrie, qui désigne la mesure d'attributs caractéristiques du corps humain, est très utile pour authentifier un individu, comme par exemple pour le contrôle d'accès.
Le marché de l'authentification par des approches biométriques est favorisé par les récents progrès des technologies informatiques, et l'essor du commerce électronique et des objets de communication nomades (téléphones et ordinateurs portables, PDA, etc.), qui nécessitent d'identifier automatiquement une personne physique plutôt que d'utiliser un mot de passe ou une carte d'accès.
Bien que chacune des techniques biométriques présente un intérêt particulier suivant l'application visée, nous constatons que les systèmes de reconnaissance basés sur l'iris sont parmi les plus fiables.
Nous montrerons la faisabilité d'intégration de la technologie de l'iris sur les futurs terminaux mobiles, et plus particulièrement la portabilité de la chaîne algorithmique de traitement d'images d'iris sur une plate-forme multimédia embarquée basée sur le module-cœur ARM920T.
La suppression d'échos permet d'obtenir l'agrément d'une conversation à mains libres, le feedback et le guidage vocal rendent possible l'utilisation du téléphone dans les situations qui requiérent toute l'attention visuelle, enfin et surtout, la reconnaissance de la parole libère l'utilisateur de la manipulation du clavier pour composer le numéro d'appel.
L'article présente de manière claire et accessible un appareil qui incorpore ces diverses technologies et qui a été réalisé comme une extension à la série des produits Philips pour la téléphonie mobile.
L'accent est mis sur les algorithmes de reconnaissance de la parole.
La robustesse aux changements d'environnement acoustique a été améliorée par l'estimation et la soustraction du spectre à long terme.
Nous montrons que lorsque cette dernière opération est effectuée récursivement, elle est équivalente aux techniques de filtrage de type passe-haut de même qu'aux méthodes “RASTA” (approaches spectrales relatives) qui ont été récemment proposées dans la litérature.
La téléconférence par le RNIS, les services multi-média sur CD-ROM, la télévision haute définition créent de nouvelles applications et de nouveaux paris pour le codage large-bande des signaux et en particulier de la parole.
Dans le codage de la parole large-bande, un point de repère important est le standard CCITT à 64 kbit/s pour la parole à 7 kHz.
Des résultats de recherche récents mettent en avant de meilleures performances — une plus large bande à 64 kbit/s, ou une bande de 7 kHz pour des débits réduits à 32 ou 16 kbit/s.
Le codage audio de signaux dans une bande de 20 kHz suscite une attention accrue, avec l'activité récente de l'ISO (International Standard Organisation), dans le but de stocker un canal audio monophonique de la qualité du Compact Disque à un débit n'excédant pas 128 kbit/s.
Les chances d'y parvenir sont élevées.
En contrepartie, les algorithmes qui apparaissent offriront une option très attirante à des débits tels que 96 et 64 kbit/s.
L'étude présentée ici s'interroge sur le contenu émotionnel perçu des “Affect Bursts” pour l'Allemand.
Les Affect Bursts sont définis comme expressions courtes, émotionnelles et non-verbales.
Cette étude démontre que les Affect Bursts, présentés sans contexte, peuvent transmettre un sens émotionnel clairement identifiable.
L'influence de la structure ségmentale sur la reconnaissance des émotions, vis-à-vis de la prosodie et du timbre, est étudiée.
Le degré d'accord entre les transcripteurs est utilisé comme critère expérimental pour distinguer entre les Affect Bursts Crus, réflexifs, et les Emblèmes Affectives conventionalisées.
Un compte rendu détaillé de 28 classes d'Affect Bursts est présenté, comprenant l'émotion perçue et le taux de reconnaissance dans des tests de perception orale et écrite, ainsi qu'une transcription phonétique de la structure ségmentale, du timbre et de l'intonation.
En situant dans des contextes culturels et philosophiques les tentatives d'enseigner le langage à des espèces non humaines, les auteurs ont examiné les positions évolutionnistes de l'origine du langage ainsi que la thèse de Quine sur l'adéquation imparfaite de la traduction.
L'opposition du langage naturel acquis par les humains et du langage artificiel enseigné aux non humains, les amènent à traiter l'esprit humain comme une combinaison d'apprentissage, de cablages et de cognition.
Les auteurs discutent la nature de chacun de ces composants, suggèrent des interactions possibles et comparent le système humain à trois composants avec le système à deux composants des autres espèces.
Récemment, nous avons proposé un modèle entrée-sortie de l'impulsion glottique.
Mathématiquement parlant, l'impulsion est décomposée en une excitation cosinusoïdale et deux formeurs non linéaires.
L'impulsion est reconstituée lorsque la cosinusoïde est injectée dans les formeurs.
Dans cet article, nous montrons que les cycles de l'onde glottique d'un locuteur peuvent être synthétisés à partir des formeurs d'un faible nombre de cycles de référence.
En effet, les systèmes non linéaires ne sont pas caractérisés par une fonction de transfert.
Par conséquent, on peut supposer que les formeurs non linéaires d'une impulsion glottique sont moins variables que la forme d'onde proprement dite.
Deux expériences ont été réalisées afin de vérifier ces hypothèses.
Dans la première, les formes d'onde statiques émises par un modèle à deux masses des cordes vocales ont été copiées.
Dans la deuxième, le signal glottique obtenu pour un logatome [ama], produit par un locuteur masculin, a été analysé et synthétisé.
Chaque impulsion a été caractérisée par son amplitude de crête, sa période et son facteur de forme.
Dans les deux expériences, les indices de toutes les impulsions glottiques ont été copiés en calculant les coefficients des formeurs de deux cycles de référence et en ajustant les paramètres de contrôle de la cosinusoïde à l'entrée jusqu'à obtention des valeurs d'indices souhaitées à la sortie.
Ce texte porte sur un codeur d'analyse-synthèse encodant les objets plutôt que les blocs N × N constituant l'image.
Les objets sont décrits par trois ensembles de paramètres décrivant leur trajectoire, leur forme et leur couleur.
Ces ensembles sont obtenus par une analyse de l'image basée sur des modèles de source d'objets 2D ou 3D en mouvement.
Des techniques de codage connues sont utilisés pouur encoder ces ensembles.
Ce codage par objets introduit des distorsions géométriques au lieu d'erreurs de quantification.
A l'aide des ensembles de paramètres transmis une image peut être reconstruite par synthèse basée sur modèle.
Des résultats expérimentaux obtenus lors d'une première mise en oeuvre du codeur sont présentés et commentés.
Nous proposons une méthode statistique de modélisation du dialogue basée sur la théorie de l'information et la théorie des actes de langage.
Le modèle du dialogue consiste en trigrammes d'énoncés classés en fonction de leurs actes de langage associés.
Il peut être utilisé pour éliminer, à la sortie de l'étage de reconnaissance, les candidats erronés (syntaxiquement et sémantiquement corrects mais incorrects d'un point de vue contextuel) en examinant si ces énoncés candidats forment un discours local naturel du point de vue du séquencement des actes de langage.
Comme ce modèle est basé sur la théorie de l'information, nous pouvons définir des mesures objectives de la qualité du modèle de dialogue, comme la perplexité du discours.
Des expériences sur 100 dialogues au clavier, incluant 2722 énoncés et 38954 mots, montrent que ce modèle de dialogue peut prédire le type d'acte de langage de l'énoncé subséquent.
En utilisant 90 dialogues pour l'apprentissage et les 10 autres dialogues pour le test, on obtient un score de prédiction correcte de 39.7% pour le premier candidat et de 61.7% pour les 3 premiers candidats.
Nous montrons également que l'on peut améliorer le modèle de langage en combinant le modèle de dialogue avec le modèle de phrases.
Calculée sur les 100 dialogues, la perplexité de mots des bigrammes de mots associés à des trigrammes d'actes de langage est de 7.27 alors que celle d'un simple bigramme de mots est de 1.16.
Pour construire un système robuste de traitement du langage naturel, on doit incorporer un module de gestion du dialogue.
L'élaboration d'un tel module requiert une bonne compréhension des conversations homme-homme.
Dans ce but, on propose ici une tentative de présentation de la notion de conversation comme collaboration.
Les arguments justifiant le fait que la conversation est, de façon inhérente, une collaboration sont basés sur des phénomènes syntaxiques observés dans des conversations spontanées en Anglais.
Dans cet article, “collaboration” signifie co-production simultanée d'une conversation par les participants et non pas seulement la construction de significations conversationnelles par des contributions ponctuelles, alternantes, des interlocuteurs.
Les structures syntaxiques que l'on évoque pour soutenir ce point de vue sont des listes, des questions en écho, des réponses brèves, des productions associées et ce que nous appelons ici des “structures parallèles” et des “accomodations”.
Des suggestions sont faites en ce qui concerne l'impact de cette approche sur l'analyse des conversations et l'élaboration d'interfaces homme-machine.
Cet article est consacré à l'évaluation statistique des descriptions de tables de contingence fournies par les arbres d'induction.
On se limite au cas particulier de données catégorielles.
Trois aspects sont successivement abordés.
i) La nature de l'ajustement en apprentissage supervisé, où l'on souligne la distinction entre prédiction de valeurs individuelles et prédiction de leur représentation sous forme de table de contingence.
ii) La description de tables fournies par les arbres d'induction que l'on compare notamment à la modélisation log-linéaire utilisée en statistique.
iii) L'adaptation au cas des arbres d'induction des mesures et statistiques de qualité d'ajustement utilisées en modélisation log-linéaire.
La discussion est complétée par une illustration sur les données du Titanic.
Une représentation du signal de parole comme somme de fonctions élémentaires permet de réaliser simplement des modifications spectro-temporelles, globales ou locales.
Après avoir discuté de la modélisation du signal de parole en somme de fonctions élémentaires, un système automatique d'analyse-synthèse est présenté.
Les paramètres du modèle sont estimés grâce à une analyse trame par trame : modélisation et segmentation spectrale en utilisant la transformée de Fourier à court terme et la prédiction linéare, filtrage adaptatif par transformée de Fourier à court terme suivant cette segmentation, détection et modélisation des formes d'ondes élémentaires dans chaque bande d'analyse.
Les paramètres du modèle étant pertinents pour le modèle acoustique linéaire de production de la parole, leur manipulation autorise des traitements comme, par example, l'expansion/compression spectrale, la modification de la fréquence de voisement, des formants, du bruit de frication.
C'est dans ce dernier secteur que se situent l'appareil d'enseignement et l'Académie, simple institution spécialisée, y compris les textes (circulaires, programmes) qui expliquent cette action organisatrice.
Dans les trois cas, l'État ne produit qu'indirectement et marginalement des normes de langue – en dépit de mythes tenaces.
Tout cela souligne la composante idéologique, centrée sur le concept de légitimité – mais aussi consensus, norme-normalisation, insécurité, etc.
Ces débats concernent la nature de l'État et la cohésion du corps politique.
Nous proposons, dans le présent papier, une approche originale dans un cadre statistique pour l'identification automatique des reins (sains et pathologiques) sur des images tomographiques bidimensionnelles (CT).
Notre approche est constituée de deux phases : une phase de localisation suivie d'une phase de délimitation.
La phase de localisation est guidée, d'une part, par un modèle a priori spatial et d'autre part, par un modèle a priori sur les niveaux de gris, statistiquement appris.
La seconde phase consiste à utiliser les résultats de la localisation afin de délimiter la région du rein en utilisant un ensemble de règles.
Cette approche est testée sur des images cliniquement acquises et des résultats satisfaisants sont obtenus.
On présente ici un bilan de certaines études récentes menées au KTH sur l'acoustique de la source vocale et sur l'analyse et la modélisation du flux glottique associé.
On examine les aspects temporels et fréquentiels du processus de production dans le but de relier les paramètres du flux glottique obtenus par filtrage inverse et les fonctions de transfer du conduit vocal à l'amplitude et à la largeur de bande des formants.
On propose de nouvelles méthodes de détermination de la constante de temps Ta = 1 (2πFa) dans la phase de retour de la dérivée du flux glottique après l'instant d'excitation, et donc de la pente globale du spectre glottique.
Un filtrage inverse sélectif, ne conservant qu'un seul formant, semble être à cet égard une méthode prometteuse.
L'influence des imprécisions dans la quantification des fonctions de transfert du conduit vocal est illustrée sur un exemple : on montre par calcul que l'introduction d'un effet fini d'atténuation de la tête ajoute une amplification supplémentaire haute-fréquence à l'amplification standard de 6 dB/octave.
Une attention particulière a été apportée aux variations temporelles au sein d'un énoncé, telles que dérivées d'un filtrage inverse continu.
On examine les questions de voisement bruité et de différences homme-femme dans la production de la voix.
On démontre que, sur un énoncé produit par un locuteur masculin, le profil temporel de l'amplitude d'excitation E e (t) peut être approché par l'enveloppe de la partie négative du signal de parole.
Cet article présente une étude acoustique et perceptive des accents focal et postfocal dans les questions en italien de Naples.
Dans cette variété de l'italien, les accents mélodiques des questions à reponse “oui/non” sont caractérisés par une montée suivie d'une descente avec un pic très marqué (L*+H).
Lorsque la focalisation intentionelle est placée tôt, un accent postfocal aligné avec la dernière syllabe accentuée de la phrase intonative est produit ( !H*).
Les résultats perceptifs suggerènt que l'accent postfocal ( !H*) n'est pas l'accent nucléaire de la phrase intonative, malgré sa position finale.
La nature phonétique et phonologique des accents focal (L*+H) et postfocal ( !H*) sont aussi analysées en production à l'aide de questions “oui/non” caractérisées par types de focalisation différents.
Les résultats défendent l'hypothèse que les accents focal et postfocal sont structurellement différents dans la mesure où l'accent postfocal est plus réduit.
Finalement, nous avons étudié l'alignement temporel et les valeurs mélodiques de la montée initiale et de la chute finale pour des constituants focaliques de différentes tailles dans des questions où la focalisation est placée tôt.
Les résultats suggèrent un effet de “répulsion tonale” (Silverman et Pierrehumbert, 1990) à la position temporelle du pic de l'accent L*+H, ainsi qu'un effet de “troncature apparente” de la chute finale, dans les constituants focaliques d'un seul mot.
Les simulations à base d'agents (MABS) ont été utilisées avec succès pour modéliser des systèmes complexes dans de nombreux domaines.
Néanmoins, un problème des MABS est que leur complexité augmente avec le nombre d'agents et de types de comportements différents considérés dans les modèles.
Pour des systèmes de taille moyenne à grande, il est impossible de valider, voire même d'observer simplement les trajectoires des agents individuels lors d'une simulation.
Les approches de validation classiques, où seuls des indicateurs globaux sont calculés, sont trop simplistes pour permettre d'évaluer le modèle de simulation avec un degré de confiance suffisant.
Il est alors nécessaire d'introduire des niveaux intermédiaires de validation et d'observation.
Dans cet article, nous proposons l'utilisation de la classification automatique de données (clustering) combinée à la caractérisation automatisée de clusters pour construire, décrire et suivre l'évolution de groupes d'agents en simulation.
La description de clusters est utilisée pour générer des profils d'agents qui sont réintroduits dans les simulations avec l'objectif d'étudier la stabilité des descriptions et des structures des clusters sur plusieurs simulations et de décider de leur capacité à décrire les phénomènes modélisés.
Ces outils permettent au modélisateur d'avoir un point de vue intermédiaire sur l'évolution du modèle.
Ils sont suffisammentflexibles pour être appliqués à la fois hors ligne et en ligne comme le montrent les analyses réalisées à la fois sur des simulations Netlogo et sur des logs de simulations.
Les modèles dominants de la reconnaissance des mots parlés font jouer un rôle essentiel au composant d'accès lexical dans l'identification des mots.
Pour ces modèles, le processus de reconnaissance est une opération simple qui associe l'input à des représentations lexicales emmagasinéés en mémoire ou qui active directement ces représentations.
Etant donnée cette caractérisation du problème, on voit mal comment la connaissance qu'a l'auditeur de la structure de sa langue pourrait faciliter la reconnaissance des mots.
Et pourtant, il peut être nécessaire d'utiliser la connaissance grammaticale pour résoudre bien de problèmes considérés comme importants par les théories de la reconnaissance des mots : la segmentation, la variabilité de la réalisation acoustique des mots et la reconnaissance de mots nouveaux.
Cet article prend au sérieux la possibilité que la connaissance grammaticale participe à la reconnaissance des mots.
Il étudie quels types d'information pourraient servir à cette fin.
Il essaye aussi d'esquisser quel type de systéme de reconnaissance des mots pourrait tirer profit des régularités que l'on rencontre dans les langues naturelles.
Nous présentons un environnement flexible pour la génération d'hypothéses lexicales en parole contibue.
Aprés avoir décrit l'interface avec les autres modules de notre système de compréhension de la parole, un algorithme de vérification et de détection de mots baśe sur HMM est discuté.
La génération des modèled de réfŕence pour la procédure de mise en correspondance est effectuée automatiquement en utilisant la prononciation standard du mot ainsi qu'un ensemble de règles phonologiques sur les phénomǹes d'assimilation internes au mot.
Les prononciations possibles sont représentées par des graphes à extrémités libellées.
Nous présentons quelques résultats obtenus par la procédure de mise en correspondance.
Une étude sur l'utilisation de l'apprentissage bayésien des paramètres de densités multigaussiennes a t́é effectuée.
Dans le cadre des modèles markoviens cachés à densitiés d'observations continues (CDHMM), l'apprentissage bayésian est un outil très général applicable au lissage des paramètres, à l'adaptation au locuteur, à l'estimation de modèles par groupe de locuteurs et à l'apprentissage correctif.
Le but est d'augmenter la robustesse des modèles d'un système de reconnaissance afin d'en améliorer les performances.
Notre approche consiste a utiliser l'apprentissage bayésien pour incorporer une connaissance a priori dans le processus d'apprentissage sous forme de densités de probabilités des paramètres des modéles markoviens.
La base théorique de cette procédure est présentéee, ansi que les résultats obtenus pour le lissage des paramètres, l'adaptation au locuteur, l'estimation de modèles propres à chaque sexe et l'apprentissage correctif.
Vingt locuteurs masculins néerlandais ont lu 47 mots-test présentés en liste et dans de courtes phrases.
Ils ont également eu à énoncer un sous-ensemble de ces mots directement à partir de la présentation d'images.
On a demandé à un groupe de vingt auditeurs d'identifier une voyelle non-accentuée dans chacun de ces mots-test.
Les réponses des auditeurs ont été codées en deux grandes catégories : “voyelle complète” et “schwa”.
Nos buts étaient (1) de trouver dans quelle mesure les auditeurs étaient capables de distinguer ces deux classes de façon non-ambigue, (2) d'étudier l'influence de la fréquence d'occurrence des mots sur la classification des voyelles test, (3) d'étudier l'influence des styles d'élocution sur cette classification en comparant les conditions “liste”, “images” et “phrases”.
Les résultats expérimentaux montrent que (1) les auditeurs, souvent, ne peuvent pas classer de façon non-ambigue les voyelles test, en particulier quand elles apparaissent en position “interstress”, (2) le nombre des réponses “schwa” était beaucoup plus élevé pour les voyelles contenues dans des mots à fréquence d'occurrence élevé, (3) le nombre de réponses “schwa” augmentait dans le cas d'élocution plus relachée.
Des mesures acoustiques sur les voyelles test montrent une relation claire entre les résultats perceptifs et les indices acoustiques.
Bien que les pré-conditions du changement “voyelle complète → schwa” soient excellentes dans un certain nombre de mots en néerlandais, la réalisation concrète de ce changement est, de notre point de vue, bloqué dans une large mesure par la correspondance assez étroite en néerlandais entre les réalisations acoustiques des voyelles et leurs représentations orthographiques.
Récemment, des résultats quantitatifs ont été publiés sur les changements acoustiques observés dans la parole Lombard par rapport à la parole normale.
Ces résultats montrent que le réflexe Lombard est très dépendant du locuteur.
Dans cet article, nous examinons brièvement l'influence de l'environment acoustique sur la production de la parole et résumons les principales caractéristiques de l'effet Lombard.
Nous présentons ensuite des résultats expérimentaux montrant comment le réflexe Lombard varie avec le sexe du locuteur, le langage et l'environnement (e.g. type de bruit).
Enfin, nous discutons comment l'utilisation d'indices relationnels peut permettre de réduire l'influence du réflexe Lombard sur les systèmes automatiques de reconnaissance de parole.
Dans le présent article, nous utilisons les résultats d'une étude de la langue anglaise et l'appliquons dans un algorithme de syllabification.
Notre système permet aussi l'identification automatique d'accents étrangers et peut etre utilisé avec des données générées de façon manuelle ou automatique.
Les éléments de la structure syllabique sont definis selon leurs positions à l'intérieur des structures des syllables et des mots.
Les éléments de la structure syllabique qui apparaissent uniquement à la bordure d'un morphème sont identifiés comme éléments périphériques ; ceux qui se présente à n'importe quel endroit de la morphologie du mot sont identifiés comme éléments centraux.
Toutes les langues font potentiellement une distinction entre les éléments centraux et péripheriques de leur structure syllabique. Cependant, les formes que prennent ces structures syllabiques varient d'une langue à l'autre.
En plus des problémes posés par les différences entre les inventaires de phonèmes, nous nous attendons à ce que les personnes avec de grandes différences structurelles entre la langue maternelle et étrangere sont celles qui ont les plus grandes difficultés dans la prononciation de la langue étrangère.
Dans cet article, nous analysons deux accents étrangères de l'anglais australien : la première est basé sur l'arabe, dont la structure centrale et périphérique est similaire à l'anglais, et la seconde est fondé sur le vietnamien, dont la structure diffère au plus au degré de celle de l'anglais.
Nous proposons une approche nouvelle en reconnaissance de la parole continue fondée sur un décodage phonétique ; à chaque phonème doit être assocée une fonction temporelle : la plausibilité d'observer ce phonème.
Nous présentons un critère de choix de la meillure phrase, lié à la somme des plausbilités des symboles qui la composent.
En se fondant sur l'idée d'utiliser les régions de forte plausibilité pour réduire la complexité de calcul tout en préservant l'optimalité, notre méthode trouve les phrases les plus plausibles pour la parole en entrée, étant donnée la plausibilité μ a,n d'observer le phonème a à l'instant n.
Deux procédures d'optimisation ont été défines pour traiter les deux problémes de recherche imbriqués suivants : (1) trouver le meilleur chemin joignant les pics de la fonction de plausibilité de deux symboles successifs, et (2) trouver l'instant de transition optimal entre deux pic donnés.
On emploie dans les deux cas la programmation dynamique.
La reconnaissance est très performante, car l'algorithme de recherche du meilleur chemin ne procède pas trame.
Les résultats expérimentaux du système VINICS montrent que cette méthode produit la meilleure précision de reconnaissance et qu'elle demande environ 1/20 du temps de calcul méthodes traditionnelles de programmation dynamique.
Le système expérimental a obtenu 95% de reconnaissance de phrases dans un test pluri-locuteur.
On présente un nouvel algorithme de recherche pour la reconnaissance de la parole continue á trés grand vocabulaire.
La complexité de cet algorithme augmente de seulement dix fois en passant des mots isolés á la parole continue.
On donne des résultats préliminaires obtenus en testant le systéme de reconnaissance sur des livres sur cassette utilisant un dictionnaire de 60 000 mots.
Cet article traite d'un système d'analyse-synthèse basé sur la méthode de covariance et qui est capable de manipuler indépendamment les fréquences des formants et leur largeuz de bande.
L'analyse synchronisée de la fréquence fondamentale est faite sur la base du minimum local de l'erreur quadratique normalisée.
Après avoir estimé les fréquences des formants et leur largeur de bande, on modifie les coefficients prédicteurs de manière à ce qu'ils soient la solution de la nouvelle équation polynominiale.
Ce système peut être appliqué pour modifier la voix et comme moyen de recherche en perception de la parole sur la qualité vocale et la personnalité du locuteur.
Depuis de nombreuses années, l'analyse de réseaux neuronaux suit un essor fantastique.
L'étude suivant cette approche des fonctions postulées dans le système nerveux requiert de puissants outils de simulation.
En nous inspirant à la fois des caractéristiques générales en traitement des signaux et des tendances actuelles vers le parallélisme, en matière de structures de calculateurs, nous proposons une architecture d' « array » processeur performante pour l'étude de réseaux récursifs adaptatifs en particulier et pour l'analyse de données (signal, image) en général : c'est le processeur « CRASY » (Calculateur de Réseaux Adaptatifs SYstolique).
Nous présentons dans cet article un opérateur de classification rapide adapté au traitement d'images, ses performances en classification, ainsi que son intégration dans un circuit ASIC.
Pour effectuer une segmentation ou un classement d'images en vue de la détection de défauts, il est souvent impossible de trouver un nombre réduit de paramètres caractéristiques pertinents qui permettent de discriminer les classes.
Nous proposons une méthode de classification géométrique par apprentissage de polytopes de contraintes, qui autorise l'utilisation d'un grand nombre de paramètres et assure une vitesse de décision élevée.
L'opérateur de décision associé à cette classification a été intégré sous forme de circuit précaractérisé dont la simplicité de mise en œuvre, la rapidité et la robustesse en classification sont des qualités qui lui permettent de rivaliser avec les opérateurs neuronaux.
Dans cet article, un algorithme de segmentation d'images basé sur une technique d'étiquetage crédibiliste est présenté.
La contribution essentielle de ce travail réside dans la façon dont les images sont modélisées par des fonctions de croyance de façon à représenter l'incertitude inhérente à l'étiquetage d'un voxel à une classe.
L'allocation de masse réalisée pour chaque voxel est construite à partir des caractéristiques intrinsèques des régions qui composent l'image.
Afin de limiter cette incertitude dans la phase d'étiquetage, on diminue de façon progressive un seuil de décision tout au long d'un processus itératif jusqu'à sa stabilisation.
Cet algorithme est appliqué à la segmentation de volumes d'intérêt sur des images TDM.
Cette étude explore l'héritage intellectuel laissé par le juriste koufien Muḥammad b. Sulaymān al-Kūfī (m. au début du iv e/x e siècle) qui s'établit au Yémen et appartint au cercle de lettrés dans l'entourage d'al-Hādī ilā l-Ḥaqq Yaḥyā b. al-Ḥusayn (m. 298/911).
Elle propose une analyse de son œuvre la plus importante, le Kitāb al-Muntaḫab, à travers un examen singulier des pratiques rituelles qui se focalise sur l'emploi de la basmala dans la prière quotidienne.
En conclusion, nous montrons la valeur du Muntaḫab comme voie d'accès possible à un courant de jurisprudence koufien qui n'a pas survécu à l'époque moderne.
Grâce aux progrès en technologie des transducteurs, en traitement du signal et en informatique, il est désormais possible de réaliser des saisies sonores de bonne qualité dans des zones spatiales données malgré des conditions acoustiques difficiles.
Les techniques de formation de voie et de filtrage adapté sont appliquées à des antennes bi- ou tri-dimensionnelles de capteurs.
La performance des antennes est évaluée de amnière préliminaire par la simulation informatique des salles et par une analyse des images de leur effet d'environnement.
Les résultats incitent à penser que des signaux de bonne qualité peuvent être récupérés dans des zones spatialement localisées de milieux fermés fortement réverbérants.
Réciproquement, on peut appliquer les mêmes techniques à la projection sonore spatialement sélective.
Les contraintes grammaticales et sémantiques jouent un rôle essentiel dans l'interprétation ou la compréhension des expressions linguistiques.
Toutefois, elles semblent être inadéquates pour faire un choix parmi plusieurs candidats qui sont tous soit corrects, soit incorrects d'un point de vue grammatical ou sémantique.
Il est clair que l'être humain interprète une expression linguistique d'une manière contextuelle, même s'il y a plusieurs interprétations possibles.
Cette méthode calcule des scores de similarité entre expressions linguistiques et des probabilitiés pour sélectionner l'expression la plus adéquate.
Elle tient compte de classifications de type morpho-syntaxique et de type force illocutoire, qui sont associées aux fréquences de séries d'expressions linguistiques voisines, stockées dans des bases de données d'exemples.
Une unité expérimentale de traitement, effectuant ce type d'analyse du contexte local, a été implémentée dans un prototype de traduction bilatérale (anglais et japonais), prouvant son applicabilité à la sélection de candidats dépendants du contexte pour la traduction.
Ce mécanisme d'analyse du contexte local peut être utilisé dans des systèmes de traduction conventionnels sans traitement contextuel pour augmenter la précision de la traduction.
Dans cet article, nous montrons comment le problème de l'acquisition des grammaires peut être ramené à un problème d'apprentissage inductif de règles heuristiques.
Nous décrivons le système GASRIA (grammaire apprise par simulation répétée intelligemment et automatiquement) composé de : un module d'apprentissage inductif appelé SAGE, basé sur une méthode originale capable d'analyser des chaînes de caractères non analysables par les méthodes existantes, en l'occurrence l'algorithme de l'analyse partielle (AAP), développé et testé, un environnement de programmation en logique de premier ordre, une base de connaissances comprenant une base de règles avec variables.
Nous présentons une nouvelle méthode interactive de visualisation 3D de données multimédia (numériques, symboliques, sons, images, vidéos, sites Web) en réalité virtuelle.
Nous utilisons un affichage 3D stéréoscopique permettant de représenter les données numériques.
La navigation au sein des visualisations est effectuée grâce à l'utilisation d'un capteur 3D à six degrés de liberté qui simule une caméra virtuelle.
Des requêtes interactives peuvent être posées à la machine par l'utilisation d'un gant de données reconnaissant les gestes.
Nous montrons comment cet outil est appliqué sur un cas réel concernant l'étude de la peau humaine saine.
Dans cet article, on décrit un système mono-plaquette en ligne destiné à la reconnaissance d'un mot isolé et indépendant du locuteur.
Le systéme est constitué, d'une part, d'un prétraitement câblé qui est un modèle simplifié du traitement auditif périphérique et, d'autre part, d'un microprocesseur.
A l'aide de plusieurs tests, l'influence de la longueur du mot, de la gamme dynamique et de la configuration du filtre sur la performance de reconnaissance a été étudiée.
Les résultats indiquent qu'une gamme dynamique de 30 dB semble adéquate et que trois ou quatre bits par canal sont suffisants pour encoder l'information d'amplitude.
Enfin, on a examiné l'influence que pouvait avoir le fait de varier les paramètres du filtre.
Nous présentons une nouvelle méthode de reconnaissance de caractères manuscrits fondée sur le principe des modèles de Markov pseudo-2D ou planaires (PHMM).
Cette méthode se situe dans la classe des techniques qui tentent de rétablir la forme prototype d'un caractère à partir d'une version déformée de celle-ci.
Contrairement aux approches souvent proposées dans ce domaine, notre méthode ne repose sur aucune extraction de traits explicite et produit des scores de reconnaissance qui sont des estimations des probabilités bayesiennes.
Elle se prête à un apprentissage automatique.
Une de ses caractéristiques distinctive est l'utilisation d'un modèle probabiliste (réseau de Markov) réellement bi-dimensionnel mais causal pour estimer les probabilités.
Enfin, on montre comment on peut engendrer des images synthétiques des caractères possibles et ce suivant leur probabilité estimée par le modèle.
Nous décrivons dans cet article des problèmes typiques rencontrés avec les équipements mains-libres dans le contexte de la radiotéléphonie GSM.
Nous commencons par résumer les principales caractéristiques du bruit dans un véhicule en mouvement, et nous décrivons également le phénomène d'écho acoustique.
Nous montrons que, pour assurer une qualité de parole suffisante, ces équipements mains-libres doivent introduire des dispositifs de réduction de bruit (NR) et de contrôle de l'écho acoustique (AEC).
Nous présentons ensuite deux solutions techniques envisageables pour l'amélioration de la qualité du signal de parole dans un tel contexte.
En conclusion, nous soulignons le fait que le choix d'une structure particulière parmi celles proposées est conditionné par l'algorithme d'adaptation du système de contrôle de l'écho acoustique considéré.
Une nouvelle approche de la modélisation adaptative du niveau sémantique du langage a récemment été proposée.
La reconnaissance ou la compréhension sont envisagées comme une procédure de Transduction de Formats qui exploite l'ensemble des contraintes acoustiques et linguistiques qui ont été encodées dans les modèles appris pour entrer directement des signaux acoustiques bruts et sortir les messages sémantiques qui sont convoyés par ces signaux.
Dans cet article, l'approche proposée est revue et de nouvelles améliorations sont présentées.
Des résultats préliminaires sur de la parole continue avec un grand espace sémantique (nombres espagnols de l à l million) sont présentés.
Un système est décrit dans cet article qui ajoute à la parole synthétique des effets d'émotions simulées.
Les paramêtres de contrôle d'un synthétiseur de parole sont contrôlés par des règles de facon à simuler les caractéristiques des émotions exprimées dans la voix humaine.
Le système, qui simule six émotions vocales, a été testé sur les auditeurs naïfs.
Les résultats montrent que le système reproduit des émotions vocales reconnaissables avec un taux de perception égal à celui de recherches antérieures sur la parole “expressive”.
Ce système a été développé comme prothèse vocale pour les personnes muettes, mais on peut envisager son utilisation pour améliorer toute application de synthèse de parole par règles.
Cet article décrit un système de classification phonétique basé sur un réseau de neurones récurrent appliqué aux informations visuelle et auditive.
On fournit les résultats de quelques expériences de reconnaissance phonétique, en modes dépendant et indépendant du locuteur, concernant les plosives en Italien.
Dans cet article, nous nous efforçons de présenter, de la façon la plus complète possible, une approche pour la conception de systèmes adaptatifs complexes, basée sur les systèmes multi-agents adaptatifs et l'émergence.
Pour cela, nous décrivons tout d'abord la théorie des Amas (Adaptive Multi-Agent Systems).
Cette théorie donne des critères locaux de conception des agents qui permet l'émergence d'une organisation au sein du système et donc également l'émergence de sa fonction globale.
Puis, nous étudions une application en e-éducation utilisant cette théorie à travers son fonctionnement technique et des expérimentations qui sont ensuite discutées.
D'autres applications (prévision de crues, e-commerce, routage téléphonique) sont aussi décrites, illustrant divers domaines où la théorie a également été appliquée avec succès.
Finalement, nous caractérisons les phénomènes d'émergence dans ces applications et positionnons notre théorie par rapport à d'autres.
La modélisation statistique du signal de parole a été largement utilisée en reconnaissance automatique du locuteur.
Les performances obtenues avec cette approche sont excellentes, en laboratoire.
Cependant, une dégradation significative des performances est observée avec de la parole de qualité téléphonique ou bruitée.
Pour palier ce problème, il est nécessaire de mieux comprendre la nature de l'information spécifique du locuteur exploitée par ces méthodes statistiques.
Cette connaissance doit permettre de mieux prendre en compte l'information pertinente et/ou de mettre à contribution de nouvelles sources d'information.
La première partie de cet article reporte des expériences visant à spécifier les événements acoustiques les plus utiles à la reconnaissance du locuteur.
Enfin, la possibilité d'exploiter l'information dynamique contenue dans la relation entre une trame et les p suivantes est évaluée.
Dans une seconde partie, les auteurs proposent une nouvelle procédure de sélection de l'information spécifique du locuteur.
En effet, les méthodes conventionnelles de sélection de paramètres (sélection ascendante, méthode du knock-out) ne permettent d'évaluer la pertinence d'une source d'information que de façon globale et a posteriori.
Cependant, certains locuteurs sont mieux caractérisés par une source d'information que d'autres.
De plus, la pertinence des sources d'information dépend de la qualité de l'échantillon de test.
Face à ce besoin de traitements spécifiques suivant le locuteur et d'adaptation à l'environnement, nous proposons un système permettant de sélectionner automatiquement les parties les plus discriminantes d'une portion de parole.
L'architecture proposée divise le signal de test en blocs temps–fréquence.
Le score de vraisemblance correspondant est calculé en sélectionnant dynamiquement les blocs temps–fréquence les plus pertinents.
Une réduction significative du taux de mauvaise identification (jusqu'à 41% de réduction relative du taux de mauvaise identification sur TIMIT) est observée.
Finalement, des expériences réalisées dans le cas d'un bruit simulé, montrent le potentiel de cette méthode pour traiter des signaux de parole partiellement dégradés.
Cette étude compare l'efficacité de trois types d'informations acoustiques pour compléter la lecture labiale : la sortie, filtrée passe-bas, d'un électroglottographe, un substitut sinusoïdal, de fréquence variable, pour la fréquence fondamentale (Fo) et un substitut sinusoïdal, de fréquence fixe, pour représenter le voisement.
Les deux signaux sinusoïdaux ont été synthétisés avec une amplitude constante pendant les périodes de voisement.
Ces signaux ont été préparés “off-line” en combinant des estimations automatiques et manuelles de contours de Fo extraits de phrases enregistrées en vidéo.
Ces signaux ont ensuite été resynchronisés avec la partie audio des enregistrements.
Pour 12 adultes bien entendants, on a pu observer que tant le signal électroglottographique que le substitut de Fo (sinusoïdal et de fréquence variable) augmentent de 30 et 35%, respectivement, le nombre de mots reconnus dans des phrases portant sur des sujets connus.
L'ampleur de cet effet est plus importante pour des phrases longues mais est indépendante des capacités de base de lecture labiale.
Le substitut de fréquence fixe fournit une amélioration de 13%, ce qui suggère que approximativement un tiers de l'effet d'amélioration de la lecture labiale du à Fo peut être attribué à la seule détection du voicement.
Dans cet article, nous présentons un cadre d'apprentissage général pour la classification supervisée.
Ce cadre ne nécessite que la définition d'un opérateur de généralisation et fournit en particulier des méthodes d'ensemble.
Pour les tâches de classification de séquences, nous montrons que l'inférence grammaticale, avec des objectifs différents, a déjà défini de tels apprenants pour certaines familles d'automates comme les réversibles ou les k-TSS.
Nous proposons ensuite un opérateur de généralisation original pour la famille des boules de mots.
Enfin, nous montrons au travers de différentes expérimentations que notre approche permet effectivement de résoudre des tâches de classification de séquences.
La majorité de la recherche publiée sur le sujet de l'adaptation se concentre sur l'adaptation au locuteur, ainsi qu'à l'adaptation aux canaux et environnements bruités.
Dans cet article, nous présentons une étude sur l'adaptation à une tâche, qui fournit également des gains de performance significatifs.
Nous explorons plusieurs questions qui n'ont pas été adressées auparavant sur le sujet de l'adaptation, et présentons de nouvelles solutions à ces problèmes.
En particulier, nous montrons que l'adaptation peut conduire à un accroissement du taux d'erreur sur les phrases considérées non-grammaticales par le système.
Nous présentons un algorithme automatisé d'ajustement des indices de confiance pour corriger ce problème.
Nous montrons qu'adapter séparément chaque grammaire améliore la qualité de l'adaptation acoustique.
Nous montrons également que l'adaptation acoustique concentrée sur des données considérées grammaticales par le système améliore les résultats de manière significative.
Nous faisons l'étude de l'adaptation à une tâche au niveau acoustique ainsi qu'au niveau grammatical, et montrons que les gains obtenus sont additifs.
Enfin, nous montrons que l'adaptation améliore aussi bien la vitesse que le taux d'erreur, alors que les études traditionnelles se sont plutôt concentrées sur le taux d'erreur uniquement.
Nous étudions également les modes d'adaptation traditionnels tels que l'adaptation supervisée ou non-supervisée, l'utilisation de niveaux minimums de confiance pour l'adaptation non-supervisée, ainsi que l'influence de la quantité de données sur la performance.
L'objectif de la recherche rapportée dans cet article est d'identifier les causes qui peuvent expliquer la performance de différentes techniques de normalisation du canal de transmission.
A cet effet nous avons comparé quatre techniques de normalisation de canal dans le contexte de reconnaissance de chiffres connectés sur ligne téléphonique : substraction de la moyenne cepstrale, la représentation cepstrale dynamique, ainsi que le filtrage RASTA et RASTA corrigé pour rotation de phase.
Nous avons employé des Modèles de Markov dépendants et indépendants du contexte qui ont été entraı̂nés avec une grande variation de complexités des modèles.
Les résultats de nos expériences de reconnaissance indiquent que chaque technique de normalisation doit conserver les fréquences de modulation entre 2 et 16 Hz dans le spectre du signal de parole.
De plus, les composantes DC du spectre de modulation doivent être écartés.
Dans les modèles indépendants du contexte, le filtre de normalisation du canal doit avoir une réponse de phase plate.
Enfin, nous avons trouvé que la soustraction de la moyenne cepstrale et RASTA corrigé pour rotation de phase conduisent à des performances similaires pour les modèles dépendants et indépendants du contexte si l'on emploie des nombres égaux de paramètres de modèles.
En phonétique articulatoire, la parole est décrite comme étant une séquence de gestes articulatoires distincts, produisant un événement acoustique qui devrait se rapprocher d'une cible phonétique.
Du fait du recouvrement des gestes, ces cibles phonétiques ne sont souvent atteintes qu'en partie.
Atal (1983) a proposé une méthode permettant le codage de la parole, basée sur ce que l'on appelle la décomposition temporelle de la parole en une séquence de fonctions-cibles de recouvrement et de vecteurs-cibles correspondants.
Les vecteurs-cibles peuvent être associés à de positions articulatoires idéales.
Les fonctions-cibles décrivent l'évolution temporelle de ces cibles.
Cette méthode ne requiert pas de connaissances articulatoires ou phonétiques particulières.
Nous l'avons élargie et modifiée pour mieux déterminer le nombre et la position des fonctions-cibles ainsi que pour corriger les défauts de la méthode originelle.
Grâce à ces améliorations, la décomposition temporelle est devenue un outil robuste pour l'analyse de la parole, dont pourraient bénéficier les chercheurs travaillant au codage, à la reconnaissance et à la synthèse de la parole.
Les compagnies de téléphone américaines reçoivent plus de six milliards d'appels de renseignement par jour.
L'automatisation, ne serait-ce que d'une partie du service des appels téléphoniques, pourrait réduire considérablement le coût de ces services.
Cet article explore deux facteurs affectant une automatisation réussie des renseignements téléphoniques : (a) l'impact de la taille de l'annuaire sur la reconnaissance de parole, et (b) la complexité des interactions des appels.
Les performances du système de reconnaissance de parole pour un ensemble de 200 noms prononcés ont été mesurées pour des services de renseignement ayant entre deux cents et un million et demi de noms à une seule occurrence.
Le taux de reconnaissance décroit de 82,5% pour des renseignements sur deux cents noms à 16,5% pour un corpus de 1,5 million de noms.
Il est probable que la mise en pratique de l'automatisation de ces services se concentrera sur un petit pourcentage d'appels, demandant un vocabulaire plus petit, en partie parce que un taux très élevé de reconnaissance ne peut pas être obtenu pour des vocabulaires extrêmement vastes, hors contexte.
Afin de maximiser les gains potentiels, le vocabulaire optimal est constitué des listes de noms les plus demandés.
Afin d'identifier les questions critiques dans l'automatisation des demandes fréquentes aux services de renseignement, à peu près 13.000 appels de renseignement d'un bureau proche d'une grande ville des Etats-Unis ont été étudiés.
Dans cet échantillon, 245 intitulés couvrent 10% du volume d'appels, 870 intitulés couvrent 20% du volume d'appels.
Cet article propose un cadre général pour la reconaissance de la parole continue sous contraintes grammaticales, dans lequel le développement du graphe syntaxique et l'analyse sont menés grâce aux mêmes opérations élémentaires simples, à savoir la multiplication et l'addition de matrices de vraisemblance.
On montre que cette analyse syntaxique matricielle est une généralisation de la méthode CYK.
De plus, elle autorise la prise en compte de la durée suprasegmentale pour toutes les catégories grammaticales.
Les résultats préliminaires montrent que grâce à l'amélioration due à la prise en compte des durées des mores, les taux de reconnaissance au niveau de la phrase s'élèvent de 86.7% à 88.5%.
Une forme originale de débats poétiques en relation avec la lyrique courtoise s'élabore aux XIIé et XIIIè s.
Appelés jeux-partis en langue d'oïl ils connaissent un certain succès dans le cadre urbain du puy d'Arras.
Formulant une casuistique amoureuse, ils sont à la croisée de différentes formalisations : poétique, juridique et scolastique.
L'enjeu du débat est exprimé sous le mode dilemmatique.
L'argumentation mise en oeuvre se déploie autour de trois énoncés fondamentaux : sentences, proverbes et images.
La sentence, inscrite dans le discours lyrique qu'elle érige en axiomatique de l'amour, occupe une fonction ambivalente : elle est l'énonce 'e que l'on se propose de discuter et al forme qu'adopte la démonstration.
Se combinant avec une syntaxe de la démonstration, elle ne produit que l'illusion de la dialectique et révèle, en creux, le caractère tautologique du jeu-parti.
Sur 120 poèmes, il n'y a pas moins de 80 expressions proverbiales qui s'articulent différemment sur le contexte — quoique majoritairement par des formules assertives — mais qui toutes rompent avec son isotopie, illustrant et énonçant en même temps la règle.
La procédure d'exemplarisation, sous la forme d'énoncés imagés, opére d'autres déplacements.
Si le proverbe, issu de l'univers empirique, universalise la situation à laquelle il se réfère, l'image procède à l'inverse : d'un thème général, elle offre l'exemple d'une ou de plusieurs situations anecdotiques, d'où surgit l'universalité de la règle.
Or aucun de ces énonc'es n'entrent dans une logique démonstrative et proprement argumentative.
Ils resten clos sur eux-mêmes.
Les interlocuteurs ne reprennent pas ce qui vient d'être dit, sinon sous une forme brutale de réfutation.
La véritable formalisation est polémique.
Discours qui manipule l'ironie, frôle l'insulte et recherche l'effet plus que le raisonnement.
Discours qui bâtit sa vérité - contradictoire - comme un jeu tourné vers un auditoire dont on saisit et quête les complicités. Car la vérité reste bien au coeur du débat et elle est sans cesse assertée. Comme elle l'est dans les disputes, ces combats des clercs.
Disputes et ieux-partis mettent en scéne une logique de la controverse.
Ils sont des “argumentations spectacles”.
L'élaboration de la vérité se fait ailleurs, dans les sommes par exemple.
Le jeu-parti est aporétique.
S'il est une réponse á son questionnement, elle se lit dans le poéme d'amour dont la forme enferme le sic et non de la joy, joie et jeu d'amour.
L'utilisation de connaissances supplémentaires conjointement au signal de parole est une méthode classique pour améliorer les performances et la robustesse des systèmes de reconnaissance automatique de la parole (RAP).
Ce papier décrit la méthode que nous avons développée pour l'intégration adaptative des informations acoustiques et visuelles dans un système de RAP.
Chaque modalité est impliquée dans le processus de reconnaissance avec un poids différent ; celui-ci est adapté dynamiquement pendant le processus en fonction du rapport signal sur bruit et du contexte.
Les systèmes que nous avons développés en utilisant les modèles de Markov cachés respectent successivement les trois modèles perceptifs suivants : intégration directe (DI), intégration séparée (SI) et intégration hybride (DI+SI).
Les expériences réalisées avec différents niveaux de bruit ont montré que d'une part, de meilleures performances sont obtenues pour le système basé sur DI+SI que pour ceux basés sur DI ou SI, et que d'autre part les résultats des systèmes sont globalement améliorés en utilisant une pondération adaptative des deux modalités.
De plus, ces résultats confirment l'importance d'utiliser une unité de décision spécifique pour la reconnaissance purement visuelle dans les systèmes basés sur SI ou DI+SI.
La résolution de problèmes à états et actions continus par l'optimisation de politiques paramétriques est un sujet d'intérêt récent en apprentissage par renforcement.
Dans cet article, nous considérons PI2 en tant que membre de la famille plus vaste des méthodes qui optimisent une fonction de coût via une moyenne des valeurs des paramètres pondérée par les récompenses.
Nous comparons PI2 à d'autres membres de la même famille – la « méthode de l'entropie croisée » et CMA-ES 1 – au niveau conceptuel et en termes de performance.
La comparaison débouche sur la dérivation d'un nouvel algorithme que nous appelons PI2-CMA pour « Path Integral Policy Improvement with Covariance Matrix Adaptation » .
Le principal avantage de PI2-CMA est qu'il détermine l'amplitude du bruit d'exploration automatiquement.
Nous illustrons cet avantage sur un exemple non trivial de robotique simulée.
Ces dix dernières années de nombreuses recherches dans le domaine des contextes ont été menées, cependant, peu d'entre elles ont utilisé la logique comme sémantique.
Cette théorie est étendue par les enregistrements à types dépendants - DTR - qui permettent la représentation de connaissances partielles et le raisonnement sur des données évolutives.
Cet article présente différentes approches pour la reconnaissance de parole en présence de bruit, intégrées à une Modélisation Stochastique des Trajectoires de parole (STM).
Nous décrivons 4 méthodes : adaptation des modèles acoustiques par régression linéaire, transformation de l'espace acoustique de référence, combinaison de modèles stochastiques de parole et bruit, filtrage par état du signal bruité.
L'évaluation des différentes approches est effectuée en mode dépendant du locuteur, sur une application de reconnaissance de parole continue comportant un vocabulaire de 1011 mots avec une grammaire de perplexité 28 (paire de mots).
Les différentes approches sont évaluées sous des conditions de bruit additifs variées, comprenant différents types de bruit et différents rapports signal-à-bruit.
Les expériences montrent que l'adaptation des modèles par régression linéaire conduit aux meilleurs résultats, pour tous les types de bruits testés, et pour des rapports signal-à-bruit modérés (de 6 à 24 dB).
En présence d'un bruit Gaussien, pour un rapport signal-à-bruit variant de 6 à 24dB, l'adaptation par régression linéaire réduit le taux d'erreur de mots de 20% à 59% par rapport aux autres approches.
Cet article traite du problème de l'apprentissage des réseaux de neurones à fonctions radiales de base pour l'approximation de fonctions non linéaires L 2 de R d vers R.
Pour ce type de problème, les algorithmes hybrides sont les plus utilisés.
Ils font appel à des techniques d'apprentissage non supervisées pour l'estimation des centres et des paramètres d'échelle des fonctions radiales, et à des techniques d'apprentissage supervisées pour l'estimation des paramètres linéaires.
Les méthodes d'apprentissage supervisées reposent généralement sur l'estimateur (ou le critère) des moindres carrées (MC).
Cet estimateur est optimal dans le cas où le jeu de données d'apprentissage (z i,y i) i = 1, 2,.., q est constitué de sorties y i, i = l,..,q bruitées et d'entrées z, i = l,..,q exactes.
Cependant lors de la collecte des données expérimentales il est rarement possible de mesurer l'entrée z i sans bruit.
L'utilisation de l'estimateur des MC produit une estimation biaisée des paramètres linéaires dans le cas où le jeux de données d'apprentissage est à entrées et sorties bruitées, ce qui engendre une estimation erronée de la sortie.
Cet article propose l'utilisation d'une procédure d'estimation fondée sur le modèle avec variables entachées d'erreurs pour l'estimation des paramètres linéaires (pour l'apprentissage supervisé) dans le cas où le jeux de données d'apprentissage est à entrées et sorties bruitées.
L'interprétation géométrique du critère d'estimation proposé est établie afin de mettre en évidence son avantage relativement au critère des moindres carrés.
L'amélioration des performances en terme d'approximation de fonctions non linéaires est illustrée sur un exemple.
Cet article porte sur la reconnaissance automatique de cibles partiellement immergées par imagerie IR.
On dispose d'un ensemble d'images réelles acquises dans des conditions différentes (présence de bruits, plus ou moins contrastées).
Très classiquement, en reconnaissance des formes, on procède à une étape de prétraitement avant la phase de décision (détection, éventuellement classification).
Le prétraitement permet alors d'extraire des cibles dans des images très peu contrastées et bruitées.
La phase de décision considérée ici est basée sur l'opération de corrélation c'est-à-dire d'un simple filtrage suivi d'un seuillage.
Nous verrons que ces deux opérations peuvent être fusionnées en une seule.
Cet article propose un estimateur de l'espérance du retour de politiques de décision déterministes en boucle fermée à partir d'un échantillon de transitions d'un système dynamique.
Cet estimateur, appelé en anglais Model-free Monte Carlo (MFMC) estimator, calcule une moyenne des retours d'un ensemble de « trajectoires artificielles » construites à partir de la politique à évaluer ainsi que de transitions du système disponibles dans un échantillon fixé dont l'acquisition s'est faite indépendamment de la politique à évaluer.
Sous certaines hypothèses de continuité lipschitzienne de la dynamique du système, de la fonction de récompense et de la politique de décision à évaluer, on montre que le biais et la variance de l'estimateur proposé sont bornés par des termes qui dépendent des constantes de Lipschitz, du nombre de trajectoires artificielles, de la parcimonie de l'échantillon de transitions ainsi que de la variance « naturelle » du retour de la politique.
Le nombre nécessairement limité de données ne permet pas de distinguer sûrement entre les régularités réelles et les régularités accidentelles lorsque les données sont décrites par un grand nombre d'attributs.
Cette méthode a été testée dans le cadre d'une application de classification de scènes naturelles.
Une partie des images disponibles a été utilisée pour identifier I(X)0 motifs fréquents, grâce à un algorithme spécialement adapté.
L'objectif du travail présenté est, dans le cadre de l'apprentissage par renforcement, de guider l'agent grâce à de la connaissance a priori que l'on aurait sur un environnement donné.
Nous proposons un formalisme procédural permettant d'introduire cette connaissance sous forme de programme.
L'idée de base de notre méthode est de proposer à l'agent deux ensembles d'actions par état ; un ensemble réduit d'actions qui est utilisé au départ, et un autre ensemble moins contraint qui sera utilisé plus tard.
Les contraintes initiales réduisent l'espace d'états et, par conséquent, réduisent le temps d'apprentissage.
Mais, parce que les contraintes initiales peuvent être trop fortes, nous définissons un mécanisme de relaxation de contraintes qui permettra d'augmenter graduellement l'espace de recherche.
Le fait de relâcher les contraintes initiales va nous permettre de prouver, pour une large classe de programmes, que la politique apprise par l'agent est aussi bonne que s'il n'y avait pas eu de contraintes.
Cette communication présente un projet de recherche consacré à la prosodie de la parole spontanée.
Deux questions inaugurent ce projet.
La première soulève le problème des différences entre la prosodie de la lecture en laboratoire et celle de la parole spontanée.
Des données du suédois montrent à l'évidence que ces différences ne sont pas fondamentales.
La seconde question concerne le rôle joué par la prosodie dans la structuration discursive de la parole spontanée.
Une méthodologie a étéélaborée afin de mieux cerner ce rôle.
Quatre analyses différentes sont ainsi appliquées : (1) une analyse discursive, (2) une analyse auditive, (3) une analyse acoustico-phonétique et (4) une analyse par synthèse.
Certaines de ces analyses sont exemplifiées à l'aide d'un des monologues persuasifs produits au cours d'un débat entre deux politiciens français.
L'accent focal et les contrastes de registre de voix semblent constituer des figures prosodiques typiques de la rhétorique politique.
Nous présentons un modèle d'apprentissage général pour la classification de documents structurés permettant de prendre en compte simultanément la structure et le contenu.
Pour cela, nous définissons tout d'abord un modèle génératif de documents structurés à l'aide de réseaux Bayésiens.
Nous transformons ensuite ce modèle génératif en un modèle discriminant en utilisant la méthode du noyau de Fisher.
Nous détaillons enfin une instance de ce modèle dédié à la classification de pages HTML.
Les expériences sur un corpus de référence montrent que la prise en compte de la structure permet un gain de performance par rapport aux modèles classiques de classification génératifs et discriminants.
Cette courte étude présente certaines caractéristiques de la phonétique acoustique qui reflètent et les caractéristiques de la voix et les mouvements articulatoires de 20 pré-adolescents (âgés de 6, 8 et 10 ans) et de 9 adults.
Les caractéristiques de la phonétique acoustique examinées comprennent les valeurs de la fréquence des formants, la coarticulation, le chevauchement gesturel et les patrons temporels.
Les caractéristiques vocales et les mouvements articulatoires nous révèlent des différences selon l'âge et le sexe, et des interactions entre l'âge et le sexe.
En plus, il y avait d'importantes corrélations entre les fréquences des formants et la variation (ou bien les déviations) associée à ces fréquences.
On a constaté également des différences individuelles quant aux modes de maturation que ne conformaient pas à l'âge strictement chronologique des sujets.
Nous rapportons et discutons de ce données par rapport au dimorphisme sexuel de l'appareil vocal, du développement de caractéristiques vocales et du mécanisme de la parole et de son comportement.
La tolérance aux fautes informatique peut aujourd'hui s'appuyer sur presque trente ans de résultats théoriques et expérimentaux.
Depuis le début des années quatre-vingt, un effort important a en particulier été porté sur l'intégration de mécanismes génériques pour la tolérance aux fautes dans des intergiciels répartis.
La présence oú l'absence d'un contexte phrastique permettait une estimation des effects inter-phrases sur les processus locaux et globaux.
Dans cette étude on présente deux expériences au cours desquelles les sujets doivent faire des décisions lexicales rapides sur des noms infléchis précédés d'adjectifs infléchis ou de pseudo adjectifs grammaticalement accordés ou non.
Les adjectifs et les pseudo adjectifs affectent tous deux les temps de décision lexicale pour les noms suggérant que l'effet de facilitation des noms infléchis par des adjectifs infléchis se fait au niveau des inflections.
Les pseudonoms infléchis cependant ne sont pas affectés de la même manière ce qui des facteurs lexicaux contribuent, avec les facteurs grammaticaux, à la facilitation.
Cet exemple de facilitation grammaticale est interprété comme un effet post-lexical sur les produits relativement indépendants des processeurs lexicaux et syntaxiques.
Une synthèse des problèmes concernant l'utilisation des filtres de Volterra transverses (FVT) en détection, estimation et filtrage spatial d'antenne est présentée, que les données à traiter soient réelles ou complexes.
Celle ci permet de couvrir simultanément des questions aussi variées que la maximisation du contraste à l'intérieur de l'espace de Hilbert des sorties des FVT, et l'estimation non linéaire en moyenne quadratique d'un processus aléatoire inconnu.
Les FVT optimaux relatifs aux trois problèmes abordés sont obtenus comme solution d'un système unique d'équations normales généralisées.
Des indications sont données à propos de l'identification adaptative, en temps réel, de ces FVT optimaux.
Cet article décrit une procédure d'anonymisation en plusieurs étapes qui a été développée sur des documents cliniques narratifs en français dans le domaine de la cardiologie.
Notre approche se fonde sur la combinaison de plusieurs méthodes, une approche à base de règles suivie par l'application d'un système à base d'apprentissage.
La combinaison des deux approches permet d'obtenir de meilleurs résultats (F-mesure de 0,881) que prises séparément, avec un rappel (0,912) plus important que la précision (0,851).
Nous présentons une extension de la notion de l'entropie conditionnelle de Shannon à une forme plus générale d'entropie conditionnelle qui formalise l'entropie conditionnelle de Shannon et une notion semblable liée à l'index de Gini.
La famille proposée des entropies conditionnelles produit d'une collection de métriques sur l'ensemble de partitions des ensembles finis, qui peuvent être employées pour construire des arbres de décision.
Les résultats expérimentaux suggèrent qu'en changeant le paramétre qui définit l'entropie il soit possible d'obtenir de plus petits arbres de décision pour certaines bases de données sans sacrifier l'exactitude de la classification.
Les méthodes d'analyse spatiale développées depuis 2 décennies reposent sur des hypothèses contraignantes qui ne sont généralement pas respectées en pratique et, par suite, donnent des résultats souvent décevants;
il s'agit notamment des hypothèses de non-corrélation entre sources, de planéité des fronts d'ondes et d'identité des capteurs.
Classiquement, l'estimation des paramètres des sources est basée sur l'information contenue dans les valeurs et vecteurs propres de la matrice interspectrale des signaux captés.
Partant des propriétés de cette matrice, on montre dans cet article qu'il existe une variante de la méthode des éléments propres, fondée sur la relation de dépendance linéaire existant entre les lignes de la matrice-sources.
L'examen des diverses causes de perte de performance met en évidence l'intérêt d'une identification aussi complète que possible des fronts d'ondes distordus.
Un nouvel algorithme est proposé in fine, permettant d'estimer les phases des éléments des vecteurs-sources, sous l'hypothèse de modules égaux.
La présente étude a pour objectif de définir les règles de détection des groupes consonantiques en français, fondées essentiellement sur le paramètre temporel.
Les syllabes cibles CCV, VCC, CV et VC combinées avec les voyelles /i, a, ã/ ont été intégrées dans des mots bisyllabiques et trisyllabiques.
Quatre corpus ont été constitués.
L'ensemble des mots (603) a été lu à cinq reprises par dix locuteurs en chambre sourde.
La durée des mots, des syllabes et des phonèmes a été mesurée avec un éditeur de signal.
Ces paramètres ont été intégrés dans 17 règles de détection des groupes consonantiques, sept macro-classes ont été définies.
Ces règles ont été testées sur le corpus GRECO BDSONS, elles ont permis un classement correct de 90.13% des groupes consonantiques.
Les algorithmes actuels d'apprentissage multiagent sont pour la plupart limités dans la mesure où ils sont incapables de gérer la multiplicité des équilibres de Nash et de converger vers l'équilibre Pareto optimal.
Nous présentons des résultats expérimentaux montrant la convergence d'un tel algorithme.
Nous étendons ensuite notre approche à un autre aspect essentiel des systèmes complexes qui est la non stationnarité des agents adverses et qui jusqu'ici a été peu étudiée.
Finalement, nous abordons la question de la non-stationnarité dans les systèmes multiagents, et présentons des pistes qui nous semblent pertinentes pour améliorer les performances d'adaptation de notre algorithme à des agents non stationnaires.
Cet article décrit la recherche actuelle sur les systèmes connexionnistes pour des tâches de reconnaissance du locuteur.
Nous considérons deux approches principales, la première est basée sur la classification directe et la seconde concerne la modélisation du locuteur.
Le potentiel des modèles connexionnistes pour la reconnaissance du locuteur est d'abord discuté puis les principaux algorithmes des deux familles sont présentés.
Nous discutons leurs possibilités et leurs performances respectives pour des tâches de reconnaissance du locuteur.
Nous comparons ces techniques avec des méthodes conventionnelles comme la quantification vectorielle et les chaînes de Markov cachées.
Le papier se termine avec quelques suggestions pour des recherches futures.
Toute approche réaliste du langage, se doit de rendre compte du passage de la communication par prélangage de l'enfant à l'utilisation de la langue à proprement parler.
Pour cela, on peut montrer qu'il existe de nombreuses bases, préalables ou nécessaires aux traits organisationnels de la syntaxe, de la sémantique, de la pragmatique et même de la phonologie, dans les activités prélangagères des enfants.
Des illustrations de ces bases préalables sont étudiées ici dans 4 domaines différents : le mode d'interprétation, par la mère des intentions de communication de l'enfant, le développement de la combinaison des référentiels, rendant le langage conforme à l'environnement, l'évolution de stratégies permettant l'utilisation de l'activité conjointe au langage, la transformation d'une organisation de type topic-comment à la prédication.
En dernier lieu on propose la conjecture suivante : la connaissance, par l'enfant des besoins de l'action et de l'interaction peut elle fournir la base à l'élaboration initiale de la grammaire.
Utilisable par les internautes mais également par les webmestres, les plans de sites obtenus présentent la particularité de s'adapter aux capacités de mémorisation des personnes handicapées visuelles.
Dans cet article nous présentons notre méthode de génération de plan qui utilise des fourmis artificielles afin de créer des regroupements de pages web sémantiquement proche, puis qui exécute l'algorithme de Prim sur chacun des regroupements réalisés par les fourmis artificielles.
Ces ouvrages sont les étapes d'un parcours qui, à travers l'emploi et la réélaboration qu'en font des humanistes comme Cristoforo Scarpa, Giovanni Tortelli, Nestore Avogadro, Niccolò Perotti, prélude à la création de la pratique lexicographique moderne.
Cet article veut illustrer justement ce parcours à travers l'analyse de quelques mots latins, dont le traitement suggère un classement typologique reflétant les différentes manières dont la lexicographie monolingue médiévale et humaniste a conçu et réalisé le rapport entre grammaire et lexique.
Nous présentons quelques aspects des principaux thèmes de recherche étudiés en Australie dans le domaine de la parole et qui ne sont pas repris de manière adéquate dans ce numéro spéciale.
Nous étudions également le développement d'outils et d'appareillages pour la recherche.
Pour les applications nécessitant une analyse par orientations, les fonctions de Gabor produisent une décomposition en ondelettes très utilisée.
Cette décomposition par orientation est très lourde en temps de calcul, pour des filtres orientés de type passe-bande positionnés à basse fréquence et appliqués par convolution directe.
Pour annuler ces modifications, des corrections adéquates doivent être prises en compte dès la génération du noyau de convolution.
Deux exemples de décomposition pyramidale sont étudiés, à titre d'illustration et de comparaison.
L 'UMLS® (Unified Medical Language System®), que l'on pourrait traduire par « Système d'unification de la langue médicale » , est un produit terminologique extrêmement riche, construit de façon pragmatique, et que l'on peut appréhender de façon multiple.
Nous donnons un aperçu de ce qu'est l'UMLS et mettons l'accent sur deux de ses aspects potentiellement antinomiques : sa relation aux ontologies et sa relation à la langue.
Nous confrontons une approche linguistique/psycholinguistique de l'organisation du lexique mental avec une approche mathématique/informatique de l'organisation « implicite » du lexique dans les dictionnaires considérés comme des graphes et dont la structure est de type « petit monde » .
Dans cet article, nous proposons un système complet d'analyse d'images comprenant toute la chaîne de traitements depuis le bas-niveau jusqu'à l'interprétation.
Il utilise à la fois un réseau de neurones et un système à base de règles.
Nous montrons que la mise en œuvre d'un système-expert fournit des informations précieuses pour la conception des réseaux.
La réalisation mixte permet d'utiliser au mieux les spécificités de chacune des approches.
Nous montrons également comment faire apprendre des configurations localement contradictoires à un réseau de neurones.
Cet article passe en revue des recherches sur le traitement moteur, perceptif et auditif de la parole, basées en particulier sur les études menées à l'Institut I.P. Pavlov de Physiologie de Léningrad par V. Kozhevnikov et L. Chistovich.
L'accent a été mis sur l'emploi de réponses orales à des stimuli de parole, c'est-à-dire les réponses instantanées (shadowing) et l'imitation.
Des versions antérieures d'une théorie motrice de la perception de la parole sont aussi présentées.
Nous étudions dans cet article une méthode de calcul approché de la Transformée de Fourier.
Sous certaines conditions, que nous précisons, le calcul des différentes composantes du spectre ne nécessite aucune multiplication.
Le calcul ainsi simplifié, peut s'effectuer en temps réel sur un micro-ordinateur non spécialisé.
De plus, le calcul des différentes composantes du spectre s'effectue d'une manière indépendante les unes des autres.
Nous avons vérifié la validité de cette méthode d'approximation sur un signal de spectre connu et sur un signal de parole réel.
Le spectre approché est obtenu avec une précision de quelques pour cent à condition que le signal soit suréchantillonné.
Cet article présente un système de reconnaissance en ligne de lettres cursives isolées qui s'appuie sur une modélisation structurée et logique des lettres (amorce, corps, ligature,..,) par l'intermédiaire de modèles de Markov cachés.
Après différents prétraitements spécifiques, on opère une segmentation dynamique des lettres en primitives locales représentatives de la trajectoire de la pointe du stylet (aspect gestuel), associées à des primitives de nature plus globale représentatives de la géométrie du tracé (aspect visuel).
Une phase d'apprentissage est ensuite réalisée sur chaque modèle associé à chaque type de lettre.
Lors de la reconnaissance, le système va estimer les probabilités de génération de la lettre à reconnaître pour chaque modèle.
On effectue ainsi une classification basée sur un critère de ressemblance.
Deux types de Réseaux de Neurones Formels (ANN) ont été comparés, pour la classification de voyelles extraites de la parole continue, à un classificateur traditionnel à moyenne la plus proche, à un VQ et à un réseau de Kohonen.
Les résultats montrent que les deux ANN offrent les meilleures performances et indiquent qu'un réseau auto-organisateur de Kohonen ne détériore pas l'information présentée à la seconde couche du réseau.
Cette investigation est la première dans une série qui a pour objet d'étudier a relation entre l'intelligibilité et la présence de différentes sortes d'erreurs dans la parole des sourds.
Dans cet article, nous avons étudié le rôle es erreurs temporelles en comparant l'intelligibilité des phrases dites par des enfants sourds avec et sans corrections des diverses déviations temporelles.
Dans un corpus de 30 phrases, énoncées par dix enfants sourds, âgés de 12 à 14 ans, la structure temporelle a été modifiée, après introduction des phrases dans la mémoire d'un ordinateur, à l'aide des programmes d'analyse et de (re)synthèse à base de L.P.C.
Ainsi six versions ont été obtenues qui différaient surtout dans la mesure où leurs caractéristiques temporelles approchaient celles des mêmes phrases énoncées par deux enfants non sourds.
L'intelligibilité de toutes les versions, y compris les phrases originales, a été déterminée par des expériences perceptives.
Les données montrent que pour la plupart des phrases un accroissement modeste mais significatif de 4.5% à 6% est obtenu à la suite des corrections temporelles.
L'élimination des pauses réduit généralement l'intelligibilité.
Ces résultants sont opposés à ceux rapportés dans des recherches appliquant la méthode de corrélation qui suggèrent un plus grand effet des facteurs temporels.
Ils correspondent davantage aux résultats d'une autre recherche (Osberger & Levitt, 1979) où l'on s'est servi de la même méthode que celle utilisée dans notre travail.
Ce travail présente une synthèse des méthodes de sous-espaces pour l'estimation de directions d'arrivée de sources ou pour l'estimation de fréquences pures noyées dans du bruit, qui ne requièrent pas de décomposition en éléments propres de la matrice de covariance des observations.
Ces méthodes qualifiées de « linéaires » , par opposition à la méthode MUSIC, parce qu'elles n'utilisent que des opérations linéaires sur la matrice de covariance des observations, possèdent en effet un intérêt certain pour des applications en temps réel, du fait de leur faible coût calculatoire et du fait qu'elles peuvent être facilement rendues adaptatives.
Au cours d'une présentation de ces méthodes, qui seront appelées dans la suite BEWE, la Méthode du Propagateur (MP) et SWEDE, nous établissons les liens qui existent entre ces méthodes et leurs différentes versions.
La complexité de chacune de ces méthodes est étudiée et discutée.
Il apparaît que BEWE est la moins complexe des méthodes linéaires.
Les performances asymptotiques (pour un grand nombre d'observations) des méthodes BEWE et SWEDE ayant déjà été étudiées dans la littérature, nous proposons ici le calcul des performances asymptotiques d'une version de la MP, la Méthode du Propagateur avec élimination du bruit ou MPEB.
Nous montrons que la MPEB est la plus performante des méthodes linéaires et que ses performances sont celles de MUSIC.
Des simulations viennent confirmer les résultats théoriques présentés ici et illustrer la comparaison des différentes méthodes.
L'implémentation de tout algorithme adaptatif linéaire quadratique nécessite, au préalable, afin d'utiliser au mieux les informations a priori du processus observation, d'estimer l'importance relative des moments d'ordre trois par le calcul du skew et du kurtosis.
Si ces moments sont nuls, c'est-à-dire pratiquement faibles, la structure adaptée de tout algorithme adaptatif LQ, LMS ou RLS, standard et rapide, est « découplée » , au sens où deux procédures stochastiques indépendantes, de pas et de gain différents, identifient récursivement et respectivement les noyaux linéaire et quadratique du filtre optimal.
Si ces mêmes moments sont d'intensité non négligeable, la structure de ces algorithmes devient couplée : une seule procédure stochastique, caractérisée par un seul pas et un gain unique, rafraîchit simultanément et de façon conjointe les noyaux linéaire et quadratique.
Ces considérations se déduisent de la façon dont s'interprète tout algorithme stochastique, à savoir comme estimée d'une procédure récursive déterministe du type gradient ou Newton-Raphson.
Vu qu'un filtre de Volterra transverse reste une fonction linéaire des paramètres, les propriétés de l'algorithme LMSLQ demeurent très semblables, en dehors de la participation supplémentaire des moments d'ordre trois et quatre, à celles de l'algorithme LMS classique.
En particulier, l'étude de la convergence fondée sur la théorie de la Mindépendance, introduite dans le contexte linéaire, s'étend sans difficulté.
En présence de moments d'ordre trois, l'inadéquation de structure d'un algorithme LMSLQ, provenant d'une mauvaise utilisation des informations a priori, se modélise, commodément, par un bruit qui vient se superposer au bruit de modèle, appelé bruit « d'inadéquation » .
La détermination de la variance de ce dernier montre à quel point ses effets s'avèrent néfastes sur le régime permanent de l'algorithme.
Nous décrivons un système de décodage acoustico-phonétique qui produit des treillis phonétiques en localisant et identifiant simultanément les unités au moyen de divers types de distances spectrales ajustées en fonction des phonèmes, du contexte et de certaines caractéristiques du locuteur.
Les résultats obtenus — pour des mots isolés ou pour des énoncés continus — font apparaître tous les phonèmes effectivement énoncés avec un score d'identification particulièrement intéressant pour la sélection ascendante de cohortes d'items dans un lexique étendu.
Partant d'une comparaison entre le premier grand dictionnaire français de synonymes de Lafaye qui distingue les entrées lexicales sémantiquement proches en opposant des cooccurrences différentes au sein de phrases où ces entrées sont habituellement produites et se pose ainsi en précurseur des analyses du Lexique-grammaire initiées par Maurice Gross et le premier dictionnaire analogique de la langue française de Boissière qui permet de trouver des mots inconnus mais en écrasant les différences grammaticales et sémantiques des entrées lexicalement proches, l'auteur propose de calculer le degré d'analogie entre deux entrées par le biais d'une description analytique définitoire se référant à la matrice dont sont issus les énoncés définitoires des deux entrées comparées.
Dans cet article, nous proposons une nouvelle méthode de détermination de pondérations d'états discriminantes, basée sur la méthode de descente du gradient généralisée, en assumant que le score d'un énoncé est la somme pondérée des logarithmes des probabilités d'états des HMMs.
Enfin, elle peut être appliquée à la reconnaissance de parole continue aussi bien qu'à la reconnaissance de mots isolés.
Pour évaluer les performances du système de reconnaissance HMM à pondération d'états, des expériences ont été menées avec des pondérations d'états au niveau du phonème ou au niveau du mot, en utilisant diverses bases de données.
Les résultats expérimentaux montrent que les systèmes de reconnaissance utilisant des pondérations d'états au niveau du phonème et du mot ont des taux d'erreur par mot réduits de 20% et de 50%, respectivement, pour des mots isolés, et de 5% pour de la parole continue.
Notre méthode aboutit à des performance de reconnaissance similaires à celles obtenues avec d'autres approches pour la parole continue mais est beaucoup plus simple à implémenter.
De nos jours, l'ordinateur est en passe de changer de l'objet gros, gris et bruyant sur notre bureau à un objet petit, transportable et connecté que la plupart d'entre nous transporte.
Cette nouvelle mobilité modifie notre façon de voir les ordinateurs et la manière dont nous travaillons avec eux.
Lorsque nous pouvons interagir avec les autres n'importe où, n'importe quand, il est impératif que ce soit le système qui s'adapte à l'utilisateur et à la situation, et non l'inverse.
Dans un deuxième temps, nous proposons une évaluation automatique d'une situation à partir d'un raisonnement à base de cas.
Nous montrons finalement comment un système multi-agent peut fournir des services sensibles au contexte dans un environnement mobile.
Nous proposons une méthode d'apprentissage à partir d'exemples qui se situe à la jonction des méthodes statistiques et de celles basées sur des techniques d'intelligence artificielle.
Notre modélisation se base sur la génération automatique de règles de classification et sur une utilisation originale du raisonnement approximatif.
La méthode d'apprentissage proposée, basée sur une recherche de corrélations linéaires entre les composantes des vecteurs d'apprentissage est multi-attributs.
L'incertitude des règles est gérée aussi bien dans la phase d'apprentissage que dans celle de la reconnaissance.
Un système baptisé SUCRAGE a été implémenté et confronté à une application réelle dans le domaine du traitement d'images.
Les résultats obtenus permettent de valider notre approche et nous autorisent à envisager d'autres domaines d'application.
De plus, ces résultats confortent notre hypothèse des imperfections : le raisonnement approximatif ou une recherche de corrélations intraclasses peuvent sensiblement améliorer les résultats.
Un environnement bruité dégrade généralement l'intelligibilité d'un locuteur humain et les performances d'un système de reconnaissance de la parole.
Ces dernières années, des efforts spécifiques ont été produits pour analyser et traiter l'effet Lombard dans le cadre de la reconnaissance automatique de la parole.
L'objectif principal du travail présenté dans cet article concerne l'étude de tendances communes à certains paramètres acoustiques dans différentes unités phonétiques, liées à l'effet Lombard.
Un autre objectif concerne l'étude de l'influence du sexe du locuteur sur la caractérisation des tendances recherchées.
Des tests statistiques approfondis sont effectués pour chaque paramètre et chaque unité phonétique sur une large base de données de parole continue en Espagnol.
Les résultats reportés confirment les changements produits sur la parole naturelle par l'effet Lombard.
Quelques nouvelles tendances ont été observées suite aux tests statistiques effectués.
Nous présentons une théorie de la reconstruction des croyances, intégrable au modèle de communication d'un agent, qui rend compte de la persistance et de la révision des croyances.
Nous analysons les théories de Cohen et Levesque (1990a) et de Perrault (1990), mettons en évidence des problèmes qu'elles ont et montrons que notre théorie ne souffre pas de ces problèmes.
Son point de départ est ce que nous appelons le principe d'observation.
Ce principe rend compte de la distinction entre ce qu'un agent observe d'un autre agent et l'action que ce dernier a réellement accomplie.
La théorie est exprimée dans un cadre de logique autoépistémique utilisée objectivement, dans le sens de Levesque (1990).
Appliquée au contexte de la communication, nous montrons que cette théorie prédit correctement les changements des croyances de l'observateur dans des cas tests tels que l'assertion sincère et le mensonge (détecté ou non).
Ces cas mettent en évidence la capacité de la théorie à appréhender des situations de dialogue régulières, mais aussi celles où apparaissent des problèmes dus à une perception erronée, telle q'une mauvaise reconnaissance en communication par la parole.
Un enregistrement simultané de la vibration des cordes vocales et du signal de parole a été effectué sur trois sujets diplophoniques, grâce à un numériseur rapide d'images (développé par les auteurs).
L'étude des sujets (le premier présentant une analyse unilatérale du nerf récurrent, deux autres une paralysie unilatérale de la branche externe du nerf laryngal supérieur) a montré dans tous les cas une différence dans la fréquence vibratoire entre la corde vocale gauche et droite.
La différence de phase entre les cordes vocales varie dans le temps;
quand elle dépasse un certain seuil, elle est réinitialisée, ce qui replace les mouvements des cordes vocales en phase.
Dans ce cas, la fermeture glottique est totale et l'excitation apparaît clairement dans le signal de parole, alors que dans le cas de déphasage des mouvements la fermeture glottique est incomplète et l'excitation est faible, ce qui se traduit par une vibration quasi-périodique dans le signal de parole.
Cet étude a pour but de déterminer les caractéristiques acoustiques des séquences voyelle-voyelle (hiatus) et semivoyelle-voyelle (diphtongue) en espagnol ainsi que d'observer les modifications de ces caractéristiques selon les proprietés du contexte de communication.
Nous avons utilisé deux groupes des données : des échantillons provenant des dialogues entre deux locuteurs qui suivent la tâche de la carte géographique, où les items du corpus correspondent aux noms dans les cartes, et la lecture des mêmes séquences.
La comparaison a été faite phonétiquement et phonologiquement : d′abord, les durées et la dynamique spectrale des diphthongues et des hiatus ont été analysées, et après, les processus phonétiques de réduction ont été inventoriés.
Les résultats montrent que la séquence voyelle-voyelle est différente de la séquence semivoyelle-voyelle dans les domaines du temps et fréquence : les hiatus sont plus longs et ils ont une trajectoire de F2 plus courbée que les diphthongues.
D′autre côté, les processus de réduction signalent la présence d′une ligne d′affaiblissement qui explique les prononciations des hiatus comme des diphthongues, et des diphthongues comme des voyelles.
Il apparaı̂t que les hiatus et les diphthongues sont deux catégories phonétiques qui peuvent être décrites selon leurs caractéristiques acoustiques et qui suivent des processus de réduction, également à d′autres catégories phonétiques, quand ils apparaı̂ssent dans des contextes de prononciation relaxés.
Les techniques de synthèse du japonais fondées sur des unités multiphonémiques sont passées en revue.
Le japonais est une langue à syllabation ouverte et son système phonologique ne comprend pas de groupes consonantiques.
Dès lors, le japonais est structuré à l'aide de syllables du type CV-auxquelles il est fait largement appel dans la synthèse par règles plutôt que le recourir à des phonèmes.
D'autres unités composites, telles que des séquences VCV ou CVC sont également utilisées pour tenir compte des effets de coarticulation.
Dans cet article, on décrit une méthode préliminaire de synthèse utilisant des unités CVC et un codage de la forme d'onde résiduelle afin d'améliorer la qualité de la parole synthétique.
Dans les systèmes de dialogue oral Homme-Machine, la compréhension de la parole spontanée est un problème difficile qui requiert des méthodes d'analyse robustes.
La plupart des systèmes sont destinés à des actions très spécifiques : la compréhension repose sur la détection de mots ou segments clefs pour remplir les différents champs de requêtes prédéterminées.
LOGUS, le système de compréhension présenté dans cet article s'appuie sur des formalismes logiques, grammaires catégorielles et graphes conceptuels, hors du champ habituel de leur utilisation.
L'analyse, incrémentielle, construit une formule logique par compositions progressives des concepts reconnus de l'énoncé.
L'article décrit et compare les deux premières versions du système.
Leurs évaluations donnent des résultats prometteurs : elles font apparaître la bonne robustesse de l'analyse et son assez bonne capacité à reconstituer le sens des énoncés.
La prise en compte plus large du contexte et la gestion du dialogue feront l'objet de travaux futurs.
Les performances des systèmes de reconnaissance de mots isolés sont généralement mesurées en fonction du taux d'erreurs.
L'Effective Vocabulary Capacity (EVC) est le vocabulaire maximum qu'un système de reconnaissance peut traiter pour un taux d'erreurs donné.
Il se base sur des mesures relativement indépendantes du vocabulaire testé et ne demandant que quelques dizaines ou centaines d'énoncés-tests.
L'algorithme EVC est testé sur des données synthétiques et réeles de systèmes de reconnaissance.
Les méthodes de visualisation souvent utilisées s'adaptent difficilement aux données ayant un grand nombre de dimensions.
Nous présentons ici un algorithme de segmentation en régions pouvant s'appliquer à des problèmes très variés car il ne tient compte d'aucune information a priori sur le type d'images traitées.
C'est un algorithme de type division-fusion.
Lors d'une première étape, l'image est découpée en fenêtres, selon une grille.
L'algorithme de division travaille alors indépendamment sur chaque fenêtre, et utilise un critère d'homogénéité basé uniquement sur les niveaux de gris.
La texture de chacune des régions ainsi obtenues est alors calculée.
A chaque région sera associé un vecteur de caractéristiques comprenant des paramètres de luminance, et des paramètres de texture.
Les régions ainsi définies jouent alors le rôle de sites élémentaires pour le processus de fusion.
Celui-ci est fondé sur la modélisation des champs exploités (champ d'observations et champ d'étiquettes) par des champs de Markov.
Les relations entre émotions et indices multimodaux ont été peu étudiées dans le cadre d'émotions autres que les émotions de base jouées par des acteurs.
Dans cet article nous présentons deux expériences permettant d'étudier ces relations par une démarche d'analyse-synthèse.
Nous partons d'interviews télévisées montrant des comportements émotionnels naturels.
Un protocole et un schéma de codage ont été définis pour les annoter à plusieurs niveaux (contexte, émotion, multimodalité).
La première expérience manuelle a permis d'identifier les niveaux de représentation qui interviennent pour faire rejouer ces comportements par un agent expressif.
La deuxième expérience fait intervenir une extraction automatique à partir des annotations multimodales.
Une telle démarche permet d'apporter des connaissances sur les relations complexes entre émotions et multimodalité.
Les modéles de reconnaissance visuelle de mots différent selon les hypothéses sur l'utilisation de l'information phonologique et les processus qui la rendent accessible.
L'anglais et le chinois ont des systémes structurés l'un selon l'alphabet, l'autre, la logographie.
Deux études ont utilisé ces caractéristiques les résultats montrant qu'avec chacun des systémes d'écriture les mots trés fréquents sont reconnus sur une base visuelle sans médiation phonologique.
La phonologie ne joue que pour le traitement de mots peu fréquents.
Ainsi, bien que d'autres différences dans les systémes d'écriture peuvent influencer sur le traitement, les différences dans la maniére dont ces écritures représentent la phonologie ne sont pas pertinents pour la reconnaissance des mots usuels.
Ces résultats sont compatibles avec un mode de reconnaissance des mots interactif et en paralléle dans lequel l'information orthographique et phonologique est activée á des latences différentes.
Comment choisir l'ensemble des variables pertinentes pour résoudre une tâche fixée ?
La sélection de variables neuronale essaye de résoudre le problème pendant l'apprentissage du réseau de neurones.
Parmi les méthodes utilisées avec les réseaux de neurones de type perceptron multicouches, certaines sont issues d'une technique d'élagage des poids, OBD (Optimal Brain Damage), proposée par LeCun et al. en 1990.
Après avoir rappelé ces différentes méthodes, cet article montre comment essayer de les améliorer en suivant quelques principes simples.
Une étude comparative situera ces différentes méthodes par rapport à d'autres techniques statistiques ou neuronales.
Nous expliquons dans un premier temps pourquoi il est nécessaire de séparer sur un signal les parties purement déterministes et les parties purement non déterministes.
Après présentation de la méthode de Prony nous donnons des exemples d'applications sur des signaux avec traitement par microordinateur.
Lorsqu'on utilise des modèles de Markov cachés pour la reconnaissance de la parole, l'on fait habituellement l'hypothèse que la probabilité d'émission d'un vecteur acoustique ne dépend que de l'état courant et du vecteur acoustique actuellement observé.
Dans cet article, nous envisageons une hypothèse moins restrictive : nous considérons qu'au sein d'un même état de la chaîne de Markov, les vecteurs acoustiques sont générés par un processus markovien continu.
En effet, l'évolution temporelle du signal de parole est intrinsèquement continue, et l'échantillonnage n'est réalisé que pour les besoins de calculs numériques.
Nous assignons une densité de probabilité à la trajectoire temporelle observée du vecteur acoustique, reflétant la probabilité que cette particulière a été générée par le processus markovien continu associé à l'état.
Elle mesure “l'adéquation” de cette trajectoire observée par rapport à une trajectoire idéale, supposée générée par équation différentielle vectorielle linéaire.
La segmentation la plus probable (au sens du maximum de vraisemblance) s'obtient en échantillonnant le processus continu, et en calculant le meilleur chemin à travers toutes les segmentations possibles et toutes les durées possibles par programmation dynamique.
Nous décrivons l'utilisation de transformations spectrales pour l'adaptation au locuteur de systèmes HMM de reconnaissance de mots isolés.
Cet article expose et compare trois méthodes pour réaliser cette transformation ; la minimisation de l'erreur quadratique moyenne (MMSE), l'analyse canonique des corrélations (CCA) et l'emploi d'un perceptron multi-couches (MLP).
Sur les mots isolés du corpus TI-46, les meilleurs résultats sont fournis par la méthode CCA.
Nous comparons également trois stratégies d'apprentissage et d'adaptation des HMM.
La technique “sans ré-apprentissage” calcule la transformation spectrale à partir d'une petite quantité de données d'adaptation : elle peut surtout être utilisée pour l'adaptation en ligne.
La technique “d'apprentissage après adaptation” calcule la transformation avant l'apprentissage des HMM ; celui-ci doit être fait hors-ligne, mais les modèles obtenus sont de meilleure qualité.
La troisième approche combine de manière originale les deux techniques précédentes en deux étapes ; elle réalise une adaptation à la fois rapide et de bonne qualité.
Nos expériences montrent qu'une adaptation de ce type (à 2 étages et utilisant la méthode CCA de transformation spectrale) avec en moyenne seulement 10% des données d'apprentissage d'un nouveau locuteur permet d'obtenir un taux de reconnaissance meilleur que celui fourni par le modèle appris individuellement sur ce locuteur.
Cet article aborde la problématique de l'insertion de Jean Miélot et de ses œuvres dans le milieu de la cour de Bourgogne.
On observe que, malgré la grande variété des textes que Miélot nous a laissée et leur grand intérêt, ils ne semblent pas avoir eu beaucoup de succès à son époque.
Beaucoup de ses œuvres ne sont connues que par un ou deux manuscrits, et ses livres se trouvaient dans peu de bibliothèques.
Deux questions s'imposent donc : tout d'abord, celle de savoir si l'on doit voir Jean Miélot comme un écrivain isolé ou bien comme un auteur bien inséré dans un milieu des artisans du livre à la cour de Bourgogne ; deuxièmement, si ses œuvres n'ont effectivement pas eu beaucoup de rayonnement ou si elles étaient par contre largement diffusées.
Des recherches dans le domaine de l'écriture et de la décoration des manuscrits de Miélot confirment que le chanoine n'est pas un auteur isolé, mais qu'il est bien intégré dans un réseau de producteurs de livres pour la cour de Bourgogne.
Des liens étroits avec David Aubert peuvent être supposés.
D'autre part, un aperçu des possesseurs des manuscrits contenant des textes de Miélot au XVe siècle révèle que ses lecteurs étaient confinés au cercle très restreint de la famille ducale et de quelques membres de la haute noblesse à la cour de Bourgogne.
Ses textes ne se sont pas, ou à peine, diffusés vers d'autres groupes sociaux ou vers d'autres aires géographiques.
Jean Miélot est une figure originale et intéressante du XVe siècle. Une approche intégrant codicologie, paléographie et l'étude du texte et de l'image peut nous apporter un nouveau regard sur son rôle et sur ses œuvres.
Dans cet article, nous présentons un nouvel algorithme de transformation du timbre de la voix.
Cette méthode présente deux particularités importantes.
Premièrement, elle modifie les fréquences des formants et l'intensité spectrale en utilisant des règles de conversion “linéaires par morceaux”, permettant de contrôler les détails du spectre.
La deuxième particularité est que l'algorithme prend en compte non seulement la fréquence et la bande passante des formants, mais aussi l'intensité spectrale (l'amplitude formantique).
Ceci est obtenu au moyen d'une procédure itérative de modifications des pôles du filtre de synthèse.
Des tests d'écoute ont démontré que cet algorithme transforme de façon efficace les caractéristiques individuelles du timbre du locuteur, tout en conservant une très bonne qualité de synthèse.
Cet article esquisse un cadre théorique pour la compréhension des bases neurales de la mémoire.
Ce modèle réfuse l'existence d'un site anatomique unique pour l'intégration sensori-motrice et d'une mémoire unique gardant le sens d'entités ou d'événements.
Le sens résulte de la rétrp-activation distribuée et synchrone de fragments.
Seuls ces derniers atteignent le seuil du conscient.
Nous nous trouvons actuellement au milieu d'une révolution des moyens de communication qui promet de fournir un accès permanent aux services de communication multi-média.
Pour y parvenir, cette révolution a besoin d'interfaces de haute-qualité, faciles d'usage et “sans couture” à offrir pour la communication large-bande entre les utilisateurs et les machines.
Dans cet article, nous défendons l'idée que les interfaces en parole naturelle (SLIs) sont essentielles pour faire de cette vision une réalité.
Nous présentons les applications potentielles des SLIs, les technologies qui les sous-tendent, les principes que nous avons développés pour les définir, ainsi que les domaines-clé de recherche future, tant en traitement de la parole naturelle qu'en interfaces personne–machine.
Ce papier présente des modèles de cartes auto-organisatrices de Kohonen (self-organizing map, som) hiérarchiques pour la classification phonémique.
Cette méthode som hiérarchique utilise comme principe de base un apprentissage non supervisé et une organisation spatiale des données.
Cette approche de classification étend la méthode de carte de Kohonen en introduisant le principe de vecteurs prototypes multiples, par l'intermédiaire de la méthode d'enrichissement de l'information auxiliaire dans une carte.
L'étude de cas des modèles de classification som hiérarchiques porte sur la reconnaissance phonémique pour la parole continue, multilocuteur et indépendant du contexte.
Les modèles som proposés servent comme des outils pour le développement des systèmes intelligents et poursuivant des applications de l'intelligence artificielle.
La question de savoir si les mots de la classe fermée et les mots de la classe ouverte suivaient la même voie d'accès au lexique a été récemment l'objet d'un vif débat.
On a suggèré que les différences dans la sensibilité aux fréquences pouvaient indiquer des voies d'accès séparées.
Nous n'avons pas trouvé de preuves à I'appui de l'hypothèse que les mots de la classe fermée ont une voie d'accès lexical différente ou spéciale.
On ne trouve pour aucune des deux classes de mots d'effet de fréquence pour les fréquences de Kučera-Francis de 400 million ou plus et ceci aussi bien pour les temps de réaction que dansl'analyse des erreurs.
Les mots de la classe ouverte donnent parfois lieu à des réponses plus rapides que les mots comparables de la classe fermée mais ceci pent etre en contradiction avec des interprétations sur l'effet de la classe des mots (Bradley et al 1980).
En outre nos données montrent également quelles pourraient être les influences spécifiques aux mots sur les temps de décision lexicale — effets qui pourraient être impossibles à séparer de l'effet de la classe des mots en anglais.
Pour interpréter la non sensibilité aux fréquences trouvée dans ces expériences on doit aménager les modèles d'accs̀ lexical basés sur le logogen et y inclure une limite inférieure pour les seuils de positionnements.
Les modèles de résonnance (Gordon, 1983) prédisent dejà cette insensibilité aux fréquences.
Il aurait du être possible de distinguer entre les deux modèles lors des tâches de décision lexicale avec masquage mais des effets inattendus inhérents aux mots nous ont empéché de le faire.
Le système apollonien peut être expliqué – nous semble-t-il – par sa conception du nom, censé signifier à la fois la substance et la qualité du référent et donc lié à la signification de l'hyparxis / ousía : le fait de nommer un référent en implique l'existence du moins au niveau linguistique.
Cet article consiste en un examen des tenants et des aboutissants de l'activité de relecture, considérée principalement sous l'angle de la temporalité.
Après une brève étape de définition, y sont successivement abordées les tentatives de « programmation » textuelle de la relecture face aux relectures spontanées, les modalités mêmes de la relecture de textes fictionnels et non fictionnels, les motivations de la pratique de relecture.
Ainsi se dégage une manière de bilan du phénomène de relecture dans son rapport au temps, implications comprises, sur le triple plan poétologique, pragmatique et anthropologique.
Cet article étudie le problèma de la reconnaissance des objects à partir de leur forme st propose une nouvelle approche, l'alignement des descriptions graphiques.
La première partie de l'article passe en revue les approches générales de la reconnaissance visuelle des objets et divise ces approches en trois grandes catégories : méthodes des propriétés invariantes, méthodes de décomposition des objets et méthodes d'alignment.
La seconde partie présente la méthode d'alignment.
Dans cette approache le processus de reconnaissance est divisé en deux étapes.
La première détermine la transformation dans l'escape nécessaire pour amener l'objet vu en alignement avec les modèles possibles d'objets.
Cette étepa peut se dérouler sur la base d'une information minimale, telle que l'orientation dominante de l'objet, ou un petit nombre de traits communs entre l'objet et le modèle.
La seconde étape détermine le modèle qui correspond le mieux avec l'objet vu.
Au cours de cette étape, la recherche se fait sur tous les modèles d'objets possibles, mais pas sur l'ensemble des vues possibles de ces objets, étant donné que la transformation a déja été déterminée d'une manière unique au cours de l'étape d'alignment.
La méthode d'alignment proposée utilise également des descriptions abstraites mais à la différence des méthodes de description structurelles, elle les utilise de manière graphique plutôt que sous la forme de descriptions structurelles symboliques.
Concevoir le comportement des personnages non joueurs (PNJ), dans un jeu vidéo de type jeu de rôles, est un problème difficile tant du point de vue de la réalisation informatique que de la modélisation du comportement lui-même.
A ce moteur nous associons un mécanisme de sélection d'action basé sur les motivations.
Ce mécanisme permet la définition de différents comportements de PNJ en jouant sur les paramètres des motivations et sans nouvelle programmation de code.
Notre proposition constitue un moteur comportemental pour la modélisation de comportements de personnages situés.
Par sa généricité, ce moteur peut être utilisé dans différents environnements (jeux) et pour différents agents en s'adaptant aux spécificités et capacités de chacun tout en proposant une diversité dans les comportements réalisables.
Deux modè;es de simulation du conduit vocal non-stationnaire à l'aide de filtres digitaux (WDF) sont décrits.
Le premier modèle se limite à redéfinir périodiquement les coefficients du filtre WDF afin de simuler les changements d'aire.
Le second modèle repose sur un filtre d'onde digital variable dans le temps qui permet une description physiquement cohérente du conduit vocal non-stationnaire.
Dans le but de tester si le premier modèle suffit à la synthèse de la parole, les deux modèles sont comparés l'un à l'autre.
De plus, trois modèles distincts de simulation de la source glottique sont décrits.
Une première simulation est basée sur un filtre WDF variable dans le temps, une deuxième simulation est basée sur un filtre WDF ordinaire et une troisième simulation se fonde sur un modele dans lequel la glotte est supposée purement résistive.
Les signaux produits par ces modèles et leurs interactions avec le conduit vocal sont comparés entre eux.
Il apparaît que les différences entre les deux modèles du conduit sont très faibles en simulation de la parole et ne sont pas audibles dans des expériences d'audition.
Les trois modèles de la source glottique produisent des différences plus importantes qui sont audibles.
La simulation “simple” au moyen d'une source purement résistive produit des effets similaires aux autres modèles.
Dans les modèles autorégressifs de production de la parole, il est courant de modéliser l'excitation laryngienne ('glottal volume velocity') à l'aide de deux pôles réels.
Cette communication discute des implications de ce procédé sur certaines caractéristiques temporelles fondamentales et conclut qu'une telle modélisation est physiologiquement irréaliste.
Les conséquences sur les performances des algorithmes autorégressifs d'analyse et sur la qualité (perceptive) des voyelles ainsi produites sont discutées.
Cet article présente les performances d'un codec d'image bas debit pour le vidéophone et les applications RNIS concernant le canal B/2B.
Au niveau du récepteur, les trames manquantes sont reconstruites à l'aide d'une interpolation linéaire qui utilise les trames codées et les vecteurs de mouvement associés.
L'efficacité du codec est évaluée en utilisant deux séquences vidéo à l'entrée.
Les séquences vidéo obtenues montrent que pour le vidéophone, la qualité est bonne.
Par conséquent, ce codec peut être un bon candidat pour le vidéophone dans le réseau RNIS.
Les considérations d'implémentation du codec sont également données.
Cet article résume les résultats d'études récentes sur le rôle de la mémoire à long-terme en perception de parole et en reconnaissance de mots.
Ces expériences sur la variabilité des locuteurs, le débit de parole et l'apprentissage perceptif fournissent des preuves fortes en faveur de l'existence d'une mémoire implicite pour les détails perceptifs très fins de la parole.
Apparemment, les auditeurs encodent des attributs spécifiques de la voix du locuteur et de son débit dans une mémoire à long-terme.
La variabilité acoustico-phonétique ne semble pas être “perdue” à l'issue de l'analyse phonétique.
Le processus de normalisation perceptive en oeuvre lors de la perception de parole semble donc comporter un encodage d'événements ou “épisodes” spécifiques du stimulus d'entrée et les opérations d'analyse perceptive.
Ces opérations perceptives pourraient être effectuées au sein d'une “mémoire procédurale” pour chaque voix de locuteur donnée.
Globalement, l'ensemble actuel d'observations est en accord avec les présentations non-analytiques de la perception, de la mémoire et de la cognition qui mettent l'accent sur la contribution d'un encodage épisodique ou “à base d'exemples” au processus de mémoire à long terme.
Les résultats de ces études posent également la question des dissociations tradionnelles en phonétique entre les propriétés linguistiques et d'indexation de la parole.
Les auditeurs retiennent apparemment, dans la mémoire à long-terme, des informations non-linguistiques sur le genre du locuteur, son dialecte, son débit de parole et son état émotionnel, attributs du signal de parole qui ne sont généralement pas considérés comme relevant des représentations phonétiques ou lexicales des mots.
Ces propriétés influencent l'encodage perceptif initial et la rétention des mots parlés et doivent par conséquent jouer un rôle important dans les hypothèses théoriques concernant la façon dont le système nerveux associe les signaux de parole aux représentations linguistiques du lexique mental.
Dans cet article, nous présentons une solution multisensorielle temps réel pour la détection et le suivi d'obstacles sur route.
Cette solution est basée sur l'utilisation d'un capteur mixte caméra vidéo/capteur de profondeur placé à l'avant d'un véhicule expérimental.
Le capteur multisensoriel est décrit.
Le calibrage permet l'alignement des données hétérogènes.
Deux facultés du capteur sont développées : la perception dirigée permet l'acquisition d'une image de profondeur dans une zone définie dans l'image de luminance ; l'asservissement visuel réalise la focalisation du faisceau laser sur un point de l'image de luminance.
De façon générale, ces facultés permettent un contrôle par rétroaction sur le mode d'acquisition du capteur en fonction de la situation dans laquelle se trouve le système de perception.
La stratégie de perception est basée sur la sélection du capteur adéquat pour un objectif donné.
La détection d'obstacle repose sur la segmentation et l'interprétation des données de profondeur qui sont d'une grande pertinence dans ce contexte.
En revanche, la cadence d'acquisition de ces données n'est pas suffisante si l'on souhaite dériver les caractéristiques cinématiques des obstacles.
En conséquence, le suivi des obstacles combine un traitement de l'image de luminance rapide avec un traitement de l'information 3D. Le premier permet de réactualiser la position de l'obstacle afin d'asservir le faisceau laser sur celui-ci et le second assure la connaissance de la taille du modèle de l'obstacle à chercher dans l'image.
Cet algorithme de fusion de données hétérogènes accompagné d'un filtrage de Kalman permet d'inférer les caractéristiques cinématiques des obstacles dont la connaissance est indispensable pour aborder ceux-ci dans de bonnes conditions.
Ces recherches sont menées dans le cadre du projet européen PROMETHEUS et sont validées en situation réelle à bord du véhicule expérimental Prolab.
Nous décrivons un corpus de monosyllabes servant à tester l'intelligibilité des consonnes en parole synthétique.
Ce corpus se différencie de ceux utilisés pour d'autres tests en ce qu'il recouvre une large variété de sons de l'anglais et est utile pour le diagnostic et pour l'évaluation comparative.
Les résultats obtenus avec certains tests “standards” d'intelligibilité utilisant un matériel phonétique restreint pourraient ne pas être représentatifs de l'intelligibilité d'un plus grand échantillon réellement représentatif de l'anglais.
Pour illustrer ceci, nous présentons les résultats d'une comparaison effectuée sur une ligne téléphonique entre un synthétiseur de demi-syllabes actuellement en développment à Bellcore (“Orator”), un synthétiseur déjà commercialisé et basé sur les phonèmes et de la parole naturelle produite par deux locuteurs.
Ces données de parole naturelle pourraient être utilisées par d'autres laboratoires désireux de comparer l'intelligibilité des consonnes d'autres systèmes de synthèse par rapport à la parole naturelle.
Le système VOICE (Voice Oriented Interactive Computing Environment) a été développé pour la langue hindie, dans le but de fournir un outil d'interaction visuelle et vocale.
Le système de reconnaissance de 200 mots isolés s'applique dans une tâche de réservation ferroviaire ; il utilise des segments acoustico-phonétiques pour unités de base de reconnaissance.
A chaque trame, une classification en grandes classes acoustico-phonétiques est réalisée grâce à un classificateur par maximum de vraisemblance ; la segmentation par classification hiérarchique des vecteurs des valeurs de vraisemblance associées aux trames est effectuée grâce à des semi-modèles de Makov cachés, qui utilisent expliticitement la durée.
Une classification plus précise par réseaux connexionnistes est réalisée pour quelques classes (voyelles, barres de voisement et nasales dans une première étape).
Un décodage de séquences par programmation dynamique permet l'accès lexical, c'est-à-dire le décodage comme des mots de la suite des symboles représentant les catégories phonétiques.
Une implantation répartie des tâches permet de reconnaître un mot en quatre fois le temps réel.
Un processeur linguistique lève les ambiguités entre les choix multiples fournis par le reconnaisseur à chaque mot, et corrige éventuellement des erreurs de reconnaissance au niveau acoustique.
C'est le premier système de reconnaissance qui fonctionne sur la langue hindie ; le taux de reconnaissance des mots est de 85%.
A fins de comparison, un système de reconnaissance de mots par Modèles de Markov cachés a également été développé.
Les performances du système devraient encore s'accroître car il lui reste un large potential d'amélioration.
Plusieurs méthodes efficaces de collage de la parole à débit variable (VBR) adaptées aux techniques à falble retardà base de collage en arbre à decision différée (DDTC) et de CELP sont présentées dans vet article.
Ces méthodes reposent sur la modification du (des) dictionnaire(s) existant(s) par des codes en blocs ou en treillis.
Pour réaliser one réduction de débit du codeur DDTC, one technique nouvelle basée sur un code de parité est développée et s'est avérée apporter jusqu'à 2 dB d'amélioration du rapport signal sur bruit des méthodes conventionnelles.
Pour one augmentation du debit do codeur LD-CELP, one approace basée sur un code en treillis est utilisée.
Le dictionnaire en treillis modifié ou supplémentaire possède one structure géométrique (algébrique) permettant one procédure de recherche efficace pour one quantité minimum de mémoire supplémentaire.
Les résultats des simulations de ces méthodes UBR pour un codeur DDTC à 16 kbit/s et pour le codeur LD-CELP à 16 kbit/s proposé pour la recommendation CCITT (G728) sont présentér.
Ce papier propose une méthode permettant d'identifier le scripteur d'un texte quelconque de quelques lignes en le comparant à des écritures de références.
La comparaison est basée sur une mesure de mise en correspondance des distributions des allographes de lettres représentatifs des styles d'écriture.
Un système automatique segmente le texte en lettres, puis classe chaque lettre de manière probabiliste parmi les prototypes disponibles pour cette lettre.
Deux bases de complexité différentes sont utilisées pour valuer ce système.
Cette méthode est développée sur de l'écriture en ligne.
Deux expériences utilisent une procédure développée par Carter et Bradshaw (Speech Communication, Vol. 3 (1984) pp. 347–360) pour examiner le rôle de la structure de la syllabe sur la production de la parole.
Dans cette procédure, les sujets intervertissent des segments phonologiques dans des positions correspondantes d'une paire de mots ou de non-mots présentés visuellement et doivent produire les mots ou non-mots résultants aussi vite que possible.
Carter et Bradshaw ont montré que les latences reflètent les fréquences des erreurs d'inversion dans la parole naturelle.
La première expérience de la présente étude montre que les échanges de la consonne initiale sont favorisés par la similarité phonétique des consonnes échangées et reflètent un biais pour la production de mots réels.
En contrôlant les influences, l'Expérience 2 réduplique et étend le résultat de Carter et Bradshaw selon lequel les échanges de la consonne initiale sont réalisés plus rapidement que ceux de la consonne finale.
La discussion relie la différence de latence entre ces conditions à la difference de “cohésion” entre consonnes initiale et finale avee leur voyelle.
En particulier, dans l'Expérience 2, plus d'un tiers des erreurs commises sur les échanges de consonne ou de voyelle finales sont des échanges de la syllabe rime entère (VC) alors que 10% seulement des erreurs commises sur la consonne ou voyelle initiales sont des échanges de la syllabe initiale (CV) du mot.
Dans les Analyses menées a posteriori, diverses explications quant à la différence de cohésion sont examinées.
Cet article propose une revue de trente années de développement des algorithmes de démosaïçage utilisés dans les caméras numériques pour la reconstruction des images couleurs.
La plupart des caméras numériques actuelles utilisent un seul capteur devant lequel est placée une matrice de filtres couleurs.
Ce capteur échantillonne par conséquent une seule couleur par position spatiale et un algorithme d'interpolation est nécessaire pour la définition d'une image couleur avec trois composantes par position spatiale.
Cet article montre que l'ensemble des techniques du traitement du signal et des images a été utilisé pour résoudre ce problème.
Aussi, une nouvelle méthode proposée récemment par l'auteur et collaborateurs est décrite.
Cette méthode, basée sur un modèle d'échantillonnage couleur par les cônes de la rétine, révèle la nature de l'échantillonnage spatio-chromatique dans les caméras couleur à un seul capteur.
Près de quatre décennies de recherches sur la perception de la parole ont échoué à démêler la relation entre son et phone.
Ceci est-il dû fait que la discrimination des sons de la parole est à ce point complexe ou singulière qu'elle résiste à l'étude ?
Ou parce que les bonnes questions n'ont pas été posées ?
Cet article passe en revue certaines des recherches les plus récentes menées à l'Institut Pavlov de Léningrad qui suggèrent une réponse affirmative à la deuxième question.
Cette nouvelle perspective provient d'un modèle original de la perception de la parole qui prend la forme d'une analyse de la modulation.
Une considération plus étendue de cette approche basée sur la modulation pourrait fournir de réelles alternatives aux modèles spectraux développés en vue de la solution du problème de “l'invariance”.
Après une introduction générale, le modèle est examiné dans le contexte d'expériences et de résultats représentatifs.
Dans cet article, nous proposons une étude de la Programmation Génétique (PG) du point de vue de la théorie de l'Apprentissage Statistique dans le cadre de la régression symbolique.
En particulier, nous nous sommes intéressés à la consistence universelle en PG, c'est-à- dire la convergence presque sûre vers l'erreur bayésienne à mesure que le nombre d'exemples augmente, ainsi qu'au problème bien connu en PG de la croissance incontrôlée de la taille du code (i.e. le "bloat").
Les résultats que nous avons obtenus montrent d'une part que l'on peut identifier plusieurs types de bloat et d'autre part que la consistence universelle et l'absence de bloat peuvent être obtenues sous certaines conditions.
Nous proposons finalement une méthode ad hoc évitant justement le bloat tout en garantissant la consistence universelle.
Nous introduisons une famille d'algorithmes adaptatifs permettant l'utilisation de réseaux de neurones comme filtres adaptatifs non linéaires, systèmes susceptibles de subir un apprentissage permanent à partir d'un nombre éventuellement infini d'exemples présentés dans un ordre déterminé.
Cet article présente les résultats expérimentaux obtenus avec une architecture originale permettant un apprentissage générique dans le cadre de processus décisionnels de Markov factorisés observables dans le désordre (PDMFOD).
L'article décrit tout d'abord le cadre formel des PDMFOD puis le fonctionnement de l'algorithme, notamment le principe de parallélisation et l'attribution dynamique des récompenses.
L'architecture est ensuite appliquée à deux problèmes de navigation, l'un dans un labyrinthe et l'autre dans un trafic routier (New York Driving).
Les tests montrent que l'architecture permet effectivement d'apprendre une politique de décisions performante et générique malgré le nombre élevé de dimensions des espaces d'états des deux systèmes.
Dans cet article théorique, nous proposons de comparer les techniques “classiques” employées en inférence grammaticale de langages réguliers par exemples positifs avec celles employées pour l'inférence de grammaires catégorielles.
Pour cela, nous commençons par étudier les traductions réciproques entre automates finis et grammaires catégorielles.
Nous montrons ensuite que les opérateurs de généralisation utilisés dans chacun des domaines sont comparables, et que le résultat de leur application peut toujours se représenter à l'aide d'automates généralisés appelés “récursifs”.
Les liens entre ces automates généralisés et les grammaires catégorielles sont étudiés en détail.
Enfin, nous exhibons de nouvelles sous-classes apprenables de grammaires catégorielles pour lesquelles l'apprentissage à partir de textes n'est presque pas plus coûteux que l'apprentissage à partir de structures.
Les modéles connexionnistes des conduites linguistiques acquises qui ontétérécemment proposés, incorporent en fait des représentations linguistiques fondées sur un systéme de régles.
Des modéles connexionnistes similaires pour l'acquisition du langage possédent de méme un appareillage et une architecture ad hoc qui leur fait mimer les effets des régles.
Les modéles connexionnistes en général ne sont pas adéquats pour rendre compte de l'acquisition de connaissances structurelles : ils nécessitent des structures prédéterminées, méme pour simuler des faits linguistiquesélémentaires.
De tels modéles sont plus appropriés pour décrire la formation d'associations complexes entre des structures qui sont déjáreprésentées de maniére indépendante.
Ceci rend les modéles connexionnistes des outils potentiellement importants pourétudier les relations entre des conduites fréquentes et les structures qui sous-tendent les connaissances et les représentations.
Ces modéles peuvent offrir de puissants moyens imformatiques pour démontrer les limites d'une description associationiste des conduites.
Le « Traité chinois des particules et des principaux termes de grammaire » , qui figure dans la Syntaxe nouvelle de la langue chinoise (1869) de Stanislas Julien, représente la première traduction de l'Explication des particules dans les Classiques et dans les commentaires (Jīngzhuàn shìcí 經傳釋詞, 1819) de Wáng Yǐnzhī dans une langue occidentale.
Si l'ouvrage de Wáng peut être considéré comme le plus important répertoire de particules grammaticales de la tradition philologique chinoise, sa traduction constitue un exemple remarquable de présentation des méthodologies et de la terminologie linguistique chinoises pour un public européen.
Cet article se propose de comparer les deux ouvrages, en analysant les modalités de traduction des lemmes et de transposition des catégories et de la terminologie linguistiques.
Dans cette étude, on a utilisé une base de données multi-locuteurs contenant des scores d'intelligibilité de 2000 phrases (20 locuteurs, 100 phrases) pour identifier les corrélats de l'intelligibilité qui sont dépendants du locuteur.
Nous avons d'abord étudié des caractéristiques “globales” des locuteurs (genre, F0 et vitesse d'élocution).
Il est apparu que les locutrices sont globalement plus intelligibles que les locuteurs.
Nous avons également observé que l'ampleur du registre de F0 avait tendance à être corrélée positivement avec des scores d'intelligibilité plus élevés.
Toutefois, la valeur moyenne de F0 et la vitesse d'élocution ne semblent pas être corrélées avec l'intelligibilité.
Nous avons ensuite examiné d'autres corrélats de l'intelligibilité globale, plus fins au niveau des caractéristiques acoustico-phonétiques du locuteur.
Nous avons observé que les locuteurs présentant des triangles vocaliques larges étaient généralement plus intelligibles que les locuteurs présentant des triangles vocaliques serrés.
En étudiant deux cas d'erreurs d'écoute consistantes (destruction d'un segment et attribution à une syllabe), nous avons trouvé que ces erreurs perceptives pouvaient être dérivées directement des caractéristiques de timing détaillées du signal de parole.
Les résultats suggèrent qu'une part substantielle de la variabilité de l'intelligibilité de la parole normale est attribuable aux caractéristiques acoustico-phonétiques spécifiques du locuteur.
La connaissance de ces facteurs peut être utile pour améliorer la synthèse de la parole et les stratégies de reconnaissance, et pour des populations spécifiques (comme par exemple, les mal-entendants ou ceux qui apprennent une langue étrangère) qui sont particulièrement sensibles aux écarts d'intelligilibité entre locuteurs.
Le dévelopment d'un systéme de reconnaissance du locuteur à haute précision () et indépendant du texte est envisagé dans ce travail en deux étapes.
La premiére phase traite de l'évaluation des caractéristiques sélectives du locuteur pour les différents ensembles de paramétres qui caractérisent le langage humain.
Dans la seconde étape, on apparie deux à deux tous les ensembles de paramétres et on les combine logiquement pour obtenir une précision de reconnaissance plus élevée que celle qui pourrait être atteinte avec chaque ensemble isolé.
Cet algorithme permet d'utiliser des phrases supplémentaires pour résoudre les décisions contradictoires résultant de l'application à la premiére phrase-test de la procédure à deux ensembles de paramètres.
Pour completer la discussion, une bréve revue de la littérature pertinente est également présentée.
Un système à deux niveaux pour la reconnaissance du locuteur est proposé.
Le premier classificateur est construit sur la base du réseau neuromimétique de Kohonen (SOM) et utilise les coefficients LPCC comme vecteurs d'entrée.
Les résultats de la classification réalisée par les PDM servent d'entrées pour le classificateur de deuxième niveau.
Ce classificateur est construit sur des réseaux neuromimétiques de type MLP, formés pour chaque locuteur.
Le classificateur au premier niveau représente un pré-processeur pour le classificateur de deuxième niveau, qui réalise la classification finale.
Le but du système proposé est de combiner les avantages des deux types de réseaux neuromimétiques afin d'effectuer une reconnaissance du locuteur plus précise.
Les résultats expérimentaux montrent que ce système à deux niveaux améliore les performances surtout pour des signaux bruités (enregistrés par une ligne téléphonique).
La prescription peut inclure toute intervention dans le parler des autres.
Depuis longtemps ignorée par les linguistes comme non scientifique, la prescription fait naturellement partie du comportement linguistique.
Nous cherchons à mettre en lumière la logique et la méthode de la prescription à travers les manuels d'usage : leurs auteurs, leurs sources et le public visé ; le contexte social de ces ouvrages ; les catégories de « fautes » visées ; les prétendues raisons de la rectification ; la phraséologie de la prescription ; le rapport entre l'usage attesté et l'usage prescrit ; l'effet de la prescription.
Nous regroupons dans notre corpus une trentaine de ces manuels de la tradition française.
Nous espérons pouvoir créer une base de données permettant une étude comparative de ces aspects.
Un des thèmes importants de l'apprentissage par renforcement est l'approximation en ligne de la fonction de valeur.
En plus de leur capacité à prendre en compte de grands espaces d'état, les algorithmes associés devraient présenter certaines caractéristiques comme un apprentissage rapide, la faculté de traquer la solution plutôt que de converger vers elle (particulièrement en raison de l'entrelacement entre contrôle et apprentissage) ou encore la gestion de l'incertitude relative aux estimations faites.
Dans cette optique, nous introduisons un cadre de travail général inspiré du filtrage de Kalman que nous nommons différences temporelles de Kalman.
Une forme d'apprentissage actif utilisant l'information d'incertitude est également introduite, et comparaison est faite à l'état de l'art sur des problèmes classiques.
L'élastographie dynamique par force de radiation ultrasonore est une technique d'imagerie des propriétés élastiques des tissus biologiques.
D'un point de vue mécanique, nous supposons que ces milieux sont isotropes c'est-à-dire que leurs propriétés sont indépendantes du choix des axes de référence.
Le tenseur élastique qui définit les constantes physiques de ce milieu s'exprime en fonction de deux constantes indépendantes, le module d'élasticité volumique K (qui intervient lors de la propagation des ondes de compression), et le module d'élasticité de cisaillement μ (qui intervient lors de la propagation des ondes de cisaillement).
L'apparition de certains type de cancers entraîne de faibles variations du module d'élasticité volumique K, mais peut modifier considérablement le module d'élasticité de cisaillement μ.
La mesure de ce paramètre μ peut ainsi aider au diagnostic de ce type de pathologie des tissus.
Un moyen judicieux de mesurer ce paramètre est d'utiliser un effet non linéaire de force de radiation ultrasonore.
Cette force est proportionnelle à l'atténuation et à l'intensité des ultrasons émis dans le tissu par le système d'imagerie.
Cette source de contrainte génère principalement une onde de cisaillement qui se propage avec une vitesse de phase proportionnelle au module de cisaillement et une polarisation purement transversale en champ lointain (loin de la source de contrainte).
La mesure des déplacements du milieu, induits par la propagation de cette onde, peut permettre par résolution du problème inverse de remonter au module de cisaillement.
Nous avons réalisé ces mesures à partir des lignes radiofréquences (RF) obtenues par un transducteur d'imagerie ultrasonore.
Ce travail décrit précisément le traitement que nous avons réalisé sur les lignes RF.
Ce traitement est basé sur l'utilisation d'une méthode d'estimation des retards temporels entre les lignes radiofréquences obtenues pendant la propagation de l'onde de cisaillement.
L'influence de différents paramètres (taille de la fenêtre glissante d'analyse, rapport signal sur bruit des lignes RF, fréquence d'échantillonnage, caractéristiques du transducteur ultrasonore…) sur la précision de mesure des déplacements a été étudiée.
Nous présentons les courbes des déplacements en fonction du temps obtenus après optimisation des paramètres de traitement.
Ces résultats expérimentaux ont été favorablement comparés à un modèle physique et nous ont permis de remonter au module de cisaillement du milieu.
Cette méthode consiste à calculer d'abord les histogrammes de projection obtenus pour différents angles, puis à déterminer la valeur maximale de la représentation temps-fréquence de la racine carrée de ces histogrammes.
L'orientation du document est alors estimée par l'angle de projection fournissant la valeur maximale la plus élevée.
La méthode proposée a été testée sur 864 documents inclinés avec 9 représentations temps-fréquence différentes.
Les résultats sont présentés et analysés à la fin de cet article.
Une nouvelle méthode d'acquisition automatique des Fragments pour la compréhension du langage courant est désormais proposée.
L'objectif de cette méthode est de générer une collection de Fragments, chacun représentant un ensemble de phrases similaires d'un point de vue syntaxique et sémantique.
En premier lieu, les phrases fréquemment rencontrées dans le kit de formation sont sélectionnées en qualité de phrases candidates.
Chaque phrase candidate présente trois répartitions de probabilité associées : contextes suivants, contextes précédents et actions sémantiques associées.
La similitude entre les phrases candidates est mesurée en appliquant la distance Kullback–Leibler à ces trois répartitions de probabilité.
Les phrases candidates proches des trois distances sont regroupées au sein d'un Fragment.
Les séquences représentatives de ces Fragments sont ensuite acquises automatiquement, et exploitées par un module de compréhension du langage courant afin de classer les apples dans la tâche AT&T dénommée “How May I Help You ?” (“Comment puis-je vous aider ?”).
Ces fragments nous permettent de généraliser les phrases non observées.
Par exemple, ils ont permis de détecter 246 phrases présentes dans les kits de test et absentes des kits de formation.
Ce résultat montre que les phrases qui n'ont pas été vues peuvent être découvertes automatiquement grâce à notre nouvelle méthode.
Des résultats expérimentaux montrent une amélioration de 2,8% des performances de classification des types d'appels après la mise en place de ces Fragments.
A partir de l'information acoustique, il a été possible, avec un algorithme génétique, de retrouver des trajectoires articulatoires, en se servant d'un modèle dynamique de production de la parole.
Des tests sur des logatomes simulés /əbæ/ et /ədæ/ montrent que cette méthode peut recouvrir la plûpart de trajectories originales ; cependant le traitement du timing précis pose des problèmes.
Pour récupérer l'articulation, il est nécessaire de rajouter, aux trajectoires de fréquences formantiques, de l'information acoustique supplémentaire, comme l'amplitude RMS.
La distinction entre ce qui est prévu et ce qui est fait est bien connue.
On oppose les tâches prescrite et effective, la logique de fonctionnement à la logique d'utilisation, la procédure à la pratique, etc.
En fait, il existe une solution grâce à un formalisme qui permet une représentation uniforme des éléments de raisonnement et de contextes, appelé Graphes Contextuels.
Nous proposons également une nouvelle notion de « modèle de tâche contextualisée » qui est un compromis opérationnel entre les tâches prescrite et effective.
Le développement de tels modèles de tâches contextualisées permettrait d'établir des procédures plus robustes comme cela est montré à travers quatre applications.
Les sujets intègrent naturellement les informations auditives et visuelles en perception bimodale.
Pour évaluer la robustesse de ce processus d'intégration, on a fait varier systématiquement les instants de déclenchement de sources auditives et visuelles.
Dans la première expérience, des syllabes bimodales composées des versions auditives et visuelles des syllabes /ba/ et /da/ on été présentées avec cinq valeurs différentes d'asynchronie.
La deuxième expérience dupliquait la première en utilisant les voyelles /i/ et /u/.
Les résultats montrent que les sujets intègrent dans leur perception les deux sources d'information pour toutes les valeurs d'asynchronie.
Les réponses groupées (par exemple /bda/ pour une source visuelle /ba/ et une source auditive /da/) apparaissent essentiellement pour les consonnes et non pour les voyelles.
De plus, ces réponses groupées nécessitent que les informations visuelle et auditive soient, chacunes, raisonnablement compatibles avec les propriétés physiques d'une articulation de groupe consonantique.
Tant pour les voyelles que pour les syllables consonnes-voyelles, l'information provenant des sources auditive et visuelle est continue, indépendante et combinée dans un processus de traitement des traits à trois niveaux : évaluation, intégration et décision.
Nous nous intéressons aux problèmes soulevés par les modifications des images couleur consécutives à des changements d'illuminant.
Les images considérées dans cet article contiennent un seul objet placé sur un fond uniforme et éclairé avec un illuminant qui diffère d'une image à l'autre.
Nous proposons de traiter ce problème, non pas en analysant les images de la base indépendamment les unes des autres, mais en analysant tous les couples constitués de l'image requête et de chacune des images candidates.
Plus précisément, nous proposons d'analyser chaque couple d'histogrammes couleur pour comparer le contenu de chaque couple d'images.
Pour cela, la procédure détermine un couple d'histogrammes couleur dits « spécifiques » à chaque couple d'histogrammes couleur considéré, de telle sorte que l'intersection entre ces histogrammes couleur spécifiques soit élevée uniquement lorsque les deux objets contenus dans les deux images sont similaires.
Cette procédure est basée sur une nouvelle hypothèse sur les conséquences d'un changement d'illuminant qui ne porte pas directement sur les couleurs des pixels, mais sur les mesures de rang des pixels.
Cet article décrit une base de données élaborée dans le cadre d'une étude générale sur la parole spontanée sous stress du à un manque de sommeil.
C'est un corpus de 216 dialogues orientés-tâche, à script non-écrits, produits par des adultes normaux au cours d'une large étude sur la privation de sommeil.
L'étude elle-même examinait l'évolution continue de la performance, pendant les périodes normales, sans sommeil, et de récupération, obtenue par des groupes traités avec placébo ou avec l'une de deux drogues (Modafinil, d-amphétamine) réputées pour contrebalancer les effets du manque de sommeil.
Les dialogues ont tous été produits sur la tâche de communication d'itinéraire utilisée dans le “HCRC Map Task Corpus”.
Des paires de locuteurs collaboraient pour reproduire sur la carte du partenaire l'itinéraire imprimé sur la carte de l'autre.
Des différences contrôlées entre les cartes et l'utilisation de noms de lieux imaginaires étiquetés limitaient les effets du genre, du vocabulaire et de la connaissance des lieux.
La construction des cartes et l'affectation des cartes aux sujets font du corpus une expérience de devinement contrôlée.
Chaque locuteur a participé à 12 dialogues au cours de l'étude.
L'étude préliminaire de la longueur des dialogues ainsi que les mesures de performance sur la tâche font apparaître des effets clairs du traitement par la drogue, du manque de sommeil, et du nombre de partenaires dans la conversation.
Le corpus est disponible pour les chercheurs intéressés par tous les niveaux d'analyse de la parole et du dialogue, dans les deux conditions, normale et sous stress.
Dans cette contribution, nous décrivons différentes techniques multi-capteurs de réduction de bruit à la prise de son pour la reconnaissance de mots indépendante du locuteur appliquée à un environnement de bureau.
Nous examinons le taux de reconnaissance si la source perturbatrice n'est pas un bruit gaussien et stationnaire, mais un second locuteur présent dans le même local.
Dans ce cas, les techniques de réduction de bruit classiques comme la soustraction spectrale sont inefficaces, alors que les méthodes multi-microphones peuvent améliorer le taux de reconnaissance en utilisant l'information spatiale.
Nous comparons l'antenne retard-somme, l'antenne super-directive, et deux techniques de post-traitement.
Un nouveau post-filtre adaptatif pour les antennes super-directives (APES) est proposé.
Nos résultats montrent que les méthodes multi-capteurs peuvent améliorer significativement le taux de reconnaissance, parmi lesquelles la méthode APES se détache en performances.
L'article étudie la possibilité d'utiliser un module de reconnaissance automatique de la parole en tant que périphérique d'ordinateur en vue du traitement des caractères du chinois.
Nous avons mené à bien une expérience de reconnaissance de la parole avec l'ensemble des syllabes du chinois standard à ton ascendant.
Nous montrons que les distributions des distances intra- et intersyllabiques se chevauchent considérablement pour l'inventaire complet des 260 syllabes.
Le taux de reconnaissance a èté évalué en fonction de la dimension du vocabulaire, il est de 47.3% pour l'inventaire syllabique complet.
Les fréquences caractéristiques des formants d'une voyelle pour différents locuteurs sont déterminées par plusieus facteurs dont, entre autres, les spécificités du tractus vocal du locuteur, son sexe, son accent régional et ses habitudes langagières.
Dans les données, c'est la différence de sexe qui apparaît comme la source la plus importante de variation inter-locuteurs.
Dans cet article, plusiers méthodes de compensation de ces différences ont étéétudiées.
On a d'abord essayé la méthode des formants calculés avec l'échelle des Bark et la soustraction du ler formant de la fréquence fondamentale en Bark.
Contrairement à ce qui est affirmé dans des articles récents, cette technique s'est révélée inadéquate.
Les transformations se sont, par exemple, montrées incapables d'améliorer les groupements de voyelles cardinales.
Cette technique a permis de rendre compte de la plus grande part de la variance due aux différences interlocuteurs, à partir d'une petite quantité de données d'entraînement.
Cette technique a été appliquée à létude de voyelles américano-anglaises en contexte.
Les fréquences des formants ont été étudiées pour 125 locuteurs de l'anglais américain générail.
Les graphiques des formants pour les hommes et les femmes montrent des phénomènes intéressants.
Premièrement, la limite fréquentielle inférieure du second formant ne diffère pas beaucoup en fonction du sexe tandis que celle du premier formant est plus basse pour les hommes.
Deuxièmement, les maxima fréquentiels des premiers et seconds formants sont plus hauts pour les femmes.
Grâce à la méthode modifiée de Gerstman, les cibles des formants pour une même voyelle dans un même contexte produite par différents locuteurs ont pu être superposées dans la même région de l'espace des deux premiers formants.
Il restait néanmoins une variance résiduelle due à la différence de sexe.
Ces tendances sont montrées dans une série de graphiques représentant les fréquences vocaliques cibles.
Le module de contrôle de gain d'un quantificateur “shape-gain” est rendu adaptatif, donnant lieu à un quantificateur vectoriel qui s'adapte à la variation temporelle de l'amplitude du signal de parole.
La version adaptative n'exige qu'un faible accrossement de la charge de calcul.
Nous présentons les méthodes de conception nécessaires pour optimiser le rapport signal sui bruit ou, alternativement, pour optimiser le rapport signal sur bruit arithmétique segmenté ; les deux produisent des résultats comparables.
Le quantificateur fonctionne le mieux sur des segments de parole riches en fréquences basses à cause de la forme lisse du signal.
Le fonctionnement global à débit unité est comparable au quantificateur adaptatif en amont de Chen et Gersho et il est légèrement inférieur à débit double.
De très petites irrégularités spectrales peuvent être détectées par le système auditif, ce qui laisse supposer l'existence d'un mécanisme de type dérivation.
A l'opposé, le phénomène de “centre de gavité” suggère l'existence d'un mécanisme d'intégration.
Notre objectif est d'analyser les résultats précédents à l'aide du test du seuil de pulsation, outil d'évaluation des représentation internes de spectres de signaux statiques.
Il est généralement admis que cette méthode permet d'estimer le résultat de l'analyse spectrale du signal realisée par le système auditif périphérique, en tenant compte des effets de suppression latérale.
La première partie de ce travail a consisté à déterminer les conditions expérimentales favorables pour effectuer nos mesures.
Nous décrivons les résultats de cette étude préliminaire ainsi que les solutions adoptées.
Nous montrons ensuite, sur un premier ensemble de données expérimentales, comment la représentation interne d'un signal à un formant est modifiée avec l'émergence d'un second formant ou la variation de la valeur numérique des pentes latérales du formant.
La principale conclusion est que les mécanismes de lissage spectral liés à l'existence d'une sélectivité fréquentielle non infinie dominent les effets de suppression latérale dans ces deux cas expérimentaux.
La présélection lexicale est basée sur une segmentation et une classification du signal en six grandes classes phonétiques.
Lors de la vérification, une représentation phonémique détaillée des mots candidats est utilisée pour sélectionner les plus probables.
Chaque mot candidat est modélisé par un graphe de sous-mots représentés par des chaînes de Markov.
Une arborescence d'un sous-ensemble de mots est construite afin de permettre une implantation efficace d'un algorithme de Viterbi “Beam Search” qui estime la vraisemblance de chaque candidat.
Les résultats montrent qu'une réduction de 73% de la complexité peut être obtenue grâce à l'approche en deux temps si on la compare à l'approche directe, pour une précision de reconnaissance comparable.
COMPOST propose aux premiers un langage de programmation de systèmes de synthèse multilingues et multi-entrées et aux derniers une infrastructure logicielle basée sur la notion de serveur-client.
Chaque système de synthèse pouvant être utilisé par le serveur est décrit par un scénario écrit dans un langage spécialisé.
Les idées développées dans cet article sont assorties d'exemples concrets extraits d'un système de synthèse du français développé à l'aide de COMPOST.
Dans les études précédentes, on a constaté que dans une tâche de détection, les voyelles donnent lieu à des temps de réaction plus longs que les consonnes, ce qui suggère que les voyelles sont plus difficiles à percevoir.
Une seconde explication met en cause les rôles différents des voyelles et des consonnes dans la structure syllabique des mots.
Dans la présente expérience, on a examiné la seconde possibilité.
On a utilisé comme cibles deux paires de phonèmes, chaque paire étant constituée d'une voyelle et d'une consonne qui se ressemblent en termes des caractéristiques phonétiques.
Les sujets devaient appuyer sur un bouton aussitôt qu'ils avaient repéré dans une liste de mots anglais un phonème cible préspécifié.
Cette fois, les phonèmes jouant le rôle de voyelle dans une structure syllabique ont donné des temps de réaction inférieurs à ceux des phonèmes jouant le rôle de consonne.
Ceci élimine une explication de la différence des temps de réaction qui serait basée sur le rôle du phonème dans la structure syllabique.
Nous proposons, en revanche, que cette différence provienne de la variation acoustique telle qu'elle se manifeste dans la réalisation des phonèmes cibles et dans leur représentation mentale chez les auditeurs.
On présente un quotient d'amplitude pour la paramétrisation du signal de source glottique obtenu par filtrage inverse.
Le nouveau quotient, AQ (amplitude-domain quotient), est déterminé comme étant la proportion entre l'amplitude du flux AC de l'onde glottique et l'amplitude du minimum du flux dérivé.
Ce quotient peut être employé même si le matériel d'enregistrement ne donne pas de valeurs absolues de flux.
Le comportement d'AQ a été comparé aux quotients conventionnels, basés sur le temps, pour l'analyse de voix produites par différents types de phonation.
Comme l'indiquent les résultats, les types de phonation peuvent être quantifiés de façon efficace lorsque la paramétrisation du flux glottique estimé par filtrage inverse se base sur le quotient AQ.
Cette étude porte sur la compréhension des phrases impératives dans des langues artificielles par deux dauphins (Tursiops truncatus).
Le premier dauphin (Phoenix) a été instruit avec un langage acoustique dont les mots étaient générés par un computer à travers des hauts parleurs sous-marins.
Avec l'autre dauphin (Akeakamai) on a utilisé un langage visuel dont les mots correspondent aux gestes de bras ou de mains d'un instructeur.
Los mots correspondant à des agents, des objets, des modificateurs d'objets, des actions pouvalent se combiner selon une série de règles syntaxiques pour donner une centaine de phrases significatives de 2 à 5 mots.
Ces phrases correspondaient à des ordres enjoignant aux dauphins d'effectuer des actions relatives aux objets dénommés ou aux modificateurs. La compréhension se mesurait par l'acuité de la réponse à l'ordre et était testée de façon à ce que soient éliminés les biais contextuels, les indices non linguistiques et les biais de l'observateur.
Le traitement correct d'une grammaire de gauche à droite (Phoenix) ou d'une grammaire inverse (Akeakamai) montre que des règles syntaxiques entièrement arbitraires peuvent être comprises et que la compréhension des mots fonctionnels se présentant tôt dans la phrase est interpretée par les dauphins sur la base des mots suivants inclue, dans au moins un cas, des mots non-adjacents.
Cette approche de la compréhension se distingue radicalement de l'emphase sur la production que l'on trouve dans les études des capacités linguistiques des primates. Les résultats obtenus offrent les premières preuves convaincantes de la capacité des animaux à traiter les traits syntaxiques et sémantiques des phrases.
La capacité des dauphins à utiliser les modalités visuelles comme les modalités acoustiques dans ces tâches souligne la dépendance amodale de leur capacité de compréhension des phrases.
On présente des comparaisons entre les performances des dauphins, celles des primates entrainés pour le langage et celles des jeunes enfants, sur des tâches reliées et pertinentes.
De nombreuses études sont en cours afin de développer des méthodes de traitement automatique des langues des signes.
Plusieurs approches nécessitent de grandes quantités de données annotées pour l'apprentissage des systèmes de reconnaissance.
Nos travaux concernent l'annotation semi-automatique de ces corpus de données vidéo.
Nous proposons une méthode de suivi de composantes corporelles, de segmentation de la main pendant occultation et de segmentation des gestes à l'aide des caractéristiques de mouvement et de forme de la main.
Afin de montrer les avantages et limitations de nos contributions, nous avons évalué chacune des méthodes proposées à l'aide de corpus internationaux.
Le système de segmentation des signes montre des résultats prometteurs.
C'est pourquoi nous avons mis à l' épreuve l'hypothèse que l'entraı̂nement de modeles distincts pour des variantes accentuées et non-accentuées soit profitable pour la reconnaisance de la parole continue.
Nous avons appliqué le modèlage de l'accent lexique aussi bien dans l'entraı̂nement que dans le test de reconnaissance.
Des expériences de reconnaissance sur un ensemble de test indépendent a montré que les taux de reconnaissance n'ont pas été améliorés par l'emploi de l'accent lexique dans le modèlage.
Cependant, apres avoir échangé les marques d'accent dans le lexique de reconnaissance, nous avons obtenu une réduction significative des taux de reconnaissance.
Cela suggère que les modèles acoustiques pour les variantes accentuées et non-accentuées des voyelles onte été différentes.
Un problème dans cette expérience était la confusion éventuelle des données sur l'accentuation lexique et le contexte phonématique.
Dans une autre expérience nous avons exclu l'influence du contexte en employant des modèles de contexte généralisés.
Les taux de reconnaissance ne se sont pas améliorés, bienque les modèles vocaliques étaient plus propres à représenter l'information liée à l'accent lexique.
Notre conclusion est que la représentation de l'information sur l'accent lexique dans la surface acoustique n'est pas suffisamment directe pour aider la reconnaissance de la parole continue.
Dans cet article, nous présentons une nouvelle approche particulièrement performante de discrimination parole/musique dans le cadre d'applications réelles de transcription de nouvelles diffusées.
Dans cette approche, un réseau de neurones artificiels (ANN) entraı̂né exclusivement sur de la parole claire (provenant d'un système standard de reconnaissance de la parole grand vocabulaire) est utilisé comme modèle de canal à la sortie duquel nous mesurons toutes les 10 ms l'entropie et le “dynamisme”.
Ces caractéristiques sont alors intégrées dans le temps à l'aide d'un modèles de Markov caché (HMM) ergodique à deux états (parole et non-parole) incluant également des contraintes de durée minimum sur chaque état.
Par exemple, dans le cas de l'entropie, il est effectivement clair (et observé en pratique) que l'entropie à la sortie du ANN sera en moyenne plus élevée pour des segments non-parole que des segments de parole présentés à son entrée.
Dans notre cas, le modèle acoustique ANN est un perceptron multi-couche (MLP, comme souvent utilisé dans les systèmes hybrides HMM/ANN) générant à sa sortie des estimateurs de probabilités a posteriori de phonèmes étant donné les vecteurs acoustiques d'entrée.
C'est à partir de ces sorties, et donc de “vraies” probabilités que l'entropie et le dynamisme sont estimés.
Le modèle HMM parole/musique à deux états prends ensuite ces deux caractéristiques (entropie et dynamisme) dont les distributions sont modélisées par des densités multi-gaussiennes ou par un second MLP.
Les paramètres de ce modèle HMM sont entraînés par un Viterbi supervisé.
Bien que l'approche proposée ici puisse être facilement adaptée à d'autres applications de discrimination parole/non-parole, nous nous focalisons ici sur le problème de segmentation parole/musique.
Différentes expériences, incluant différents styles de parole et musique, ainsi que différentes distributions temporelles des signaux de parole et musique (distributions réelles, surtout parole, ou surtout musique), illustrent la robustesse de l'approche qui résulte toujours en des performances de segmentation correcte supérieure à 90%.
Finalement, nous montrons comment l'utilisation d'un niveau de confiance peut améliorer les résultats de segmentation, et comment ceci peut être utilisé pour traiter les cas de mélanges de parole et musique.
Dans cet article, nous proposons un algorithme parallèle pour la restauration d'images et la détection de contours.
Son originalité réside dans l'emploi de techniques de relaxation stochastique combinées à des réseaux résistifs non-linéaires.
Sa pertinence pour l'amélioration d'images et son adéquation à une architecture cellulaire mixte analogique numérique sont simultanément considérées.
Les simulations présentées montrent l'efficacité de cet algorithme, y compris dans le contexte d'images particulièrement bruitées.
Une méthode d'extraction de la fondamentale du signal de parole basée sur la synthèse est proposée.
Elle procède par la synthèse d'un certain nombre de spectres logarithmiques de puissance pour diffénentes valeurs de la fréquence fondamentale, et pour la comparaison de ces spectres avec le spectre logarithmique de puissance du segment de parole analysé.
La différence d'amplitude moyenne (AM) entre les deux spectres est utilisée pour la comparaison.
La valeur de la fréquence fondamentale qui fournit le minimum de différence AM entre le spectre synthétisé et le spectre du signal d'entrée est choisie comme estimation de la hauteur.
La décision voisé/non-voisé est établie sur la valeur de la différence AM au minimum.
Pour synthétiser le spectre de puissance, le signal de parole est supposé représenter la sortie d'un filtre tout pôle.
La fonction de transfert du filtre tout pôle est estimée à partir du segment de parole par la méthode d'autocorrélation de la prédiction linéaire.
Cette méthode basée sur la synthèse est testée sur des segments de parole naturelle et les résultats sont discutés.
En reconnaissance automatique de locuteurs, des situations peuvent se présenter, où ne peut pas être certain qu'une voix à reconnaître appartienne à un ensemble connu de classes de voix (ensemble fermé).
Par conséquent, le problème se pose d'élaborer un algorithme de reconnaissance qui puisse opérer dans des ensembles ouverts de locuteurs, c.à.d. sans la prémisse qu'un échantillon de voix doive appartenir à l'un des locuteurs d'un ensemble donné.
Deux mots clés, des ensembles différents de paramètres ainsi que de petites comme de grandes populations de locuteurs ont été examinés.
Les prémisses méthodologiques et les résultats expérimentaux permettent de constater que notre méthode de reconnaissance de voix dans les ensembles ouverts est très souple et permet d'ajuster les caractéristiques globales, c.à.d. les erreus α et β, à la stratégie adoptée par le système de reconnaissance.
Pour un ensemble donné d'échantillons de voix, il est toujours possible d'optimaliser la reconnaissance par une sélection correcte des approximations de la distribution de la classe de base, c.à.d. par une sélection appropriée de seuils de décision.
Nous sommes intéressés par la production de services automatisés par des systèmes de dialogue utilisant la parole naturelle.
Nous entendons par naturel que la machine comprend et agit selon ce que les personnes effectivement disent, en opposition à ce que l'on aimerait qu'ils disent.
Plusieurs problèmes apparaissent quand de tels systèmes sont visés pour une population large d'utilisateurs qui ne sont pas des experts.
Dans ce papier, nous focalisons sur la tâche de routage automatique des appels téléphoniques se basant sur la réponse spontanée des utilisateurs à la question ouverte “How may I help you ?”.
Nous décrivons d'abord la base de données générées par 1000 transactions orales entre des utilisateurs et des agents humains.
Nous décrivons ensuite les méthodes pour l'acquisition automatique, à partir des données, des modèles de langage pour la reconnaissance et la compréhension.
Les résultats expérimentaux pour l'évaluation de la classification des appels sont rapportés pour cette base de données.
Ces méthodes ont été incorporées dans un système de dialogue oral avec des traitements subséquents pour le tri des informations et le remplissage des formes.
Face au passage à l'échelle des données, les méthodes de visualisation peinent à fournir de bons résultats.
Dans cet article, nous présentons une approche de prétraitement de données (CTBFS) pour la fouille visuelle de données basée sur la théorie du consensus, le regroupement de données et une affectation visuelle de poids.
Nous utilisons des ensembles de données de l'UCI et du Kent Ridge Bio Medical Dataset Repository pour évaluer les performances de notre nouvelle approche.
Durant ces dernières années, les évaluations Hub-4 des systèmes de reconnaissance de la parole continue sponsorisées par DARPA ont fait progresser les techniques de reconnaissance de la parole pour la transcription de nouvelles audio-diffusées (“broadcast news”).
Dans cet article, nous présentons notre recherche et nos progrès dans ce domaine, en nous concentrant plus particulièrement sur une modélisation efficace utilisant sensiblement moins de paramètres, permettant ainsi d'améliorer la vitesse et les performances de la reconnaissance vocale.
En termes de modélisation acoustique, les améliorations ont été obtenues en utilisant une nouvelle méthode pour l'ajustement des paramètres, l'agrégation des Gaussiennes, et le seuillage des poids des mélanges de Gaussiennes.
L'efficacité de l'adaptation acoustique est grandement améliorée par l'agrégation non supervisée des données de test.
En modélisation du langage, nous avons étudié le résultat de l'utilization de données d'entraı̂nement ne provenant pas de “broadcast news”, ainsi que de l'effet de l'adaptation au sujet et au style de parole.
Nous avons développé une technique efficace d'élagage des paramètres pour des modèles de langage avec repli (“backoff”) ce qui nous a permis de supporter l'accroissement continuel de la quantité de données d'entraı̂nement et l'extension de la portée des N-grammes.
Enfin, nous avons amélioré notre architecture de recherche progressive en utilisant des algorithmes plus efficaces pour la génération et la compaction de treillis, ainsi qu'en y incorporant des modèles de langage d'ordre supérieur.
Produites en grandes quantités, les connaissances issues d'un processus d'extraction de connaissances à partir de données (ECD) font l'objet d'une opération de filtrage automatique.
Cette phase importante du processus, qui a pour objectif de réduire de manière drastique le nombre de règles que l'expert devra examiner "de près", est basée sur l'utilisation d'indices proposant des évaluations quantitatives de la qualité des connaissances.
La recherche des meilleures connaissances parmi le vaste ensemble de connaissances produit, passe aussi par la recherche et l'utilisation des bons indices.
Nous abordons dans cet article le problème du choix pertinent d'un indice par un utilisateur métier dont les critères d'appréciation rendent souvent difficile la sélection de l'indice le mieux adapté.
Nous examinons dans cet article l'intérêt d'une approche de type "aide multicritère à la décision" (AMD) pour aborder ce problème de choix d'indices.
On étudie dans cette recherche les relations entre les niveaux de représentation de phrases et le vocabulaire.
Dans une tâche de détection lexicale on a fait varier la classe du mot cible (ouverte ou fermée) ainsi que le rôle fonctionnel de différents éléments de la classe fermée (propositions lexicales, prépositions obligatoires et particules verbales).
Différents contextes (phrases reliées sémantiquement ou non) ont été utilisées pour tester l'effet sémantique et/ou l'information syntaxique sur les différents types d'items.
Les résultats combinés de sujets normaux et aggrammatiques prouvent que les différent types de vocabulaire sont traités de facon distincte et par conséquence attribués à différents niveaux de traitement.
Ces résultats suggèrent également que l'information lexicale et l'information non-lexicale sont traitées à différents niveaux même si ces informations dependent du même item.
La notion de contraste dans les images numériques est proposée dans une approche multirésolution.
L'amélioration du contraste en est une application principale par le biais d'un procédé itératif.
De plus, ce procédé itératif permet l'obtention d'une forme simplifiée, i.e. binaire, de l'image de départ.
De nombreux exemples sont présentés tout au long de cet article montrant les performances de notre algorithme tant sur des images synthétiques que sur des scènes réelles.
Les réseaux sémantiques que l'on construit à présent sont nommés ontologies.
Au-delà de la logique, ce choix repose sur un lien réaffirmé avec la tradition métaphysique.
Cependant, les lexiques des langues ne sont pas structurés comme des ontologies. Les relations sémantiques sont en effet plus complexes et variables que ce que prévoient les constructeurs d'ontologies.
Pour construire une « dé-ontologie » , il faut restituer la diversité des discours et des genres, qui rendent illusoire une ontologie unique ; insister sur le problème de la diversité sémiotique des textes, les corrélations complexes entre contenu et expression, l'incidence constituante du contexte.
C'est là une condition pour remplir des tâches de caractérisation, notamment en linguistique de corpus.
Les représentations d'images basées sur des régions offrent plusieurs avantages par rapport aux méthodes basées sur des blocs, tels que l'adaptation aux caractéristiques locales de l'image ou la compensation de mouvement d'objet par opposition à la compensation de mouvement par bloc.
Pour la compression des données images, c'est-à-dire pour le codage d'images, de nouveaux algorithmes sont nécessaires qui peuvent travailler sur des régions de forme quelconque, appelées segments au lieu des blocs d'image rectangulaire.
En utilisant une approche des moments généralisés, la fonction de luminance à l'intérieur du segment est approchée par une somme pondérée de fonctions de base, par exemple des polynômes.
Un ensemble de fonctionsde base qui est orthogonal par rapport à la forme du segment à coder peut être obtenu en utilisant des méthodes d'orthogonalisation.
Ceci conduit à l'établissement d'un codeur transformé adapté à la forme.
Des structures appropriées de codeurs et de décodeurs qui ne nécessitent pas la transmission des fonctions de base pour chaque segment sont introduites.
Finalement, une application de l'algorithme obtenu au codage de séquence d'images à faible débit est montrée, basée sur la segmentation de l'image d'erreur de prédiction de mouvement.
Cet article décrit la méthodologie de test utilisée pour évoluer los performances du Codeur CCITT LD-CELP à 16 Kbit/s (recommandation G.728), sur des signaux non-vocaux.
Cette méthodologie est suffisamment genorale pour pouvoir être employee à l'évolution d'autres Codeurs de parole numériques à bas début de transmission.
Les signaux non-vocaux envisagés dans cet article comprennent los données en bande téléphonique, la signalisation du réseau, los tonalités de signalisation et los signaux multifréquences (DTMF).
Dans cette étude, on présente une nouvelle méthode de mesure qui permet d'estimer la source vocale et ses valeurs d'amplitude par filtrage inverse de l'onde de pression acoustique, sans utiliser de masque de Rothenberg.
Cette nouvelle technique est basée sur l'ajustement du gain DC du modèle du conduit vocal par filtrage inverse en unités.
Les performances de cette nouvelle méthode sont testées par l'analyse de la corrélation entre l'amplitude de crête minimale du flux glottique différentiel fourni par la nouvelle méthode et le niveau de pression acoustique de la parole.
Les résultats prouvent que la nouvelle méthode fournit des informations fiables sur les valeurs d'amplitude de la source glottique sans appliquer de masque de Rothenberg.
La diminution de l'efficacité des systèmes de reconnaissance vocale lors d'applications pratiques est due aux différentes exigences acoustiques pendant l'apprentissage et pendant la reconnaissance.
Deux facteurs importants sont la présence de perturbations sonores en arrière plan et la transmission de fréquences entre le micro et l'entrée audio du reconnaisseur.
L'étude présentée ici comporte deux modes de traitement de signaux pour estimer les perturbations sonores actuelles et la différence lors de la transmission de fréquences pendant l'apprentissage et pendant la reconnaissance.
L'évaluation des perturbations sonores est utilisée pour une adaptation des paramètres cepstral des références du reconnaisseur, encore appelées HMM (Hidden Markov Models).
Le mode d'aptation est basé sur la combinaison parallèle de modèles (PMC).
Des améliorations importantes peuvent être apportées pour la reconnaissance d'un mot ou d'une chaı̂ne de mots malgré la présence d'un ou des deux types de perturbations sonores simultanément.
Cette procédure d'adaptation a de plus été intégrée à un système de dialogue et de reconnaissance qui est disponible sur le réseau téléphonique public.
L'application et le gain d'efficacité du reconnaisseur sont démontrés pour cette application dans un environnement de télécommunication existant et en temps réel.
Initialisé avec trois langues : le français, l'anglais et l'italien, le système mis au point dans le cadre du projet SPELL se veut un outil d'enseignement destiné à des utilisateurs de niveau moyen.
Imbroquées dans un environnement intégré nommé DELTA, des informations de type graphique et sonore seront employées pour aider les étudiants à améliorer leur prononciation.
L'analyse prosodique, de niveau macroscopique, prendra en compte les aspects intonation, durée, accent et rythme.
L'approche phonologique utilisée lors du traitement de l'intonation fournit un système bien structuré d'unités contrastées faciles à corréler avec des fonctions linguistiques discrètes.
Dans le système envisagé, l'accent et le rythme seront enseignés de manière moins approfondie en s'appuyant d'une part sur les traits acoustiques relativement simples liés à la qualité des voyelles et d'autre part sur la durée relative des segments.
L'analyse des traits microprosodiques va être focalisée tout particulièrement sur les voyelles.
Une approche à base de traits discriminants est utilisée pour caractériser la prononciation de voyelles par un locuteur ne parlant pas sa langue maternelle.
Il est prévu de mettre en oeuvre des propriétés acoustiques si possible indépendantes du locuteur.
Cette étude propose une première exploration de l'importance des paramètres physiologiques dans les transformations de la voix.
Une démarche générale est décrite pour transformer la qualité de la voix dans des phrases sans en modifier le contenu phonétique.
Les transformations peuvent être reliées au sexe, à l'âge, à la qualité de la voix, à l'état émotionnel, à un état pathologique, à un dialecte ou à une imitation.
Dans cet article, une seule qualité de voix, la voix nasillarde, est décrite en exemple.
La question de base sous-jacente est la suivante : si on les compare à des méthodes fondées purement sur des techniques de traitement du signal, peut-on dire que les méthodes exploitant des principes de mise à l'échelle, dans les domaines biomécanique, acoustique et anatomique sont plus efficaces pour la transformation des voix ?
Dans cet article, deux méthodes sont comparées, l'une fondée sur la Prédiction Linéaire, et l'autre sur des simulations biomécaniques.
La recherche de connaissances dans les données structurées a fait l'objet de nombreux travaux ces dernières années.
La structure de ces objets est irrégulière et il est judicieux de penser qu 'une requête sur la structure des documents est aussi importante qu'une requête sur les données.
De plus, les données manipulées ne sont pas statiques parce que de nouvelles mises à jour sont constamment réalisées.
Le problème de maintenir de telles sous-structures devient aussi prioritaire que de les rechercher car, au fur et à mesure des mises à jour, les sous-structures trouvées peuvent devenir invalides.
Dans cet article, nous proposons un système appelé AUSMS (Automatic Update Schema Mining System), permettant de collecter les données, de rechercher les sous-structures fréquentes et de maintenir les connaissances extraites suite aux évolutions des sources.
Cet article examine les correspondances existant entre les voyelles hautes du japonais ancien de l'Ouest et les voyelles moyennes du japonais ancien de l'Est, à la lumière des récentes hypothèses sur le vocalisme du proto-japonique.
Des correspondances à la fois dans le lexique et dans la morphologie sont établies, puis des données comparatives de plusieurs dialectes japonais modernes et de langues ryukyu sont fournies pour confirmer qu'il s'agit de cas de rétention des voyelles *e et *o du proto-japonique
Cet article présente une méthode de régression pour les signaux non uniformément échantillonnés basée sur les ondelettes.
Nous utilisons une formulation issue de l'apprentissage supervisé et des méthodes à noyaux qui combine une fonction coût L2 et une régularisation L1 multi-échelles.
L'utilisation de l'algorithme Least Angle Régression pour la résolution du problème est à la fois efficace et intéressante, elle permet de calculer le chemin complet de régularisation et d'introduire de nouvelles solutions pour régler le compromis biais-variance.
Dans les bases hétérogènes, les images appartiennent souvent à différentes classes thématiques et nécessitent une large description permettant leur reconnaissance.
Cependant, les caractéristiques utilisées ne sont pas toujours adaptées au contenu de la base d'images considérée.
Nous proposons dans cet article une nouvelle approche se basant sur deux originalités, à savoir la sélection adaptative de caractéristiques et la classification multi- modèle intitulée MC-MM.
La sélection adaptative permet de ne considérer que les caractéristiques les mieux adaptées au contenu de la base d'images utilisée.
La méthode MC- MM assure la reconnaissance des images en se servant hiérarchiquement des caractéristiques sélectionnées.
Les résultats expérimentaux obtenus confirment l'efficacité et la robustesse de notre approche.
Celles-ci peuvent être utilisées, après division par les probabilités a priori, comme probabilités d'émission dans des chaînes de Markov cachées.
L'avantage d'un système hybride de ce type, incorporant un perceptron multicouche comme estimateur des probabilités d'émission d'un modèle de Markov, est son pouvoir discriminant et sa capacité d'intégration de multiples sources d'information (indices acoustiques, contexte temporel), sans devoir formuler d'hypothèses restrictives quant aux formes des distributions et à l'indépendance statistique.
Bien que cette approche se soit révélée fructueuse en reconnaissance de la parole, il est cependant important de comprendre les problèmes sous-jacents et les limites, et de considérer leurs conséquences sur d'autres algorithmes.
Par exemple, les systèmes de reconnaissance actuels les plus performants utilisent des triphones au lieu de phonèmes comme unité de base, tenant ainsi compte du contexte phonétique.
En revanche, la plupart des systèmes basés sur l'estimation des probabilités d'émission à l'aide de perceptrons multicouches peuvent également être généralisé à des unités contextuelles.
Nous discutons brièvement comment les résultats théoriques peuvent influencer le développement de nouveaux algorithmes, comme par exemple des Modéles Autorégressifs non linéaires, et de “Radial Basis Functions”.
Nous proposons une nouvelle manière de mettre en oeuvre un système de dialogue vocal.
Avec cette méthode appelée Compréhension Simultanée, les opérations de reconnaissance de la parole, interprétation et réaction se déroulent pendant que le système reçoit l'énoncé.
Le système de dialogue ainsi rendu plus “interactif” présente les avantages suivants : l'utilisateur n'a pas à attendre à moins qu'il ne souhaite confirmer le résultat de l'action et, au cas où il constate une erreur de saisie, il peut corriger avant l'accès à la base de données.
Nous avons développé deux systèmes de réservation de billets par dialogue vocal en utilisant en procédé de reconnaissance de la parole existant;
le premier système est une application de notre méthode et accepte l'énoncé suivant pendant qu'il analyse le précédent, alors que le deuxième n'accepte l'énoncé suivant qu'après en avoir terminé avec l'analyse du précédent.
Il s'est avéré qu'avec le système reposant sur notre méthode, la reconnaissance des expressions est améliorée de 5,4%, et le temps de réaction de 4,7%.
Cette méthode semble donc prometteuse dans le domaine des systèmes de dialogue vocal interactifs.
Est-il possible de compenser le mauvais fonctionnement de la sélectivité fréquentielle qui accompagne la perte d'audition d'origine cochléaire en accroissant le contraste spectral du signal acoustique de la parole ?
Deux groupes de sujets adultes ont été formés, le premier doté d'une audition normale, le second caractérisé par une perte auditive d'origine cochléaire.
Pour chaque groupe, on constate que les taux de reconnaissance décroissent avec l'accroissement des largeurs de bande.
Lorsque les largeurs de bandes formantiques sont fixées à la moitié de leurs valeurs normales nominales on observe une tendance vers une meilleure identification dans les deux groupes, mais uniquement pour les consonnes en position finale.
Des mesures psychiacoustiques de sélectivité fréquentielle permettent d'appuyer les résultats des tests d'identification obtenus pour les consonnes en position finale.
Ces résultats suggèrent qu'en plus des facteurs de résolution réduite en fréquence et de sensibilité globale réduite, il doit y avoir d'autres éléments qui contrôlent l'identification de la parole dans les cas de pertes d'audition d'origine cochléaire, en particulier pour les consonnes en début de syllable.
On pourrait en trouver l'origine — non traitée ici — dans une résolution temporelle réduite et dans une susceptibilité croissante à l'effet de masque rétroactif.
Les avantages très restreints de la réduction de largeur de bandes formantiques pour l'identification de la parole s'expliqueraient par le fait que cette modification ne peut pas compenser les déficiences du processus temporel auditif.
Cette recherche s'inscrit dans le courant actuel des études psychogénétiques genevoises qui, sans négliger le cadre structural des fonctions cognitives, s'orientent résolument vers l'analyse des processus.
Au-delà de l'étude de la macro-genèse des étapes du développement, nous nous intéressons plus particulièrement à la micro-formation des processus de découverte au cours des séances expérimentales.
Nous présentons ici un premier exemple de l'ensemble de travaux destinés à rendre compte de l'organisation des séquences d'action orientées vers un but.
Notre analyse porte sur 67 enfants de 4;6 à 9;5 ans qui ont à résoudre un problème d'équilibre de plots sur un support étroit.
Moins centrée que dans le passé sur l'explication de notions physiques particulières (en l'occurrence le principe de gravité) notre étude porte essentiellement sur l'aspect dynamique de l'interaction entre l'organisation des séquences d'action du sujet et ses théories implicites.
Enfin, les résultats suggèrent des analogies de nature fonctionnelle plutôt que structurale entre la formation des connaissances physiques et celle du langage.
En compréhension automatique de la parole, la segmentation de parole continue en composants syntaxiques pose un grand problème.
Ces composants sont souvent délimitées par des indices prosodiques.
Cependant l'entraı̂nement de modèles d'étiquetage de frontières prosodiques statistiques nécessite de très grandes bases de données.
Dans le cadre du projet allemand Verbmobil (traduction automatique de parole à parole), nous avons donc développé une méthode d'étiquetage prosodique–syntaxique de larges corpus de parole spontanée où deux principaux types de frontières (frontières syntaxiques majeures et frontières syntaxiques ambigües) et certaines autres frontières spéciales sont étiquetés.
Cette méthode d'étiquetage est présentée et comparée à d'autres méthodes d'étiquetage de frontières basées sur des critères prosodiques perceptifs, syntaxiques, et de dynamique du dialogue.
L'un des avantages principaux de la méthode d'étiquetage prosodique–syntaxique présentée dans cet article est la rapidité avec laquelle elle permet d'étiqueter de grandes bases de données.
De plus, les classificateurs entraı̂nés avec les étiquettes de frontière produites se révèlent être très performants, les taux de reconnaissance atteignant 96%.
Des contrastes articulatoires simples pour une seule opposition phonologique génèrent une multiplicité d'indices acoustiques.
La connaissance des covariations des traits de configurations acoustiques inter- et intra-locuteurs sont nécessaires pour la reconnaissance automatique de la parole.
Un modèle des processus de production de la parole a été utilisé pour générer des stimuli suivant un continuum articulatoire : le degré d'abduction des cordes vocales pour les fricatives dans les mots anglais “hiss” et “his”.
Les durées de transition imposent de fortes contraintes sur les schémas articulatoires servant d'entrées au modèle.
Les durées des segments acoustiques produits par le modèle co-varient de manière semblable à celles produites par 5 locuteurs ; un bon accord quantitatif a été observé dans la plupart des cas.
Les réponses des auditeurs indiquent que la dimension articulatoire synthétisée est appropriée à la parole naturelle.
Finalement, sont discutées des approches futures pour la modélisation des dimensions articulatoires multiples et pour la mise en correspondance de configurations articulatoires spécifiques au locuteur et leurs perturbation avec la stabilité d'indices acoustiques particuliers.
Cet article décrit une méthode permettant un étiquettage automatique des événements prosodiques de la parole, à partir de l'information fournie par les durées segmentales.
Il précise une façon de différencier, à partir des seuls indices de durée, les allongements dus à la prominence de ceux dus à la présence d'une frontière, et expose une anomalie trouvée dans le découpage syntagmatique effectué par 4 locuteurs lisant 200 phrases phonétiquement équilibrées.
On décrit un algorithme qui utilise les différences de durée normalisée au niveau syllabique pour détecter les frontières prosodiques dans le signalle de parole.
Des tests effectués sur des données de parole lue émanant de 4 locuteurs anglais-britanniques, montrent une forte concordance inter-locuteur en ce qui concerne le nombre de frontières détectées et la longueur des syntagmes délimités par chaque paire de frontière, mais la corrélation inter-locuteur sur la localisation effective des frontières est faible.
On observe en particulier une différence nette, entre locuteurs, dans le cas de mots fonctionnels uniques liant deux groupes de mots lexicaux.
Ce problème peut être résolu si l'on considère que la frontière est sur la position du mot fonctionnel lui-même plutôt que à gauche ou à droite du mot lexical.
Ces résultats semblent montrer qu'il existe, dans ce cas, une certaine liberté dans la localisation des frontières prosodiques, qui peuvent êre déterminées soit par la frontière syntaxique, soit par des critères rythmiques.
Les locuteurs natifs du japonais sont incapables d'identifier correctement les phonèmes /l/ et /r/ de l'anglais.
Pourtant, on peut montrer qu'ils sont capables de réagir comme s'ils étaient sensibles aux gestes articulatoires différents qui sont nécessaires pour produire /l/ et /r/.
Dans une étude, des locuteurs natifs du japonais et des locuteurs natifs de l'anglais devaient classer des stimuli le long d'un continuum /da/-/ga/ lorsque le stimulus était précédé par des occurrences naturelles de /s/ ou /∫/, de /al/ ou /ar/.
Chaque paire de “prédécesseurs” avait des effets différents sur l'emplacement de la frontière catégorielle entre /da/ et /ga/, et ni la direction ni l'étendue de l'effet ne dépendait de l'expérience linguistique.
De manière intéressante, /al/ donnait naissance à plus de percepts de /ga/ que /ar/, à la fois pour les locuteurs japonais et anglais, indépendamment de leur aptitude à identifier /al/ et /ar/ en tant que tels.
L'interprétation des résultats repose sur des observations plus anciennes selon lesquelles les effets perceptuellement contrastifs de /al/ vs. /ar/ et de /s/ vs. /∫/ ont un correspondant dans la structure acoustique des énoncés naturels de /al-da/, /ar-da/, etc. du fait de la co-articulation du geste qui produit la consonne qui précède et de celui qui produit le /ga/ ou /da/ suivant.
Il semblerait que les locuteurs natifs du japonais soient sensibles aux conséquences acoustiques de la co-articulation de /l/ ou /r/ avec /d/ ou /g/, alors qu'ils sont incapables de catégoriser /l/ et /r/ en tant que phonèmes distincts.
Il se pourrait donc qu'il existe, en deçà du niveau de perception propre au langage dans lequel les sons linguistiques sont représentés conformément aux contraintes d'un système phonologique, un niveau universellement partagé où la représentation des sons linguistiques correspond de plus près aux gestes articulatoires qui donnent naissance au signal linguistique.
Nous présentons une évaluation d'un modèle de contrôle de la production de la parole exploitant la notion de cible, fondé sur l'Hypothèse du Point d'Équilibre de Feldman.
Elle consiste en des simulations, avec un modèle biomécanique bi-dimensionnel de la langue, de mouvements articulatoires lors de transitions vocaliques.
Dans le modèle, les principaux muscles agissant sur la forme de la langue dans le plan sagittal sont représentés.
La méthode des éléments finis modélise les propriétes élastiques de l'articulateur, et les principes de génération de force sont conformes aux Caractéristiques Invariantes Force-Longueur non linéaires de Feldman.
Le mouvement est produit en déplaçant les variables de contrôle à vitesse constante pour chacune des transitions.
Les contours externes de la langue sont ajustés pour correspondre aux données radiographiques acquises sur un locuteur français, et le modèle est placé à l'intérieur des contours fixes du conduit vocal de ce locuteur.
Nous insistons sur le caractère réaliste des trajectoires formantiques synthétisées, et sur le rôle potentiel des propriétés biomécaniques de la langue sur les caractéristiques cinématiques mesurables.
Cet article présente un panorama rapide des travaux de recherche en technologie de la parole téléphonique menée chez les Opérateurs de Télécommunication européens.
Des études précedentes (Pind, 1986, 1995a) ont mis en évidence que le rapport de durée voyelle sur rime (voyelle + consonne) sert d'invariant d'ordre supérieur pour l'auditeur en lui permettant de distinguer les transformations de durée qui sont dues à des variations de débit de celles qui impliquent un changement de quantité phonémique.
Pour que l'auditeur puisse calculer ce rapport, il faut que les deux segments soient présents dans le signal acoustique.
Dans cet article, on décrit deux expériences perceptives où de la parole naturelle a été manipulée, de manière à ce qu'une fermeture à la suite de la voyelle soit audible ou non (dans le dernier cas la fermeture n'est pas relâchée).
Les résultats soutienment l'hypothèse que le contexte a un effect plus grand sur la localisation de frontières entre phonémes pour la quantité vocalique dans les syllables non relâchées parce que dans ce cas l'auditeur ne peut pas calculer le rapport de voyelle sur rime.
Une expérience a été réalisée dans laquelle divers modèles à deux formants, proposés dans la littérature, sont évalués en fonction de leur capacité de prédire les fréquences formantiques obtenues lors d'une tâche d'identification de voyelles.
Un modèle concurrent est proposé dans lequel le traitemenauditif des voyelles est supposé intervenir en deux étapes.
A l'étape périphérique, le spectre de parole est transformé en son correspondant auditif et les fréquences formantiques sont extraites par une procédure de détection de pics.
L'étape centrale effectuee une approximation à deux formants sur la sortie fournie par le traitement précédent ; l'identification de la voyelle s'appuie sur la paire de formants ainsi extraite.
Les première et secondes fréquences formantiques de ce modèle sont obtenues en additionant un terme correctif aux fréquences des deux premiers formants extraits lors de la première phase.
Ce terme correctif est inverément proportionnel à la distance séparant les formants principaux des formants voisins.
Confronté aux modèles antérieurs, le nôtre se comporte avantage usement quant à la prédiction des fréquences formantiques obtenues dans une tâche d'identification de voyelles.
Cet article traite principalement du choix d'un critère pour évaluer l'efficacité d'un système de reconnaissance de mots enchaînes, lorsque l'on utilise la programmation dynamique pour quantifier les correspondances entre suites de mots.
Certaines propriétés de l'évaluation par la méthode de programmation dynamique sont analysées.
Des expériences menées avec des données du DARPA Research Management Task ont confirmé les prédictions faites à partir de simulations avec des nombres aléatoires : l'évaluation par programmation dynamique surestime le nombre d'erreurs de substitution et sous estime le nombre d'erreurs d'insertion et d'omission.
Le biais du critère habituellement utilisé, le nombre total d'erreurs, est en conséquence particulièrement grand.
Un autre critère, le pourcentage de mots reconnus, diminue ce biais mais ne tient pas compte des erreurs d'insertion.
Un nouveau critère, le nombre total des erreurs pondérées, tient compte des trois types d'erreurs et minimise le biais.
Enfin, certains critères d'évaluation plus sophistiqués sont abordés.
La résolution de processus décisionnels de Markov (PDMs) de grande dimension est habituellement basée sur le calcul hors-ligne d'une approximation de la fonction de valeur optimale et sur son exploitation en ligne pour définir une politique gloutonne.
Dans cet article, nous proposons une méthode alternative reposant sur le développement en chaque état courant d'un arbre de recherche construit à partir de la simulation stochastique du PDM sur un certain horizon de raisonnement.
Nous montrons expérimentalement son bon comportement anytime sur un problème de navigation modélisé comme un plus court chemin stochastique.
La méthode présentée dans cet article, constitue un nouvel outil d'extraction des contours d'une image en niveaux de gris, par coopération de techniques : décomposition en ondelettes et réseaux neuromimétiques.
La première partie est consacrée aux rappels nécessaires quant au formalisme de la décomposition en ondelettes, ainsi que ses principales propriétés.
La phase délicate de l'algorithme réside dans la recomposition optimale des différentes résolutions, afin d'obtenir des contours fins et sans bruit.
Cette tâche est avantageusement confiée à un réseau de neurones, objet de la deuxième partie.
L'attrait majeur de cette nouvelle technique, est sa capacité à traiter correctement des images aux caractéristiques très différentes, sans avoir à modifier de paramètres.
Cet article explique comment l'information visuelle sur le mouvement des lèvres et l'information acoustique du signal de parole peuvent être combinées pour segmenter le flux de parole.
On rapelle les aspects psychologiques de la lecture labiale ainsi que l'état actuel des systèmes automatiques dans ce domaine.
Cet article décrit un système de traitement d'image qui peut déduire la vitesse du mouvement des lèvres à partir de séquences d'images.
La vélocité des lèvres est estimée par une combinaison de techniques d'analyse morphologique des images et de comparaison par blocs.
Cette vélocité calculée est utilisée pour localiser la frontière des syllabes.
Cette information est particulièrement utile quand le signal de parole est bruité.
Cet article démontre également la corrélation qui existe entre le signal de parole et l'information fournie par les lèvres.
Des techniques de fusion de données sont utilisées pour combiner les informations acoustiques et visuelles pour la segmentation de la parole.
Les principaux résultats montrent que cette combinaision des informations visuelles et acoustiques peut réduire les erreurs de segmentation de 10.4% au moins quand le rapport signal sur bruit est inférieur à 15 dB.
Dans le cadre d'un décodeur acoustique-phonétique hybride (ANN/HMM), trois réseaux de neurones (ANNs) spécialisés ont été développés et évalués.
Un de ces ANNs détecte le mode d'articulation.
Les deux autres ANNs décrivent le signal en termes du lieu d'articulation.
Le design de ces réseaux est inspiré par des connaissances acoustiques et phonétiques.
Les entrées, la topologie et le codage des sorties ont été optimisés pour chacun des réseaux.
Ceux-ci sont évalués sur la base de données TIMIT.
Les erreurs de classification trame par trame sur un test de 77 locuteurs indépendants sont de 17.7% pour les 5 modes d'articulation, de 25.4% pour les consonnes nasales et plosives (10 phonèmes) et de 25.2% pour les consonnes fricatives (11 phonèmes).
On évalue aussi la performance d'un système hybride ANN/HMM, pour lequel a été développé un algorithme d'optimisation globale de tous les paramètres.
Le réseau dédié au mode d'articulation et un réseau dédié au lieu d'articulation ont été unis dans un seul ANN.
Les sorties de ce nouveau réseau sont modélisées par un HMM.
Ce système hybride atteint un taux d'erreur de 14% pour la reconnaissance en parole continue de 8 classes de phonèmes (7 plosives et une classe représentant tous les autres phonémes).
Dans cet article, nous présentons une nouvelle méthode pour sélectionner les variables les plus pertinentes à mettre en entrée d'un réseau de neurones de type Multi-Layer Perceptron (MLP) ou Radial Basis Function (RBF).
Cette méthode repose sur l'analyse statistique des dérivées des sorties du réseau par rapport aux entrées.
Lorsque toutes les dérivées des sorties par rapport à une entrée donnée peuvent statistiquement être considérées comme nulle, l'influence de l'entrée en question est négligeable et cette entrée pourra donc être éliminée.
Les résultats seront présentés sur diverses séries temporelles simulées ou réelles.
Cette étude apporte une contribution à la classification de textures à l'appui de signatures déduites des corrélations et/ou spectres d'ordre élevé.
La première partie montre que les textures ont pour la plupart une statistique non gaussienne, bien représentée à l'ordre trois.
Les tests de normalité choisis, les tests d'asymétrie et du Kurtosis, ont été calibrés dans cette étude.
Pour limiter la complexité calculatoire et obtenir de faibles variances d'estimation, nous nous sommes orientés vers des attributs fondés sur les corrélations d'ordre trois dans un cadre stationnaire : la bicorrélation, le bispectre et le bicorspectre.
Nous avons opté pour une stratégie 2D séparable.
Nous proposons dans une seconde partie une caractérisation fondée sur des représentations décrivant les corrélations spatiales d'ordre trois.
La dernière partie est consacrée à la classification des textures s'appuyant sur les attributs précédents.
Nous donnons des performances de discrimination sur neuf textures de Brodatz, en comparant avec les résultats des paramètres, extraits de matrices de cooccurrences, proposés par Haralick.
Jusqu'à ce jour la plupart des théories sur l'aptitude à la lecture n'ont proposé qu'un seul facteur en tant que source essentielle des différences individuelles.
L'accord sur le facteur a été faible et parmi les principaux candidats proposés on trouve la discrimination visuelle, le recodage phonologique et sémantique, la mémoire à court terme et l'utilisation du contexte linguistique.
Dans cet article on résume les théories à un seul facteur et on passe en revue la littérature pour montrer qu'aucune théorie à un seul facteur ne peut être adéquate.
La réussite en lecture estcorrellée à de multiples savoir-faire.
On conclut à la nécessité d'un modéle complexe multifactoriel et on présente certains essais récents pour rassembler les données contribuant à un tel modèls.
Deux méthodes d'analyse des savoir-faire sont présentées et on recommande de les utiliser dans des opérations convergentes.
Enfin on utilise les résultats de l'analyse des savoir-faire pour proposer un premier exemple d'une classe de modèles hiérarchiques de la capacité de lecture qui peut être étudié de façon developmentale.
Les travaux d'avant-garde de Chistovich et de ses collègues se sont servis de la tâche de répétition (shadowing) pour étudier le traitement immédiat de la parole. Ce faisant ils ont exploité le phénomène de répétition instantanée durant lequel le délai entre l'audition et la répétition du stimulus est réduit à 250 ms ou moins.
La recherche ici décrite a débuté par étendre les découvertes de Chistovich à la répétition instantanée de la prose continue.
25% de nos sujets féminins furent capables de répéter avec précision de la prose continue avec des délais moyens de 250 à 300 ms.
Les autres sujets féminins et tous les hommes testés manifestèrent des latences plus longues atteignant en moyenne plus de 500 ms.
Ces sujets sont dénommés des répétiteurs distants.
La seconde série d'expériences permit d'établir que les deux catégories de sujets procédaient à une analyse syntaxique et sémantique du matériau verbal pendant qu'ils le répétaient.
Ceci est manifesté par les contraintes s'exerçant sur leurs erreurs spontanées et dans leur sensibilité aux ruptures de la structure syntaxique et sémantique du matériau verbal en cours de répétition.
Une troisième série d'expériences a montré que la différence entre répétiteurs immédiats et distants résidait dans leur stratégie d'expression.
Les premiers sont capables d'utiliser les résultats de leur analyse en temps réel pour diriger leur appareil articulatoire avant même d'être plenement conscients de ce que sont ces résultats.
Ceci signifie que la répétition immédiate, non seulement reflète de manière continue le résultat du processus de compréhension du langage mais également y parvient en n'étant que relativement peu affecté par les processus post-perceptifs.
En ce sens, donc, la répétition immédiate nous fournit un accès singulièrement privilégié aux propriétés du système.
Les nouveaux développements en matière d'intelligence artificielle ont modifié les suppositions de base sur lesquelles le progrès de la planification de l'usinage assistée par ordinateur (modelage et méthodologies) était fondé pendant les 30 dernières années.
Les calculs numériques représentent donc dans certains cas des alternatives moins bonnes pour les modèles qualitatifs et/ou semi-quantitatifs ainsi que pour les procédés basés et utilisant des sources scientifiques à fondement plus large.
Cet article montre comment le développement et la conception des processus ainsi que la planification, la commande, la surveillance, l'analyse et le contrôle des activités de processus peuvent profiter des schémas de représentation scientifique améliorés et des stratégies de contrôle modernes résonnables.
Il est aussi argumenté que le défi central qui résulte des progrès accomplis dans le développement d'une intelligence artificielle est un “modelage des connaissances”, c'est-à-dire un modelage : (a) des phénomènes physiques et des systèmes dans lesquels ils se déroulent ; (b) des systimès de traitement et de transformation des informations ; et (c) des stratégies qui résolvent les problèmes lors de la planification, de l'exploitation et du contrôle.
C'est pourquoi les diverses strategies d'argumentation exigent diverses forrnes de savoir explicatif et le succés ou l'échec de diverses planifications d'ébauches, de diagnostics et de systèmes de contrôle dépent de l'étendue du savoir réellement utile.
Cet article represents également l'étendue théorique d'importants apports par l'intelligence artificielle ainsi que leurs répercussions sur la formulation et la solution apportées aux problèmes de la planification de la production dans le passé et à l'avenir.
Ce modèle s'appui sur la construction d'un arbre minimal, ses propriétés sont étudiées à travers le problème de la reconnaissance de symboles complexes.
L'invariance de la reconnaissance - face aux translations et rotations de symboles dégradés - est vérifiée dans un contexte d'images binaires à faible résolution.
Si les résultats sont concluant, le coût algorithmique peut être assez élevé.
Une alternative consiste à exprimer l'objet cible dans l'espace Cosinus Discret (Transformation en Cosinus Discrète).
La technique opère non plus dans l'espace image mais dans un espace compact où les données sont mieux décorrélées.
Certains de nos choix font référence à des concepts de compression d'images.
Ces résultats sont d'abord observés lors d'une expérience élémentaire puis confirmés par un test à moyenne échelle, mettant en jeu 500 symboles issus de la base de données Graphics Recognition - GREC2003.
Le livre soulève plusieurs interrogations sur la notion même de ponctuation qui y est développée.
Ce compte-rendu critique attire l'attention sur quelques confusions relevées dans certaines contributions.
Il propose en outre une contre-étude sur un problème paléographique concernant le point d'interrogation et enfin, parce que le terme « ponctuation » semble avoir été employé dans un sens trop large, souhaite apporter des nuances à ce qui apparaît comme une dérive terminologique.
Cet article décrit un algorithme de suivi automatique de formants intégrant des connaissances de parole.
Il opère en deuxé tapes.
La première détecte et interprète les lignes de pics du spectrogramme en termes de formants.
La seconde utilise une méthode d'extraction des contours d'une image pour régulariser les lignes de pics détectés à la premiére étape.
Des connaissances sur la parole servent de contraintes pour guider l'interprétation des lignes de pics.
L'algorithme proposé a l'avantage de fournir des trajectoires formantiques, qui en plus d'être suffisamment proches des pics spectraux correspondant aux formants, sont suffisamment lisses pour permettre une évaluation précise des transitions formantiques.
Les résultats obtenus montrent l'intérêt de l'approche proposée.
Il existe un point de vue selon lequel des processus fondamentaux différents sont impliqués dans la reconnaissance des mots pour la parole et l'écriture.
Nous soutenons que ce point de vue est infondé, et que les modéles de l'accés lexical mis au point pour l'écriture sont aussi appropriés pour la parole, compte tenu des différences évidentes dues aux propiétés physiques des signaux de la parole.
Un accent particulier est mis sur le rôle de la fréquence des mots dans le processus de reconnaissance, car cela limite les types de modéles à prendre en considération (par exemple, le modéle de la cohorte).
Nous rejetons le point de vue selon lequel il n'existe pas d'effects de fréquence dans la reconnaissance des mots parlés, et nous rejetons aussi le point de vue selon lequel les effets de fréquence dans la reconnaissance des mots écrits ne sont que le reflet de décisions post-accès lexical.
Nous présentons un problème de caractérisation issu de la biologie végétale.
Ce problème de caractérisation multiple consiste à traiter simultanément plusieurs groupes d'entités (représentées par des interprétations booléennes) et à trouver une caractérisation pour chacun d'eux sous forme de formules booléennes.
Cette caractérisation doit être exacte et une des difficultés majeures est de trouver une caractérisation minimale.
Une étude de complexité de ce problème est réalisée et nous arrivons à la conclusion que le problème est W[2]-Complet.
Nous proposons une reformulation du problème en programmation linéaire ainsi que différentes approches de résolutions.
Une étude expérimentale est réalisée sur des instances aléatoires ainsi que des instances réelles.
Nous décrivons une méthode permettant de déterminer la fréquence fondamentale de la parole voisée.
Cette méthode exploite la structure harmonique du spectre à court terme afin de construire un 'histogramme harmonique'.
La hauteur détectée est celle de la fréquence de la composante la plus importante.
L'examen des histogrammes pondérés en échelles linéaire et logarithmique montre que ce sont ces derniers qui fournissent la meilleure indication de la hauteur.
Les spectres sont dérivés à partir d'un banc de filtres digitaux comprenant 187 canaux se situant entre 50 Hz et 3200 Hz (échelle log).
En tronquant les spectres à leur extrémité basse fréquence, il apparaît que l'on peut encore obtenir une indication claire de la hauteur dans la bande téléphonique de 200 Hz à 3200 Hz.
L'influence du F1 de certaines voyelles sur les amplitudes des harmoniques perturbe l'indication correcte de la hauteur.
Une méthode de 'renforcement harmonique' est proposée qui permet effectivement de surmonter cette difficulté.
Des exemples de contours mélodiques obtenus par cette méthode montrent que la hauteur peut être déterminée avec succès pour des nasales, des voyelles et des fricatives voisées.
Nous présentons le premier formalisme de recherche heuristique permettant de résoudre les POMDPs décentralisés (DEC-POMDP).
Nous présentons ici une nouvelle classe d'algorithmes qui fait le lien entre les méthodes de recherche heuristique classiques et la théorie du contrôle décentralisé.
Nous prouvons l'optimalité de ce formalisme dans le cadre des politiques déterministes, et nous évaluons sa performance sur quelques exemples d'applications répandus dans le domaine du contrôle décentralisé.
Les réseaux de microphones (ou antennes acoustiques) peuvent être un moyen efficace pour combattre les dégradations dues au bruit et à la réverbération dans les terminaux de communication mains-libres.
Dans cet article, nous discutons de la formation de voie classique retard-somme, ainsi que de la formation de voie filtrage-somme, plus générale.
Cette dernière possède la capacité supplémentaire de contrôler le diagramme de directivité du réseau en fonction de la fréquence, et une nouvelle méthode de conception pour les réseaux à largeur de lobe constante est présentée.
Les réseaux à formations de voie retard-somme et filtrage-somme doivent posséder des dimensions comparables aux longueurs d'onde.
Cette caractéristique peut conduire à des dispositifs de grande taille.
Pour les applications où on dispose de peu d'espace, nous présentons des réseaux de microphones différentiels.
Finalement, deux types de systèmes adaptatifs de formation de voie sont présentés : un réseau en configuration “broadside” et un microphone différentiel à deux éléments.
Nous décrivons des expériences dans le domaine de la conversion de la voix qui utilisent des paramètres acoustiques provenant de la voix de deux individus (source et cible).
Des transformations sont faites sur les paramètres de la source afin de les faire coïncider autant que possible avec ceux de la cible.
Le signal de parole des deux individus et celui obtenu après transformation sont synthétisés et comparés au signal original.
Le but de cette recherche est de développer un modèle pour (1) créer de nouvelles voix synthétiques, (2) étudier les facteurs responsables de la qualité des voix synthétiques, et (3) déterminer des méthodes pour la normalisation de la voix de différents individus.
A cause des énormes quantités de règles qui peuvent être produites par les algorithmes d'extraction, la validation des connaissances reste l'une des étapes les plus problématiques de la découverte de règles d'association.
Il est nécessaire d'aider l'utilisateur à s'approprier ce volume de règles et de l'assister dans sa recherche de la connaissance pertinente en organisant une véritable fouille de règles.
Pour cela, les techniques de réalité virtuelle, qui associent une interactivité forte et intuitive à des représentations 3D immersives capables d'intégrer une grande densité d'information tout en demeurant intelligibles, peuvent s'avérer très profitables.
Nous proposons dans cet article une représentation graphique dynamique des règles d'association, fondée sur des métaphores en réalité virtuelle, qui supporte l'utilisateur dans sa tâche de fouille interactive de règles.
Un premier prototype implémentant cette nouvelle représentation a été développé afin de valider notre approche.
Dans cet article, nous proposons une nouvelle méthode de détection de mots-clés basée sur des mesures de confiance et les machines à vecteur support (SVM).
Les mesures de confiance sont calculées à l'aide des probabilités a posteriori des phonèmes fournies par un système de reconnaissance basé sur les modèles de Markov cachés.
Nous présentons trois types de moyennes (arithmétique, géométrique et harmonique) pour calculer une mesure de confiance pour chaque mot.
Nous proposons également l'utilisation des machines à vecteur support qui constituent une technique de classification développée à partir de la théorie de minimisation du risque structurel.
La décision d'accepter/rejeter un mot est basée sur le vecteur de mesures de confiance qui est l'entrée du classifieur SVM.
Les performances du classifieur SVM proposé sont comparées à celles obtenues par les différentes méthodes basées sur les mesures de confiance à base de moyenne.
Les polynômes qui n'ont que des racines à partie réelle négative et ceux qui n'ont que des racines à l'intérieur du cercle unité peuvent être caractérisés de différentes manières.
Les critères fondés sur l'introduction d'une paire de polynômes dont les racines alternent sur l'axe des imaginaires ou sur le cercle unité sont étendus aux polynômes à coefficients complexes.
Un algorithme de calcul pour tester les polynômes à coefficients réels ou complexes admettant toutes leurs racines dans un demi-plan donné, ou dans un secteur du plan délimité par deux demi-droites passant par l'origine, est établi.
Une démonstration complète du critère de Routh pour tester la stabilité des filtres linéaires à temps continu et à coefficients réels ou complexes est proposée.
“Contour” et “trajectoire” sont devenus des termes familiers qui, pour toute voyelle, servent à décrire, sur l'axe des temps, l'évolution des fréquences propres à chacun des formants. Par contre, il y aurait lieu d'établir des vocables analogues permettant de préciser le profil de ces fréquences sur “l'axe des voyelles”.
On introduit, donc, le concept de vowel-formant ensemble (et l'acronyme VFE qui en découle) afin de pouvoir regrouper, de voyelle à voyelle, les fréquences d'un formant (e.g., F2) qui sont obtenues, à un instant fixe de l'axe des temps, pour le même locuteur et dans le même contexte syllabique CVC.
Notons que le concept de VFE contient à lui seul toute la démarche adoptée ici, à savoir que notre modélisation précédente des trajectoires des formants (Broad et Clermont, J. Acoust. Soc. Am. 81, 1987, 155–165) repose sur une fonction linéaire de la cible des voyelles et, de ce fait, suggère l'hypothèse que des relations linéaires devraient aussi servir à caractériser les VFEs propres à un locuteur et chacun des formants à la fois.
On aborde cette question pour les fréquences des formants F1 et F2 de 7 voyelles de l'anglais australien qui ont été prononcées par un locuteur masculin, 5 fois de suite, dans 7 contextes syllabiques du type CVd.
Nonobstant les variations aléatoires inhérentes aux 5 répétitions, l'application de notre hypothèse aux voyelles en question engendre un écart quadratique moyen qui ne dépasse pas la valeur, remarquablement faible, de 14 Hz pour F1 et F2.
Les relations linéaires ainsi obtenues se prêtent à une normalisation par rapport au facteur contexte, que l'on démontre par une réduction de la dispersion intra-voyelle dans l'espace planaire F1F2.
Les relations dérivées du concept de VFE constituent également un nouvel outil devant permettre la mise en évidence des effets de différents contextes au travers des noyaux vocaliques de syllabes.
Le signal de parole produit par deux locuteurs a été segmenté manuellement, donnant lieu à deux based de données comprenant 18,000 et 6,000 segments vocaliques.
Suite à l'abondance des textes spécialisés, une demande accrue de leur traitement automatique est souhaitée.
L'extraction terminologique, c-à-d. l'extraction de groupes de mots significatifs pour le domaine, est une information communément recherchée dans tes domaines spécialisés.
Dans cet article, nous proposons une méthode d'extraction automatique des termes spécifiques.
Dans notre approche, nous avons en entrée un corpus spécialisé à partir duquel nous effectuons des traitements préliminaires : nettoyage et étiquetage.
Lorsque ces prétraitements sont effectués, nous nous appuyons sur des mesures d'association classiques pour extraire la terminologie du domaine.
Notre principale contribution est le fait d'ajouter différents paramètres afin d'améliorer la recherche des termes du domaine.
Nous explorons des méthodes d'amélioration de la parole altérée par un bruit additif indépendant avec une source unique.
Nous présentons des systèmes d'amélioration adaptative basés sur une technique non-adaptative [Ephraim, Y., 19992a. IEEE Transactions on Signal Processing 40 (4), 725–735].
Cette approche permet de construire des modèles statistiques de la parole et du bruit en utilisant des modèles de Markov cachés auto-regressifs.
Nous développons deux méthodes principales.
La première méthode estime les modèles statistiques du bruit à partir des silences détectés.
La deuxième crée des estimations du maximum de vraisemblance des paramètres du bruit inconnu en utilisant l'ensemble de la phrase.
Les deux techniques opèrent sur un schéma auto-regressive hidden Markov models (AR-HMM).
Nous avons montré précédemment que les possibilités des AR-HMMs pour modéliser la parole pouvaient être améliorées en incorporant une fréquence de perception utilisant une transformée bilinéaire.
Nous introduisons cette correction dans nos systèmes d'amélioration de la parole.
Nos approches sont évaluées sur les bases de données NOISEX-92 et Resource Management, en donnant des indications de la performance respectivement sur des taches simples et complexes.
Les deux schémas permettent d'améliorer les résultats de base.
La technique qui crée des estimations ML des paramètres du bruit apparaît être la plus efficace.
Son efficacité est évaluée sur une large variété de bruits allant de −6 dB à 18 dB et sur divers types de bruit stationnaires réels.
Une nouvelle méthode d'analyse de la parole qui utilise plusieurs concepts psychoacoustiques bien établis, est appliquée à l'analyse des voyelles.
Cette analyse par prédiction linéaire perceptivement fondée (PLP), modélise le spectre auditif par celui d'un modéle tout pôle d'ordre réduit.
Le spectre auditif est dérivé de l'onde de parole par filtrage en bandes critiques, pré-accentuation par courbes d'égale sonie, et conversion d'intensité en sonie par extraction de racine (loi de puissance de Steven).
Analysant la parole tant naturelle que synthétique, nous démontrons que les concepts psychoacoustiques d'intégration auditive du spectre s'appliquant aux voyelles, notamment le concept F1, F2′ de Carlson et Fant et le concept de Chistovich d'intégration auditive de 3.5 Barks sont bien modélisés par la méthode PLP.
Un système complet d'analyse-synthèse, basé sur la méthode PLP, est également décrit dans cet article.
Ce papier présente les évaluations, sur des données d'exploitation (terrain), de deux serveurs d'information qui utilisent des technologies vocales et des interfaces homme–machine différentes.
Le premier système, RAILTEL, utilise la reconnaissance de mots isolés et un dialogue contraint.
Le second système, Dialogos, comprend la parole spontanée et implante une stratégie de dialogue avec initiative mixte.
Les deux systèmes permettent l'accès à une base de données italienne d'horaires de trains à travers le réseau téléphonique publique.
RAILTEL et Dialogos ont été testés par des utilisateurs non expérimentés dans de larges essais terrain.
De plus, des essais comparatifs sur un nombre limité d'usagers ont été effectués afin d'évaluer l'impact des différentes technologies vocales sur le comportement des utilisateurs.
Nous avons proposé un ensemble de caractéristiques de ces règles et une classification : « directes » et de « modulation » .
Pour pouvoir traiter de telles règles dans les systèmes hybrides, nous nous sommes inspirés des connexions présynaptiques étudiées en neurobiologie, qui permettent des traitements du type modulation.
L'équivalent de ces connexions synaptiques dans les systèmes connexionnistes est constitué par les connexions Sigma-Pi.
Nous avons proposé un nouveau type de connexion neuronale : les unités Sigma-Pi Asymétriques, lesquelles représentent le mieux les processus de modulation entre certaines entrées d'un réseau neuronal.
Ces connexions ont été implémentées dans un SHNS pour l'apprentissage de bases d'exemples contenant des relations de modulation.
Cette étude concerne l'adaptation à de nouvelles langues, à partir de peu de données d'apprentissage, d'un système d'identification de langue déjà existant.
La plate-forme utilisée dans cette étude est le système récemment développé pour exploiter les contraintes phonotactiques basées sur une reconnaissance de phones dépendante de la langue (Yan and Barnard 1995a, b).
En utilisant la technique de ré-estimation du modèle de langage basée sur la descente de gradient probabiliste, deux nouvelles approches, ainsi que leur combinaison, sont proposées et testées.
Toutes ces approches modifient les modèles de langage phonotactiques, et ne correspondent donc plus à l'estimation conventionnelle de maximum de probabilité.
La spécificité de ces méthodes peut être vue comme une ré-estimation différente de l'information sur le même ensemble de données.
Des expériences ont été menées en utilisant la base de données standard OGI_TS (Muthusamy et al., 1992).
Le système de base (avec l'estimation conventionnelle des données) a également été soumis au même ensemble de tests, pour comparaison.
Les systèmes, entraı̂nés avec diverses quantités de données d'apprentissage dans les nouvelles langues, ont été évalués.
Les résultats montrent que les nouvelles méthodes améliorent l'adaptation à de nouvelles langues, par rapport à l'estimation des modèles conventionnelle.
Le succès du modèle discriminant montre que l'estimation de modèle conventionnelle n'est pas optimale pour l'identification de langue, et donc que des améliorations peuvent être obtenues en modifiant les estimations de maximum de probabilité des modèles de langage.
Cet article fournit des données phonético-acoustiques sur trois styles langagiers : la parole conversationnelle informelle, la parole “claire” et le “baby-talk”.
Ces ensembles d'observations illustrent le fait que la prononciation par un locuteur d'une forme linguistique donnée peut subir de fortes transformations physiques, particulièrement dans la grande variété de contextes qu'offre la parole produite spontanément.
En dépit de leurs variations extensives, les mesures des formants vocaliques montrent un haut degré de prédictibilité.
Les observations mettent au premier plan une question classique de la recherche en parole : la variabilité du signal et l'invariance phonétique.
Bien que les résultats présents n'éliminent pas définitivement la possibilité que les échantillons de parole analysés soient organisés autour d'un noyau d'invariants du signal, l'ampleur aussi bien que la caractére systématique de la variabilité observée ouvrent une perspective différente.
L'article propose que les patrons acoustiques dispersés de la parole soient considérés comme des produits de l'adaptation.
Selon cette interprétation, les gestes phonétiques et les signaux sont modulés adaptivement en fonction des besoins de la communication en temps réel et de la situation socio-linguistique (par exemple, en contrôllant la “distance sociale” entre les locuteurs, en préservant l'intelligibilité, en utilisant les fonctions “phatique” et “émotive”, etc.).
De plus, il est argué linguistique du signal phonétique n'est pas d'encoder des invariants mais de compléter l'information déjá disponible par le systéme de traitement de la parole du locuteur.
Ainsi, les variations phonétiques intra-locuteurs ne doivent plus être considérées comme des invariants enchâssés dans une variabilité linguistiquement non pertinente.
Elles représentent plutôt de réelles adaptations comportementales qui peuvent mettre en danger ou détruire l'invariance du signal mais qui transforment les patrons de parole selon des voies qui relèvent essentiellement de principes.
Les deux principaux instruments utilisés comme capteurs de perception en robotique sous-marine sont le sonar et la vidéo.
Dans une première partie, nous présentons les principes utilisés en imagerie acoustique.
Si certaines techniques sont classiques, d'autres, telles que l'antenne synthétique, l'interférométrie et l'antenne paramétrique sont encore du domaine de la recherche appliquée.
Dans une seconde partie, les applications de l'imagerie acoustique sont évoquées.
Elles sont essentiellement axées sur la caractérisation des fonds sous-marins et sur l'aide que l'image peut apporter à la navigation du robot.
Enfin, la troisième partie évoque les technologies et les traitements vidéo.
Ce capteur s'avère très complémentaire du sonar grâce à sa haute résolution et à la facilité d'interprétation des images.
Cet article traite le cas de l'inversion articulatoire acoustique en production de la parole.
On propose un cadre d'analyse fondé sur une combinaison explicite des contraintes morphologiques et acoustiques du conduit vocal.
La solution se base sur une analyse de Fourier du logarithme de la fonction d'aire du conduit vocal : le rapport entre les coefficients de Fourier en cosinus du logarithme de l'aire et les formants correspondants est utilisé pour formuler une contrainte acoustique.
Le même jeu de coefficients est alors utilisé pour obtenir une contrainte morphologique.
Cette représentation des contraintes acoustiques et morphologiques dans le même espace des paramètres permet une résolution efficace du problème inverse.
Le principe de la formulation de la contrainte acoustique a été proposé pour la première fois par Mermelstein (1967).
Cependant, la combinaison avec les contraintes morphologiques n'a pas été réalisée dans le cadre de ce travail.
La méthode est testée pour quelques voyelles.
Les résultats confirment la validité du modèle, mais montrent aussi la nécessité d'inclure des contraintes dynamiques.
On s'intéresse aux problèmes inverses sous déterminés, et plus particulièrement à la localisation de sources en magnéto et électro-encéphalographie (M/EEG).
Bien que l'on ait à disposition un modèle physique de la diffusion (ou du « mélange » ) des sources, le caractère très sous-déterminé de ces problèmes rend l'inversion difficile.
La nécessité de trouver des a priori forts et pertinents physiquement sur les sources est une des difficultés.
Dans le cadre du problème inverse de la M/EEG, la parcimonie classique mesurée par une norme l\ n'est pas suffisante, et donne des résultats non réalistes.
On propose ici de prendre en compte une parcimonie structurée grâce à l'utilisation de normes mixtes - notamment d'une norme mixte sur trois niveaux.
La méthode est utilisée sur des signaux MEG issus d'expériences de stimulation somesthésique.
Lorsqu'ils sont stimulés, les différents doigts de la main activent des régions distinctes du cortex sensoriel primaire.
L'utilisation d'une norme mixte à trois niveaux permet d'injecter cet a priori dans le problème inverse et ainsi de retrouver la bonne organisation corticale des zones actives.
Nous montrons également que la méthode la plus classiquement utilisée dans le domaine échoue dans cette tâche.
Le problème de séparation de sources, très classique en traitement du signal, correspond aussi à une réalité dans les systèmes biologiques.
En effet, les capteurs biologiques sont sensibles à de multiples sources, le système nerveux central traite donc des signaux multi-capteurs dont chaque composante est un mélange inconnu de sources inconnues, supposées indépendantes.
Par rapport aux règles utilisées en filtrage adaptatif, l'incrément d'adaptation fait intervenir nécessairement le produit de deux fonctions non linéaires.
Plusieurs résultats expérimentaux dans le domaine du Traitement du Signal ou du Traitement d'images illustrent les performances de l'algorithme.
La généralisation à des mélanges plus complexes, ou dégénérés est également discutée et illustrée.
Cet algorithme révèle aussi un nouveau concept d'Analyse en Composantes Indépendantes, plus fort que celui d'Analyse en Composantes Principales, applicable dans le cadre général de l'analyse de données.
Dans cet article, nous proposons une méthode originale de détection et de correction des erreurs d'accord, que nous appliquons au cas de l'arabe.
La détection se base sur une analyse syntaxique globale de la phrase.
Il s'agit d'une « analyse syntagmatique étendue » qui permet, dans un premier temps, de localiser les frontières syntagmatiques et de regrouper, dans un deuxième temps, tous les éléments de la phrase qui ont un lien d'accord entre eux.
La correction des erreurs détectées se base sur une méthode d'analyse multicritère permettant le classement des scénarios de correction en vue d'en faire émerger le meilleur.
Cette méthode a l'avantage de réduire, d'emblée, les scénarios dominés, et de classer le reste selon différents critères d'évaluation.
Les trajectoires articulatoires de la fricative [s] produite par un locuteur féminin parlant l'anglais américain général ont été analysées dans la parole continue et dans des séquences logatomiques phonétiquement contrôlées et articulées à un débit plus lent.
Les traces dérivées aérodynamiquement indiquant le paramètre acoustiquement pertinent de l'aire de section de la région de constriction du conduit vocal suggérent que, contrairement aux données rapportées pour les mouvements des structures solides, certaines portions des traces pour cet indicateur du conduit vocal ont la même forme de trajectoire à travers différents contextes vocaliques et à travers différents styles de parole.
Dans la parole continue, la trajectoire entière de l'aire de section semble être invariante à travers différents contextes de voyelles accentuées.
Les traits du patron acoustique associés aux portions invariantes de l'articulation du conduit vocal, en combinaision avec les articulations laryngées et respiratoires appropriées, sont discutées.
Dans un travail récent sur le syndrome aphasique de l'agrammatisme, Kean (1980a) soutient qu'on peut rendre compte des éléments dégradés vs. les éléments conservés en ne considérant que le niveau de représentation linguistique intermédiaire entre les structures syntaxiques et phonologiques.
Le burāq est la monture que le prophète Muḥammad aurait enfourchée lors de son voyage nocturne de la Mecque à Jerusalem (isrāʾ) et à l'occasion de son ascension céleste (miʿrāğ).
Cet article examine la notion du burāq au sein des sources musulmanes comme occidentales non-musulmanes.
Nous retraçons l'évolution de descriptions relativement sommaires du burāq telles qu'on les rencontre dans les premières sources musulmanes jusqu'aux comptes rendus tardifs enjolivés et pittoresques.
Nous examinons également le rôle que le burāq joue dans des discussions musulmanes ayant trait à l'isrāʾ et au miʿrāğ.
Pour ce qui ressortit aux approches occidentales non-musulmanes, l'article démontre comment le burāq fut tout d'abord utilisé dans la polémique contre l'islam, avant de devenir un objet de fascination et, plus tard, un sujet d'étude universitaire sur un mode plus impartial.
Les relations de compensation entre les indices de l'intensité de l'aspiration et ceux de l'intensité de la voyelle suivante pour la distinction entre /d/ et /t/ ont été étudiées aux niveaux auditif, phonétique, syllabique, du mot et de la phrase.
Les résultats montrent qu'au fur et à mesure que s'élève le niveau du traitement, le moins catégoriel est l'identification et le plus efficace la compensation d'indices.
Les relations de compensation plus fortes au niveau de la syllabe, du mot et de la phrase suggèrent que le mode “parole” de la perception est un mode distinct.
Néanmoins, une tendance à la compensation entre indices persiste aux niveaux auditif et phonétique.
Les résultats semblent aussi venir à l'appui du modèle interactif du traitement de la parole.
Généralement, la mesure de dissimilarité inter-mots fournie par les algorithmes de “distorsion temporelle dynamique” (DTW) ne peut sevir de métrique, car elle ne satisfait pas à toutes les propriétes requises (l'inégralité triangulaire en particulier).
Dans cet article, cependant, l'évidence empirique selon laquelle la parole naturelle satisfait approximativement à ces propriétés, est mise en lumière ; ce fait autorise l'hypothèse d'une structure “d'espace métrique vague” dans l'ensemble des représentations paramétriques des mots dans un vocabulaire donné.
Basé sur cette structure, un algorithme de recherche dans cet espace est introduit, algorithme qui en Reconnaissance de Mots Isolés, élimine la nécessité de calculer la distance-DTW entre le mot-test et un grand nombre de mots prototypes du dictionnaire.
Des expériences menées sur des vocabulaires de caractéristiques différentes ont démontré que cet algorithme détermine le prototype le plus proche en effectuant les opérations de “distortion temporelle dynamique” (DTW) sur une moyenne de 30% seulement des mots du dictionnaire.
Il a été observé que ce chiffre tend à décroître avec l'accroissement de la taille du dictionnaire, et aussi dans le cas où le mot est proche du prototype correspondant (mot “beien prononcé”).
Nous présentons dans cet article l'intégration d'un système de reconnaissance analytique de la parole dans une console sonar.
Cette application correspond à un besoin des opérateurs qui ont continuellement les yeux occupés à scruter l'écran sonar.
Le système DIAPASON présente deux particularités : le décodage acoustico-phonétique utilisé dans le système n'est pas fondé sur une reconnaissance phonétique classique ; son objectif est d'obtenir pour chaque segment de parole une description précise en termes de traits acoustico-phonétiques.
Ce niveau de décodage est associé à une procédure spécifique d'accès lexical et de reconnaissance de phrases ; le système DIAPASON est de plus un véritable système de dialogue homme-machine et ne se limite pas à une simple reconnaissance de phrases comme c'est le plus souvent le cas.
L'historique du dialogue est considéré comme une source de connaissances à part entière durant la phase de reconnaissance et cela permet d'augmenter de façon notable les performances globales du système.
Cet article détaille l'architecture de DIAPASON et décrit ses diverses composantes.
Il présente aussi les résultats expérimentaux obtenus en mode multilocuteurs et compare les systèmes de dialogue avec parole et sans parole pour une console sonar réelle.
Dans le cadre des télécommunications mains-libres, les deux problèmes que sont la réduction de bruit et l'annulation d'écho acoustique doivent être résolus pour améliorer la qualité de la communication.
Notre objectif est de trouver une combinaison des systèmes résolvant chacun des problèmes pour obtenir un signal à transmettre faiblement distordu pour des niveaux d'écho et de bruit résiduels acceptables.
Quatre structures (à 1 ou 2 microphones et 1 haut-parleur) sont étudiées dans lesquelles l'annulation d'écho précède la réduction de bruit.
Des expérimentations sur signaux réels ont été conduites.
Au vu des résultats obtenus, nous sommes conduits à proposer une structure contrôlée par un détecteur d'écho seul.
Nous décrivons dans cet article un réseau de microphones destiné à l'enregistrement de la parole dans une voiture (cette étude a été réalisée dans le cadre du projet ESPRIT ARS “adverse environment recognition of speech”).
Le réseau est conçu pour un radio-téléphone mains-libres, et sert de point d'entrée pour un système de reconnaissance automatique de la parole.
Nous commencerons par résumer les techniques de formation de voie adaptive que nous avons utilisées.
Nous présenterons ensuite plusieurs aspects de l'implantation de ce réseau (configuration, conception de la formation de voie fixe, adaptation, réduction de la complexité).
Dans la dernière section, nous évaluons les performances du réseau.
Deux mesures sont utilisées, l'une est le rapport signal sur bruit, l'autre est le score obtenu par un système de reconnaissance de mots isolés.
En stéréovision binoculaire, la mise en correspondance est une étape cruciale pour réaliser la reconstruction 3D de la scène.
De très nombreuses publications traitent ce problème.
Ainsi, le premier objectif est de proposer un état de l'art des méthodes de mise en correspondance.
Nous synthétisons cette étude en présentant un algorithme générique complet faisant intervenir des éléments constituants permettant de décrire les différentes étapes de la recherche de correspondances.
Enfin, le dernier objectif est de présenter de nouvelles méthodes hybrides, dans le cadre des méthodes locales à base de corrélation.
Nous nous appuyons sur l'utilisation de deux mesures de corrélation permettant de mieux prendre en compte le problème des occultations.
Les résultats mettent en évidence la meilleure méthode qui consiste à fusionner deux cartes de disparités obtenues avec des mesures différentes.
Au cours de la dernière décennie, de multiples méthodes pour l'analyse et la classification de données fondées sur la théorie des espaces de Hilbert à noyau reproduisant ont été développées.
Elles reposent sur le principe fondamental du kernel trick, initialement exploité par Vapnik et col. dans le cadre des Support Vector Machines.
Celui-ci permet d'étendre au cas non-linéaire des traitements initialement linéaires en utilisant la notion de noyau.
La méthode KFD, pour Kernel Fisher Discriminant, constitue ainsi une généralisation non-linéaire de l'analyse discriminante de Fisher.
Cet article présente un algorithme séquentiel palliant cette difficulté puisqu'il ne nécessite, ni la manipulation, ni même le stockage de telles matrices.
Un parallèle est également proposé entre KFD et KPCA, acronyme de Kernel Principal Component Analysis, cette dernière méthode constituant une extension au cas non-linéaire de l'analyse en composantes principales.
La synthèse de voyelles au moyen d'un modèle source-filtre s'effectue souvent avec un train d'impulsions delta comme signal d'entrée.
Bien que des modèles sophistiqués de la source glottale puissent être employés, dans une certaine mesure, afin de simuler une voix soufflée, une simulation plus naturelle exige l'addition d'un bruit d'aspiration.
Cependant, lorsqu'un bruit stationnaire est employé, il sera perçu en bonne partie comme provenant d'une source sonore séparée qui ne contribue pas au timbre soufflé de la voyelle.
Ce problème peut être résolu en utilisant un bruit ayant une enveloppe temporelle de la même périodicité que le train d'impulsions.
Dans un simple modèle source-filtre, des impulsions filtrées passe-bas combinées à un bruit pulsatif synchrone filtré passe-haut à énergie égalisée ont été employées comme signal de source.
De cette façon, le bruit n'est plus perçu séparément, mais est intégré perceptuellement à la partie strictement périodique du signal.
Il sera démontré que cette intégration consiste à la fois en une réduction de la force sonore du bruit et en une altération du timbre de la voyelle soufflée.
Dans cet article, nous présentons une nouvelle mesure de la vraisemblance qui étend à la fois le mécanisme d'adaptation a la réduction de la matrice de covariance et l'adaptation du biais du vecteur moyen.
Un ensemble de fonctions d'adaptation est proposé afin d'obtenir les facteurs de compensation.
Les expériences montrent que la mesure de vraisemblance proposée augmente de manière significative le taux de reconnaissance.
Cet article décrit un cas de dysgraphie pour lequel nous postulons une lesion selective de la mémoire-tampon graphémique.
Les difficultés analogues que rencontre le patient pour l'orthographe orale et écrite et les difficultés d'orthographe comparables qu'il rencontre pour la denomination écrite, la copie avec temps de latence et l'écriture sous dictée interdisent de penser qu'il y a eu lésion selective des mécanismes d'input ou d'output.
Plus important, la nature des erreurs du patient et le fait que la distribution de ces erreurs est virtuellement identique pour les mots familiers et les mots nouveaux, nous semble démontrer que les troubles de L.B. résultent d'une lésion sélective de la mémoire-tampon graphémique.
Divers aspects de la performance du patient sont analysés par rapport à l'architecture fonctionnelle du processus d'orthographe et en termes de la structure de la mémoire-tampon graphémique.
Bien que moins appliquée, la conception de Henri Ey est en grande partie connue en Roumanie.
Elle est présentée par l'œuvre du Pr Pamfil (de Timisoara), élève de Henri Ey pendant quelques années en France, et par les traductions en roumain de « La conscience » ainsi que des trois volumes des « Études psychiatriques » .
En 2004, l'auteur vient de rééditer et compléter « Psihiatrie » , publié en 1997 à Bucarest et qui réserve, aux conceptions en psychiatrie en général et aux conceptions organodynamiques en particulier, le rôle de prémisses de sa propre conception sur la resocialisation des malades psychiques.
La conception originale de Henri Ey est la domination du corps psychique par les structures de l'ego, du réfléchissement de la réalité et de la conscience-de-soi, et, enfin, de l'identité.
Ainsi, Henri Ey a assimilé la psychanalyse, le psycho-organicisme comme la psychogenèse, dans une entité dominée par un ego rationnel.
Henri Ey aura été un anthropologue et il a élevé la psychiatrie à une initiation théorique.
La perception auditive est modélisée à l'aide d'un automate à niveaux hiérarchiques multiples.
Un certain nombre de niveaux hiérarchiques sont définis plus exactement, les aspects conceptueles sont décrits et le traitement du signal, qui a lieu dans le canal d'entrée/sortie, est exposé.
Le système d'indices, qui tent compte de l'effect de masque, est décrit.
La modélisation de l'effet de masque permet d'éliminer, sans perte d'information, les composantes spectrales qui sont masquées par des composantes voisines plus énergðiques.
La validation dy système d'indices ainsi obtenu a été effectuée avec la participation de 5 locuteurs ; sur des lexiques comprenant jusqu'à 200 mots.
Le taux de reconnaissance mono-locuteur est proche de 100% et le taux de reconnaissance multi-locuteurs varie entre 90 36 99%.
Notre but est d'obtenir des taux de succès de dialogue élevés avec une interaction très ouverte, offrant à l'utilisateur la possibilité de poser des questions ou de donner des informations à n'importe quel moment.
Afin d'améliorer les performances avec une telle stratégie, nous avons utilisé la confirmation implicite en fonction des formulations des utilisateurs ainsi qu'un dialogue plus contraint lorsque l'interaction se passe mal.
Dans le codage de parole à bas débit binaire à base de source-filtres, une représentation efficace des impulsions d'excitation est nécessaire pour obtenir une bonne qualité de parole synthétisée.
Dans cet article, nous traitons d'une représentation en forme d'onde des impulsions par des dictionnaires composés de formes d'impulsions.
Les dictionnaires sont construits à partir de dérivées d'impulsions glottales obtenues par une technique de filtrage inverse à prédiction linéaire.
Les impulsions sont extraites et normalisées en durée et amplitude pour former des d'impulsions typiques.
Les méthodes de construction et l'évaluation des performances des dictionnaires sont traitées dans le cadre d'une quantification vectorielle (QV).
Les gains de quantification obtenus en exploitant la corrélation entre impulsions sont étudiés de manière théorique, ce qui montre qu'on peut épargner 2 bits par vecteur (d'un budget de 7–10 bits) en exploitant la corrélation.
La QV à mémoire est un schéma de quantification qui utilise les impulsions précédemment quantifiées.
Nous étudions deux méthodes traditionnelles de QV à mémoire ainsi qu'une extension de QV à mémoire par une QV sans mémoire, appelée filet de sécurité.
Les tests montrent que les performances s'améliorent quand on adjoint un filet de sécurité.
Nous avons trouvé qu'aux débits binaires demandés, la QV à mémoire utilisant un filet de sécurité peut gagner jusqu'à 1.5–2 bits par rapport à la QV sans mémoire.
La recherche des images par le contenu dans des grandes bases généralistes d'images est une tâche fastidieuse vu le taux d'hétérogénéité assez élevé même au sein d'une seule classe de la base ainsi que la grande dimension de l'espace des descripteurs relatifs aux images.
Pour cela, nous proposons de guider la recherche par des descripteurs de bas-niveau de taille réduite à base de sous-bandes d'ondelettes relatives aux régions les plus significatives dans chaque image après une phase de segmentation floue.
De plus, nous proposons une technique de bouclage de pertinence négatif sur les poids relatifs aux régions.
Les expérimentations et l'étude comparative avec des approches similaires prouvent la robustesse de l'approche proposée en termes d'apport sémantique offert par l'utilisation des sous-bandes d'ondelettes ainsi que par le bouclage de pertinence négatif.
Dans cet article nous proposons une méthode de segmentation d'images couleur selon une nouvelle approche que nous appelons bi-marginale.
Afin de pallier les défauts des approches marginales classiques, nous considérons les composantes couleur deux à deux afin d'avoir une vue partielle de leur corrélation.
Travaillant selon cette vision bi-composante, nous considérons les trois combinaisons possible comme trois sources d'informations indépendantes.
Chaque information bi-composante est tout d'abord analysée selon un schéma de coalescence morphologique non supervisé qui recherche les couleurs dominantes d'un histogramme bidimensionnel.
Cela permet de construire trois cartes de segmentation distinctes qui sont combinées par intersection après avoir été simplifiées.
L'intersection produisant une sur-segmentation, une fusion des régions deux à deux est opérée selon un critère de similarité et selon la combinaison de Dempster-Shafer jusqu'à un critère de terminaison.
Le terme analogie couvre une confédération complexe de concepts liés entre eux et à d'autres concepts comme « métaphore » et « expression » .
Dans cet article, nous proposons d'analyser d'une part, les différentes acceptions du terme analogie, et d'autre part, les relations entre l'analogie et l'expression.
En ce qui concerne la relation avec l'expression métaphorique, les analogies régressives motivent la création de métaphores conventionnelles dont l'expression se réduit à un rôle instrumental, alors que les analogies projectives sont rendues possibles par l'expression et indissociables d'elle, et ne peuvent être conçues que comme le but d'interprétations créatrices de contenus complexes conflictuels.
La reconnaissance automatique de scénarios est cruciale pour l'aide à la supervision de systèmes dynamiques.
La construction de tels scénarios représentatifs de situations de bon ou de mauvais fonctionnement n'est pas réalisable à partir de la connaissance experte.
Notre objectif est donc d'extraire, à partir des données et des informations disponibles, des séquences d'événements pertinents et ensuite de construire des scénarios validés par les experts.
Nous présentons notre méthodologie d'abstraction des données qui permettra au fur et à mesure de construire des séquences de granularité croissante, puis des scénarios.
Au fil des années, la controverse s'est étendue sur les mérites relatifs des liaisons en cascade ou en parallèle des générateurs de formants dans la réalisation d'un synthétiseur de parole.
Ce rapport montre que la connexion en parallèle, théoriquement moins satisfaisante, permet de produire de meilleures approximations des propriétés du signal de parole que ce qui est générlement obtenu à partir d'un synthétiseur en cascade, et ce, à a fois pour les voyelles et pour les consonnes.
Cependant, pour atteindre ce résultat, il est nécessaire de prendre garde aux caractéristiques de phase des générateurs de formants et de modeler de manère appropriée les pentes des courbes de réponses formantiques.
Bien que dans le cas d'un synthétiseur en parallèle de l'information supplémentaire sur l'amplitude soit nécessaire, le mode de spécification acoustique est en conséquence plus directement relié aux proprétes de la parole humaine telles qu'elles peuvent être aisément measurées á partir d'une représentation spectrale.
Cette méthode est basée sur la normalité asymptotique d'une fonction non-linéaire des coefficients de détail et d'échelle de la transformée de Haar, appelée la transformée de Fisz.
Nous exposons quelques résultats asymptotiques, tels que la normalité et la décorrélation des pixels de l'image transformée.
Fort de ces résultats, l'image originale bruitée par un processus de Poisson, peut être considérée après transformation de Fisz comme étant contaminée par un bruit additif gaussien et blanc.
Ainsi, les débruiteurs classiques s'appliquent directement.
Elle dépasse clairement ces approches lorsque le taux de comptage est faible.
Combiner la transformation de Fisz avec le débruiteur bayésien FKB offre les meilleurs résultats.
Cet article décrit une structure de codeur prédictif excité par codes pour des applications en transmission et clarifie les relations existant entre celle-ci et d'autres codeurs prédictifs ainsi que la quantification vectorielle.
Pour un débit de 1 bit par échantillon on spécifie un dictionnaire constitué de 2 L formes d'onde de L échantillons à valeurs dans {t-1, +1}.
Chaque forme d'onde est déduite de l'index binaire dans le dictionnaire par une correspondance bi-univoque entre chaque bit de l'index et chaque échantillon de la forme d'onde.
Par construction un tel dictionnaire possède une robustesse intrinsèque vis à vis des erreurs de transmission et sa structure algébrique interne conduit à des algorithmes très rapides de sélection de la forme d'onde optimale.
Les résultats objectifs et subjectifs confirment le haut niveau des performances obtenues par le codeur CELP à 16 kbit/s dans différentes conditions de transmission réalistes telles que la transmission en présence d'erreurs et de bruit ambiant.
Pouvoir synthétiser des voix pathologiques peut être un outil utile pour le développement de protocoles standards pour l'évaluation de la qualité vocale.
On a appliqué une méthode d'analyse-synthèse, utilisant le synthétiseur de Klatt, à 24 énoncés de la voyelle /a/ prononcés par des locuteurs hommes et femmes ayant des troubles de la parole allant de modéré à sévère.
Les indices tant spectraux que temporels des formes d'onde naturelles ont été analysés et les résultats utilisés pour guider la synthèse.
Une évaluation perceptive montre qu'environ la moitié des voix synthétiques sont considérées comme identiques en qualité aux voix naturelles qu'elles sont sensées modéliser.
Les stimuli considérés comme différents reflètent les échecs de la modélisation de voix très instables ou “gargarisées” ou d'imperfection dans la reproduction des spectres naturels.
Diverses modifications apportées au synthétiseur de Klatt pourraient améliorer la synthèse des voix pathologiques.
Ces modifications concernent l'introduction de paramètres de jitter et de shimmer ; la mise à jour des paramètres de synthèse en fonction de la période plutôt qu'en fonction d'une durée absolue ; la modélisation de la diplophonie par des paramètres indépendants pour les variations de fréquence fondamentale et d'amplitude ; la disponibilité d'un paramètre permettant d'augmenter l'énergie dans les basses fréquences ; et l'ajoût de plus de paires pôle-zéro.
Les systèmes d'inférence floue peuvent avoir une place importante dans un processus de modélisation, quand l'intégration de données et d'expertise est nécessaire.
L'objectif de cet article est de donner des lignes directrices pour ce type de modélisation, basées sur notre expérience pratique dans les domaines de l'agronomie et de l'environnement.
Nous discutons les points originaux de ces systèmes, leur capacité à intégrer expertise et données dans un cadre commun, ainsi que leur place par rapport à d'autres modèles.
L'implémen- tation dans le logiciel open source FisPro est également présentée et deux cas d'étude illustrent l'approche.
Satisfaire la propriété de l'inégalité triangulaire (IT) par les mesures de dissimilarité de la méthode DTW (“dynamic time warping”) semble être une condition préalable importante à l'application des nouvelles méthodes de réduction du nombre de calculs pour la reconnaissance automatique de mots isolés.
Le degré de satisfaction de cette propriété pour des échantillons de parole naturelle a été étudié empiriquement par plusieurs auteurs.
Les résultats sont peu concluants quant aux différentes méthodes DTW employées et aux données expérimentales utilisées.
Au cours des expériences intensives que nous avons menées, la propriété IT a toujours été satisfaite quelles que soient les nombreuses combinaisons de paramètres acoustiques, les méthodes DTW, métriques locales et les signaux de parole utilisés.
Cette constatation a motivé une étude préliminaire des causes possibles de la violation de l'inégalité triangulaire constatée ailleurs.
Les résultats suggèrent que l'usage de certaines méthodes de compression temporelles ainsi que de méthodes DTW sous-optimales sont à l'origine de la non-satisfaction de la propriété IT.
On désigne par microperturbations de la période fondamentale les légères fluctuations des durée des cycles glottiques.
Afin d'analyser les microperturbations, il est nécessaire de mesurer la durée des cycles glottiques avec une très grande précision.
En général, on y arrive en échantillonnant le signal de parole à une cadence moyenne et en interpolant le signal numérisé.
Dans cet article nous décrivons un algorithme dédié à la mesure précise des microperturbations et qui propose une solution aux deux problèmes de traitement du signal suivants.
Premièrement, les échantillons qui sont obtenus par interpolation ne sont que des estimations des échantillons d'origine qui sont inconnus.
Le problème est d'évaluer la qualité de la reconstruction du signal.
Deuxièmement, les petites variations dans les durées des cycles glottiques sont facilement biaisées par du bruit et des erreurs de mesure.
Ici, le problème est d'estimer l'impact des erreurs de mesure sur le résultat final.
Dans notre algorithme, la qualité de l'interpolation est évaluée à l'aide d'un test statistique qui tient compte de la distribution des corrections (dues à l'interpolation) des positions des événements qui marquent le début des périodes.
Trois méthodes d'interpolation différentes ont été implantées.
Les erreurs de mesure sont contrôlées en estimant indépendamment les durées des cycles glottiques à partir du signal de parole et à partir du laryngogramme.
Si les deux séries coïncident, alors on peut conclure qu'elles reflètent l'activité des cordes vocales et qu'elles n'ont pas été biaisées exagérément par des erreurs de mesure ou du bruit.
L'algorithme a été vérifié sur 77 signaux produits par des locuteurs sains et dysphoniques.
Ses performances sont très satisfaisantes en général.
Le comportement de la source vocale dans la parole continue a été examiné.
Des paramètres de la source vocale ont été obtenus à l'aide d'un filtrage inverse automatique, suivi par un appariement automatique d'un modèle d'onde de débit glottique aux observations.
Des relations consistantes entre les paramètres de la source vocale et des traits prosodiques ont été observées.
Cet article a un double objet : examiner les méthodes de codage de la dernière décennie dans la gamme des 4.8–9.6 kbps et discuter les courants les plus récents de la recherche.
L'essentiel de l'article est consacré au codage CELP, méthode obligée à la base de plusieurs standards en cours d'émergence.
Suit une brève revue d'une méthode alternative de modélisation sinusoïdale.
Une comparaison de ces techniques opposées permet d'identifier les futurs domaines de la recherche.
Dans cet article, nous décrivons une technique par laquelle une fonction, comportant des pôles et des zéros, peut être ajustée à la réponse en fréquence d'un système en résolvant un système d'équations simultanées.
L'ordre de la fonction est défini par le double du nombre de maxima spectraux de la réponse en fréquence.
La fonction est contrainte à ajuster les maxima et minima de la courbe de réponse.
La procédure d'ajustement permet de faire varier la forme des vallées entre les maxima à l'aide d'un seul paramètre.
Le problème que pose l'obtention d'une bonne approximation de la réponse en fréquence du conduit vocal à partir du spectre à court terme de la parole est résolu à l'aide d'une méthode de lissage spectral.
La structure harmonique du spectre à court terme fourni par un banc de filtres digitaux est éliminée en tronquant le cepstre.
A titre d'exemple, la procédure est appliquée à une voyelle synthétique. (Une indication quant au degré d'approximation propre à la procédure est ainsi obtenue).
Nous proposons, dans cet article, d'améliorer la classification d'images, en utilisant une approche de classification visuo-textuelle (à base de caractéristiques visuelles et textuelles), et en étendant automatiquement les annotations existantes aux images non annotées.
L'approche proposée est dérivée de la théorie des modèles graphiques probabilistes et dédiée aux deux tâches de classification et d'annotation d'images partiellement annotées.
Nous considérons une image comme partiellement annotée si elle ne possède pas le nombre maximal de mots-clés disponibles par image dans la vérité-terrain.
Grâce à leur capacité à fonctionner en présence de données manquantes, un modèle graphique probabiliste a été proposé pour représenter les images partiellement annotées.
Ce modèle est basé sur un mélange de lois multinomiales et de mélanges de Gaussiennes.
La distribution des caractéristiques visuelles est estimée par des mélanges de Gaussiennes et celle des mots-clés par une loi multinomiale.
Par conséquent, le modèle proposé ne requiert pas que toutes les images soient annotées : lorsqu'une image est partiellement annotées, les mots-clés manquants sont considérés comme des valeurs manquantes.
De plus, notre modèle peut automatiquement étendre des annotations existantes à des images partiellement annotées, sans l'intervention de l'utilisateur.
L'incertitude autour de l'association entre un ensemble de mots-clés et une image est capturée par une distribution de probabilité jointe (définie par un mélange de lois multinomiales et de mélanges de Gaussiennes) sur le dictionnaire de mots-clés et les caractéristiques visuelles extraites de notre collection d'images.
De plus, de façon à résoudre le problème de dimensionnalité dû à la grande dimension des caractéristiques visuelles, nous avons adapté une méthode de sélection de variables.
Les résultats de la classification visuo-textuelle, obtenus sur une base d'images collectées sur Internet, partiellement et manuellement annotée, montrent une amélioration de 32.3 % en terme de taux de reconnaissance, par rapport à la classification basée sur l'information visuelle uniquement.
Par ailleurs, l'extension automatique d'annotations, avec notre modèle, sur des images avec mots-clés manquants, améliore encore la classification visuo-textuelle de 6.8 %.
Enfin, la méthode proposée s'est montrée compétitive avec des classificateurs de l'état de l'art.
Dans cet article est proposée une nouvelle méthode de compression numérique d'image.
Dans une première étape l'image est décomposée en sous-images au moyen d'une transformée en ondelettes.
Les ondelettes choisies sont à la fois bien localisées dans l'espace et en fréquence, et privilégient les directions horizontale et verticale, ce qui permet de respecter les caractéristiques de la vision humaine.
La décomposition est effectuée par un algorithme Tapide à structure pyramidale.
Ensuite les coefficients d'ondelette sont codés par une méthode de quantification vectorielle.
Un codebook est élaboré pour chaque résolution et direction privilégiée à partir d'une séquence d'apprentissage en utilisant un critère quadratique.
Ainsi un vecteur à coder est classifié (direction, résolution) puis la seule sous-classe appropriée est scrutée.
Les réponses aux remarques formulées par L.J. Boë et P. Perrier sont organisées selon trois classes d'aspects : acoustiques, articulatoires et phonétiques.
Au préalable, une présentation synthétique met en évidence les traits caract́eristiques du modèle en vue d'éviter des incompréhensions sur a nature.
Dans cet article, nous faisons le point sur les possibilités offertes par les modèles hybrides harmonique/stochastique (H/S) dans le cadre de la synthèse vocale à large bande par concaténation.
Après un bref rappel des hypothèses sous-jacentes et d'un algorithme d'analyse bien connu, à savoir celui fourni dans le cadre de l'excitation multi-bandes (MBE), nous étudions comment les modèles H/S permettent de modifier la prosodie des segments et comment leur concaténation peut être organisée, dans le but de minimiser les incohérences au droit des frontières de segments.
Nous proposons à cette occasion une méthode originale qui tire parti de certaines erreurs d'analyse.
Nous examinons ensuite les algorithmes de synthèse, introduisons une méthode originale basée sur le calcul d'IFFTs judicieusement choisies, et détaillons la qualité segmentale du résultat.
Nous insistons plus particulièrement sur les différences entre la qualité obtenue avec ce modèle dans le cadre du codage à bande étroite et dans le cadre de la synthèse à large bande par concaténation.
Nous étudions trois explications possibles : le choix du critère d'analyse, l'inadéquation du modèle aux variations de pitch, et l'effet de la coarticulation sur les phases.
Les systèmes de recherche d'information renvoient généralement une liste ordonnée de documents, où seuls le titre et parfois un extrait comportant les mots de la requête permettent d'en évaluer la pertinence pour son besoin initial.
Ces types de résultat conduisent toujours à devoir consulter de nombreux documents pour réellement trouver des réponses pertinentes.
Afin d'éviter cet écueil nous nous sommes intéressés à la visualisation d'un texte : que doit-on montrer et comment ?
Dans notre système, RÉGAL (RÉsumé Guidé par les Attentes du Lecteur), les informations nécessaires à la visualisation sont extraites automatiquement des textes par l'application d'une analyse thématique, sans présupposer l'existence d'une structure préalable ou d'un formatage du texte, et en combinant des approches fondées sur la cohésion lexicale et le repérage de marques clés.
Nous proposons une méthode permettant de générer automatiquement un dictionnaire de prononciations à l'aide d'un réseau de neurones qui prédit les prononciations les plus plausibles (variantes) à partir d'une prononciation standard.
Le réseau de prononciation peut générer de multiples variantes de prononciation.
Pour générer un dictionnaire de variantes de prononciation sophistiqué, deux techniques sont décrites : (1) variantes de prononciation avec valeur de vraisemblance, et (2) variantes de prononciation pour les phonèmes aux frontières des mots.
Les résultats de nos expériences sur de la parole spontanée montrent que les dictionnaires de prononciation dérivés automatiquement permettent d'améliorer de façon significative le taux de reconnaissance, par rapport à un dictionnaire conventionnel.
Cet article concerne l'utilisation d'un système commercial de reconnaissance de parole à grande vocabulaire par une équipe d'utilisateurs au cours de leur travail quotidien.
Il décrit en particulier son utilisation par des traducteurs utilisant quatre langues dans un milieu multilingue au sein de la Commission Européenne.
L'article évoque tout d'abord quelques différences entre le point de vue d'un utilisateur courant typique et celui d'un chercheur en reconnaissance de la parole.
Il montre que certaines barrières psychologiques doivent être surmontées pour que la reconnaissance de la parole soit largement acceptée et il conclut que cette acceptation dépendra au moins autant du partage d'expérience entre utilisateurs que des avancées techniques.
Les résultats globaux des expériences menées au sein de la Commission Européenne ont été positifs et encourageants, mais plusieurs problèmes inattendus ont été rencontrés, pour la plupart liés au milieu multilingue.
Cet article décrit comment la plupart de ces problèmes ont été traités.
Cet article décrit le projet de technologie vocale SPELL (Système interactif pour l'apprentissage des langues parlées européennes) ; son principal but est de définir un système automatique d'amélioration de la prononciation des langues étrangères pour les étudiants de l'anglais, du français et de l'italien.
La phase d'étude de faisabilité a été conclue ; il en est issu un prototype comprenant des modules d'enseignement pour l'intonation, le rythme et la prononciation des voyelles.
Cet article décrit les méthodes de traitement du signal employées, les mesures de ressemblance et les interfaces utilisateur qui ont été intégrées pour réaliser le système de démonstration de base.
Une évaluation préliminaire, par un groupe de professionnels de l'enseignement des langues, montre que SPELL est un outil adapté pour explorer l'enseignement automatique de la prononciation.
La littérature en moyen français est dominée par l'imaginaire juridique.
La présence de ce thème dans les Ballades de Charles d'Orléans reflète-t-elle seulement la mode contemporaine ?
Les images de la loi donnent au recueil un cadre, de la Retenue à la Departie d'Amour.
En transformant la traditionnelle loyauté courtoise en législation amoureuse, le poète met en valeur la communication particulière avec sa dame.
L'amour à distance doit se dire à travers des écritures qui sont autant d'actes, engageant les deux amants à respecter le désir comme une loi.
Les Ballades sont également un univers peuplé de juristes.
Le sujet lyrique, comme les allégories avec lesquelles il dialogue, est juge, procureur, avocat.
Mais si les lieux qui ouvrent le recueil sont des espaces de législation, ils deviennent, lorsque le poète s'égare, des tribunaux où les procédures judiciaires n'aboutissent jamais.
La fiction juridique transforme enfin le dialogue avec d'autres poètes en un jeu dont le duc est à la fois juge et partie.
L'objet de cet article est une étude comparée des performances de ces méthodes pondérées en vue de leur application à court terme synchronisée à la fondamentale de brefs segments voisés non nasalisés (durée de l'intervalle d'analyse plus courte qu'une période).
Les erreurs dans l'estimation du spectre de puissance, des fréquences formantiques et des largeurs de bande sont utilisées comme critère de performance.
Nous montrons que la méthode de Burg pondérée proposée par Kaveh et Lippert est la plus performante.
Les résultats d'une comparaison de ces méthodes avec les méthodes d'autocorrélation et de covariance sont également discutés.
Modeling user behavior on computer interface is a very complex task as data, usually manipulated by cognitive psychologist, are very abundant and few methods for visualizing them for seeking purpose exist.
Cet article présente une théorie et une implémentation informatique de la génération d'une intonation appropriée pour la parole de synthèse, dans un contexte de réponse à une interrogation de bases de données.
Les distinctions spécifiques de contraste et d'emphase sont exprimées sous la forme de contours prosodiques qui sont synthétisés par règles sous le contrôle d'une grammaire, d'un modèle du discours et d'une base de connaissance.
La théorie est basée sur la Grammaire Catégorielle Combinatoire, un formalisme qui permet d'intégrer aisément les notions de constituant syntaxique, de sémantique, de structure prosodique et de structure inormationnelle.
Les résultats de l'implémentation actuelle montrent que le système est capable de générer diverses intonations pour une même phrase, suivant le contexte du discours.
De nombreux systèmes de traduction texte-parole sont actuellement en développment en Europe et dans le reste du monde. Nous discuterons en particulier du système en développement au Centre for Speech Technology Research (CSTR) de l'université d'Edinbourg. En général, ces systèmes génèrent les propriétés intonatives de phrases synthétiques sur base d'une représentation phonologique intermédiaire abstraite des traits prosodiques qui est indépendente de la réalisation acoustique.
L'accent tonique et la conversion graphème-phonème est évaluée plus facilement à partir d'une représentation symbolique qu'à partir de la sortie acoustique. De même, la représentation abstraite est plus appropriée que la sortie acoustique du système pour l'évaluation de certains aspects de la prosodic synthétique (notamment le placement de l'accent et la division en domaines).
Comme illustration, nous présentons les résultats d'un exercice d'évaluation effectué sur les règles de l'accent de phrase du système CSTR. Le choix d'une représentation abstraite a été utile pour l'amérioration de nos règles.
Dans cet article, nous proposons une méthodologie originale permettant la détection et la reconnaissance de caractères multi-orientés et multi-échelles.
Les supports sur lesquels la méthode est appliquée sont des documents techniques représentant le réseau de l'opérateur de télécommunication français France Télécom.
La technique adoptée, basée sur la transformation de Fourier-Mellin (TFM), est intégrée dans une stratégie globale permettant la résolution de situations ambiguës, par intégration d'informations contextuelles.
La stratégie appliquée pour résoudre ce problème de reconnaissance de caractères et symboles multi-orientés et multi-échelles peut être divisée en deux étapes.
La première réside dans l'extraction d'un ensemble de descripteurs invariants pour chacune des formes isolées de la couche « caractères » identifiée à partir d'un extracteur de composantes connexes.
La seconde étape, basée sur un processus de filtrage, consiste à détecter et reconnaître les formes connectées au réseau ou à d'autres formes.
Les résultats de l'application de cette technique sont très encourageants puisque le taux de classification atteint d'excellents niveaux en comparaison avec les techniques classiques de la littérature.
Dans cet article, nous présentons une méthode pour la résolution du problème d'estimation des angles d'arrivée de D cibles, par un réseau de L capteurs, où D < L, pour les systèmes actifs émettant des signaux codés en « Frequency Hopped » .
Cette méthode est basée sur l'application de l'estimateur du maximum de vraisemblance à un nouveau modèle de données reçues sur différents canaux.
Les résultats de simulation montrent que cette approche améliore la résolution des angles d'arrivée des cibles, comparativement à celle de la fréquence monotone.
Cependant, quand le rapport signal sur bruit (Signal to Noise Ratio, SNR) est faible, la performance se dégrade et nécessite donc un nombre d'échantillons plus élevé.
Avant qu'existe une nouvelle génération de traitement du langage naturel qui puisse fonctionner der façon satisfaisante dans les conditions réelles actuelles, des techniques d'interfaçage seront nécessaires pour faire en sorte que le langage de l'utilissateur puisse coïncider avec les capacités des systémes actuels.
Dans cet article, on examine comment la modalité d'entrée et la structure de la présentation influent sur la complexité linguistique des énoncés écrits et oraux à l'entrée d'un systéme interactif.
En utilisant une technique de simulation semi-automatique, des énoncés ont été collectés lors d'échanges soit entiérement oraux, soit entiérement écrits, soit combinant écrit et voix et utilisant des formats de présentation ou bien structurés, ou bien non contraints.
Les résultats indiquent que tant la modalité que le format de présentation ont une influence déterminante sur la complexité linguistique, bien que les natures de leur impact soient différentes.
On présente une analyse approfondie de la façon dont ces deux facteurs jouent sur le nombre total de mots, les hésitations, la longuer des énoncés, la variabilité lexicale, la perplexité, l'ambiguïté syntaxique et l'intégration sémantique.
Les préférences des utilisateurs en termes de formats et de modalité sont aussi analysées et l'on discute certaines implications pour le contrôle transparent du langage de l'utilisateur.
Le but à long-terme de cette recherche est de développer des techniques d'interfaçage permettant de gérer les sources de grande variabilité dans le langage humain de façon à réaliser des technologies de traitement du langage natural qui soient robustes.
L'objectif de nos travaux est de comparer et fusionner les points de vue de paysans malgaches sur leur territoire.
Notre démarche de modélisation utilise le modèle des graphes conceptuels afin de représenter les connaissances spatiales exprimées par les paysans, etl'ana- lyse formelle de concepts pour organiser la fusion de ces représentations.
L'interprétation des résultats s'appuie sur une mesure d'intérêt évaluant les concepts formels obtenus après fusion et sur une expertise du terrain.
Nous montrons la pertinence des concepts formels retenus pour analyser la façon dont les paysans organisent leur espace limité par la conservation de la forêt.
Dans le but d'améliorer les connaissances sur les relations articulatori-acoustiques, d'importantes quantités de données sont nécessaires.
L'objectif de ce travail a donc été double : (1) obtenir pour un même sujet, un ensemble cohérent de fonctions sagittales, fonctions d'aire et formants sur un nombre réduit de voyelles et consonnes fricatives, et (2) déterminer un modèle de passage de la fonction sagittale à la fonction d'aire valable pour ce même sujet.
Des vues radiographiques du conduit vocal et le son associé sont disponibles pour ce locuteur ainsi que des vues de face des lèvres et des moulages du palais dur.
Le modèle utilisé est une extension du modèle de fonction d'aire A = αd β de Heinz et Stevens modifié pour que α dépende continûment de la position le long du conduit et de la distance sagittale.
Les coefficients du modèle ont été déterminés par un algorithme d'optimisation fondé sur une technique de descente de gradient.
Le gradient de l'erreur entre les formants désirés et les formants obtenus est déterminé à l'aide d'un réseau à rétro-propagation incluant le passage de la fonction sagittale à la fonction d'aire ainsi que la propagation des ondes acoustiques.
Le fait que le modèle soit déterminé pour des configurations aussi différentes que des voyelles et des consonnes et qu'il soit cohérent aussi bien sur le plan sagittal qu'acoustique devrait assurer que les fonctions d'aire obtenues soient relativement proches de la réalité.
Dans cet article, on considère l'apprentissage des Modèles de Markov cachés (HMM) comme un problème général d'optimisation sous contraintes linéaires.
On propose une méthode de projection de gradient, utilisée en programmation non linéaire sous contraintes linéaires, pour trouver les valeurs “optimales” des paramètres du modèle.
En appliquant cette méthode classique à l'apprentissage des HMM (dont les densités sont soit des mélanges de densités gaussiennes, soit des densités discrètes), on peut dériver une formulation simple grâce à la structure particulière des contraintes sur les paramètres.
On peut alors utiliser une méthode apparentée à l'optimisation classique par descente de gradient pour modéliser de manière plus souple le signal de parole, et fournir un apprentissage plus élaboré des paramètres des modèles de reconnaissance de la parole.
Dans cet article, nous nous intéressons au problème de la segmentation en locuteurs, étape préliminaire nécessaire à plusieurs tâches d'indexation.
Le but de la segmentation en locuteurs est d'extraire des segments homogènes ne contenant les paroles que d'un seul locuteur et aussi longs que possible.
Dans notre contexte, nous faisons l'hypothèse qu'aucune connaissance a priori des locuteurs ou des caractéristiques du signal n'est à notre disposition (pas de modèle de locuteur, pas de modèle de parole).
Nous supposons néanmoins que les personnes ne parlent pas simultanément et que nous n'avons pas de contrainte de temps réel.
Nous présentons les techniques de segmentation existantes et nous proposons une nouvelle méthode qui combine les avantages de deux techniques de segmentation.
Cette nouvelle méthode de segmentation, appelée DISTBIC, s'opère en deux passes : les changements de locuteurs les plus probables sont tout d'abord détectés et ils sont ensuite validés ou annulés au cours de la deuxième passe.
L'avantage de notre algorithme est son efficacité à détecter des changements de locuteurs proches les uns des autres (i.e. espacés de quelques secondes).
Dans les systèmes de reconnaissance de la parole fondés sur les segments, deux classes d'algorithmes de reconnaissance peuvent être distinguées.
La première classe contient les systèmes s'appuyant sur le calcul de la densité de probabilité conditionnelle de la séquence de vecteurs acoustiques, étant données la séquence d'unités de parole et la segmentation.
La seconde contient ceux qui s'appuient sur le calcul de la distribution a posteriori conjointe d'une séquence d'unités de parole et d'une segmentation possible, étant donnée la séquence de vecteurs acoustiques.
Pour les systèmes de cette seconde classe, la distribution a posteriori conjointe peut être représentée sous forme du produit de la probabilité de segmentation et de la probabilité de séquence d'unités.
Dans cet article, nous mettons en évidence le rôle joué par la probabilité de segmentation.
Après avoir montré que la probabilité de segmentation n'est pas nécessaire dans les systèmes s'appuyant sur le calcul de la densité de probabilité conditionnelle, nous présentons les motivations de l'utilisation de la probabilité de segmentation dans les systèmes s'appuyant sur le calcul de la distribution a posteriori.
Nous décrivons la modélisation et l'apprentissage de la probabilité de segmentation dans le cadre de ces systèmes.
Nous présentons des expériences menées sur plusieurs tâches de reconnaissance et avec deux systèmes de reconnaissance différents et s'appuyant sur le calcul de la distribution a posteriori.
Nous montrons finalement que l'importance de la segmentation est fortement liée aux valeurs des probabilités des unités sur les segments qui ne correspondent pas avec une unité.
Cet article traite de l'apprentissage par analogie dans l'univers des séquences, fondé sur la résolution d'équations analogiques.
Il présente une définition de la relation d'analogie entre séquences à partir de la distance d'édition et étudie la résolution d'une équation analogique sur les séquences.
Il donne un système de transducteurs à états finis pour calculer les solutions de cette équation, qui ramène également le problème à celui de l'analogie sur un alphabet fini.
Il étudie aussi l'analogie sur les alphabets finis et examine deux structures algébriques compatibles avec le calcul des solutions sur les séquences.
Pour finir, il présente un algorithme sous-optimal direct pour calculer une solution à une équation analogique sur les séquences.
Cet article présente un système complet de synthèse de la parole à partir du texte pour la langue Grecque, développé à WCL, qui est basé sur la synthèse de formants avec une excitation synchronisée sur le pitch (PSE).
Le système utilise une base de données de 140 segments de parole du type Consonne (C), Voyelle (V), CV et CCV.
On examine en détail la méthode de concaténation des segments, et la gestion de leur durée en fonction du contexte et des règles de co-articulation formulées pour le Grec.
Le synthétiseur à formants est implementé sur une carte DSP32C.
L'objet de cet article est l'étude de la relation entre la notion de complémentation verbale et celle de valence verbale en français moderne.
La construction trivalente, dans laquelle un verbe a un sujet et deux compléments d'objet (direct et/ou indirect), est au centre de la réflexion.
Une étude des verbes trivalents dans un corpus extrait d'un exemplaire du journal Le Monde permet d'établir la distinction entre trivalence théorique d'un verbe et trivalence actualisée dans un contexte donné.
Dans cet article, nous étudions le problème de l'authentification de signatures à partir des paramètres temporels de l'écriture.
Nous proposons ici une méthode qui, après normalisation des signaux, extrait des mesures de dissimilitude entre signatures.
Ces mesures sont ensuite traitées par un réseau de neurones.
Comme nous ne disposons pas toujours d'imitations de signatures, le problème se présente comme un problème à une classe statistique et l'utilisation d'algorithmes neuronaux classiques nous est donc interdite.
Pour pallier ce type d'inconvénient, nous présentons également un nouvel algorithme d'apprentissage de réseau de neurones.
Un analyseur grammatical pour la parole continue traite des treillis lexicaux où les mots hypothèses des phrases correctes ne sont généralement pas alignés de façon parfaite, et où les mots fonctionnels courts peuvent manquer.
Afin de résoudre ces problèmes, nous avons mis au point une intéraction bidirectionnelle entre le module de reconnaissance et l'analyseur grammatical.
La solution possédant le meilleur score est choisie par l'analyseur grammatical.
Nous présentons des résultats obtenus pour des essais de reconnaissance de parole continue indépendant du locuteur à travers le téléphone.
Les changements environnementaux brutaux et massifs, qui affectent généralement de grandes surfaces, doivent être localisés le plus rapidement possible pour gérer l'impact immédiat de ce type d'événements sur les écosystèmes et prévenir les risques associes.
Il est donc nécessaire de développer des méthodes permettant d'établir efficacement une carte des changements.
Dans cette optique, une approche region quasi non supervisee de detection de changements sur des images satellite a haute résolution spatiale est proposée.
Un procédé innovant de selection automatique d'attributs, inspire des procédures de calibrage, optimise la segmentation et la classification.
Un nouveau descripteur spatio-temporel, base sur le taux de fragmentation des régions détectées, permet alors de réaliser une classification binaire des changements en zones intactes et altérées.
Cette méthode paisse par des étapes de segmentation et de classification mean shift.
L'approche a été évaluée en milieu forestier sur un couple d'images satellite multispectrales Formosat-2 acquises avant et après une tempête majeure pour reconnaêtre et cartographier les dégats.
Cet article étudie le problème de la traduction de ('information visuelle en de l'information utilisable par la faculté de langage, c'est-à-dire la question de savoir comment il est possible de parler de ce que l'on voit.
Notre hypothèse est qu'il existe une traduction entre le modèle 3D de la théorie de la vision de Marr's (1982) et la structure sémantique/conceptuelle de la théorie sémantique de Jackendoff's (1983).
Nous montrons qu'il existe de nombreuses correspondences par lesquelles le codage des objets physiques et de leur emplacement et déplacement peut être coordonné entre ces deux niveaux de représentation, et que ces deux niveaux jouent un rl̂e fondamental dans la compréhension aussi bien visuelle que linguistique.
Une étude sur la perception des consonnes, par lecture labiale seule, ainsi que par lecture labiale et par dispositif vibro-tactile, a mis en évidence une amélioration de la perception du voisement et du lieu d'articulation sous la deuxiéme condition.
Les résultats varient d'un patient à l'autreslors de tests portant sur des mots et des phrases.
Un patient, sourd de naissance, n'ayant jamais utilisé un appareil auditif, n'a pas amélioré ses performance, tandis qu'un autre ayant toujours fait bon usage d'un appareil auditif a réalisé de meilleures performances pour les deux sortes de matériaux.
Au niveau du discours continu, deux sujets ont démontré des améliorations par opposition à un troisième qui n'avait eu qu'une expérience limitée avec un appareil auditif.
L'interpretetation des résultats, ainsi que les recherches futures sont discutées.
Le traitement de formes non grammaticales dans lesquelles les contraintes sur les règles de mouvement n'ont pas été respectées a été étudié avec un tâche d'appariement.
Curieusement, il n'y a effet des ces formes ni dans le cas de non-respect de la Contrainte du Sujet Spécifié ni dans celui de non respect de la Contrainte de Sous-Jacence.
Ces phrases “sur-engendrées” peuvent être traitées aussi facilement que des phrases grammaticales contrôles alors que d'autres types de non-grammaticalité induisent des temps d'appariement plus grands.
Il est possible que la tâche d'appariement fasse appel à un niveau de représentation mentale où les phrases surengendrées ne sont pas distinguées des phrases grammaticales.
Ceci implique une correspondence étroite entre les mécanismes de dérivation formelle et des aspects de l'opération du processeur du langage.
Les modèles d'apparence permettent d'encoder les variabilités de forme, de pose, et d'illumination dans une seule représentation compacte.
Le modèle d'apparence probabiliste de Moghaddam et al. (Moghaddam and Pentland, 1997 ; Tipping and Bishop, 1997b), reposant sur une interprétation statistique de l'Analyse en Composantes Principales (ACP) s'est récemment illustré par ses excellentes performances en détection et en reconnaissance des formes, surpassant de nombreuses autres méthodes linéaires et non linéaires.
Ce modèle, performant, se heurte toutefois à une complexité calculatoire importante.
Nous proposons, dans cet article, une approximation de ce modèle qui se prête à une mise en œuvre rapide, dans le cadre de schémas d'estimation statistique.
Des gains en complexité et en temps de calcul supérieurs à 10, sont obtenus, sans aucune perte de qualité dans les résultats des traitements.
Comme mentionné dans l'article récent de Bourlard et al. (1996) publié dans ce journal, la solution, à ce jour la meilleure et la plus simple, pour introduire des connaissances sur la durée dans les systèmes de reconnaissance de parole standard serait d'imposer une durée minimale (apprise) par segment, simplement en duplicant ou en ajoutant des états qui ne peuvent pas être omis.
Dans notre article, nous défendons le point de vue que les performances de reconnaissance peuvent être encore améliorées en incorporant des connaissances “spécifiques” (comme la durée et le pitch) au sein des systèmes.
Ceci peut être obtenu en optimisant les modèles acoustiques et de langage, et probablement aussi par un post-traitement entièrement basé sur cette connaissance spécifique.
Nous avons utilisé la base de données TIMIT, largement répandue et segmentée manuellement, pour extraire les régularités de durée qui persistent, malgré la grande variabilité inter- et intra-locuteur.
Deux approches ont été principalement mises en oeuvre.
Dans la première approche, on considère les distributions de durée pour les phones individuels, ainsi que pour des classes plus larges, comme celles spécifiées par les voyelles longues ou courtes, l'accent lexical, la position de la syllable au sein du mot et du syntagme, les consonnes post-vocaliques et la vitesse d'articulation.
L'autre approche utilise une analyse hiérarchique de la variance pour étudier la contribution numérique de 11 facteurs différents aux variations de durée.
Quant à savoir si l'exploitation de cette connaissance sur la durée dans un étage de post-traitement va réellement améliorer les performances de reconnaissance, ceci reste à montrer.
Cependant, conformément à l'esprit du message prophétique de l'article de Bourlard et al, nous considérons ici l'amélioration des performances comme un enjeu d'importance pour l'instant secondaire.
Cet article présente des statistiques en relation avec différents algorithmes de devinement de phonèmes.
Les statistiques de ont été obtenues à partir de l'analyse exhaustive d'un lexique de 96,998 mots phonétiques.
Les résultats montrent qu'il est possible de formuler un algorithme prenant en compte une connaissance phonotactique détaillée à une large connaissance phonétique, dont le taux de devinement correct de phonèmes atteint 67% en moyenne.
Ceci implique, au moins d'un point de vue informatique, que l'anglais parlé présente, au minimum, 67% de redondance.
Les résultats montrent aussi que la performance de devinement correct dépend de la longueur et de la position du mot ; le type de phonème et le nombre de phonèmes inconnus dans le mot ont orès peu d'effet sur les résultats finaux.
La décomposition temporelle d'un message vocal fournit une description des paramètres vocaux sous la forme de fonctions-cibles se recouvrant et des vecteurs-cibles correspondants.
Les premières peuvent correspondre à des gestes articulatoires et les seconds à des positions articulatoires idéales.
Bien que développée pour un codage économique de la parole, cette méthode constitue également un outil intéressant pour extraire des informations phonétiques du signal vocal acoustique.
Les paramètres vocaux utilisés par Ataal lorsqu'il a proposé cette méthode (1983) sont les paramètres “log-area”.
Notre méthode de décomposition temporelle modifée (1987) utilise ces paramètres comme information d'entrée.
Toutefois, la méthode ne se limite pas aux paramètres “log-area” : en principe, on peut également choisir les groupes de paramètres plus couramment utilisés.
Dans ce présent article, nous comparons les résultats obtenus avec neuf groupes différents de paramètres vocaux, notamment les paramètres “log-area”, les formants, les coefficients de réflexion et les paramètres de filtre de bande.
Le principal critère de qualité du résultat sera la correspondence entre les fonctions-cibles et les phonèmes ou sous-phonèmes.
On considérera aussi l'importance phonétique des vecteurs-cibles, quoique de manière moins détaillée.
La resynthèse du signal vocal fournira un autre critère ; tant les erreurs de perception que les erreurs physiques fournissent des informations sur l'efficcié du groupe de parametrés pour la décomposition temporelle.
On peut conclure de ces expériences que les parametrés “log-area” constituent le groupe de paramétres le plus approprié pour la décomposition temporelle.
Les paramétres de filtre de bande donnent de meilleurs résultats à certains égards, mais compte tenu des propriétés relatives à la resynthèse, ce groupe n'est pas considéré comme étant le meilleur.
La structure prosodique de la parole est le résultat d'interactions complexes entre différents niveaux d'organisation et à l'intérieur de ces niveaux.
La hiérarchie intonative, manifestée essentiellement par la nature des marqueurs prosodiques est le produit d'interactions et de contraintes entre ces niveaux d'organisation.
Je présente ici un modèle de prédiction et d'interprétation de l'organisation des énoncés en parole spontanée.
Ce modèle est un système hiérarchique composé de six modules : (1) sémantique-pragmatique, (2) syntaxique, (3) phonotactique, (4) accentuel, (5) d'ajustement sémantique, (6) rythmique.
Pour un énoncé donné, le système détermine (i) les niveaux de frontières et les marqueurs prosodiques sur la base de l'information sématique, et de la structure syntaxique définie sur la base de la nouvelle syntaxe X barre(s) ; (ii) les structures accentuelles et rythmiques conditionnées par les contraintes phonotactiques.
L'étape phonétique qui devrait transformer les étiquettes abstraites en valeurs acoustiques n'est pas présentée ice.
Ce modèle peut et doit être développé ultérieurement.
Les améliorations à apporter concernent (a) la nature des règles, (b) les différents aspects de la conversation, (c) certains problèmes théoriques.
En ce qui concerne ce dernier point, les développements récents et en cours de la théorie X barre(s) sont susceptibles d'apporter des modifications positives dans l'interprétation des règles qui permettront de rendre compte de certains faits encore inexpliqués.
Toutefois, dans son état actuel, le modèle fournit des résultats très convaincants, puisqu'il prédit l'organisation intonative et accentuelle avec un haut degré de fiabilité.
Les différentes régles trouvées ont donné lieu à un logiciel baptisé PHONEMIA, premier module d'un ensemble plus vaste qui a pour but la synthése à partir d'un texte éerit.
Ce programme a également été utilisé pour définir les diphones du gree moderne.
Ce papier discute la nature des données qui forment l'entrée des descriptions linguistiques, et particulièrement en ce qui concerne la plus ou moins grande artificialité de ces données.
L'article préconise d'accorder beaucoup plus d'importance dans les travaux de linguistique aux données de parole naturelle en incluant même de la parole spontanée.
Une partie majeure de l'article est consacrée à la discussion générale de quelques préliminaires à l'étude de la variation linguistique en parole naturelle.
Nous présentons une approche inspirée par la biologie du système auditif humain, qui prédit l'intelligibilité d'enregistrements directes de paroles ou après transmissions sous différentes conditions de bruit propre, réverbérations, et autres déformations.
La méthode est basée sur un modèle auditif qui analyse les effets du bruit sur les modulations conjointes de temps et fréquences, présentes dans la parole. Par ailleurs, cette méthode analyse la capacité d'un canal à transmettre fidèlement ces modulations.
Les effets sur les modulations sont convertis en un indice des modulations spectro-temporelles, appelé STMI.
La validité de cet indice est établie en comparant ses prédictions à celles du STI classique ; ainsi qu'aux résultats expérimentaux des taux d'erreurs de sujets humains qui écoutent de la parole contaminée par des combinaisons de bruit propre et de réverbération.
Nous démontrons également que le STMI est capable de manipuler des conditions encore plus sévères, comme les déformations non-linéaires, tels les décalages et autres instabilités des phases ; conditions auxquelles le STI classique s'avère être insensible.
Il s'agit de la structure de treillis utilisée dans la méthode Navigala, méthode de reconnaissance de symboles basée sur un parcours (de type arbre de décision) dans le treillis.
De ce lien de fusion nous déduisons un algorithme d'extraction d'un arbre de décision à partir d'un treillis dichotomique.
Nous finissons par des expérimentations visant à comparer, pour de la reconnaissance de symboles, les performances des arbres de classification et des treillis construits avec la méthode Navigala.
L'efficacité des algorithmes de reconnaissance des formes est fortement conditionnée par la définition adéquate des formes censées structurer les données.
Le modèle multigramme fournit un outil statistique permettant de retrouver des régularités séquentielles de longueur variable dans des flux de données.
Dans cet article, nous présentons une formulation générale de ce modèle, applicable à des flux, uniques ou multiples, de données, ayant des valeurs discrètes ou continues.
La première évaluation du modèle concerne l'extraction, à partir de données textuelles, d'un répertoire de séquences de lettres de longueur variable, dans lesquelles tous les espaces entre mots ont été supprimés.
Il apparaı̂t que les séquences de lettres inférées lors de cette procédure totalement non supervisée sont clairement liées à la structure morphologique du texte.
Le modèle est ensuite utilisé pour inférer, directement à partir des données de parole, un ensemble d'unités acoustiques de longueur variable.
Des fichiers audio contenant des exemples d'unités acoustiques sont fournis avec cet article afin de montrer leur consistance d'un point de vue auditif.
Cet article rend également compte des résultats d'expériences utilisant ces unités, définies acoustiquement, pour la reconnaissance de parole continue.
Les réseaux bayésiens sont d'excellents outils de modélisation de l'incertain grâce à leur représentation graphique claire et aux lois de probabilités conditionnelles définies sur ce graphe.
La structure et les probabilités sont généralement données par un expert.
Dans ce papier nous nous sommes intéressés à l'apprentissage direct de la structure de tels réseaux à partir de bases de données.
Nous avons adopté une approche bayésienne qui permet de retrouver une adéquation entre la structure et les données.
Notre point de départ a été deux algorithmes : K2 de Cooper et Herskovits et B de Buntine.
Nous avons alors développé un nouvel algorithme K2B qui profite des avantages de ces deux algorithmes tout en essayant d'atténuer leurs inconvénients.
L'implémentation de K2B a donné naissance à Alexso, qui arrive à trouver un compromis entre représentativité et simplicité de la structure.
Le but de cet article est de présenter le schéma d'une théorie sémantique fondée sur l'analogie entre le langage naturel et le langage programmation de l'ordinateur.
On décrit un modéle unique de compréhension et de perception de phrases pour illustrer la métaphore centrale “compiler et executer” qui sous-tend les sémantiques des méthodes.
On réanalyse á la lumiére de la théorie des méthodes (procedural theory) le rôle de la connaissance générale interne au lexique et du mécanisme arbitrant les restrictions sélectives.
Trois projets concernant la reconnaissance auditive des mots et la structure de lexique sont décrits.
Le premier a été conçu pour tester expérimentalement certaines prédictions spécifiques dérivées de MACS, un modèle de simulation de la théorie de la cohorte.
A l'aide d'un paradigme d'amorçage, des indications en faveur d'une activation acousticophonétique dans la reconnaissance de mots ont été obtenues dans trois expériences.
Le second projet décrit les résultats d'analyses de la structure et de la distribution de mots dans le lexique en utilisant une large base de données lexicales.
Des statistiques sur des espaces de similitudes pour des mots de haute et basse fréquence ont été appliquées à des données antérieures sur l'intelligibilité de mots présentés dans le bruit.
Il apparaît que les différences d'identification sont liées à des facteurs structuraux à propos des mots spécifiques et à la distribution des mots similaires dans leurs voisinage.
Le troisième projet développe une nouvelle théorie de la reconnaissance de mots connue sous le nom de Théorie d'Affinement Phonétique.
Cette théorie basée sur des données propres à l'auditeur humain, a été conçue pour incorporer certaines des connaissances acoustico-phonétiques et phonotactiques dont disposent les auditeurs humains sur la structure interne des mots et leur organisation dans le lexique, ainsi que sur la manière dont ils utilisent cette connaissance pour reconnaître les mots.
La théorie s'appuie sur plusieurs techniques nouvelles formalisant des stratégies de recherche de réduction spatiale à partir de grands vocabulaires dans des situations où seules des informations partielles sur le contenu phonétique d'un mot sont disponibles.
Dans l'ensemble, nos résultats aboutissent à des découvertes nombreuses, nouvelles et importantes sur la relation entre la perception de la parole et la reconnaissance auditive des mots, deux domaines de recherche qui ont traditionellement été dans le passé sous des perspectives très différentes des nôtres.
L'étude de la mémoire est un grand défi, peut-être le plus grand dans les sciences biologiques.
La mémoire implique des changements dans une infime fraction d'un ensemble extrêmement vaste d'éléments, une conclusion qui rend formidable la tâche de trouver ces changements en utilisant les technologies courantes.
Que peut-on faire pour contourner cet obstacle dans les recherches neurologiques de l'apprentissage ?
Une voie de réponse devenue particuliérement productive au cours des années récentes est d'étudier les phénoménes d'apprentisage dans des systémes “modéles” relativement simples.
L'idée est d'extraire des principes de base de ces modéles dans lesquels les détails moléculaires et anatomiques peuvent être étudiés, puis de les utiliser en analysant l'apprentissage dans les régions supérieures du cerveau.
Nous présentons dans cet article les progrés en cours et les nouveaux concepts dérivés de cette approche sur des modéles animaux simples.
Nous décrivons un analyseur stochastique pour la compréhension de la parole spontanée dans une application de demande d'informations pour les transports ferroviaires en langue française.
Un autre facteur important concerne la création, par une méthode d'étiquetage itérative, d'un corpus de représentations sémantiques sur la base desquelles le modèle stochastique peut être établi.
L'analyseur a été testé sur les transcriptions corrigées des requêtes formulées par des utilisateurs et sur les transcriptions exactes du module de reconnaissance.
Un sonar latéral de cartographie enregistre les signaux qui ont été rétrodiffusés par le fond marin sur une large fauchée.
L'analyse des statistiques de ces signaux rétrodiffusés montre une dépendance à ces angles de rasance, ce qui pénalise fortement la segmentation des images en régions homogènes.
Pour améliorer cette segmentation, l'approche classique consiste à corriger les artefacts dus à la formation de l'image sonar (géométrie d'acquisition, gains variables, etc.) en considérant un fond marin plat et en estimant des lois physiques (Lambert, Jackson, etc.) ou des modèles empiriques.
L'approche choisie dans ce travail propose de diviser l'image sonar en bandes dans le sens de la portée ; la largeur de ces bandes étant suffisamment faible afin que l'analyse statistique de la rétrodiffusion puisse être considérée indépendante de l'angle de rasance.
Deux types d'analyse de texture sont utilisés sur chaque bande de l'image.
La première technique est basée sur l'estimation d'une matrice des cooccurrences et de différents attributs d'Haralick.
Le deuxième type d'analyse est l'estimation d'attributs spectraux.
La bande centrale localisée à la moitié de la portée du sonar est segmentée en premier par un réseau de neurones compétitifs basé sur l'algorithme SOFM (Self-Organizing Feature Maps) de Kohonen.
Cette nouvelle méthode de segmentation est évaluée sur des données réelles acquises par le sonar latéral Klein 5000.
Les performances de segmentation de l'algorithme proposé sont comparées avec celles obtenues par des techniques classiques comme l'algorithme K-moyennes (K-means).
L'un des principaux défis posés à l'industrie du jeu vidéo est de mettre en scène des personnages non joueurs (PNJ) dont les comportements sont crédibles.
Or, les recherches montrent que les émotions jouent un rôle déterminant dans le comportement des individus.
Pour augmenter la crédibilité des comportements des PNJ, nous proposons dans cet article un modèle de la dynamique des émotions prenant en compte la personnalité et les relations sociales du personnage.
Nous présentons tout d'abord les travaux de la littérature sur les émotions, la personnalité et les relations sociales en informatique et en sciences humaines et sociales.
Nous soulignons l'influence de la personnalité sur le déclenchement des émotions et des émotions sur la dynamique des relations sociales.
En nous appuyant sur ces travaux, nous proposons un modèle dynamique de l'état socio-émotionnel et son implémentation sous la forme d'un outil simple qui permet de simuler la dynamique de l'évolution des émotions et des relations sociales des PNJ suivant leur personnalité et leur rôle.
Cet article expose de récentes recherches sur les techniques de contrôle de la qualité ainsi que de conversion de la voix.
Après un bref rappel de quelques résultats scientifiques de base sur les corrélations acoustiques dans le caractère individuel de la voix, nous décrivons les dernières techniques de traitement de la parole en matière de contrôle de la voix et de reproduction des caractéristiques du locuteur.
Nous nous concentrons plus particulièrement sur une description des méthodes non paramètriques d'identification spectrale segmentée des caractéristiques des diffèrents locuteurs et nous introduisons á ce sujet plusieurs types d'identifications spectrales, qui ont évolué de pair avec les techniques d'adaptation au locuteur développées en reconnaissance de la parole.
Cet article est consacré à l'utilisation des statistiques d'ordre quatre en Traitement d'Antenne.
Après avoir rappelé brièvement les propriétés des moments et cumulants, on propose un formalisme algébrique pour exprimer commodément les statistiques d'ordre deux et quatre de variables aléatoires vectorielles.
Nous définissons ainsi la « quadricovariance » qui est une représentation exhaustive des cumulants du quatrième ordre, et ses « matrices propres » qui en fournissent une décomposition orthogonale.
Dans notre écriture, il y a une forte analogie entre la covariance et la quadricovariance, ce qui suggère une extension au quatrième ordre des méthodes développées pour exploiter le second ordre.
On obtient aussi avec « 4-MUSIC » une capacité théorique de détection supérieure au nombre de capteurs.
Elle passe de N − 1 pour 2-MUSIC à 2(N − 1) pour 4-MUSIC sur une antenne linéaire de N capteurs équidistants.
Cette capacité peut être portée jusqu'à N(N − 1) sources en choisissant une antenne non uniforme.
La notion de matrice propre permet aussi d'obtenir une solution directe au problème de « séparation de sources » où il s'agit de séparer un mélange vectoriel de composantes statistiquement indépendantes.
Pour évaluer la composante d'émission d'une voix synthétique intégrée dans un système complexe, on a besoin de méthodes et de méthodologies qui ne sont pas fournies par les tests d'évaluation standards et indépendants d'une application.
Ces derniers analysent la parole synthétique surtout dans sa forme (structure de surface) et seulement en second lieu dans la signification qui lui est attribuée (structure profonde).
Pour obtenir un cadre d'évaluation fiable pour un système d'application, les aspects fonctionnels de la parole doivent être pris en compte.
Dans cet article, on présente deux études d'acceptabilité des dimensions qualifiantes de la voix synthétique utilisée dans différents scénarios d'application.
Pour des raisons de dépenses (temps et coûts), on s'est limité à une évaluation en laboratoire.
La première étude évalue la voix synthétisée d'un système de navigation et d'information de trafic, intégré dans une voiture.
La deuxième étude est relative à la voix synthétisée d'un système de dialogue naturel.
Les résultats montrent que les différentes dimensions contribuent à des degrés variables à l'acceptabilité globale, en fonction du scénario d'application.
Il est donc nécessaire d'utiliser des méthodes d'évaluation orientées vers les applications pour identifier les dimensions qui lui sont spécifiques.
Les caractéristiques de l'application qui demandent une modélisation dans les tests d'évaluation sont discutées, et des exemples sont fournis pour les deux domaines considérés.
Le problème type du classement par masses de véhicules à partir de mesures d'accélérations et de vitesses est traité.
Une méthode de discrimination optimale pour ce problème est construite en considérant 4 niveaux : choix de l'espace de recherche (sélection de variables par algorithmes évolutionnaires ou par heuristiques, analyse discriminante), choix du critère de discrimination (critère Bayésien ou de marge large), choix de la complexité et choix des paramètres des algorithmes.
Dans la production de consonnes occlusives par les mal-entendants, les erreurs de voisement sont beaucoup plus fréquentes que celles de lieu d'articulation.
Le problème vient de la complexité phonétique de certains types d'occlusives homorganiques.
Les sourds angophones éprouvent des difficultés particulières à prononcer les /ptk/ aspirés qui requièrent un décalage temporel entre l'occlusion orale et l'ouverture du laryyx.
Les données présentées dans cet article monrrent que les sourds francophones éprouvent des difficultés similaires pour produire les /bdg/ prévoisés qui demandent un d'elai temporel précis entre le début des vibrations laryngées et la détente de l'occlusion orale.
La succession rapide de ces deux gestes articulatoires permet de soutenir la voix jusqu'à la fin de l'occlusion, ce qui a une importance décisive pour la perception du voisement.
Nos données montrent par ailleurs que les locuteurs qui parviennent à produire correctement le prévoisement sont généralement ceux qui parviennen à percevoir correctement le trait de voisement.
Bienque le lieu d'articulation des occlusives ne soit pas mieux perçu que leur catégorie de voisement, la plupart des sujets modérément sourds de cette expérience peuvent parfaitement produire les distinctions de lieu.
Le rôle des rétroactions perceptives dans la maîtrise des gestes articulatoires est abordé dans la discussion.
Plus spécifiquement on avance que la construction des propositions relatives rend conflictuels les principes d'origine structurelle et ceux d'origine perceptuelle dans les langues de type SOV.
La manière dont une proposition relative est structurée dans un langage SOV est un obstacleàson calcul perceptuel.
Ce conflit serait un des facteurs majeursàl'origine du changement diachronique d'un langage d'une typologie OVàun langage d'une typologie VO.
Ce système est totalement interactif et ouvert en ce sens qu'il permet l'intégration facile de nouveaux paramètres.
Une présentation unifiée des méthodes de séparation de sources fondées sur les moments d'ordre supérieur est proposée.
L'approche considérée part du recensement systématique des paramètres caractéristiques de toutes les méthodes, comme : les familles d'hypothèses statistiques concernant les données ; les catégories de modèles conjecturés (standards ou doublement orthogonaux) ; les critères de séparation (d'indépendance), qui conduisent à la restitution des sources, énoncés à partir de moments ou de cumulants ; les principes effectifs des méthodes de séparation (démarches directes utilisant une matrice de restitution des sources, démarches indirectes ou globales identifiant au préalable ou simultanément d'autres entités propres aux sources, amplitudes etc…).
De manière générale, il est établi que l'ensemble de ces méthodes conduisent à restituer, non pas un jeu de sources uniques, mais plutôt les éléments appartenant à l'intersection de deux classes d'équivalence.
La première appelée classe du second ordre est associée à tous les vecteurs aléatoires de même matrice de covariance.
La seconde qualifiée de classe d'indépendance, doit son existence à l'invariance de l'indépendance mutuelle de variables aléatoires dans toute opération de permutation- homothétie.
La problématique abordée dans cet article est celle de la conception automatique d'agents autonomes devant résoudre des tâches complexes mettant en œuvre plusieurs objectifs potentiellement concurrents.
Nous proposons alors une approche modulaire s'appuyant sur les principes de la sélection d'action où les actions recommandées par plusieurs comportements de base sont combinées en une décision globale.
Dans ce cadre, notre principale contribution est une méthode pour qu'un agent puisse définir et construire automatiquement les comportements de base dont il a besoin via des méthodes d'apprentissage par renforcement incrémentales.
Nous obtenons ainsi une architecture très autonome ne nécessitant que peu de réglages.
Cette approche est testée et discutée sur un problème représentatif issu du "monde des tuiles"
De grandes bases de données sont nécessaires pour tester des modèles de production et de perception à différents niveaux linguistiques.
La gestion de telles bases de données pose d'importants problèmes à la fois pour étiqueter le signal de parole et pour accéder au matériel stocké.
De manière à simplifier certains de ces problèmes, nous avons créé un système d'analyse de la parole.
Les signaux de parole sont archivés dans des fichiers de la taille d'une phrase.
Ces fichiers sont segmentés et transcrits de manière semi-automatique à partir d'une transcription phonétique de la phrase.
La transcription est générée par les règles lettres-sons de notre système de conversion texte-parole.
La base de données est plutôt orientée vers la recherche de type acoustico-phonétique que, par exemple, vers l'évaluation de systèmes de reconnaissance.
L'accent a donc été mis sur la souplesse et la spécification linguistique des requêtes de recherche dans la base de données.
Dans notre solution — inhabituelle — à ce problème, nous utilisons pour l'accès aux données la structure de nos règles de synthèse qui est similaire aux notations de la phonologie générative.
Grâce à une formulation sous forme de règles, on peut identifier facilement les segments de parole satisfaisant certaines conditions relatives au contexte.
On peut recueillir directement des informations sur la durée durant la recherche dans la base de données.
Des programmes d'analyse spectrale utilisant une variété de représentations spectrales sont également disponibles.
Le taux de reconnaissance de la parole contnue réalisé par des algorithmes de focalisation dépend de facteurs tels que la perplexité, la structure de recombinaison du modéle linguistique, la difficulté du vocabulaire, la qualité du décodeur acoustico-phonétique et la stratégie d'élagage choisie.
Un modèle statistique qui prend l'ensemble de ces facteurs en considération est dèveloppé ici.
Un algorithme itératif est présenté qui calcule la distribution des différents chemins parcourus ainsi que le taux d'erreurs par phrase qui en résulte.
Il n'est donc guère étonnant qu'on puisse finalement se demander à quel genre de problème répond la présence centrale du concept de représentation dans la linguistique de Gustave Guillaume.
Si « promouvoir le langage à l'existence, c'est le promouvoir à la représentation − ce sans quoi rien n'existe pour l'esprit » , ce n'est pas en raison de la nature du langage que le linguiste recourt au concept de représentation mais parce qu'il existe une certaine façon d'être un linguiste qui consiste à poser le langage comme un objet pour l'esprit, attitude qui se distingue, parmi bien d'autres solutions, de celle consistant par exemple à le voir comme un objet ou un « paramètre » de la vie sociale.
L'article décrit le développement et l'évaluation du module de conversion graphème-phonème d'un système complet de synthèse en temps réel développé à l'Université de Macquarie.
L'évaluation du système a été grandement facilitée par l'usage de statistiques pondérées qui reflètent la fréquence d'occurrence de chaque mot contenu dans les corpus LOB et Brown de l'anglais.
Ces statistiques sont déduites d'une base de données de mots test qui inclut pour chaque mot toutes les prononciations acceptables de l'australien (selon le dictionnaire de Macqaurie) ainsi que leur fréquences d'après LOB et Brown.
Ces scores facilitent les décisions quant aux modifications des règles et du lexique afin d'obtenir une amélioration maximale des performances globales sur un texte ordinaire.
Le centre d'intérêt dans la recherche de la reconnaissance automatique de la parole (ASR), parti des mots isolés, s'est engagé vers le discours conversationnel.
Par conséquence, la quantité de variation de prononciation présente dans le discours dont nous rapportons les résultats a graduellement augmenté.
La variation de prononciation détériorera la performance d'un système ASR si l'on n'en rend pas compte.
C'est probablement la raison principale pourquoi la recherche dans le domaine de la modélisation de la variation de prononciation pour ASR a augmenté récemment.
Dans cette contribution on fournit une vue d'ensemble des publications sur ce sujet, et en particulier on référe aux articles de cette edition spéciale et aux contributions présentées dans les sessions qui ont eu lieu a 'Rolduc'.
Puis les questions d'évaluation et de comparaison sont adressées.
Une attention particulière est prêtée à certains des facteurs les plus importants qui rendent difficile de comparer les différentes méthodes d'une maniere objective.
Enfin quelques conclusions sont tirées quant à l'importance de l'évaluation objective et de la façon dans laquelle elle pourrait être effectuée.
Cet article stipule que la spécification des traits d'un ça déficient dans une variété de français suisse permet de rendre compte de sa distribution syntaxique.
Contrairement aux clitiques accusatifs ordinaires le/la/les, cette forme pronominale ne présente pas de trait de genre, de nombre et de Cas, mais possède un trait aspectuo-temporel locatif.
Les arguments qui appuient l'hypothèse de l'absence de Cas pour le ça déficient sont les suivants : 1. (i) l'impossibilité de le redoubler, 2. (ii) l'impossibilité pour ça d'être un enclitique et 3. (iii) son interaction avec la topicalisation et la dislocation à droite qui diffère de ce qui est observé avec les clitiques ordinaires dans ce contexte.
Une autre distinction entre les traits de ça et ceux des pronoms ordinaires réside dans le statut catégoriel double de ça en tant que D et DP.
On observe que la composition en traits de ça exige une quantification générique événementielle.
Cette interprétation est toujours disponible avec les verbes transitifs événementiels mais avec les inaccusatifs et les statifs transitifs, cette lecture est bloquée au temps présent.
Nous montrons que c'est la non-ambiguïté aspectuelle de ça, i.e. le fait qu'il n'apparaisse que dans des contextes de quantification générique événementielle, qui est responsable de séquences agrammaticales telles que ∗Je ça aime dans cette grammaire.
Le but de cet article est d'étudier différentes techniques de lissage de modèles de langage et différents algorithmes de construction de modèles de langage à base d'arbres de décision.
Pour cela, nous construisons des modèles de langage pour des caractères écrits (lettres) à partir du Brown corpus.
Nous considérons deux classes de modèles pour le processus de génération du texte : le modèle de langage n-gram, et différents modèles de langage à base d'arbres de décision.
Dans la première partie de l'article, nous comparons les algorithmes de lissage les plus couramment appliqués au modèle de langage n-gram.
L'algorithme “bottom-up deleted interpolation” donne les meilleurs résultats pour le lissage du modèle de langage n-gram, dépassant de façon significative la technique de lissage “back-off” pour de grandes valeurs de n.
Dans la seconde partie de l'article, nous considérons différents algorithmes de développement d'arbres de décision.
Parmi eux, un algorithme de type classification K-means aboutit aux meilleurs résultats pour la construction des arbres de décision.
Cependant, le modèle de langage n-gram fournit de meilleurs résultats que les modèles de langage à arbres de décision pour la modélisation du langage écrit.
Nous croyons que cela est dû à la nature prédictive des chaı̂nes de caractères, qui semble être naturellement modélisée par les n-grams.
Par `annotation linguistique' nous désignons toute notation descriptive ou analytique appliquée à des données langagières brutes.
Ces données brutes peuvent être des signaux temporels – enregistrements audio, vidéo et/ou physiologiques – ou du texte.
Les notations ajoutées peuvent être des transcriptions de toute nature (des traits phonétiques aux structures du discours), des catégories grammaticales ou sémantiques, une analyse syntaxique, l'identification d' `entités nommées', l'annotation de coréférences, etc.
Malgré les efforts entrepris pour créer des formats et des outils adaptés à de telles annotations et pour diffuser des bases de données linguistiques annotées, le manque de standards largement acceptés devient un problème critique.
Les standards proposés, lorsqu'ils existent, se concentrent sur les formats de fichiers.
Cet article se concentre au contraire sur la structure logique des annotations linguistiques.
Nous passons en revue une grande variété de formats d'annotations existants et en dégageons une structure conceptuelle commune, le graphe d'annotation.
Ceci fournit un cadre formel pour construire des annotations linguistiques, les tenir à jour et y effectuer des requètes, tout en restant cohérent avec de nombreux autres structures de données et formats de fichiers.
Dans les représentations visuelles du signal acoustique de la parole telles que l'oscillogramme ou le spectrogramme, la parole apparaît comme des séries concaténées de phénomènes acoustiques qui varient dans leur spectre, en amplitude et en durée.
Les résultats d'une grande variété d'expériences psychiacoustiques, de la fusion auditive au masquage temporal en passant par les études sur la fluence (streaming) peuvent être interprétées comme pertinentes pour la découverte des capacités auditives utilisées dans l'écoute de ces séquences de parole.
Un échantillonnage de ces résultats expérimentaux servira à illustrer les relations entre la psychoacoustique des phénomènes non verbaux et ceux de la parole de manière à suggérer des lignes de conduite pour le travail futur sur les configurations temporelles non verbales afin d'aboutir à une psychophysique plus complète des sons complexes.
Dans le cadre du déploiement des réseaux cellulaires de radiocommunication, il est nécessaire de prédire la zone de couverture des émetteurs.
Pour un site d'émission, la technique classique consiste à appliquer un modèle de propagation des ondes électromagnétiques en différentes positions définies selon un pas spatial constant.
Toutefois, cette méthode conduit à un temps de calcul très important voire prohibitif dans des environnements géographiques complexes.
Des approches existent pour réduire le temps de calcul ;
elles consistent à simplifier la complexité du modèle de propagation utilisé.
La démarche proposée dans cet article est complémentaire.
En effet, elle est indépendante du modèle et porte sur la réduction du nombre de points d'application de ce modèle.
La méthode présentée s'appuie sur une hypothèse dont la vérification nécessite la segmentation de signaux mesurés par un récepteur mobile et un logiciel d'analyse électromagnétique de l'environnement de mesures.
Ainsi, l'objectif est de segmenter le signal reçu en intervalles correspondant à des combinaisons particulières de phénomènes physiques.
Pour cela, une représentation proposée par Mallat et Zhong appelée « représentation en maxima d'ondelettes » est étudiée.
Cette décomposition permet l'étude des dérivées d'une fonction à différentes échelles.
Nous proposons une méthode de segmentation de signaux basée sur le chaînage des maxima à travers les échelles.
Ce chaînage permet de sélectionner les discontinuités les plus importantes du signal et ainsi de le segmenter.
Nous présentons une méthode originale de classification de trajectoires dans des séquences vidéos pour la reconnaissance d'événements dynamiques.
Les Modèles de Markov Cachés (MMC) sont utilisés afin de représenter chaque trajectoire et d'évaluer leurs similarités.
Nous avons pu valider notre méthode en la comparant à plusieurs autres méthodes telles que la comparaison d'histogrammes, une méthode utilisant les Séparateurs à Vaste Marge (SVM) ainsi qu'une méthode de MMC utilisant des modélisations par mélanges de gaussiennes.
Des descripteurs appropriés, invariants à la translation, à la rotation ainsi qu'au facteur d'échelle sont calculés sur les trajectoires, puis exploités dans une représentation par MMC.
Une méthode statistique est également proposée pour le choix du nombre d'états pour la modélisation par MMC choisie.
Cet article décrit comment les performances d'un reconnaisseur de parole continue (CSR) pour le néerlandais ont été améliorées en modelant la variation de prononciation.
Nous proposons une procédure générale pour modeler cette variation.
En bref, elle consiste à ajouter des variantes de prononciation au lexique et dans le ré-apprentissage des modèles de phones en utilisant des modèles de langage auxquels les variantes de prononciation ont été ajoutées.
D'abord, des variantes de prononciation à l'intérieur de mot ont été produites en appliquant un ensemble de cinq règles phonologiques optionnelles aux mots dans le lexique de base.
Ensuite, un nombre limité de processus entre-mots ont été modelés, en utilisant deux méthodes différentes.
Dans la première approche, des processus entre-mots ont été modelés en ajoutant directement les variantes “entre-mots” au lexique, et dans la deuxième approche ceci a été fait en utilisant des “mots-multiples”.
En conclusion, la combinaison de la méthode qui se limite aux processus à l'intérieur de mot avec les deux méthodes “entre-mots” a été testée.
La performance de base était un taux d'erreur de 12.75% mots (WER);
comparée à cette performance de base, une amélioration petite mais significative de 0.68% dans WER a été obtenue avec la méthode 'à l'intérieur de mot', tandis que les deux méthodes d'entre-mots en isolation ont mené à des petites améliorations non significatives.
La combinaison de la méthode “à l'intérieur de mot” avec la méthode 2 “entre-mots” a mené au meilleur résultat : une amélioration absolue de 1.12% dans le WER a été trouvée comparée à la ligne de base, qui est une amélioration relative de 8.8% dans le WER.
Il a été montré que des techniques d'analyse typologique hiérarchique constituent des outils puissants pour construire des références indépendantes du locuteur dans les systémes de reconnaissance de la parole basés sur des méthodes de distorsion dynamique de l'échelle temporelle (DTW).
Dans cet article, nous proposons un algorithme d'analyse typologique qui génère, en considérant simultanément le spectre et la durée, des patrons de références pour des systémes basés sur des modéles markoviens (HMM) à densité continue.
De meilleures performances de reconnaissance sont ainsi démontrées dans le cadre d'une tâche de reconnaissance de chiffres utilisant la base de données TI/NBS de chiffres connectés.
Cet article présente, d'une manière informelle, l'historie de la recherche sur la synthèse de la parole depuis la machine à parler de Von Kempelen jusqu'à la prédiction linéaire.
Dans cet article, nous proposons un cadre d'étude pour rendre compte de certaines variations mélodiques dans la parole.
Ce cadre consiste à définir des points cibles de F 0 et les régles gouvernant leurs réalisations.
Les cibles sont définies comme les plus petites unités opératives associées à des unités fonctionnelles d'un point de vue linguistique ; elles sont comparables à des phonèmes segmentaux.
Les règles d'implémentation sont fondées sur des contraintes articulatoires influant la production des contours mélodiques.
En raison de ces contraintes, la réalisation d'une même cible peut résulter en diverses formes de F 0 qui ne reflètent que partiellement les cibles sous-jacentes.
Nous discuterons également de l'intérêt de ce cadre d'étude pour la compréhension de certains contours de F 0, incluant les variations liées au recouvrement et à l'anticipation, la descente graduelle, la déclinaison, et l'alignement des pics de F 0.
Enfin, nous considèrerons les interactions possibles entre les cibles locales et non-locales.
Après avoir rappelé le principe de rehaussement par filtrage polynomial de l'image du relief Braille numérisée, il est décrit la méthode de reconnaissance choisie pour ce type particulier de forme d'objets.
Cette méthode est basée sur la projection de chaque graphème sur deux axes orthogonaux.
Il est décrit les cinq érapes de reconnaissance de chaque rangée de relief Braille qui tiennent compte de ses irrégularités de forme, et qui exploitent une méthode de maximum de vraisemblance.
Le relevé de la dispersion des positions des axes des graphèmes permet de donner une estimation théorique du taux de réussite de reconnaissance pour des reliefs fabriqués manuellement.
Le taux vérifié dans la pratique est voisin de 99 %.
La description des langues en danger est cruciale pour la typologie linguistique, comme source possible d'information sur des types de structures non attestés dans les langues préalablement décrites.
Inversement, une bonne information sur l'état des connaissances en typologie est particulièrement utile aux linguistes qui décrivent des langues jusque là peu décrites, ce qui est le cas de la plupart des langues en danger.
Toutefois, les typologues ne doivent pas perdre de vue qu'il y a un problème à mettre sur le même plan, dans la recherche de généralisations typologiques, des langues toujours bien vivantes et largement décrites, et d'autres dont l'unique description est impossible à vérifier ou à compléter.
En outre, on ne peut pas travailler sur des langues moribondes comme sur des langues qui sont encore le moyen de communication usuel d'une communauté, et on peut avoir des doutes sur la représentativité des données recueillies dans ces conditions, surtout dans des domaines comme la syntaxe.
Dans cet article, nous introduisons un nouveau concept : les transversaux cubiques sur te treillis cube d'une relation d'attributs catégories.
La découverte des transversaux cubiques est un sous-problême de la recherche des transversaux d'un hypergraphe car il existe un plongement d'ordre du treillis cube vers le treillis des parties des attributs binaires (valeurs).
En se basant sur ce fait, nous proposons un algorithme par niveau pour l'extraction des minimaux transversaux cubiques.
Nous appliquons ce concept à la recherche d'une représentation condensée de l'espace de version émergent, une nouvelle fonctionnalité OLAP.
L'espace de version émergent représente la différence de deux cubes de données uni-compatibles ou l'ensemble des tuples les plus fréquents dans cette différence.
Finalement, nous proposons un algorithme par niveau avec partition et fusion pour le calcul des bordures de l'espace de version émergent sans calculer les deux datacubes associés.
Des variations fines de la qualité de la voix (phonatoire) peuvent révéler des aspects de l'humeur et de l'attitude du locuteur, et forment donc un aspect important du style langagier.
Cet article illustre une recherche en cours sur les corrélats dans la source vocale de certaines de ces différences de qualité.
Les qualités de voix incluaient les voix modale, soufflée, chuchotée, tendue, lâche et grinçante décrites par Laver (1980). Les analyses présentées mettent l'accent sur un mot extrait d'un passage de prose lu avec ces qualités.
La méthode principale utilisée pour analyser la source de voix impliquait un filtrage inverse de l'onde de parole.
Pour quantifier les paramètres de la source, un modèle de la source de voix à quatre paramètres (le modèle LF) était mis en correspondance avec l'onde obtenue par filtrage inverse.
Des analyses fréquentielles de l'onde de parole, basées sur des sections spectrales à bande étroite et sur un moyennage spectral ont également été effectuées.
Des comparisons détaillées des données mesurées directement à partir de l'onde glottique et de celles mesurées sur la parole à la sortie ont permis d'entrevoir des phénomènes qui n'auraient pas été inférés à partir d'une seule de ces deux analyses.
Les résultats mettent à jour aussi bien des differences importantes entre les qualités qu'une variation considérable au sein d'une seule qualité.
Les données pourraient également se révéler utiles pour la resynthése, qui constitue un outil important pour tester les aspects perceptifs de la qualité vocale ; entre autres la “coloration” d'attitude et d'émotion qui peut être associée à des qualités vocales particulières.
Cet article expose une méthode simple pour accélérer les calculs des k plus proches voisins.
Malgré l'augmentation sensible du taux d'erreur, cette méthode devrait trouver sa place dans des applications qui souhaitent intégrer un algorithme de classification et où la contrainte de vitesse d'exécution est forte.
Cet article présente un nouvel outil théorique fondé sur la Théorie de l'Information afin de réaliser une évaluation d'un outil de classement plus fine que les mesures classiques.
Nous travaillons dans le cadre de la Reconnaissance d'objets naturels complexes et compliqués.
Nous montrons que les réseaux connexionnistes permettent l'apprentissage d'une fonction de fusion optimisée selon la nature du problème et la structure du Système de Reconnaissance.
Nous montrons aussi que la répartition de l'information sur chaque outil de classement contribue à une meilleure reconnaissance.
Une approche de type génétique est alors conçue pour adapter la partition de l'espace des paramètres relativement à l'ensemble des outils disponibles.
Nous présentons “Transcriber”, un outil d'aide à la création de corpus de parole, et nous décrivons des éléments de son développement et de son utilisation.
Transcriber a été conçu pour permettre la segmentation manuelle et la transcription d'enregistrements de nouvelles radio-diffusées de longue durée, ainsi que l'annotation des tours de parole, des thèmes et des conditions acoustiques.
Cet outil très portable, reposant sur le langage de script Tcl/Tk et des extensions telles que Snack pour les fonctionnalités audio et tcLex pour l'analyse lexicale, a été testé sur différents systèmes Unix et sous Windows.
Le format de données respecte le standard XML avec un support d'Unicode pour les transcriptions multilingues.
Distribué sous license libre pour encourager la production de corpus, faciliter leur échange, augmenter le retour d'expérience des utilisateurs et motiver les contributions logicielles extérieures, Transcriber est utilisé depuis plus d'un an dans plusieurs pays.
Suite à cette utilisation, de nouveaux besoins sont apparus comme le support de formats de données supplémentaires, de la vidéo, et un meilleur traitement de la parole conversationnelle.
En utilisant le modèle des graphes d'annotation formalisé récemment, l'adaptation de l'outil vers de nouvelles tâches et le support de différents formats de données sera facilité.
Cet article explore plus avant la question de savoir à partir de quel moment les musulmans commencèrent d'ajouter des imprécations divines à la mention des Francs, sujet que nous avions déjà abordé dans cette revue.
Cette pratique mit un certain temps à s'imposer dans les usages après l'arrivée des Francs dans le Levant et nous avions fait valoir que les premières invocations n'avaient pas collé aux préoccupations des contemporains, à la différence de celles employées par la suite, plus en phase avec les besoins de l'époque.
Depuis la publication de cet article, nous avons eu l'opportunité de travailler sur le manuscrit original de notre source la plus ancienne, lequel a révélé que son auteur avait bel et bien recouru à une forme d'imprécation qui allait devenir populaire ultérieurement, mais que la manière dont il avait procédé demeurait suffisamment ambiguë pour en empêcher l'adoption par d'autres auteurs.
Si les mouvements articulatoires peuvent être calculés, les paramètres articulatoires qui représentent le mouvement des organes articulatoires pourraient être des traits utiles pour la reconnaissance de la parole.
Une méthode efficace pour l'estimation de ces mouvements est décrite ainsi que son application à la reconnaissance de la parole.
Nous décrivons d'abord une méthode dite de modèle apparié et nous évaluons diverses mesures de distance spectrale.
Les résultats indiquent qu'en moyenne la meilleure mesure est celle de la distance cepstrale d'ordre élevé.
Ensuite, les parametres articulatoires sont utilisés pour la reconnaissance des voyelles.
Il est montré que l'adaptation du modèle en fonction de la longueur estimée du conduit vocal normalise efficacement les différences inter-locuteurs.
Enfin, les commandes motrices des mouvements articulatoires sont estimées en tenant compte de la dynamique articulatoire, ce qui permet de reconnaître les voyelles émises de manière continue grâce à ces commandes.
Une part considérable des effets de coarticulation peut aussi être compensée à l'aide de ces commandes estimées, ce qui rend la méthode utile pour la reconnaissance de la parole continue.
Conformément aux principes du modèle de la séquence des tons le contour F0 est analysé comme une suite de valeurs de temps et fréquence discrètes qui sont liées par des fonctions de transition.
Leur position est représentée par rapport à la syllabe et l'étendue de la fréquence fondamentale.
Toutes les étiquettes tonales sont examinées sur la base de ces paramètres.
Les résultats sont transformés en une série de règles qui rend possible la génération d'un contour F0.
Tones and Break Indices (ToBI), un système servant à la transcription des structures tonales de l'anglais américain, fournit un inventaire d'étiquettes tonales et une collection d'énoncés d'exemples pour l'analyse.
Des énoncés de ToBI et de la Boston Radio News Corpus ont été utilisés pour l'évaluation des règles de génération : on a déterminé l'erreur quadratique moyenne et la corrélation entre les contours générés et les contours originaux et, dans une expérience de perception des locuteurs natifs ont estimé que la qualité des contours resynthétisés s'avérait naturel dans l'ensemble. De même ils n'ont constaté que de petites différences par rapport aux originaux correspondants.
Une approche variationnelle et robuste est proposée pour le recalage de signaux 1D et appliquée au calcul des géodésiques de formes pour la classification.
L'approche est ensuite étendue au recalage d'images de séquences de formes.
Cette approche de recalage basé-géométrie est plus adaptée aux images peu contrastées pour lesquelles le recalage basé-intensité trouve toutes ses limites.
Une étude de validation est menée sur des signaux et des images issus d'archives biologiques marines, qui présentent une grande variabilité interindividuelle, où les approches de recalage sont d'un intérêt tout particulier.
Le but de cette étude a été d'examiner les capacités de compréhension de parole de quatre patients sourds après implantation de la prothèse cochléaire Nucleus 22-canaux (N-22).
Les expériences ont été menées dans deux conditions.
D'abord, des tests d'intelligibilité (voyelles isolées, mots bisyllabiques et parole continue, en listes fermées et ouvertes) ont été menés avec les frontières de fréquence par défaut (DFBs) du processeur de parole de la prothèse.
Ces DFBs de chaque électrode, spécifiées par le logiciel du système, sont censées avoir été optimisées pour l'Anglais.
Les patients ont été alors de nouveau testés avec ces valeurs de fréquence s modifiées (MFBs) sur le même matériau de parole.
Pour chaque classe de sons de parole, les résultats obtenus dans les deux cas ont été comparés.
Les meilleures performances observées, au moins sur certaines classes de sons, avec les fréquences modifiées (MFBs) suggèrent que l'ajustement, en fonction de la langue, de la carte des associations fréquence-électrode de la prothèse N-22 peut améliorer, au moins en partie, la compréhension de parole.
Les tests d'articulation standards ne sont pas toujours suffisamment sensibles pour être à même de discriminer entre échantillons de parole hautement intelligibles.
On peut augmenter la sensibilité de ces tests en brouillant ces échantillons avec du bruit.
De cette manière, de petites différences d'intelligibilité sont amplifiées dans les tests d'articulation.
Nous avons utilisé deux tests pour évaluer l'intelligibilité de neuf techniques différentes de codage de la parole : un test d'articulation conventionnel et un test d'interférence monosyllabique adaptatif.
Ces deux tests ont abouti à différentes types de réponses.
Ces divergences peuvent être expliquées par le fait que des méthodes de codages différentes encodent des propriétes acoustiques et phonétiques différentes.
Certaines de ces p ropriétés sont plus facilement masquées par du bruit que d'autres.
Nos résultats montrent que dans le cas de la parole synthéique des différences d'intelligibilité ne sont pas toujours amplifiées par l'addition de bruit : elles peuvent même disparaître.
Etudiée dès l'Antiquité, la rhétorique, ou art de l'argumentation, a été tour à tour thème central d'enseignement, puis objet de nombreuses critiques.
Elle a été finalement réhabilitée en tant que support fondamental au développement des sciences et techniques de la communication.
En effet, ces dernières décennies, l'apparition des systèmes de dialogues, de génération du langage naturel et les systèmes multi-agents autour d'un noyau central d'argumentation ainsi que des développements de modélisations basées sur des approches formelles et informelles de la représentation de l'argumentation ont étayé cette réhabilitation.
Cet article propose une synthèse de ces travaux.
Il s'attache à mesurer l'adéquation de différentes approches à la modélisation de l'argumentation naturelle et souligne les difficultés qui restent à surmonter.
Le système de planification de l'argumentation APLA basé sur le modèle MARINE, développé par les auteurs dans le but d'atténuer ces difficultés est également présenté dans cet article.
L'analyse de parole montre que les transitions du second formant dans des séquences voyelle–voyelle n'ont pas toujours la même durée que celle des transitions du premier formant et qu'elles ne sont pas toujours synchronisées.
De plus, les transitions des formants ont souvent des directions différentes de leurs cibles finales.
Pour étudier si ces déviations de linéarité et de synchronisation sont perceptivement significatives, des tests de perception on été conduits avec la paire de voyelle /a/–/i/.
On a montré que des retards entre les transitions du premier et du second formant inférieur à 30 ms ne sont pas perçus, ainsi que des différences en durée inférieur à 40 ms si les premier et second formants débutant ou bien se terminent en même temps.
Si la transition du second formant est symétrique dans le temps par rapport au premier formant, des différences inférieur à 50 ms sont tolérées.
Des excursions dans la forme de la transition du second formant inférieures à 500 Hz ne sont pas perçues.
Ces résultats suggèrent que la plupart des déviations de linéarité et de synchronisation trouvées des séquences naturelles de voyelle–voyelle ne sont pas perceptivement significatives.
Grâce aux méthodes transformationnelles, l'acceptabilité est devenue la mesure et le système de classification par excellence de la linguistique.
De tous les paramètres possibles qui peuvent influencer les résultats de ce test, le problème soumis au lecteur dans cet article est celui de l'interprétation sémantique.
L'auteur essaie de démontrer la nécessité de dissocier tout facteur extra-linguistique (y compris l'univers du discours de celui qui parle, la culture à laquelle il appartient, l'importance de certains facteurs de connaissance ou ignorance du monde) d'une interprétation hypothétique linguistiquement pertinente étroitement liée à la syntaxe et à la morphologie.
Les concepts sémantiques extra-linguistiques sont 'naturels', peuvent être facilement évoqués et filtrent la recherche des concepts sémantico-linguistiques beaucoup plus abstraits, moins connus et probablement inconscients.
Ceci représente la limite qu'il faut établir dans la sémantique entre ce qui appartient au domaine de la linguistique et ce qui n'y appartient pas, c'est-à-dire entre ce qui doit, et ce qui ne doit pas être considéré comme appartenant à la grammaire générative.
Il faut, d'ailleurs, insister à ce sujet sur le besoin qui existe dans la linguistique de contrôler et donc de varier les univers du discours qui peuvent modifier l'acceptabilité d'une phrase donnée.
Dans cet article, la discussion est centrée sur les divers usages du verbe 'planter'.
Dans le domaine de l'analyse de scène en vision par ordinateur, un compromis doit être trouvé entre la qualité des résultats attendus et les ressources allouées pour effectuer les traitements.
Une solution flexible consiste à utiliser un système de vision adaptatif capable de moduler sa stratégie d'analyse en fonction de l'information disponible et du contexte.
Dans cet article, nous décrivons comment concevoir et évaluer un système d'attention visuelle conçu pour interagir avec un système de vision de façon à ce que ce dernier adapte ses traitements en fonction de l'intérêt (de la saillance) de chaque élément de la scène.
Nous proposons également un nouvel ensemble de contraintes nommé PAIRED, permettant d'évaluer l'adéquation du modèle à différentes applications.
Nous justifions le choix des systèmes dynamiques par leurs propriétés intéressantes pour simuler la compétition entre différentes sources d'informations.
Nous présentons enfin une validation à travers différentes métriques montrant que nos résultats sont rapides, hautement configurables et pertinents.
L'hypothése de la classe fermée affirme que les mots fonctionnels jouent un roˆle privilégiédans les processus syntaxiques.
On fait, en production de langage, la conjecture suivante : les mot fonctionnels sont intrinséques et non superposés au squelette de la phrase ; autrement dit, ils peuventeˆtre identifiésáce squelette.
Deux expériences ont testécette hypothéseál'aide de la procédure de facilitation syntaxique.
Dans chacune de ces expériences, les sujets avaient tendanceáproduire des phrases de structure syntaxique semblableácelle des phrases qui leurétaient présentées aupravant ; les structures des phrases générées par les sujets variaent en fonction des différences dans les structures des phrases qui leurétaient présentées.
Les changements intervenant parmi leséléments de la classe fermée de ces phrases n'avaient aucun effet sur cette tendance au deláde l'impact des changements de structures.
Ces résultats suggérent que les morphémes libres de la classe fermée ne sont pas des composants inhérents au squelette structural de la phrase.
Dans les situations de communication quotidiennes, toutes les parties du message parlé ne sont pas prononcées de manière également claire.
En particulier, les mots portant une lourde charge d'information sémantique sont mis en évidence par le locuteur.
L'objet de cette étude est de savoir comment cela est réalisé dans la parole spontanée et dans la parole lue et si la connaissance ainsi acquise pourra être appliquée à la synthèse pour en améliorer le naturel et l'acceptabilité.
En introduisant un modèle “crêtes et paliers”, nous examinons des aspects spectraux et temporels de mots mis en évidence et de mots non mis en évidence extraits de matériaux identiques, spontanés et lus après transcription orthographique.
On a enregistré un locuteur masculin professionnel dont la voix avait également servi pour la composante à diphones du programme national hollandais de synthèse de parole.
Pour un certain nombre de paramètres acoustiques, on peut conclure qu'il y a une différence nette, à la fois en “valeurs de crêtes” et en “valeurs de paliers”, entre les deux styles de parole naturelle, bien que des contrastes comparables apparaissent dans les deux styles.
Let résultats de nos mesures en parole naturelle ont été comparés aux données fournies par les mêmes textes synthétisés à l'aide du système à diphones hollandais de conversion texte-parole.
Au cours d'une expérience pilote dans laquelle les aspects temporels de la parole synthétique ont été modulés, il a été demandé aux sujets de juger du naturel et de l'intelligibilité afin de pouvoir déterminer le point de départ d'une future évaluation de la synthèse texte-parole incluant des contrastes “crêtes et paliers”.
L'article décrit un détecteur de formants simple destiné à être utilisé avec implant cochléare multicanaux.
Treize patients implantés se sont entraînés avec un processeur qui traitait la fréquence du deuxième formant, la fréquence fondamentale et l'enveloppe du signal de parole (F 0 F 2).
Neuf patients ont été entrînés avec un processeur qui présentait la fréquence du premier et deuxième formant, la fréquence fondamentale et l'amplitude du premier et deuxième formant (F 0 F 1 F 2).
Le groupe F 0 F 1 F 2 a obtenu de meilleures performances en discrimination et en reconnaissance de mots et de phrases à l'audition seule.
Au cors d'une tâche de détection de parole, le même groupe a également démontré une amélioration significativement plus forte lorsque l'audition et la lecture labiale étaient associées qu'en lecture labiale seule.
Un étude de reconnaissance de spondée en présence de bruit à l'aide de la seule audition a aussi indiqué que l'information fournie par F 1 produisait une amélioration équivalente à une augmentation de 5 dB du rapport signal sur bruit.
Nous étudions dans cet article le problème de la combinaison de classifieurs binaires.
Cette approche consiste à résoudre un problème de discrimination multi-classes, en combinant les solutions de sous-problèmes binaires;
nous nous intéressons aux stratégies opposant chaque classe à chaque autre, et chaque classe à toutes les autres.
La combinaison est considérée ici du point de vue de la théorie de Dempster-Shafer :
les sorties des classifieurs sont ainsi interprétées comme des fonctions de croyance, conditionnelles ou exprimées dans un cadre plus grossier que le cadre initial.
Elles sont combinées en calculant une fonction de croyance consistante avec les informations disponibles.
Les performances des deux approches sont comparées à celles d'autres méthodes et illustrées sur divers jeux de données.
La complexité des relations entre l'information acoustique du signal de la parole et les catégories phonétiques des locuteurs adultes est bien connue.
On recherche si les mêmes relations complexes existent entre les signaux de la parole et les catégories perceptives prelinguistiques chez les nourrissons.
Pour deux classes de relations la manière dont se font la catégorisation de l'information pour la parole est virtuellement identique pour les adultes et les nourrissons.
Ces résultats indiquent que les bébés possèdent des capacités perceptives linguistiquement pertinentes finement ajustées qui facilitent et orientent leur acquisition du langage.
Nous avons adapté un algorithme de reconnaissance de mots pour une application en téléphonie mobile.
Pour ce faire, nous avons évalué plusieurs manières de générer des vecteurs de traits en utilisant deux bases de données collectées dans une petite voiture roulant à 120 km/h.
Les bases de données contiennent des chiffres et des séquences de chiffres pour 10 locuteurs utilisant un combiné téléphonique ou agissant en mode “main libre”.
Dans un premier temps, nous avons utilisé comme vecteur de traits les coefficients d'un banc de filtres redimensionné linéairement.
Avec un apprentissage sur des mots isolés et en utilisant le combiné, cette approche donne un taux d'erreurs de 6% sur une séquence de 7 chiffres.
La deuxième approache remplace les coefficients du banc de filtres par des coefficients d'énergie, l'énergie étant redimensionnée logarithmiquement.
Ces résultats obtenus dans un environnement simulé ont été vérifiés dans une automobile avec notre appareillage.
Nous proposons dans cet article une nouvelle méthode d'extraction de caractéristiques appliquée à la reconnaissance de phonèmes.
Le modèle proposé : le codage neuronal prédictif (NPC pour Neural Predictive Coding) et ses deux déclinaisons NPC-2 et DFE-NPC (Discriminant Feature Extraction - NPC), est un modèle connexionniste de type perceptron multicouches (PMC) basé sur la prédiction non linéaire du signal de parole.
Nous montrons qu'il est possible d'améliorer les capacités discriminantes d'un tel codeur en exploitant des informations de classe d'appartenance phonétique des signaux dès l'étape d'analyse.
À ce titre, il entre dans la catégorie des extracteurs DFE déjà proposés dans la littérature.
Dans cette étude, nous présentons une validation théorique du modèle dans l'hypothèse de signaux respectivement non bruités et bruités (bruit additif gaussien).
Les performances de l'extracteur NPC pour la classification de phonèmes sont comparées avec celles obtenues par les méthodes traditionnellement utilisées en extraction de caractéristiques sur des signaux des bases Darpa Timit et Ntimit.
Les simulations présentées montrent que les taux de reconnaissance sont nettement améliorés, en particulier dans le cas de phonèmes de la langue anglaise fréquents mais réputés délicats à catégoriser.
Enfin, une application en reconnaissance de mots isolés et petit vocabulaire est présentée dans le but de montrer comment l'on peut insérer les paramètres NPC dans une application de reconnaissance à l'aide d'un système mixte ANN-HMM (Artificial Neural Networks - Hidden Markov Models).
Comme reporté à IVTTA-94, le service de numérotation vocale (`VoiceDialingSM') de NYNEX a été d'abord déployé dans la région de NYNEX à la mi 1993.
Depuis il a été déployé dans les régions de plusiers opérateurs Bell et de nouveaux développements importants ont été effectués.
Un de ces développements a été la transition, commencée en 1995, d'une implantation hardware d'un algorithme de reconnaissance à base de DTW vers une implantation sur un DSP à usage général d'un algorithme de reconnaissance à base de HMM ayant des densités continues multigaussiennes.
Ceci a permis une extension du service d'une simple reconnaissance de noms en mode dépendant du locuteur en ajoutant la reconnaissance en mode indépendant du locuteur des chiffres en continu et des mots de commande.
Ce papier décrit les principaux travaux conduits lors de cette transition et fournit une description plus détaillée des composantes du système de reconnaissance de la parole.
Pour résoudre ce problème, nous proposons un nouveau modèle de langue, les “n-grammes multi-classes composites”.
Dans les classes de mots, les commutations possibles en chaque position sont retenues comme attribut des n-grammes, et une classe différente est créée pour chaque position.
Les n-grammes multi-classes sont étendus en n-grammes multi-classes composites en considérant les n-grammes de longueurs supérieures les plus fréquents qui représentent les suites de mots les plus fréquentes.
Lors des expériences réalisées, nous avons constaté d'une part une diminution de la perplexité de 9,5% et d'autre part une diminution de 16% du taux d'erreurs en nombre de mots non reconnus en reconnaissance de la parole, ce avec une réduction de 40% de la taille des paramètres par rapport aux trigrammes conventionnels.
Le locus de la consonne et le nucleus sont significativement plus proches dans les mots de la parole spontanée que dans les mêmes mots lus, dans les syllabes non proéminentes que dans les syllabes proéminentes, dans les mots apparaissant pour la seconde fois que dans les mots nouveaux.
Ces resultats peuvent refléter des différences de coarticulation, qui peuvent être dues à un effet d'anticipation plus grand de la voyelle sur la consonne précédente et/ou un degré plus élevé d' “undershoot” du F 2 dans la parole spontanée.
La distance du locus au nucleus semble dépendre en partie de la durée de l voyelle ce qui confirme le modèle de Lindblom (1963), mais elle paraît aussi être influencée par d'autres facteurs tels que la situation de communication.
Le bruit perçu dans les segments du signal vocal est du aux irregularites des oscillations de la glotte et au bruit additif en présence d'un fuseau glottique.
On peut déterminer la contribution du bruit à l'aide du SNR (signal-to-noise ratio).
Dans la première partie de cette étude, nous discutons les avantages et les inconvénients des méthodes de mesure du SNR pratiquées couramment.
Dans la deuxième partie, nous présentons une nouvelle méthode pour mesurer le SNR qui reconstruit les oscillations harmoniques par un algorithme utilisant le principe de la synthèse par analyse.
Enfin, nous étudions les voyelles synthétiques et le signal vocal émis par des locuteurs normaux et dysphoniques.
La mesure du SNR ne permet qu'une différenciation globale des pathologies.
Les problèmes de la reconnaissance des effets de bruit sont également discutés.
Nous décrivons une méthode de lecture automatique des chaînes de caractères sur cartes scannées.
Cette méthode prend en compte les chaînes orientées qui sont fréquentes sur les cartes.
L'application se décompose en trois modules principaux : analyse des particules connexes, chaînage des hypothèses, recherche des caractères connectés.
Les résultats obtenus semblent suffisants pour passer au stade opérationnel à condition d'utiliser quelques règles syntaxiques de haut niveau pour améliorer la détection des cas douteux.
Cet article décrit notre système de reconnaissance de mots isolés à grand vocabulaire Dspell.
Le système utilise un modèle à base de diphones et un algorithme de décodage des mots très efficace. Le système est implanté sur le multiprocesseur emma-2∗ d'ELSAG.dspell permet un apprentissage rapide et exhibe un taux de reconnaissance élevé avec un temps de réponse très court sur des vocabulaires allant jusqu'à 10,000 mots.
Cet article traite du problème du débruitage de la parole pour les radiocommunications mobiles.
Nous proposons ici de nouvelles méthodes de réduction de bruit qui se sont avérées satisfaisantes à la suite de tests informels d'écoute : en approche monovoie, une technique de “soustraction spectrale modifiée” incluant la surestimation du bruit dépendant de la fréquence et la segmentation du signal est proposée.
Dans le cas bivoies où les bruits sont décorrélés ou faiblement corrélés, l'approche développée fait apparaître l'intérêt de la fonction de cohérence entre signaux en tant que critère de discrimination entre une source localisée et une source diffuse;
une nouvelle technique basée sur la mesure de la cohérence entre observations est utilisée pour filtrer les observations;
utilisée comme outil de détection d'apparition du signal dans le cas de bruits décorrélés, cette fonction présente également un grand intérêt pour toute méthode de débruitage nécessitant un système de détection.
Ces méthodes sont finalement comparées en fonction de leur complexité de mise en œuvre et de leurs performances.
Il apparaît que la soustraction spectrale modifiée est prometteuse dans le cas de bruits stationnaires et que dans le cas de bruits non stationnaires et décorrélés, la méthode basée sur la cohérence est plus attractive.
Un test d'intelligibilité a permis d'évaluer différents synthétiseurs à partir du texte à l'aide de vingt phrases sémantiquement imprédictibles générées dans chacune des cinq structures syntaxiques retenues.
Ces structures de base ont été définies dans le cadre d'une méthodologie translinguistique visant à la génération de corpus dans un contexte européen.
Les réponses des vingt auditeurs ont été également analysées.
Leurs distributions montrent une forte relation entre la proportion des phrases (p s) et celle des mots (p w) correctement retranscrits.
Le rapport de leurs logarithmes r = Log(p s)/Log(p w) apparaît comme un indice robuste de la complexité d'un message oral.
Des données extraites de la littérature confirment l'hypothèse selon laquelle plus une phrase contient d'informations contextuelles (sémantiques, syntaxiques, etc.), plus faible est cet indice r.
Dans cette optique, l'index r pourrait être relié au nombre d'unités de décision qu'un auditeur doit traiter en écoutant une phrase.
Les synthétiseurs de parole distordent ici la compréhension des phrases.
Or, les omissions et les erreurs n'obéisser pas à la loi binomiale qui régitrait leur distribution si l'on considérait un modèle simplifié dans lequel les unités auraient la même probabilité indépendente d'être correctement identifiées.
L'analyse des divergences entre l'observation et la théorie issue de ce modèle simpliste explique clairement le fait que les relations linguistiques entre unités corrigent des mots “théoriquement incompris” et entachent d'erreur des mots “théoriquement compris”.
Ce phénomène de correction/distorsion dépend essentiellement du contenu linguistique des phrases, lequel peut être quantifié au moyen de l'indice r suggéré.
Cet indice présente également des variations du second ordre dues à d'autres facteurs tels que la compétence des sujets, leur entraînement, ou le niveau de dégradation acoustique du message.
Dans la plupart des systèmes d'identification de locuteurs on suppose que tous les locuteurs donnent lieu à la même matrice de covariance.
Cette hypothèse conduisant à un classificateur linéaire simplifie l'algorithme de classification.
Cependant, les sujets ne diffèrent pas seulement par leurs vecteurs caractéristiques moyens mais aussi par leurs matrices de covariance.
L'utilisation d'une matrice de covariance individuelle, propre à chaque sujet, conduit à un classificateur quadratique.
Si on fait l'hypothèse que la matrice de covariance est commune à tous les locuteurs, un espace d'indices optimal unique peut être déterminé.
Avec une matrice de covariance individuelle, chaque personne est reconnue dans un espace d'indices propres.
La procédure de reconnaissance exige par conséquent la comparison d'un locuteur inconnu avec des référence définies dans des espaces différents.
L'utilisation du classificateur quadratique avec des espaces d'indices individuels augmente considérablement la précision du processus de reconnaissance alors que l'exigence en mémoire supplémentaire est négligeable.
Le classificateur quadratique proposé basé sur des vecteurs caractéristiques optimisés individuellement a été testé à l'aide d'un système d'identification utilisant six locuteurs masculins.
Pour une mesure de séparation donnée, ce classificateur améliore la performance de deux fois par rapport à l'approche conventionnelle.
Les comparaisons interlangues peuvent éclaircir les niveaux de traîtement impliqués dans l'exécution des tâches psycholinguistiques.
Par exemple, si l'on observe le même type de réponses, que les sujets comprennent le matériel expérimental ou non, on peut en conclure que ces résultats ne reflètent point des processus linguistiques de haut-niveau.
Dans cette expérience des auditeurs français et anglais ont accompli deux tâches - localisation de clics et détection de clics accélérée - en phrases françaises, et en phrases anglaises.
Ces phrases étaient bien appareillées au niveau de leur structure syntaxique et phonologique. Les clics étaient localisés avec plus de précision dans les mots à contenu que dans les mots fonction en anglais, mais non en français.
Les deux groupes d'auditeurs ont manifesté ces mêmes résultats ; ce qui semble indiquer que les processus linguistiques de haut-niveau ne jouaient aucun rôle dans la performance des auditeurs.
En conclusion on peut dire que la détection de clics est une tâche qui est sensible principalement aux effets de bas-niveau, par exemple des effets acoustiques, et donc ne se prête guère à l'étude des processus linguistiques.
Cet article présente une étude des coordinations temporelles entre les différents articulateurs du Langage Parlé Complété.
Le Langage Parlé Complété (LPC) est un augment manuel de la lecture labiale.
Il est composé de clés digitales réalisées à l'aide de la main placée à différentes positions particulières sur le côté du visage afin de désambiguïser des syllabes de type CV.
Le mouvement de la main, le geste des lèvres ainsi que le signal acoustique ont été analysés à partir de l'enregistrement d'une codeuse diplômée en LPC prononçant et codant un corpus constitué d'un ensemble de syllabes.
L'expérience I analyse la position de la main en relation avec le mouvement des lèvres et le son correspondant.
Les résultats montrent que le mouvement de la main peut débuter jusqu'à 239ms avant le début de la réalisation acoustique de la consonne de la syllabe CV.
La main atteint sa cible durant la consonne et bien avant la cible vocalique aux lèvres.
Les résultats montrent que la clé digitale se met en forme durant le déplacement de la main d'une position à l'autre.
Les deux expériences montrent ainsi une anticipation du geste de la main sur celui des lèvres.
Le contrôle de l'information sur la consonne et sur la voyelle transmise par la main est discuté dans le cadre de la coarticulation en parole.
Enfin la coordination temporelle observée entre les articulateurs du LPC et le son correspondant a permis de définir des règles pour le contrôle d'un système audiovisuel délivrant des syllabes CV en Français.
La biométrie, qui consiste à identifier un individu à partir de ses caractéristiques physiques ou comportementales, connaît depuis quelques années un renouveau spectaculaire dans la communauté du traitement du signal.
Elle a aussi reçu une attention accrue de la part des médias depuis les tragiques événements du 11 septembre 2001.
Dans cet article nous introduisons tout d'abord la notion de biométrie.
Nous décrivons l'architecture d'un système biométrique ainsi que les métriques utilisées pour évaluer leur performance.
Nous donnons un bref aperçu des technologies biométriques les plus courantes et des moyens de les fusionner pour obtenir des systèmes multimo-daux.
Nous présentons enfin les applications possibles de la biométrie.
Cet article etude comment les participants a une conversation coordonnent leur utilisation et lent interprétation du langage dans un contexte restreint.
Cette etude repose sur l'analyse de descriptions spatiales qui sont apparues au cours de 56 dialogues obtenus en laboratoire en utilisant un jeu de labyrinthe sur ordinateur specialement conçu a cette fin.
Nous avons effectué deux types d'analyses.
D'abord, une analyse sémantique des differents types de description qui indique comment des couples de locuteurs développent differents schémas linguistiques associés a differents modèles mentaux de la configuration du labyrinthe.
Ensuite, une analyse de la manière dont les communicants coordonnent la mise sur pied de leurs descriptions.
Les résultats de cette etude nous paraissent suggérer que le traitement du langage au cents d'un dialogue est pent-titre régi par des principes locaux d'interaction qui ont reçu pen d'attention de la part des psychologues et des linguistes jusqu'à aujourd'hui.
Une nouvelle combinaison de méthodes de codage pour les systèmes de transmission à 64 kbit/s pour des situations typiques de vidéotéléphone est étudiée.
La structure du codeur est basée sur un codeur standard hybride DCT avec prédiction temporelle.
L'image est divisée bloc par bloc en zones changées et inchangées.
Un vecteur de mouvement de précision supérieure à la taille d'un point image est calculé et transmis pour chaque bloc de la zone changée.
Pour l'analyse progressive, l'erreur de prédiction par bloc est calculée dans l'image entière.
Seuls les blocs avec les plus grandes erreurs de prédiction sont mis à jour par une transformée discrète en cosinus (DCT) et une quantification adaptive perceptuelle.
Le nombre de blocs mis à jour par DCT dépend des bits restant après la transmission de l'information de gestion.
Le codeur est régulé par l'analyse progressive de l'erreur de prédiction et non par rétroréglage á l'aide de mémoires-rampons.
La résolution spatiale du signal de source est réduite en deux étapes pour empêcher la saturation du codeur par une trop grande activité entre deux trames.
On propose que l'idée tous les F sont G est souvent interprétée comme “neutre au point de vue structure” c'est-à-dire comme tous, F, G sans distinction sujet-prédicat.
Dans une premiére expérience, on demandeàdes enfants agés de 7–8 ans et de 11–12 ans ainsi qu'àdes adultes d'agir selon des instructions du type “Faîtes un baˆtiment oùtous les blocs jaunes sont carrés”.
Dans une seconde expérience faite avec des meˆmes groupes d'aˆge, on présente comme prémise majeur de syllogismes, des propositions de la forme tous les sont G qui varient selon les relations d'inclusion des faits exprimés.
Les résultats montrent qu'il y a, sous certaines conditions, présence d'interprétations “neutres au point de vue structure” chez les adultes aussi bien que chez les enfants.
Les résultats indiquent aussi l'existence chez tous les sujets d'un mode de “traitement pragmatique” qui devient moins nécessaire avec l'aˆge.
Dans les interprétations pragmatiques, le sens est déterminéplus par les relations factuelles connues entre les choses représentées par les mots que par les relations grammaticales entre les mots.
Dans cette contribution, nous proposons d'évaluer la qualité de différents algorithmes de réduction de la palette couleur d'une image.
Deux techniques originales sont particulièrement détaillées (avec deux variantes pour chacune d'elles) : l'une basée sur la transformation du boulanger et l'autre employant la matrice des palettes locales.
Dans la campagne d'évaluation, les résultats de ces deux techniques sont comparés à ceux d'algorithmes standards tels que « median cut » , « octree » et « split & merge » .
L'évaluation se veut à la fois objective (utilisation d'une métrique et de descripteurs locaux de qualité) et subjective (utilisation d'expériences psycho-visuelles).
En effet, l'usage seul d'une métrique n'intègre pas la notion de HVS (Human Visual System).
La couleur étant davantage considérée comme propriété perceptuelle que comme donnée quantitative, une mesure classique ne peut décrire correctement l'altération que subit une image lors d'une réduction des couleurs.
Les résultats de la campagne d'évaluation psycho-visuelle montrent une fois de plus que les métriques classiques sont souvent en contradiction avec la perception humaine de la couleur.
Cet article présente une étude portant sur la capacité de la prosodie à les frontières du discours subséquentes.
Plus spécifiquement, on a étudié si la fin proche d'une description d'itinéraire peut être signalée en avance par des caractéristiques mélodiques et temporelles.
L'experience 1 fait apparaître que les auditeurs sont capables d'estimer, sur la base de ces propriétés prosodiques, à quelle distance de la fin d'une description se trouve un énoncé donné.
Toutefois, la porteé de cette prédiction prosodique est relativement limitée : les auditeurs ne peuvent estimer la position absole dans le discours que pour les deux dernières phrased du monologue analyse.
L'experiénce 2 a été menée pour explorer systématiquement, par un test en parole de synthèse, dans quelle mesure les propriétés mélodiques et de durée suffisent pour influencer les jugements de finalité.
Au cours de ces dernières années, les systèmes de reconnaissance automatique de la parole ainsi que les systèmes de synthèse de la parole à partir du texte ont atteint une qualité leur permettant d'être intégrés à des applications quotidiennes.
Un problème demeure cependant quant à ces applications, à savoir, l'expansion fréquente des inventaires de phones d'une langue spécifique à des phones issus d'autres langues, problème d'autant plus important dans un monde où les services automatisés de la parole multilingues sont un souhait.
Cet article examine la nature de l'expansion des phones en suédois.
Nous discuterons du statut de ces phones, et, puisque ces phones supplémentaires n'ont pas une fonction phonémique (ou allophonique), nous proposons le terme de `xénophones'.
Les résultats montrent que peu d'informants ont recours à une rephonématisation complète, l'expansion xénophonique étant la règle, bien que la distribution soit inégale selon les phonèmes, allant de ceux produits par la plupart des informants à ceux produits par très peu d'informants.
Parmi les facteurs explicatifs potentiels analysés, tels les origines régionales, le sexe, l'âge et le niveau d'étude, ce dernier s'avère de loin être le plus important.
Dans tout dialogue, il existe au moins deux sortes de frontiéres entre unités de discours.
Un premier type de frontière signale la fin d'une unité de thème du discours ; l'autre tupe de frontiére indique la fin d'un tour de parole.
Ces deux types de frontiéres ne coïncident pas nécessairement, car le locuteur peut souhaiter passer à autre sujet sans vouloir être interrompu par son interlocuteur.
Pour tester si des indices prosodiques peuvent permettre de distinguer, de fçon non ambigue, les frontières de théme des frontières de tour de parole, une série d'expériences de production de parole a été menée. Les paramètres finalité-thème et finalité-tour de parole étaient manipulés de façon indépendante. Les indices visuels ainsi que les indice verbaux non-prosodiques ne pouvaient pas être utilisés.
Dans la condition la plus complexe, le locuteur devait donner des indices clairs de finalité-thème sans perdre la parole prématurément.
Dans cette condition, les locuteurs évitent d'utiliser une intonation basse pour les frontières de thèmes intra-tour de parole, et les réservent pour les frontières de thème qui sont aussi des fins de tour de parole.
Les auditeurs confrontés à des extraits hors contexte ont pu distinguer de façon fiable les unités de thème finales de tour de parole des unités non finales.
Il est intéressant de remarquer que, même quand les parties finales des unités de thème supprimées, les auditeurs continuaient à faire la distinction entre les expressions de fin de tour de parole et de non-fin de tour de parole. Apparemment, ils se basaient sur d'autres indices prosodiques, plus globaux.
Ceci a été observé également pour des unités minimalement et maximalement incomplètes.
Dans le passé, les enseignants dependaient de l'observation des sujets en classe pour decider de l'importance de diverses questions d'ordre pedagogique.
L'apprentissage des langues automatisé (ALA) nous permet de poser ces questions de manière bien plus rigoureuse.
Nous pouvons utiliser un système ALA comme base, traçant toutes les reponses de l'utilisateur et controlant strictement les informations fournies.
Nous avons utilisé le système Fluency [Proceedings of Speech Technology in Language and Learning, 1998, p. 77] de cette manière pour examiner la question du choix de la voix que l'utilisateur doit imiter lors de l'apprentissage de la prononciation.
Dans cet article nous allons nous demander s'il doit y avoir un choix de locuteur à copier et si oui, quelles caracteristiques de sa voix sont importantes dans ce choix.
Dans cet article, nous nous sommes intéressés aux problèmes de reconnaissance non- coopérative de cibles (NCTR) en tant que problème de classification supervisée.
Dans un second temps, cet algorithme a été parallélisé sur un processeur many-cœurs (GPU : Graphics Processing Unit).
Les opérations arithmétiques et le modèle d'accès mémoire ont été étudiés pour obtenir la meilleure parallélisation des calculs.
Enfin, nous terminons par une discussion autour des perspectives envisageables pour la méthode proposée, notamment en s'intéressant à d'autres espaces de représentation ou à d'autres méthodes de classification.
Les approches de modélisation d'entreprises actuelles décrivent des conceptions de tâches génériques qui ne capturent pas les pratiques du terrain.
Cependant, il existe plusieurs voies d'amélioration dans les organisations qui sont reliées aux manières particulières dont les tâches sont implantées.
Le but de cette recherche est d'enrichir la modélisation d'entreprises par une approche méthodologique pour capturer et modéliser les pratiques de travail, basée sur un modèle d'agents organisationnels et de leurs contextes.
L'approche permet l'acquisition et l'analyse de cadres de travail personnels et interpersonnels.
Ces cadres de travail sont acquis à partir d'entrepôts d'actions.
L'approche est illustrée avec une étude de cas.
Nous présentons des résultats concernant la découverte automatique de contextes personnels.
Cet article traite du problème de l'apprentissage des réseaux de neurones à fonctions radiales de base pour l'approximation de fonctions non linéaires L 2 de R d vers R.
Pour ce type de problème, les algorithmes hybrides sont les plus utilisés.
Ils font appel à des techniques d'apprentissage non supervisées pour l'estimation des centres et des paramètres d'échelle des fonctions radiales, et à des techniques d'apprentissage supervisées pour l'estimation des paramètres linéaires.
Les méthodes d'apprentissage supervisées reposent généralement sur l'estimateur (ou le critère) des moindres carrées (MC).
Cet estimateur est optimal dans le cas où le jeu de données d'apprentissage (z i,y i)i=i,2,..,q est constitué de sorties y i, i = l,..,q bruitées et d'entrées z i, i = l,..,q exactes.
Cependant lors de la collecte des données expérimentales il est rarement possible de mesurer l'entrée z i sans bruit.
L'utilisation de l'estimateur des MC produit une estimation biaisée des paramètres linéaires dans le cas où le jeux de données d'apprentissage est à entrées et sorties bruitées, ce qui engendre une estimation erronée de la sortie.
Cet article propose l'utilisation d'une procédure d'estimation fondée sur le modèle avec variables entachées d'erreurs pour l'estimation des paramètres linéaires (pour l'apprentissage supervisé) dans le cas où le jeux de données d'apprentissage est à entrées et sorties bruitées.
L'interprétation géométrique du critère d'estimation proposé est établie afin de mettre en évidence son avantage relativement au critère des moindres carrés.
L'amélioration des performances en terme d'approximation de fonctions non linéaires est illustrée sur un exemple.
La reconnaissance automatique de séquences de chiffres connectés (séquences composées des chiffres zéro à neuf, et oh forme un domaine important de la reconnaissance de la parole.
Les applications de cette technologie comprennent les autorisations de cartes de crédit, la commande sur catalogue, la composition de numéros de téléphone et la saisie de données.
Depuis deux ans, AT&T a expérimenté un système de reconnaissance automatique de codes marchands d'identification de 10 chiffres et de numéros de cartes de crédit de clients de 15 chiffres, dans le but de permettre des achats par carte de crédit.
Notre évaluation a utilisé des données récoltées chez 1000 clients qui ont produit 2000 séquences de chiffres connectés au travers du système téléphonique basé sur la composition de chiffre 800.
Notre système a correctement reconnu 97% des séquences de chiffres sans rejet en utilisant des contraintes sur la validité du code marchand d'identification et du numéro de carte de crédit.
Plusieurs schémas d'application de ces contraintes dans une implémentation pratique sont discutés dans l'article.
De même, la reconnaissance du montant en dollars de la transaction est présentée, avec quelques résultats préliminaires.
Avec une cochlée artificielle, des personnes mal entendantes sont capables de comprendre la parole lorsque les conditions d'écoute sont bonnes.
Toutefois, en présence de bruit ou de réverbération la compréhension pose des problèmes.
Des tests ont été menés pour améliorer l'intelligibilité de la parole bruitée en utilisant deux techniques différentes de suppression de bruit à un seul canal.
Ceci fut réalisé par prétraitement, c'est a dire en réinjectant a la cochlée artificielle le signal de parole résynthétisé.
Des tests d'intelligibilité ont été menés en collaboration avec un service médical.
Des instabilités de la source vocale apparaissent, en parole normale, sous certaines conditions (cris de nourrisson, voix “cassée”, etc.) et sont aussi symptomatiques des voix pathologiques.
Ces instabilités sont intimement liées aux bifurcations dans le système dynamique non-linéaire sous-jacent.
Dans cet article, nous analysons les bifurcations dans un modèle à 2 masses des cordes vocales et étudions, en particulier, comment l'incorporation du conduit vocal modifie les diagrammes de bifurcation.
Une comparaison d'un modèle simplifié (Steinecke et Herzel, 1995) et d'une version étendue incluant les résonances du conduit vocal révèle que les caractéristiques essentielles des diagrammes de bifurcation (c'est à dire, le verrouillage en fréquence à la fois des oscillations des cordes et des oscillations torroïdales) sont observées dans les deux types de modèles.
Toutefois, les instabilités vocales apparaissent, dans le modèle étendu, pour des pressions sous-glottiques plus basses et même pour des assymétries faibles.
L'analyse et la reconnaissance des documents écrits consistent à traduire leurs images numérisées sous une forme électronique réutilisable.
L'analyse permet d'extraire à partir de l'image d'un document une structure dite physique, tandis que la reconnaissance associe aux composants de la structure physique leurs fonctions logiques dans le document.
Le travail présenté dans cet article porte sur la phase de reconnaissance de documents dont la structuration logique est caractérisée par des marquages typographiques tels que les sommaires ou les tables des matières.
Nous proposons une approche perceptuelle qui se base sur l'extraction de ces marquages typographiques directement à partir des images des documents.
Ces documents présentent cependant une structuration variable et complexe.
Notre objectif est d'aborder ce problème de reconnaissance en présence de ces difficultés.
Nous avons développé un système de reconnaissance automatique basé sur un modèle hybride combinant un classifieur bayésien et un automate probabiliste.
Le rôle du classifieur est la correspondance entre les blocs de texte extraits dans les images des documents et les entités logiques à un niveau de structuration de base, alors que l'automate permet de regrouper ces entités logiques sur plusieurs niveaux hiérarchiques reconstruisant ainsi toute la structure logique.
Ce modèle hybride est construit par apprentissage semi-supervisé, en s'appuyant d'une part sur la connaissance fournie de manière interactive par l'utilisateur, et d'autre part sur les propriétés typographiques des documents considérés.
Nous avons expérimenté le système proposé pour l'indexation de sommaires de revues.
La complexité et la variabilité de la structuration de ces documents nous ont permis de montrer l'efficacité de l'approche développée.
Diverses applications du traitement de la parole utilisent la préaccentuation du signal aux fréquences élevées.
Dans cet article, nous étudions l'effet de la préaccentuation sur la performance de reconnaissance de voyelles.
La préaccentuation est réalisée par différentiation du premier ordre.
Les coefficients cepstraux, obtenus à partir d'une analyse par prédiction linéaire, sont utilisés comme paramètres de reconnaissance.
Il est fait usage d'un classificateur par minimum de distance,et la performance de reconnaissance est étudiée pour quatre mesures de distance différentes : la distance euclidienne, la distance par corrélation, la distance de Mahalanobis et la distance d'Itakura.
Nous montrons que la préaccentuation conduit à une détérioration du taux de reconnaissance des voyelles.
Des implications de ce résultat pour la reconnaissance de mots isolés sont également discutées.
Dans ce papier, nous traitons des structures syllabiques et de leur variation dans un corpus de parole en français issu d'entrevues radio-diffusées.
Un des buts est de montrer comment des systèmes de reconnaissance automatique de la parole (RAP) peuvent servir d'outils linguistiques pour explorer de façon cohérente des corpus virtuellement illimités.
Des sous-ensembles automatiquement sélectionnés peuvent être vérifiés manuellement pour accroître notre connaissance des variantes de prononciation.
Pour se focaliser sur elles, une méthodologie a été mise au point, utilisant des descriptions aux niveaux phonématique, syllabique et lexical.
Cette étude repose sur un corpus de parole de radio constitué de trente émissions d'une heure.
Des phénomènes, moins bien décrits ont également été observés : d'autres voyelles telles que /u/, /ε/, /i/ et /a/ peuvent tomber en position inaccentuée (non finale).
Les syllabes CV non accentuées, précédées d'une syllabe ouverte, sont enclines à la restructuration : effacement de la voyelle et transfert attaque-coda.
Les syllabes complexes tendent à être simplifiées : les consonnes liquides tombent souvent, plus en position de coda qu'en position d'attaque.
Le /v/ est la consonne la plus facilement élidée indépendamment de sa position dans la syllabe.
Enfin un pourcentage substantiel de syllabes faibles de fin de mot, ayant un schwa comme noyau, peuvent disparaître, ainsi que les mots outils monosyllabiques indépendamment de l'identité de leur noyau vocalique.
Le problème abordé dans ce travail est celui de la reconnaissance des états affectifs d'un utilisateur à partir de mesures physiques (accéléromètres) et physiologiques (ECG, EMG...) issues de capteurs portés.
Etant donné la nature complexe de la relation entre les signaux dont nous disposons et les états affectifs à reconnaître, nous proposons d'utiliser une méthode d'apprentissage statistique.
Nous commençons par discuter des états de l'art dans les domaines de l'apprentissage statistique et de la reconnaissance d'émotions.
Nous présentons ensuite un cadre permettant de comparer les différents algorithmes d'apprentissages et leurs conditions d'utilisation.
A l'issue de cette préétude, nous proposons une architecture globale d'un système embarqué de reconnaissance en temps réel.
Au lieu de chercher à directement reconnaître les états affectifs, nous proposons de commencer par une phase de détection de changement dans les signaux et par la suite nous étiquetterons les segments identifiés entre deux ruptures.
Nous démontrons enfin l'intérêt de notre approche sur deux exemples réels.
Cet article évalue deux hypothéses qui jouent un rôle central dans des modèles récents de l'acquisition du langage : (1) la connaissance de la structure linguistique est “projetée” sur des formes préalables de connaissance non linguistique, et (2) l'acquisition d'une langue est un apprentissage continu dans lequel l'enfant passe d'une communication gestuelle précoce a la maitrise de l'expression linguistique.
Nous avons étudié l'acquisition des pronoms de premiere et deuxième personne MOI et TOI chez deux enfants sourds, nés de parents sourds, qui apprenaient l'American Sign Language (ASL) en tant que langue maternelle.
En ASL, les pronoms personnels sont formes en montrant directement du doigt l'interlocuteur (YOU) oun soi-meme (ME), et ne sont done pas des symboles arbitraires.
De ce fait, les pronoms personnels en ASL ressemblent à des gestes para-linguistiques qui accompagnent souvent la parole et sont utilisés pré-linguistiquement par les enfants sourds et entendants a partir d'environ 9 mois.
Cela permet d'étudier le passage du geste pré-linguistique à l'expression linguistique dans un cas où geste et langage appartiennent à la même modalité.
Les résultats indiquent qu'il faut un certain temps aux enfants sourds pour acquérir les pronoms, et qu'ils commettent des erreurs du type de celles que commettent les enfants entendants, en dépit de la transparence des gestes.
Au départ (les enfants dtaient respectivement âgés de 10 et 12 mois), ils montraient du doigt des personnes, des objets et des endroits.
Les deux enfants ont ensuite connu une longue pdriode d'évitement, pendant laquelle Tune des fonctions du geste (montrer les autres et soi-meme du doigt) disparut complétement.
Pendant cette pdriode, leur langage et leur développement cognitif étaient par ailleurs entièrement normaux, et ils continuérent a montrer du doigt des objets, par exemple.
Lorsqu'ils recommencèrent a montrer du doigt les autres et eux-mêmes, ils commettaient des erreurs courantes chez des enfants entendants ; un des enfants commettait des erreurs systdmatiques d'inversion, pensant que le signe TOI l désignait lui-même, alors que l'autre commettait des erreurs d'inversion non systdmatiques.
Les résultats des tâches expérimentales pour le premier enfant montrent qu'il produisait également ces erreurs en compréhension.
L'usage des pronoms MOI et TOI ne fut completement maitrisd que vets Page de 25–27 mois, ce qui correspond a Page vets lequel les enfants entenclants maitrisent ces formes.
Notre étude étaye done l'iddée qu'il existe une discontinuité chez l'enfant clans le passage de la communication prd-linguistique à la communication linguistique.
Nous essayons de montrer que l'acquisition de la structure linguistique repose vraisemblablement sur des connaissances bien dĺimitées, propres au langage.
Pour permettre à l'utilisateur d'interrompre les réponses du système, TOSBURG II utilise une technique adaptive spécifique.
Le développement d'un système de dialogue qui soit fiable en situation réelle requiert de pouvoir le perfectionner en en testant des versions prototype successives dans des contextes de dialogues réels.
Pour accroitre les performances de TOSBURG II, nous avons donc mis l'accent sur l'élaboration d'un environnement performant d'évaluation.
Contrairement à la méthode prédominante, de type Wizard of Oz, pour la collecte de données de dialogue, TOSBURG II transmet, en temps réels, les données du dialogue.
Ces données comportent non seulement les résultats de la compréhension des énoncés mais aussi les résultats des traitements intermédiaires, ce qui permet d'analyser précisement les comportements globaux du système.
La reconnaissance de parole a été utilisée pour automatiser le service des renseignements annuaire lors d'une expérimentation de six mois avec les usagers résidentiels de Bell Canada.
Ce service bilingue donnait à l'usager le choix de s'exprimer en anglais ou en français.
Plus de 89% des appels ont été partiellement ou complètement automatisés.
Les réactions des utilisateurs et des téléphonistes vis à vis du service ont été positives.
Le système de reconnaissance à vocabulaire flexible de Bell Northern Research a fourni de bonnes performances avec un vocabulaire de 1700 noms de ville et synonymes.
On a pu mettre en évidence une réduction significative, du point de vue économique, du temps de travail des opératrices.
Cet article décrit une interface pour accès vocal à un grand nombre d'applications de divertissement, navigation et de communication dans l'environnement de l'automobile par un dialogue oral homme-machine.
Le système a été développé dans le cadre du projet européen SENECA.
Il fait appel à des techniques de réduction du bruit, à la reconnaissance de la parole ainsi qu'à la gestion des dialogues.
Un aspect intéressant du système réside dans le fait que les résultats à faible confidence du module de reconnaissance ainsi que les ambiguı̈tés au niveau du mot sont compensés par un dialogue de clarification flexible que le système mène avec l'utilisateur.
Un prototype de SENECA à été testé par des utilisateurs réels.
La sécurité routière peut être améliorée de manière significative en utilisant la parole, notamment pour manipuler des tâches complexes.
L'impression d'être distrait de la conduite est également moins prononcée en utilisant la parole par rapport à l'interaction manuelle.
Cet article présente les résultats d'un système de reconnaissance de mots isolés, indépendant du locuteur développé pour permettre l'accès à une base de données vocale australienne, à travers le réseau téléphonique commuté (RTC).
Le Système de reconnaissance est basé sur une modélisation markovienne (HMM) utilisant des densités continues.
La base de parole d'apprentissage a été enregistrée à travers le RTC par une large variété de locuteurs de régions différentes.
Cette base comporte 55 mots : 41 noms de pays et leurs variantes de prononciation, plus quelques mots de commande.
Les performances de reconnaissance, testées sur 100 locuteurs diférents (50 hommes et 50 femmes), sans grammaire, atteignent 97.3%.
Cet article décrit la méthodologie d'apprentissage des HMMs qui comporte trois étapes : apprentissage des modèles à partir d'une segmentation manuelle, segmentation automatique des mots et ré-estimation.
Pour faciliter l'implantation ultérieure du système de reconnaissance sur DS3, un algorithme de Viterbi rapide et trame-synchrome a été implémenté sans dégradation des performances.
La détection bruit-parole est effectuée en associant un modèle silence/bruit aux modèles de mots.
Pour les paires de mots pouvant être confondus, des modèles sub-lexicaux sont utilisés qui améliorent le taux de reconnaissance.
Une approache par post-traitement est aussi utilisèe pour améliorer les performances : tous les candidats classés par le décodage de Viterbi sont soumis à des tests de durée minimale des mots et de différence statistique entre le premier et le deuxième candidat.
Dans les applications pratiques, les systèmes de reconnaissance de la parole ont besoin d'être insensibles aux différences entre les conditions acoustiques des ensembles d'entraı̂nement et de test.
Les différences d'environment acoustique peuvent résulter de différentes sources telles que bruit de fond ambiant, variations des caractéristiques du canal de transmission, et le stress du locuteur.
Ces perturbations peuvent détériorer fortement les performances d'un système de reconnaissance.
Plusieurs techniques ont été proposées pour améliorer la robustesse à ces facteurs de bruit.
Cet article considère une approche particulière de technique de compensation basée sur des modèles de bruit, appelée compensation prédictive, qui s'est montrée particulièrement robuste au bruit dans le cas de nombreux environnements acoustiques.
La caractéristique de ces approaches est de combiner un modèle de parole avec un modèle de bruit additif, un modèle du canal de transmission et, dans le cas général, un modèle de stress du locuteur, afin de créer un modèle de parole déteriorée.
La théorie générale de ces techniques prédictives est discutée ici.
Différentes approximations permettant d'effectuer rapidement la combinaison des modèles ont été proposées et sont décrites ici.
Les avantages et les limitations de ces méthodes prédictives pour la robustesse au bruit sont également discutés.
De plus, des méthodes permettant de combiner des approches prédictives avec des méthodes utilisant les données de parole dans le nouvel environnement, à savoir les méthodes adaptatives, sont discutées en détail.
Cette approche combinée a alors l'avantage d'effacer certaines des limitations des approches prédictives.
La taille croissante du web et l'hétérogénéité de l'information accessible rendent l'extraction d'informations (EI) de pages web de plus en plus complexe.
Dans le contexte de la collecte d'information sur des domaines restreints du web, cette recherche concerne le développement de systèmes d'EI de pages web, dits adaptatifs dans la mesure où ils peuvent être adaptés à de nouveaux domaines par apprentissage sur un corpus de pages web annotées de ces domaines.
Les performances de WEPAIES sont évaluées sur trois corpora standard plus ou moins structurés, et comparées à celles d'autres systèmes d'extraction d'information adaptatifs.
A l'aide d'un vocabulaire de 12 mots artificiels, les auteurs ont développé une méthode pour évaluer les performances auditives de patients équipés avec un implant cochléaire.
La répétition aléatoire de ces 12 mots a conduit à une liste de 129 éléments qui a été lue aux patients et qui teste simultanément plusieurs caractérs du langage.
L'exploitation statistique des résultats a concerné la reconnaissance des mots, des consonnes, des éléments de base du vocabulaire. L'association de ces pourcentages est aussi étudiée.
Les résultats sont présentés pour deux patients porteurs d'une prothèse Chorimac.
Ils montrent que : (1) les différents tests ne sont pas équivalents (même les associations de pourcentages), (2) quelques propriétés principales de la prothèse Chrorimac sont retrouvées, telles que la bonne discrimination de l'opposition plosive-fricative et la mauvaise distinction du voisement, et (3) l'aide à la lecture labiale n'est pas mise en évidence ; ceci incite à discuter la sensibilité de la méthode proposée.
Cet article est dédié au problème de l'approximation de la fonction de valeur dans le cadre des algorithmes d'apprentissage par renforcement.
Nous présentons une méthode de modélisation de la fonction de valeur qui alloue de nouvelles ressources au fur et à mesure que l'agent explore son environnement.
La fonction de valeur est représentée par un réseau de fonctions de base radiale gaussiennes.
Le modèle est construit incrémentalement en ajoutant de nouvelles unités à chaque fois que le système entre dans une région inconnue de l'espace des états.
Les paramètres du modèle sont adaptés en utilisant la descente du gradient et l'algorithme Sarsa(X).
Cette méthode ne requiert ni un modèle de l'environnement ni une approximation de ce modèle.
La performance de la méthode est évaluée sur deux problèmes type : l'Acrobot et le Bioréacteur.
Dans les deux cas, les systèmes sont simulés dans un espace d'états continu et un pas de temps discret.
Les étudiants anglais de Londres, du niveau licence, font une erreur systématique en épellant Gandhi sous la forme Ghandi, et cela malgré le fait qu'ils aient souvent vu ce nom correctement écrit.
C'est une faute d'orthographe modéle que nous étudions dans cet article.
On en conclut que même avec une connaissance négligeable des mots et noms 'hindous', les lecteurs anglais utilisent des 'règles' pour de telles tâches.
Cela laisse espérer que, une fois acquise la façon correcte d'épeller, celle-ci se maintiendra car de nouvelles règles remplaçeront sans doute les anciennes.
Dans cet article nous proposons une approche de communication entre agents logiciels basée sur les engagements sociaux et les arguments.
Dans cette approche, les agents doivent utiliser leurs capacités de raisonnement pour raisonner au sujet de leurs états mentaux avant d'agir sur les engagements ou les contenus des engagements sociaux d'une façon pertinente.
Afin de choisir les arguments les plus pertinents à chaque étape de l'interaction dialogique, les agents utilisent deux types de raisonnements : stratégique et tactique.
Le raisonnement stratégique permet d'une part, de choisir le plan global de la communication en termes de sous-buts à accomplir afin de réaliser le but conversationnel.
Ainsi, le raisonnement tactique permet aux agents de choisir localement, à chaque tour de l'interaction dialogique, l'argument le plus pertinent selon la stratégie adoptée.
La performance des algorithmes de renforcement du langage baisse rapidement avec le rapport signal sur bruit (SNR).
A faible SNR, la probabilité de renforcement des phonèmes à haute intensité, tels que certaines voyelles, est plus grande que celle de la plupart des consonnes.
Même si le renforcement sélectif des consonnes améliore la reconnaissance des voyelles, il diminue, d'autre part, les amplitudes relatives.
Nous avons mené des expériences sur des sujets bien-entendants afin de déterminer l'effet global que peut avoir le renforcement sélectif des voyelles sur la compréhension des consonnes dans des phonèmes consonne–voyelle–consonne.
En l'absence de bruit, si l'on compare avec les conditions de contrôle de non-renforcement à 65 dB (A), le renforcement des voyelles par 12 dB n'affecte guère la compréhension des consonnes.
En présence d'un bruit de fond correspondant à un SNR de −6 dB, une fraction de 50,1% des phonèmes non-renforcés ont été reconnus, alors que ce taux augmentait à 69,8% avec le même SNR mais un renforcement sélectif des voyelles de 12 dB.
Or, un renforcement simultané des voyelles et des consonnes de 12 dB, donnait un taux de reconnaissance des consonnes de 91,5%.
En conclusion, les algorithmes de renforcement du langage devraient agir le plus fortement possible sur l'ensemble des éléments du langage, même si cela résulte en un renforcement sélectif de certaines catégories de phonèmes, au détriment d'autres.
Afin d'évaluer cette fi- délité, nous proposons un moyen de visualiser toute mesure associée aux données en coloriant les cellules de Voronoï de l'image de ces données dans l'espace de projection, et nous étudions des mesures spécifiques.
Nous présentons des cas d'analyse de données réelles et artificielles à partir des projections obtenues par l'Analyse en Composantes Principales et l'Analyse en Composantes Curvilignes.
Cet article de synthèse s'adresse à un large public provenant de différentes disciplines mais également aux spécialistes de l'intonation.
Il comprend cinq parties.
Dans la première partie, ou Introduction, sont brièvement rappelés les concepts de base de l'intonation et de la prosodie et mises en lumière les périodes charnières de la recherche intonative.
Dans la deuxième partie, Fonctions et formes de l'intonation, un large éventail des fonctions des niveaux morpholexical, phrastique, discursif ou dialogal est examiné ; des formes intonatives provenant de langues différentes sont présentées et comparées.
Dans la troisième partie, Mod lisation et transcription de l'intonation, on fait référence aux modèles courants de l'intonation et aux systèmes d'étiquetage opérationnels.
Dans la quatrième partie, Les applications de l'intonation, sont présentées les applications les plus courantes de l'intonation, plus particulièrement les applications technologiques ; une discussion est engagée sur les problèmes méthodologiques.
Dans la cinquième partie, Perspectives de recherche, des directions de recherche sont tracées ; on précise les buts à atteindre et on souligne le sens et l'intérêt de la recherche intonative pour les années à venir.
L'analyse specrale des signaux non stationnaires nécessite de mettre en œuvre des outils spécifiques permettant de décrire une évolution temporelle de caractéristiques fréquentielles.
De tels outils, appelés représentations temps-fréquence, peuvent être définis de manière objective en partant de contraintes imposées a priori.
Si Ton se place dans un cadre aléatoire et non paramétrique, deux grandes classes d'approches sont offertes, suivant que Ton privilégie l'existence d'une décomposition doublement orthogonale, ou que Ton cherche à préserver le concept usuel de fréquence.
Après avoir établi les définitions correspondantes et souligné l'importance de la transformation de Wigner-Ville, on s'intéresse à leurs possibilités d'estimation et on discute comment une représentation temps-fréquence peut être utilisée pour des opérations de traitement dépassant la seule description.
Plusieurs traductions du Speculum humanae salvationis ont été composées et copiées dans les Pays-Bas bourguignons durant la seconde moitié du XVe siècle.
Dans ce contexte, la traduction de Miélot, commandée par le duc Philippe le Bon, et ses deux manuscrits, dont une minute, a souvent été considérée comme un échec.
La comparaison des caractéristiques textuelles et matérielles de la traduction de Miélot avec celles des trois autres traductions contemporaines, en prose, permet de mettre en évidence la spécificité du travail de traduction et de mise en livre de Miélot.
Cet article illustre les avantages apportés par l'utilisation de la Transformation Cosinus Discrète (DCT) par rapport à celle de la Transformée de Fourier Discrète (DFT) standard, pour le débruitage de la parole bruitée.
On montre comment dériver un filtre MMSE à partir de la modélisation statistique des coefficients DCT.
On montre également comment dériver un facteur de sur-atténuation basé sur le fait que, dans les signaux bruités, l'énergie de la parole n'est pas toujours présente à chaque instant ni dans chaque coefficient.
Ce facteur de sur-atténuation est utile pour supprimer tout bruit résiduel musical.
Les méthods proposées ont été évaluées favorablement par rapport du filtre de réduction de bruit proposé par Ephraim et Malah (1994), en utilisant tant du bruit blanc guassien que du bruit de ventilateur enregistré
La coopération entre agents, quelle que soit leur nature, ne peut s'établir que si ceux-ci disposent d'un ensemble de représentations.
En fonction de ses capacités de perception et de raisonnement, chacun d'eux construit une représentation plus ou moins riche de la situation dans laquelle il coopère.
Les décisions qu 'il prend dépendent en grande partie de ces représentations.
On montre comment la perception, le raisonnement et l'action sont liés dans le cadre général de l'activité coopérative.
Pour cela, on observe la relation entre l'activité coopérative et l'activité individuelle.
Ensuite, on examine le cas particulier de la coopération homme/machine comme cas particulier de la coopération entre agents, pour lequel la machine doit être dotée de représentations adaptées et doit permettre à l'utilisateur de construire des représentations performantes.
La modélisation pour la conception des systèmes à base de connaissances apporte une base de représentation essentiellement conceptuelle.
Deux points de vue doivent en effet cohabiter dans les systèmes coopératifs ; celui de l'organisation dans laquelle intervient la coopération et celui de l'utilisateur qui coopère avec le système pour réaliser une tâche.
Outre le modèle conceptuel de l'application, on trouvera donc dans les systèmes coopératifs un modèle de coopération (point de vue de l'organisation) et un modèle d'utilisateur.
Plusieurs auteurs soutiennent que le dialogue doit être considéré comme une activité conjointe (voir par example (Clark et Wilkes-Gibbs, 1986 ; Grosz et Sidner, 1990 ; Schegloff, 1981 ; Suchman, 1987)) - quelque chose que les agents font ensemble - plutôt que simplement comme le produit de générateurs et reconnaisseurs de plans fonctionnant en synchronie et harmonie, comme le proposent les théories fondées sur la notion de plans.
Les approches à base de plans n'expliquent pas pourquoi les destinataires posent des questions de clarification, pourquoi ils confirment, ou même pourquoi ils ne s'en vont pas.
En revanche, le modèle d'action conjointe suggère que les deux parties participant à un dialoque sont conjointement responsables de son maintien.
Participer à un dialogue requiert des conversants d'avoir au moins un engagement conjoint, pour pouvoir se compredre.
Parmi les questions clés auxquelles il faut répondre, il y a la formalisation précise de ces engagements généraux et la manière dont ils prédisent cette fine synchronie si frappante dans une conversation ordinaire.
Afin de commencer à répondre à ces questions, nous esquissons ici la façon dont une théorie formelle de l'action conjointe explique les confirmations que l'on trouve dans les dialogues téléphoniques orientés-tâche.
Un développement plus formel est donné dans (Cohen et Levesque, 1991a).
Nous montrons ensuite que des extensions de cette analyse à un cadre général de dialogue sera difficile.
En particulier, ceci nous forcera à abadonner nos analyses simplistes de contenu propositionnel et de sens littéral.
Le système tient dans un ordinateur personnel - type 386 et fonctionne sous MS-DOS.
Parmi ses caractéristiques les plus intéressantes, nous pouvons citer la capacité d'adapter rapidement le système au locuteur et ce, de manière indépendante de l'application, ainsi que la facilité de créer des dictionnaires spécifiques aux applications.
L'adaptation est rapide parce que le système extrait les distributions spectrales-cibles des phonèmes à partir des mots d'entraînement et combine cette information avec des données indépendantes du locuteur concernant d'autres aspects du signal, comme le profil énergétique, la durée des phonèmes et leur similarité.
Le système accepte le langage naturel sans aucune restriction et peut être utilisé pour dicter des documents de n'importe quelle nature, pour autant que le dictionnaire du domaine d'application soit spécifié.
La dictée n'est toutefois pas la seule application envisagée.
Dans cet article, nous décrivons notre approche de la reconnaissance de la parole et fournissons quelques informations sur l'implémentation et les performances actuelles de notre système.
Nous décrivons un nouveau modèle stochastique en vue de générer des signaux de parole appropriés au codage à bas débit.
Dans ce modèle, l'onde de parole est représentée sous la forme d'un processus gaussian à moyenne nulle possédant un spectre de puissance à variation lente.
La séquence “innovée” optimale est obtenue en minimisant un critère d'erreur fondé sur les propriétés du système auditif humain.
Chaque bloc de 40 échantillons (représentant 5 ms du signal de parole échantillonné à 8 kHz) du signal nouveau est codé dans l'une des 1.024 séquences gaussiennes de longueur 40 générées de manière aléatoire.
La séquence choisie minimalise un critère d'erreur pondéré spectralement.
Le nouveau signal est dès lors encodé à 2 kbits/s.
Un filtre linéaire variant temporellement et dont les paramètres sont déterminés directement à partir du signal de parole est utilisé pour produire le spectre de puissance voulu.
Même à ce très faible débit, la parole resynthétisée peut à peine être distinguée de l'original.
Nous présentons une nouvelle théorie de la production de la parole.
Le formalisme est basé sur la théorie de la perturbation et s'appuie sur des simulations sur ordinateur.
Des fonctions de sensibilité qui relient les variations des fréquences formantiques à de petites perturbations des fonctions d'aires sont reformulées de manière à étre applicables à des variations importantes.
Des phénomènes phonétiques, articulatoires et acoustiques bien connus devienneny explicables au sein d'une théorie globale et formalle confirmée par des données expérimentales.
Notre système formel éclaire sous un jour nouveau quelques principes propres aux relations acoustico-articulatoires et leur application à la théorie phonétique.
Nous montrons dans cet article que de notre formalisme découlent une simplification des relations acoustico-articulatoires, une confirmation de la nature quantale de la parole, une reconsidération des universaux et systèmes phonétiques, une explication et normalisation des transitions formantiques et un modèle à 9 paramètres facilement contrôlables.
D'autres phénomènes tels que les effets de compensation et de symétroe sont également interprétés.
Le modèle peut également servir à la formulation et au calcul de structures dynamiques de base.
Plusieurs applications et de nouvelles perspectives de recherche sont également proposées.
Un aspect bien connu de la prononciation anglaise est l'existence de consonnes syllabiques.
On considère en général la syllabicité consonantique comme une conséquence de l'élision vocalique, en suggérant que par exemple la prononciation du mot “button” comme /bλtm/ est la réalisation d'une forme sous-jacente /bλtən/ ou même /bλt⊃n/.
Puisque l'élision est un phénomène soumis ă l'influence du style parlé, il semblerait donc que dans le parler rapide ou familier on devrait s'attendre à trouver davantage de cas de consonnes syllabiques, et moins de cas de voyelles non-accentuées suivies de consonnes continues.
Cette communication se propose de montrer que le phénomène n'est pas si simple : nous examinons les problèmes qui font obstacle à nos tentatives de reconnaissance automatique des syllabes et autres unités inférieures au mot et considérons les facteurs phonotactiques et phonétiques qui peuvent permettre de les résoudre.
Cet article présente une édition critique accompagnée d'une étude d'un recueil de poésie des XVIIe et XVIIIe siècles que l'on avait jusque-là pris pour le Kitāb al-Ġilmān, œuvre d'al-Ṯaʿālibī disparue.
Nous l'accompagnons d'une analyse codicologique du manuscrit Berlin MS Wetzstein II 1786 – qui contient ledit recueil – et nous expliquons et corrigeons l'attribution erronée de longue date à al-Ṯaʿālibī.
Cet article décrit le langage SSML (Speech Synthesis Markup Language) qui a été défini pour servir de standard d'interface, indépendant de la plate-forme, pour des systèmes de synthèse de la parole.
Cet article traite de la nécessité de normalisation au sein des systèmes de synthèse et explique en quoi cela peut aider les constructeurs de systèmes à faire un meilleur usage de la synthèse.
Les caractéristiques principales de SSML (qui est basé sur SGML) sont ensuite présentées.
Les performances d'un système de reconnaissance sont souvent dégradées par la présence de bruit.
Cette dégradation est en partie due au fait qu'il y a souvent une grande différence entre les conditions d'appentissage et de test, c'est-à-dire que les valeurs des paramètres mesurés lors de l'apprentissage sont très différences des valeurs mesurées en condition de test sous l'effet du bruit.
Une solution pour contourner ce problème est d'utiliser des paramètres qui sont moins sensibles aux conditions de bruit.
Dans cet article, nous proposons l'utilisation d'une famille de limiteurs de signal pour la reconnaissance de parole bruitée.
Le limiteur de signal correctement ajusté équivaut à appliquer une transformation arcsin sur les fonctions d'autocorrelation du signal d'origine.
L'effet de l'utilisation du limiteur de signal comme préprocesseur est de réduire la variabilité du vecteur de paramètres de sorte que la différence entre les conditions d'apprentissage et de test se trouve réduite.
L'évaluation sur le vocabulaire des chiffres etdes lettres en anglais, en mode monolocuteur, montre que les performances d'un système de reconnaissance par alignement temporel dynamique (DTW) peuvent etre augmentées lorsque le limiteur de signal est utilisé comme préprocesseur pour réduire la variabilité due à des conditions très différentes.
Des technologies récentes telles que les microarrays permettent de mesurer le niveau d'expression de milliers de gènes au cours du temps.
Cependant, le nombre de points réduit de ces séries temporelles rend difficile la prise en compte des dépendances entre les temps par les algorithmes de classification.
De plus, certaines des classes les plus intéressantes pour le biologiste peuvent être totalement omises par les algorithmes classiques du fait du faible nombre de gènes qui les composent.
Nous proposons une approche bayésienne de ce problème.
Un modèle de mélange est utilisé pour décrire et classer les données.
Les paramètres de ce modèle sont contraints par une distribution a priori définie grâce à un nouveau type de modèles qui exprime les connaissances a priori dont on dispose.
Ces connaissances permettent de traiter les dépendances temporelles d'une manière très naturelle, et de prendre en compte des connaissances approximatives concernant les profils temporels les plus intéressants.
Nous examinons des facteurs qui ont affecté la synthèse de parole de haute qualité par l'analyse-synthése.
L'influence d'un sous-groupe de ces facteurs sur la qualité de voix synthétique a été évaluée par un test d'écoute comparant la parole naturelle à la parole synthétique de deux synthétiseurs : le synthétiseur à prédiction linéaire (LPC) et le synthétiseur à formants.
Plusieurs sources d'excitation pour la synthèse ont été considérées,
incluant des paramètres importants qui reproduisent certains faits ayant lieu au niveau de la glotte, tels que les instants d'ouverture et de fermeture de la glotte.
De plus, le fait d'identifier les parties voisées, non voisées, et silencieusses du signal de la parole, et de mesurer la fréquence fondamentale des parties voisées, a contribué à la synthèse de parole de haute qualité.
Une approache à deux niveaux concernant l'analyse de la parole est recommandée afin d'améliorer le traitement automatique de la parole ; le premier niveau est constitué par le traditionnel signal de la parole, alors que le deuxiéme niveau est constitué par l'éléctroglottogramme (EEG).
Pour identifier avec précision la suite des phones correspondant à un signal de parole continu, nous proposons une nouvelle approche caractérisée par des traitements de bas en haut (ascendant) et de haut en bas (descendant) fortement couplés.
La segmentation, la reconnaissance et l'étiquetage caractérisent le chemin ascendant.
L'étiquetage, la génération de la parole et la segmentation sont les constituants du traitement descendant.
Ainsi ces quatre processus forment une boucle de “feed-back” qui permet une interprétation optimale du signal de parole, pour une observation bruitée et une connaissance a priori.
Le but essentiel de cette étude est d'identifier un système modèle en employant la théorie de l'estimation statistique et celle du champ moyen.
Les résultats expérimentaux sont obtenus sur la base des données TIMIT.
Par conséquent l'ensemble du système est capable de transformer le signal d'entrée continu en un des 61 sons de base, avec un taux de reconnaissance de 73,7%.
Dans ce papier, nous avons cherché à évaluer l'intérêt de la création de classes de signatures manuscrites pour un système d'authentification en ligne.
Dans notre étude, la création des classes s'effectue grâce à deux algorithmes de clustering et en se basant sur différents sous-espaces de description des signatures.
La spécialisation du système consiste à déterminer non plus un seuil de décision (acceptation ou rejet) global au système (i.e. le même pour toutes les personnes qui s'enrôleront) mais un seuil adapté à chacune des classes.
Cependant la sensibilité de la classification est très grande et la notion de classe unique pour un signataire semble trop restrictive.
On décrit un algorithme qui fournit une courbe de référence de la mélodie d'un signal de parole en vue d'études comparatives de détecteurs du fondamental.
On discute d'abord le problème d'analyse des erreurs et particulièrement la question de l'exactitude de la mesure.
On montre que l'exactitude de l'algorithme doit étre supérieure à 0.5% afin de rendre inaudibles les petites inexactitudes de mesure.
L'algorithme développé dans notre laboratoire traite le signal de sortie d'un laryngographe qui mesure les vibrations laryngiennes par la conductance électrique du larynx.
Le point d'inflexion du laryngogramme est interprété comme l'instant de clóture de la glotte, e'est-a-dirc le début d'une période fondamentale.
En utilisant un filtre interpolateur local on maintient l'erreur de quantification de la mesure au-dessous de 0.5%.
Dans cet article, nous suggérons que la cognition humaine utilise deux modes d'apprentissage, le mode s et le mode u.
Le mode s fait appel à une mémoire de travail abstraite ; l'apprentissage est sélectif et peut être verbalement décrit.
Le mode u ne fait pas intervenir de mémoire de travail abstraite ; l'apprentissage est non sélectif et ne peut pas être verbalement décrit.
Dans trois expériences, nous avons utilisé deux tâches dont il a été démontré qu'elles donnent lieu à deux types d'apprentissage compatibles avec la distinction entre deux modes que nous postulons.
Au cours de ces expériences, nous avons étudié l'effet que produit l'introduction d'une tâche verbale secondaire sur l'apprentissage, la performance et le ré-apprentissage pour la première tâche.
On pouvait s'attendre à ce que la tâche secondaire interfère avec le mode s d'apprentissage.
Nos résultats montrent que l'introduction de la tâche secondaire interfère avec la performance et le ré-apprentissage pour l'une de ces tâches, celle pour laquelle les sujets pouvaient décrire verbalement ce qu'ils avaient appris.
Par contre, pour la tâche où les sujets ne pouvaient pas décrire verbalement leur apprentissage, la performance et le ré-apprentissage étaient facilités par l'introduction de la tâche supplémentaire.
Ces résultats militent pour une distinction entre deux modes d'apprentissage, dont l'un seulement repose sur le langage.
On émet l'hypothèse que les cris des bébés sourds sont différents de ceux des bébés otologiquement sains à cause de l'absence de contrôle auditif.
Les résultats montrent que les experts sont capables d'une discrimination auditive des cris des deux groupes de bébés (sourds et entendants), sur la base de propriétés vocales et mélodiques des cris.
Les cris des bébés sourds diffèrent par le timbre, le rythme et la mélodie.
Les propriétés sonores sont corrélées aux caractéristiques spectrales, et l'on peut extraire des paramètres mélodiques et rythmiques qui diffèrent de manière significative entre les deux groupes des bébés.
Les résultats sont discutés sur la base d'un modèle de production des cris.
Les paramètres des signaux ainsi déterminés permettent une classification automatique à partir des cartes neuronaux topologiques et pourraient être utilisés ultérieurement pour l'établissement d'un diagnostic précoce.
Nous nous plaçons dans le cadre de la segmentation bayésienne.
Parmi les trois étapes (modélisation, estimation, optimisation), nous considérons la modélisation et l'optimisation.
La modélisation est appréhendée sous l'angle des champs de Markov.
Nous montrons les limites du modèle de Potts couramment employé et proposons un nouveau modèle (le chien-modèle) permettant de contrôler la longueur des contours et des lignes dans l'image segmentée.
Nous préservons ainsi les structures fines présentes dans les données.
Nous comparons ensuite les critères MPM et MAP conjointement aux algorithmes qui permettent de les optimiser.
Les différents résultats sont obtenus sur des images synthétiques et des images SPOT.
Le problème de la classification fait l'objet d'une seconde partie.
De plus en plus, les produits hypermédias (CD-Rom ou sites web) constituent des ressources riches en informations mais faiblement structurées.
Pour pouvoir exploiter la richesse de tels hypermédias, il est nécessaire de proposer une aide efficace à l'utilisateur, qui lui soit adaptée.
Pour cela on cherche à modéliser le comportement de navigation de l'utilisateur, qui traduit son but.
Cet article traite de l'apprentissage et de la reconnaissance de comportements d'utilisateurs de produits hypermédias à l'aide de modèles markoviens.
Une étape cruciale dans le traitement de la parole pour l'extraction d'information, la détection du sujet de conversation et la navigation est la segmentation du discours.
Celle-ci est difficile car les indices aidant à segmenter un texte (en-têtes, paragraphes, ponctuation) n'apparaissent pas dans le language parlé.
Nous étudions l'usage de la prosodie (l'information extraite du rythme et de la mélodie de la parole) à cet effet.
Nos résultats indiquent que le modèle prosodique est équivalent ou supérieur au modèle du langage, et qu'il requiert moins de données d'entraı̂nement.
De plus, nous obtenons un gain significatif en combinant de manière probabiliste l'information prosodique et lexicale, et ce pour différents corpora et applications.
Une inspection plus détaillée des résultats révèle que les modèles prosodiques identifient les indicateurs de début et de fin de segments, tel que décrit dans la littérature.
Par exemple, le ton s'avère extrèmement utile pour la segmentation des bulletins télévisés, alors que les caracteristiques de durée et celles extraites du modèle du langage servent davantage pour la segmentation de conversations naturelles.
La transcription automatique d'émissions parlées d'informations radio-télévisées (tâche désignée par “Hub-4”) a été l'objet d'intenses travaux de recherche ces dernières années.
Ce papier présente les lignes principales de nos efforts d'élaboration d'un système de reconnaissance de parole continue qui soit à même de traiter le signal hétérogène provenant d'émissions d'information sans entraı̂ner une trop grande complexité ou le recours à des ressources de calculs excessives.
L'essentiel de nos efforts a porté sur les points suivants : • La segmentation automatique du signal audio en une suite de passages parlés ; • Le décodage rapide en une passe intégrant un modèle de trigrammes avec une technique d'anticipation ; • L'interpolation log-linéaire optimale d'une variété de modèles acoustiques et grammaticaux au moyen d'une technique de combinaison discriminative de modèles (DMC) ; • La prise en compte de corrélations linguistiques à court terme et, plus faiblement, à long terme au moyen de groupements de mots (phrases) et de modèles de languages dits “à distance” ; • L'amélioration de la modélisation acoustique à l'aide d'une extraction robuste du contenu du signal combinée à la normalisation des canaux, l'adaptation des modèles phonétiques ainsi que la sélection et la vérification des scripts du corpus d'entraînement.
Notre point de départ fut le système Philips “NAB-64k” fondé sur l'emploi de triphones intra-mots et de modèles de trigrammes.
Pour la tâche “NAB” impliquant la transcription d'articles lus à l'aide d'un microphone connu, ce système indépendant du locuteur atteint un taux d'erreur moyen de 10% au niveau du mot.
Au terme de ce travail, nous avons développé un système qui combine par DMC des modèles phonétique intra-mots et inter-mots, des pentaphones, des groupements de mots ainsi que des modèles de langage jusqu'à l'ordre 4.
Ce système produit une transcription d'émissions parlées d'information avec un taux d'erreur global d'environ 17%.
Cet article montre comment un modèle articulatoire, doté de la capacité de produire des sons à partir des déplacements de ses articulateurs, peut apprendre à parler, c'est-à-dire à coordonner ses mouvements de telle manière qu'ils produisent des séquences de sons appartenant à un langage donné.
Cet apprentissage complexe est accompli en quatre phases :
(a) une phase de babillage, où le modèle construit une copie interne des tranformations directes, c'est-à-dire la transformation articulo-audio-visuelle ; (b) une phase d'imitation, où il cherche à reproduire un jeu limité de séquences sonores par inversion ; (c) une phase de construction de représentation, où il se dote de prototypes sensori-moteurs des cibles caractéristiques des sons du langage ; et (d) une phase “rythmique”, où il apprend les coordinations nécessaires entre les activations de ces représentations.
Cet article présente un outil pour l'étude de l'usage de sites internet qui repose sur une analyse automatique de l'activité des internautes et sur un mécanisme interactif de visualisation des navigations.
La phase d'analyse résume l'information avant la phase de visualisation ;
elle catégorise les traces d'activité des internautes à l'aide d'un algorithme de clustering relationnel biomimétique, et fournit un ensemble de profils représentatifs pour chacun des clusters, définis à partir de degrés de typicalité.
L'outil proposé a été appliqué et évalué sur des fichiers log issus du muséum de Bourges et a montré sa capacité à produire des visualisations pertinentes et interprétables des navigations des internautes.
ASTEC est un projet de recherche dont l'objectif est d'augmenter le taux d'inclusion des patients dans les essais cliniques de cancérologie grâce à l'exploitation automatique des données contenues dans le dossier médical informatisé.
Le projet ASTEC explore deux enjeux scientifiques majeurs dans le champ de l'informatique médicale :
1) l'interopérabilité syntactique et sémantique entre systèmes d'information.
Il s'agit ici de faire interopérer un dossier structuré informatisé avec un outil d'aide à la décision.
Le projet ASTEC propose un cadre syntaxico-sémantique d'interopérabilité, basé sur des standards internationaux.
Sont développées des méthodes génériques de médiation sémantique basées sur les ontologies pour adapter l'information issue des dossiers aux critères d'inclusion/exclusion des études cliniques ;
2) l'aide à décision : développer des méthodes d'inférence sur les dossiers patients en utilisant un formalisme de représentation adapté aussi bien à la structuration des données cliniques qu'à celle des critères d'inclusion et d'exclusion des essais cliniques.
Nous exposons ici les choix procéduraux et technologiques retenus dans le cadre de ce projet.
Cet article présente une vue générale du système de reconnaissance de parole continue pour de grands vocabulaires développé par les chercheurs de Philips.
Utilisant des modèles de phonèmes, ce système a été appliqué avec succès à diverses tâches couvrant l'éventail des petits jusqu'aux très grands vocabulaires, dans les langues allemande et anglo-américaine.
Ce texte est consacré aux applications de la reconnaissance de la parole à des tâches de dictée réelle, en particulier celles concernant des rapports juridiques et radiologiques dans la langue Allemande.
Ces tâches sont décrites de même que les résultats obtenus expérimentalement.
Nous décrivons également une version commerciale d'un système de dictée issue de notre prototype et qui a été implémentée sur PC.
Afin de rendre possible une comparaison des performances avec d'autres systèmes, une section est consacrée à l'expérimentation sur des données provenant du quotidien Américain “Wall Street Journal”, incluant les tests réalisés lors de la procédure d'évaluation de novembre 1993.
L'architecture générale est fondée sur une approche statistique intégrée.
Par rapport à d'autres systèmes, les caractéristiques majeures qui se dégagent de notre système sont :
1. l'application constante du critère de Viterbi aussi bien pour l'apprentissage que pour la reconnaissance ; 2. l'usage de mixtures de densités de probabilité continues sans recourir à aucune forme de partage ou de lissage ; 3. un décodage synchrone avec une technique d'élagage en faisceau utilisée conjointement avec une organisation en arbre du lexique et une méthode d'anticipation rapide du phonème suivant.
Des voyelles produits dans un même contexte par des adultes, hommes et femmes, ont été analysées et transformées pour obtenir des représentations spectrales mesurables.
L'information phonématue contenue dans le spectre, ainsi que celle concernant le sexe du locuteur, a été examinée par classification automatiques sous des conditions variées, en utilisant la moyenne spectrale appropriée pour opérer une taxinomie des catégories.
Les pourcentages de classification obtenus selon cette méthode sont éleveés et ils montrent que, en ce qui concerne l'identité phonémique, un seul exemplaire de voyelle n'est pas aussi ambigu que peuvent le suggérer les mesures de formats.
Les resultats s'accordent avec ceux des expériences sur l'identification de voyelles pronocées par des locuteurs inconnus et qui montrent que la connaissance antérieure de la voix d'un locuteur n'est pas cruciale pour corriger l'identification.
Il peut être utile d'opérer un calibrage, particulièrement en fonction du sexe du locuteur, mais il est montré que la plus grande par de l'information néceessaire à cette normalisation est présente dans le spectre de l'exemplaire témoin.
Une procédure préliminaire de normalisation algorithmique basée sur l'information spectrale contenue dans le témoin est présentée.
Cet article de tonalité théorique vise à dresser un état des lieux des dernières avancées en ingénierie des connaissances médicales dans le domaine spécifique de la conception d'ontologies et systèmes à base de connaissances.
Reprenant des débats ayant animé le paysage de l'intelligence artificielle (IA) à partir des années 1960, sous l'impulsion des travaux de H. L. Dreyfus, il vise notamment à montrer que la plupart des difficultés aujourd 'hui rencontrées par l'ingénierie des connaissances dans le domaine de la santé sont inhérentes à la nature même de l'IA, en tant que projet de mécanisation de l'activité cognitive.
Et il promeut à ce titre l'idée que seule une juste compréhension de ce que les machines ne peuvent faire, étant donné leur caractère machinique même, et qui reste, malgré sa finitude cognitive, une propriété exclusive de l'humain, pourra offrir d'équilibrer la balance entre les tâches allouables aux machines et celles laissées à la charge de ce dernier.
Les systèmes actuellement les plus performants en reconnaissance de la parole continue, indépendants du locuteur, atteignent un taux de reconnaissance de mots qui dépasse 94% en utilisant un lexique de 1000 mots maximum et une grammaire ou un modèle de langage d'une perplexité d'au maximum 60.
Ces performances décroissent rapidement lorsque la perplexité de la grammaire augmente.
Si l'on permet à l'usager de parler naturellement et d'utiliser des constructions de son choix, les perplexités augmentent de plus d'un ordre de grandeur.
Heureusement, grâce à la technologie actuelle, la connaissance d'un domaine et celle des comportements de communication et de résolution de problème peuvent être utilisées pour faire décroître dynamiquement la perplexité et obtenir une interaction plus naturelle.
La réduction de la perplexité à partir de ces connaissances aboutit à des performances égales à celles de systèmes usant d'un modèle de langage de basse perplexité dans le même ou dans des domaines différents.
Dans cet article, nous abordons les problèmes liés à l'utilisation de connaissances sur la sémantique du domaine, sur le dialogue, les conventions de communication et le comportement de résolution de problème pour améliorer la reconnaissance et la compréhension automatique de la parole.
Nous expliquons également les principles de base du système minds et en décrivons les sources de connaissance importantes et les heuristiques.
Suit une brève analyse de certaines heuristiques qui ne doivent pas être réimplantées pour différents domaines.
Nous abordons plus spécifiquement le fait de savoir pourquoi les heuristiques sont efficaces et de combien chacune d'elles pourra diminuer l'entropie et le facteur de branchement moyen pour tout domaine d'application possible.
Dans les applications, les erreurs d'un système de reconnaissance automatique de parole sont principalement dues à un manque d'efficacité de la détection des segments de parole dans le signal, à un manque de fiabilité du rejet des mots hors vocabulaire ou des bruits, et à une considération insuffisante des effets du bruit et des canaux de transmission.
Dans ce papier, nous passons en revue un ensemble de techniques développées au CNET pour augmenter la robustesse aux variations des conditions d'utilisation et d'apprentissage d'un système de reconnaissance.
Ces techniques se divisent en deux classes : prétraitement et adaptation des paramètres des modèles de Markov cachés (HMM).
Les résultats de plusieurs expériences menées sur des bases de données d'exploitation, ainsi que sur des bases de données collectées à travers les réseaux RTC et GSM, sont présentées.
Les sources principales d'erreurs sont analysées.
On montre que l'égalisation aveugle des effets des lignes améliore significativement les performances de reconnaissance sur les données d'exploitation et les données GSM.
Le module de détection de la parole dans le signal permet au système de déterminer les frontières des mots à reconnaı̂tre.
Des techniques de prétraitement ont été utilisées pour améliorer la robustesse de la détection dans l'environnement GSM bruyant.
On montre que la soustraction spectrale améliore la détection dans l'environnement GSM bruyant.
Nos expériences montrent qu'un niveau de performance équivalent peut être obtenu par les adaptations Bayésienne et par régression des paramètres des HMMs.
Les résultats obtenus prouvent que l'adaptation et les techniques de prétraitement peuvent être avantageusement combinées pour améliorer la robustesse de la reconnaissance automatique de la parole.
Cet article décrit les travaux menés au CNET ces dernières années, dans le domaine de la reconnaissance de la parole.
Après avoir rappelé le contexte de cette recherche, on décrit le logiciel PHIL86 destiné à reconnaître des vocabulaires de petite taille, indépendamment du locuteur, et les développements matériels qui lui ont été associés.
Deux expérimentations de la reconnaissance dans le domaine des Télécommunications sont ensuite présentées, en insistant principalement sur les enseignements qui en ont été tirés et les résultats des évaluations menées sur le terrain.
Dans cet article, nous décrivons notre avancée durant ces quatre dernières années (1995–1999) en transcription automatique d'émissions d'information provenant de la radio et de la télévision en utilisant le système de reconnaissance de la parole Byblos de BBN.
De façon générale, nous avons réalisé des progrès constants comme le reflètent les résultats des quatre dernières évaluations Hub-4 de DARPA, avec des taux d'erreurs de 42.7%, 31.8%, 20.4% et 14.7% pour 1995, 1996, 1997 et 1998, respectivement.
Ce progrès peut être attribué à des améliorations dans la modélisation acoustique, dans l'adaptation du locuteur et du canal de transmission, dans les algorithmes de recherche ainsi que dans la prise en compte de caractéristiques précises de la parole spontanée et de sa nature variable que l'on trouve dans ces émissions d'information radio et télédiffusées.
En plus d'améliorer la précision de la reconnaissance, nous avons aussi réussi à développer plusieurs algorithmes afin d'atteindre une vitesse de reconnaissance proche du temps-réel sans sacrifier pour cela de façon significative la précision de la reconnaissance.
Dans cet article, nous nous intéressons à l'extraction automatique des contours des traits permanents du visage à savoir : les yeux, les sourcils et les lèvres.
Pour chacun des traits considérés, un modèle paramétrique spécifique capable de rendre compte de toutes les déformations possibles est défini.
Dans la phase d'évolution, chaque modèle est déformé afin de coïncider au mieux avec les contours des traits présents sur le visage analysé.
Cette déformation se fait par maximisation d'un flux de gradient (de luminance et/ou de chrominance) le long des contours définis par chaque courbe du modèle.
La définition de modèles permet d'introduire naturellement une contrainte de régularisation sur les contours recherchés.
Néanmoins, les modèles choisis restent suffisamment flexibles pour permettre une extraction réaliste des contours des yeux, des sourcils et de la bouche.
L'extraction précise des contours des principaux traits du visage constitue la première étape d'un ensemble d'applications multimédia.
Dans cette communication nous abordons des aspects multilingues de la reconnaissance automatique de la parole et nous essayons de les lier au concept d'interoperabilité.
Après une définition possible de l'interopérabilité multilingue, les composantes d'un système de reconnaissance sont discutées en vue de séparer des éléments spécifiques à une langue des parties plus génériques.
Nous donnons un aperçu de quelques projets de recherche en matière de reconnaissance multilingue en les séparant suivant différents styles de parole (lu, préparé, spontané).
Le problème de l'adaptation à d'autres langues est ensuite abordé.
Concernant la modélisation acoustique il existe actuellement des approches de modélisation (indépendante de la langue, inter-lingue,…) permettant le passage à une nouvelle langue sans corpus acoustique spécifique.
Cependant l'ajout de telles données constitue toujours un avantage indéniable.
Les dictionnaires de prononciation et les corpus textuels apparaissent aujourd'hui comme les ressources vraiment indispensables pour un portage rapide à une nouvelle langue.
Or le portage rapide constitue une ètape importante vers l'interopérabilité multilingue.
Les efforts en cours visant à produire des dictionnaires de prononciation et des corpus des texte, y compris des transcriptions de parole, devraient être étendus au plus grand nombre de langues possible.
Ces efforts pourraient être combinés avec des initiatives ayant comme but la sauvegarde des langues minoritaires.
Les logiciels de CAO ont pour perspective de devenir de véritables outils d'aide à la conception d'objets physiques.
Mais la conception préliminaire reste un domaine de recherche largement ouvert.
Cet article de synthèse s'efforce de montrer qu'une approche par contraintes du processus de conception est incontournable pour atteindre ce but.
La conception est vue ici comme un processus cyclique d'élaboration et de satisfaction de contraintes.
Le modèle conceptuel associé est un modèle d'objets géométriques sous contraintes.
Il est à plusieurs niveaux et intègre des connaissances géométriques et non géométriques.
L'article se poursuit par l'exposé des différentes approches de la modélisation géométrique : paramétrique, variationnelle, par caractéristiques (features en anglais) et déclarative.
Il se termine par une présentation rapide des méthodes de résolution et de décomposition des systèmes de contraintes utilisées en CAO.
De ce fait, l'introduction d'un détecteur d'activité vocale s'avère indispensable pour distinguer les séquences de bruit de celles du signal.
Ce papier présente un algorithme de classification parole/bruit basé sur la fonction de cohérence estimée entre deux observations reçues et évalue son influence sur la méthode de réduction de bruit proposée par Ephraïm et Malah.
Il apparaît que la détection ainsi réalisée conduit à des performances comparables à celles obtenues avec un détecteur manuel.
Le propos de cet article est de présenter une bibliographie récente sur l'utilisation des méthodes de représentation temps-fréquence en analyse et en traitement automatique de la parole.
Les méthodes sont classées en trois grandes familles : méthodes dérivées de la production, méthodes d'analyse du signal, méthodes modélisant la perception.
Après ce panorama, quelques rapides conclusions sur l'état actuel de l'utilisation de ces méthodes, et quelques perspectives sont tentées.
Ce papier présente un système de décision multi-classifieurs dont la conception est pilotée par la topologie des données d'apprentissage.
Celle-ci est extraite grâce à l'introduction d'un nouvel algorithme d'apprentissage de carte neuronale auto-organisée qui a la propriété d'être incrémentale en données.
Cette carte est utilisée en apprentissage pour distribuer la tâche de classification sur un ensemble de classifieurs.
Elle permet ensuite d'activer en phase de décision le ou les classifieurs utiles pour une nouvelle donnée.
Des résultats comparatifs sont donnés sur des exemples synthétiques, sur la base de segmentation d'images de l'UCI et sur le problème de reconnaissance de chiffres manuscrits sur des données de la base NIST.
Cet algorithme a été testé sur 3 bases de données (parole téléphonique bruitée artificiellement à 0 dB, enregistrement dans une automobile et parole “propre”) regroupant cinquante-huit locuteurs.
Le système a été comparé, à AMPEX (modèle auditif) et à des contours de fréquence glottale obtenus de façon manuelle ou par laryngogrammes.
Notre algorithme inclut un module de sélection automatique des canaux significatifs ainsi qu'un module d'extraction de fréquence glottale basé sur un pseudo-histogramme périodique (obtenu par combinaison de produits scalaires normalisés des signaux provenant des canaux sélectionnés).
Sur les enregistrements bruités (voiture et parole téléphonique à 0 dB), le système proposé dépasse AMPEX.
Il a été observé que la sélection automatique des canaux améliore les performances sur la parole à 0 dB mais pas sur les enregistrements en véhicule automobile.
L'article décrit le système proposé ainsi que les performances en termes de décisions voisé/non voisé, d'erreur fine et grossière.
Cet article décrit un cadre théorique pour l'optimisation de la structure et des paramètres d'un système de reconnaissance de grands vocabulaires basé sur des HMMs avec des densités d'émission continues, utilisant le critère d'estimation de l'information mutuelle maximale (MMIE).
Pour réduire la complexité calculatoire de l'algorithme d'apprentissage MMIE, les segments de parole pouvant être confondus sont identifiés et stockés sous forme de treillis de mots des diverses hypothèses de séquences.
Une procédure itérative de division des gaussiennes est également employée pour ajuster, à chaque état, le nombre de composantes des mélanges pendant l'apprentissage afin d'obtenir le meilleur compromis entre le nombre de paramètres et le volume de données d'apprentissage disponible.
On présente des résultats expérimentaux sur divers ensembles de test de la base de données “Wall Street Journal” qui utilisent jusqu'à 66 heures de données de parole pour l'apprentissage.
Ces résultats démontrent que l'utilisation de treillis rend l'apprentissage MMIE utilisable pour des systèmes de reconnaissance très complexes et des ensembles d'apprentissage très grands.
De plus, ils montrent que l'optimisation MMIE de la structure des systèmes et des paramètres peut aboutir à des améliorations utiles des performances de reconnaissance.
Nous nous intéressons au problème d'appartement de primitives entre deux images.
Nous proposons dans ce papier une approche utilisant un modèle de réseau de neurones pour résoudre le problème.
Nous avons choisi le modèle de Hopfield d'une part parce qu'il est souple et ouvert, d'autre part parce qu'il peut s'implanter aisément sur des calculateurs massivement parallèles.
Le maintien à domicile des personnes âgées est devenu un enjeu important de santé publique.
Nos travaux sont destinés à traiter à grande échelle l'information recueillie au domicile des personnes pour en faire une information consolidée décrivant des comportements globaux.
Nous proposons pour ce faire des algorithmes de classification pour identifier des profils collectifs et y rattacher les personnes suivies.
Ces algorithmes tirent profit des technologies multi-agents afin de gérer l'hétérogénéité des équipements et des services produisant l'information ainsi que leur totale distribution.
Les profils obtenus sont utilisés pour estimer l'état des personnes âgées et les efforts à déployer pour qu'elles puissent continuer à vivre à leur domicile, et à un niveau plus global pour évaluer des tendances sanitaires générales (épidémie, fortes chaleurs, etc.).
Puisque l'optimisation simultanéee du codage de source et de la voie de transmission est souvent impossible en ce qui concerne le codage de la parole, il est utile de développer des codages optimaux correspondant à l'agorithme spécifique de codage de la parole qui fonctionne sous des conditions spécifiées d'erreur de transmission.
De tels codes d'erreur de transmission qui dépendent de la source peuvent être obtenus en minimisant un critère approprié de distorsion de la parole.
Nous utilisons une procédure de “recuit simulé” afin d'accomplir cette minimisation.
Les codes qui en résultent fonctionnent bien puisqu'ils fournissent une correction d'erreur non uniforme (les niveaux de quantification à grande probabilité sont corrigés de manière plus précise) et/ou une détection d'erreur non uniforme (les erreurs qui ont un impact important sur la qualité de la parole ont plus de chances d'être détectées).
On peut obtenir un compromis optimal entre les corrections et détections d'erreurs.
Les codes de voie de transmission dépendants de la source et développés pour réagir conre les faibles taux d'erreurs aléatoires (pas plus de 2%) sont appliqués à algorithme de CELP et les performances qui en résultent sont décrites.
Il a été constaté qu'une petite allocation de mots de code pour la protection d'un paramètre particulier (qui équivaut à moins d'un bit) aboutit souvent à une grande amélioration des performances.
Cet article présente un panorama des travaux de recherche réalisés à l'occasion du projet européen CAVE, consacré à la vérification du locuteur en mode dépendant du texte sur le réseau téléphonique, et basé sur la modélisation de mots par modèles de Markov cachés.
Ce texte détaille les différents aspects technologiques et méthodologiques sur lesquels ces recherches se sont appuyées.
Il traite plus particulièrement du problème de l'estimation de modèles du locuteur dans le contexte de données d'apprentissage limitées et du réglage a priori du seuil de décision.
Les expériences sont effectuées sur une base de données de parole téléphonique réaliste : la base de données SESP.
Les performances obtenues sont au niveau de l'état de l'art, ce qui valide les choix techniques développés et évalués lors du projet, ainsi que l'infrastructure de travail qui a favorisé la coopération entre les partenaires.
Notre étude traite de l'estimation des paramètres dans les chaînes de Markov cachées et de la segmentation statistique non supervisée d'images.
Nous proposons deux algorithmes originaux d'estimation obtenus à partir des méthodes Iterative Conditional Estimation (ICE) et Stochastic Expectation Maximisation (SEM), notés MICE et MSEM respectivement, et montrons leur compétitivité vis-à-vis de l'algorithme Expectation Maximisation (EM) dans différents cas d'homogénéité et de bruitage des chaînes.
L'étude du comportement des trois algorithmes de restauration non supervisée des chaînes obtenus par l'adjonction à la méthode Mode de la Marginale a Posteriori (MPM) des algorithmes EM, MICE, MSEM respectivement est ensuite proposée.
La transformation des processus bi- dimentionnels en processus mono-dimentionnels par le parcours de Peano rend possible l'application de ces algorithmes au problème de la segmentation statistique non supervisée d'images.
On obtient ainsi des méthodes plus rapides que celles utilisant des modélisations par champs de Markov cachés et nous montrons que la perte de l'efficacité est, en général, acceptable.
La souplesse de notre modélisation permet par ailleurs la conception de nombreux algorithmes de segmentation statistique non supervisée spatio-temporelle d'images.
Nous en proposons trois et présentons les résultats de leur application à la segmentation d'une séquence d'images réelles.
Cet article présente un réseau RFR récurrent (Réseaux Récurrent à Fonction de base Radiales) appliqué à un problème de pronostic d'un système non linéaire.
Le processus d'apprentissage du réseau RRFR se décompose en deux étapes.
Dans une seconde étape, les poids des connexions de sortie sont déterminés par une technique supervisée de régression linéaire.
La technique FuzzyMinMax rend la convergence de l'algorithme des K-moyensplus stable.
Les techniques de modifications de l'échelle temporelle et, à un degré moindre, de la fréquence fondamentale présentent un intérêt théorique et pratique certain.
Les applications sont nombreuses, et incluent, pour n'en citer que quelques-unes, la synthèse de parole à partir du texte (par concaténation d'unités acoustiques), les transformations du timbre de la voix, l'apprentissage des langues étrangères, mais aussi le contrôle audio ou la post-synchronisation.
Pour satisfaire ces besoins, un grand nombre d'algorithmes ont été proposés ces dernières années, apparemment différents mais en fait très proches sur le plan des mécanismes de base.
Cet article présente les méthodes frequentielles (vocodeur de phase) et temporelles (Time-Domain Pitch-Synchronous Overlap/Add) dans un même formalisme.
Plusieurs méthodes plus récentes, dérivées de ces techniques fondamentales sont également décrites.
La reconnaissance automatique de la parole par des ordinateurs peut fournir le moyen de communication homme — machine le plus naturel et le plus efficace.
Bien que ces dernières années des systèmes de reconnaissance très performants aient déjà émergé des centres de recherche, les scientifiques s'accordent unanimement à dire que le déploiement de systèmes de reconnaissance de la parole dans un environnement de travail réel va nécessiter de nombreuses heures de données de parole pour pouvoir modéliser la variabilité inhérente au signal de parole.
Cette dernière base de données est particulièrement utile comme source de phrases spontanées induites dans un environnement réaliste et ciblé.
Il faut résoudre límportant problème de la relation entre le traitement de la parole et le traitement linguistique pour pouvoir utiliser la reconnaissance de la parole continue avec de grands vocabulaires dans des systèmes de traduction de la parole ou comme interface de système homme-machine.
Dans une première phase, notre système reconnaît les phonèmes à l'aide de chaînes de Markov cachées (HMM).
Ensuite, un analyseur grammatical LR généralisé est utilisé pour prédire les mots et phonèmes suivants.
La combinaison des deux méthodes (HMM-LR) permet de reconnaître une phrase japonaise.
L'utilisation d'informations linguistiques permet d'éliminer un grand nombre de phrases candidates.
Un système expérimental de traduction japonais-anglais a été implémenté.
La traduction se déroule en trois phases : analyse, transfert, génération.
Une nouvelle méthode incorporée dans le système analyse l'expression des intentions contenues dans les énoncés.
Cet article présente un résumé des usages actuels, en 1994–1995, en France, des systèmes de Reconnaissance et de Synthèse de la parole du CNET dans des Services Vocaux Interactifs.
On y analyse également les résultats d'évaluation de certains de ces services, en condition d'exploitation.
Enfin, cet article décrit brièvement les principaux développements récents des technologies de reconnaissance et de synthèse de la parole du CNET.
Mais en l'absence d'information sur le signal d'excitation, l'application de ces méthodes ne peut être directe.
Après avoir mis en évidence le mécanisme qui limite la précision de l'estimation dans ce cas, nous présenterons une solution fondée sur la détection et la suppression d'impulsions dans le résidu.
Cette approche permet d'améliorer l'estimation des paramètres dans le cas où le résidu est proche d'un train d'impulsions.
Nous examinerons enfin l'utilité de cette méthode pour l'analyse spectrale du signal de parole.
Dans cet article, nous proposons une nouvelle méthode d'apprentissage des unités de synthèse pour une synthèse de la parole multi-langues et nous en décrivons une application à la synthèse de la parole en anglais.
Cette méthode appelée Agglomération Orientée vers un Contexte Multi-Couches (ML-COC en anglais) repose sur la généralisation de la méthode COC qui a été appliquée à la synthèse de la parole pour le japonais.
La méthode COC classique produit un ensemble d'unités dépendantes du contexte phonétique par un processus de clivage d'un agglomérat.
Avec la ML-COC, la notion de contexte est généralisée et des facteurs autres que le contexte phonétique, comme l'accentuation et les frontières syntaxiques, sont pris en compte pour appréhender la plus grande diversité des phonèmes de la langue anglaise.
Une expérience de génération d'unités de synthèse a montré que la ML-COC produit environ trois fois plus d'unités de synthèse que la classique SL-COC, c'est-à-dire COC à simple couche, et qu'il y a environ 20% de variance en moins sur les unités de synthèse ML-COC.
Ces résultats suggèrent que les unités de synthèse produites par la ML-COC reflètent plus fidèlement la structure phonologique de l'anglais que celles obtenues par la SL-COC.
Pour valider l'efficacité de la méthode ML-COC, nous avons effectué une expérience où les sujets devaient choisir la parole synthétique qu'ils préféraient.
L'essai portait sur 52 phrases et a été répété avec 10 sujets.
La méthode ML-COC l'emporte sur la méthode classique SL-COC avec un score de 70% à 30%.
Ce travail concerne les techniques de reconstruction d'images tridimensionnelles pour la détection de petits foyers tumoraux et de métastases, à partir de données acquises en Tomographie d'Emission de Positons (TEP).
En TEP, la présence d'un foyer tumoral se traduit par une hyperfixation du traceur injecté, localisée au niveau de la tumeur.
Ceci nous conduit à modéliser la distribution volumique du radiotraceur à l'aide d'un mélange de lois, qui traduit le fait que chaque point de l'objet a une activité soit normale, soit surélevée.
Nous proposons de résoudre ce problème par une méthode de Maximum d'Entropie sur la Moyenne.
Les résultats obtenus avec l'approche proposée sont comparés à ceux fournis par deux méthodes de référence en milieu hospitalier.
Sur données simulées, les résultats obtenus avec MEM sont significativement meilleurs que ceux obtenus par les autres méthodes, au sens d'un critère d'évaluation développé afin de quantifier la qualité des images en terme de détectabilité d'hyperfixations.
La faisabilité clinique de la méthode est également illustrée.
En radiothérapie conformationnelle, le calcul des doses et des balistiques des faisceaux à appliquer pour irradier la tumeur se fait en se basant sur des images tomographiques (IRM ou scanner (TDM)) réalisées avant le traitement.
Celui-ci dure plusieurs séances réparties sur plusieurs semaines.
Au début de chaque séance le patient doit être installé sur la table de traitement dans les conditions initiales de planification.
Actuellement, les méthodes les plus utilisées pour ce repositionnement se basent sur l'anatomie externe du patient et supposent une immobilité des organes internes.
Ce travail présente une nouvelle approche, adaptée aux conditions cliniques, pour le repositionnement automatique du patient en radiothérapie de la prostate.
Elle est basée sur un repérage temps réel par échographie et une mise en correspondance rapide et précise avec des images générées dans le volume de planification.
La méthode exploite une modélisation statistique de la prostate pour extraire automatiquement ses contours.
Les premiers tests de la méthode dans les conditions réelles d'une séance de radiothérapie montrent que le repositionnement peut être obtenu avec une précision de l'ordre de 1.4 mm.
Les consonnes occlusives sont produites en réalisant une occlusion du conduit vocal, en créant une pression de la bouche derrière cette occlusion puis en relâchant cette occlusion.
Au moment de la détente, ces composantes comportent une transitoire initiale, une bouffée de bruit fricatif, et un intervalle où l'on observe une source sonore au niveau de la glotte et des transitions de formants.
Les modèles prédisent les niveaux absolus des ces différentes composantes pour divers lieux d'articulation des consonnes.
Ce papier présente une nouvelle approche pour l'égalisation des effets des lignes téléphoniques sur le signal observé à l'entrée d'un système de reconnaissance automatique de la parole afin d'augmenter la robustesse de ce système.
Des mesures effectuées sur des données téléphoniques réelles confirment que les lignes téléphoniques introduisent des composantes convolutives dans le signal observé.
Les effets de ligne sont presque constants pour un appel donné mais varient d'un appel à un autre.
Le filtrage adaptif proposé est comparé à deux techniques classiques, la soustraction du cepstre à long terme et le filtrage passe-haut des trajectoires cepstrales.
Des expériences de reconnaissance en mode indépendant du locuteur sont effectuées sur des bases de données téléphoniques.
Les résultats montrent que la réduction des effets des lignes téléphoniques améliore les performances d'une manière significative.
Ces résultats montrent aussi que la méthode proposée donne des meilleures performances que le filtrage passe-haut des trajectoires cepstrales.
Néanmoins, la soustraction du cepstre à long terme, qui n'est pas une approche en ligne, reste légèrement plus performante que l'approche proposée.
Quelques expériences ont été effectuées pour mesurer la quantité du signal nécessaire pour une bonne estimation des effets du canal.
Il semble que moyenner les cepstres des trames sur quelques secondes de parole permette d'obtenir une estimation fiable du cepstre du canal.
Dans le cadre d'une discussion sur le phénomène du Pétrarquisme, en tant que procédé d'imitatio à l'intérieur des différents genres littéraires, on étudie les modalités de transmission des schémas et des images pétrarquistes, en comparant les sonnets préliminaires (I et II) des Erreurs amoureuses de Pontus de Tyard (1549), analysés en parallèle avec Voi ch'ascoltate in rime sparse il suono et avec le premier sonnet des Amours de Ronsard.
Les deux premiers sonnets des Erreurs amoureuses représentent un dédoublement de l'apostrophe préliminaire traditionnelle, adressée à un public qui est spectateur des souffrances d'amour et prend part lui-même à ces souffrances.
Ces sonnets dérivent de Pétrarque le thème du giovenile errore, tout en interprétant erreur selon les deux acceptions principales, dans la polysémie du terme : erreur en tant que voyage (errance), et erreur en tant que faute.
Pour ce qui concerne Pontus de Tyard, de toute évidence Pétrarque est le modèle fondamental, aussi bien au niveau de la structure qu' au niveau du lexique.
En effet, c'est le niveau lexical qui nous fournit des renseignements dans ce domaine : l'emploi même des lexèmes errore/erreur nous démontre dans quelle mesure la tendance à la variatio sur l'archétype pétrarchiste devient un fait en premier lieu rhétorique.
Les termes de 'racine', 'affixe', puis 'suffixe' et 'préfixe' n'appartiennent pas au matériel lexical créé pendant l'Antiquité, mais comme celui de 'scheva' ou 'shewa', résultent de la découverte de la grammaire hébraïque par les grammairiens de langue latine, à la Renaissance.
Ces mots nouveaux, dont on examine ici la naissance et le développement, nous semblent caractéristiques d'un nouveau type d'approche des langues.
Pour la première fois, les grammairiens de langue latine sont confrontés à une tradition étrangère, vivante et prestigieuse.
La tradition grammaticale hébraïque, qu'ils découvrent, leur fournit aussi un champ de réflexion et des outils d'analyse.
De cette rencontre avec une langue et avec sa tradition critique, qui offre vaste matière à comparaisons, vont sortir des termes nouveaux et une perspective renouvelée sur l'analyse morphologique des langues.
L'article présente une méthode d'inversion acoustico-géométrique basée sur le calcul direct des dérivées par rapport au temps des sections et longueurs d'un modèle du conduit vocal.
Ce modèle du conduit est une concaténation de tubes uniformes dont la taille varie avec le temps.
Ces dérivées sont obtenues en résolvant un système algébrique linéaire d'équations.
Elles sont intégrées numériquement pour aboutir aux mouvements des sections et longueurs.
Des contraintes sur les pseudo-énergies potentielles et cinétiques sont imposées afin d'arriver à une solution unique.
Nous nous intéressons aux méthodes heuristiques pour résoudre un problème de classification de bouchons naturels en liège.
Plus précisément, nous cherchons à optimiser les valeurs de paramètres d'une règle de tri utilisée quotidiennement pour classer les bouchons.
Nous expérimentons plusieurs métaheuristiques et nous les comparons à l'aide d'un jeu de données réelles issues de l'industrie.
Les résultats que nous obtenons permettent d'améliorer le taux de classement actuel.
Nous proposons aussi, à partir de nos observations, de nouvelles pistes de recherche qui pourront permettre d'améliorer encore plus la qualité de la classification des bouchons en liège.
Nous présentons un système de lecture automatique des montants numériques des chèques dont le principe repose sur une technique de segmentation des caractères validée par la reconnaissance.
Ce système est décrit depuis la phase de localisation du champ montant sur le document numérisé, jusqu'à la génération de la liste des hypothèses de montant.
La segmentation, de type explicite, permet de déterminer des zones de coupure potentielles entre caractères et fournit une représentation spatiale des composantes segmentées.
Le meilleur chemin de segmentation du montant est déterminé par la combinaison des scores de reconnaissance des caractères, de la vraisemblance de la segmentation et de la probabilité d'apparition de ce montant.
Afin de pouvoir quantifier la robustesse de ce système, nous avons mesuré ses performances à partir d'une base de 10 000 images de montants de chèques réels.
Cet article décrit le système RailTel développé au LIMSI, destiné à l'accès vocal en Français aux horaires des trains de la SNCF et permettant d'évaluer l'adéquation des techniques vocales pour les services interactifs.
Le système utilisé pour le recueil de corpus des tests a été développé à partir du système Mask du LIMSI. Il fonctionne sur une station Unix avec une interface téléphonique de haute qualité.
Le système offre un dialogue à initiative partagée où l'utilisateur peut fournir les informations à tout instant.
Les utilisateurs expérimentés peuvent fournir toutes les informations en une seule phrase, tandis que les utilisateurs moins expérimentés ont tendance à donner de courtes réponses laissant le système les guider.
Les tests de RailTel ont été effectués selon une méthodologie commune définie par le consortium.
100 sujets naı̈fs ont participé aux tests, chacun a appelé le système une seule fois et rempli un questionnaire.
72% des sujets ont achevé leurs scénarios avec succès.
L'évaluation subjective du prototype était en majorité favourable, avec un intérêt d'utiliser un tel service.
Nous étudions le comportement des prédicteurs ARMA adaptatifs du point de vue de leur stabilité, en utilisant, pour l'adaptation, l'algorithme LMS récursif et un algorithme avec erreur a posteriori.
L'entrée du prédicteur est un signal stationnaire à bandes étroites, ou un signal non stationnaire de parole.
D'une part, nous montrons que l'utilisation de l'erreur a posteriori dans l'algorithme d'adaptation amortit les oscillations dues au phénomène d'autostabilisation, par rapport au cas de l'algorithme LMS.
D'autre part, nous faisons le lien entre l'instabilité de l'algorithme LMS et la non-stationnarité due à des sauts de puissance dans les signaux de parole.
Finalement l'utilisation d'un algorithme avec erreur a posteriori assure la stabilité au sens entrée bornée/sortie bornée, même pour -un signal non stationnaire.
Cet article étudie les performances d'un système d'identification du locuteur en mode dépendent du texte pour un petit ensemble de locuteurs.
Une phrase commune àtous les locuteurs est utilisée comme mot de passe, chaque locuteur disposant de son modèle acoustique de phrase représentépar un modèle de Markov caché(HMM).
Différentes méthodes d'estimation de modèles sont étudiées : l'une fondée sur un critère du maximum de vraisemblance (MLE), l'autre sur un critère du minimum d'erreurs de classification (MCE).
Le modèle MLE de chaque locuteur est estimé à partir des données d'apprentissage associées, alors que le modèle MCE est construit à partir de toutes les phrases d'apprentissage du groupe de locuteurs.
Dans une autre série d'expèriences, des phrases d'apprentissage supplémentaires, provenant soit des locuteurs du groupe, soit de locuteurs externes au groupe, sont utilisées pour estimer les modèles.
Les résultats expérimentaux montrent que les taux d'erreurs d'identification sont multipliés par 2 lorsque l'on passe d'un ensemble de 5 locuteurs àun ensemble de 10 locuteurs.
Les taux d'erreurs d'identification en ensemble ouvert et fermésont réduits de 25% lorsque MCE est utiliséàla place de MLE, alors que les taux de fausse acceptation d'imposteurs diminuent d'environ 10%.
L'amélioration la plus importante est obtenue lorsque des phrases additionnelles sont utilisées pour l'apprentissage, à la fois pour les modèles MLE et MCE.
Sous cette configuration, avec les modèles MCE, les taux d'erreurs d'identification sont approximativement 0.4% et 0.6% pour les ensembles de 5 et 10 locuteurs, alors que les taux d'acceptation des imposteurs sont respectivement de 4% et 10%, pour un taux de faux rejet de 5%.
Nous présentons un formateur de faisceau à double fonction applicable dans le cas d'un système de reconnaissance
La sortie à niveau de bruit diminué convient aussi bien pour la transmission que pour l'entrée d'un système de reconnaissance de la parole.
Les conditions ambiantes visées sont la voiture, l'atelier ou un bureau bruyant.
La structure sous-jacente est constituée par un formateur de faisceau Griffiths-Jim dirigé, avec interrupteur supplémentaire de détection de la parole à utiliser pour l'adaptation sélective des deux fonctions.
Ce formateur de faisceau est efficace pour la suppression des interférences stationnaires et des interférences non-stationnaires et constitue de la sorte une unité de pré-traitement destinée à une gamme d'applications de reconnaissance de la parole plus large que celles pourrait traiter un système de suppression de bruit de fond à canal unique.
Des expériences ont été réalisées dans une chambre à écho équipée d'un réseau à 4 microphones.
Les améliorations typiques en matière de rapports signal/bruit à des fins de communication se situent entre 4 et 12 dB.
L'amélioration effective du rapport signal/bruit à des fins de reconnaissance de la parole se situe entre 4 et 8 dB.
A nos jours, la synthèse de la parole automatise avantageusement les services en donnant oralement l'information contenue dans des bases de données d'ordinateur.
Quelques uns de ces services offrent des informations routières, des informations sur le trafic et sur les horaires, des quota de stock et des informations financières correspondantes, et des commandes sur catalogue.
Un service de télécommunications, qui connaı̂t un succès tout particulier, est l'automatisation des noms et adresses des abonnés (`Automated Customer Name and Adress' ACNA), connu parfois sous le nom d'annuaire inverse (`Reverse Directory Assistance' RDA). Ce service demande une synthèse de parole avec une grande intelligibilité et une bonne prononciation des noms, ce qui peut être atteint par l'état de l'art de la technologie de synthèse.
Néanmoins, même la meilleure des technologies actuelles n'est pas suffisamment performante pour s'appliquer directement dans des services complexes.
Un prétraitement d'un répertoire personnalisé est nécessaire pour transformer une liste de données, qui généralement contient des abréviations non conventionnelles, des acronymes non connus par le synthétiseur, et un ordre de mots chiffré, en une phrase appropriée pour la synthèse.
Nous décrivons nos programmes de prétraitement des répertoires qui sont utilisés avec succès dans des implantations de la synthèse chez deux opérateurs téléphoniques majeurs aux états unis.
Il est aussi nécessaire que les noms des localités, qui ont une variabilité géographique considérable, soient prononcés en accord avec les accents locaux ; sinon, le service sera perçu comme étranger par les usagers.
Nous décrivons aussi des expériences visant à déterminer si le naturel de la parole enregistrée pour des prompts et des messages fixes compensent l'effet non désiré de la discontinuité entre les mots synthétisés et naturels.
Dans cet article nous proposons une approche de reconnaissance de l'écriture manuscrite.
L'objectif étant de proposer un système indépendant de la nature du script, nous procédons alors sans segmentation.
Pour valider l'approche proposée nous avons effectué des expérimentations sur deux bases de données de référence, la base de mots arabes IFN/ENIT et la base IRONOFF de mots latins.
Les résultats montrent que le système proposé donne de bons résultats comparables aux meilleurs approches rapportées dans la litérature, aussi bien sur le Latin que sur l'Arabe.
Dans cet article, des développements récent concernant l'extraction, à partir de l'onde de parole, des indices dépendants du locuteur, l'identification et la vérification automatiques du locuteur, l'adaptation au locuteur en reconnaissance automatique de la parole et les techniques de conversion de voix sont discutés.
L'information concernant le locuteur se trouve à la fois dans l'enveloppe spectrale et dans les traits prosodiques de la parole.
Cette information peut de plus être classée en traits temporels et traits dynamiques.
Les méthodes de vérification/identification du locuteur peuvent être divisées en méthodes dépendantes du texte et méthodes indépendantes du texte.
Bien que les techniques de vérification du locuteur dépendantes du texte aient presque atteint le niveau de développement approprié pour l'implémentation pratique, les techniques indépendantes du texte en sont toujours au stade de la recherche fondamentale.
En reconnaissance de parole, des algorithmes d'adaptation au locuteur supervisés et non supervisés ont récemment été proposés, et des progrès remarquables on été réalisés dans ce domaine.
L'amélioration de la qualité de la parole synthétique par l'ajout de caractéristiques vocales individuelles et la conversion de l'individualité vocale synthétique d'un locuteur à l'autre sont des sujets de recherche peu exploités actuellement qui devraient être étudiés dans un proche avenir.
La recherche sur l'information dépendante du locuteur constitue l'une des plus importantes directions à suivre pour réaliser des systèmes avancés de traitement de l'information dans le domaine de la parole.
La communication interethnique au Cameroun se caractérise souvent par un discours dévalorisant l'identité ethnique de l'autre.
Lequel discours apparaît généralement sous forme d'insultes, de vannes, railleries ou blagues, racontées, chantées, romancées, radiodiffusées et télévisées parfois, et colportées de génération en génération.
Cet article rend compte de quelques stratégies employées pour dénigrer l'altérité ethnique au Cameroun.
Les analyses menées à partir de données (questionnaires, observations participantes, interviews), collectées à Yaoundé et dans d'autres régions du pays, montrent comment les Camerounais utilisent les emprunts, compositions nominales, métaphores, glissements sémantiques, métonymies, entre autres, pour déprécier, déshumaniser, diaboliser les membres de certains groupes ethniques et/ou pour gommer, phagocyter ou contester l'identité ethnique d'autres groupes.
La reconnaissance des syllabes est un problème-clé pour les systèmes de reconnaissance de larges vocabulaires en Madarin.
Traditionnellement, le ton et la syllabe de base correspondant à une syllabe donnée sont reconnus séparément.
Dans cet article, on propose une approche de la reconnaissance des syllabes en Madarin basée sur la classification d'unités sub-syllabiques : parties initiales, finales et transitions.
Les unités finales sont classées en fonction des variations des tons pour optimiser les possibilités de discrimination tonale.
Nous avons développé un système de reconnaissance des syllabes en Mandarin qui utilise des modèles de Markov cachés (HMM) à partir de paramètres cepstraux et dans lequel les syllabes de base et leurs tons associés sont reconnus conjointement.
Les résultats expérimentaux montrent que ce système fournit une taux de reconnaissance plus élevé que les systèmes de reconnaissance de syllabes tranditionnels quand on utilise une quantité suffisante de données d'apprentissage.
On montre également que les performances de ce système peuvent être améliorées en y incorporant un système de reconnaissance de tons.
Cet article se propose de dresser une synthèse et une classification des diverses applications du dialogue oral homme-machine.
Il présente, dans une première partie, les avantages et les limites de la parole comme moyen de communication entre un utilisateur et un système automatisé.
Le problème essentiel pour le développement d'interfaces utilisateur à composante orale, à côté du choix d'applications appropriées, est la reconnaissance de la parole, spécialement de la parole continue.
Comme les approches diffèrent suivant le type d'applications, nous présentons tout d'abord les problèmes et les techniques spécifiques à l'entrée orale de données et, à titre d'exemple, nous décrivons rapidement l'approche que nous avons adoptée pour la machine à dicter que nous développons dans notre laboratoire.
Ensuite nous abordons la compréhension et la gestion de dialogues oraux.
Pour illustrer cette présentation, nous présentons l'architecture et les fonctionnalités de divers prototypes que nous avons mis en œuvre : système de messagerie électronique, dialogue pour la commande d'une console sonar et dialogue entre un chirurgien et un système d'aide au diagnostic.
Enfin, nous détaillons le gestionnaire de dialogues DIAL, en cours de développement, dont l'objectif est d'aider et de guider un utilisateur dans des activités cognitives complexes telle la recherche de renseignements administratifs.
Nous présentons et discutons le modèle SAPHO (segmentation par les connaissances acoustico-phonétiques) mis en œuvre en langage AWK sous UNIX, sur une station de travail Masscomp.
Ce système est conçu comme une procédure de segmentation indépendante du locuteur fondée sur une reconnaissance préalable du mode d'articulation phonétique.
Dans la plupart des modèles RAP, les connaissances phonétiques sont toujours utilisées, au moins de façon implicite.
Elles doivent l'être de façon explicite.
Les unités phonémiques ne peuvent pas être directement construites à partir du signal acoustique ; elles ne sont pas encore disponibles à la sortie de SAPHO.
Suivant le modèle de Construction de Niveaux (Level Building), SAPHO fournit un ensemble hiérarchisé de propriétés et de segments acoustiques, de propriétés et de segments phonétiques congruents avec les unités phonétiques et leur structure interne.
La souplesse de ce système est assurée par sa modularité.
La fiabilité de SAPHO est corroborée par l'exactitude des résultats.
Des spectres moyens à long terme (LTS) définis sur 400 canaux de largeur de bande constante (12.5 Hz) ont été tirés de productions françaises au moyen d'un analyseur BK 2033.
Le coefficient de correlation a été utilise pour étudier la variabilité résiduelle intralocuteur du LTS en conditions tant inter-qu'intratexte.
D'importantes différences provenant des sujets ont été relevées dans les deux types de conditions.
Elles indiquent l'existence de variations de la coherence vocale parmi les locuteurs.
L'abaissement des corrélations en conditions intertexte révéle, en outre, la dépendance du LTS vis-à-vis du contenu.
La nécessite d'entreprendre des recherches fondamentales sur le LTS avant de passer aux applications est dés lors soulignée.
L'utilisation massive des catégories cliniques de l'aphasie dans les recherches en neurolinguistique et en neuropsychologie cognitive laisse présumer que les classements des patients reflètent les atteintes du système de traitement du language selon des cadres théoriquement pertinents.
On examine ce présupposé en se rapportant plus particulièrement à l'aggramatisme.
De nombreuses raisons amènent à s' interroger sur la cohérence de l'aggramatisme comme entité psychologique.
Pour répondre aux objections, il est nécessaire de remplacer les intuitions cliniques qui fondent cette catégorie d'aphasie par des critères objectifs permettant un groupement theoriquement pertinent des patients.
Pour cela on doit établir une distinction theoritiquement motivée entre la variation inter et intra catégorie.
Dans le cas de l'agrammatisme il semble des obstacles méthodologiques sérieux rendent ce propos impossible.
Les théories ne doivent donc pas prendre les catégories telles que l'agrammatisme comme des données psychologiques, particulièrement lorsque le but de la recherche est de comprendre les mécanismes de traitement du langage ou les déficits aphasiques eux-mêmes dans l'étude de l'aphasie.
Notre conception de ce que le mécanisme de production de parole essayd'implémenter lors de l'acte de parole vient de la linguistique.
Mais la linguistique a d'abord développé ses méthodes pour d'autres buts.
Au début du 19èmes siècle, elle a mis au point une méthode pour retracer les relations de parenté entre des mots de même origine et les sons qui les constituent.
Cette méthode, la méthode comparative, impliquait d'établir une filiation optimale entre ces formes via une forme-parent reconstruite.
La linguistique structuraliste du 20ème siècle (y compris la phonologie générative) a appliqué essentiellement la même méthode pour tâcher de découvrir les constituants phonémiques sous-jacents des mots.
Bien que la structure sous-jacente découverte de cette manière puisse constituer une bonne hypothèse de ce que sont les èlèments mentaux qui déterminent les phrases effectivement produites, il y a des raisons de suspecter que cela est trop simple.
Trop d'importance est attachée à la simplicité du système et à la fonction purement lexicale (en opposition avec les fonctions démarcative et stylistiqué) des éléments dans la parole.
Cet article présente quelques essais préliminaires pour différencier les variantes phonétiques de la parole qui prennent leur origine dans une seule forme sous-jacente de celles qui proviennent de formes sous-jacentes séparées (bien qu'elles puissent avoir eu historiquement une source commune).
L 'information n 'est utile que si elle répond à un besoin et lorsqu 'elle est communiquée de manière à en faciliter la compréhension et l'utilisation.
Nous avons tous fait l'expérience de la recherche d'information sur le web, multipliant les requêtes à l'aide d'un moteur de recherche dans l'espoir d'obtenir un lien intéressant, et finalement, essayant d'organiser ensemble les différents résultats obtenus.
Dans cet article, nous présentons le Virtual Document Planner, une plateforme conçue pour aider à la génération de documents personnalisés.
Le VDP sélectionne et organise de l'information pouvant provenir de sources variées et nécessitant d'être présentée de manière à en faciliter la compréhension et l'utilisation.
Pour illustrer cette approche, nous présentons une application dans laquelle le VDP génère des brochures présentant les recherches menées à CSIRO dans un domaine donné.
Enfin, nous présentons les résultats préliminaires d'une expérience que nous avons menée pour évaluer l'impact d'une telle approche sur l'utilisateur.
Cet article propose une comparaison des processus de sélection séquentielle dans la recherche d'indices pertinents permettant la prédiction anticipée de la syncope lors d'un test diagnostique.
Afin d'exploiter au mieux les qualités des différentes approches de sélection de variables, une approche hybride a été proposée combinant un processus séquentiel et un algorithme génétique.
Les résultats obtenus permettent de prédire l'apparition de la syncope avec une sensibilité de 100 % et une spécificité de 94 %.
Le rôle de l'Analyse des Scènes Auditives dans la perception de la parole est encore mal défini, notamment en ce qui concerne l'établissement d'un groupement perceptif des formants par les auditeurs.
Les expériences présentées ici ont pour objet d'étudier le rôle des formants vocaliques dans la perception de syllabes synthétiques voyelle-nasale et l'importance de la continuité entre voyelle et nasale.
Lorsque la transition de la voyelle vers la nasale est abrupte, l'accroissement de la fréquence de F2 favorise la perception d'un /n/.
L'introduction d'une transition formantique continue supprime cet effet, permettant aux auditeurs de percevoir le phonème approprié pour chacune des nasales.
Cependant, lorsque transition formantique et prototype nasal sont discordants, le percept est déterminé uniquement par la pente de la transition.
Dans chacune des expériences présentées, le percept est donc déterminé par la cible de la transition du F2 vers la nasale, cet indice dominant la structure formantique de la consonne nasale.
Les résultats ne favorisent pas un modèle de poursuite des formants et conduisent plutôt à envisager des processus d'appariement de formes.
Cet article présente une méthode d'adaptation des modèles de Markov cachés (HMMs) pour la reconnaissance de parole téléphonique.
Notre but est d'adapter automatiquement les paramètres HMM à l'environnement téléphonique.
Dans cet article, on étudie deux types de d'adaptation basées sur des transformations.
L'une est la transformation par biais et l'autre la transformation affine.
Pour estimer les paramètres de la transformation, on applique une technique d'estimation Bayésienne qui incorpore la connaissance a priori dans la transformation.
Les expériences montrent que l'approche proposée peut être appliquée avec succès tant pour l'auto-adaptation que pour l'adaptation supervisée.
De plus, on montre que les performances de la reconnaissance de parole téléphonique utilisant l'adaptation Bayésienne sont supérieures à celles utilisant l'adaptation par maximum de vraisemblance.
On montre enfin que la transformation affine est également nettement plus efficace que la transformation par biais.
Nous expliquons le bégaiement par un déficit au niveau de la quantité de neurones pouvant véhiculer l'information sensorimotrice.
Nous pensons que le bègue manque des ressources de traitement nécessaires au maintien et à l'adaptation des modèles internes qui sous-tendent la production de la parole.
Nous précisons les mécanismes computationnels sous la forme d'un circuit d'un contrôleur adaptatif autorégulé pour le contrôle d'un système à sorties et à entrées dynamiques multiples, non-linéaires et variables.
Le système auditif dispose souvent d'une quantité suffisante d'indices qui lui permettent de déterminer si les composantes du son se prolongent pendant ces obstructions.
Cet article recense les situations au cours desquelles des assomptions de continuité sont garanties et démontre comment les principes sous-tendant ce qu'on appelle “l'illusion de continuité” peuvent être utilisés au sein d'un modèle informatique de séparation des sources acoustiques.
Pour traiter de grands lexiques (plus de 2000 mots), les systèmes de reconnaissance automatique de la parole (RAP) utilisent une représentation phonétique interne du signal de parole et des modèles phonémiques de prononciation du lexique pour rechercher la séquence de mots émis ou la phrase.
Il est donc possible de modéliser différentes prononciations d'un mot dans le lexique.
En allemand, on observe que les locuteurs individuels prononcent les mots d'une manière typique qui dépend de divers facteurs comme le sexe, l'âge, le lieu d'habitation, le lieu de naissance, etc.
Notre but est d'améliorer la reconnaissance de parole en adaptant automatiquement les modéles de prononciation du lexique aux nouveaux locuteurs.
Une autre méthode présentée dans cet article concerne l'adaptation au locuteur par ré-estimation des probabilités a posteriori des unités phonétiques utilisées dans un système de RAP “bas-haut”.
Une hypothèse de mots est basée sur le produit des probabilités a posteriori des unités phonétiques produites par la classification.
Normalement, ces probabilités sont estimées pendant la phase d'apprentissage et restent fixes pendant la reconnaissance.
Nous proposons un algorithme qui prend en compte les observations des confusions typiques entre unités phonétiques pour le locuteur et adapte les probabilités a posteriori de façon continue.
Les sources de bruit magnétique sont nombreuses et viennent perturber les signaux magnétiques sous-marins.
Afin de détecter toute distorsion du champ magnétique terrestre, engendrée par le déplacement d'une masse ferromagnétique, nous utilisons quatre magnétomètres pour lesquels nous savons que le signal utile (s'il existe) ne peut être vu que par l'un d'entre eux.
Nous proposons un outil graphique interactif, CA Viz, qui permet de visualiser et d'extraire des connaissances à partir des résultats de l'AFC sur les images.
En analyse de données textuelles, le tableau de contingence croise mots et documents.
Pour l'adaptation de l'AFC aux images, la première étape consiste donc à définir des « mots visuels » dans les images (analogue des mots dans les textes).
Ces mots sont construits à partir de descripteurs locaux (SIFT, Scale Invariant Feature Transform) des points d'intérêt des images.
CAViz projette le nuage de points dans des plans factoriels et permet d'extraire visuellement des informations intéressantes comme des mots caractérisants, des facteurs importants en utilisant des indicateurs pertinents de l'AFC (qualité de représentation et contribution à l'inertie).
Une application à la base Caltech4 démontre l'intérêt de CAViz pour l'analyse des résultats de l'AFC.
Mrayati et al. utilisent les fonctions de sensibilité d'un tuyua uniforme pour proposer un découpage en huit régions associées aux combinaisons possibles des variations des trois premières résonances;
ils mettent ainsi en évidence des propriétés géométrico-acoustiques de symétrie et de compensation. Les auteurs considèrent que la production des voyelles et des consonnes repose sur ces propriétés : le conduit vocal peut être découpé en ces huit zones auxquelles ils associent des caractéristiques morphologiques et articulatoires.
Ils proposent une nouvelle théorie de la production des voyelles et un nouveau système phonologique universel des consonnes.
Mais plusieurs critiques peuvent être faites aux auteurs : • - les limitations des fonctions de sensibilité sont sous-estimées ; • - l'anthropomorphisme du découpage est pauvre ; • - leurs prédictions ne permettent pas de retrouver des données acoustiques bien connues ; • - la classification universelle est en contradiction avec des connaissances phonétiques de base.
D'une manière générale ce type d'approche semble intrinsèquement très limité : le conduit vocal peut difficilement être réduit à une série de tuyaux manipulés indépendamment les uns des autres, sans aucune référence à un modèle articulatoire sous-jacent.
La Nouvelle Théorie ne pourra être véritablement appréciée qu'une fois replacée dans son véritable champ de validité : la description des propriétés acoustiques du conduit vocal autour de sa position neutre, et un outil pour la synthèse de la parole.
Dans le cadre d'un système de reconnaissance automatique de mots isolés par approche analytique pour un vocabulaire de grande taille, nous avons envisagé une adaptation automatique du système au locuteur.
L'adaptation se fait par l'apprentissage automatique des formes de référence du locuteur, ainsi que par l'adjustement automatique des paramètres du système.
Un algorithme de cadrage de segments phonétiques décodés à l'aide de traits acoustico-phonétique peu dépendants du locuteur est à la base de cet apprentissage.
La session d'apprentissage a été testée avec succès sur 18 locuteurs parmi un échantillon de 20 personnes (10 femmes et 10 hommes) et les formes de références ainsi obtenues ont fourni de bons résultats pendant la phase de reconnaissance.
Une analyse des voyelles de 15 locuteurs fondée sur la statistique descriptive et l'interprétation statistique a été entreprise en vue délaborer des procédures de normalisation et de génération automatique des formes de référence des voyelles d'un locuteur.
Dans le cadre du traitement STAP, une modélisation autorégressive (AR) des interférences utilisée avec un détecteur appelé Parametric Adaptive Matched Filter (PAMF) donne lieu à un filtre de réjection du fouillis pour lequel le domaine d'entraînement est réduit.
La principale difficulté de cette approche réside alors dans l'estimation des matrices AR à l'aide des données d'entraînement.
Dans cette publication, les auteurs proposent une estimation récursive fondée sur un filtrage de Kalman et ses variantes.
Une étude comparative des différentes méthodes est menée sur les données fournies par la DGA - Maîtrise de l'Information.
Cet article présente une architecture logicielle basée sur les services web et permettant la création et l'évaluation d'applications visuelles interactives.
Les services web sont une standardisation d'échange de données dans un système distribué, tel que le web.
Ils servent principalement à la publication de données (au moyen d'API) stockées dans des bases de données, mais peuvent également servir au traitement de celles-ci.
Nous montrons que leur composition permet la création de représentations visuelles de données, en reconstituant le modèle de référencede celles-ci.
Les visualisations ainsi générées peuvent être rendues interactives une fois couplées avec un programme interactif (tel qu'un navigateur web), afin de permettre à l'utilisateur de réaliser des tâches d'exploration et d'analyse visuelle de données.
Nous présentons une interface de composition de services et l'application à la visualisation de graphes et de nuages de mots.
Enfin nous montrons comment l'usage des logs générés côté serveur permet la représentation et l'évaluation de l'activité de l'utilisateur.
La réalisation d'une Machine à Ecrire à Entrée Vocale en Françcaise nécessite à la fois l'étude de comment faire la reconnaissance acoustique, et de comment obtenir un modèle de la langue Française.
Un tel projet a été lancé au LIMSI il y a 15 ans.
Cet article présente les différentes étapes qui ont été franchies depuis le début du projet.
Tout d'abord, les résultats d'une étude de la transformation phonè—graphème pour des suites de phonémes exactes, avec un grand vocabulaire et une syntaxe du langage naturel, ont été présentés en 1979.
Ces résultats ont été alors élargis, avec quelques essais pour traiter des suites de phonèmes entachées d'erreurs simulées, tout en appliquant la même approche au cas de la conversion sténotypes-graphèmes.
Dans le projet ESPRIT 860 “Analyse Linguistique des Langues Européennes”, notre approche de la modélisation du langage a été comparée à d'autres approaches pour le traitement de 7 langues européennes.
La liaison entre la reconnaissance acoustique et le modéle de langage a conduit à un système de reconnaissance complet (“Hamlet”), pour un vocabulaire limité (2000 mots), prononcés de façon isolée, qui a ensuite été étendu à un vocabulaire de 5000 mots, grâbe à un circuit intégré de Programmation Dynamique (MUpCD) également réalisé au LIMSI.
Cette étude a conduit à la conclusion que la dictée de texte par mots isolés n'était pas acceptable.
Nous développons à présent un système de reconnaissance de parole continue multilocuteur pour des vocabulaires de 5000 à 20000 mots.
Le principe de la stéganalyse est de classer un document incriminé comme original ou comme stéganographié.
Il est montré que l'écart type des résultats obtenus habituellement en classification peut être très important (jusqu'à 5 %) lorsque des ensembles d'entrainements comportant trop peu d'échantillons sont utilisés.
Ces tests sont menés sur six algorithmes de stéganographie, utilisés avec quatre taux d'insertions différents : 5,10,15 et 20 %.
D'autre part, les caractéristiques sélectionnées (généralement 10 à 13 fois moins nombreuses que dans l'ensemble complet) permettent effectivement de faire ressortir les faiblesses ainsi que les avantages des algorithmes utilisés.
Nous présentons dans cet article une nouvelle technique de transformation de timbre de la voix.
Cette technique s'articule autour d'un synthétiseur dérive de l'approche PSOLA (Pitch-Synchronous Overlap and Add) et d'un module de transformation des paramètres spectraux.
Le synthétiseur allie décomposition source-filtre et modification prosodique du signal d'excitation par application de TD-PSOLA (Time Domain PSOLA).
Deux approches de transformation spectrale, dérivées de techniques d'adaptation en reconnaissance de parole, sont comparées : la Régression Linéaire Multiple (LMR) et l'Alignement Dynamique en Fréquence (DFW).
Une étape préliminaire de quantification vectorielle permet de rendre ces transformations dépendantes des réalisations acoustiques des sons.
Un test d'écoute formel démontre que le synthétiseur permet d'obtenir une voix “transformée” d'un naturel satisfaisant.
L'interaction main-libre est très importante pour augmenter la flexibilité des applications actuelles de reconnaissance de la parole et pour développer de nouvelles applications pour lesquelles l'utilisateur ne peut pas être gêné par un microphone tenu dans la main ou posé sur la tête.
Lorsque le microphone est loin du locuteur, le signal reçu est perturbé par des dégradations de différente nature souvent imprévisibles.
L'utilisation de microphones spéciaux ou de systèmes multi-microphones représente une façon de réduire les effets des bruits environnementaux.
Des techniques de traitement robuste et d'adaptation peuvent également être utilisées pour compenser les différentes sources de variabilité qui peuvent agir à l'entrée d'un système de reconnaissance.
Le but de cet article est de revoir certaines des hypothèses relatives aux différentes sources de variabilité et de discuter à la fois des systèmes de “transducers” spéciaux et de techniques de compensation/adaptation qui peuvent être adoptées.
Plus spécifiquement, cet article considérera l'utilisation de réseaux de microphones pour traiter certains des effets non désirés résultant de l'acoustique ambiante (par exemple, réverbération) ainsi que du bruit cohérent/incohérent (par exemple, locuteurs simultanés, ventilateur d'ordinateur).
Pour finir, nous présentons des expériences qui ont été réalisées sur des signaux réels et simulés.
Cet article examine les caractéristiques de l'intonation de plusieurs types de “non-mots” comme les chiffres, dates, heures et abréviations, que l'on trouve dans des textes et qui sont aisément identifiables.
Cet article présente des illustrations de ce phénomène et propose des heuristiques pour sa prise en compte dans un système automatique.
Une évaluation formelle donne un taux de succés de plus de 94% pour ces heuristiques.
Les avantages et inconvénients de ce traitement sont discutés et des suggestions sont faites pour de futures recherches dans ce domaine.
Cet article propose l'édition originale d'une maqāma jusqu'alors inconnue et attribuée à Badīʿ al-Zamān al-Hamad̠ānī (m. 398/1008).
Il établit dans un premier temps un état de la recherche sur les manuscrits des Maqāmāt d'al-Hamad̠ānī puis examine les raisons pour lesquelles le texte de cette maqāma ne se trouva préservé que dans un seul manuscrit [Yale University, Beinecke Library, Salisbury collection no. 63].
Celui-ci, copié en 603/1206, était au demeurant bien connu des chercheurs européens, pour s'être trouvé entre les mains d'Everard Scheidius (1742-1794), Silvestre de Sacy (1775-1838) et Edward Eldridge Salisbury (1814-1901).
La maqāma qui y est insérée narre la vente malhonnête par un médecin d'ingrédients médicinaux censés avoir été élaborés à partir de substances pharmacologiques rares.
Nous faisons figurer en facsimilé le texte de cette maqāma que les auteurs de la présente contribution ont intitulée al-Maqāma l-Ṭibbiyya, ainsi qu'une édition critique et une traduction annotée en langue anglaise.
Suit une analyse détaillée de ladite maqāma, qui en examine la forme, le sujet, la langue et le style, en relation avec le corpus des autres maqāmāt d'al-Hamad̠ānī.
En conclusion, nous avançons un certain nombre d'hypothèses sur la possible authenticité de cette œuvre perdue.
La théorie formelle de l'apprentissage doit se concevoir comme un moyen de relier les théories de grammaire comparative aux études sur le développement linguistique.
Aprés avoir brièvement passé en revue les concepts pertinents, on examine dans cet article certains effets formels dans la théorie de l'apprentissage qui suggèrent des contraintes correspondantes sur la théorie linguistique.
On porte une attention particulière à la question : combien y-a-t-il de langues naturelles possibles ?
Le radar à visée latérale et à ouverture synthétique est un système d'imagerie micro-onde capable de produire des images de très haute résolution des terrains, et ceci à partir d'un traitement approprié des signaux reçus par une antenne de faible dimension.
Dans cet article, nous présentons la formulation exacte du signal reçu, ainsi que les traitements associés afin de former l'image.
La qualité de l'image ainsi formée est déterminée par celle de la fonction d'ambiguïté.
Cette dernière est analysée et optimisée pour deux critères de performance.
Pour un récepteur filtre adapté (récepteur optimal au sens meilleur rapport signal sur bruit), la forme d'onde optimale est une onde FM non linéaire dont la fonction d'ambiguïté est de type Taylor.
La fonction de pondération optimale en azimut est liée à celle de Taylor par une transformée de Fourier.
Pour un récepteur filtre de Wiener (récepteur optimal au sens des moindres carrés) la forme d'onde optimale est la première fonction sphéroïdale.
Afin de mesurer simultanément les quatre termes de la matrice de rétrodiffusion d'une cible, l'émission de deux ondes optimales orthogonales est discutée.
Les systèmes d'extraction d'objets sont mis à mal par la diversité de ces derniers.
Leur adaptation est donc nécessaire pour maintenir des performances équivalentes quelle que soit la nature des objets sur lesquels ceux-ci sont appliqués.
S'attachant plus particulièrement, dans l'optique de cette adaptation, à la tâche d'optimisation du paramétrage de ces systèmes, nous proposons dans cet article une méthode originale de ciblage de l'optimisation aux seuls paramètres des opérateurs du système estimés responsables des différentes catégories d'erreurs produites par le système.
Cette méthode s'appuie alors sur deux analyses distinctes.
La première porte sur les performances du système considéré et permet d'extraire les différentes catégories d'erreur déjà mentionnées.
La seconde concerne le fonctionnement des différents opérateurs composant le système et donne lieu à la détermination d'un opérateur responsable pour chaque catégorie d'erreur.
Une application de cette méthodologie à un système de détection de texte est par ailleurs détaillée.
Nous verrons que le résultat de ces recherches est loin d'être réductible au seul travail philologique : plutôt une des retombées critiques et épistémologiques qui permette d'aller plus loin.
Il existe un fort consensus pour dire que les sons et les patrons sonores observés dans le babillage et dans les premiers mots sont fondamentalement les mêmes.
Cet état commun relève de la notion de “Frame Dominance” — un schéma syllabique produit par une oscillation ouverture/fermeture de la mandibule qui prédomine dans ces deux étapes du développement, la capacité des autres articulateurs, et en particulier celle de la langue, à produire des changements intra- et inter-syllabiques étant limitée.
La question de savoir si les premiers mots sont similaires à tous égards au babillage, a étéévaluée chez 4 sujets, par le biais d'un ensemble de données correspondant à 152 heures d'enregistrements audio.
Les progrès observés dans les mots prennent la forme d'une augmentation du panachage des énonciations, principalement dû à un panachage des voyelles, dont l'essentiel provient d'un accroissement de l'usage des voyelles hautes et moyennes arrières, en particulier en position finale dans le mot.
La présence de régression et la nature limitée des progrès observés nous semblent être une indication de la force du patron de “Frame Dominance”, et de la difficulté qui en découle, de s'en échapper.
Cet article présente une série d'études phonétiques qui sont fondées sur une analyse du corpus TIMIT de l'anglais américain parlé, et qui sont susceptibles d'intéresser les spécialistes de linguistique et de reconnaissance de la parole.
D'abord, nous discutons les avantages et les défauts de TIMIT pour la recherche linguistique, et nous indiquons l'esquisse d'une méthodologie utilisant la base de données.
Ensuite, plusieurs petites enquêtes révèlent de nouveaux résultats sur les effets du sexe et du dialecte des locuteurs sur la prononciation.
Cet article essaie d'utiliser la base de données pour examiner la variation attribuée aux différences de sexe et de dialectes afin d'établir les différences qui pourraient mériter une étude experimentale plus approfondie.
Ce rapport porte sur la variation entre locuteurs qui se manifeste dans certaines caractéristiques phonétiques qui sont souvent associées à la réduction, telles que le débit de la parole, les relâches d'occlusives, les sons battus, les voyelles centrales, les laryngales, les consonnes syllabiques, et le processus de palatalisation.
Plus précisément, nous suggérons que les traits phonétiques qui sont le plus fréquent chez les locuteurs mâles sont ceux qui caractérisent la réduction dans le langage parlé.
La disponibilité croissante de grandes collections de documents faiblement structurées a fait émerger en recherche d'information le besoin d'une structuration globale de ces collections et de l'établissement de liens sémantiques entre documents.
En Recherche d'information, les principaux éléments de structuration globale de corpus utilisés sont l'hyperlien entre documents et la structuration des documents en hiérarchies de concepts.
Nous proposons ici un algorithme pour construire et maintenir des hiérarchies de concepts et de documents de manière automatique.
Nous présentons également des mesures de qualité pour l'évaluation des hiérarchies générées et des tests effectués sur des données issues du site Looksmart qui montrent la pertinence des méthodes proposées.
Ce travail concerne la prosodie du suédois dans le cadre de la synthèse de la parole.
On examine deux problèmes principaux : la prominence et la formation des groupes prosodiques.
Dans ce modèle de la prosodie du suédois, les niveaux de prominence (accent tonique, accent mélodique, focus) sont représentés hiérarchiquement et de façon multidimensionelle pour différents domaines (syllabe, pied, mot).
La formation des groupes prosodiques implique à la fois une cohérence dans la forme des combinaisons spécifiques des gestes accentuels existants et un marquage des gestes de frontières.
On décrit les principales caractéristiques de ce modèle d'intonation.
Les éxperiences sur la prominence incluent la modélisation des durées dans le cadre d'une base de données de parole et d'un système de synthèse par règles ou l'alternance accentué-non accentué apparait être le facteur le plus important pour la durée.
Une autre expérience concerne l'étude des principales différences dans les caractéristiques temporelles des gestes intonatifs pour l'accent focal entre les mots composés et les mots simples à accent II.
Les expériences sur la formation des constituants portent sur des données de production extraites de divers corpus de parole ainsi que sur leur synthèse et leur perception.
Nos expériences montrent que les indices de cohérence et les indices de frontière contribuent ensemble à la mise en évidence de constituants qui sont typiquement signalés par une combinaison d'indices de F 0 et de durée.
Nour prévoyons de poursuivre ce travail en modélisant la prosodie du suédois dans un contexte de dialogue et pour la synthèse à partir de concepts.
Nous comparons dans ce papier deux méthodes pouvant être utilisées pour annoter phonétiquement et de manière automatique un corpus de parole continue, comme c'est généralement nécessaire pour la mise au point de systèmes de reconnaissance ou de synthèse de la parole.
Les deux systèmes ont été évalués sur des phrases lues n'ayant pas servi à l'entraı̂nement des systèmes (HMM ou hybride) et manuellement segmentées.
Cette étude met en évidence les avantages et inconvénients de chacune des méthodes.
Le système basé sur le synthétiseur a le grand avantage qu'aucune phase d'entraı̂nement (et donc aucun grand corpus segmenté) n'est nécessaire, alors que les systèmes classiques basés sur les HMMs peuvent facilement prendre en compte des transcriptions phonétiques multiples.
Ces méthodes de segmentation automatique sont d'une grande importance pour le dévelopement de systèmes de synthèse et de reconnaissance de la parole multilingues.
Vérard fit paraître son editio princeps du Merlin en 1498, dans une belle édition en trois tomes in-folio ; le troisième tome contient les Prophéties.
Ce texte difficile – « détestable logorrhée » selon Langlois – est doté par le grand éditeur d'un jeu de rubriques d'une remarquable densité : comme aucun des manuscrits existants ne bénéficie de rubriques, c'est vraisemblablement l'atelier de Vérard qui en fut responsable.
Alors que les rubriques introduisant les chapitres « narratifs » du texte – les malheurs de Merlin – se contentent très souvent de composer un titre à partir de quelques mots saisis dans la/les première(s) phrase(s) du chapitre, pour les prophéties proprement dites le rubricateur semble avoir voulu saisir, plus ou moins bien, la totalité du chapitre.
Avec la table en début de volume, Vérard et son atelier semblent avoir en vue – toutes proportions gardées – de fournir un instrument de travail.
Nous présentons différents algorithmes de génération du signal qui améliorent de façon significative la qualité sonore de systèmes de synthèse de parole à partir du texte par concaténation d'unités acoustiques (Moulines et Charpentier, 1988 ; Hamon et al., 1989).
Ces algorithmes qui permettent de modifier les paramètres prosodiques et de concaténer les unités acoustiques sont fondés sur le principe de l'addition-recouvrement synchrone de la fréquence fondamentale de formes d'onde élémentaires (PSOLA).
Les modifications du signal de parole sont réalisées soit dans le domaine fréquentiel (FD-PSOLA), soit directement dans le domaine temporel (TD-PSOLA), suivant la résolution spectrale et temporelle de la fenêtre utilisée lors du processus de synthèse.
Nous discutons les différentes catégories de distorsions entraînées par ces algorithmes.
Les recherches entreprises jusqu'à maintenant sur le nom propre en français nous permettent de mettre à profit les résultats auxquels ont abouti les rhétoriciens, les grammairiens et les linguistes pour un traitement lexicographique bilingue des unités lexicales et phraséologiques contenant des noms propres employés figurativement en français et macédonien.
Dans cet article, nous essayons de distinguer l'ensemble des critères pertinents permettant au lexicographe de sélectionner et recenser ces unités, de proposer des solutions sur leur traitement et d'identifier les problèmes et les spécificités du nom propre.
Nous présentons un algorithme d'intelligence en essaim pour résoudre le problème du fourragement dans le cas discret.
Nous illustrons l'algorithme proposé à l'aide de simulations et nous faisons une analyse complète de convergence : nous démontrons que la population d'agents simples qui compose l'essaim calcule la solution d'un problème de contrôle optimal et que sa dynamique converge.
Nous étudions le taux de convergence de l'algorithme en fonction de la taille de la population et donnons des arguments expérimentaux et théoriques qui suggèrent que ce taux de convergence est superlinéaire en fonction du nombre d'agents.
En outre, nous expliquons comment ce modèle peut être étendu au cas où l'espace est continu et pour résoudre des problèmes de contrôle optimal en général.
Nous argumentons qu 'une telle approche peut être appliquée à tout problème qui implique le calcul du point fixe d'une contraction.
Ceci permet de concevoir une grande classe d'algorithmes d'intelligence en essaim bien compris formellement.
L'algorithme de transformation d'échelle de fréquence dans le domaine temporel (TDHS) fournit une méthode numériquement efficace (appropriée à une implantation en temps réel) pour la compression et l'expansion de la largeur de la bande fréuentielle de la parole.
Dans cet article, nous étudions un système de codage.
TDHS/bandes-partielles de la parole opérant à 16 k bits/s, et nous investigons l'efficacité relative de cinq méthodes différentes d'estimation du pitch (la méthode d'autocorrélation, la méthode du cepstre, la technique du filtrage inverse simplifié, la méthode AMDF et la méthode du maximum de vraisemblance.
Un test d'audition formel avec 17 auditeurs a été entrepis pour évaluer les performances comparatives des cinq méthodes considérées.
La méthode AMDF se dégage comme étant la meilleure procédure d'estimation du pitch pour le codage par TDHS/bandes-partielles.
Une tâche de répétition de mots (“shadowing”) a été utilisée pour étudier la contribution de la position du point d'unicité (PU, c'est-á-dire le moment auquel l'information acoustique déjà présentée n'est plus compatible qu'avec une seule représentation lexicale) au point de reconnaissance des mots parlés.
La comparaison destemps de réponse à des mots à PU précoce et tardif a mis en évidence un effet significantif mais relativement faible du PU.
Cependant, l'examen des résultats de chacune des deux séries de mots a fait appraître des différences importantes entre elles.
Une analyse de régression multiple prenant en considération la position du PU, ainsi que la durée et la fréquence des mots, a montré que la position du PU était le meilleur prédicteur du temps de réponse pour les mots de la série précoce mais qu'elle ne contribuait pas du tout au temps mis pourrépéter ceux de la série tardive.
Il semble donc que, pour une gamme relativement précoce de positions dans les mots, le PU soit un déterminant important du point de rreconnaissance.
Pour les mots de la série tardive, le meilleur prédicteur du temps de réponse était la durée des mots chez les sujets lents et leur fréquence chez les plus rapides, ce qui suggère l'intervention de mécanismes différents.
L'adaptation dans les jeux qu 'ils soient ludiques ou sérieux est une fonctionnalité importante qui permet d'individualiser et de contextualiser l'expérience de jeu.
Elle permet également de gérer la frustration des joueurs-apprenants tout en augmentant leurs motivations.
Cet article présente l'état de l'art des travaux traitant de l'adaptation dans les jeux ludiques et sérieux.
Ces travaux sont ensuite analysés suivant un cadre d'évaluation qui détermine le périmètre, les paramètres, le modèle de l'adaptation ainsi que le type de jeu : mono ou multijoueur.
L'analyse de l'état de l'art montre que la réalisation de jeux sérieux multijoueurs adaptables soulève des problèmes importants à tous les niveaux allant de la conception jusqu 'à la gestion des vues et interactions avec les joueurs-apprenants.
Ceci nous conduit à identifier le développement de jeux sérieux multijoueurs adaptables comme un défi majeur à aborder par la communauté à moyen terme.
Cet article s'interroge sur les bases phonétiques des théories traitant de la structure interne des segments.
Il porte un regard sur la naissance et le développement de l'idée que le phonème n'est pas le constituant ultime de la chaîne phonique indissociable en unités plus petites, mais le produit de l'association d'une liste finie de paramètres universels appelés différemment par les divers modèles : traits, éléments, gestes.
La rupture avec la conception « atomique » du phonème et l'élaboration des théories des primitives phonologiques vont cependant de pair avec une complication du formalisme et une hermétisation de la phonologie par son isolement de la phonétique.
Normalement, le lexique d'un système de reconnaisance de la parole contient des modèles de pronunciation qui décrivent comment les differents mots peuvent être réalisés en fonction des unités phonétiques (p.e. phonèmes).
Dans cette contribution on présente une méthode pour améliorer ces modèles simples et peu réalistes grâce à l'addition de plusieurs variantes de pronunciation pour chaque mot.
Puisque la stratégie présentée peut produire des variantes de prononciation et dériver des dependances à travers des mots, elle est également une alternative attirante pour l'encodage manuel de plusieurs pronunciations dans le lexique.
En apprenant des règles plutôt que des variantes, on peut créer des variantes de mots pas observés dans le corpus, tandis que l'on garde tout de même les avantages d'une procedure dirigée par les données.
Les caractéristiques importantes de la méthode sont qu'elle tient compte des dépendances entre les règles dès le début, qu'elle supporte des règles d'exception affectant la production de variantes par d'autres règles, et finalement qu'elle sait estimer d'une façon bien fondée les probabilités des variantes de pronunciation.
Les résultats expérimentaux indiquent que l'intégration des variantes obtenues dans les modéles de pronunciation diminue le taux d'erreurs de manière significative : les erreurs résultant d'une reconnaissance de timit au niveau des mots sont reduites relativement avec 17%.
Cet article présente les résultats d'une analyse statistique et déterministe de deux lexiques phonémiques qui tient compte du stockage et de la génération de règles orthographiques utilisant des graphèmes.
Le but de l'article est de montrer la faisabilité de la génération de mots anglais orthographiés correctement sur base de règles phonèmes/graphèmes.
Un algorithme pour générer les règles est présenté.
Un ensemble de règles orthographiques a été identifié en analysant deux lexiques de 93.939 et 11.638 mots, le dernier étant un sous-ensemble du premier.
Ces règles ont été ensuite testées quant à leur utilité en général.
62.3% des 96.939 mots ont été orthographiés correctement avec la seule aide des règles.
Un petit lexique contenant des mots communs et une sélection de mots moins fréquents a montré que 84.5 de ce lexique pouvait être orthographié correctement en utilisant des règles générées par l'analyse du même lexique.
Par contre, seulement 62.3% de ce lexique ont pu être orthographiés correctement sur base des règles découlant du lexique de 96.939 mots.
On a aussi montré que la correspondance entre phonèmes et graphèmes était alphabétique de 63% à 69% en fonction de la taille du lexique.
59 règles par défaut ont été identifiées ; mais seulement 22.6% du petit dictionnaire ont été orthographiés correctement à l'aide de ces règles.
L'introduction de connaissances dans les systèmes de reconnaissance de parole (ASR) est un bon moyen d'améliorer leurs performances.
Dans cet article, nous proposons le système orion dans le cadre d'une application de reconnaissance multicuteur de mots isolés.
orion est un système hybride à deux phases intégrant plusiers sources de connaissances : psychoacoustiques, physiologiques et phonétiques.
Pendant la première phase un modèle d'analyse acoustique perceptivement fondé (PLP), combinant des caractéristiques spectrales statiques et dynamiques, est utilisé pour fournir des vecteurs de paramètres à un algorithme de programmation dynamique.
A l'issue de cette première phase, plus de 98% de mots ont été correctement reconnus pour un vocabulaire de chiffres avec 12 références par mot.
Pour un vocabulaire de mots acoustiquement similaires (E-SET), l'introduction de connaissances phonétiques durant la deuxième phase diminue l'erreur de reconnaissance de plus de 60% (par rapport aux résultats obtenus lors de la première phase).
Cet article présente les travaux effectués au LIMSI pour le développement d'un système de traitement automatique d'informations radio et télédiffusées.
Partant d'un système de transcription de textes lus, nous décrivons les adaptations qui ont été nécessaires pour le traitement d'un flux audio continu et de données dites “trouvées”.
Ces développements ont été validés dans le cadre des évaluations ARPA BN (Nov96, Nov97, Nov98 et Dec99).
Les principales difficultés posées par ce type de données sont liées à leur nature hétérogène, qu'il s'agisse de changements de nature acoustique (environnement, communication, musique) ou de nature linguistique (styles d'élocution, diversités des sujets et des locuteurs).
La partition du flux continu est effectuée de manière itérative, par un algorithme de segmentation–agglomération reposant sur des mélanges de Gaussiennes.
Le système de reconnaissance utilise des modèles de Markov cachés à densités continues pour la modélisation acoustique, et des statistiques 4-grammes de mots estimées sur un grand corpus de textes et de parole transcrite pour modèle de langage.
La transcription en mots est obtenue en plusieurs passes de décodage, où les hypothèses intermédiaires sont utilisées pour adapter les modèles acoustiques.
Les taux d'erreur obtenues avec différentes versions de ce système lors des évaluations ARPA sont 27,1% (Nov96 avec partition manuelle), 18,3% (Nov97), 13,6% (Nov98) et 17,1% (Dec99, moins de 10 fois le temps réel).
La mise au point de mécanismes de coordination spatiale pour des agents évoluant dans des univers continus et dynamiques est un problème difficile.
Alors que la démarche descendante ne parvient pas à appliquer sa méthode de décomposition de façon satisfaisante sur cette classe de problèmes, l'approche ascendante obtient des résultats plus convaincants, mais elle implique souvent de fastidieux réglages manuels qui posent des problèmes de passage à l'échelle.
Notre démarche pour traiter cette difficulté consiste à adjoindre à un formalisme de coordination spatiale ascendante un algorithme évolutionniste multicritère dédié à ce type de problèmes.
Nous montrons sur un problème de coordination spatiale traité précédemment par Balch et Hybinette que les solutions obtenues avec notre plate-forme, GACS, sont comparables à celles obtenues par ces auteurs, malgré un investissement moindre de la part du concepteur.
On décrit ici une procédure permettant d'acquérir automatiquement les règles intonatives à appliquer aux syntagmes, à partir d'un texte annoté, pour la synthèse à partir du texte.
Les règles générées par cette méthode ont été implémentées dans la version Anglaise du système de synthèse à partir du texte des laboratoires Bell et ont été développées pour la version de ce système en Espagnol Mexicain.
Ces règles fournissent, à ce jour, des prédictions adéquates dans plus de 95% des cas pour l'Anglais et dans plus de 94% pour l'Espagnol.
Cet article décrit les prinicipes de réalisation d'une système de dictée vocale à large vocabulaire, employant une technologie avancéee de reconnaissance de parole.
Il peut être utilisé pour une grande variété de tâches, et est rapidement adaptable à de nouveaux domaines.
On décrit ici une étude de cas d'une application de cette technologie dans le contexte de la création de rapports médicaux.
Les objectifs de l'utilisation d'une technologie de reconnaissance de grands vocabulaires pour réaliser un système de dictée de texte libre sont décrits.
On discute également du processus de création des textes, de la première à la dernière version, en termes de prérequis tant vis à vis de la technologie que de son implémentation.
On évoque également les contraintes sur les performances et l'acceptabilité du système.
On décrit également l'implémentation d'une composante d'un système flexible permettant l'adaptation à l'utilisateur.
On évoque également l'utilité, pour une grand nombre d'applications potentielles, d'une système de dictée autonome, non-spécialisé.
On décrit également les possibilités d'adaptation du système à une nouvelle tâche par reconstruction rapide de ses tables de paramêtres.
L'étude de cas concernant la station de travail pour rapports médicaux sert d'exemple pour montrer comment une version adaptée du système peut être exploitée dans l'environnement réel d'un utilisateur.
Dans cet article, nous décrivons sphinx, le premier systéme de reconnaissance automatique de la parole continue, indépendant du locuteur et à grand vocabulaire.
Nous présentons ses premiers résultats, comparons ses performances à celles d'autres systèmes semblables et expliquons sa grande précision.
On a construit un système de microprocesseur qui convertit en temps réel n'importe quel texte français en parole.
Le système, dont le logiciel est écrit en langage Pascal modulé, traduit le texte en une série de phonèmes, et désigne une durée et une hauteur pour chaque phonème en utilisant une analyse syntaxique simple.
Un circuit intégré spécialisé pour programmer des traitements de signaux numériques génère dix mille échantillons de la parole par seconde.
Le système consiste en deux cartes de circuits imprimés : l'une avec un microprocesseur Intel 8086 et la mémoire associée, et l'autre avec la puce spéciale avec un interface, un convertisseur numérique-analogique et un amplificateur.
Une version antérieure du système, qui fonctionnait sur un ordinateur VAX-11/780, exigeait huit secondes de calcul pour chaque seconde de parole.
Le système actuel qui fonctionne en temps reel démontre la faisabilité d'une synthèse du français de haute qualité.
La variabilité interlocuteur est une source majeure d'erreurs en reconnaissance automatique de la parole (RAP).
Cet article décrit une série d'expériences, menées par l'Équipe « Reconnaissance des Formes et Traitement de la Parole » de TÉLÉCOM Paris, dans le but de contrôler certains aspects de cette variabilité, et permettre ainsi une adaptation au locuteur des systèmes actuels de reconnaissance de parole.
Les premières expériences utilisent une technique linéaire empruntée à l'analyse des données, la régression linéaire multiple.
L'amélioration des taux de reconnaissance obtenue est, en moyenne, de 16% pour les secondes, contre 15 % pour les premières.
Ces techniques peuvent également être utilisées pour l'adaptation des reconnaisseurs à de nouveaux environnements acoustiques ou conditions de prise de son.
Dans cet article nous présentons les résultats de tests auprès d'utilisateurs du kiosque Mask (Multimodal Multimedia Service Kiosk).
Le but du projet Esprit Mask était de développer un kiosque d'information et de distribution avec une interface innovante et conviviale combinant les modalités tactiles et vocales.
Le prototype a été développé après une analyse des besoins dans le cadre du transport ferroviaire en collaboration avec la SNCF et le groupe d'ergonomie de l'université College of London (UCL).
Tous les objectifs fixés par la SNCF au début du projet ont été atteints par le prototype, qu'ils concernent le taux de succès, le temps de transaction, ou la satisfaction des utilisateurs.
Nous présentons dans cet article deux nouvelles méthodes de détection de l'instant de fermeture de glotte à partir de la forme d'onde acoustique.
Ces deux méthodes détectent les discontinuités dans les caractéristiques spectrales à courtterme du signal de parole apparaissant à l'intérieur de chaque cycle d'excitation glottique.
Ces deux méthodes exploitent la même approche de détection séquentielle d'événements à l'aide de tests statistiques d'hypothèses.
La première méthode est fondée sur la maximisation séquentielle d'un rapport de vraisemblance.
Un certain nombre de résultats expérimentaux sur des données de parole démontrent la robustesse des méthodes proposées.
Dans cet article, nous comparons deux approches différentes de vérification du locuteur basées sur la technologie des modèles de Markov cachés (HMM) : HMMs à gaussiennes simples et différents types de HMMs multi-gaussiennes liées.
Pour évaluer la performance en situation réelle, nous avons testé chaque système avec une base de données de chiffres connectés, enregistrés via lignes téléphoniques locales et longue-distance.
Selon nos expériences, les modèles multi-gaussiennes liées sont plus performants à condition d'utiliser suffisamment de données d'entraînement.
Cependant, nos expériences indiquent que les HMMs à gaussiennes simples doivent être préférés pour la vérification du locuteur en situation réelle, lorsque on dispose seulement de quantitées limitées de données d'entraînement.
Les résultats sont discutés pour la vérification dépendante et indépendante du texte.
A la recherche d'une meilleure performance, les systèmes de reconnaisance de la parole actuelles inclinent à des modèles acoustiques et des modèles de langage de plus en plus compliqués.
Des modèles de phones en contexte intramot et des modèles de langage de longue envergure sont maintenant largement répandus.
Dans cet article, nous présentons une topologie de recherche qui permet l'utilisation de tels modèles détaillés, dans un système de reconnaisance temps-synchrone à une passe.
Caractéristique à notre approche est (1) le découplage des deux sources de connaissance de base, à savoir l'information de prononciation et l'information de modèle de langage, et (2) la représentation compacte d'information de prononciation – le lexique en termes de phones en contexte – au moyen d'un réseau statique.
L'information de modèle de langage est incorporée à la recherche au délai d'exécution au moyen d'un algorithme de passage de jeton (token passing) légèrement modifié.
Le découplage du modèle de langage et du lexique permet une grande flexibilité dans le choix des modèles de langage, alors que la représentation statique du lexique évite le coût d'expansion dynamique du réseau et facilite l'intégration d'information de prononciation supplémentaire telle que des règles d'assimilation.
En plus, la représentation par un réseau est très efficace quand les mots ont des prononciations multiples, et à cause de sa construction, la structure proposée offre l'anticipation partielle de modèle de langage sans coûts supplémentaires.
La source de voix est un facteur important dans la production de différentes qualités de voix.
Celles-ci sont utilisées dans la parole pour tarnsmettre, entre autres choses, différents aspects suprasegmentaux, par exemple l'emphase, les frontières de phrase ainsi que différents styles de parole comme le style autoritaire ou soumis.
Les variations de la source vocale sont également un moyen important de transmettre des informations extralinguistiques de différentes sortes dans la parole ordinaire.
Dans la présente étude, les variations de la source dans la parole normale de locuteurs féminins sont investiguées en utilisant un filtrage inverse.
Les résultats du filtrage inverse sont donnés en paramètres de source de voix appropriés pour contrôler la synthèse de parole.
De cette manière, les descriptions résultantes ont été utilisées pour produire des variations de la voix dans notre nouveau système de synthèse.
Ce papier présente quatre nouvelles méthodes de vérification du locuteur.
La première, un modèle hybride de Perceptron multicouche (MLP) et de Fonctions Radiales de Base (RBF), est un prédicteur MLP dont les poids sont ensuite utilisés comme entrées pour un classificateur RBF - lequel effectue la vérification du locuteur.
La seconde utilise un tableau de prédicteurs linéaires pour modéliser le locuteur de référence ; chaque élément est associé avec une sous-unité particulière de la phrase de test.
La troisième, un Modèle Prédictif Neuronique, est composé d'un tableau de prédicteurs MLP ; la quatrième, un Réseau Neuronique à contrôle caché, est un prédicteur MLP unique auquel sont ajoutées des entrées de commande;
celles-ci modulent la fonction réalisée par le MLP et permet à ce seul MLP de modéliser une phrase complète.
Chaque méthode a été apprise et testée sur une petite base de données. Les performances sont 100% de taux de vérification poor les 3 premiers modèles, 90% pour le Réseau Neuronique à contrôle caché.
Lors d'une étude trans-culturelle sur des enfants japonais et américains nous avons examiné le développement de la conscience des syllabes et des phonèmes.
Les expériences I et III, qui utilisent des tests de comptage et d'effacement, montrent que, à la différence des élèves américains de première année d'école primaire, qui ont en général conscience à la fois des phonèmes et des syllabes, presque tous les élèves en première année d'école primaire au Japon ont conscience d'unités phonologiques de l'ordre de la syllabe alors qu'assez peu ont conscience des phonèmes.
Cette différence est attribuable au fait que les élèves japonais apprennent à lire un syllabaire alors que les élèves américains apprennent à lire un alphabet.
Pour la plupart des enfants de cet âge, la conscience des phonèmes nécessite l'expérience d'une transcription alphabétique, alors que la conscience des syllabes peut être facilitée par l'expérience d'un syllabaire, sans en dépendre aussi fortement.
Pour éclaircir davantage le rôle de la connaissance d'un alphabet sur la conscience des phonèmes chez les enfants, nous avons fait effectuer des tâches de comptage et d'effacement (expériences II et IV) à des enfants japonais en fin d'école primaire.
Les résultats montrent que beaucoup d'enfants japonais prennent conscience des phonèmes vers dix ans d'âge, qu'ils aient ou non appris une transcription alphabétique.
La discussion de ces résultats porte sur certains autres facteurs qui peuvent produire la conscience phonologique.
Cet article présente une étude systématique d'un modèle de reconnaissance des voyelles, indépendant du locuteur.
La technique de transformation de Karhunen-Loève (KLT), ou analyse en composantes principales, a été appliquee apres une analyse spectrale du signal parlé à l'aide de 18 filtres dont les bandes critiques ne se recouvrent pas.
4 expériences ont été conduites avec segments de 8 voyelles isolées de Putonghua (Mandarin), prononcées 2 fois, avec 5 tons différents par 38 femmes et 13 hommes.
La première expérience utilise le même énoncé pour l'entraînement et le contrôle pour évaluer les effets du KLT, la normalisation du locuteur, la distance métrique et le nombre de classes vocaliques.
Une distance de Mahalanobis modifiée couplée à une condition de 7 classes a donné les meilleures performances.
Dans l'expérience suivante, les deux échantillons du même énoncé sont produits par un groupe de locuteurs. Le premier échantillon est utilisé pour l'apprentissage et le deuxième pour le test.
On a trouvé, qu'en général, on peut renoncer à une normalisation spécifique au sexe et au ton sans affectation notable de la performance.
La troisième expérience entraîne, de façon répétée, le modèle avec 50 sujets et le teste avec un sujet, ceci pour les 51 locuteurs.
Dans ces conditions sévères, on atteint un taux de reconnaissance moyen de 88,2% en utilisant seulement une classification en 4 dimensions.
Dans la dernière expérience, tous les segments d'une voyelle sont étiquetés sous les conditions les plus strictes.
Il est confirmé que le modèle fonctionne bien pour un homme et une femme, sélectionnnés au hasard.
La voyelle qui a causé le plus de confusions est bien reconnue quand elle est traitée comme un allophone d'une autre voyelle.
Enfin, quelques résultats préliminaires discutent la possibilité d'étendre cette technique à la reconnaissance des diphtongues.
L'anglais et l'italien présentent des contrastes intéressants et pertinents pour un problème crucial en psycholinguistique, celui de la frontière entre connaissance grammaticale et extragrammaticale dans le traitement des phrases.
Bien que tous deux soient des langues avec un ordre SVO sans inflections de cas pour indiquer les relations grammaticales de base, l'italien autorise beaucoup plus de variations dans l'ordre des mots pour des buts pragmatiques.
Les italiens doivent, donc, s'appuyer plus que les anglais sur des facteurs autres que l'ordre des mots.
Dans l'expérience présentée, on a demandè à des adultes anglais et italiens d'interpréter 81 phrases simples où variaient l'ordre des mots, les contrastes animés/non animés entre deux noms, le stress contrastif et la topicalisation.
Les italiens s'appuient principalement sur des stratégies sémantiques alors que les auditeurs anglais s'appuient sur l'ordre des mots et cela inclue une tendance à interpréter le second nom comme sujet dans les ordres de mots non-canoniques (correspondant aux variations d'ordre de la production de l'anglais informei).
Les italiens font un plus grand usage du thème et de l'information donnée par l'accent.
Enfin, les italiens sont beaucoup plus lents et moins consistants dans l'application de stratégies d'ordre de mot même pour des phrases reversibles NVN où il n'existe pas de conflit entre l'ordre et la sémantique.
Cela suggère que l'italien est 'moins' une langue SVO que l'anglais.
Les stratégies sémantiques tiennent apparemment au 'coeur' de l'italien les mêmes rôles que les stratégies d'ordre des mots au 'coeur' de l'anglais.
Ces résultats font problème pour parler d'une séparation 'universelle' entre sémantique et syntaxe et pour les théories qui postulent une priorité 'universelle' d'un type d'information sur l'autre.
Les résultats sont examinés dans le cadre d'un modèle de compétition, approche fonctionnaliste de la grammaire qui rend compte de façon rigoureuse des données probabilistiques et des poids différentiels des différentes sources (converges et rivales) d'information dans le traitement des phrases.
On décrit un système de synthèse de la parole en temps réel.
Ce système s'applique sans restriction à tout texte en langue allemande et se base sur l'enchaînement d'éléments préenregistrés : les sons simples et les transitions diphoniques.
L'entrée se présente (pour l'instant) sous forme d'un texte phonétique accompagné de l'indication de la fréquence fondamentale.
Le signal de sortie est généré par un vocodeur à 'log-area-ratio' (LAR), contrôlé par un ordinateur.
Les sons stationnaires sont codés par des cadres simples de paramètres du vocodeur et les transitions par des paires de cadres, extraites, moyennant quelques légères modifications, de paroles réelles.
Les cadres intermédiaires sont interpolés en cours de synthèse.
On décrit les expériences préliminaires sur l'interpolation et le choix des cadres à préenregistrer ainsi que le procédé de synthèse (2 variantes) qui comprend la structure des tables, le traitement de la fréquence fondamentale et de la durée des sons.
Des mesures d'intelligibilité et des comparisons sur la qualité des deux variantes ont étè effectuées.
Des recherches ont été menées pour déterminer si les caractéristiques acoustiques de la voix étaient altérées pendant les périodes de travail soutenu.
Douze hommes de l'équipage d'un bombardier B-1B de la force aérienne des Etats-Unis ont participé à cette étude.
Les participants ont servi dans des équipes de 4 personnes et ont effectué trois périodes (missions) expérimentales de 36 heures chacune dans un simulateur de haute fidélité.
Les missions étaient séparées de 36 heures de repos.
Les données de deux membres de la troisième équipe ont été perdues à cause d'un défaut de communication.
Des données de parole, des données cognitives et les impressions subjectives de fatigue ont été collectées toutes les trois heures environ et pour onze essais par mission.
On a trouvé que la fréquence fondamentale et la durée des mots variaient signifïcativement d'un essai à l'autre (fréquence fondamentale F(10,90) = 2.63, p = 0.0076, durée des mots F(10,90) = 2.5, p = 0.0106).
Les résultats sur la durée ont aussi montré, et d'une manière significative, un effet important de la mission (F(2,18) = 6.91, p = 0.0082).
Les données de parole suivent les mêmes tendances que les tests cognitifs et les mesures subjectives de la fatigue.
Un fort effet diurne apparait dans presque toutes les mesures de dépendance.
Globalement, les résultats montrent que la parole peut être un indicateur valide de l'état de fatigue du locuteur.
Les structures syntaxique et phonotactique des phrases sont variées systématiquement pour comprendre comment deux fonctions peuvent être véhiculées en paralléle dans le continuum prosodique : (1) énonciative : démarcation des constituants ; (2) illocutoire : attitude du locuteur.
L'analyse statistique du corpus démontre que des contours prosodiques globaux caractérisent chaque attitude.
Cet encodage global s'accorde avec des expériences de dévoilement progressif montrant que les attitudes peuvent être identifiées très tôt dans l'énoncé.
Ces résultats sont commentés dans la perspective d'un modèle morphologique et superpositional de l'intonation.
Ce modèle, propose que l'information spécifique à chaque niveau linguistique (structure, hiérarchie des constituants, attributs sémantiques et pragmatiques) est encodée sous forme de contours multiparamétriques.
Nous décrivons une implémentation de ce modèle qui capture puis génère automatiquement ces contours prosodiques prototypiques.
Il s'agit d'un ensemble de réseaux de neurones récurrents, chacun d'eux encodant un niveau linguistique particulier.
Les scores d'identification des attitudes pour des phrases synthétiques d'apprentissage ou de test sont similaires aux scores obtenus pour des stimuli naturels.
Nous concluons que l'étude d'attributs linguistiques au niveau discursif, comme ici les attitudes prosodiques, est un paradigme intéressant pour comparer les modèles de l'intonation.
Nous avons utilisé un réseau de neurones récurrent pour reconnaître des successions de noms de letters b, d, e et v de l'anglais américain prononcées par plusieurs locuteurs.
L'appretissage est basé sur une propagation du potentiel des unités au lieu de la rétropropagation de l'erreur des unités dans le temps.
La fonction cible se base sur un paramètre caractérisant le signal de parole qui est activé puis désactivé au début de la prononciation de chaque lettre.
Le réseau apprend à reproduire ce même paramètre à l'unité de sortie correspondent au nom de lettre correcte.
Les résultats sur les phrases de test atteignent un taux de discrimination de 85%.
Cet article porte sur les analyses définitionnelles de la structure du langage.
Plusieurs classes d'arguments ayant trait aux définitions sont passées en revue, entre autres, celles liées aux théories classiques de la référence, aux théories de lalidation informelles, aux théories de la compréhension de phrases et aux théories de l'apprentissage de concept.
On suggére que, dans chacun de ces domaines, les travaux qui s'appuient sur une définition ne sont pas plus justifiés par les preuves qu'une alternative plausible non-définitionnele.
On présente, en outre, une série d'observations expérimentales portant sur un de ces domaines : celui de la compréhension de phrase.
De façon indépendante, on montre que les jugements du sujet sont sensibles aux relations structurales de type comparable dans des formes linguistiques.
La présence d'une distance de visibilité réduite sur un réseau routier (épais brouillard, pluie forte, etc.) affecte la sécurité de celui-ci.
Nous avons conçu un système de bord de voies qui vise à détecter des situations critiques telles que le brouillard dense ou les fortes chutes de pluie à l'aide d'une caméra vidéo.
Les différents traitements d'image sont présentés, en particulier l'estimation de la distance de visibilité, la détection de brouillard, ainsi que la détection de pluie.
En se fondant sur les principes sous-jacents de ces algorithmes, une caméra est ensuite spécifiée pour répondre aux besoins exprimés par la norme NF P 99-320 sur la météorologie routière.
Des résultats expérimentaux sont présentés ainsi que des perspectives de validation à plus grande échelle.
On présente une nouvelle méthode d'analyse du flux glottique : le PSIAIF (Pitch Synchronous Iterative Adaptive Inverse Filtering).
Cet algorithme se base sur une méthode (IAIF) développée précédemment.
La contribution glottique totale au spectre de la parole y était tout d'abord évaluée itérativement.
Dans la nouvelle méthode, l'onde glottique est calculée en appliquant deux fois l'algorithme IAIF au même signal.
La première analyse donne une estimation de l'excitation glottique qui s'étend sur plusieurs périodes.
L'onde ainsi obtenue est utilisée ensuite pour déterminer les positions et les longueurs des fenêtres d'analyse synchronisées.
Pour obtenir le résultat final, il ne rest plus qu'à analyser le signal original de la parole, période fondamentale par période fondamentale, avec l'algorithme IAIF.
L'algorithme PSIAIF a été appliqué à l'analyse du signal glottique, dans le cas de voyelles naturelles et synthétiques.
Les résultats montrent que la méthode est capable de fournir une estimation relativement précise de flux glottique, si l'on exclut l'analyse des voyelles à premier formant bas produites par un type de phonation pressée.
Cet article présente une analyse d'un corpus de grammaires écrites pour l'apprentissage du français en Angleterre de 1660 à 1820, une période parfois qualifiée par euphémisme de « long siècle » où l'enseignement des langues évolua en fonction de mutations plus larges, y compris la codification de la grammaire vernaculaire contre un fond de rationalisme scientifique et l'instauration des pédagogies scolaires.
Mon analyse comporte deux axes complémentaires : il s'agit, premièrement, d'identifier quelques changements-clés dans la formulation du contenu, en particulier des changements dans la structure générale et la répartition des sections, y compris des différences dans la nomenclature grammaticale, et deuxièmement, de contextualiser ces évolutions en considérant la mutation du rôle des enseignants grammairiens et la manière dont ils se positionnent en tant qu'auteurs auprès de publics différents.
Cet article présente une application de l'algorithme de programmation dynamique utilisé dans l'étape de décodage acoustico-phonétique d'un systéme de reconnaissance de parole.
Deux méthodes sont comparées utilisant comme unités de reconnaissance : (1) les demysllabes, et (2) les groupes consonantiques et les voyelles (ou les diphtongues).
L'utilisation des demisyllables ainsi que das groupes consonantiques et les voyelles a permis d'obtenir de bons résultats en ce qui concerne la reconnaissance de la parole continue.
De plus, la programmation dynamique est une méthode qui est largement utilisée en reconnaisance de mots connectés.
Dans cet article, nous présentons un processus de décodage acoustico-phonétique utilisant le principe de programmation dynamique appliqué précisement à ces unités de reconnaissance.
Dans son principe, l'algorithme est le même que celui que l'on utilise pour reconnaissance de mots connectés;
quelques modifications ont été introduites du fail de la nature des unités, en particulier par l'apport d'une syntaxe interne.
L'algorithme ainsi modifié a été testé sur un corpus de phrases à partir d'un lexique de 75 mots contenant les groupes consonantiques et les voyelles les plus fréquents de la langue allemande.
Différents tests ont été effectués pour mettre en évidence l'influence des contextes dans la reconnaissance d'une unité, d'abord à partir des phrases du corpus d'apprentissage, ensuite sur de nouvelles phrases.
Enfin une comparaison avec une méthode similaire utilisant une segmentation “explicite” est présentée.
Dans une conversation, les participants ont parfois des difficultés à se comprendre, mais, quand ils sont conscients du problème, ils collaborent ou négocient afin de déterminer le sens de l'énoncé qui pose le problème.
Pour traiter l'incompréhension, nous avons développé des modèles de collaboration à deux plans en identifiant le référent correct d'une description : l'un des plans couvre les situations où les deux interlocuteurs connaissent le référent, l'autre plan couvre les situations, comme celle d'indication de direction, dans lesquelles le récipiendaire ne le connait pas.
Dans les modèles, les interlocuteurs utilisent les mécanismes de reformulation, de suggestion et d'élaboration pour raffiner ensemble une expression du référent jusqu'à ce qu'elle soit satisfaisante.
Pour traiter la mècompréhension, nous avons développé un modèle qui combine les aspects intentionnels et sociaux du discours pour aider à la négociation de la signification.
L'approche étend les aspects intentionnels en utilisant des prédictions dérivées des conventions sociales pour guider l'interprétation.
Reflétant la symétrie inhérente à la négociation du sens, tous nos modèles peuvent être utilisés comme locuteur ou comme auditeur.
L'introduction de connaissances dans les systèmes de reconnaissance de parole (RAP) est un bon moyen d'améliorer les performances des systèmes actuels.
Dans cet article nous proposons le système ORION dans le cadre d'une application de reconnaissance multilocuteur de mots isolés.
Pendant la première passe un modèle d'analyse acoustique perceptivement fondé (PLP), combinant des caractéristiques instantanées et des caractéristiques spectrales dynamiques, est utilisé pour fournir des vecteurs de paramètres à un algorithme de programmation dynamique.
A l'issue de cette première passe plus de 98 % de mots ont été correctement reconnus pour un vocabulaire de chiffres et 12 références par mot.
L'introduction de connaissances phonétiques durant la deuxième passe diminue l'erreur de reconnaissance de plus de 60 % (par rapport aux résultats obtenus lors de la première passe) pour un vocabulaire de mots acoustiquement similaires (E-SET).
L'expérience gagnée avec le système à micro-faisceau a confirmé que l'on peut considérablement le dosage des Rx.
Les mouvements de 6 pastilles sur la langue et les dents sont suivis à plus de 100 images par seconde, avec une surface effective d'exposition d'environ 1 cm2 par image et un taux d'exposition de 120 mR par minute sur cette surface.
L'approximation des mouvements de ces pastilles sur la langue, la mâchoire et le voile au moyen d'une réponse en échelon d'un système linéaire du second ordre a révélé qu'il y avait de grandes différences dans les valeurs des constantes de temps entre ces différents organes.
La durée de la commande-échelon varie également selon le type de la voyelle.
Ces différences sont réfléchies par l'allure du sous-dépassement et par le schème coarticulatoire entre consonne et voyelle.
L'investigation des mouvements du voile a montré qu'une observation simultanée par EMG est importante pour interpréter la forme du contrôle moteur sous-jacent.
Le présent article traite du choix d'intonations appropriées pour les systèmes intelligents de dialogue homme-machine dans le contexte, par exemple, des bases de données.
On propose une approche qui réunit dans le cadre du dialogue homme-machine deux paradigmes jusqu'ici divergents, celui de la génération automatique de textes et celui de la synthèse de la parole.
Une telle approche permettrait de combler les lacunes bien connues des systèmes de génération de parole à partir de textes écrits ou de concepts.
À l'heure d'une internationalisation accrue des échanges scientifiques, la question de la langue des publications scientifiques – réglée en sciences de la nature depuis les années 1980 – est devenue un enjeu pour les sciences sociales.
Cet article propose une analyse détaillée des stratégies linguistiques adoptées par deux revues françaises majeures de sciences sociales, Population et Revue française de sociologie (RFS), qui ont choisi de traduire en anglais une sélection (RFS) ou la totalité (Population) d'articles.
Au vu des résultats en termes de visibilité dans le champ scientifique international – accroissement de la visibilité de la revue Population au détriment de la version française et effets marginaux pour la RFS –, on s'interroge sur le rôle joué par les revues nationales de sciences sociales.
Dans cet article, nous proposons une synthèse des stratégies mises en œuvre pour la conception de discriminateurs avec options de rejet opérant en deux étapes séquentielles.
Outre l'approche classique dite « accepte d'abord » , nous avons récemment défini des classes générales qui suivent deux approches différentes dites « rejette d'abord » [Fré98a, MF01b] et « mélange d'abord » [SFM02].
Ces trois approches diffèrent par la nature, et l'ordre, des tests effectués pour produire la sortie du discriminateur.
La première consiste à tester en premier lieu le rejet de distance, puis seulement si nécessaire à tester l'affectation exclusive contre le rejet d'ambiguïté, la deuxième et la troisième, quant à elles, débutent, respectivement, par un test pour le classement exclusif et un test pour le rejet d'ambiguïté à opposer aux alternatives correspondantes.
Nous unifions ici ces trois familles de discriminateurs par l'utilisation d'opérateurs flous fondés sur des opérateurs de De Morgan (t-norme, t-conorme, complément).
Les comportements des différentes approches sont illustrées sur des exemples synthétiques.
Quand les locuteurs de référence sont représentés par un modèle de mélange de gaussiennes, l'approche conventionnelle est d'accumuler les probabilités de trame sur l'énoncé de test entier et de comparer les résultats pour l'identification du locuteur ou d'appliquer un seuil pour la vérification du locuteur.
Dans cet article, nous décrivons une méthode dans laquelle les probabilités de trame sont transformées, avant d'être sommées, en de nouveaux scores, suivant une certaine fonction non-linéaire.
Nous avons étudié deux familles de fonctions.
La première effectue de fait une normalisation des probabilités – une technique largement utilisée en vérification du locuteur –, mais qui est appliquée ici au niveau des états.
Le deuxième type de fonctions transforme les probabilités en poids, suivant un certain critère.
Nous appelons cette transformation “Weighting Models Rank” (WMR).
Les deux types de transformations requièrent de pouvoir disposer de tous (ou d'un sous-ensemble de tous) les modèles de référence.
Pour obtenir ceci, chaque trame de l'énoncé d'entrée est incorporée en parallèle dans les modèles de référence requis, puis la transformation des probabilités est appliquée.
Les nouveaux scores sont ensuite accumulés sur l'ensemble de l'énoncé pour obtenir un score de l'énoncé pour un modèle de locuteur donné.
Nous avons trouvé que la normalisation de ces scores d'énoncés est également efficace pour la vérification du locuteur.
L'amélioration de la qualité du son synthétique est le problème le plus urgent auquel l'étude de la synthèse du Chinois est confrontée.
Cet article présente la structure et les traits de notre système de synthèse pour le Chinois standard.
Le système a été construit sur base d'une analyse acoustico-phonétique des syllabes du Chinois.
Le système incorpore plusieurs règles et modèles originaux.
Toutes les 1268 syllabes du Chinois standard ont été synthétisées avec une qualité proche de l'original en ce qui concerne l'intelligibilite et le naturel.
Cet article décrit les résultats d'un projet lancé par deux laboratoires de recherche français (LIMSI-CNRS et INSERM-CREARE) et une organisation d'utilisateurs, l'Institut National des Jeunes Aveugles (INJA).
Ce projet vise à exploiter des interfaces multi-modales (incluant de la reconnaissance et de la synthèse de parole) pour faciliter l'accès des aveugles aux ordinateurs.
Un éditeur de texte multi-modal a été développé pour fournir des textes enrichis, une manipulation directe et un retour immédiat.
Toutefois, la combinaison de la parole avec d'autres modalités au sein d'une même interface fait également apparaı̂tre des problèmes techniques nouveaux qui ne sont pas visibles quand la parole est utilisée seule.
Ces problèmes sont discutés dans cet article qui présente également les besoins et attentes des utilisateurs.
Il présente l'implémentation de ce modèle, sa spécialisation pour certains systèmes de classeurs et son utilisation pour des applications diverses.
Nous exposons une méthode novatrice concernant le codage des paramètres “Line Spectrum Pair (LSP)”, ceci pour une transmission par canal bruité.
Typiquement, les codeurs de parole à faible vitesse de transmission utilisent ces paramètres pour transmettre des données spectrales perceptivement importantes.
Aussi, il est nécessaire que ces paramètres ne soient pas seulement quantifiés d'une manière efficace, mais également protégés tout au long de la transmission.
Cette méthode utilise une technique combinant le codage de la source et celui du canal, appliquée à une structure de treillis joints entre-eux.
Opérant en tant que codeur de source, le système encode avec une distorsion spectrale inférieure à 1 dB.
Il est démontré que modifier la fonction-coût de l'encodeur, afin d'inclure la distorsion de canal déterminé à l'avance, améliore la robustesse du codage aux erreurs de canal.
Ceci est réalisé avec un accroissement minimal de la complexité du système et sans augmenter la vitesse de transmission.
De plus, elle est favorablement comparable au standard du quantificateur scalaire LSP couplé à un codeur de canal, du point de vue de la vitesse de transmission ainsi que du point de vue de l'immunité au bruit de canal.
Dans les explications traditionnelles de la prosodie de la parole, la fréquence fondamantale, la durée et l'intensité ont été décrites comme les attributs les plus importants.
Parmi ceux-ci, l'intensité a le moins attiré l'attention.
Dans les études perceptives, à la fois la fréquence fondamentale et la durée ont eu un rôle indiscutable dans le signalement des catégories prosodiques mais le rôle de l'intensité a été moins clair.
Il en a résulté une accentuation de ces premiers attributs dans les schèmes actuels de synthèse de la parole.
Dans cette étude, nous explorons l'emploi de l'intensité ainsi que d'autres corrélats segmentaux de la prosodie.
L'intensité a un aspect dynamique, discriminant les portions de parole emphatiques et réduites.
Un aspect plus global de l'intensité doit être contrôlé lorsqu'on essaye de modéliser les styles langagiers.
Spécifiquement, nous avons essayé de modéliser le continuum de la parole faible à la parole forte.
Plusieurs groupes ont étudié la relation entre le taux d'erreur au niveau du mot et la perplexité du modèle de langage.
Cette question est d'un intérêt central dans la mesure où la perplexité peut être optimisée indépendamment du système de reconnaissance et que, dans la plupart des cas, il est possible d'aboutir à des procédures simples d'optimisation.
De plus, de nombreuses tâches intervenant lors de l'entraı̂nement d'un modèle de langage, par exemple, l'optimisation des classes de mots, sont suceptibles d'utiliser la mesure de perplexité comme objectif ce qui conduit à des formules explicites d'optimisation qui ne seraient pas accessibles si le taux d'erreur avait été choisi comme objectif.
Cet article présente d'abord des arguments théoriques en faveur d'une relation étroite entre perplexité et taux d'erreur.
Ensuite, la notion d'incertitude d'une mesure est introduite et appliquée aux fins de tester l'hypothèse que la corrélation entre perplexité et taux d'erreur est régie par une loi de puissance.
Il n'y a pas d'évidence pour rejeter une telle hypothèse.
Les dérivés en -(cu)-lus, -(cu)-la, -(cu)-lum posent des problèmes de traduction dans les textes techniques latins.
Dans certains cas le suffixe joue un rôle de diminutif qui lui est bien connu : l'objet désigné par le dérivé est plus petit que l'objet désigné par le simple.
Dans d'autres cas nous avons affaire à des « emplois décalés » dans le temps ou dans l'environnement thématique : le simple et le dérivé sont utilisés pour désigner le même objet, dans le même contexte, mais à deux époques différentes, ou bien ils désignent deux objets différents (ou similaires) dans des contextes différents, sans distinction de taille.
Il reste enfin de curieux cas de synonymie parfaite que nous examinons ici : comment et pourquoi, dans un même chapitre, un même paragraphe, voire une même phrase, un auteur technique désigne-t-il un même objet alternativement par son nom simple et par son nom dérivé ?
Ce système procure toute l'information utile aux connections entre 1200 villes allemandes.
L'utilisateur peut s'exprimer librement, de manière naturelle et continue, ainsi qu'il le ferait normalement vis à vis d'un opérateur humain et il ne reçoit aucune instruction au préalable.
Le système est constitué de quatre composantes majeures, à savoir la reconnaissance de la parole, la compréhension du message, la gestion du dialogue et la synthèse vocale, qui sont organisées en modules indépendants et exécutées séquentiellement.
Dans le cadre d'une procédure d'évaluation toujours en cours, le système a été mis à la disposition du public, d'une part pour collecter des données et d'autre part afin de mesurer ses performances.
Ces tests de validation ont été organisés par étapes successives : le système a d'abord été entrainé à l'aide des seules voix de ses concepteurs, pour être ensuite testé dans l'équipe.
Après quoi, le numéro de téléphone du système a été communiqué à l'ensemble de l'entreprise, et enfin au monde extérieur.
Dans le cadre de la reconnaissance et de l'anticipation de situations dynamiques, différentes méthodes calculatoires basées sur des outils mathématiques existent déjà, cependant, leur implémentation est souvent complexe et débouche sur des programmes dont les temps de calcul sont longs.
Nous proposons, dans cet article, une autre méthode d'apprentissage et d'anticipation, prévue pour assister un utilisateur dans le cadre de situations dynamiques.
Ses connaissances sont donc structurées de manière à limiter la complexité de la solution et à faciliter l'apprentissage et l'anticipation.
Un grand ensemble de facteurs ont été avancés comme pouvant expliquer des différences dans le traitement des phrases relatives.
Parmi ces facteurs, on trouve : le rôle grammatical de la téte de la phrase relative, l'ordre de surface des constituants, l'existence d'interruptions de la phrase principale, et l'existence ou non d'indications morphologiques.
Comme l'anglais posséde un ordre strictement SVO, les relatives qui modifient le sujet de la principale interrompent nécessairement celle-ci, et par conséquent il est impossible de séparer les effets dûs au rôle grammatical et aux interruptions.
Le hongrois, dont l'ordre des mots est variable, permet de mieux distinguer l'effet du rôle grammatical, des configurations, des interruptions et des indications morphologiques.
Une étude basée sur 144 types de relatives en hongrois suggére que trois facteurs jouent un rôle important dans le traitement des relatives.
Premiérement, l'importance de la conservation de la perspective est démontrée par le fait que les phrases SS sont les plus faciles á traiter, et les phrases SO les plus difficiles.
Deuxiémement, la grande difficulté de traitement des phrases NNV, où la relative modifie le second substantif, démontre les limitations importantes du processus de construction de fragments par une analyse syntaxique “bottom-up”.
L'existence d'un marquage de l'antécédent pour les relatives extraposées dans le cas de langues SOV avec ordre des mots variable comme le hongrois et le géorgien, est une autre indication des limitations importantes que conanait la construction de fragments.
Troisiémement, le conflit qui apparait entre une phrase relative focalisée et une phrase principale focalisée montre que la conservation du focus joue un rôle important.
Un ensemble d'autres facteurs auxquels on attribute souvent un rôle dans le traitement des relatives ne semblent pas avoir d'influence sur le traitement des relatives en hongrois.
Un codage de parole haute qualité à faible retard de 8 à 16 kbit/s peat être obtenu grâce aux algorithmesd'analyse par synthèse aver adaptation arrière : par exemple le CELP à faible retard (LD-CELP, le nouveau standard CCITT à 16 Kbit/s), le codage faible retard à excitation vectorielle (LD-VXC) et les codecs en arbre ou en trellis à adaption arrière.
Cet article examine et passe en revue certaines des techniques de base saus-jacentes aux algorithmes de codage “faible retard” et présente des compromis performance/conception pour les codecs faible-retard à analyse par synthèse aux débits de 8 à 16 kbit/s.
Plusieurs approches pour améliorer la qualité de la parole à 8 kbit/s sont examinées.
Une prédiction arriére du fondamental est comparée à one configuration avant en boucle fermée (semblable à celle du dictionnaire adaptatif des codeurs CELP classiques).
Pour conclure, on analyse la robustnese aux errears de transmission et on propose un certain nombre de compromis permettant de réduire la sensibilité aux erreurs de transmission.
Une théorie motrice de la perception proposée initialement pour rendre compte des résultats des premières expériences avec de la parole synthétique a été largement révisée afin d'interpréter les données récentes et de relier les propositions de cette théorie à celles que l'on peut faire pour d'autres modalités de perception.
La révision de cette théorie stipule que l'information phonétique est fournie par un système biologique distinct, un 'module' spécialisé pour détecter les gestes que le locuteur a eu l'intention de faire : ces gestes fondent les catégories phonétiques.
En conséquence le module provoque la perception de la structure phonétique sans traduction à partir d'impressions auditives préliminaires.
Ce module est ainsi comparable à d'autres modules tels que celui qui permet à l'animal de localiser les sons.
La particularité de ce module tient à la relation entre perception et production qu'il incorpore et an fait qu'il doit rivaliser avec d'autres modules pour de mêmes variations de stimulus.
Le propos de cet article est de faire un bilan des recherches récentes sur la première révolution urbaine, en s'appuyant sur deux sites emblématiques : Uruk puis Mari.
La démarche ne se limite pas à l'analyse du bâti mais est élargie à celle des rapports entre ces villes et leur arrière pays, en combinant résultats des fouilles anciennes et résultats des recherches les plus récentes.
Le stress provoqué par divers types de situations conduit à des modifications du signal vocal.
Des études précédentes ont indiqué que la parole stressée est caractérisée par une fréquence fondamentale plus élevée et des altérations du spectre des voyelles.
Cet article présente les analyses conjointes de ces deux paramètres à partir de corpus de parole stressée obtenus à la fois dans une situation réelle et dans une situation artificielle.
Le corpus de laboratoire est celui du test de Stroop et le corpus de la situation réelle est extráit d'un enregistreur des conversations d'un avion accidenté.
La fréquence fondamentale est étudiée macroscopiquement et un index μ de la variation microprosodique est introduit.
Les indicateurs spectraux du stress résultent d'un histogramme cumulé du niveau sonore et d'analyses statistiques des fréquences des formants.
Les distances par rapport au centre F1-F2-F3 sont aussi étudiées.
Toutes ces variations, à travers les deux situations, montrent un lien direct entre certains nouveaux paramètres du signal vocal et les apparitions du stress.
Les résultats confirment la validité des expérimentations de laboratoire, mais mettent également en évidence des différences quantitatives aussi bien que qualitatives entre les situations et les locuteurs concernés.
Des expériences sont présentées aussi bien sur des phonèmes isolés que sur de la parole continue.
Le CITH de Rennes a une double vocation d'innovation et d'évaluation des technologies pour la santé.
Il s'inscrit, plus spécifiquement, dans le cadre des systèmes de surveillance multivariés de diagnostic et des prothèses actives implantables permettant d'explorer, d'évaluer et de traiter les fonctions cardiovasculaire, nerveuse et respiratoire.
Cet article décrit brièvement sa genèse, ses partenaires et quelques-unes des activités conduites depuis 2001.
Dans cet article, nous déterminons la densité spectrale de signaux ayant périodiquement des données manquantes, et nous établissons leur modèle lorsqu'ils sont issus d'un processus ARMA.
Les performances de la reconnaissance sont fortement dégradées lorsque les systèmes de reconnaissance sont employés sur des réseaux téléphoniques particulièrement difficiles et dans des environnements bruités.
Il apparaît évident que la détection de parole/non-parole est une source importante de cette dégradation.
Ainsi la robustesse de la détection de parole est un problème crucial à examiner pour améliorer les performances de la reconnaissance pour des communications très bruitées.
De nombreuses études ont conduit à améliorer la robustesse de la détection de parole/non-parole pour une utilisation de la reconnaissance de parole dans des conditions difficiles.
Ce papier propose des solutions pour l'amélioration de la détection de parole/non-parole en environnement très bruité.
Des pré-traitements à la détection de parole sont d'abord considérés.
Nous proposons et comparons ensuite deux versions d'un algorithme de détection de parole/non-parole, fondées sur des critères statistiques.
Finalement, une technique de post-traitement est introduite dans le but de rejeter les détections de bruits prises pour de la parole.
Le classificateur par décision floue en deux passes (TSFDC) est un réseau de neurones fournissant une première phase de classification suivie d'une post-classification.
Cette dernière intègre, pour chaque classe de données considérée, une source d'information provenant d'ensembles de référence flous.
Le réseau isole les deux classes auxquelles un vecteur de données a le plus de chance d'appartenir.
La post-classification sélectionne la classe gagnante parmi les deux.
Le TSFDC applique sa post-classification seulement aux classes que le réseau a du mal à identifier.
Trois expériences d'identification automatique de locuteurs (ASI), indépendantes du texte, sont menées dans un cadre médico-légal.
Dans ces expériences, le signal est dégradé par un ensemble de facteurs influant sur les canaux de communication.
Lorsque les locuteurs sont médiocrement classifies par le réseau, le TSFDC permet d'accroître le pourcentage de trames correctement identifiées de 3.27% en moyenne, sur les trois expériences.
Simultanément, la différence entre le nombre de trames identifiées avec un locuteur légitime et un locuteur de second choix augmente de 5.27% en moyenne.
Ainsi, la post-classification diminue, de plus de moitié, le nombre de locuteurs que le réseau a classifié avec erreur.
Les partitions musicales sont des documents qui comportent de nombreux symboles constitués de segments de droite.
Dans le but d'extraire ces segments, nous avons mis au point un détecteur basé sur la technique du filtrage de Kalman.
En appliquant méthodiquement ce détecteur et en utilisant quelques règles simples de classification sur les segments trouvés, on reconnaît les portées, les queues de note, les liaisons, les barres de croche et les têtes noires.
Nous proposons un algorithme amélioré de filtrage du bruit multiplicatif, en traitement d'images numériques basé sur un développement homomorphique spatial.
Nous avons adopté une approche simple pour évaluer la statistique locale et supposé que la distribution locale de l'image originale est uniforme, ceci afin de traduire une méconnaissance de la probabilité originale qui peut être quelconque
La modélisation prédictive des usages du web a connu une période intense d'investigation jusque la fin des années 1990.
Pourtant, deux caractéristiques du web ont rarement été prises en compte : la présence de bruit et de navigations parallèles.
Dans cet article, nous proposons un nouveau modèle, le modèle SBR (Skipping-Based Recommender), qui utilise une technique appelée skipping, et qui est capable de prendre en compte ces caractéristiques de la navigation web.
Dans une série d'études expérimentales, nous mettons en avant les diverses contributions que possède ce modèle, et montrons que sa qualité surpasse celle des modèles de l'état de l'art.
En dépit de ressemblances superficielles, la phrase nominale (jumla ismiyya) de la tradition grammaticale arabe n'a que peu à voir avec ce qu'il est convenu de nommer ainsi en linguistique générale depuis au moins Meillet (1906).
Elle ne se caractérise pas par l'absence de verbe ou de copule, mais par une structure thème + propos (mubtadaˀ + ḫabar), et regroupe un ensemble assez consistant de faits, tout en permettant d'en donner une explication cohérente.
Cet article étudie la manière dont ces faits sont présentés et analysés dans un ensemble de grammaires produites en Europe depuis le XVII e siècle jusqu'à nos jours, posant le problème de l'inter-traductibilité des catégories linguistiques d'une tradition à l'autre.
Nous présentons dans ce papier une approche de l'ingénierie du Web qui prend en compte l'attention au contexte d'une manière compréhensive et intégrée permettant ainsi une meilleure adaptation à l'application de l'utilisateur final.
Nous présentons un modèle conceptuel, permettant la combinaison de l'ontologie du domaine avec des paramètres contextuels pertinents et un degré de significativité sur ces paramètres.
Nous discutons ensuite l'utilisation d'un tel modèle dans un processus d'ingénierie du Web en incluant un logiciel de modélisation approprié et les prérequis pour en faire un système temps-réel.
Cet article expose de façon générale le service VoiceDialingSM de NYNEX — la première réalisation d'un service téléphonique fondé sur une technique de reconnaissance de la parole à l'intention des abonnés privés et des entreprises.
Mis en place sur tout le réseau téléphonique, ce service permet aux usagers de composer leur appel uniquement par la voix, en énonçant le nom de l'appellé ou du lieu qu'ils veulent atteindre.
VoiceDialingSM est compatible avec les services multitouches et à cadran, ainsi conçu pour opérer sur tout type d'appareils téléphoniques et donc tout ce qui peut être raccordé chez l'abonné.
Cet article décrit l'architecture du réseau, l'interface usager et la technique de reconnaissance de la parole en insistant sur les conditions du succès de la mise en oeuvre du service et de son acceptation par les usagers.
L'article présente en introduction une vue générale des activités de recherche et de développement poursuivies à NYNEX Science & Technology dans le domaine du traitement automatique de la parole.
Nous explorons l'utilisation de représentations dérivées de l'analyse multirésolution de la parole et de l'opérateur d'énergie de Teager pour la classification de la parole de conducteurs en condition de stress.
Nous appliquons cette analyse à corpus d'énoncés courts pour créer des fonctions discriminantes dépendantes du locuteur pour quatre catégories de stress.
En outre nous adressons le problème du choix d'une échelle temporelle appropriée pour catégoriser les données.
Ceci mène à deux approches pour la modélisation.
Dans la première approche, la dynamique des variables issues de l'analyse d'un énoncé donné est supposée pertinente pour la classification.
Ces variables sont alors modélisées au moyen de réseaux bayésiens dynamiques (DBN) ou par un mélange des modèles de Markov cachés (M-HMM).
Pour la seconde approche, nous ne gardons que les valeurs moyennes de ces variables pour chaque énoncé.
Le vecteur résultant est alors modélisé au moyen d'une machine à support de vecteur et d'un perceptron multicouches.
Nous comparons les performances de ces deux approches à un tirage aléatoire (25%), les meilleurs résultats étant obtenus avec le mélange de modèles dépendant du locuteur (96,44% sur les données d'apprentissage, et 61,20% sur un jeu de test distinct).
Nous étudions également les performances de modèles indépendants du locuteur.
Bien que les performances se dégradent par rapport des modèles spécifiques aux locuteurs, le mélange de modèles surpasse encore les autres modèles et obtient un taux de reconnaissance sensiblement meilleur qu'un tirage aléatoire (80,42 sur les données d'apprentissage, et 51,22% sur le jeu de test).
Dans ce papier on remet en cause l'argument théorique principal qui sous tend les modules de stades pour le développement du langage (voir Gleitman).
Plus précisément on critique la proposition que les grammaires précoces sont exclusivement de nature sémantique.
On pense que l'utilisation des pronoms référentiels et de verbes infléchis ainsi que le rôle de la distinction animé/non animé dans le développement du genre linguistique peuvent impliquer des généralisations formelles non sémantiques dés leur apparution dans les productions des enfants de deux ans et plus.
Le stade précoce de la grammaire de deux mots ne peut être exclusivement 'sémantique'.
Puisqu'on trouve des généralisations non-sémantiques aussi bien que des généralisations sémantiques, il n'est pas nécessaire de postuler que les grammaires plus développées nécessitant des changements qualitatifs qui appuieraient un modèle de stades pour le développement du langage.
Dans cet article, des expériences de modélisation de voix utilisant une version récente du système de synthèse de parole du KTH sont présentées.
Il contient une source glottique améliorée basée sur le modèle LF de la source vocale, quelques paramètres supplémentaires de contrôle des sources voisée et bruitée, ainsi qu'une paire pole/zero pour le branchement nasal.
De plus, les versions du système de synthèse texte-parole présentées dans cet article rendent possibles les manipulations interactives au niveau des paramètres avec référence sur écran à la parole naturelle.
Le système de synthèse constitue donc un environnement flexible pour les expériences de modélisation de voix.
Les nouveaux modèles et outils de synthèse ont été utilisés dans des expériences de synthèse par analyse.
Une locutrice produisait une phrase dont on effectuait une copie stylisée qui employait respectivement l'ancien et le nouveau système de synthèse.
A l'écoute, la copie synthétique effectuée avec le nouveau système ressemble fort à la parole naturelle.
Cette perméabilité aux influences extérieures a naturellement entrainé l'assimilation au peul d'une grande quantité d'éléments signifiants provenant d'autres langues.
Parmi ces apports, les emprunts faits à l'arabe occupent une place particulière tant de par leur mode particulier de transmission que par leur importance numérique et leur présence dans tous les dialectes de la langue.
Des paramètres articulatoires sont déterminés à partir des cinq premières fréquences et des trois premières amplitudes formantiques par minimisation de l'erreur quadratique dans l'espace acoustique.
Les propriétés non linéaires de la transformation relians les paramètres articulatoires aux paramètres acoustiques sont analysées au préalable, grâce à une représentation paramétrique de la fonction d'aire et à un analogue électrique du conduit vocal.
Une analyse globale de la transformation est accomplie, de facon à localiser les non linéarités excessives qui induisent la procédure de minimisation en erreur.
Une table de couples articulatoires-acoustiques de référence est construite à partir de cette analyse.
La procédure d'identification des paramètres articulatoires comprend une estimation initiale à partir de la table de référence et un algorithme des moindres carrés.
Des tests sur des valeurs de formants issues du modèle lui-même montrent que la méthode est efficace pour résoudre le problème inverse.
Un autre test sur des valeurs de formants correspondant aux fonctions d'aire de Fant produit des résultats qualitativement acceptables mais le manque de précision dénote les limitations du modèle articulatoire utilisé.
L'objet de cet article est d'examiner la méthode classique d'extraction de la fondamentale basée sur l'analyse par autocorrélation à court terme du signal de parole.
Il est tenu compte de deux estimateurs communément utilisés et de l'effet de la fenêtre de prélèvement du signal.
Il est montré qu'une décomposition similaire de l'autocorrélation estimée vaut pour le cas d'un signal périodique comme pour le cas d'un signal aléatoire.
Une telle décomposition permet de prédire les mérites relatifs des estimateurs considérés pour autant qu'il soit question des erreurs grossières et des erreurs de voisement.
Le comportement prédit s'avère être en bon accord avec les résultats obtenus en appliquant l'algorithme SIFT à la parole naturelle.
Cet article décrit certaines des implications de l'effet de “centre de gravité spectrale” (SCG) mis en évidence par Chistovich, pour un modèle de la représentation auditive des voyelles de l'anglis.
Le travail de Chistovich sur la définition d'une distance critique pour l'effet SCG est étroitement lié à deux des problèmes les plus fondamentaux dans les recherches sur la communication parlée : la relation entre attributs acoustiques et traits phonémiques et le problème de l'invariance en dépit de grandes différences acoustiques entre locuteurs.
D'abord, nous passons en revue les découvertes expérimentales liées à l'effet SCG.
Ensuite, un modèle qui incorpore ces effects perceptifs est décrit, et enfin, trois aspects du modèle sont discutés : (1) l'analyse résultante en traits, (2) la normalisation et (3) la variation acoustique telle qu'elle est représentée dans le modèle et sa relation avec la théorie quantale de la production de la parole de Stevens.
Une telle simplification permet de transformer le problème d'apprentissage en un problème d'optimisation qui autorise une stratégie gloutonne ne nécessitant qu'une seule passe sur les données.
Notre stratégie d'optimisation pénalise la profondeur de l'arbre par le recours à la correction du R2.
Les expérimentations ont montré que la précision en généralisation des arbres par niveau n'est pas détériorée par rapport aux arbres usuels.
Il prononce également un petit vocabulaire, appelé vocabulaire d'adaptation.
Chaque nouveau locuteur prononce ensuite seulement le vocabulaire d'adaptation.
Nous avons comparé deux méthodes d'adaptation, établissant une correspondance entre les codes du locuteur de référence et ceux des autres locuteurs, sur une base de données produite par 20 locuteurs et contenant un vocabulaire d'adaptation de 104 mots.
La première méthode utilise un code transposé pour représenter le nouveau locuteur pendant le processus de reconnaissance tandis que la seconde utilise un code obtenu en regroupant les analyses effectuées sur la prononciation du vocabulaire d'adaptation.
Le vocabulaire d'adaptation contient 136 mots.
La comparaison des performances des deux méthodes montre qu'un nouveau code n'est pas nécessaire pour représenter un nouveau locuteur.
En conséquence de quoi nous avons utilisé la première méthode pour effectuer des tests sur un vocabulaire d'application de 5000 mots et sur une base de données produite par 4 locuteurs.
L'adaptation est toujours efficace, l'amélioration moyenne étant d'environ 14%, bien que l'amélioration relative n'est plus que de 30% par rapport à celle de 56% obtenue dans l'expérience sur le vocabulaire d'application de 104 mots.
D'autres expériences montrent que la précision de la reconnaissance peut être améliorée en augmentant la taille du vocabulaire d'adaptation ainsi que celle du code.
La détection de la fréquence fondamentale reste l'un des problèmes les plus difficiles de l'analyse de la parole.
Nous avons développé à cet égard une nouvelle méthode qui diffère des techniques conventionnelles.
Elle utilise un banc de filtres passe-bande couplés par paires;
elle est pleinement séquentielle dans le domaine temporel.
Cet article décrit l'optimisation paramétrique des filtres pairés pour une base de données et l'intégration de la méthode des filtres pairés par l'addition d'un détecteur de voisement.
Comparée aux autres méthodes, la nôtre produit un faible taux d'erreurs grossières.
Cet article expose un modèle qui permet de représenter à peu près n 'importe quelle règle de jeu de table conceptuellement programmable.
Ensuite, il s'attache à montrer de quelle manière un tel modèle est effectivement transposable sous la forme d'un code informatique.
Il décrit ensuite brièvement une application conçue pour mettre en œuvre ce modèle dans une problématique de confrontation de plusieurs types de moteurs de prises de décisions.
Enfin, il fournit un aperçu d'un protocole manipulable par un superviseur et permettant de contrôler et de tester les diverses confrontations des joueurs machines autour des règles autorisées par ce modèle.
Nous traitons dans cet article du problème de la reconnaissance de la parole en environnement bruité.
Les informations statistiques locales sur la parole et le bruit sont estimées en ligne, puis utilisées comme entrée pour les estimateurs.
Pour un rapport signal à bruit (SNR) de 20 dB, les résultats observés sont comparables à ceux obtenus dans des conditions non bruitées.
Une amélioration importante est également obtenue pour des rapports signal à bruit plus défavorables.
L'analyse attentive des résultats montre que les estimateurs MLP semblent fonctionner assez mal quand aucun signal de parole n'est pratiquement détectable.
Ceci nous a conduit à introduire une modification de la fonction de gain qui a encore augmenté les performances.
Cet article présente un essai d'optimisation du jeu de règles extrait par la technique des motifs fréquents.
On définit ensuite des règles « fortuites » par des techniques de simulation.
On discute alors du choix de celles qu'il convient de supprimer afin d'optimiser le jeu de règles de départ.
Les indices associés à des règles extraites de données s'appuient généralement sur le support et la confiance.
On mentionne dans l'article les résultats obtenus avec d'autres indices de qualité des règles utilisés actuellement en fouille de données.
Nous présentons un nouvel outil graphique interactifpour l'exploration des résultats d'arbre de décision, incluant simultanément : notre visualisation radiale, le focus+context, le zoom/pan, le fisheye, la visualisation hiérarchique, la treemap et l'icicletree pour la représentation et l'exploration des résultats des algorithmes d'arbre de décision.
L'utilisateur peut ainsi extraire facilement des règles d'induction et élaguer l'arbre obtenu dans une phase de post-traitement.
Cela lui permet d'avoir une meilleure compréhension des résultats obtenus.
Nous avons utilisé des critères d'intérêt pour évaluer la performance du système avec des ensembles de données réelles.
Cet article propose un principe de normalisation pour la vérification du locuteur en mode dépendant du texte fondée sur une modélisation HMM, principe dans lequel le score sur le modèle du locuteur prétendu et le score sur le modèle de normalisation sont calculés pour un même alignement, effectué sur le modèle du mot de passe indépendant du locuteur.
On montre que cette normalisation préserve une part de l'information caractéristique du locuteur contenue dans l'alignement, et augmente la pertinence du score normalisé en insistant sur les parties remarquables du modèle du locuteur.
Une procédure d'apprentissage spécifique est proposée.
Des évaluations sur une base de donné téléphonique réaliste sont décrites.
Enfin, les premiéres expériences sur l'intégration de l'information contenue dans l'alignement temporel dans la prise de décision sont présentées.
Tous ces résultats montrent l'intérêt de l'approche et encourage de futures recherches sur la caractérisation du locuteur dans une telle approche.
Nous présentons une méthode d'estimation de la direction d'arrivée de signaux non circulaires par un réseau d'antennes.
Basée sur l'algorithme Root-MUSIC (par résolution d'un polynôme), la méthode proposée est limitée aux réseaux d'antennes linéaires uniformes.
En revanche, elle permet de réduire considérablement le temps de calcul et d'augmenter le pouvoir de résolution par rapport aux méthodes qui nécessitent une recherche sur l'étendue de l'espace des paramètres (MUSIC et NC-MUSIC).
La supériorité de l'algorithme proposé est montrée par des simulations comparant sa performance d'estimation à celle d'algorithmes connus.
L'article décrit une approache phonétique expérimentale de l'étude de la mélodie de la parole développée à l'IPO.
La méthode proposée mène à des modèles intonatifs qui sont utiles pour interpréter des données acoustiques et physiologiques concernant la fréquence fondamentale en parole naturelle.
Elle constitue également un cadre pour le développement de règles pour la synthèse de l'intonation dans diverses langues.
Cet article passe en revue quelques théories récentes qui rendent compte de la façon dont les informations sensorielles et perceptuelles sont transmises au systéme de reconnaissance de mots par les processus qui sous-tendent la perception de la parole.
Dans la premiére partie, nous évoquons quelques problémes que tentent de résoudre depuis une trentaine d'années les chercheurs du domaine.
Dans la deuxième partie, nous examinons un cadre théorique de la perception de la parole où les étapes de traitement sont associés à des niveaux d'analyse linguistique.
Dans ce cadre on part de l'hypothèse que la parole est traitée dans une série d'étapes analytiques allant du traitement auditoire périphérique, de l'analyse phonétique acoustique et phonologique à la reconnaissance de mots et l'accés lexical.
Enfin, dans la dernière partie, diverses approaches des problèmes de la reconnaissance de mots et de l'accès lexical sont évaluées.
Nous examinons différentes propositions concernant l'analyse de “bas-en-haut”, les unités perceptuelles postulées et l'interaction entre différents types d'information dans la reconnaissance de mots.
Un objectif supplémentaire de ce travail consiste à établir l'importance des représentations segmentales dans la reconnaissance.
On présente ici un nouveau paramètre de fréquence, PSP (Parabolic Spectral Parameter), pour la quantification de la vélocité de volume de l'onde glottique.
PSP est basé sur l'adaptation d'une fonction parabolique à la partie basse-fréquence du spectre pitch-synchrone du flux glottique estimé.
PSP donne une valeur numérique qui décrit comment la décroissance spectrale d'un flux glottique obtenu se comporte par rapport à la limite théorique correspondant à la décroissance spectrale maximale.
Les performances de ce nouveau paramètre, pour l'analyse de signaux de parole caractéristiques de différents types de phonation, sont comparées à celles de trois paramètres d'usage courant, basés sur le temps, ainsi qu'à une méthode fréquentielle développée antérieurement.
Différents types de connaissances peuvent être extraits des données issues d'un questionnaire.
Elles dépendent du questionnement de l'analyste mais aussi des méthodes de traitement des données qui sont utilisées.
C'est ainsi que l'on peut obtenir le rejet d'une hypothèse nulle, mais aussi une typologie des items du questionnaire, des sujets qui y ont répondu, mais encore une structure graphique de filiation inférentielle, une hiérarchie de règles comportementales, etc.
Dans cet article, nous présentons plusieurs approches de traitement possibles d'un questionnaire visant à structurer des traits de personnalité dégagés de comportements de réponse au questionnaire.
Avec notre méthode, la parole–avec l'emotion qui convient–peut être produit synthétiquement en changeant tout simplement entre des bases de données de source crées par le corpus.
Les caractéristiques acoustiques de chaque corpus ne sont pas les mêmes et sont reconnaissables par émotion.
Les caractéristiques acoustiques de chaque parole émotionnelle produit synthétiquement par notre méthode montrent des corrélations évidentes avec les caractéristiques acoustiques de chaque corpus.
Des expériences perceptuelles utilisant la parole produit synthétiquement indiquent que notre méthode réussit à produire synthétiquement la parole émotionnelle de maniére reconnaissable.
Nous avons evalué davantage l'intelligibilité et l'impression generale que notre méthode a fait sur les auditeurs.
Les résultats montrent que la méthode proposée peut produire synthétiquement la parole avec un niveau élevé d'intelligibilité et donne une impression favorable.
Avec ces résultats encourageants nous avons developpé un systéme TTS valable avec la capacité d'émotion pour répondre aux besoins immédiats des individus qui ne peuvent pas parler.
Cet exposé décrit la méthode proposée, les caractéristiques acoustiques et de conception du corpus, et les résultats des évaluations perceptuelles.
Nous présentons dans cet article un algorithme d'estimation combinée qui calcule simultanément le spectre lissé du signal de parole ainsi que l'excitation de forme impulsionelle utilisée dans le codage prédictif multipulse (MPLPC).
Quoique la forme de l'excitation ainsi obtenue différe de l'excitation à impulsions multiples usuelle, les résultats expérimentaux indiquent que la différence entre les paramètres de codage prédictif optimisés et non optimisés et minimale tant du point de vue numérique que subjectif.
Cet article présente une méthode originale de détermination de la qualité d'une image en niveaux de gris.
Un exemple d'application de la méthode sur des images comprimées selon la norme JPEG est présenté.
Contrairement à la plupart des méthodes existantes, cette évaluation de qualité est univariante, c'est-à-dire ne nécessite aucune image de référence.
La qualité est donnée sous la forme d'une note dont la progression est étalonnée selon l'utilisation qui doit être faite de l'image : visuelle, mathématique, informatique.
Pour ce faire, un apprentissage est effectué à l'aide d'un réseau de neurones sur une base d'exemples connus faite d'images dont on a préalablement noté la qualité avec le modèle souhaité.
Pour s'assurer de sa fiabilité, la méthode est comparée à des méthodes bivariantes classiques.
Elle permet de retrouver avec une erreur inférieure à 7 % les résultats prévus par celles-ci.
Le but de cette étude est de proposer une nouvelle approche pour l'identification automatique des langues, basée sur une modélisation du rythme, ne nécessitant pas de données étiquetées manuellement.
Il faut tout d'abord savoir comment apporter des informations sur la prosodie, le rythme pour l'identification automatique des langues.
Pour répondre à cette question nous avons introduit une nouvelle unité, la pseudo-syllabe, qui est automatiquement extraite.
Des paramètres rythmiques et intonatifs sont alors calculés à partir de cette unité.
Des modèles élémentaires pour chaque type de paramètres sont définis en utilisant des mélanges de lois gaussiennes.
Ces modélisations de la prosodie sont couplées à une approche plus classique utilisant une modélisation acoustique des systèmes vocaliques.
Les expériences sont menées sur les cinq langues européennes du corpus MULTEXT.
L'intérêt des paramètres rythmiques, et l'efficacité de chaque système (modèle rythmique, modèle de la fréquence fondamentale et modèle vocalique) sont évalués.
L'impact de ces approches sur les performances d'identification est analysé.
Nous obtenons des résultats de 91 % d'identification correcte avec des fichiers de 21 secondes.
Des travaux récents ([CHI98], [DEN 98]) ont montré l'intérêt de l'utilisation de procédures de Metropolis dans un cadre bayésien, pour la recherche d'arbres de classification performants.
Pour une classe particulière de distributions a priori sur les arbres, nous introduisons un nouvel algorithme d'échantillonnage MCMC, semblable à un échantillonneur de Gibbs, utilisant le principe de l'algorithme de pondération récursive introduit par Willems et al. [WIL 95], ce qui permet de prendre en compte effectivement un nombre de modèles beaucoup plus important.
Les arbres ainsi échantillonnés sont moyennés pour obtenir un estimateur agrégé.
Nous présentons les résultats de simulations sur trois jeux de données de référence, montrant l'intérêt pratique de cette procédure.
Cette étude présente une analyse inter-linguistique des stratégies utilisées par des enfants coréens, japonais et anglais pour traiter les relatives.
Les résultats d'une expérience de comprehension des relatives en Coréen sont comparés avec des résultats obtenus précédemment sur l'acquisition des relatives en Anglais et en Japonais.
On a demandé à des enfants coréens de 6 ans de représenter des relatives branchées à gauche ou enchassées au centre de la phrase dans deux conditions d'ordre de mots SOV et OSV et dans deux conditions d'intonation : intonation “claire” motivée par la syntaxe et intonation “en liste”.
Les résultats montrent, pour le Coréen, une stratégie fondamentale de gauche à droite et des rôles significatifs pour une stratégie de phrase canonique et pour une stratégie de fonction parallèle.
Il est proposé que l'interprétation des relatives dans les langues se fonde sur l'intégration de plusieurs stratégies universelles de traitement dont l'application dépend des propriétés structurales des relatives spécifiques aux langues et au stade de développement de l'enfant.
Les modèles de Volterra sont très utilisés dans de nombreux domaines d'application du fait qu'ils permettent de représenter, avec une précision arbitraire, tout système non linéaire de mémoire finie.
Ils possèdent de plus la propriété d'être linéaires vis-à-vis de leurs paramètres, les coefficients des noyaux.
Le principal inconvénient de ces modèles est leur complexité paramétrique qui nécessite d'estimer un très grand nombre de paramètres.
Cet article présente une nouvelle méthode permettant de réduire cette complexité paramétrique en considérant les noyaux de Volterra d'ordre supérieur à un comme des tenseurs symétriques et en les décomposant à l'aide de la décomposition PARAFAC.
Les modèles de Volterra-Parafac ainsi obtenus peuvent être vus comme une série de modèles de Wiener mis en parallèle.
En exploitant cette nouvelle formulation des modèles de Volterra, nous proposons un algorithme d'identification récursif basé sur le filtre de Kalman étendu.
Des résultats de simulation illustrent le comportement de la méthode d'identification proposée, dans le cas de systèmes de Volterra cubiques.
KEAL est un système de reconnaissance de la parole continue développé au CNET à Lannion.
L'une des extensions en cours consiste à en faire un système de compréhension et de dialogue homme-machine.
Un dialogue de type question-réponse est mis en oeuvre en vue de fournir un renseignement à l'utilisateur (l'application actuellement étudiée est la simulation d'un centre de renseignements téléphoniques).
Cet article montre comment les connaissances syntaxiques, sémantiques et pragmatiques sont utilisées pour réaliser un tel dialogue, et discute des principaux avantages et inconvénients des méthodes retenues.
La reconnaissance des phrases est effectuée par un analyseur syntaxique ascendant de gauche à droite, à l'aide d'une grammaire sémantique hors-contexte.
On interprète ensuite l'arbre syntaxique, par une méthode analogue à celle des attributs sémantiques, afin d'obtenir une structure sémantique qui représente les informations utiles pour la suite du dialogue.
Le module de gestion de dialogue utilise la structure sémantique pour instancier un graphe-modèle qui représente à tout instant l'état du dialogue ; il indique le prochain message à envoyer à l'utilisateur et la manière d'analyser la réponse de celui-ci.
On décrit un exemple tiré du centre de renseignements téléphoniques.
Cet article propose une démarche d'intégration de connaissances pour l'amélioration d'un système de reconnaissance de défauts par vision sur des planches de bois.
Nous situons le problème de vision qui est à la base de cette étude, puis nous explicitons les connaissances métier nécessaires, aussi bien dans le domaine du métier du bois que dans le domaine de la vision.
Nous utilisons pour cela un modèle symbolique basé sur la méthode NIAM/ORM, formalisant ces connaissances métier à partir de leur expression en langage naturel.
Puis nous présentons la façon dont nous exploitons ces connaissances métier pour générer les nœuds d'une structure en arborescence pour l'identification des défauts des planches de bois.
Chacun des noeuds consiste en un moteur d'inférence à base de règles linguistiques floues.
Les résultats obtenus prouvent l'intérêt de cette démarche.
Cet article présente les résultats du développement, du déploiement et du test d'une application à large échelle de dialogue oral destinée au grand-public.
Nous avons construit un système oral automatique de questionnaire pour le bureau américain de recensement.
Dans la première phase du projet, les systèmes de reconnaissance et de dialogue ont été élaborés en utilisant 4000 appels.
Dans la seconde phase, le système a été adapté pour correspondre aux exigences du bureau de recensement puis déployé dans le cadre de la campagne 1995 de tests de nouvelles technologies lancée par cet organisme.
Dans la troisième phase, nous avons redéfini le système et avons montré empiriquement qu'un système oral automatique pouvait collecter et reconnaı̂tre les données de recensement avec succès, et que les sujets préféraient le système oral aux questionnaires écrits.
Notre collecte d'une quantité importante de données et les deux campagnes d'expérimentation terrain subséquentes ont montré que, quand les questions sont posées correctement, les réponses contiennent l'information attendue dans la catégorie de réponses correspondante dans environ 99% des cas.
Jean Marot a composé, dans La vraye disant Advocate des Dames (1506), un poème-rébus, sous la forme d'un « neuvain picard » .
Si les Grands Rhétoriqueurs sont connus, entre autres, pour être des « jongleurs de syllabes » , l'artifice littéraire ici employé n'est pas que le simple fruit d'un divertissement.
À travers l'analyse des différents niveaux de lecture du poème (jeu littéraire, poème historique, poème polémique …), l'auteur tente de démontrer l'impact qu'a eu cette oeuvre sur la future carrière du poète.
Plus encore sans doute que dans son rébus-rondeau précédemment analysé par Adrian Armstrong, on constate que les relations spatiales spécifiques de la mise en page du neuvain sont significatives à plusieurs niveaux.
Le langage implique une structure et un processus.
En rendant à chacun son dû, nous présentons un modèle du processus cognitif et montronse comment sa valeur empirique est reliée aux propositions sur la structure syntaxique.
Toutefois, ce qu'on observe dans les expériences n'est pas une structure syntaxique mais l'exécution d'un plan.
Nous présentons un langage de processus pour représenter ces plans et fournir une explication unifée de plusieurs phénomènes dans le développement, en incluant les résultats des expériences récentes et ceux de nouvelles expériences suggérées par notre approche.
L'explication se fait en termes de resources cognitives requises pour formuler et exécuter un plan.
Comme cette explication s'appuie sur un traitement non syntaxique, la syntaxe des entants n'a pas à être tenue pour fautive.
La conclusion renforce la proposition que le nombre de structures syntagmatiques disponibles pour les enfants est biologiquement contraint.
Comprendre les bases neurales de la cognition est devenu un problème abordable scientifiquement, et des modèles sont proposés dans le but d'établir un lien causal entre organisation neurale et fonction cognitive.
Au cours du développement et chez l'adulte, cette évolution interne est de nature épigénétique : elle ne requiert pas d'altération du génôme.
L'activité (spontanée ou évoquée) d'un réseau de neurones au cours du développement stabilise de manière sélective certaines synapses et en élimine d'autres, contribuant, de ce fait, à la mise en place de la connectivité adulte à l'intérieur d'une enveloppe de potentialités définies génétiquement.
A un niveau supérieur, la modélisation de représentations mentales par des états d'activité de populations restreintes de neurones est réalisée par les méthodes de la physique statistique : la mémorisation de ces représentations est envisagée comme un processus de sélection parmi des “pré-représentations” variables et instables.
Des modèles théoriques montrent que des fonctions cognitives comme la mémoire à courtterme ou la manipulation de séquences temporelles peuvent dépendre de paramètres physiques élémentaires.
Une implémentation neuronale et sélectionniste des intentions est envisagée.
L'objectif de cet article est de montrer que l'étude des systèmes socio-techniques complexes peut bénéficier des concepts développés dans le cadre de la théorie de la complexité.
Nous prendrons comme exemple celui de la conception d'un service d'urgence médicale.
Dans ce papier, les expériences menées à l'ITC-irst visant à transférer notre système de reconnaissance d'informations télédiffusées en italien vers deux applications de dialogue en parole spontanée sont présentées.
Cette étude porte sur l'utilisation des techniques de l'état de l'art pour l'adaptation des modèles acoustiques et linguistiques du système et sur l'évaluation de la relation entre performance et qauntité de données annotées utilisée.
Différents niveaux de supervision ont aussi été étudiés pour l'adaptation des modèles acoustiques.
Deux heures de parole manuellement annotées ont permis d'obtenir des taux d'erreur en mots de 26,0% et 28,4% avec les systèmes adaptés.
Ces résultats sont à comparer avec 22,6% et 21,2% qui oint été obtenus par les systèmes spécifiquement développés pour ces deux tâches avec une plus grande quantité de données.
Finalement, une méthode robuste permettant de régler, lors du décodage, l'insertion de phénomènes particuliers à la parole spontanée est présentée.
Un enjeu important de la recherche de formes dans une base d'images est la définition de seuils non supervisés permettant d'éviter une déferlante de fausses détections, ou, au contraire, des rejets de formes qui auraient dû être reconnues.
En prenant comme exemple une méthode de reconnaissance de forme proposée par Lisani [15, 16], nous montrons que l'on peut répondre à la question suivante : étant donnée une forme requête et une base d'images, à partir de quelle distance entre la forme requête et une forme détectée est-on sûrs que la forme est reconnue ?
Cette assurance est quantifiée par le nombre de fausses alarmes associé à la paire requête - forme candidate.
Cette méthode ne considère pour l'instant que des morceaux de forme et permet pourtant déjà d'aboutir à des détections sûres basées sur un seul morceau de forme.
A l'audition, les sons de parole contiennent de l'information à la fois phonétique, individuelle et de transmission.
Il ressort de plusieurs études dans différentes languages que les différences entre les fréquences principles des voyelles pronouncees par des hommes, des femmes et des enfants indiquent une tendance assez uniforme. Ces différences sont considérées comme relevant des qualités (timbre) personnelles.
Les différences entre les sexes sont principalement dues à la descente du larynx qui a lieu chez les mâles durant la puberte.
Nous reproduisons la tendance observée concrnant les fréquences des formants chez les hommes et les femmes par un calcul où nous prenons en considération les conséquences phyiologiques de la descente du larynx, tout en supposant que les commandes nerveuses aux articulateurs, pour chaque voyelle, restent inchangées.
La perception du timbre phonétique est vue comme un processus de reconnaissance de figures tonotopique.
Nous affirmons et montrons que, dans les voyelles phonétiquement identiques, les distances tonotopiques inférieures à 6 Bark entre les formants sont invariantes.
La position absolue des formants permet une variation personnelle.
La distance tonotopique entre le premier formant et la fondamentale est moins grande pour la plupart des voyelles éimises par des femmes que chez les hommes et les enfants.
En ce qui concerne le rôle du son fundamental par rapport à ces faits, on propose quelques hypothèses alternatives.
Les experts en classification d'images utilisent des caractéristiques variées pour représenter les textures.
Nous proposons de choisir les plus pertinentes à l'aide d'une procédure automatique de sélection de caractéristiques.
Nous comparons pour cela l'efficacité de plusieurs algorithmes de sélection récents.
L'ensemble des algorithmes est évalué à l'aide de critères heuristiques ainsi que de performances de classification.
Nous démontrons l'intérêt d'une telle procédure de sélection à partir d'images de Brodatz et d'images satellitaires.
Le marqueur arabe bien connu de l'objet pronominal iyyā- s'acquitte d'autres fonctions au sein de la langue, parmi lesquelles on citera le démonstratif.
Celui-ci a été reconnu dans l'arabe dialectal égyptien, mais il passe virtuellement inaperçu dans l'arabe écrit.
Néanmoins, il est utilisé plus souvent par les écrivains du monde arabophone oriental que par ceux de l'Occident.
En tant que tel, il remplit habituellement quatre rôles dans la structuration de l'information : exprimer le contraste, la réflexivité emphatique et deux degrés de la deixis distale.
Alors que les écrivains modernes arabes semblent l'utiliser démonstrativement plus souvent que ceux du Moyen Âge et de l'arabe classique, le recours des écrivains antérieurs laisse penser que sa propriété démonstrative est une caractéristique inhérente.
Ceci est confirmé par la comparaison des marqueurs d'objet dans d'autres langues sémitiques – en hébreu et en araméen – où ils peuvent fonctionner comme démonstratifs, et comme réflexifs en syriaque et dans la deixis à distance en langue amharique.
Dans le cadre d'une étude de faisabilité du dénombrement d'épis de blé par imagerie couleur, une méthode d'analyse de textures sur des composantes d'espaces couleurs a été développée.
L'objectif agronomique est la prévision de rendement avant la moisson par évaluation du nombre moyen d'épis par unité de surface en tenant compte de la variabilité intra-parcellaire.
Pour ce dénombrement par image, nous étudions six paramètres de texture (deux valeurs statistiques et quatre coefficients d'Haralick issus de la matrice de cooccurrence) que nous évaluons sur les composantes d'espaces couleurs utilisées en agronomie.
Un nouvel espace hybride permet de créer une représentation d'images de blé prises en milieu naturel dans lesquelles l'extraction d'épis sera améliorée.
La méthode basée sur des mesures de distances (Euclidienne, de Mahalanobis) permet d'extraire les épis avec quelques erreurs corrigées par de la morphologie mathématique.
Malgré les difficultés dues à la variation de luminosité et à l'entropie élevée des scènes, les résultats permettent de trouver en partie les épis, et les dernières images en court de traitement permettent une meilleure segmentation.
L'utilisation de la reconnaissance automatique de la parole pour l'automatisation des transactions téléphoniques permet de réduire considérablement les coûts de fonctionnement et d'améliorer la qualité du service pour le client.
A GTE, nous ayons concentré nos efforts sur le développement de services interactifs à commande vocale, appelés OASIS.
Dans cet article, nous exposons la méthodologie utilisée pour le développement des dialogues, nous décrivons le dialogue mis au point pour l'application d'interruption de service téléphonique et nous présentons les résultats d'une expérimentation terrain.
Notre méthodologie de développement de dialogues comprend la mise au point d'un modèle de transaction, la définition d'un schéma de dialogue, l'élaboration de structures de langage et la construction du vocabulaire du système.
La structure du dialogue et le vocabulaire de reconnaissance sont définis en tenant compte des capacités de reconnaissance et d'interprétation du système.
Les principales caractéristiques de cette méthodologie sont les suivantes : une représentation structurée des transactions verbales permettant une acquisition progressive des informations ; une formulation des questions suivant un style de discours qui suscite des réponses prédictibles ; une utilisation des résultats de la reconnaissance et des actions correspondantes du système pour définir l'évolution du dialogue ; et un développement des solutions adaptables.
Pour évaluer l'efficacité de notre approche, nous présentons les résultats d'une expérimentation terrain concernant l'interruption de service.
Globalement, les usagers ont réagi de façon coopérative, ont adhéré à une interaction structurée, n'ont donné que rarement des informations de façon anticipée et ont fourni des réponses pertinentes aux questions du système.
Nous présentons dans cet article une nouvelle approche de la Reconnaissance de Formes basée sur le Modèle des Croyances Transférables, une interprétation non probabiliste de la théorie des fonctions de croyance de Dempster et Shafer.
Le principe de cette méthode consiste à caractériser sous la forme d'une fonction de croyance l'information apportée par un ensemble d'apprentissage, relativement à la classe d'un nouveau vecteur.
Différentes stratégies de décision avec coûts arbitraires, généralisant l'approche bayésienne, sont présentées et illustrées à l'aide d'un exemple.
La base de données SUSAS est une collection de phrases enregistrées dans des conditions de stress simulé ou réel dans le but d'étudier l'influence du stress et du style sur le signal acoustique.
L'objet de la présente étude était la validation perceptuelle de la partie simulée de la base de données.
Sept auditeurs jugeaient que les mots monosyllabiques ou dissyllabiques énoncés par des locuteurs étaient perçus comme étant prononcés de façon Colérique, Claire, Rapide, Forte, Neutre, Interrogative, Lente ou Douce.
Les locuteurs ont un accent soit de Boston, de New-York ou plus généralement de type americain.
La moyenne des pourcentages de jugements “corrects” étaient soumise à une analyse de la variance, qui montre que le pourcentage des classifications correctes était seulement de 58%, et que ce pourcentage était une fonction de l'accent, du style de parole, et du nombre de syllabes.
Dans cet article, la structure des groupes phonématiques de l'allemand est examinée.
La connaissance des groupes phonématiques possibles est indispensable par example pour l'analyse des influences contextuelles sur les indices acoustiques-phonétiques dans un système automatique pour la reconnaissance de la parole.
Comme les principaux effets de coarticulation se limitent à la région syllabique finale, il est nécessaire de distinguer la position des phonèmes groupes consonantiques dans la syllabe.
Nous avons ainsi obtenu deux graphes pour l'ordre temporel du mode d'articulation dans tous les groupes consonantiques initiaux et finals.
Un système multi-agent est constitué d'un grand nombre d'entités, appelées agents, en interaction entre elles au sein d'un même environnement.
Cette technologie aborde de nombreux domaines d'applications comme la vision par ordinateur, la robotique, la simulation de systèmes, le commerce électronique.
Nous considérons que les questions abordées en traitement du signal sont très pertinentes dans un cadre multi-agent.
Nous présentons d'abord les principaux outils dont disposent les concepteurs de systèmes multi-agents à savoir : des modèles, des plates-formes et des méthodes de développement.
Puis, le projet SCALA de simulation de résolution de problèmes par des patrouilles aériennes et un projet de simulation de système de transports illustrent la résolution de problèmes à l'aide de systèmes multi-agents.
Ensuite, nous nous intéressons plus particulièrement aux capacités d'adaptation de tels systèmes que nous abordons comme une question de résolution émergente de problèmes.
Dans ce cadre nous décrivons en détail la théorie AMAS (Adaptive Multi-Agent System) qui permet de concevoir des systèmes dont la fonction globale émerge à partir d'un processus d'auto-organisation coopérative de ses parties.
Une application en prévision de crues donne une indication plus précise des capacités de telles approches.
Un modèle d'identification de voyelles stationnaires à un et deux fromants est présenté.
Le modèdele comprend un algorithme de détection de formants et un algorithme de classification.
Le signal est représenté dans le modèle par une combinaison de plusieurs configurations de trois types.
Chaque configuration se caractérise par sa position sur un axe de tonalité et par un “coeeficient type”.
Les positions dese configurations sont déterminées par les positions des pics spectraux détectés.
Les coefficients types dépendent des distances entre le pics détectés et le centre de gravité du spectre auditif.
Le modèle fournit en sortie une “distribution de réponses” ; un vecteur normalisé de similitudes entre un signal et des classes définies (phonèmes).
Une expérience d'identification de signaux à deux formants avec des relations d'amplitude variables entre formants est décrite, et les résultats sont comparés avec les “distributions de réponse” du modèle.
Les résultats de la seconde expérience sur l'identification des signaux à un formant sont utilisés pour vérifier les paramètres de l'algorithme de détection de formants.
Une interface cerveau-ordinateur (ICO) est un nouveau type d'interface homme-machine qui permet la communication directe entre l'utilisateur et la machine en décodant l'activité cérébrale.
Les potentiels cognitifs évoqués comme le P300 peuvent être obtenus grâce au paradigme oddball - stimulus discordant - où les cibles sont sélectionnées par l'utilisateur.
Une nouvelle méthode pour la réduction des capteurs des signaux électroencéphalographiqes (EEG) est proposée.
La réduction du nombre de capteurs permet d'accroître le confort de l'utilisateur en diminuant le temps nécessaire à la pose des capteurs.
L'approche proposée est basée sur une élimination récursive des capteurs où la fonction de coût est basée sur une évaluation du rapport signal sur signal plus bruit (RSSB), après un filtrage spatial.
Nous montrons que cette fonction de coût est plus robuste et moins coûteuse en temps de calcul que d'autres fonctions basées sur l'évaluation de la détection du P300 ou des cibles, permettant ainsi d'éviter une étape de classification.
Nous proposons également une fonction de décision qui permet de mieux catégoriser l'importance d'un capteur en fonction du nombre de capteurs désirés.
L'approche proposée est testée et validée sur 20 sujets au cours de plusieurs sessions.
Les algorithmes génétiques, la programmation génétique, les stratégies d'évolution, et ce que l'on appelle maintenant en général les algorithmes évolutionnaires, sont des techniques d'optimisation stochastiques inspirées de la théorie de l'évolution selon Darwin.
Nous donnons ici une vision globale de ces techniques, en insistant sur l'extrême flexibilité du concept d'évolution artificielle.
Cet outil a un champ très vaste d'applications, qui ne se limite pas à l'optimisation pure.
Leur mise en œuvre se fait cependant au prix d'un coût calculatoire important, d'où la nécessité de bien comprendre ces mécanismes d'évolution pour adapter et régler efficacement les différentes composantes de ces algorithmes.
Par ailleurs, on note que les applications-phares de ce domaine sont assez souvent fondées sur une hybridation avec d'autres techniques d'optimisation.
Les algorithmes évolutionnaires ne sont donc pas à considérer comme une méthode d'optimisation concurrente des méthodes d'optimisation classiques, mais plutôt comme une approche complémentaire.
Nous présentons une nouvelle approche expérimentale pour l'étude des lapsus : les sujets doivent intervertir délibérément des phonèmes à des positions spécifiées dans des paires de mots présentées sous la forme d;une liste, les mesures de la performance étant la vitesse et la précision.
Dans le cas de mots CVC, les performances de transposition ont été les meilleures pour les phonèmes initiaux et les plus faibles pour les phonèmes finaux.
Le degré de facilité de transposition des phonèmes médians dépendait de ce que la voyelle médiane était, ou non, une diphtongue et aussi du fait qu'un échange de lapsus produirait, ou non, un changement orthographique important, à supposer que la réponse purement articulatoire fût effectivement transcrite.
Les variables orthographiques apparaissent donc influencer le processus, mm̂e dans une tâche qui se situe vraisemblablement à un niveau purement acoustico-articulatoire.
Bien que la performance n'ait pas été affectée par le status de mot/non-mot de la réponse, l'environnment articulatoire des phonèmes adjacents a influencé effectivement la performance d'inversion des phonemes médian.
Enfin, les gauchers ont fait preuve d'une habileté supérieure dans la production de tels lapsus sur demande, ce qui pourrait reflécter une habileté supérieure à lire et à écrire en miroir.
Ces résultats induisent des hypothesès nouvelles pour l'étude des lapsus spontanés.
Ce texte présente l'approximation simultanée de l'affaiblissement et de la distorsion du temps de propagation de groupe pour les filtres numériques récursifs (à réponse impulsionnelle infinie).
La méthode développée est caractérisée par l'utilisation de deux étapes successives : l'approximation portant sur l'affaiblissement seul, faisant appel à un algorithme itératif n'exigeant que la résolution d'un système d'équations linéaires et donc très rapide, suivie par l'approximation simultanée, basée sur la résolution à chaque itération d'un problème de programmation linéaire.
La convergence des deux algorithmes est garantie ; leur efficacité est démontrée par application à un exemple test.
Plusieurs sortes de caractére de non-texte apparaissent souvent dans les textes.
L'expression orale de ces caractéres peut être changé selon les sens de textes.
Ce travail propose un classeur de trois-couches (TLC, three-layer classifier) qui peut efficacement résoudre le probléme ambigu de ces non-texte caractéres dans le mandarin TTC systéme.
Ces trois couches sont empoyées en ordre.
La premiére couche est composée en deux éléments : le tableau de modéle et l'arbre de décison.
Si cette couche peut désambiguı̈ser les sens de caractére prévu, la tâche de désambiguı̈té va arrêter.
Sinon les deux couches suivantes va être déclenché.
D'aprés l'algorithme de confiance, la troisiéme couche peut exploiter un modéle de remplacement pour améliorer la performance.
L'expérience montre que l'approche proposée ici peut avoir une très bonne assimilation même avec très peu de données.
Les précisions d'entraı̂nement et d'essai sont respectivement 99.8% et 97.5%.
Dans cet article nous présentons la base de données parole du CTH que nous créons actuellement à l'École Polytechnique Chalmers à Göteborg (Suède).
Le matériel comprend aujourd'hui des sons isolés (phones et diphones), des phrases sémantiquement non reliées ainsi que des textes cohérents.
Cette collection de données est restreinte à du suédois lu.
L'enregistrement du signal de parole a été effectué en chambre sourde en utilisant un enregistreur audio-numérique SONY PCM-F1.
La segmentation, la classification et la transcription sont exécutées sur huit niveaux d'analyse linguistique, incluant les niveaux acoustique, phonétique et prosodique.
La méthode DFE (Extraction Discriminante de Paramètres) fournit un formalisme adéquat pour la conception d'un module d'extraction de paramètres pour un système de classification de formes.
Au cours des dernières années, cette méthode a été appliquée avec succès à différents problèmes en reconnaissance de la parole tels que la classification de voyelles et de phonèmes ou la reconnaissance de mots isolés.
Le formalisme DFE peut être utilisé pour pondérer les contributions des différentes composantes d'un vecteur de paramètres.
Cette variante de DFE, que nous appelons DFW (Pondération Discriminante de paramètres), améliore un système de classification de formes en favorisant les composantes assurant la meilleure discrimination interclasses.
Cet article est consacré à l'application du formalisme DFW à la reconnaissance de la parole continue par modèles de Markov cachés (HMM).
Deux types différents de reconnaisseurs sont étudiés : ceux fondés sur des HMM discrets (utilisant une distance euclidienne) et ceux fondés sur des HMM semi-continus (utilisant des mélanges de gaussiennes).
Nous montrons comment les composantes peuvent être pondérées et comment les poids peuvent être appris de façon discriminante.
Des résultats expérimentaux sont fournis pour différentes tâches de parole continue.
Ces résultats montent l'intérêt du formalisme en reconnaissance de parole continue par HMM.
Traditionnellement le raisonnement à base de cas (CBR) s'appuie sur des expériences décrites dans des formats complètement structurés tels que des objets ou des enregistrements de base de données.
Toutefois, d'autres modèles ont été proposés pour surmonter les limitations de cette approche structurelle et rendre possible l'application à des domaines plus variés.
Dans cet article, nous passons en revue les extensions du formalisme CBR proposées pour traiter des expériences décrites dans des documents textuels, travaux regroupés sous la bannière CBR textuel.
Après une présentation succincte des principes généraux du raisonnement à base de cas, nous décrivons les principaux travaux du CBR textuel et nous les comparons selon différents aspects techniques et applicatifs.
Finalement, nous proposons quelques problèmes et avenues de recherche méritant d'être explorés dans des travaux futurs.
On a fait deux expériences pourétudier le ro˛le de la présupposition syntaxique dans la compréhension des phrases.
Dans la premiére expérience les sujets doivent vérifier, en fonction de contextes présentés avant les phrases, des phrases clivées, des pseudo-clivées et des phrases avec des compléments factitifs.
Les sujets mettent significativement plus de temps pour vérifier les phrases avec des présuppositions fausses que pour vérifier les phrases avec des assertions fausses.
Dans l'expérience II, les sujets vérifient les phrases clivées et pseudo-clivées en fonction d'images présentées aprés les phrases.
Les temps de vérification pour les phrases avec des présuppositions fausses sont ici aussi significativement plus longs que les temps de vérification pour les phrases avec des assertions fausses.
On rend mieux compte de ces données avec une hypothése “structurale” qu'en termes de stratégies ayant pour but de localiser les informations données ou nouvelles.
Les variations de prononciation observées aujourd'hui chez les locuteurs sont parallèles, sur de nombreux points, aux variations de prononciation reportées au cours des siècles (le changement phont́ique).
Il est raisonnable de conclure qu'il existe une certaine relation nécessaire entre les deux.
Je soutiens que les variations diachroniques émergent, pour la plupart, à partir des variations synchroniques, et donc que des contraintes physiques universelles et atemporelles de la production et de la perception de parole conduisent les auditeurs à mal appréhender le signal de parole.
Chacune de ces méprises qui conduisent l'auditeur à prononcer des choses d'une façon différente est potentiellement le début d'une changement phonétique.
L'étude de le changement phonétique permet donc bénéficier de certains éclairages sur la façon dont la parole est produite et perçue.
Je montre cela sur un exemple, en étudiant toute une série des changements phonétiques portant sur des fricatives sourdes : ce que l'on appelle la nasalisation spontanée, la s-aspiration et l'effacement nasal.
Ces changements suggèrent que, pour cette classe de sons, la qualité de voix spécifique sur la portion de la voyelle qui suit immédiatement la fricative constitue un indice.
Trois expériences de décision lexicale cherchent à étudier la séparabilité du traitement syntaxique et sémantique au cours de la reconnaissance auditive des mots.
Une quatrième expérience étudie le problème de la mesure des temps de réponse à un stimulus auditif.
Les mots utilisés sont du Serbo-croate, chaque stimulus consistant en une racine nominale (qui correspond à un mot attesté ou bien à un pseudo-mot) et d'une flexion casuelle qui véhicule de l'information sur le cas grammatical du nom.
La vitesse d'identification des formes fléchies d'un mot dépend de leur sens syntaxique plutôt que de leur forme physique ou de leur fréquence d'apparition.
En plus, l'identification d'un nom est facilitée lorsque celui-ci est précédé par un stimulus véhiculant de l'information permettant de prédire le cas du nom, qu'il s'agisse d'un véritable adjectif ou d'un pseudoadjectif.
Ces résultats sont analogues à ceux obtenus préalablement pour la perception des mots en présentation visuelle ; ils suggèrent qu'il existe une grande uniformité dans le traitement des suffixes flexionnels pour le langage écrit et parlé.
Dans les deux cas, les résultats suggèrent que le traitement des suffixes flexionnels est modulaire, du moins dans la mesure où il est indépendant du traitement sémantique pendant la partie initiale de son déroulement.
Plusieurs activités ont été entreprises en italie dans le domaine de la communication numérique entre moyens mobiles.
En particulier pour le codage de la voix, deux codeurs ont été développés et on les a comparés sur un système de transmission du type SCPC (Single Channel Per Carrier).
Les codeurs sont décrits dans l'article et leurs caractéristiques sont comparées.
La caractéristique principale des deux codeurs réside dans l'emploi de la quantification liée au spectre du signal à envoyer au récepteur.
Le codeur SB-APC utilise un post-filtre pour modeler le bruit de quantification de chaque sous-bande.
L'article souligne que les deux codeurs présentent presque les mêmes résultats du point de vue de la qualité subjective malgré les différences de structures.
Nous proposons un nouveau modèle du signal glottique.
Conventionellement, le signal de source est modélisé en concaténant un faible nombre de segments de courbes afin d'approcher la forme de l'impulsion glottique.
Nous proposons une alternative qui développe le signal en une combinaison d'un ensemble de fonctions élémentaires.
Celles-ci sont choisies de manière à tenir compte du caractère ponctuel de la source voisée et de la non-linéarité de son fonctionnement.
Nous établissons les relations mathématiques entre les poids des fonctions temporelles de base et les coefficients de Fourier du signal à modéliser.
Les paramètres de contrôle sont la fréquence et l'amplitude d'une fonction excitatrice cosinusoïdale.
L'enveloppe du signal émis (c.-à-d. des impulsions glottiques) évolue avec la fréquence fondamentale et son contenu spectral change avec l'amplitude de la fonction excitatrice.
L'ensemble des traitements est réalisé à l'aide de réseaux de neurones.
Ce système est une sorte de robot simulé capable d'agir dans son environnement afin de reconnaître des objets dépà appris.
L'un de ses principaux attraits est qu'il permet une communication simple entre les traitements de haut et de bas niveau.
Enfin et surtout, il a été conçu pour montrer que l'on n'a pas besoins d'avoir des régions bien fermées ou des contours parfaits pour réaliser une bonne interprétation.
Notre robot est un exemple relativement simple de ce que les réseaux de neurones intégrés à une approche cybernétique permettront de réaliser.
Malgré cela, il est déjà intrinsèquement capable de reconnaître plusieurs objets dans une scène complexe même s'ils sont bruités, déformés, ou en partie occultés.
Les performances des systèmes actuels de reconnaissance de parole se dégradent rapidement en présence de bruit.
Une nouvelle représentation du signal de parole, basée sur la prédiction linéaire de séquence d'autocorrélation unilatérale (One-Sided Autocorrelation Linear Prediction : OSALPC), s'est avérée être intéressante pour la reconnaissance de la parole bruitée, à la fois pour ses bonnes performances (par rapport au codage LPC conventionnel) dans des conditions difficiles de bruit blanc additif et pour sa simplicité de calcul.
Le but du travail présenté dans cet article est double : (1) il s'agit de montrer que OSALPC fournit également de bonnes performances pour de la parole bruitée en contexte réel d'usage (en voiture), et (2) d'explorer sa combinaison avec diverses techniques robustes de mesure de similarité, en montrant que ses performances s'améliorent en utilisant une pondération cepstrale, des indices dynamiques et l'étiquetage multiple.
Cet article concerne le probléme de l'intégration temporalle de l'information dans la perception des voyelles ainsi que la nature de cette information.
Des stimuli caractérisés, soit par un formant, avec, soit par 2 formants, F1 et F2, soit encore par un formant évolutif produit par trains d'impulsions alternantes de F1 et F2, en proportions différentes, ont été utilisés dans des expériences d'identification de voyelles.
Les distributions de réponses correspondant aux stimuli à 2 formants et à ceux avec formant évolutif ont été approchées par des sommes pondérées de deux distributions “de base”, 1 et 2, correspondant aux stimuli à un seul formant.
Un accroissement de la proportion d'impulsions F2 dans les stimuli à formant évolutif est accompagné d'un accroissement systématique de la composante 2 de la distribution de réposens.
Les stimuli avec formant évolutif n'ont pas suscité les réponses phonémiques caractéristiques des stimuli à 2 formants avec mémes fréquences de F1 et F2.
Les données suggérent qu'il y a intégration temporelle des résultats du processus d'idenfication continue du stimulus.
Ce papier décrit une méthode tentant d'extraire l'information de transition entre segments pour les tâches de reconnaissance de la parole.
Les caractéristiques dynamiques (variant lentement) des trajectories spectrales contiennent beaucoup d'information discriminante qui est mal modélisée dans les approaches HMM traditionnelles.
Dans les approches telles que les réseaux de neurones récurrents, il y a l'espoir, mais pas de démonstration convainquante, que cette information de transition pourrait être utilisée.
La méthode présentée ici se base sur un principe assez différent et consistant à modéliser explicitement la trajectoire des paramètres spectaux à court terme dans un sous-espace où l'information temporelle est préservée.
Ceci est réalisé en introduisant une contrainte temporelle dans la technique bien connue de l'Analyse en composantes Principales.
Dans ce sous-espace, on a alors défini un modèle paramétrique de la trajectoire, et une measure de distances a été utilisée pour effectuer la classification en diphones.
En utilisant la méthode de “Principal Curves” de Hastie et Stuetzle et la “Generative Topographic Map” de Bishop, Svensen et Williams une description de l'évolution temporelle en termes de variables latentes a été effectuée.
Sur le problème difficile de /bee/, /dee/ et /gee/, il a été possible de conserver l'information discriminante avec un ensemble réduit de paramètres.
Des illustrations expérimentales sont présentées sur les bases de données ISOLET et TIMIT.
Le manque de descriptions quantitatives des sons de la parole tant pour les consonnes que pour voyelles pose souvent des problèmes pour la synthèse de la parole.
Habituellement, seules les voyelles sont caractérisées à l'aide of fréquences formatiques.
Cet article propose une description paramétrique des zones quasi stationnaires de tous les sons non-plosifs à l'aide de centroïdes dans l'espace des coefficients de prédiction linéaire “log area ratio”.
Pour chacun des sons un centroïde est calculé comme centre de gravité sur un enxemble de réalisations en situations contextuelles différentes prononcées par un seul locuteur.
Les centroïdes sont ensuite comparés à l'aide d'une mesure de distance objective.
Le comportement dynamique de certains sons est également analysé dans des essais de destruction et de construction du signal de la parole.
Enfin, toutes ces informations servent à discuter plusieurs problèmes phono-acoustiques concernant la délimitation des sons de l'allemand et surtout les relations entre la qualité et la quantité des voyelles.
Dans cet article, nous présentons Strada, une approche globale utilisant l'apprentissage pour la conception automatique de stratégies dans l'environnement de ces jeux.
Strada combine de nouvelles idées avec des techniques avancées d'apprentissage automatique.
Ces solutions sont intégrées dans un système efficace, dont les performances sont démontrées dans le cadre d'un wargame commercial.
Cet articles décrit les méthodes d'analyse et de synthèse développées au ECL de NTT.
Les procédures PARCOR et LSP basées sur un filtre tout pôle sont expliquées à partir de la méthode LPC.
Les principes et les interprétations physiques de ces méthodes sont comparés entre elles.
Les caratéristiques paramétriques des traints sont clarifiées par diverses expériences.
La qualité de la parole synthétisée est également illustrée par des expériences objectives et subjectives.
Grâce à ces méthodes, de la parole synthétique de haute qualité peut être obtenue à un faible taux de transmission, au-dessous de 9600 bps.
Dans cet article, nous proposons une méthode de caractérisation d'images d'ouvrages anciens basée sur une approche texture.
Cette caractérisation est réalisée à l'aide d'une étude multirésolution des textures contenues dans les images de documents.
Ainsi, en extrayant cinq indices liés aux fréquences et aux orientations dans les différentes parties d'une page, il est possible d'extraire et de comparer des éléments de haut niveau sémantique sans émettre d'hypothèses sur la structure physique ou logique des documents analysés.
Au travers de ces expérimentations, nous mettrons en avant la pertinence de ces indices et les avancées qu'ils représentent en terme de caractérisation de contenu d'un corpus fortement hétérogène.
La localisation des syllabes est une étape importante pour la plupart des systèmes d'analyse prosodique.
Cette localisation est utilisée pour identifier les accents lexicaux ou intonatifs qui servent ensuite de base pour l'analyse du rythme et de l'intonation.
Cet article présente un nouveau système de syllabation, utilisant des réseaux neuronaux récurrents, qui fonctionne sur de la parole continue, indépendante du locuteur.
Il est appris et testé sur la région dialectale 1 de la base de données TIMIT : il y détecte 94% des syllabes et place la plupart des frontières syllabiques à moins de 20 msec des emplacements souhaités.
On présente également diverses méthodes d'optimisation des performances et d'apprentissage des réseaux récurrents.
Dans cet article, nous décrivons l'implantation du synthétiseur à formants de Klatt (série et parallèle) opérant en temps réel sur un seul processeur du signal ST8940 de SGS-Thomson.
La simulation de l'algorithme de synthèse en virgule flottante a permis la détermination des conditions optimales de fonctionnement.
La simulation en viTgule fixe est réalisée pour l'estimation de la longueur adéquate du mot et la mise à l'échelle de l'ensemble des variables impliquées.
La réalisation de ces deux étapes a permis la programmation de l'algorithme de synthèse en langage assembleur du processeur.
Notre objectif à court terme est la réalisation d'une carte compatible PC de synthèse de la parole autour du processeur ST18940.
Nous traitons dans cet article du problème de la segmentation d'images à partir de niveaux de gris et sans prise en compte de la notion de texture.
Toutes les méthodes peuvent être rendues automatiques, ou non supervisées, en leur adjoignant une méthode d'estimation de mélanges.
Des études antérieures ont montré que le choix de la méthode d'estimation a, dans le cas gaussien, peu d'influence sur le résultat final.
Cependant, les comportements généraux des méthodes locales et globales sont très différents et aucune famille n'est supérieure à l'autre dans toutes les situations.
Le choix de l'algorithme est fait à partir de l'homogénéité de l'image des classes et de la corrélation spatiale du bruit.
La pertinence des choix est montrée via simulations et segmentations des images réelles.
On discute le fait que les théories de la mémoire sémantique suivent une divergence parallèle à celle des controverses concernant la représentation de la signification.
Le modèle de comparison des traits (Smith, Shoben et Rips, 1974) applique la théorie linguistique de Lakoff (1972) pour prédire le temps de réaction dans la vérification des phrases, alors que le modèle de recherche des marques, décrit ici, utilise le type de représentation sémantique défini par Katz (1972) pour expliquer des donnés analogues.
Les deux modèles sont décrits et leur portée est revue.
Le modèle de recherche de marques se vérifie bien mais en revanche une prédiction majeure de modèle de comparaison des traits est infirmée.
On discute le fait que le modèle de comparaison des traits est inadéquant pour rendre compte de la représentation sémantique tant que sa conception des consituants sémantiques reste inchangée.
Au cours de ces dernières années, la gestion des agro-écosystèmes est devenue un enjeu majeur du développement durable.
Cette gestion doit permettre de résoudre des problèmes environnementaux cruciaux et doit prendre en compte les brusques changements de contexte tels que les changements climatiques ou de politique agricole, etc.
La réponse à ces problèmes de décisions complexes passe par un recours accru à la modélisation, la simulation et l'expérimentation virtuelle.
Dans cet article, nous présentons des travaux récents de l'intelligence artificielle ayant contribué au thème de la modélisation et la simulation de systèmes complexes pour l'analyse des modèles agronomiques et la conception de décision.
Nous présentons des formalismes originaux pour la modélisation et la simulation de systèmes complexes ainsi que pour la conception de stratégies basés sur les réseaux de contraintes pondérées ou les processus décisionnels de Markov.
Nous abordons ensuite le couplage entre les deux thèmes simulation et décision.
Enfin, nous illustrons l'utilisation de ces méthodes et modèles sur plusieurs cas d'études en gestion des agro-écosystèmes.
On présente la théorie de Piaget sur la perception de l'espace par les jeunes bébés dans le cadre d'un systéme hypothético-déductif.
Les auteurs définissent 11 hypothèses portant sur : l'agent du changement visuel, des constances de forme et de taille, de la distance et de la perception des relations d'un ordre supérieur entre leséléments spaciaux.
Les propositions de Piaget pour chacune de ces hypothèses sont présentées en plusieursétapes : la preuve comportementale, l'interprétation en terme d'états intérieurs, les inférences et généralisations.
Enfin, les conclusions générales sont brièvement discutées.
Dans cet article sont étudiées des mesures acoustiques du bruit et des microperturbations du signal de parole en vue de la discrimination entre voix normales et dysphoniques.
Uné revue de travaux similaires effectués par d'autres chercheurs japonais est également présentée.
Ce chapitre propose à la discussion, à travers onze questions, un ensemble organisé de principes pour une analyse orientée vers la conception des systèmes technico- organisationnels en tant que systèmes dynamiques, vivants, sociaux et culturels.
Après avoir précisé une notion ontologique de complexité adéquate aux systèmes technico- organisationnels, ces questions abordent les divers aspects théoriques, épistémologiques et méthodologiques (à la fois de recueil de données, d'analyse et de modélisation) de la connaissance de cette complexité, ainsi que leurs relations avec la conception.
La discussion fait appel à des contributions disciplinaires diverses, tant scientifiques qu 'ergonomiques ou philosophiques, et est en relation avec des idées développées par les auteurs d'autres chapitres de cet ouvrage.
Les phrases grammaticalement incorrectes (paragrammatismes) sont caractéristiques du langage spontané de certains aphasiques.
Les paragrammatismes produits par cinq aphasiques a “jargon néologique” ont été compares aux paragrammatismes de quatre sujets normaux de contrôle.
Nous montrons que les paragrammatismes des aphasiques sont qualitativement identiques aux erreurs grammaticales des sujets normaux, mais qu'ils sont beaucoup plus frequents.
Une explication est proposée en termes de modèles de production de la parole ; nous essayons de montrer que les paragrammatismes sont la consequence d'une défaillance des processus de contrôle.
Cet article contient une étude analytique de dix nouvelles inscriptions thamoudéennes, notées en graphie “thamoudéen E”, rassemblées par les auteurs au cours d'une exploration dans la région d'al-Jafr (sud-est de la Jordanie).
L'étude se propose d'analyser les inscriptions, les sens et les structures des mots et noms propres qui y figurent.
Ce groupe d'inscriptions met en relief de nouveaux noms personnels mentionnés dans les inscriptions thamoudéennes.
Un MLP a été entraîne à transformer en paramètres de contrôle d'un synthétiseur de parole basé sur un modèle du tube acoustique le spectre de puissance de voyelles et de consonnes nasales produites par un seul locuteur.
Les sorties du MLP contrôlent ces onze sections de manière à reproduire une parole synthétique dont les spectre de puissance est semblable à celui présenté à l'entrée du réseau.
Après l'apprentissage, on peut synthétiser de la parole, restreinte à ce locuteur et à ces phonèmes, avec une bonne intelligibilité.
Nous proposons un codeur à multi-impulsions de complexité réduite.
Une réduction de la complexité de 33% affecte très peu la qualité de la parole produite comme nous le montrons sur base d'un calcul du rapport signal sur bruit et sur base d'une évaluation auditive informelle.
Elle permet, par contre, l'implantation du codeur à l'aide de circuits commerciaux peu coûteux.
Le TMS32020 a été utilisé pour une implantation de l'algorithme en temps réel à 16 kbit/s en virgule fixe.
La méthode étudiée ici peut être considérée comme une alternative à la technique multi-pulse conventionnelle.
Nous étudions la conception d'un joueur artificiel pour le jeu de Tetris.
Après une revue des principaux travaux, nous soulignons le fait que comparer différentes performances doit être fait avec le plus grand soin, car les scores ont une grande variance, et de subtils détails d'implémentation ont un effet significatif sur les résultats.
Nous considérons ensuite la méthode d'entropie croisée pour optimiser la fonction d'évaluation d'un joueur artificiel, comme suggéré par Szita et al. (2006).
Dans ce contexte, nous discutons de l'influence du paramètre bruit, et nous effectuons des expériences avec plusieurs jeux de fonctions de base, comme celles introduites par Bertsekas et al. (1996), par Dellacherie (Fahey, 2003) et des fonctions originales.
Cette approche aboutit à un programme de Tetris dont les performances dépassent celles des autres programmes connus.
Sur une version simplifiée de Tetris, considérée par la plupart des travaux de recherche, il réalise 35 000 000 ± 20 % de lignes en moyenne par partie.
Dans cet article, nous proposons une approche originale d'estimation séquentielle de densités non paramétriques définies dans des espaces de grande dimension, dans le cadre méthodologique du filtrage particulaire.
En exploitant les indépendances conditionnelles de l'espace d'état, nous proposons de permuter des sous-ensembles indépendants de particules de manière à générer un nouvel ensemble échantillonnant mieux cet espace.
Nous intégrons cette approche dans deux versions classiques du filtre particulaire : celui avec échantillonnage partitionné et celui à recuit simulé de manière à prouver son efficacité.
Nous comparons notre modèle aux approches classiques dans le cadre de l'estimation des densités d'objets synthétiques articulés.
On montre en particulier que le maximum du débit d'entropie correspond au maximum de l'erreur de prédiction.
Utilisant alors les coefficients de réflexion, on montre très simplement l'équivalence de la méthode avec celle du modèle autorêgressif.
On présente également une interprétation en termes de blancheur, ce qui permet de discuter ce que donnerait le minimum d'entropie.
Dans les systemes recherche d'information vocale, la reconnaissance de la parole est appliquee a une collection pour obtenir certaines information qui correspond aux contraintes des requetes.
On a explore la recherche basee sur phoneme de “n-grams”.
L'utilisation de ces phonemes concerne le probleme du “vocabulaire exterieure”, et l'utilsation du concept de “n-gram” permet le matching approximatif dans un environnement imprecis de transcription.
Les experiences (c'est-a-dire, les tests) on permit d'explorer beaucoup d'aspects, comme par example, “word boundary information”, elimination des points d'arret, expansion des requetes, variation de la longueure des sequence de phonemes, et la combinaison des n-grams.
Nos resultat experimentals ont montre qu'il y a une deterioration dans l'efficacite de la rechere, mais pour des cas particulier de matching, cela n'etait pas important parce que la sequencee des phonemes est correcte.
Dans les cas ou les suite de phoneme sont directement reconnues, il etait important de selectionner une bonne approache de matching.
La combinaison de n-grams de different longueures (3-grams and 4-grams) on permit d'ameliorer l'efficacite de la methode de recherche.
Le manuscrit BnF, fr. 6449 constitue la seule copie connue de la Vie de sainte Katherine de Jean Miélot (1457), biographie fort développée de la sainte, originaire d'Alexandrie en Égypte, que le chanoine bourguignon a voulu encadrer dans l'histoire romaine du IVe siècle.
Cet article met en lumière l'intérêt de cette œuvre, par une présentation du manuscrit, une première enquête sur les sources latines du texte, et une analyse linguistique et des techniques de traduction de Miélot ; il offre ainsi une étude préliminaire à l'édition critique qui manque encore.
Cet article présente une description de la base de données POLYCOST qui est dédiée aux applications de reconnaissance du locuteur à travers les lignes téléphoniques.
Les caractéristiques de la base de données sont : corpus moyen à contenu varié (>100 locuteurs), anglais parlé par des étrangers, chiffres lus et parole libre, enregistrement à travers des lignes de téléphone internationales, minimum de neuf sessions d'enregistrement pour 85% des locuteurs.
Cet article illustre l'importance de divers facteurs cognitifs dans la perception et la compréhension de la parole de synthèse.
Toutefois, cette difficulté décroît avec l'exposition des sujets à la parole de synthèse.
Une charge mentale plus importante est requise lors de l'écoute de parole de synthèse et les sujets écoutant des textes en parole de synthèse sont obligés d'être plus attentifs que ceux écoutant de la parole naturelle.
En particulier, les machines qui peuvent reconnaître des images de visage sont très coûteuses.
Cet article montre comment un système de reconnaissance des visages peut être réalisé par un réseau de neurones artificiel de type perception multicouche et par un réseau de neurones à spike.
Le système à base de spike est développé pour acquérir les importantes caractéristiques du visage, pour simuler le système de la vision humaine et pour optimiser le temps de calcul, ce dernier objectif c'est la principale force derrière le développement des systèmes à base des réseaux de neurones à spike.
A noter que l'apprentissage du réseau sur différents ensembles d'images lui force à apprendre comment il se comporte vis-à-vis la variété des visages, un problème commun dans le monde réel…
Cet article décrit un système de vérification automatique du locuteur (ASV) utilisant un modèle de Markov semi-continu (SCHMM) à plusieurs dictionnaires, employant une nouvelle méthode pour la discrimination par modèle de Markov appelée “probabilité d'observation discriminante” (DOP).
Cette méthode n'est pas un apprentissage discriminant, mais simplement un moyen de mieux séparer deux HMM standard pour améliorer la discrimination entre les classes qu'ils représentent.
La technique DOP peut être superposée à tout système à base de HMM et ne demande pas d'apprentissage complémentaire.
On peut envisager de l'appliquer à toute application de classification par HMM, pour un problème à deux classes.
On présente des résultats sur des expériences dépendantes du texte avec 24 locuteurs de référence et 100 imposteurs “naïfs”, enregistrés sur le réseau téléphonique britannique.
Cet article décrit notre travail dans le domaine de la reconnaissance multilingue de parole.
D'abord nous présenterons les différentes stratégies : portation, reconnaissance à travers plusieurs langues et la reconnaissance simultane des plusieurs langues.
Puis nous présentons les résultats obtenus.
Ces dernières années nous avons porté notre système de reconnaissance en différentes langues (Italien, Slovaque, Slovène, Tchèque, Anglais et Japonais).
Nos expériences montrent que certaines langues sont plus facile a reconnaître, pour la même complexité du domaine.
La substitution des sons des langues inclues est de grand intérêt pour la reconnaissance à travers plusieurs langues et la reconnaissance simultane des plusieurs langues.
Nous comparons les résultats pour les différents systèmes de base pour la reconnaissance à travers des langues.
Nous avons trouvé que la nombre de sons communs est un critère principale pour la qualité de la reconnaissance.
Pour la reconnaissance multilingue, la reconnaissance diminue en générale comparé à la reconnaissance monolingue.
Dans peu de cas, par exemple, quand il est parlé dans une autre langue que la langue maternelle, la reconnaissance peut être améliorée.
L'objectif de ce travail est de savoir comment un auditeur identifie que deux phrases successives sont prononcées par une ou par deux personnes quand il ne dispose que des informations acoustiques relatives aux caractéristiques individuelles des voix des locuteurs et à l'intonation.
Le matériau de test utilisé consistait en fragments d'énoncés successifs, de structure lexicale figée, pouvant former une ou plusieurs répliques d'un dialogue.
Lors des expériences, on a fait varier les locuteurs, les types d'énoncés et les types de cohérence communicative.
Des paires d'énoncés, avec ou sans pause interne, étaient présentées aux sujets qui avaient pour tâche de les classer dans l'une des trois catégories suivantes : dialogue, monologue ou méta-dialogue (imitation d'un dialogue par un seul locuteur).
Les résultats montrent que les facteurs qui influent sur les décisions des auditeurs sont les suivants : les caractéristiques individuelles des voix des locuteurs, la présence ou l'absence de pause et le type communicatif des énoncés.
On suggère que deux principes sont mis en oeuvre dans le traitement des signaux.
Dans le cas où les énoncés sont séparés par une pause, c'est la comparaison auditive des timbres qui sert de critère de décision.
Nous présentons dans cet article un nouvel algorithme biomimétique permettant de créer des groupes au sein de données et de les visualiser dynamiquement.
Cet algorithme s'inspire des insectes volants se déplaçant en nuage en créant des mouvements complexes à partir de règles locales simples.
Chaque insecte représente une donnée.
Le déplacement des insectes vise à créer des groupes de données homogènes se déplaçant ensemble dans un espace à deux dimensions.
Les groupes créés et visualisés en temps réel informent l'expert du domaine qui a fourni les données sur leur structuration en classe, par exemple, le nombre de classes plausible, le regroupement de données similaires, et les données isolées représentant des cas « à part » .
Nous présentons des extensions de l'algorithme comme la diminution du temps de calcul ou l'utilisation d'un affichage 3D.
L'approche est étudiée sur des données artificielles et réelles.
Un algorithme heuristique permet d'évaluer la pertinence des partitionnements trouvés.
On décrit ici l'amélioration d'un système de reconnaissance de mots isolés à base de phonèmes. Ce système utilise une procédure robuste d'extraction des données de référence et fournit de meilleurs taux de reconnaissance.
De plus, on présente une nouvelle méthode pour son adaptation à la parole continue.
Le système de reconnaissance de mots isolés décrit ici utilise la technique de répertoires multiples ; l'algorithme LVQ, qui fournit des répertoires bien définis et efficaces, minimise l'influence de la coarticulation entre mots et permet l'utilisation de l'information temporelle dans la phase de reconnaissance.
La méthode d'adaptation est basée à la fois sur la modification des répertoires de référence à partir d'un petit nombre de données de parole continue, représentatives, et sur des transformations linéaires des principaux paramètres prosodiques (énergie et durée).
Des tests approfondis, menés sous diverses conditions (avec des données dépendantes et indépendantes du locuteur, des répertoires simples ou multiples, adaptés ou non-adaptés, reconnaissance par phonèmes ou par mots, etc.), ont montré l'efficacité des méthodes proposées.
Cet article porte sur les deux témoins des Cent Nouvelles Nouvelles qui nous sont parvenus : le manuscrit de Glasgow (University Library, Hunter 252), et l'incunable d'Antoine Vérard (Paris, BnF, Rés. Y2-172).
Un rappel des problèmes philologiques concernant la transmission du texte est suivi de l'analyse comparée des deux programmes iconographiques (enluminures et bois gravés), en particulier pour les nouvelles 9, 12, 27, 33, 46.
Chomsky & Halle (1968) soutiennent que l'accentuation et l'intonation d'un énoncé ne sont pas déterminées seulement par les propriétés physiques du signal acoustique mais également par l'organisation syntaxique de l'énoncé.
Les résultats obtenus en faisant entendre à des auditeurs la répétition continue d'une suite de mots monosyllabiques appuient tout à fait cette thèse.
spicos II rend possible un dialogue avec une base de données de bureau.
C'est un prolongement de spicos I qui était limité, du point de vue linguistique, à un simple schéma de question/réponse.
spicos II s'adapte au locuteur et possède un vocabulaire d'environ 1200 mots.
Dans le présent article, nous donnons une vue d'ensemble du traitement linguistique de notre système.
Un module de dialogue gère les composants linguistiques. Il contrôle le dialogue et évite, grâce à des questions adéquates, les problèmes de communications entre l'utilisateur et le système.
L'analyse syntaxique se base sur une grammaire syntagmatique étendue.
Parallèlement, un réseau sémantique vérifie les restrictions sémantiques.
Dans le but de construire une représentation logique de la signification de la phrase, la structure syntaxique est utilisée en liaison avec les caractéristiques sémantiques, les résultats de la résolution anaphorique et une représentation formelle des référents du discours.
Les présuppositions de l'utilisateur sont analysées et représentées.
Ces représentations formelles peuvent être transformées en une interrogation de base de données.
Cette étude traite des effets de la coarticulation sur les fréquences formantiques et sur la durée du schwa hollandais, en syllabe ouverte et fermée, à partir de l'analyse de logatomes de la forme C1əC2V et VC1əC2.
Dans ces logatomes, les consonnes C1 et C2 appartenaient au groupe /p, t, k, f, s, χ, m, n, η, r, l, j, ν/, et la voyelle V était l'une des 3 voyelles suivantes /i, a : , u/.
Les consonnes et voyelles ont été présentées systématiquement dans toutes les combinaisons possibles, ce qui donnait au total 897 mots expérimentaux lus par 3 locuteurs masculins.
Il est apparu que les effets de la coarticulation sur le schwa pourraient être décrits par un simple modèle linéaire.
Le modéle s'est révélé particuliérement bien adapté pour le suivi de F 2.
Ce modèle pour F 2 a pu aussi être utilisé avec succès pour des mots réels.
Nous pensons que le schwa doit être interprété comme une voyelle sans cible d'articulation, complètement assimilée à son contexte phonémique.
L'idée largement répandue que les fréquences formantiques des voyelles réduites se déplacent vers une position de schwa au centre du triangle vovalique n'est pas très exacte.
Dans notre interprétation, la réduction vocalique provoque un déplacement des fréquences formantiques vers une position de schwa qui peut être presque n'importe où dans l'espace vocalique, en fonction du contexte phonémique.
Le suivi de visage par caméra vidéo est abordé ici sous l'angle de la fusion évidentielle.
La méthode proposée repose sur un apprentissage sommaire basé sur une initialisation supervisée.
Le formalisme du modèle de croyances transférables est utilisé pour pallier l'incomplétude du modèle a priori de visage due au manque d'exhaustivité de la base d'apprentissage.
L'algorithme se décompose en deux étapes.
Pour fusionner les sources couleur dépendantes, nous proposons un opérateur de compromis inspiré de la règle prudente de Denœux.
Pour la phase de suivi, les probabilités pignistiques issues du modèle de visage garantissent la compatibilité entre les cadres crédibiliste et probabiliste.
Elles alimentent un filtre particulaire classique qui permet le suivi du visage en temps réel.
Nous analysons l'influence des paramètres du modèle évidentiel sur la qualité du suivi.
Un algorithme récemment proposé pour la détermination de la fréquence fondamentale du signal de parole (Dologlou et Carayannis, Speech Communication, Vol. 8, 1989) utilisait un filtrage passe-bas itératif de phase nulle, suivi de mesures du signal filtré entre deux impulsions d'excitation successives.
Le filtrage passe-bas itératif était achevé lorsque le signal avait acquis un profil suffisamment sinusoïdal.
Le critère d'arrêt choisi était que les fréquences dérivées d'une analyse d'autocorrélation et d'une analyse LPC de second ordre devaient êntre suffisamment proches l'une de l'autre.
Les auteurs affirmaient par ailleurs que, à moins que le signal d'entrée ne soit une sinusoïde pure, les deux fréquences ne devaient jamais être égales.
Nous discutons ici de ce critère d'arrð proposé et donnons un exemple concernant un signal proche du signal acoustique de la parole, consistant en deux sinusoïdes d'amplitudes comparables.
Les erreurs ont été analysées en référence à : (i) des paramètres structuraux généraux (longueur des nombres …) ; (ii) des troubles comportementaux généraux (persévérations, perturbations de l'organisation séquentielle …) ; (iii) des stratégies cognitives mises en jeu dans cette activité de transcodage.
Cette derniére analyse s'est révélée la plus intéressante ; elle a montré que les erreurs systématiques produites relevaient le plus souvent de la mise en oeuvre dans un contexte où elles ne s'appliquent pas de stratégies de transcodage efficaces dans d'autres situations.
Ces résultats recueillis en pathologie du langage peuvent aider à comprendre la nature et l'organisation des processus cognitifs utilisés par les sujets normaux.
Nous proposons un algorithme qui évalue à la fois l'excitation et les paramètres d'un modèle dynamique auto-régressif à moyenne adaptée du signal de parole.
Les coefficients du modèle auto-régressif et du modèle à moyenne adaptée sont estimés indépendamment et dans le sens des moindres carrés à l'aide de deux treillis récessifs de processus combinés ; l'excitation à l'entrée est évaluée à l'aide d'une méthode “bootstrap” à partir d'un des deux estimateurs.
On montre à partir des estimations expérimentales d'enveloppes spectrales de parole naturelle que l'algorithme proposé évalue correctement les paramètres d'un modèle du signal de parole auto-régressif et à moyenne adaptée lentement variable.
Cet article fournit une nouvelle interprétation du “delta-cepstre” ; il étend sa formulation classique en vue de la détermination optimale des caractéristiques du filtre, qui extrait la dynamique caractéristique importante d'une séquence cepstrale.
L'algorithme permettant d'obtenir les nouveaux paramètres est présenté sous une formulation utilisant un filtrage par une matrice de coefficients ; il a été testé par des expériences de reconnaissance de la parole en langue japonaise.
L'erreur moyenne de reconnaissance dans une expérience de reconnaissance des 24 phonèmes du japonais sur quatre locuteurs a été ramenée de 12.2% à 10.3%.
La maîtrise d'une maladie animale non réglementée est à l'initiative des éleveurs et est parfois incitée par des organisations professionnelles pour améliorer la situation sanitaire ou économique d'une zone.
L'enjeu est donc de pouvoir proposer des outils d'aide à la prise de décision et d'évaluer a priori l'impact des décisions proposées sur la propagation d'un agent pathogène en termes épidémiologiques (prévalence de la maladie) et économiques.
Dans cet article, nous évaluons l'apport des processus décisionnels de Markov (MDP).
Nous proposons un modèle de propagation intertroupeaux où une action de maîtrise est recommandée par un décideur collectif pour optimiser le coût de la maladie et de sa maîtrise au niveau du groupe.
Nous supposons que le décideur collectif connaît la proportion d'éleveurs qui vont suivre sa recommandation.
L'utilisation d'un MDP intégrant un modèle épidémiologique permet d'indiquer à chaque pas de temps s'il faut faire une recommandation ou non selon la situation épidémiologique.
La stratégie obtenue consiste en des recommandations non systématiques.
Bien que l'objectif soit d'optimiser les coûts, la prévalence dans la zone est aussi diminuée.
La définition d'une stratégie adaptative est un avantage de notre approche qui permet de proposer des stratégies non classiquement proposées et étudiées.
La parole est normalement entendue sur un fond sonore.
Cet article examine les travaux récents sur l'aptitude de l'auditeur à séparer la parole des autres sons.
Il est montré que des mécanismes de regroupement de bas niveau et une connaissance spécifique de la parole sont utilisées pour résoudre ce problème difficile.
Avec l'accroisssement des applications significatives de la reconnaissance et de la synthèse de parole, l'activité de notre laboratoire couvre maintenant un champ plus large d'activités, depuis les nouvelles approches algorithmiques jusqu'à l'ingéniérie des produits vocaux et le développement d'applications.
Cet article présente un bilan des produits développés à partir de nos recherches en technologies vocales.
Ce papier présente une technique fondée sur les propriétés des fonctions retard de groupe afin d'extraire les formants des signaux de parole.
L'algorithme est semblable au lissage cepstral utilisant la déconvolution homomorphique.
Les différences significatives sont les suivantes : (a) le logarithme est remplacé par un opérateur () r et (b) les propriétés additive et de haute résolution des fonctions retard sont exploitées pour accentuer les crêtes des formants.
La fonction retard de groupe (ou la dérivée négative de la phase de la transformée de Fourier) est dérivée pour un signal qui, à son tour, est dérivé de l'amplitude de la transformée de Fourier du signal.
Si une valeur convenable de r est utilisée, cette méthode donne des estimations formantiques très cohérentes comparées à celles obtenues par la technique cepstrale ou par la prédiction linéaire.
Les effets de l'exposant r et de la largeur de la fenêtre sur la technique proposée ont été étudiés.
Le problème traité est celui de l'estimation d'un signal perturbé par un bruit additif lorsque l'on dispose de deux observations chacune composée d'un signal et d'un bruit additif.
On se place dans le cas où les signaux utiles sur chaque voie sont déduits d'un même signal par filtrage linéaire et les bruits complètement décorrélés.
Nous cherchons à évaluer l'apport de la deuxième observation pour une meilleure estimation du signal par rapport au cas où une seule observation est disponible.
Le filtrage de Wiener vectoriel ainsi que deux structures sous-optimales dites PIS (Prétraitement + Identification entre Signaux) proposées pour l'amélioration de la parole bruitée sont présentés.
Une étude théorique (en supposant optimaux les filtres utilisés) est d'abord menée : ces différents systèmes ainsi que le filtrage de Wiener mono-voie sont comparés entre eux en termes de distorsion, bruit résiduel et erreur globale sur le signal à estimer.
Un classement de ces méthodes est donné en fonction des valeurs relatives des rapports signal à bruit sur chaque voie.
Nous montrons l'intérêt des deux structures PIS par rapport au filtrage de Wiener mono-voie lorsque le rapport signal à bruit de l'observation sur laquelle est présent le signal à estimer est faible.
Après une présentation des différents algorithmes effectivement utilisés dans ie cadre du débruitage de la parole, nous les appliquons à des signaux réels enregistrés dans une voiture.
Une comparaison des algorithmes est faite suivant des critères objectifs et des tests d'écoute.
Les résultats indiquent une supériorité des deux méthodes PIS par rapport aux filtrages de Wiener mono-voie et bi-voie.
On décrit ici un nouveau système de reconnaissance de parole qui exploite les structures articulatoires multi-dimensionnelles er incorpore des idées-clé de la phonologie auto-segmentale et de la phonologie articulatoire.
La nouveauté consiste dans la définition des unités atomiques de la parole afin de pouvoir rendre compte, d'une façon unifiée et économique, des mécanismes de dépendance du contexte observés au niveau acoustique.
Au coeur du système de reconnaissance, on trouve une procédure qui a été développée pour convertir automatiquement une observation probabiliste de recouvrement entre cinq dimensions articulatoires en un automate à états-finis qui sert de composant phonologique pour le système de reconnaissance.
Le module d'interface phonétique du système, basé sur le modèle de Markov caché à états non-stationnaires est également décrit.
On fournit également quelques résultats de reconnaissance phonétique obtenus sur la base de données TIMIT.
La vérification des énoncés (UV) est le processus par lequel la sortie d'un système de reconnaissance est vérifiée pour déterminer si la parole émise inclut réellement le(s) mot(s)-clé.
La sortie de ce module de vérification est une décicion binaire d'acceptation ou de rejet de l'énoncé reconnu basée sur le taux de confiance de la vérification.
Dans cet article, nous étendons la notion de vérification de l'énoncé en présentant une méthode qui va être utilisée pour trois tâches : (1) détecter les séquences qui ne sont pas des mots-clé (fausses alarmes), (2) détecter les erreurs de substitution sur les mots-clé, et (3) corriger sélectivement les erreurs de substitution quand les N meilleures hypothèses de séquences sont disponibles.
La méthode de vérification d'énoncés que nous présentons ici emploie un ensemble de modèles de vérification spécifiques qui sont indépendants des modèles utilisés dans le processus de reconnaissance.
Les modèles de vérification sont entraı̂nés en utilisant une procédure d'apprentissage discriminante qui cherche à minimiser l'erreur de vérification en maximisant simultanément le rejet des non-mots-clé reconnus à tort tout en minimisant le rejet des mots-clé correctement reconnus.
La correction d'erreur est obtenue en ré-ordonnant les hypothèses produites par le système de reconnaissance N-best basé sur un score de confiance UV.
Dans le cadre des travaux sur la détection des pathologies laryngées par analyses acoustiques, plusieurs traits acoustiques ont été proposés.
L'évaluation de leur pouvoir discriminatif a été effectuée, soit par des méthodes statistiques classiques, soit par inspection visuelle.
A notre connaissance, la performance discriminative d'un ensemble d'attributs acoustiques a toujours été évalueée en se référent explicitement à un modèle de décision.
Par ailleurs, tout laisse supposer que les distributions soushacentes à l'espace des traits sont multimodales.
En vue d'évaluer quantitativement et sans référence à un modèle statistique, le pouvoir de séparation de ces traits, nous effectuons une analyse typologique sur un ensemble de six attributs.
Les calculs sont établis à partir du signal acoustique associé à la voyelle stable /a/, prononcée par 37 locuteurs normaux et 24 locuteurs dysphoniques.
Les résultats de l'analyse confirment les bonnes performances des indices de perturbation de la périodicité (en durée et en amplitude).
Par contre, les attributs propes au signal résiduel ne font preuve que d'un faible pouvoir de séparation.
Dans cet article, nous proposons un modèle hybride MLP/HMM dans lequel les vecteurs d'entrée sont transformés par des prédicteurs non-linéaires basés sur les perceptrons multi-couches (MLPs) affectés à chaqueé tat d'un modèle de Markov caché (HMM).
Les vecteurs d'erreur de prédiction sur les états sont modélisés par des mélanges de densités gaussiennes.
L'utilisation d'un modèle hybride est motivée par le besoin de modéliser les erreurs de prédiction résultant du modèle neuronal de prédiction classique (NPM) : celles-ci varient en fonction des différents contextes et de l'identité du locuteur.
Le modèle hybride MLP/HMM présente deux avantages : la corrélation entre les trames du signal d'entrée est exploitée par les prédicteurs MLP, et la variabilité de l'erreur de prédiction est explicitement modélisée.
Nous présentons les algorithmes d'apprentissage basés sur un critère de maximum de vraisemblance (ML) et sur un critère discriminant de minimisation de l'erreur de classification.
Des expériences de reconnaissance de parole continue en mode indépendant du locuteur onté té menées.
Avec un apprentissage ML du modèle hybride, nous avons obtenu des performances nettement supérieures á celles obtenues avec un NPM classique ne modélisant pas explicitement l'erreur de prédiction.
Avec un apprentissage utilisant le critère de discrimination, la confusion entre modèles différents était réduite de façon significative et le taux d'erreur sur les mots était réduit de 56% par rapport á celui obtenu avec un apprentissage ML.
Dans cet article, nous nous intéressons à la détection d'objets dans des scènes complexes, par des méthodes basées sur des modèles statistiques d'apparence globale.
L'approche proposée associe, dans un cadre bayésien, une représentation standard des images d'apprentissage par espace propre à des modèles de bruit et à des modèles a priori non gaussiens.
Ce modèle permet d'unifier les méthodes de détection classiques rencontrées dans la littérature et conduit, de façon naturelle, à la définition d'une nouvelle classe de détecteurs statistiques, intégrant des modèles de distribution quelconque pour les images d'apprentissage.
La comparaison des caractéristiques opérationnelles des récepteurs (courbes COR) sur des bases de données communes, illustre les contributions de l'approche bayésienne.
Elle montre également que l'adoption de modèles non gaussiens permet de dépasser significativement les performances des algorithmes faisant actuellement référence dans le domaine [2, 14].
Nous présentons un paradigme pour la reconnaissance de la parole basé sur un réseau d'activités d'analyse à profondeur variable.
Le paradigme produit des descriptions des propriétés du signal de parole qui sont liées aux unités phonétiques au travers de modéles de Markov.
Des résultats en reconnaisance indépendante du locuteur de chiffres et lettres isolés sont présentés.
Ce papier présente un travail sur la comparaison entre un modèle utilisateur et le comportement réel de l'utilisateur reposant sur trois prémisses :
1) tout système interagissant avec un utilisateur possède un modèle de celui-ci ;
2) la représentation externe des utilisateurs dépend de l'utilisation qui est faite du système par l'utilisateur ;
3) connaître le type d'utilisation du système dépend du contexte dans lequel la tâche doit être exécutée.
L'explicitation du contexte en vue de son utilisation conduit à utiliser les graphes contextuels pour capturer les comportements effectifs des utilisateurs dans une activité de recherche d'information sur un site web scientifique.
Cette approche permet de composer avec un système capable d'acquérir de manière incrémentale de nouvelles connaissances de l'utilisateur, et ainsi apprendre de nouvelles pratiques développées par les utilisateurs quand il est en échec.
Ainsi qui'il est bien connu, la plupart des mal-entendants perçoivent une quantité natable d'information verbale grâce à la lecture aux lèvres.
Notre article présente quelques résultats d'une analyse structurale des contours labiaux de plusieurs locuteurs ayant articulé différentes voyellesisolées.
Notre méthode est basée sur l'analyse de Fourier des fonctions de contours.
Une telle analyse est d'une grand intérêt par rapport à la génération artificielle èt à la reconnaissance en temps réel de schemes de parole visible.
Les théries psychologiques de traitement des langues naturelles ont habituellement supposé que le processeur de phrases résolvait les ambiguités syntaxiques locales en sélectionnant une seule analyse sur la base de critéres structurels comme le principe de l'“attachement minimal” de Frazier (1978).
D'après ces théories, les analyses alternatives seront seulement envisagées si l'analyse initiale se révèle être inconsistante avec le contexte. (voir aussi Ferreira & Clifton, 1986 ; Ford, Bresnan, & Kaplan, 1982 ; Rayner, Carlson, & Frazier, 1983).
Cependant, une autre hypothèse est possible : si les phrases sont comprises de façon progressive, plus ou moins mot à mot (Marlsen-Wilson 1973, 1975), alors le traitement syntaxique peut en principe exploiter le fait que les interprétations sont disponibles, et les utiliser de façon “interactive” pour sélectionner parmis les différentes analyses syntaxiques en fonction de leur plausibilité par rapport au contexte.
Cet article considère les architectures possibles pour de tels processeurs de phrases interactifs et progressifs, et argumente en faveur d'une architecture telle que les différentes analyses sont offertes en parallèle, et sont distinguées par un appel immédiat au processus de compréhension, selon une interaction sélective ou “faible”, par opposition à l'interaction directive ou “forte”.
Nous notons qu'une telle architecture ne compromet en aucune façon l'hypothése de modularité de Fodor (1983).
Nous faisons la revue les données expérimentales présentées comme suggérant que le système de traitement des phrases humain était non-interactif et reposait sur des critères purement structurels.
Nous présentons de nouveaux résultats qui semblent incompatibles avec la proposition structurelle, et qui soutiennent l'hypothèse interactive.
Nous suggérons des raisons qui permettent d'écarter les résultats contraires obtenus auparavant, et concluons que le mécanisme de traitement des phrases humain résoud les ambiguités de type modifieur-attachment en ayant recours á des informations contextuelles et réferentielles de plus haut niveau sous l'intéraction faible.
Ce papier commence par décrire les nouvelles directions d'applications aux télécommunications de la reconnaissance automatique de la parole et de la synthèse vocale à partir du texte au Japon.
Les applications de la reconnaissance automatique de la parole se focalisent sur les services publics tels que l'automatisation du travail des opérateurs, l'assistance aux opérateurs, la commande vocale des serveurs d'information, et la numérotation vocale.
Les applications majeures de la synthèse vocale incluent les services d'information par la voix et la lecture des messages électroniques (e-mail).
On estime que l'utilisation de la reconnaissance de la parole et de la synthèse vocale à partir du texte va fortement augmenter dans un avenir proche avec la pénétration des terminaux téléphoniques mobiles et des portables, en particulier dans des domaines comme la diffusion de textes et la communication numérique.
Deuxièmement, ce papier décrit le paramètrage expérimental du système vocal interactif de NTT qui comporte (1) une reconnaissance de la parole hautement performante en mode indépendant du locuteur et grand vocabulaire, basée sur une modélisation par HMM des phonèmes en contexte dont les paramètres sont appris sur des données parole provenant de plus de 10 000 locuteurs et collectées à travers le réseau téléphonique, (2) une synthèse de parole à partir du texte de haute qualité qui génère de la parole en concaténant des segments de signal représentant des triphones, (3) une configuration logicielle qui ne demande aucune architecture matérielle spécifique autre qu'un PC équipé d'une carte son et d'un modem vocal, (4) un prototypage facile et rapide qui permet à l'utilisateur de construire un système en écrivant certains types de scénarios du service.
Dans cet article, une classe générale de filtres adaptatifs non linéaires basés sur des réseaux de neurones artificiels (ANN) à simple couche cachée et unidirectionnels est proposée pour le traitement de signaux limités en fréquence dans une approche de rehaussement du signal basée sur une méthode multi-microphone et adaptative en sous-bandes.
Les premiers résultats comparatifs obtenus en utilisant des données automobiles réverbérantes réelles et simulées montrent que le système de rehaussement de la parole proposé utilisant le traitement ANN en sous-bande est capable de surpasser les méthodes plus conventionnelles d'annulation de bruit.
L'article est consacré au problème de la sélection d'indices pour la reconnaissance automatique de la parole.
La comparaison des indices a été effectuée sur base d'un modèle de reconnaissance de mots isolés par mise en correspondance avec des patrons de référence.
Les expériences ont été basées sur des chiffres en serbo-croate prononcés par 109 locuteurs.
Le taux de reconnaissance a été établi pour chaque ensemble d'indices.
Le taux et le type d'erreurs commises ont été mis en rapport avec le vocabulaire-test.
Les techniques de planification sous incertitudes sont difficiles à appliquer à des problèmes de robotique autonome : l'effort de modélisation est parfois pénible voire rédhibitoire lorsque l'espace d'états est très grand.
Les représentations factorisées, basées sur des variables d'état, sont plus compactes, mais elles gèrent mal les variables qui ont un grand nombre de valeurs possibles, comme par exemple les variables de localisation du robot.
Les problèmes de recherche et sauvetage de personnes en danger combinent ces variables et celles de mission.
Nous proposons un modèle abstrait hiérarchique pour simplifier la phase de modélisation des problèmes de planification en robotique sous incertitude des actions.
Un algorithme ins- tancie automatiquement notre modèle abstrait, que nous présentons et évaluons sur plusieurs instances de problèmes de recherche et sauvetage par un drone hélicoptère autonome.
Nous explorons dans cet article l'utilisation de moindres généralisés corrects comme apprenant dans des techniques de boosting (Freund et al., 1996).
Les premières expérimentations sur des problèmes classiques montrent qu'ADABOOST instancié avec un apprenant à base de moindres généralisés obtient des taux d'erreur plus faibles que C4.5, GLOBO (Torre, 1999) et ADABOOST muni d'un apprenant plus classique.
Cet article étudie la réalité psychologique des traits distinctifs.
Vu l'approche non-cognitive des linguistes et l'approche concrète des phonéticiens, il n'est pas du tout clair si la représentation psycholinguistique emprunte aux deux, à l'une des deux ou à aueune de ces disciplines.
Une divergence d'opinion entre phonéticiens et linguistes sur un cas permet d'éclaircir ce problème.
Alors que les premiers considèrent [f] et [v] comme des fricatives labio-dentaled, les derniers les traitent en tant que pronènems bilabauz au niveau sous-jacent.
Dans le but de vérifier si [f] et [v[se comportent comme des consonnes bilabiales ou labio-dentales du point de vue psychololinguistique, une analyse des interactions entre ces fricatives et les autres phonèmes dans deux corpus importants de lapsus anglais et allemands a été effectuée.
Les résultats inoiquent que [f] et [v] ne different pas des consonnes bilabiales, c'est-à-dire qu'ils se substituent aux autres consonnes autant que [p], [m] ou [w].
Trois hypothèses pourraient rendre compte de ce résultat.
Selon l'hypothèse de labialité et celle de bilabialité, il n'y a pas de noeud labio-dental dans le réseau mental.
L'alternative part de l'hypothèse que les traits distinctifs sont représentés par des vecteurs dans un système spatial de coordonnées.
Ce modèle comprent un vecteur pour la bilabialité ainsi qu'un autre pour la labio-dentalité.
Nous proposons que ces vecteurs soient plus rapprochés l'un de l'autre que ne le suggère la distance phonétique réelle.
Il y a moins de support pour l'hypothèse de vecteur que pour celle de bilabialité.
Les données empiriques mettent en évidence que la représentation des traits phonétiques ne correspond pas nécessairement à celle des traits psycholinguistiques.
Les techniques d'alignement temporel (DTW) et de Quantification Vectorielle (VQ) ont été appliquées avec un grand succès au problème de la vérification du locuteur.
Il est d'usage courant de les utiliser pour calculer une mesure unique de distance, avant de seuiller cette mesure pour prendre une décision de vérification.
Dans cet article, nous examinons l'application d'une pondération statistique à un certain nombre de paramètres calculés à partir du chemin d'alignement DTW et des mécanismes de décision de la VQ.
On présente des résultats qui montrent que l'extraction de ces paramètres supplémentaires permet de prendre en compte des informations complémentaires sur le locuteur et peuvent être utiles pour améliorer les performances des systèmes classiques de vérification.
On a également étudié l'effet d'une normalisation de la mesure de distance, ce qui revient à comparer les scores DTW et VQ entre le locuteur dont l'identité est à vérifier et les autres locuteurs.
Les résultats de la méthode de base et de ces améliorations sur les algorithmes DTW et VQ sont estimés sur une population de 42 locuteurs.
Cet article traite du problème de la modélisation de la parole pour la génération de parole sous stress en utilisant le cadre “source generator framework”.
D'une manière générale, le terme stress dans ce contexte se rapporte à la condition du locuteur qui peut être émotionnelle ou induite par une tâche spécifique.
Dans cette étude, l'accent est mis sur la parole criée ou produite dans des conditions de colère ou d'effet Lombard.
A l'origine, la théorie de génération de source a été développée pour l'égalisation de la parole sous stress afin d'effectuer une reconnaissance robuste (Hansen, 1993, 1994).
Elle a été ensuite utilisée pour la génération d'occurrences d'apprentissage avec un stress simulé (Bou-Ghazale, 1993 ; Bou-Ghazale and Hansen, 1994).
L'objectif de ce travail est de générer de la parole sous stress à partir de parole normale, en utilisant un “source generator framework” préalablement utilisé pour la reconnaissance de parole sous stress.
L'approche est basée sur (i) le développement d'un modèle mathématique représentant l'effet du stress sur la production de la parole, et (ii) l'utilisation de cette modélisation pour produire les effets du stress et des émotions sur des mots isolés prononcés en parole naturele.
L'algorithme de perturbation simulant les effets du stress est basé sur la structure de synthèse de parole par CELP (“code-excited linear prediction”).
Cet algorithme a été évalué sur quatre ensembles de paramètres exprimant la perturbation dans la parole.
Les évaluations effectuées dans cette étude montrent que le pitch est capable de refléter l'état émotionnel du locuteur, alors que l'information sur les formants n'est pas aussi bien corrélée au stress.
Néanmoins, dans le cadre d'une modélisation CELP de la parole, c'est la combinaison de la localisation des formants, du pitch et de l'énergie qui produit l'indicateur le plus fiable du stress émotionnel.
Les résultats des évaluations, basées sur l'écoute de parole sous stress générée par le système, montrent des taux de classification correcte de 87% pour la parole produite avec l'effet de la colère, 75% pour la parole avec l'effet Lombard et 92% pour la parole criée.
Il existe une importante littérature traitant du problème de la conception d'un quantificateur pour un système de détection ou de classification.
A l'origine, les travaux menés dans ce domaine - notamment par Kassam, Poor, Picinbono et Bucklew - ont pour but de concevoir un quantificateur qui optimise une règle de décision basée sur l'information quantifiée.
Rompant avec cette approche classique, ces dernières années ont vu l'émergence d'une approche alternative dont l'objectif est d'optimiser conjointement les opérations de quantification et de classification.
Dans cet article, nous proposons de comparer l'approche conjointe à l'approche classique, plus courante, de Picinbono et Duvaut.
Cet article présente MAS4AT, un système multi-agent coopératif et auto-adaptatif pour le déclenchement d'alertes lors de la détection de comportements suspects dans le cadre de la surveillance maritime.
Ce système est conçu et développé dans le cadre du projet européen I2C qui vise à mettre en oeuvre une nouvelle génération de systèmes de surveillance maritime, capables d'aider les opérateurs humains (i) à identifier les comportements anormaux de navires, (ii) à évaluer la suspicion associée à ces comportements et (iii) à déclencher des alertes s'ils représentent des menaces.
Cet article introduit le projet I2C puis se consacre plus particulièrement à la présentation de MAS4AT et à ses capacités d'apprentissage par renforcement.
Une série d'expériences a été réalisée pour déterminer comment la durée d'un mot prononcé dans une phrase est influencée 1) par la catégorie grammaticale à laquelle ce mot appartient et 2) par la position de ce mot dans un constituant.
Dans l'expérience I des homophones Nom-Verbes (ex., “I saw the coach” — Nom (J'ai un rôti) “I saw him coach” — Verbe (je l'ai rôti)) sont présentés dans des phrases dont on a appareillé le contexte phonétique et le schéma d'accentuation.
Les résultats indiquent que les noms sont plus longs que les verbes dans des phrases typiques.
Les résultats des expériences II et III appuient une interprétation liant l'allongement à la position terminale du constituant et élimine l'effet dû à un emplacement ou l'effacement serait possible.
Les résultats de l'expérience IV étendent l'interprétation donnée pour l'allongement du constituant final des Noms et Verbes à deux catégories supplémentaires s'appuyant sur la comparaison de la durée de l'adjectif du syntagme initial two (deux) et de l'adverbe du syntagme too (aussi).
Enfin l'expérience V teste la distinction entre catégories mineures et majeures.
Les résultats montrent que la préposition to (à) est à 50% plus courte que l'adjectif two (deux).
En prenant l'ensemble des résultats on voit qu'il suffit d'une distinction binaire entre les catégories majeures et mineures pour ce que demande une théorie de la mesure temporelle de la parole et de sa synthèse.
On peut rendre compte de durée traditionnellement attribuée aux différences entre les classes de catégories majeures en termes de frontières de constituants déjà requise dans une théorie qui rend compte de trois autres classes de phénomènes.
Le problème de l'extraction de la voix chantée dans des enregistrements musicaux monophoniques, c'est-à-dire la séparation voix / musique avec un seul capteur, est étudié.
Les approches utilisées sont basées sur des modèles statistiques a priori des deux sources (musique et voix), notamment sur des Modèles de Mélange de Gaussiennes (MMG).
Une méthode d'adaptation des modèles aux caractéristiques des sources mélangées est proposée, et une étude comparative des différents modèles et estimateurs est effectuée.
Les résultats montrent que l'adaptation du modèle de musique sur les parties non-vocales des chansons permet d'obtenir de bonnes performances dans un cadre réaliste.
Différents critères de sélection sont étudiés.
Le critère de la Fréquence d'occurrence choisit les k transcriptions les plus fréquentes dans l'ensemble des décodages phonétiques du mot, alors que le critère du Maximum de Vraisemblance choisit les k transcriptions les plus vraisemblables dans cet ensemble.
Avec les deux critres k est le même quelque soit le mot, et chacune des k transcriptions “décrit” toutes les occurrences du mot sans exception.
Ensuite, une procédure de partitionnement permettant de déterminer le nombre “optimal” de transcriptions pour chaque mot est développée et évaluée.
Cette procédure part du principe que dans l'ensemble de transcriptions sélectionnées, chaque transcription doit “décrire” une partie des prononciations du mot.
Le but est donc de trouver les “bonnes” transcriptions et d'associer chacune d'elles à un sous-ensemble de prononciations.
Deux algorithmes itératifs sont développés et évalués, et un compromis est recherché entre vraisemblance et nombre d'éléments de l'ensemble à déterminer.
Les expériences menées en reconnaissance indépendante du locuteur ont montré la supériorité du critère du Maximum de vraisemblance par rapport au critère de la Fréquence d'occurrence.
Cette étude propose une nouvelle méthode de prédiction, la Prédiction Linéaire Séparée (PLS), pour l'analyse spectrale de la parole.
La valeur x(n) prédite pour le signal est calculée à partir des p + 1 échantillons précédents, en privilégiant, parmi les échantillons précédents, celui qui est situé immédiatement avant l'échantillon x(n).
Les p échantillons x(n − 2) àx(n − (p + 1)) sont extrapolés linéairement à l'aide de x(n − 1) pour obtenir p nouvelles valeurs, qu'on utilise pour la prédiction.
L'optimisation des coefficients de filtrage est calculée au moyen du critère d'autocorrélation, comme dans la prédiction linéaire conventionnelle.
La Prédiction Linéaire Séparée donne un filtre à pôles seuls d'ordre p + 1 avec p inconnues dans les équations normales.
La précision de la Prédiction Linéaire Séparée a été comparée à celle de la prédiction linéaire conventionnelle, en analysant les voyelles produites par deux femmes et quatre hommes.
Les résultats montrent que la méthode que nous préconisons donne en général une modélisation plus précise des formants supérieurs, et des résiduels de plus faible énergie et plus plats.
Pendant un dialogue semi-spontané, les sujets étaient obligés de répéter la même correction d'un chiffre dans une série de trois chiffres qui se compose de “five” ou “nine” suivi de “Pine Street”.
Les signaux articulatoires et acoustiques de quatre parleurs de l'anglais américain général furent enregistrés par la X-ray Microbeam (la radiographie au micro-faisceau) de l'Université de Wisconsin.
En faisant l'analyse des movements de la mâchoire, les valeurs de magnitude et réglage syllabique étaient evalués pour inférer un enchaı̂nement linéaire des pouls syllabiques pour représanter l'organisation rythmique de la énonciation.
Les résultats préliminaires suggérent non seul que la magnitude de la syllabe corrigée augmente par la correction du chiffre, mais aussi qu'il y a en plusieurs cas quelque augmentation systématique de la magnitude de la syllabe, à la fois pour le chiffre corrigé et pour les autres chiffres de la même énonciation, quand la même correction est répétée.
La variation considérable parmi les parleurs est observée et discutée sou forme de la magnitude syllabique et les modèles de réglage.
La communauté scientifique de visualisation d'information possède des solutions pour la navigation au sein des bases de connaissances.
L'objet usuel de ces actions de visualisation et de navigation est celui des graphes.
L'objectif de notre démarche est de comparer des techniques d'affichage et de manipulation de graphes sur l'apport cognitif quelles procurent aux utilisateurs.
Nous avons développé un graphe (3 000 nœuds, 10 000 arêtes), portant sur des noms communs.
Cette structure est déclinée selon trois modes d'affichage : deux modes graphiques (coloré et monochrome), et un mode textuel (par hyperliens).
Ce matériel est proposé à des utilisateurs pour une étude expérimentale de leurs préférences et performances sur des tâches de navigation dirigée (recherche de chemins dans un graphe).
Si les utilisateurs préfèrent naviguer sur un affichage graphique coloré, leurs performances (en temps et actions), ne sont pas significativement meilleures dans ce mode.
Il en ressort qu 'une combinaison de deux modes d'affichage serait une solution intéressante : une représentation globale et graphique de la structure manipulée, couplée avec une représentation locale, textuelle et plus détaillée de la zone d'intérêt de cette même structure.
L'utilisation de la couleur en vision par ordinateur est un sujet de recherche qui suscite un intérêt croissant.
Ce papier fait le point dans ce domaine, en essayant de répondre aux questions : Qu'est-ce que la couleur ?
Quelles en sont les représentations adéquates ?
Comment la déterminer ?
Que peut-on en faire ?
Pour cela, nous faisons une revue approfondie et très à jour de l'ensemble de la littérature consacrée à ce sujet en cernant les axes de recherche et les problématiques importantes et en tentant de les évaluer.
Cet article présente un algorithme d'autocalibration d'une antenne multicapteur.
Cet algorithme permet d'estimer le gain et la phase des capteurs, dont la connaissance précise est nécessaire pour localiser les sources en présence.
L'originalité de notre approche réside dans la prise en compte de la non circularité des sources.
L'exploitation de cette caractéristique permet d'augmenter la dimension de l'espace des observations, et de localiser des sources en nombre supérieur à celui des capteurs.
Des simulations montrent que la performance de cet algorithme en terme de rapidité de convergence et de précision des estimations est satisfaisante.
Cet article présente une méthode de segmentation et de classification phonétique primaire de la parole continue.
Dans ce papier, nous présentons une approche générique pour le développement de moteurs de reconnaissance de symboles manuscrits en ligne.
Nous présentons en détail notre approche et faisons le lien avec d'une part les modèles de Markov hiérarchiques et d'autre part les réseaux bayésiens dynamiques.
Nous évaluons ensuite les propriétés fondamentales de notre approche qui lui confèrent une grande flexibilité.
Puis nous montrons que l'on peut, avec cette approche générique, concevoir aussi bien des systèmes omni-scripteur rivalisant avec les meilleurs systèmes actuels sur des caractères alphanumériques usuels, que des systèmes mono-scripteur pour des symboles graphiques quelconques, nécessitant très peu d'exemples d'apprentissage et peu gourmands en ressources machine.
Si les règles naturelles de la phonologie, comme par exemple celle qui efface une consonne finale devant une consonne initiale, sont fréquentes dans les langages non structurés, ce doit être parce que ces règles concernent les traits universaux de production et/ou de perception du langage.
La présente expérience a pour but de voir, à partir d'une tâche d'apprentissage, si des sujets na ïfs ont davantage tendance à utiliser une règle naturelle que la réciproque (effacement de la consonne devant une voyelle).
On doit ensuite combiner avec chacun des quatre noms trois adjectifs nouveaux, pour l'un des groupes de sujets suivant la règle naturelle, pour l'autre siuvant la règle non-naturelle.
Les sujets ayant appris le corpus non-naturel ont tendance à donner des réponses naturelles alors que la réciproque n'est pas vraie.
En conséquence ces sujets font, au cours de la tâche, beaucoup plus d'erreurs que les autres même si la régle opératoire leur est donnée au premier essai par une présentation systématique de chaque adjectif siuvi des quatre noms correspondants.
Même si un tel processus n'a pas un rôle signiticatif en anglais, il semble que nos sujets ont une connaissance implicite de la régle naturelle.
Ce papier porte sur la modélisation de séries temporelles ou de régression à l'aide de réseaux de neurones.
En nous appuyant sur des résultats récents sur l'estimation des moindres carrés pour les séries temporelles non linéaires, nous proposons une méthodologie complète et explicite pour l'estimation des paramètres (processus d'apprentissage) et pour le choix du modèle (sélection d'architecture).
En particulier, nous donnons une solution au problème de l'élagage dans un perceptron multicouches au moyen d'une méthode pas à pas utilisant un critère de type BIC dont on démontre la consistance.
Dans ce papier, nous présentons une approche de reconnaissance du locuteur basée sur une projection spécifique à chaque utilisateur.
Cette projection est réalisée au moyen d'un réseau de neurones multi-couches.
Le but de la projection est de capturer les informations spécifiques au locuteur en transformant un ensemble de paramètres représentant l'information linguistique en un ensemble de paramètres caractérisant l'information linguistique ainsi que l'information propre au locuteur.
Dans cette étude, les paramètres les plus appropriés pour faire cette tranformation sont également évalués.
On montre aussi que la normalisation des scores, ainsi que l'utilisation du critère d'erreur du réseau de neurone pour la sélection des vecteurs acoustiques, augmentent les performances du système.
Nous montrons également que le fait de laisser tomber les composantes haute fréquence du signal résulte en une déterioration des performances du système.
Sur un ensemble de 630 locuteurs de la base de données TIMIT, un égal taux d'erreur de 0.5% et 100% d'identification sont obtenus par l'approche proposée ici.
Sur un ensemble de 38 locuteurs de la région dialectale “dr1” de la base de données NTIMIT, un égal taux d'erreur de 6.6% est obtenu.
Le système utilise des diphones produits par un circuit intégré de synthétiseru à formant.
Une notation semi-phonétique pour l'entrée du message dans le système.
Des contours d'intonation utilisant une ligne de déclinaison et diverses montées et chutes sont générés à partir des signés à partir des signes de ponctuation et d'accentuation que comporte le message d'entrée.
Un appareil autonome, portable et de faible encombrement a été conçu.
Une rapide évaluation du système a été menée avec des utilisateurs potentiels.
Cette introduction passe en revue des développements récents dont la connaissance est indispensable pour comprendre la recherche actuelle sur l'acquisition de la lecture.
L'accent est mis sur les rapports entre l'étude de la performance adulte, celle des effets des lésions neurologiques et celle du développement de la lecture.
Le problème central de ces recherches est d'identifier les causes des difficultés spécifiques auxquelles semble se heurter l'acquisition de la lecture.
En général, ce problème a été abordé par des méthodes correlationnelles, fondées sur la comparaison des performances des bons et des mauvais lecteurs.
Les mérites et les défauts de cette approache sont discutés, et on insiste sur le besoin de rattacher les études différentielles à une conception théorique générale de processus de lecture et de son développement.
On examine le courant d'études issu de l'hypothèse selon laquelle une des difficultés majeures de l'acquisition de la lecture alphabétique réside dans la manipulation du language au niveau des segments phonémiques, et on discute la manière dont les résultats de ces études peuvent être reliés aux théories actuelles de l'accès lexical.
Les limites de l'approche qui consiste à tirer des hypothèses sur le développement à partir de théories relatives au stade adulte sont décrites et illustrées par des données sur la performance d'enfants normaux et dyslexiques.
Enfin, on discute la possibilité que les difficultés d'acquisition de la lecture trouvent leur source à des niveaux supérieurs à celui de la reconnaissance des mots.
Cet article présente une nouvelle approche de la reconnaissance des mots parlés.
Nous discutoon d'abord les exigences à satisfaire par les unités linguistiques appropriées, ensuite nous exposons quelques résultats obtenus lors de l'utilisation de superensembles de phones pour la reconnaissance de mots.
Finalement, nous développons sur la base d'indices phonétiques binaires des techniques de classification robuste, de segmentation et d'accès lexical.
Dans les conversations de personne à personne, les locuteurs adaptent continuellement les aspects prosodiques et structurels de leur parole aux besoins perçus de leurs auditeurs, tant en fonction de leur évaluation des effets de masquage potentiel dus aux bruits transitoires ou ambiants qu'en réponse à des requêtes explicites de répétition de la part de leurs auditeurs.
Des stratégies d'adaptation à la répétition mettent en oeuvre un changement de certaines caractéristiques prosodiques de la parole, tels que le registre et la moyenne de la hauteur de la voix, l'intensité moyenne et le tempo général de la parole, ainsi qu'une restructuration de l'intonation.
De telles répétitions font usage de stratégies de redémarrage basées sur des connaissances structurelles linguistiques.
Les réseaux bayésiens sont des outils privilégiés pour les problèmes de diagnostic.
Nous dressons dans cet article un panorama des algorithmes utilisés classiquement pour la mise en œuvre des réseaux bayésiens dans le cadre du diagnostic, et plus particulièrement du diagnostic médical.
Pour cela, nous passons en revue un certain nombre de questions méthodologiques concernant le choix de la représentation des densités de probabilité (faut-il discrétiser les variables continues ? utiliser un modèle gaussien ?) et surtout la détermination de la structure du réseau bayésien (faut-il utiliser un réseau naïf ou essayer d'apprendre une meilleure structure à l'aide d'un expert ou de données ?).
Une étude de cas concernant le diagnostic de cancer de la thyroïde nous permettra d'illustrer une partie de ces interrogations et des solutions proposées.
La tendance récente à utiliser des méthodes basées sur des harmoniques/sinusoïdes exploitant la structure fine de la parole voisée est évidente.
Cet article discute le stade actuel des connaissances dans ce domaine sous le point de vue des méthodes d'analyse-synthèse et aussi de applications au codage.
Ses aspects principaux en sont :
• - Le modèle harmonique est un outil très efficace pour les régions voisées : il produit de la parole synthétique de très haute qualité mais il est aussi perméable aux erreurs de pitch et de décision voisé-non-voisé.
Le plus grand désavantage du codage harmonique provient de ce qu'il nécessite une méthode alternative pour traiter les régions non voisées.
ATC est l'option naturelle.
Nous présentons une simulation à 8 kbit/s, qui utilise une commutation rigide entre codage harmonique et ATC.
• - Le modèle par sinusoïdes, en levant la restriction sur la relation harmonique entre sinusoïdes, étend le cadre d'analyse-problèmes aux régions non voisées et de transition.
En ce qui concerne le codage il y a, néanmoins, encore beaucoup de problèmes à résoudre.
Quelques indications pour une recherche future sont discutées.
Le travail décrit dans cet article a été réalisé au sein du projet SPEAK ! (génération de la parole dans les systèmes informatiques multimodaux).
Le projet avait pour but l'amélioration de la qualité de la parole synthétique devant être utilisée, dans des systèmes de dialogue homme-machine, comme un module additionnel d'interface multimodale.
L'analyse de l'interaction du texte et du dialogue en allemand (partie théorique de cette recherche) a permis d'établir la prédiction du groupe tonal (TG), des frontières du syntagme à l'intérieur de la phrase et de la place du focus dans le syntagme.
Les groupes tonaux représentent la structure générale de l'intonation du syntagme lorsque le niveau de l'intonation du mot n'est pas pris en compte.
Les résultats de cette étude sont les marqueurs de l'intonation décrits dans (Teich et al., 1997).
Le synthétiseur CTS construit les principaux patrons intonatifs à partir du texte étiqueté par ces marqueurs additionnels.
Cet article décrit, pour le système MULTIVOX-SPEAK !, les résultats des travaux sur l'intonation de l'allemand et l'établissement des règles intonatives ainsi que sur l'organisation temporelle et de la génération des pauses (aux niveaux segmentai et suprasegmental), importantes pour le rythme.
Nous présentons également les règles détaillées ainsi qu'un nouveau module de génération de la prosodie (basé sur le groupe tonal) qui ont été intégrés au système MULTIVOX TTS.
Les résultats préliminaires de l'évaluation sont présentés.
Le système de règles est construit au fur et à mesure en tenant compte des constraintes phonotactiques implicites de l'italien.
Le système est organisé conformément au modèle mathématique des automates finis, qui a été généralisé et complété afin d'obtenir un schéma simple de traduction syntaxiquement orienté.
Aucune information supplémentaire relative à leur appartenance à une classe grammaticale n'est donnée.
Enfin, nous discutons les avantages et désavantages d'une approche faisant appel à la théorie mathématique des graphes ainsi que les développements futurs possibles.
On décrit un algorithme qui transforme des paramètres acoustiques du signal de parole en éléments abstraits permettant de transcrire automatiquement l'accent de phrase et les mouvements intonatifs.
Les paramètres acoustiques utilisés sont la durée, l'énergie et la fréquence fondamentale.
Les traits abstraits qui en sont dérivés visent à isoler, au sein de ces paramètres, les variations prosodiquement imposées.
On présente une méthode de syllabification à partir des paramètres acoustiques.
La prominence de chaque syllabe est déterminée de façon automatique et la transcription résultante est comparée à une transcription manuelle.
Le taux de concordance de 61.6% suggère que l'étiqueteur humain utilise probablement d'autres paramètres que ceux pris en compte par l'algorithme.
La performance des systèmes de reconnaissance se trouve dégradée de manière significative en présence de bruit.
Pour résoudre ce problème lié au bruit, il est nécessaire de reconsidèrer les approaches classiques en prenant en compte cette nouvelle contrainte.
Nous envisageons, en premier lieu, deux représentations cepstrales (paramétriques et nonparamétrique) du signal de parole et proposons une vision unifiée de ces deux schémas.
Nous introduisons un domaine de pseudo-autocorrelation qui peut être interprété comme un domaine “Root cepstral”, et nous montrons comment l'analyse cepstrale non-paramétrique et l'analyse par prédiction linéaire convergent vers la même solution optimale.
Les expériences sont developpées, utilisant un système de reconnaissance de mots isolés mono- et multilocuteur dans un environnement de bruit voiture.
Bien que la langue tamoule possède une longue tradition de description « grammaticale » , qui remonte à la première moitié du premier millénaire de notre ère, avec une acception large de « grammatical » , qui inclut la poétique et la métrique, dans un champ où sont présentes la phonétique, la morphologie et la syntaxe, cette langue a été décrite, à nouveaux frais, et dans une nouvelle perspective (qui incluait l'enseignement de la langue courante), à partir du xvi e siècle, par des missionnaires chrétiens, qui apportaient avec eux un modèle latin de description grammaticale, qu'ils tentaient d'adapter, de façon créative, à une réalité linguistique qui était pour eux nouvelle.
Pour cette raison, il sera ici fait référence au corpus de leurs productions comme étant celui des Grammatici Tamulici, bien que les premiers d'entre eux aient utilisé le Portugais comme métalangue pour décrire le tamoul.
Ne se trouvant pas dans la situation de certains linguistes de terrain, confrontés à une langue qui n'a jamais été écrite, ces missionnaires se rendirent progressivement compte du fait que le syllabaire tamoul était un outil beaucoup plus efficace pour noter les sons et les mots tamouls que l'alphabet latin, même enrichi par les extensions développées pour la notation de langues européennes diverses.
Ils découvrirent aussi que leur capacité de convaincre et de convertir dépendait en grande partie de leur adoption des hiérarchies langagières qui gouvernaient (et gouvernent toujours) la diglossie tamoule, comme nous le verrons dans cette exploration préliminaire, qui couvre cinq auteurs, actifs pendant une période qui va jusqu'à 1739.
Différentes règles du type plus proche voisin sont d'usage courant dans les applications des méthodes de reconnaissance de formes.
Dans cet article, des variantes de ces algorithmes sont utilisées pour reconnaître des mots isolés indépendants du locuteur afin de sélectionner les mots-références.
On montre que cette approache améliore le taux de reconnaissance par rapport à celui obtenu par une analyse typologique, avec désavantage cependant d'ètre plus coûteuse.
Dans cet article, un nouveau système pour la segmentation et l'étiquetage automatique de la parole est présenté.
Notre système est capable de transcrire des paroles d'origines diverses sans demander une connaissance linguistique extensive ou une base de données démesurée (segmentée et étiquetée manuellement).
A cause de la taille limitée des réseaux neuronaux, la segmentation et l'étiquetage requièrent bien évidemment une limitation des temps de calcul et une capacité d'adaptation rapide á des nouvelles tâches.
Le système est évalué en utilisant cinq corpus de mots isolés, élaborés pour le développement de systèmes TTS néerlandais, français, anglais-américain, espagnol et coréen.
Les résultats montrent que la précision de notre système est comparable à celle d'experts humains.
Pour obtenir des résultats concernant la segmentation et l'étiquetage, qui soient comparables avec des résultats publiés dans la littérature, des tests additionnels onté té effectués sur TIMIT et les parties anglaise, danoise et italienne de EUROM0.
La performance de notre systéme se compare favorablement avec celles des autres systèmes automatiques.
Les fourmis présentent un intérêt grandissant pour la classification automatique vu la richesse et la diversité de leurs comportements.
La plupart des méthodes proposées à cet effet étendent l'algorithme de base de Lumer et Faieta (Lumer et al., 1994) s'inspirant du tri du couvain chez les fourmis.
D'autres propriétés biologiques des fourmis réelles et notamment les formes de communication qu'elles utilisent sont des sources d'inspiration intéressantes pour le partitionnement des données.
Dans ce travail, nous proposons un nouvel algorithme de classification automatique par des fourmis artificielles.
Après avoir fourni quelques indications sur la fortune de la Commedia de Dante en France autour des XVeet XVIesiècles, cet article se concentre sur la traduction anonyme de l'Enfer, en alexandrins et en rime tierce, conservée actuellement à la Biblioteca Nazionale Universitaria de Turin (ms L. III. 17).
Celle-ci est à présent considérée comme la plus ancienne traduction française de la Commedia,
mais la transcription fournie à la fin du XIXesiècle n'est pas fondée sur des critères philologiques solides et nécessite d'être remplacée par une édition critique scientifiquement fiable;
quelques questions préliminaires sont abordées dans les pages qui suivent, après avoir rapidement passé en revue les études précédentes.
Premièrement, cet article fournit quelques éléments, à la fois textuels et iconographiques, susceptibles de contribuer à dater le manuscrit ainsi qu'à identifier la source du texte italien transcrit en regard dans le codex turinois, vraisemblablement utilisé comme base pour la traduction.
Dans la deuxième partie, il sera question d'abord de la possibilité de déterminer l'origine du traducteur à partir de quelques éléments lexicaux, et ensuite de l'analyse de quelques-unes des stratégies de traduction utilisées, qui pourra servir de base pour des études ultérieures sur les aspects littéraires de cet ouvrage.
Un nouveau codeur à faible délai est proposé.
Il fait appel à une quantification vectorielle du filtre de synthèse dépendante de la mémoire.
Les échantillons transformés sont considérés comme une séquence de variables aléatoires de Laplace quantifiées vectoriellement de façon efficace en utilisant un quantificateur á réseau gémétrique.
Le codeur a été testé à un débit de 8625 bit/s et avec un délai de 8 ms.
On étudie ici le contrôle de la fréquence fondamentale (F0) en utilisant à la fois un modèle stochastique de séries temporelles et une analyse de système basée sur un modèle vectoriel auto-régressif (VAR).
On utilise des séries de données sur F0 bi-dimensionnelles (ff0 et sf0), obtenues par un système de retour auditif transformé, développé par l'un des auteurs.
Les valeurs sf0 sont extraites de données de parole qui contiennent une prononciation prolongée de la voyelle /a/, et le signal ff0 est extrait de la parole modulée en amplitude par du bruit blanc gaussien.
La plupart des données ont des caractéristiques moyennes non-stationnaires.
La procédure stochastique décompose les composantes non-stationnaires et les autres en une seule étape, sans prétraitement des données.
Les composantes cycliques qui entourent les composantes moyennes non-stationnaires sont supposées être générées par le modèle VAR.
Une analyse stochastique du système, utilisant les estimations du modèle VAR nous permet d'analyser les caractéristiques physiques des données.
Une étude de simulation, utilisant les estimations fournies par le modèle, a également été menée pour découvrir le rôle du contrôle de F0 dans des situations où les capacités auditives sont totalement perdues.
Les résultats montrent clairement quelles sont, par segments, les propriétés dynamiques du contrôle de F0 qui apparaissent avec chaque respiration prise durant l'émission d'un son soutenu.
On a montré précédemment que les 'spoonerismes' du type comme “barn door → darn bore” peuvent être provoqués chez les sujets en faisant précéder le stimulus cible à articuler (barn door) d'un item biaisé contenant au moins le phonème initial (d) de l'erreur attendue.
Etant donné que certaines caractéristiques linguistiques de l'erreur sont différentes de celles du stimulus, on peut montrer que les variables qui affectent systématiquement ce résultat seul, sont induits par des processus préarticulatoires, indépendants des propriétés perceptives du stimulus-cible lui-même.
L'étude présentée montre que la fréquence de base des erreurs provoquées par la technique de biais phonétique peut augmenter de manière dramatique lorsqu'on ajoute aux paires de mots précédant le stimulus-cible quelques items sémantiquement synonymes de l'erreur attendue.
De cette manière, on démontre rigoureusement que le biais sémantique qui constitue l'une des propriétés du lapsus dit 'Freudien', accroit remarquablement son apparition.
On discute ensuite les implications de ce phénomène.
L'étude présentée ici porte sur la modélisation des émotions extrêmes manifestées dans des situations anormales.
L'application visée est la sécurité civile et plus précisément la surveillance dans les lieux publics.
Un corpus fiction (le corpus SAFE) montrant des contextes riches et variés avec la présence d'émotions extrêmes, principalement de peur, a été sélectionné.
Une stratégie d'annotation adaptée à l'application est ensuite développée : elle incorpore à la fois des descripteurs génériques et spécifiques.
Enfin, un système de détection des émotions de type peur basé sur des indices acoustiques est mis en place à titre d'évaluation.
D'une part, le système s'avère robuste aux changements de source contextuelle.
D'autre part, l'évaluation de l'influence du choix d'un support multimodal à l'annotation sur les performances du système est mineure.
Les performances issues des différents protocoles d'évaluation mis en œuvre restent inchangées : la classe peur est reconnue à 67 %.
Cette étude traite de l'effet de l'accent de phrase, de l'accent de mot et des classes de mots (mots fonctionnels versus mots de contenu) sur les propriétés acoustiques de 9 voyelles hollandaises énoncées en parole continue.
Une liste de phrases a été lue par 15 locuteurs masculins.
Nous nous sommes seulement intéressés à une syllabe dans chaque phrase.
Soit un mot fonctionnel monosyllabique, soit une syllabe inaccentuée ou une syllabe accentuée dans un mot de contenu.
Au total 3465 voyelles ont été segmentées et analysées.
Les résultats ont montré que les facteurs linguistiques ont tous les trois eu un effet distinct sur la durée et les fréquences formantiques stables (F 1 and F 2) des voyelles.
L'accent de mot et la classe de mot ont eu un effet plus fort que l'accent de phrase sur la qualité spectrale des voyelles.
Une expérience d'écoute a démontré la signification perceptive des mesures acoustiques.
Il est apparu que la réduction spectrale des voyelles pouvait être mieux interprétée comme le résultat d'une assimilation contextuelle accrue que comme une tendance à la centralisation.
Nous nous sommes également penchés sur les changements dynamiques dans les traces formantiques causées par les conditions expérimentales.
Il est apparu que les traces formantiques des voyelles réduites devenaient plus planes, ce qui soutient l'idée d'une assimilation accrue.
Trois modèles de réduction vocalique sont discutés.
Nous présentons un réseau multicouches à connexion partielle (Partial Connection Multilayered Network, PCMN), fondé sur une technique de connexions partielles et superposées entre les couches.
L'apprentissage de ce réseau peut se faire automatiquement par l'algorithme de rétro-propagation du gradient (gradient back-propagation algorithm, GBP).
Un réseau général fondé sur GBP a été implanté sur une configuration d' anneau de la machine Hypercube F.P.S. T20.
Une parallélisation efficace de l'algorithme a été assurée.
Cette implantation a Fourni un éventail de possibilités de configuration du réseau.
Dans notre expérience, le réseau a été utilisé pour la reconnaissance de mots isolés.
Les résultats montrent les advantages de la technique de connexions partielles par rapport à la connexion compléte.
Cette technique permet de traiter efficacement des informations temporelles qui sont trés importantes pour le traitement de la parole, à la différence du traitement d'images.
Cette expérience permet aussi de mieux comprende descaracteristiques de réseau dans l'application à la parole.
La connexion partielle permet d'introduire des contraintes de contete temporel et des connaissances implicites dans le réseau et peut aussi permettre un apprentissage efficace avec une petite base de données.
Un résultat satisfaisant de reconnaissance a été obtenu.
La déconvolution de tels signaux est un problème de détection-estimation, ce qui exclut un traitement purement linéaire des données.
Les formes ARMA conduisent a un problème non standard de détection-estimation d'un bruit d'état dont la résolution est complexe et coûteuse en temps calcul.
Les formes AR et l'utilisation des techniques de codage multi-impulsionnel ne permettent pas de modéliser les systèmes à phase non minimale, et présentent les inconvénients des méthodes du type « erreur de sortie » .
De plus, aucune de ces approches n'autorise un traitement en ligne des données.
De plus, cette procédure peut être mise en œuvre sous forme rapide à l'aide d'équations de Chandrasekhar modifiées.
Les résultats obtenus sur données synthétiques sont satisfaisants, et ne nécessitent qu'un volume de calcul très inférieur aux méthodes proposées jusqu'ici.
Cet article décrit un système de vision temps réel permettant de localiser des visages dans des séquences vidéo ainsi que de reconnaître leur identité.
Ces processus sont effectués en combinant des techniques de traitements d'images et des méthodes de réseaux de neurones.
La robustesse du système a été évaluée quantitativement sur un corpus de 8 séquences vidéo.
Dans le but de comparer les performances avec les autres méthodes existantes, nous avons également testé notre modèle en utilisant la banque de visages standard ORL.
Le système a aussi été implanté sur deux architectures électroniques à base de composants spécialisés ZISC et de FPGA.
Nous analysons la complexité de l'algorithme et nous présentons les résultats des implantations architecturales en termes de ressources matérielles et de vitesse de traitement.
L'une des capacités humaines les plus développées est la communication par la parole.
Au cours des années, la recherche sur la perception de la parole a démontré que les humains sont bien adaptés á l'extraction d'informations linguistiques hautement codées á partir d'un signal vocal.
La nature sophistiquée de ces capacités et leur apparence précoce au cours du développment suggérent l'existence d'un riche substrat biologique permettant la perception de la parole.
Dans le présent article, nous décrivons, quelques unes de ces importantes capacités et examinons des recherches dans différents domaines pouvant aider á éclaircir la nature de leurs fondements biologiques.
Récemment, de nombreux algorithmes de minimisation de fonctions non convexes ont été proposés pour résoudre des problèmes de vision bas niveau.
Il existe plusieurs méthodes de relaxation.
Les techniques stochastiques, telles que le recuit simulé, convergent asymptotiquement, sous certaines conditions, vers le minimum global, mais sont très coûteuses en temps de calcul.
Les méthodes de relaxation déterministes sont sous-optimales, mais donnent de bons résultats et sont plus rapides que les méthodes stochastiques.
Dans cet article, nous présentons la mise en œuvre parallèle de deux algorithmes déterministes de détection de contours et de lissage d'image : le GNC ( « Graduated Non-Convexity » ) proposé par Blake & Zisserman et le recuit par champs moyens (MFA) introduit par Geiger & Girosi et étendu aux champs de Markov composés anisotropes par Zerubia & Chellappa.
Ces deux méthodes sont fondées sur le modèle de la membrane à contraintes de continuité lâches et sont séquentielles : à chaque pas est produit une image qui est utilisée au pas suivant.
Pour le GNC, nous avons utilisé une méthode de minimisation de l'énergie appelée « successive over-relaxation (SOR) » et plus précisément une variante parallèle de cette technique.
En ce qui concerne l'algorithme MFA, nous avons utilisé une méthode de descente de gradient conjugué à pas optimal.
La capacité compensatoire du système de contrôle articulatoire a été examinée chez des patients laryngectomisés.
Des mesures radiographigues et acoustiques ont été effectuées sur trois personnes avant et après l'opération.
Deux semaines après l'opération, les fréquences des formants des voyelles russes /a, u, i/ prononcées par ces patients étaient plus proches de la norme phonétique qu'avant, et deux ans après, deux patients prononçaient ces voyelles avec des paramètres phonétiques presque normaux.
On a mesuré des caractéristiques acoustiques pour 14 patients après l'opération.
1 à 2 ans après l'opération, 4 patients étaient capables de faire des distinctions voisé/non voisé.
Un patient a retrouvé le contrôle entiér de sa source vocale.
Les résultats obtenus peuvent signifier que le processus d'adaptation du système de contrôle articulatoire a des conditions dégradées de génération de la voix peut dépendre non seulement des caractéristiques acoustiques (comme les fréquences des formants) mais aussi d'élément phonétique aussi complexe que l'indice de voisement de la sonorité.
Le système de contrôle a montré sa capacité de réorganisation de l'activité des muscles, participant à l'articulation, ainsi que de transmission des fonctions des muscles laryngaux éliminés, aux muscles qui n'ont jamais été utilisés pour le contrôle de la voix.
On discute de l'importance de ces phénomènes par rapport au concept de modèle interne.
Les systèmes de classeurs sont des systèmes à base de règles de production qui construisent leur ensemble de règles de façon automatique.
Initialement, ces systèmes visaient à modéliser l'émergence de capacités cognitives à l'aide de mécanismes adaptatifs, en particulier évolutionnistes.
Suite à un renouveau de la problématique mettant davantage l'accent sur l'apprentissage, les systèmes de classeurs ont été vus ensuite comme des outils capables de traiter des problèmes de décision séquentielle de façon compacte, en représentant l'état comme composé d'observables différenciées qu'un agent peut choisir de prendre en compte ou non.
Enfin, beaucoup plus récemment, les systèmes de classeurs se sont avérés très efficaces pour résoudre des problèmes de classification automatique, ce qui dynamise le champ de recherche correspondant.
Dans ce contexte, l'objet de cette contribution est de présenter l'état de la recherche sur les systèmes de classeurs en insistant sur les développements les plus récents, en insistant sur la décision séquentielle plutôt que sur la classification automatique.
La plupart des approches utilisées pour la caractérisation des sédiments marins est fondée sur l'utilisation des méthodes d'analyse de la texture.
En effet, les images sonar présentent différentes zones homogènes de sédiments qu'on peut considérer comme des entités de texture.
En général, les paramètres texturaux extraits sont nombreux et ne sont pas tous pertinents, une extraction et/ou réduction de ces paramètres parait nécessaire avant l'étape de la classification.
Nous présentons dans cet article une chaîne complète de classification des images sonar en essayant d'optimiser les différentes étapes de cette chaîne.
Pour l'élaboration de cette chaîne, nous nous fondons sur le processus d'extraction de connaissance à partir de données.
L'environnement sous-marin a un caractère incertain, ce qui se reflète sur les images obtenues à partir des capteurs utilisés pour leur élaboration.
Il est donc important de développer des méthodes robustes afin de lutter contre ces imperfections.
Dans ce cadre, nous résolvons ce problème de deux façons différentes en utilisant dans un premier temps des méthodes de classification classiques comme les machines à vecteurs de support ou les A"-plus proches voisins et dans un deuxième temps des méthodes de classification floues ou crédibilistes.
L'approche de la régression par SVM que nous avons introduite permet une modélisation des imperfections des données.
Nous présentons alors les résultats obtenus en utilisant différentes approches pour l'analyse de la texture et pour la classification.
Nous utilisons des approches fondées sur les théories de l'incertain pour pallier au problème des imperfections présentes sur les images sonar.
En Amérique du Nord, les gens appellent le service d'assistance annuaire pour obtenir le numéro de téléphone d'une entreprise ou d'une résidence.
L'infrastructure et la maintenance du service d'assistance annuaire nécessitent une dépense importante pour les compagnies de téléphone.
Une automatisation complète ou partielle du service d'assistance réduirait substantiellement les coûts des compagnies de téléphone.
Nortel Networks a un produit appelé 'Automated Directory Assistance System (ADAS) Plus' qui automatise partiellement le service d'assistance à l'aide d'une technologie de reconnaissance vocale automatique.
Ce système a été déployé au travers tout le Québec, ainsi que pratiquement toute la région couverte par US West et BellSouth.
ADAS Plus automatise principalement la réponse à la question “pour quelle ville ?” à l'aide de la reconnaissance vocale.
Nous fournissons ici les détailles concernant ce système de reconnaissance vocale ainsi que les performances de ce système dans les régions déployées.
Le travail coopératif se réalise à travers des interactions communicatives qui permettent d'atteindre un certain niveau de compréhension mutuelle pour coordonner les actions ou aboutir à des décisions négociées.
La compréhension mutuelle est complexe de par l'hétérogénéité des participants qui entraine l'imprédictabilité et l'incertitude des interprétations.
Nous posons ici que la coopération chronique est encore plus complexe parce que le souvenir des interactions coopératives précédentes est une source supplémentaire de différence entre les contextes cognitifs des participants.
Nous avons étudié le souvenir d'interactions coopératives dans quatre situations collaboratives différentes.
L'analyse indique un oubli massif du contenu verbal et un rappel plus aisé des positionnements relationnels, des structures interactionnelles et des émotions.
Dans ce papier, un aperçu d'une approche statistique de reconnaissance de la parole est présentée dans laquelle less sources de connaissances phonétiques et phonologiques, extraites à partir des connaissances actuelles du système de communication humain, sont intégrées dans la structure d'un modèle stochastique de la parole.
Un formalisme statistique est présenté dans lequel les sous-modèles du processus phonologique basé sur des caractéristiques discrétes, ainsi que le processus dynamique phonétique du système de production humain, interagissent.
Leur interface permet l'optimisation globale d'un ensemble de paramètres qui caractérisent de façon précise les composants symboliques, dynamiques et statiques de la production de la parole, et séparent de façon explicite les sources distinctes de la variabilité de la parole observable au niveau acoustique.
Le formalisme est fondé sur une base mathématique rigoureuse, faisant intervenir la phonologie informatique, l'analyse bayesienne etla théorie de l'estimation statistique, les séries temporelles non stationnaires et la théorie des systèmes dynamiques, et finalement la théore de l'approximation de fonctions nonlinéaires (par réseaux de neurones).
Deux méthodes principales de mise en oeurve du modèle et du reconnaisseur de parole sont présentées.
La première méthode est basée sur le modèle de Markov caché dirigé, ou du modèle de trajectoire définie de façon explicite, alors que la deuxième approche est basée sur des unités phonologiques définies récursivement ou basées sur l'espace des états.
La continuité et la structure paramétrique du modèle dynamique ainsi défini permetune caractérisation jointe des variations contextuelles et du style de parole qui se manifestent dans le signal acoustique, offrant ainsi la possibilité d'éviter certaines des limitations de la technologie de reconnaissance de la parole actuelle.
Le manque de modèles conceptuels dans la conception de systèmes attentifs au contexte limite sérieusement le développement de systèmes plus généraux et plus complexes.
De plus, on ne connaît pas clairement quelles sont les conséquences des décisions précoces dans la conception pour la qualité de l'implémentation finale, ce qui rend la conception difficile, conduisant à des erreurs qui ne seront perçues qu'une fois le système implémenté et utilisé.
Dans ce papier, nous présentons une classification des aspects architecturaux du développement de systèmes attentifs au contexte.
Nous donnons par ailleurs un cadre de qualité qui décrit les conséquences des aspects architecturaux sur la qualité de tels systèmes.
Nous montrons finalement l'utilité de ce cadre de qualité par une modélisation de l'architecture d'un système attentif au contexte.
Dans cet article, nous étudions le problème de la segmentation automatique du signal de parole enregistré dans un environnement bruité.
Des techniques de rehaussement de la parole et de compensation de paramètres basées sur les modèles de Markov cachés (HMM), et récemment proposées pour la reconnaissance robuste de la parole, sont évaluées et comparées afin d'améliorer la segmentation automatique dans le cas de bruit coloré.
Les techniques de rehaussement de la parole considérées ici sont : la Soustraction Spectrale Généralisée, la Soustraction Spectral Non Lineaire, le Rehaussement MMSE d'Ephraim–Malah, et le Filtrage Itératif de Wiener avec Contrainte Auto-LSP.
De plus, la technique de Combinaison Parallèle de Modèles (PMC) est également comparée dans le cas de la compensation de bruit additif.
Pour les applications téléphoniques, nous comparons les techniques de normalisation du canal de transmission, Normalisation de la Moyenne Cepstrale, annulation du biais du signal (SBR), et considérons une méthode de couplage de compensation du canal de transmission avec le rehaussement de la parole dans l'étage de pré-traitement afin d'améliorer la segmentation automatique.
La qualité de la segmentation résultante est évaluée pour chaque méthode de compensation sur base des données TIMIT dégradées par du bruit additif coloré (à savoir, poste de pilotage d'un avion, autoroute, etc.), les données NTIMIT de parole transmise sur ligne téléphonique, et finalement CTIMIT correspondant à la transmission à partir de téléphones cellulaires.
Nous présentons des résultats provenant de l'application de l'algorithme “Approximating and Eliminating Search Algorithm” (aesa) à des données multi-locuteurs.
Des résultats antérieurs mono-locuteurs avaient montré que la performance du aesa était peu sensible à des accroissements de la taille du dictionnaire, tandis que une performance d'autant meilleure est observée que les paroles pronunciées sont proches de leurs prototypes correspondants.
Nous montrons dans cet article qu'à la fois le taux d'erreur et le nombre de calculs de distance diminuent lorsque le nombre d'échantillons est augmenté dans un dictionnaire à plusieurs entrées par mot.
Les algorithmes d'apprentissage pour la reconnaissance de la parole continue ont besoin de très grandes quantités de données sous forme de parole transcrite.
Les livres sur cassette, disponibles commercialement, représentent une source de telles données, abondante mais difficile à exploiter avec les algorithmes d'apprentissage actuels, car ceux-ci exigent que les données soient d'abord segmentées, à la main, en blocs assez petits pour être traités en mémoire.
Pour résoudre ce problème, nous avons mis au point un algorithme d'apprentissage capable de traiter des fichiers de données de longueur arbitraire ; les besoins en calculs de cet algorithme sont linéairement proportionnels à la longueur des données et la quantité de mémoire requise est constante.
Cet article passe en revue les concepts fondamentaux de l'analyse spectrale par Prédiction linéaire (LP) et par Entropie Maximale (ME), et dégage les raisons de leur importance pratique dans l'univers des signaux réels.
Le principe puissant du Minimum d'Inter-Entropie (MCE) est ensuite introduit.
Il permet d'incorporer de l'information antérieure au processus d'analyse d'un signal.
Dans une nouvelle approche de l'analyse du signal de parole, l'application du principle MCE permet de réduire le nombre moyen de coefficients prédicteurs (pôles) à spécifier par tranche temporelle pour une résolution spectrale donnée, en se basant sur une information spectrale préalable.
Celle-ci peut être fournie par les caractéristiques de la source glottique et du rayonnement aux lévres, par les réponses fréquencielles du microphone et de la chaîne de transmission et par de l'information spectrale de segments temporels antérieurs, en particulier dans les régions stables ou lentement variables d'une séquence verbale.
Ce travail met surtout l'accent sur les principes généraux plutôt que sur les détails du calcul.
Cet article présente une étude comparative entre quatre dispositifs de scolarisation des élèves allophones au collège en France.
Notre problématique consiste à interroger les éléments facilitateurs de la réussite des élèves allophones parmi lesquels figure la question des modalités d'accueil et de leurs effets sur les apprentissages des élèves.
Nous analyserons leur prise en charge au sein des dispositifs cibles à partir de deux focales : l'intérêt intrinsèque de chacun des dispositifs et leur articulation avec « la classe ordinaire » .
Les conclusions de cette recherche qualitative qui consiste en des entretiens semi-directifs menés auprès d'élèves allophones et d'enseignants tendent à soutenir que le seul véritable parcours personnalisé et adapté aux apprenants allophones s'inscrit dans les dispositifs d'accueil spécifiques qui leur sont réservés qu'elle qu'ait été leur structure.
Pour interpréter et analyser des signaux de parole, on emploie diverses représentatiions temps-fréquence (par exemple spectrogramme, distribution de Wigner-Ville, ondellettes).
Dans cet article nous construisons dans la classe de distributions temps-fréquence de Cohen la distribution la mieux appropriée à la représentation des signaux de parole.
Pour cela, on prend en compte des connaissances de la structure temps-fréquence du signal de parole exprimée dans l'Elementary Waveform Speech Model (EWSM, d'Alessandro, 1990).
Comme application nous présentons un algorithme qui réduit un signal de parole, en utilisant la distribution optimisée, à un ensemble de points dans le plan temps-fréquence.
Ainsi on produit une représentation simple du signal qui peut être interpreté très bien non seulement dans le cas stationnaire, mais aussi pour des segments non-stationnaires.
De plus cette représentation pourra servir de base à d'autre analyses (par exemple classification).
Dans beaucoup de centres de recherche sur la parole, des travaux sont en cours qui ont pour but de récolter et d'étiqueter un grand nombre de signaux de parole.
Comme la complexité de ces bases de données augmente, il devient important de porter l'attention sur certains aspects de leur gestion, et plus particulièrement sur leur accès.
Nous présentons un formalisme convivial qui peut être utilisé pour formuler des requêtes auprès d'une base de données de parole.
Ce formalisme est lié au domaine acoustico-phonétique de telle manière qu'un chercheur puisse formuler des requêtes spécifiques sans être un informaticien chevronné.
Cet article explore la dynamique de la bourse à partir d'un point de vue comportemental en utilisant une simulation multi-agent.
L'objectif de cet article est d'étudier le comportement des investisseurs au sein du marché boursier afin de trouver un modèle qui se rapproche le plus de la réalité.
La problématique principale est de comprendre, à travers un nouveau modèle qui inclut les attitudes comportementales et cognitives des investisseurs, le fonctionnement de ce marché et de déterminer les sources de sa complexité.
Des expériences de simulation sont menées pour observer plusieurs faits stylisés des séries temporelles financières.
Ces expériences montrent qu'à partir de la représentation d'un modèle comportemental, centré agent, nous observons des phénomènes socio-économiques émergents.
Nous nous intéressons à la reconnaissance d'objets volumiques par mise en correspondance d'indices visuels.
Nous supposons que les objets à reconnaître sont représentés à l'aide de modèles tridimensionnels, composés d'indices visuels.
Cela constitue l'originalité et la force de la méthode que nous proposons.
Nous présentons de nombreux résultats expérimentaux illustrant l'utilisation de notre approche pour la reconnaissance d'objets.
Herbert Simon est à l'origine de la « découverte scientifique » (Scientific Discovery en anglais).
Ce courant de recherche dont les plus illustres représentants sont Pat Langley, Jan Zytkow et Douglas Lenat, vise à reconstruire rationnellement des découvertes anciennes de façon à les reproduire au moyen d'un ordinateur.
L'intérêt est double.
D'un côté, au plan épistémologique, cela modélise l'activité scientifique jusque dans ses aspects les plus énigmatiques.
D'un autre côté, au plan pratique, s'ouvre une perspective exaltante dans laquelle l'ordinateur épaule l'homme dans sa quête de nouveaux savoirs.
Voici, en hommage à Herbert Simon, quelques aperçus sur la découverte scientifique.
Le miroir des princes connu sous le titre de Naṣīḥat al-mulūk et attribué à al-Māwardī, sans doute un texte du dixième siècle, comprend maintes références à des sources identifiées par l'auteur comme « indiennes » .
Un grand nombre de ces textes apparaît également dans la Waṣiyyat Arisṭāṭālīs li-l-Iskandar ; quelques exemples ne sont pas sans rappeler Kalīla wa-Dimna et Bilawhar wa-Būḏāsaf.
Ces coïncidences ouvrent la porte à plusieurs possibilités : primo, que la source « indienne » de l'auteur représente un ouvrage d'origine indo-européenne traduit à partir du sanskrit ou d'une autre langue indienne vers l'arabe, probablement à l'époque où les Barmécides encourageaient ces traductions à grande échelle ; secundo, qu'elle fut transmise d'une langue indienne en moyen perse (pahlévi) à la période sassanide, puis en arabe dans les premiers siècles de l'Islam ; tertio, que le texte fut composé en moyen perse et acquit une généalogie indienne « forgée » à l'instar de tant d'attributions prétendument grecques, comme cela s'avéra ultérieurement le cas pour le Testament du pseudo-Aristote.
Cet article examine les trois hypothèses et — sur la base de considérations textuelles comme contextuelles — suggère que, dans l'état actuel de la recherche, c'est la deuxième qui semble la plus vraisemblable.
On s'intéresse dans cette étude à la détection et à la localisation de sources en acoustique sous-marine à l'aide d'une antenne longue tractée.
Cette information permet de redonner à l'antenne son gain maximal par exemple en reconstituant une antenne rectiligne, ce qui facilite l'utilisation des méthodes de traitement d'antenne classiques.
Pour cela, nous rappelons les critères de détection AIC et MDL utilisés afin de déterminer le nombre de sources présentes à la fréquence d'analyse.
Puis nous décrivons brièvement la méthode de localisation de type MUSIC utilisée pour identifier ces sources supposées à bande étroite et situées à grande distance du réseau de réception.
Nous appliquons cette suite de traitements sur des signaux expérimentaux.
Les résultats obtenus sont comparés à ceux provenant de la transformée de Fourier bidimensionnelle.
Les résultats de ce mode de représentation spatio-temporel, dont on donne rapidement un rappel théorique, confirment l'importance de la connaissance de la forme d'antenne.
En moyen français, la continuité référentielle et le choix des expressions anaphoriques – formes nominales, pronominales ou 'zéro' appuyées par la morphologie verbale – au sein de la chaîne anaphorique sont gouvernés en récit et discours par cinq règles syntactico-sémantiques : une, de concurrence référentielle, trois, 'valentiello-référentielles' et une, syntactico-valentielle.
Notre objectif est de voir quelle est leur fréquence d'application sur un texte en prose de traduction de moyen français, le Decameron de Boccace traduit par L. de Premierfait (1411–1414), et potentiellement d'observer l'influence que peut avoir la langue d'origine du texte dans la traduction médiévale.
Dans cet article, nous présentons une étude sur l'emploi de différents types de modèles de Markov en reconnaissance de l'écriture.
La reconnaissance est obtenue par calcul de la probabilité a posteriori de la classe d'une forme.
Ce calcul fait intervenir plusieurs termes qui, suivant certaines hypothèses de dépendance liées à l'application traitée, peuvent se décomposer en probabilités conditionnelles élémentaires.
Si l'on suppose que la forme suit un processus stochastique uni- ou bidimensionnel qui de plus vérifie les propriétés de Markov, alors la maximisation locale de ces probabilités permet l'atteinte d'un maximum de la vraisemblance de la forme.
Nous avons étudié plusieurs cas de conditionnement des probabilités élémentaires des sous-formes.
Chaque étude est accompagnée d'illustrations pratiques relatives au domaine de la reconnaissance de l'écriture imprimée et/ou manuscrite.
Cet article décrit un aperçu général d'une approche permettant à la fois de prédire en vue d'applications spécifiques les performances d'un système de reconnaissance de la parole, qui a été caractérisé précédemment, et d'analyser les effets de la variabilité de la parole sur les performances.
La méthode a le mérite potentiel d'ouvrir une voie au développement d'une base de données pour des buts d'évaluation.
La méthode “Recogniser Sensitivity Analysis (RSA)” a été développée chez Logica dans le cadre du programme Alvey “Evaluation des technologies de la parole” (STA n°NMI/132).
La méthode repose principalement sur la caractérisation des sources de variabilité dans le système de reconnaissance par un petit nombre de paramètres mesurables.
Les expériences conduites pour déterminer la validité de la méthode et les résultats sont décrits.
Sa pertinence pour prédire la performance d'un système dans le cas d'applications particulières ainsi que l'influence des variations inter- et intra-locateurs sur la performance sont discutées.
Les résultats préliminaires indiquent une corrélation significative entre les valeurs des paramètres du signal et la performance du système de reconnaissance.
L'article s'achève par une discussion sur les extensions futures de la méthode RSA.
Nous présentons dans cet article quatre codeurs de parole de la famille CELP (Code Excited Linear Predictive).
La qualité de la parole est améliorée en remplaçant la prédiction à long terme par une séquence d'auto-excitation (dictionnaire) de codage adaptatif), ainsi qu'en substituant un dictionnaire de codage réparti à l'ensemble des vecteurs codes à bruit gaussien couramment employé.
Les codeurs sont entièrement quantisés à 5 et 7 kbit/s, c'est-à-dire à des vitesses de transmission intéressantes pour d'éventuelles applications telles que les systèmes GSM à demi débit et INMARSAT-M.
Les performance de ces systèmes de codage sont évaluées grâce à un test conventionnel d'écoute et présentées par leur “Mean Opinion Scores (MOS)”.
Un ensemble de taux d'erreurs binaires tolérables est fourni pour le codeur dont les performances en fonction de la qualité et de la complexité sont maximales.
Il est montré que la présence de bruit de fond acoustique ne modifie en rien la qualité du codeur et que dans ce cas les erreurs binaires seront partiellement masquées par le bruit de fond, grâce à la robustesse du codeur.
Compte tenu des performances en présence de ces perturbations, le codeur semble approprié pour être utilisé dans des systèmes qui ont recours à des liaisons par satellite.
Quand, vers le milieu du XIII e siècle, les grammaires coptes apparurent pour la première fois, leurs auteurs ne pouvaient recourir qu'au modèle linguistique arabe dominant : terminologie et catégories grammaticales.
La langue copte n'appartient pourtant pas à la même famille que l'arabe ; elle était par ailleurs en voie de disparaître comme langue vivante.
À partir de quelques exemples typiques, ayant trait à l'écriture, à la phonologie et à la morphologie, nous essayons de donner une idée de la méthode suivie pour appliquer ou adapter les outils conceptuels et terminologiques arabes dans la description de l'ancienne langue égyptienne à sa dernière phase et de démontrer que, d'une manière générale, les philologues coptes du Moyen Âge ont bien mené leur tâche.
Si l'on peut relever des lacunes, celles-ci ne sont pas nécessairement imputables à la tradition linguistique qui a servi de modèle, mais plutôt aux conditionnements externes qui ont présidé au labeur intellectuel des protagonistes.
Un appareillage d'affichage de contours intonatifs de phrases contrôlé par micro-ordinateur a été développé depuis 1976.
Il a été montré qua pour différents groupes de sujets (néerlandophones apprenant l'anglais et Turcs apprenant le néerlandais), ceux qui reçoivent un feedback audio-visuel fournissent de meilleurs performances que ceux qui ne reçoivent qu'un feedback auditif.
On n'a pas trouvé d'effets d'apprentissage différentiels en fonction du niveau de compétence en L2 ou de l'âge des sujets.
L'article présente une méthodologie de simulation d'échos de cibles radar en environnement marin.
La procédure est basée sur un nouveau modèle, appelé « Ensemble de Points Brillants en Représentation Unifiée » (EPB-RU), qui permet d'approcher le signal écho d'une cible radar pour l'ensemble de ses orientations.
Ce modèle est rapide à calculer et a l'avantage de prendre en compte le masquage partiel ou total de la cible par les vagues de la mer.
Il associe à chaque orientation de la cible un ensemble de points brillants (EPB).
Pour chaque point brillant du modèle, une carte d'amplitude prend en compte son anisotropie et sa visibilité en fonction de l'angle de visée.
Un modèle virtuel combiné mer-navire est utilisé pour décrire le mouvement de la cible et le masquage introduit par les vagues de la mer.
L'influence du fouillis de mer est également prise en compte.
Les signatures radar utilisées dans nos simulations correspondent à quatre maquettes de cibles navales mesurées dans la chambre anéchoïde de l'ENSIETA.
L'article présente aussi quelques résultats d'imagerie radar et de classification, qui illustrent l'aspect inverse du problème de la caractérisation des cibles navales dans leur environnement.
Cet article décrit les techniques de traitement de parole essentielles pour les applications vocales interactives dans le domaine des télécommunications.
Ces techniques comprennent la reconnaissance et la synthèse de la parole qui visent à rendre plus naturelle la communication orale interactive entre l'homme et la machine.
La détection de mots-clé, les techniques de réduction des effets du bruit environnant et l'adaptation au locuteur et/ou aux lignes téléphoniques sont considérées comme des caractéristiques essentielles des technologies de reconnaissance de parole pour permettre une entrée vocale plus naturelle et une robustesse adéquate face aux variablités de l'environnement.
En ce qui concerne la synthèse à partir du texte, on présente ici une méthode de synthèse à base de règles appliquée au Japonais qui vise à fournir une parole de très haute qualité.
Le système commercial ANSER, résultat d'un projet antérieur, est également décrit comme exemple d'un système de traitement vocal interactif.
Enfin, un serveur vocal récemment développé et incluant une fonction de reconnaissance flexible est décrit pour illustrer le concept des techniques de mise en oeuvre de systèmes permettant d'étendre aisément les champs d'application tout en s'adaptant aux changements rapides dans le domaine des Télécommunications.
Cet article décrit deus algorithmes qui séparent deux signaux qui se chevauchent.
Ces algorithmes dépendent des mesures précises de la hauteur de la voix-cible.
Le premier algorithme utilise un seul microphone et le trait important consiste à exploiter le début du voisement pour extraire sa hauteur en la présence d'interférences.
Le deuxième algorithme utilise deux microphones et sa caractéristique majeure consiste aussi à exploiter la direction de la voix-cible.
L'auto-correction dans le discours se fait typiquement en trois temps.
Dans un premier temps, le locuteur contrôle sa propre parole et l'interrompt lorsqu'il rencontre un problème.
Une analyse de 959 corrections spontanées indique que l'interruption suit de très près la perception du problème, à l'exception près que le locuteur a tendance à finir les mots corrects.
Les résultats de cette analyse indiquent d'autre part que la perception du problème s'améliore vers la fin des constituants.
Le deuxième temps se caractérise par des hésitations, des pauses, mais surtout par l'utilisation de ce qu' on peut appeler les 'commentaires rédactionnels'.
La présence immédiate du problème est signalée par l'utilisation de 'uh'.
Dans le troisième temps a lieu la correction elle-même.
La bonne-formation des corrections ne dépend pas de ce que le locuteur respecte l'intégrité des constituants, mais plutôt de la relation structurelle qui existe entre le premier énoncé et la correction.
Cette relation est liée à la relation correspondante entre les éléments conjoints d'une coordination par une règle de bonne formation bi-conditionnelle.
On peut également suggérer qu'il existe une relation semblable entre questions et réponses.
Dans ces trois cas, le locuteur respecte les contraintes structurelles de son premier énoncé.
Enfin, l'analyse démontre que l'ensemble formé par le 'commentaire rédactionnel' et le premier mot de la correction elle-même contient presque toujours des éléments d'information permettant à l'interlocuteur de décider comment il faut relier la correction au premier énoncé.
De ce point de vue, les locuteurs ne produisent presque jamais d'énoncés qui pourraient induire leur interlocuteur en erreur.
Ces résultats indiquent que le locuteur a peu ou pas du tout d'accès au processus de production d'énoncés ; l'auto-contrôle se fait plutôt à partir de la compréhension de sa propre parole intérieure ou extérieure.
Inputs and outputs are not independent phenomena in interactive systems in general and more particularly in multimodal interaction.
We present the results of a Wizard of Oz experiment which shows that output modalities used by a multimodal system have an influence on the input modalities for a large category of users.
A part of the subjects, however, has a favorite input modality and thus cannot be influenced.
This kind of environment does not require any particular knowledge about computers and their use and thus allowed us to study the behavior of ordinary people including subjects who are not familiar with computers.
The experiment also shows that speech is a favorite modality within smart room environments for a large part of users, except when graphics modality is used by the system : pointing on a touch screen is then preferred to speech.
La conception et l'évolution de l'algorithme de codage bas debit 16 kbit/s LD-CELP (recommandation CCITT G.728) a représenté un important effort de recherche entre 1988 et 1992.
Cet article donne un historique de cet effort de quatre ans, en insistant sue les performances techniques des nombreuses variantes algorithmiques qui oat été explorées.
Dans le cadre de cette discussion, nous expliquons pourquoi nous avons retenu certaines de ces techniques dans la version finale due codeur G.728 et pourquoi nous avons écarté les autres.
Nous espérons que cet article montrera comment l'algorithme G.728 a été conçu à partir de concepts mitiaux tres simples, puis a été modifié et amélioré peu à peu du fait des contraintes d'implantation en temps réel, jusqu'à ce que la version finale satisfasse les exigences de performance qui semblaient inaccessibles au départ.
PADIS, le système de standard téléphonique automatique et d'information annuaire de Philips offre une interface utilisateur en langage naturel pour accéder à une base de données téléphoniques.
En utilisant les technologies de reconnaissance de la parole et de compréhension de langage, le système permet d'obtenir les numéros de téléphone, les numéros de fax, les adresses électroniques, les numéros de pièces ainsi que l'établissement direct d'appel vers le numéro désiré.
Dans cet article, nous présentons le cadre probabiliste sous-jacent, l'architecture du système, et les modules individuels de reconnaissance de parole, de compréhension du langage, de contrôle du dialogue, et de sortie vocale.
De plus, nous rapportons des résultats sur les performances et le comportement des usagers obtenus à partir d'un test terrain réalisé dans notre laboratoire de recherche avec une base de données de 600 entrées.
Nous dérivons une nouvelle règle de décision basée sur le critère de maximum a posteriori qui incorpore des connaissances sur la base de données et sur l'historique du dialogue comme des contraintes pour la reconnaissance de la parole et la compréhension du langage.
Ceci a permis d'améliorer la compréhension de la parole de 19% (en termes de taux d'erreur), et de réduire de 38% les erreurs de substitution des attributs (par exemple reconnaissance d'un nom erroné).
La règle de décision est implantée dans une approche multiniveaux correspondant à une combinaison d'une reconnaissance de parole au niveau de l'état de l'art, d'une recherche grammaticale partielle dans une grammaire à attributs hors contexte et stochastique, et d'un algorithme de recherche des N-meilleures solutions, qui est également décrit dans cet article.
Le projet d'un répertoire des « mises en prose » qui mettra à jour le célèbre ouvrage de Georges Doutrepont (1939) a été lancé lors du IIIe Colloque International de l'AIEMF (Gargnano, mai 2008);
sont présentées ici les grandes lignes de cette initiative, la fiche modèle qui sera remplie par les collaborateurs, et deux notices : la Manequine de Jean Wauquelin et la Belle Hélène de Constantinople anonyme.
De nombreux réseaux du monde réel peuvent être modélisés par des grands graphes.
Réduire la complexité d'un graphe de manière à ce qu'il puisse être facilement interprété par l'oeil humain est une aide précieuse pour comprendre et analyser ce type de données.
Nous comparons ici deux approches de regroupement de sommets en communautés et proposons une visualisation interactive multi-échelle de grands graphes basée sur ces classifications hiérarchiques des sommets qui nous permettent de représenter ces graphes de manière lisible et interprétable.
Nous appliquons ensuite notre méthodologie à un réseau de blogs francophones afin d'illustrer rapidement les avantages et inconvénients de cette approche.
Dans le cadre des applications mobiles, diffuses et omniprésentes, la détermination et l'explicitation du contexte sont nécessaires pour fournir des solutions IT personnalisées à un utilisateur donné dans une situation donnée.
Dans ce papier, le contexte est considéré comme une relation n-aire.
Le contexte est supposé contenu dans des ontologies qui sont utilisées pour structurer les connaissances spécifiques des applications.
Nous présentons une approche basée sur une modélisation de cas pour exprimer le contexte et sa gestion.
Nous discutons l'incorporation d'un raisonnement et la génération d'explications basées sur le contexte.
Nous montrons finalement comment appliquer notre approche dans la gestion de la confiance.
Il s'agit de l'étude et édition d'un bref traité de morpho-syntaxe latine, transmis par le ms. Londres, B.L., Add. 10352.
Manuel destiné à des élèves ayant déjà appris l'Ars minor de Donat, ce texte anonyme organisé par questions/réponses aborde trois sujets : construction de la phrase latine, régime, accord.
Son intérêt réside entre autres dans la terminologie adoptée (premières attestations et/ou acceptions techniques).
La modélisation et la simulation ont longtemps été dominées par les approches basées sur les équations, jusqu 'à l'avènement récent des approches orientées agents.
Pour freiner l'augmentation de complexité des modèles que peut entraîner l'utilisation de cette nouvelle approche, la tendance est à la sursimplification des modèles.
Des modèles plus descriptifs ont cependant été développés pour une variété de phénomènes, mais la cognition des agents est encore trop souvent négligée alors qu'elle a une grande importance dans certains domaines, en particulier en sciences humaines et sociales.
La solution que nous proposons dans cet article est d'utiliser des agents BDI.
Nous montrons qu 'il s'agit d'un paradigme expressif, réaliste et simple qui apporte de nombreux bénéfices à la simulation à base d'agents.
Wir stellen eine robuste rekursive procedure vor, die Identifikation des nichtstationären AR-Model für Sprache produktion erlaubt und auf gewichteten rekursiven Kleinsten-Quadrate-Algoritmus (weighted recursive least squares – WRLS) mit VFF (variable forgetting factor) basiert ist und sowohl quadratische Klassifikator mit gleitenden Training Daten.
Die Bewertung des Verfahrens haben mittels die Sprach-Analyse stimmhaften und gemischten Anregungen durchgefürt.
Die Ergebnisse von Simulationen haben gezeigt dass vorgeschlagene robuste rekursive procedure eine bessere AR-Parameter Schätzung ermöglicht und die Verfolgung erheblich verbessert.
Cet article propose une nouvelle méthode de représentation et de visualisation en couleur d'images multispectrales ou hyperspectrales.
Le problème de la visualisation de telles données est en effet problématique dès que le nombre de bandes spectrales est supérieur à trois, i.e., la représentation triviale RVB (Rouge, Vert, Bleu) n'est plus directe.
Le principe consiste ici à utiliser une carte de segmentation préalablement obtenue, a priori, et à réaliser une analyse factorielle discriminante permettant de distribuer au mieux l'information dans l'espace des couleurs TSL (Teinte, Saturation, Luminance).
L'information apportée par la carte de segmentation (chaque site est associé à une classe) peut se révéler judicieuse comme le montrent les résultats obtenus sur des lots d'images de tailles croissantes dans le cadre de l'imagerie astronomique.
Cette méthode est générale et s'applique également à d'autres domaines manipulant des images multicomposantes ou multivariées comme en télédétection ou en imagerie polarimétrique.
L'extraction d'information est guidée par une ressource termino-ontologique qui exploite des patrons d'extraction et un lexique.
Elle a été intégrée dans un assistant guidant l'expert pour remplir la base de données.
La conception initiale d'un système de codage d'image par analyse-synthèse basé sur un modèle (MBASIC) est décrite et une méthode de construction du modèle facial tridimensionnel (3-D) qui inclut les méthodesdes synthèses pour les expressions faciales est présentée.
Le système proposé MBASIC est une méthode de codage d'images qui utilise un modèle tridimensionnel d'un objet qui doit être reproduit.
Une image d'entrée est d'abord analysée et une image de sortie utilisant le modèle 3-D est ensuite synthétisée.
Une transmission à très faible débit peut être réalisée car le codeur transmet seulement les paramètres d'analyse requis.
Les images de sorties peuvent être reconstruites sans l'effet perturbateur du bruit qui réduit l'aspect naturel, parce que le décodeur synthétise des images à partir d'un modèle 3-D similaire.
Pour construire le modèle 3-D de la face d'une personne, une méthode qui utilise un modèle 3-D de face à trame de fil est développée.
Une image complète de face est projetèe sur ce modéle de trame de fil.
Pour la synthèse des expressions faciales, deux méthodes différentes sont proposées : une méthode de tonte et de collage et une méthode de déformation des structures faciales.
Nous étudions ici les aspects temporels de la syllable et du mot en Italien parlé.
En ce qui concerne les syllables, nous avons testé les effects de la composition syllabique sur la durée acoustique des voyelles et des consonnes.
Les effets de la structure du mot sur les durées segmentales ont été testés en variant la longueur du mot et la position de l'accent lexical.
Les résultats indiquent que tant la structure de la syllabe que celle du mot ont des effets systématiques sur la duréee des voyelles et des consonnes.
La structure syllabique et la longueur du mot ont surtout des effets anticipatoires et tendent à pŕeserver la duréee totale de ces deux unités.
Dans notre recherche, deux points sont mis en avant : 1. Les données syllabiques suggèrent que l'unité tendant à être constante en durée est l'intervalle temporel entre les débuts de deux voyelles consécutives ; 2. Les données relatives à la syllable et au mot indiqueraient que les variations de durée dues à ces deux variables sont réalisèes par deux stratégies articulatoires différentes.
Quelques observations sur le rythme syllabique et accentuel sont présentées.
Un des problèmes les plus urgents de la recherche actuelle des systèmes de reconnaissance de la parole automatique est leur robustesse déficiente envers du bruit additif et la réverbération.
Le modèle de perception auditive (PEMO) réalisé par Dau et al. (T. Dau, D. Püschel, A. Kohlrausch, J. Acoust. Soc. Am. 99 (6) (1996) 3615–3622) pour une application dans le domaine psychoacoustique peut partiellement surmonter ces difficultés, s'il est appliqué comme prétraitement pour la reconnaissance de la parole automatique.
Afin de perfectionner la performance de ce système auditif de reconnaissance de la parole automatique en bruit d'environnement, plusieurs méthodes de débruitage de la parole furent examinées, qui étaient évaluées comme composants des prothéses auditives dans le passé.
La réduction monaurale de bruit comme proposée par Ephraim and Malah (Y. Ephraim, D. Malah, IEEE Trans. Acoust. Speech Signal Process. ASSP-32 (6) (1984) 1109–1121) fut comparée avec le filtre binaural et l'algorithme de réverbération d'après Wittkop et al. (T. Wittkop, S. Albani, V. Hohmann, J. Peissig, W. Woods, B. Kollmeier, Acustica United with Acta Acustica 83 (4) (1997) 684–699).
Tous les deux algorithmes de réduction de bruit améliorent la performance de reconnaissance correspondant à une amélioration de jusqu'à 10 dB de rapport signal/bruit pour tous les bruits d' environnement étudiés, pendant que les résultats obtenus pour la parole présentée sans bruit ne furent pas diminués considérablement.
M me dans un environnement réel sans réverbération ces méthodes de réduction de bruit améliorent la performance de reconnaissance correspondant à une amélioration de jusqu'à 5 dB de rapport signal/bruit.
Ces résultats dépassent les prévisions, parce que dans des anciennes études on n'avait pas obtenu une augmentation de l'intelligibilité de la parole pour des patients avec des déficiences auditives.
Les erreurs systématiques trouvées pendant le développement phonologique peuvent fournir, au même titre que les erreurs de production et les processus phonologiques des adultes, des preuves sur les mécanismes de production du langage.
Une recherche détaillée des contextes dans lesquels les plosives velaires sont avancées ('fronted') chez un enfant phonologiquement retardé montre que le 'fronting' dépend du stress et des frontiéres du mot, présente des exceptions lexicales et se produit en production uniquement.
Ces caractéristiques suggérent que des représentations lexicales de sortie de l'enfant sont indépendantes des représentations lexicales à entrée et que les erreurs de 'fronting' ne se produisent que dans les représentations de sortie.
Cela suggére aussi que les traits prosodiques sont essentiels pour l'identification des traits articulatoires dans les représentations.
Une telle analyse a des implications pour les théories de l'accès lexical et pour le développement de l'accès lexical chez les enfants.
A la suite de la minimisation du travail des articulateurs, la problème de l'inversion pour l'obtention de la forme et de la fonction d'aire du conduit vocal a été résolu.
On a utilisé les valeurs de la fréquence des quatre formants du signal de la parole.
La forme de la langue d'un homme et d'une femme a été mesurée à l'aide de microrayons X.
Les formes du conduit vocal et les valuers des fréquences formantiques mesurées et calculées sont très proches pour l'homme et assez proches pour la femme.
Cet article tente de dresser le bilan de ce que les réseaux sémantiques ont apporté à la terminologie.
Il s'appuie sur le formalisme des graphes conceptuels et, plus marginalement, sur celui des logiques de description.
Nous avançons néanmoins l'idée que ce type de formalisme a un rôle à jouer dans le processus de modélisation, un rôle d'intermédiaire entre la sémantique d'une langue naturelle et un modèle opérationnel utilisé dans un processus automatique.
L'utilisation d'un modèle de série chronologique rend compte des faits suivants : les cycles glottiques sont produits séquentiellement et il existe des liens entre les perturbations de cycles voisins.
Le modèle représente la perturbation de la durée du cycle présent comme une somme pondérée des perturbations passées et d'un bruit blanc.
Le modèle est ajusté à des séries de perturbations observées, à l'aide de méthodes linéaires conventionnelles.
Une analyse discriminante de séries extraites de 279 vocoı̈des [a] [i] [u] montre que des indices qui décrivent isolément les composantes prédictibles et aléatoires des perturbations caractérisent mieux locuteurs dysphoniques et sains qu'un indice conventionnel.
La conclusion est que les relations entre perturbations de cycles voisins constituent un aspect indépendant de la dispersion des microperturbations décrite à l'aide des indices conventionnels.
Dans ce travail nous présentons un nouvel algorithme d'extraction de la forme par la texture appliqué à l'analyse des scènes naturelles.
L'originalité de cette approche est basée sur la structure du cortex visuel primaire (V1) dont elle modélise les fonctions.
L'algorithme est capable d'analyser une grande variété de textures présentant différents types d'irrégularités.
Tout d'abord pour réaliser l'échantillonnage du spectre d'amplitude, nous proposons de nouveaux filtres, appelés filtres log-normaux, inspirés du fonctionnement des cellules complexes de l'aire V1, en remplacement des filtres de Gabor classiques.
Ces filtres s'avèrent particulièrement appropriés aux techniques de reconnaissance de forme de part leurs différentes propriétés théoriques, notamment leur profil en fréquence radiale (adapté à la décroissance en 1/f des scènes naturelles) et leur séparabilité en orientation et en fréquence.
Nous utilisons ensuite une méthode d'estimation de la fréquence moyenne locale appliquées sur des signaux naturels.
Celle-ci ne nécessite pas la recherche d'une échelle adaptée à l'analyse et tire avantage de l'ensemble des fréquences du banc de filtres utilisé.
Finalement, à partir de l'estimation locale, l'orientation et la forme sont extraits en utilisant les propriétés géométriques de la projection perspective.
La précision de la méthode est évaluée sur différents types de textures, à la fois régulières et irrégulières, et sur des scènes naturelles.
La méthode présentée permet d'obtenir des résultats se comparant favorablement aux meilleures techniques existantes tout en conservant un faible coût de calcul.
Enfin le modèle peut être adapté à d'autres applications telles que l'analyse de textures, l'extraction de points caractéristiques ou l'indexation d'images par le contenu.
Une des plus frappantes caractéristiques typique du langage des aphasiques de Broca est l'agrammatisme caractérisé par l'ommission des mots “fonctionnels” et des morphèmes indiquant des inflexions.
L'agrammatisme est le plus souvent considéré comme symptomatique d'un déficit syntaxique.
Nous pensons que cette optique manque de rigueur grammaticale et que seule une interprétation de ce déficit en termes de structure phonologique peut être cohérente et systématique.
Une classe naturelle composée de mots de fonction et de certains morphèmes liés, peut être définie par référence aux propriétés de frontières des phrases lesquelles définissent les mots phonologiques.
C'est cette classe d'éléments qui tend à disparaître dans le discours agrammatical.
Le but de cet article n'est pas seulement de fournir une hypothèse permettant d'interpréter un syndrome aphasique, mais aussi de tester et d'illustrer l'idée qu'il est efficace de considérer attentivement les universaux substantifs de la structure grammaticale lorsqu'il s'agit de rendre compte des déficits linguistiques.
Une version modifiée de l'algorithme SEARMA est proposée pour l'estimation du spectre du signal de parole en présence d'un bruit de fond coloré.
Les hypothèses suivantes sont utilisées pour la mise en œuvre de l'algorithme.
Le processus de génération du signal de parole est modélisé par un modèle ARMA.
Compte tenu de ces hypothèses le signal de parole peut alors être représenté par un modèle ARMA étendu.
Dans cette formation, une estimation unique des paramètres AR de la fonction de transfert du conduit vocal peut être effectuée séparément si les paramètres MA du modèle du bruit peuvent être estimés séparément, mais l'estimation des paramètres MA du modèle de signal de parole nécessite l'hypothèse supplémentaire d'un rapport signal sur bruit élevé.
La validité de la méthode proposée est illustrée par l'estimation spectrale à la fois de signaux de parole synthétique et naturelle en présence de bruit additif coloré et en comparant les résultats avec ceux obtenus par la technique LPC.
Dans ce papier, une méthode est présentée pour déterminer un index utile à la recherche dans des documents audio.
La tâche diffère de l'indexation traditionelle de documents textuels, parce que les grandes bases de données sonores sont décodées par la reconnaissance automatique de la parole, et des erreurs de décodage s'y produisent fréquemment.
L'idée centrale dans cet article est de profiter de la taille de la base de données pour choisir les meilleures termes d'indexation pour chaque document et ce en considérant les autres documents qui lui sont proches dans un espace vectoriel sémantique.
Pour ce faire, le signal acoustique est d'abord converti en texte par un système de reconnaissance de la parole.
Ensuite, le texte de chaque document est représenté par un vecteur qui est la somme normalizée des vecteurs des mots du document.
Une grande collection de vecteurs de document est employée pour former une carte de Kohonen qui permet une classification des documents et une découverte des structures sémantiques dans la collection.
Comme les documents des nouvelles lues sont courts et incluent des erreurs de reconnaissance de la parole, l'idée de lisser les vecteurs de document en utilisant les classes thématiques déterminées par la carte d'auto-organisation de Kohonen est introduite pour obtenir une meilleure indexation.
Dans cet article, l'approche précédente est appliquée à l'indexation et à la recherche dans les documents de nouvelles télévisées et de radio.
Les résultats expérimentaux sont donnés en utilisant les données d'évaluation de TREC pour la tâche de recherche dans les documents sonores.
Cet article propose trois types d'algorithmes d'adaptation au bruit qui permettent d'améliorer les performances des systèmes de reconnaissance de la parole en présence de bruit.
Ce sont des techniques de correspondance de paramètres basées sur la quantification vectorielle qui transforment hiérarchiquement les vecteurs de paramètres bruités en vecteurs de paramètres pour des conditions normales.
Le premier algorithme a déjà été utilisé dans le cadre de l'adaptation au locuteur non dirigée.
Il est fondé sur une technique de classification catégorique qui adapte itérativement les données bruitées à un petit volume de données représentant la parole normale.
Le deuxième algorithme est une version modifiée du premier.
Il redéfinit la fonction de correspondance en utilisant la notion de portée d'une classe.
Le dernier algorithme propose une classification floue à la place d'une classification catégorique.
Dans le cadre de la base de données de chiffres NATO, ces techniques améliorent de façon importante les performances du système de reconnaissance de la parole du CRIM.
La méthode EXPULSE pallie l'une des principales limitations des techniques traditionnelles d'Analyse Spectrale à Haute Résolution ASHR (MUSIC, Norme minimale, etc.) à savoir la faible robustesse vis-à-vis d'une méconnaissance de leur nombre.
Son originalité repose sur l'interprétation d'un périodogramme mis en œuvre sur un processus de raies pures noyées dans un brait additif, comme la convolution, à un bruit près, d'un processus composite Bemoulli-Gaussien avec une fonction spectrale parfaitement connue qui dépend de la calibration retenue du périodogramme (type de fenêtre d'apodisation, lissage temporel ou fréquentiel, etc.).
Les fréquences discrètes où le processus de Bemoulli prend la valeur 1 sont des raies potentielles de l'espace signal ; le processus gaussien caractérise quant à lui l'amplitude des raies.
Depuis que les études d'impact des changements climatiques ont montré que le milieu marin pourrait être énormément fragilisé par la disparition de certaines espèces parmi la faune et la flore ainsi que par le vieillissement rapide des infrastructures sous-marines, de nouveaux outils d'observation efficaces et robustes deviennent nécessaires.
Dans cet article, l'utilisation de caméras acoustiques comme outil novateur d'acquisition de données sousmarines est proposée en compagnie d'un cadre conceptuel qui permet de mettre en œuvre une reconstruction tridimensionnelle pertinente et complète de l'environnement sous-marin à partir de séquences d'images acquises par ces caméras acoustiques.
Les différentes données et informations extraites, utilisables et utilisées pour élaborer ce travail ainsi que quelques résultats préliminaires sont abordés.
Une base de données d'environ 6500 syllabes et segments correspondants a été analysée pour développer un modèle de durée segmentale et syllabique pour l'Anglais Australien.
La durée segmentale a été analysée en fonction du contexte prosodique.
Les syllables ont été étiquetées suivant leur contexte prosodique, leur longueur (nombre de segments), et la nature des pics syllabiques.
La durée syllabique a été modélisée en utilisant un réseau neuronique à trois couches qui a été entrainé et testé sur des portions différentes de la base de données.
Les durées segmentales ont été étirées ou compressées pour correspondre aux durées syllabiques prédites par le réseau.
Ce modèle syllabique relativement simple a permis de rendre compte de près de 80% de la variance des durées syllabiques observées dans la base de données.
La performance des systèmes actuels de reconnaissance de la parole automatique est considérablement compromise par des niveaux d'interférence acoustique (telle que du bruit additif et de la réverbération) qui sont représentatifs de conditions réelles.
Des études sur la perception de la parole par des êtres humains et une analyse des bandes fréquencielles critiques suggèrent que la robustesse des systèmes de reconnaissance pourrait être améliorée en se focalisant sur la structure temporelle du signal qui apparaı̂t comme des modulations d'amplitude de basse fréquence (moins de 16 Hz) dans les sous-bandes.
Une représentation de la parole soulignant cette structure temporelle, appelé “spectrogramme de modulation” (modulation spectrogram), a été développée.
Des visualisations de la parole utilisant le spectrogramme de modulation sont relativement stables, malgré des niveaux élevés de bruit de fond et de réverbération.
L'utilisation du spectrogramme de modulation apporte une amélioration de performance importante en présence de beaucoup de réverbération.
La combinaison du spectrogramme de modulation avec le codage log-RASTA-PLP (log RelAtive SpecTrAl Perceptual Linear Predictive analysis) permet d'obtenir des améliorations significatives pour de nombreuses conditions de bruit et de réverbération.
Dans cet article, on présente une technique de pondération pour effectuer l'analyse LPC d'un signal de parole voisée.
Les échantillons sont pondéres sur base de leur conformité au modèle de production de parole voisée.
Dans les deux techniques de pondération présenées, la première choisit comme fonction de poids la fonction d'énergie à court-terme du signal de parole préaccentué, tandis que la seconde s'obtient par seuillage de cette même fonction d'energie.
Dans les deux cas, la méthode proposée a pour effet de pondérer sélectivement les échantillons de parole qui correspondent bien au modèle de production.
En conséquence, on obtient par cette méthode une estimation des paramètres LPC qui est à la fois plus précise et aussi moins sensible à la fréquence fondamentale que celle fournie par l'analyse LPC classique.
On sait que les performances des systèmes de reconnaissance peuvent être dégradées lorsqu'ils ont à traiter de la parole rapide.
Si l'on peut détecter le fait que la parole est rapide, par des mesures de débit d'élocution, les modèles acoustiques et les modèles de langage peuvent être adaptés pour compenser les effets liés à cette parole rapide.
Nous avons étudié diverses mesures de débit d'élocution qui ont l'avantage de pouvoir être faites avant la reconnaissance.
Les mesures que nous proposons ont été comparées aux mesures conventionnelles, à savoir les débits de mots et de phonèmes sur la base de données TIMIT.
Certaines des mesures proposées sont corrélées de façon significative avec le débit des phonèmes et la durée des voyelles.
Nous avons montré que les écarts entre les durées réelles et attendues des voyelles test peuvent être réduits si les modèles de durée des voyelles sont adaptés au débit d'élocution, tel qu'estimé par les mesures proposées.
Ces mesures peuvent être calculées à partir des indices employés communément en reconnaissance de parole.
La segmentation des images ROS (Radar à Ouverture Synthétique) consiste à produire une partition de l'image initiale en classes possédant certaines caractéristiques d'homogénéité au sens de la rétrodiffusion.
A cet effet, un modèle statistique des images ROS polarimétriques est utilisé pour les segmenter en classes homogènes ayant chacune des caractéristiques de rétrodiffusion similaires.
Cette segmentation utilise l'estimateur MAP (Maximum A Posteriori) que nous avons implémenté soit par l'algorithme déterministe ICM Modifié (Iterative Conditional Modes), soit par l'algorithme stochastique RS (Recuit Simulé).
Pour cela un champ aléatoire de Markov représentant la distribution des étiquettes des classes est combiné avec un modèle (distribution Gaussienne ou distribution-K) représentant la distribution des données polarimétriques pour chacune des classes données.
Les résultats de partition initiale et de segmentation obtenus sur une image mono-vue polarimétrique de la forêt des Landes, démontrent l'aptitude de l'algorithme proposé CMFMLAP-NSO à fournir une partition de bonne qualité d'une part et la capacité de l'algorithme ICM à affiner cette partition et à produire une segmentation de meilleure qualité d'autre part.
Six types de synthèse de la parole ont été évalués du point de vue de la compréhensibilité : analyse-synthèse LPC normale ; analyse-synthèse synchrone ; analyse synthèse synchrone à excitations multiples ; et trois techniques PSOLA.
La compréhensibilité relative des types de synthèse fut testée en utilisant la parole synthétisée pour communiquer aux sujets l'information nécessaire pour remplir un questionnaire à choix multiples basé sur un diagramme.
L'application de ces théories aux réseaux bayésiens est incomplète et nous proposons une contribution, essentiellement via les nombres de couverture.
Nous en déduisons de nombreux corollaires et notamment une approche non-fréquentiste pour l'apprentissage de paramètres et un score prenant en compte une mesure d'entropie structurelle qui affine les classiques mesures basées sur le nombre de paramètres seulement.
Nous proposons alors des méthodes algorithmiques pour traiter de l'apprentissage qui découle de nos propositions, basées sur BFGS et l'affinage adaptatif du calcul du gradient.
Afin de faciliter l'accès et l'utilisation par les personnes du grand public des applications et services en informatique qui se répandent rapidement en particulier sur l'internet, il est nécessaire de proposer de nouveaux outils d'assistance qui offrent une interaction naturelle afin d'être mieux acceptés.
L'approche des agents conversationnels semble prometteuse mais les agents ne peuvent pas se contenter d'opérer un raisonnement de type rationnel sur la structure et le fonctionnement des applications assistées.
Ils doivent aussi exprimer des comportements psychologiques incluant des relations sociales, des traits de personnalité, des affects.
Dans la première partie de l'article, nous proposons un cadre flexible pour modéliser les relations entre les réactions rationnelles et comportementales d'un agent assistant.
Ensuite ce cadre est utilisé pour implémenter une première étude de cas, fondée sur la notion de biais cognitif.
Les systèmes de reconnaissance vocale sont maintenant utilisés dans des applications où ils doivent fournir une reconnaissance satisfaisante dans différentes conditions de bruit.
Cependant, un écart entre les conditions d'entraı̂nement et de test est souvent à l'origine d'une serieuse baisse de performance des systèmes.
Le succès de cette technique a été verifiée pour plusieurs expériences utilisant différents bruits de fond et microphones.
La méthode proposée appliquée à la reconnaissance de chiffres indépendante du locuteur et dans un environement multiple a réduit le taux d'erreur de plus de 16%.
Ce papier présente un bilan des travaux comparant les performances des systèmes de reconnaissance de parole modernes à celles des locuteurs humains.
Les comparaisons sont basées sur six types de corpus de parole avec des vocabulaires allant de 10 à plus de 65000 mots et des contenus allant des mots isolés à des conversations spontanées.
Les taux d'erreurs des machines sont souvent supérieures de plus d'un ordre de grandeur à celles des humains pour la parole lue en atmosphère calme et transmise en large-bande.
Les performances des machines se dégradent encore par rapport à celles des humains dans les contextes bruités, ou de qualité de transmission variable et pour la parole spontanée.
Les locuteurs humains peuvent également reconnaitre, avec peu d'information linguistique de haut-niveau, des syllabes ou des phrases sans signification quand elles sont prononcées clairement dans des atmosphères calmes.
Ces comparaisons suggèrent que l'écart important qui subsiste entre les performances des machines et celles des humains peut être réduit par des recherches de base sur les sujets suivants : l'amélioration de la modélisation acoustico-phonétique de bas-niveau, l'amélioration de la robustesse au bruit et à la variabilité des conditions de transmission, et la modélisation plus précise de la parole spontanée.
Cet article présente une contribution au domaine de la reconnaissance de locuteurs.
Il traite de l'analyse de la parole par prédiction linéaire et examine la contribution en reconnaissance de ses deux composantes principales, le filtre de synthèse d'une part et le résidu d'autre part.
Cette étude se fonde sur la propriété d'orthogonalité ainsi que l'importance physiologique de ces deux composantes, qui suggèrent que la reconnaissance du locuteur se basant exclusivement sur le filtre de synthèse peut être améliorée.
En particulier, nous proposons une nouvelle représentation du résidu et nous examinons ses propriétés de reconnaissance au moyen d'expériences conduites dans un contexte de vérification du locuteur indépendante du texte.
Ces expériences, utilisant à la fois des méthodes connues et nouvelles, nous permettent de comparer les contributions des deux composantes au succès de la reconnaissance.
Nous commençons par comparer les méthodes séparément, puis conjointement.
Nous conduisons ces expériences en utilisant la même base de données et la même méthodologie, caractérisée par la stricte séparation des ensembles d'apprentissage et de test.
Les résultats obtenus démontrent l'utilité propre du résidu, même si elle apparaît moindre que celle du filtre de synthèse.
Cependant, le résidu se montre particulièrement utile quand ces deux composantes sont combinées.
Dans le cas reporté ici, un taux d'erreur de 5.7% a pu être réduit à 4.0%.
La possibilité de faire varier le type de locuteur et le style langagier sera un trait caractéristique de la prochaine génération de systèmes de conversion texte-parole.
Déjà actuellement, la nécessité de telles possibilités se fait sentir dans les systèmes de dialogue et lorsque la synthèse de parole est utilisée comme prothèse pour les personnes souffrant d'un handicap de communication.
Une grande part de l'information nécessaire n'est pas encore disponible.
Dans cette contribution, nous prétendons que la synthèse de parole elle-même est un outil efficace pour étudier et comprendre la variabilité de la parole.
Différentes méthodes sont passées en revue, représentant aussi bien les techniques d'analyse/synthèse et de manipulations du signal que de conversion texte-parole.
Parmi les travaux du KTH, l'accent est mis sur l'étude de la variation du locuteur et des styles langagiers dans le contexte de notre système de conversion texte-parole.
Cet article propose une méthode originale de programmation des robots fondée sur l'inférence et l'apprentissage bayésien.
Cette méthode traite formellement des problèmes d'incertitude et d'incomplétude inhérents au domaine considéré.
La principale difficulté de la programmation des robots vient de l'inévitable incomplétude des modèles utilisés.
Nous exposons le formalisme de description d'une tâche robotique ainsi que les méthodes de résolution.
Nous l'illustrons en utilisant ce système pour programmer une application de surveillance pour un robot mobile : le Khepera.
Pour cela, nous utilisons des ressources génériques de programmation appelées « descriptions » .
Nous montrons comment définir et utiliser de manière incrémentale ces ressources (comportements réactifs, fusion capteur, reconnaissance de situations et séquences de comportements) dans un cadre systématique et unifié.
L'article présente une nouvelle approche dans le domaine fréquentiel à l'implémentation de postfiltres adaptatifs pour l'amélioration de la parole bruitée.
Le postfiltre est décrit par un ensemble de coefficients TFD qui atténuent le bruit das les vallées spectrales et qui tolèrent plus de bruit dans les régions formantiques où il est masqué par le signal de parole.
D'abord, nous effectuons une analyse LPC du signal bruité et nous calculons le spectre d'amplitude logarithmique de la parole á l'entrée.
Après avoir identifié les formants et vallées (à l'aide d'une nouvelle méthode), le spectre d'amplitude logarithmique est modifié afin d'obtenir les coefficients du postfiltre.
Le filtrage est aussi effectué dans le domaine fréquentiel à l'aide d'une TFR et d'une stratégie chevauchement-addition pour obtenir le signal postfiltré.
Les résultats expérimentaux obtenus sur de la parole échantillonnée à 8 kHz montrent que cette nouvelle méthode fréquentielle produit de la parole améliorée d'une qualité perceptive meilleure que celle obtenue par une méthode temporelle.
La nouvelle méthode est particulièrement efficace pour éliminer du bruit à haute fréquence et pour préserver les faibles formants à fréquence élevée des sonantes.
Des recherches antérieures ont montré que les attentes rythmiques jouaient un rôle important dans les langues présentant un contraste entre syllabes accentuées et syllabes non-accentuées, alors que le traitement perceptif des langues comme le français qui ne présentent pas ce contraste et dont les frontières syllabiques sont claires prendrait appui sur la syllabe.
L'étude a porté sur la segmentation de mots disyllabiques contenant deux monosyllabes enchâssés.
Deux expériences ont étudié l'effet sur les segmentations de la présence du schéma iambique usuel ou du schéma inverse.
Le schéma iambique a entrainé plus souvent la reconnaissance des disyllabes, sans qu'apparaisse un effet de la fréquence des monosyllabes ou de leur structure syllabique.
Le schéma trochaïque a fortement augmenté le nombre de segmentations.
Dans l'expérience 2, la focalisation de l'attention sur la structure temporelle des séquences a renforcé ces effets.
On en concluera que les auditeurs français ont utilisé une stratégie de segmentation métrique.
Par contre, le traitement de spondées (expérience 3) a mis en évidence l'effet des paramètres structuraux sur les segmentations, ce qui suggère l'emploi d'une procédure de segmentation à base syllabique en l'absence d'information sur le rythme.
L'apport de ces résultats à l'étude des modèles de la reconnaissance de la parole est précisé.
Le problème du conflit, intrinsèque à la fusion d'informations, a poussé à de nombreuses réflexions ces dernières années, en particulier dans le cadre de la théorie des fonctions de croyance.
Nous pouvons résumer les solutions apportées par trois façons de considérer le problème : premièrement, nous pouvons chercher à réduire voire supprimer le conflit avant la combinaison d'informations, deuxièmement nous pouvons gérer le conflit de façon à ce qu'il n'intervienne pas lors de la combinaison et ne le considérer que lors de la prise de décision, et troisièmement nous pouvons prendre le conflit en compte lors de l'étape de combinaison.
Si la première solution paraît la meilleure elle n'est pas toujours réalisable ou suffisante.
Il peut être difficile de chercher à départager philosophiquement les deux dernières stratégies.
c'est donc dans cette optique que nous comparons ces approches.
Nous proposons ici une nouvelle règle qui a pour principe de répartir le conflit proportionnellement sur les éléments produisant ce conflit.
Nous comparons les différentes règles à partir de données réelles en imagerie Sonar et en classification de cibles Radar.
Ce papier présente le développement d'une interface parole pour la commande d'un répondeur ou d'une messagerie vocale respectivement.
La reconnaissance automatique de la parole a été intégrée pour faciliter la commande à distance et le tri des messages vocaux depuis n'importe quel téléphone et ce, en dialogue oral.
Le but de ce développement était que les utilisateurs perçoivent l'interface parole comme bénéfique par rapport aux commandes DTMF plus classiques.
Dans cet article, nous décrivons d'abord la technologie vocale utilisée dans le système.
Ensuite on montre comment, se basant sur cette technologie, l'interface utilisateur a été conçue dans une approche descendante.
Nous avons commencé par un premier développement que nous avons testé en utilisant la simulation Wizard-of-Oz.
Après avoir perfectionné le concept dans un développement parallèle, celui-ci a été implanté dans un prototype “haute fidélité”.
La conception a été améliorée en se basant sur trois itérations de tests qualitatifs auprès d'utilisateurs.
L'atteinte de l'objectif de ce développement a été finalement validée par des tests utilisateurs dans deux pays.
Il est connu que la distorsion acoustique introduite par l'environnement ambiant ainsi que la variabilité résultant du stress induit détériorent énormément les performances des algorithmes de reconnaissance.
Dans cet article, on explore les diverses causes de dégradation de ces performances.
On suggère que les études récentes effectuées sur l'approche appelée Source Generator Framework produisent un fondement viable pour développer des techniques robustes de reconnaissance de la parole.
L'étude décrite s'articule autour de trois axes corrélés : (i) l'analyse et la modélisation de la parole produite soit sous l'effet de stress du à la charge de travail et/ou à l'émotion, soit dans le bruit, (ii) les méthodes de traitement adaptatif du signal pour le débruitage de la parole et la réduction de l'effet du stress, et (iii) la formulation de nouveaux algorithmes robustes de reconnaissance.
Une analyse statistique d'une base de données (SUSAS) de parole sous stress simulé et réel est présentée.
Cette analyse a été menée sur plus de 200 paramètres relatifs au pitch, à la durée, à l'intensité, à la source glottique et aux variations des spectres du conduit vocal.
Ces études ont motivé le développement de l'approche appelée Source Generator Framework qui permet de modéliser la dynamique de la parole sous stress.
Ce cadre offre des moyens intéressants pour effectuer l'égalisation des paramètres de la parole sous stress.
Dans la seconde moitié de l'article, trois nouvelles approches pour le débruitage de la parole et la réduction de l'effet du stress sont considérées.
La première méthode utilise la technique itérative contrainte (Auto : I,LSP : T) de débruitage et une égalisation par maximum de vraisemblance de la parole à travers la localisation des formants et leurs bandes passantes.
Pour la reconnaissance de mots clés, la seconde méthode utilise un réseau de neurones qui transforme les vecteurs de paramètres de la parole sous stress pendant la phase de paramétrisation.
La dernière méthode applique une technique de rehaussement des paramètres basée sur des contraintes morphologiques pour effectuer le débruitage et utilise un algorithme adaptatif sur les cepstres-Mel pour égaliser les effets du stress.
Les performances de reconnaissance sont données pour la parole produite dans plusieurs conditions de stress, avec plusieurs rapports signal/bruit, et pour différents types de bruit ambiant.
Dans ce papier, nous proposons un algorithme pour la communication entre agents dans le but d'apprendre quoi, à qui et quand communiquer.
Nous nous appuyons pour cela sur l'utilisation d'agents introspectifs capables de raisonner sur leurs actions et sur leurs états afin de construire les actes de communication.
Nous proposons une extension d'algorithmes d'apprentissage par renforcement définis sur les PDMpour la prise en compte de la communication multi-agent.
Nous montrons comment il est possible de résoudre les problèmes liés à l asynchronisme et à la non-markovité des SMA par l'utilisation d'une mémoire des actions et des interactions de l'agent.
L'algorithme hybride DCT/PCM à compensation de mouvement a été adopté avec succès dans plusieurs standards de codage vidéo tels que H.261, H.263, MPEG-1 et MPEG-2.
Toutefois, sa robustesse est mise à l'épreuve en cas d'allocation inadéquate des bits, soit globalement pour la séquence entière, soit localement comme résultat d'une distribution inappropriée des bits disponibles.
Dans l'une ou l'autre situation, le compromis entre qualité et disponibilité des bits a pour résultat une détérioration de la qualité de la séquence vidéo décodée, à la fois en termes de perte d'information et d'introduction d'artefacts de codage.
Ces distortions sont un facteur important dans les domaines du filtrage, de la conception du codec, et de la recherche de métriques de qualité objectives basées sur des concepts psycho-visuels ; de ce fait, cet article présente une analyse approfondie et une classification des nombreux artefacts de codage introduits dans la séquence vidéo reconstruite en utilisant l'algorithme de codage hybride MC/DPCM/DCT.
Les artefacts qui ont déjàété brièvement décrits dans la littérature, tels que l'effet de bloc, le tremblement, l'effet moustique, le mésalignement MC, le flou, et le bavage des couleurs sont analysés en profondeur.
Additionnellement, nous présentons des artefacts ayant des propriétés uniques et qui n'ont pas encore été identifiés dans la littérature.
La prochaine génération de systèmes de synthèse à partir du texte devra être plus sensible aux variations socio-linguistiques.
Dans ce contexte, on a étudié plusieurs paramètres socio-linguistiques qui ont une influence sur la réalisation de la négation dans le langage parlé, en examinant leurs effets sur l'accentuation des négatives lors de la lecture de prose anglaise.
Conformément aux observations faites lors d'une étude antérieure, cette analyse a montré que la prominence intonative n'est pas fréquente dans la lecture de prose et encore moins fréquente dans des dialogues lus.
Les résultats montrent une étonnante absence de conformité avec les prédictions linguistiques 'théoriques'.
Quand nous communiquons par la parole (naturelle), nous ne disons pas explicitement tout.
Tant le locuteur que l'auditeur utilisent des informations liées à la situation de communication qui incluent les états mentaux des deux interlocuteurs.
Des cas intéressants sont fréquemment observés lors de l'usage du Japonais dans des situations de dialogue.
Les contraintes syntaxiques (ou configurationnelles) du Japonais sont plus faibles que celles de l'Anglais : le locuteur japonais peut omettre presque n'importe quel élément de l'enoncé.
Dans cet article, on présente une modélisation de l'auditeur en phase de raisonnement dépendant de la situation e'on montre comment l'information manquante peut être fournie par la situation.
Bien que ce modéle capture, à notre avis, les caractéristiques essentielles de la communication, il est peut-être trop naïf pour servir de modéle de la cognition humaine.
Ce modéle vise plutôt à être utilisé dans l'élaboration d'agents logiciels communiquant entre eux d'une façon mécanique mais flexible et efficace.
L'évidence d'un contrôle syntaxique sur la structuration prosodique d'un énoncé en français a été exploitée dans de nombreux modèles de génération.
Pourtant, des modèles sans connaissances syntaxiques, basés seulement sur des contraintes de type rythmique peuvent générer des structures prosodiques acceptables.
Le modèle présenté ici montre que les stratégies dirigées par la syntaxe et celles dirigées par le rythme peuvent être considérées comme des cas extrêmes d'un modèle plus complexe intégrant à la fois des contraintes syntaxiques et rythmiques.
Ce modèle de génération a été intégré dans divers systèmes de synthèse à partir de texte.
Des tests perceptifs comparatifs ont montré l'amélioration de qualité par rapport à un système guidé par une connaissance syntaxique élémentaire.
Cet article décrit l'implémentation d'un nouveau modèle paramétrique de la géométrie de la glotte qui a pour but d'amélioration, la synthèse de voix masculines et féminines dans le cadre d'une analyse/synthèse articulatoire.
Ce modèle est inclu dans un système d'analyse/synthèse articulatoire visant une simulation articulatoire de la parole.
Pour introduire dans les ondes synthétiques du flux glottique des détails que l'on observe au naturel, deux types de fuite différents ont été modélisés : la fuite couplée et la fuite parallèle.
Alors que la première correspond, en principe, à une fermeture incomplète de la glotte, la deuxième correspond à la modélisation d'un conduit glottique supplémentaire, indépendant de la partie membranique (vibratoire) de la glotte.
Ces deux types de fuite ont pour trait caractéristique commun d'augmenter le flux DC et l'interaction source/conduit vocal.
Toutefois, une fuite couplée produit une pente plus forte du spectre du flux glottique, alors qu'une fuite parallèle réduit l'énergie plus fortement dans les basses fréquences que dans les hautes fréquences.
En fait, pour une fuite parallèle, la pente dans les hautes fréquences est à peu près la même que celle qu'on observe dans les cas sans fuite.
Dans les systèmes de reconnaissance de la parole, la robustesse à l'environnement par adaptation des paramètres peut être obtenue de deux façons complémentaires.
Une première approche consiste à modifier les paramètres acoustiques de la parole dégradée par l'environnement de façon à ressembler aux paramètres de la parole (habituellement non dégradée) qui a été utilisée lors de l'entraı̂nement.
La deuxième solution est de modifier les paramètres statistiques internes au reconnaisseur de façon à mieux représenter les caractèristiques de la parole dégradée dans un environnement cible particulier.
Le présent papier tente d'unifier ces deux approches de reconnaissance robuste de la parole en présentant plusieurs techniques qui partagent les mêmes hypothèses de base et la même structure, tout en différant dans le choix de savoir si elles modifient les paramètres d'entrée ou les paramètres statistiques du reconnaisseur.
Nous présentons ici la famille d'algorithmes basés sur la normalisation cepstrale gaussienne multi-variable (RATZ) qui modifient les caractéristiques cepstrales d'entrée, ainsi que les algorithmes STAR (re-estimation statistique), qui modifient les paramètres internes du reconnaisseur.
Les deux types d'algorithmes sont basés sur les données et utilisent une certaine quantité de donnée d'adaptation pour estimer les paramètres de compensation.
Les algorithmes ont été évalués en utilisant le système de reconnaissance SPHINX-II sur un sous-ensemble de la base de donnée Wall Street Journal.
Bien que tous les algorithmes conduisaient une amélioration des performances en comparaison des algorithmes précédents, la famille d'algorithmes STAR donnait généralement des taux d'erreur plus faibles que la famille d'algorithmes RATZ lorsque le rapport signal/bruit diminuait.
Le matériel décrit et les résultats d'essais présentés sont ceux de l'équipement testé dans le cadre du concours européen qui s'est déroulé à Turin en Italie.
Ce codeur-décodeur offre une bonne qualité vocale et plusieurs avantages importants comme par exemple un faible temps de propagation, une faible complexité des calculs et une bonne tolérance en présence d'erreurs de transmission.
Ce codeur-décodeur fait appel à un algorithme de codage en sous-bandes, à 8 bandes, avec quantification et prédiction, toutes deux adaptatives vers l'arrière.
Le matériel dont la mise en ocuvre est décrite fait appel à une paire de dispositifs de traitement des signaux numériques, ce qui lui permet d'être très compact.
Ce document récapitule également le résultat des essais subjectifs et objectifs.
Cet article concerne les relations prosodiques perçues dans la prose, dans la poésie et dans la musique, en mettant l'emphase sur les caractéristiquesde durée.
Pour approfondir notre compréhension de la prosodie du langage parlé, nous nous sommes pour l'instant attachés à comparer les relations temporelles observables dans des activités telles que la lecture de la poésie et l'exécution musicale, activités où l'on observe habituellement une structure rythmique très marquée des séquences de sons produites.
On peut aussi trouver des parallèles très intéressants en comparant les notations formelles de la prose, de la poésie et de la musique.
En général, il n'y a aucune relation simple entre la notation formelle et l'exécution et, de plus, les systèmes de notation ont varié avec la tradition et suivant des besoins spécifiques.
Pourtant, il y a là un défi : essayer de lier plus étroitement les systèmes descriptifs aux contraintes humaines de production et de perception des sons.
Cet article présente une méthode basée sur la décomposition harmonique du spectre de Hildebrand–Prony.
Cette analyse spectrale de Hildebrand–Prony est appliquée pour sa haute résolution et sa précision.
Des tests comparatifs avec des indices LP et LP-cepstraux ont été réalisés sur 50 sujets provenant de la base de données slovène SNABI (corpus de mots isolés) et sur 50 sujets allemands provenant de la base de données BAS Siemens 100 (phrases).
Sur ces deux bases de données les avantages des indices harmonique ont été observés surtout pour l'identification des locuteurs.
Cet article présente une méthodologie pour quantifier la distorsion apportée par un codeur de parole à bas ou moyen débit.
Puisque c'est l'acuité perceptive de l'être humain qui fixe la précision avec laquelle on doit traiter le signal de parole, celui-ci est transformé en une représentation perceptive;
on utilise pour cela le modèle cochléaire (auditif) de Lyon, dont les sorties représentent la probabilité d'excitation des fibres nerveuses à un instant donné.
Nous utilisons dans ce travail un modèle de Markov caché pour modéliser le processus élémentaire d'excitation/non excitation opératoire dans le système auditif.
Un modèle d'ordre un, à deux états et complètement connecté est associé à chaque canal neuronal ; les deux états du modèle représentent les événements d'excitation et de non-excitation.
En supposant les modèles stationnaires sur une durée fixe, leurs paramètres sont calculés à partir des représentations perceptives du signal original.
Ensuite, les représentations perceptives de la parole codée passent à travers les modèles correspondants et les probabilités associées sont calculées.
Ces scores permettent de définir une mesure de distorsion à partir d'une “cochlée markovienne caché” (CHM).
Cette méthode prend en compte la succession temporelle des profils de l'excitation neuronale.
La mesure CHM, qui prend en compte l'information contextuelle présente dans le profil d'excitation, est robuste vis à vis du délai de codage.
Nous présentons ici un prototype complet et opérationnel intégrant la compression et la reconnaissance de gestes dansés issus d'un ballet contemporain.
Les données traitées sont des trajectoires de mouvement suivies par les articulations d'un corps dansant.
Nous proposons un outil efficace pour le sous-échantillonnage non uniforme de signaux spatio-temporels.
Notre approche utilise une approximation polygonale des contours pour construire une représentation compacte et efficace des trajectoires de mouvement.
Notre méthode de reconnaissance de gestes dansés repose sur un ensemble de Modèles de Markov Cachés (MMC) chacun étant associé à la trajectoire d'un marqueur.
Nous avons validé notre système de reconnaissance sur 12 mouvements de base effectués par 4 danseurs d'un ballet contemporain.
Les techniques de recherche d'information s'appuient sur l'extraction de termes dans les documents, termes qui servent de base pour l'accès à ces documents.
Nous proposons dans cet article une approche pour permettre une extraction plus riche sémantiquement en intégrant des connaissances issues d'un thesaurus et de corpus de domaine.
Plus spécifiquement, nous proposons une méthodologie visant à transformer un thesaurus préexistant en une ontologie légère de domaine qui sera utilisée pour indexer sémantiquement une collection de documents.
Nous proposons également des techniques assurant cette transformation et une évaluation dans le domaine de l'astronomie.
Nous rapportons les résultats d'une étude EPG que montrent qu'on ne peut interpréter l'organisation des gestes articulatoires comme la simple concanténation de segments assimilés.
Entre 1399 et 1400, Christine de Pizan rédige son Epistre Othea.
Soixante ans plus tard, Jean Miélot rédige un remaniement de cette œuvre à succès pour la cour des ducs de Bourgogne.
Outre une mise au point sur les véritables particularités de Jean Miélot sur ses contemporains tels Jean Wauquelin ou David Aubert, le présent article entend contextualiser à la fois le texte-source et le texte cible, pour tenter de répondre à la question de l'émergence du remaniement de Miélot, mais aussi des possibles explications de son absence de diffusion.
Au final, on constate que l'approche traditionnelle, textuelle et philologique, ne rend pas compte entièrement de la réception d'un texte dans un contexte, et que celle-ci doit dès lors se doubler d'une approche factuelle et contextualisée.
Les caractéristiques spectrales sont décrites par les vecteurs cepstraux et les vecteurs cepstraux normalisés ; leur vitesse de variation par les vecteurs des coefficients orthogonaux de premier ordre.
Ces vecteurs ont été employés, soit séparément, soit unifiés dans un seul vecteur, dans des procédures de vérification qui se basent sur un algorithme DTW.
Les expériences effectusées sur une population de 22 locuteurs (conditions acoustiques favorables), ont montré qu'en éliminant de la phrase d'essai la partie du spectre constante dans le temps, ce qui a lieu explicitement par la normalisation cepstrale et implicitement par le calcul des coefficients orthogonaux de premier ordre, on obtient une réduction des taux d'erreurs.
De plus, la performance de la vitesse de variation des caracteéristiques spectrales est comparable à celle des caractéristiques spectrales elles-mêmes, tandis qu'une combinaison des deux types d'information spectrale ne donne pas d'améliorations ultérieures.
La communication présente un modéle pour l'analyse du signal de parole fondé sur les aspects déjà connus du système auditif périphérique.
L'objectif principal de ce travail est de formuler des transformations de signal qui préservent et accentuent les caractéristiques du signal de parole indispensables a la perception.
On présente un modèle d'ordinateur du système auditif périphérique qui incorpore les phénomènes de synchronisation, de suppression à deux tons, et d'adaptation additive.
Les recherches récentes sur la physiologie de l'audition ont mis en lumière l'importante contribution de ces phénoménes à la perception normale de la parole.
Le modéle proposé peut accentuer les régions spectrales dynamiques telles qu'on les trouve dans la parole conversationnelle normale.
Cet article présente des méthodes d'évaluation de la qualité pour les postes téléphoniques mains-libres, à partir de différents exemples.
Des résultats de mesures “instrumentales” (objectives) obtenus avec les différents postes mains-libres testés sont présentés, avec les méthodes de mesure correspondantes.
Nous présentons dans cet article une stratégie de recherche génétique pour un moteur de recherche.
Nous commençons par montrer que des relations importantes existent entre les études statistiques des propriétés du Web. Les moteurs de recherche fondés sur les approches à base d'agents, et les techniques utilisées classiquement en optimisation : le Web est un graphe qui peut être exploré à l'aide d'une fonction d'évaluation et d'opérateurs fondés sur la création ou l'exploration locale.
Il devient alors possible de définir une fonction d évaluation qui est une formulation mathématique de la requête de l'utilisateur et de définir un algorithme génétique qui fait évoluer une population de pages avec des opérateurs spécifiques.
La création d individu consiste à interroger des moteurs classiques.
La mutation consiste à explorer le voisinage d'une page grâce à ses hyperliens.
Nous présentons des résultats comparatifs obtenus avec un protocole de tests directement calqués sur ceux utilisés en optimisation.
Cet article présente un modèle pour la création d'un agent synthétique pouvant exprimer des performatifs à l'aide d'expressions faciales.
Le performatif d'un acte parlé ou d'un acte communicatif est l'intention communicative particulière que celui qui parle a envers son interlocuteur, la relation sociale que celui-ci veut établir avec celui-là.
Les performatifs sont décomposés suivant leur signification et leur signal : du point de vue de la signification, un performatif est représenté par un ensemble d'unités cognitives, qui à leur tour incluent des sous-structures sur le but général de celui qui parle (informer, demander, solliciter), sur la relation de pouvoir entre celui qui parle et celui qui écoute, sur l'état émotif du premier, et sur d'autres informations particulières liées à des performatifs spécifiques ; pour le signal, les expressions faciales sont décomposées en Unités d'Action.
Le système proposé ici calcule le performatif approprié pour un acte communicatif en considérant le contexte de la communication, plus particulièrement la capacité cognitive, les relations sociales et la personalité de l'interlocuteur. Ensuite le système exprime les performatifs calculés par des expressions faciales.
L'article se centre sur les différences vocaliques entre parole spontanée et parole de laboratoire en espagnol.
Les premier et second formants de 954 réalisations vocaliques ont été mesurés (477 en parole de laboratoire et 477 en parole spontanée).
Ils constituent des agrégats dans l'espace F 1/F 2.
L'article décrit les variabilités inter et intra agrégat induites par le changement de situation de communication.
En parole spontanée, les valeurs formantiques présentent (1) une tendance marquée vers schwa ; (2) une augmentation de la variabilité intra cluster.
Les deux phénomènes produisent une diminution de la différenciation des sons en parole spontanée.
Un système automatique de reconnaissance du locuteur, indépendant du texte, est présenté.
Le système est basé sur la localisation de voyelles dans la parole testée, l'extraction des vecteurs de paramètres et leur classification en utilisant un ensemble de références dépendant du locuteur.
Cet ensemble contient L prototypes par locuteur, qui représentent les voyelles de la langue.
Les prototypes sont formés par l'application d'un algorithme “k-means” à L groupes aux vecteurs de voyelles extraits du texte d'apprentissage du locuteur.
Le système a été testé pendant quatre mois sur une population de 15 locuteurs masculins et féminins et avec des textes non corrélés aux textes d'apprentissage.
L'exactitude mesurée du système (91.39% pour la vérification, 90.19% pour l'identification dans un ensemble fermé de locuteurs et 92.28% dans un ensemble ouvert) est satisfaisante en considérant que les énoncés de l'apprentissage ont une durée inférieure à 50 sec et les énoncés testés une durée moyenne de 1.3 sec.
L'exactitude est significativement accrue quand on augmente la dureé de l'énonce testé (p.e. 93.75% pour la vérification avec des énoncés de 4 sec de moyenne).
D'autres avantages du système sont la petite mémoire exigée et sa rapidité.
Il s'agit de déterminer l'appartenance effective de points de contour obtenus par une méthode de segmentation par contour actif au cortical.
Plusieurs paramètres associés à chacun de ces points sont pris en compte (niveau de gris, niveau de gris moyen et écart type sur un voisinage, distance entre points appartenant à des coupes voisines).
L'architecture est basée sur le formalisme de la théorie de l'évidence.
Nous discutons des résultats obtenus, de leur validité et nous donnons les perspectives envisagées de la suite de ce travail.
L'analyse temps-fréquence des signaux magnétiques, générés par des objets ferromagnétiques sous-marins, est utilisée afin de trouver un ensemble de paramètres discriminants pour leur classification.
Après l'étape de sélection de caractéristiques, une étude étendue est menée pour comparer différentes structures de classifieurs, en fonction du taux moyen de bonne classification et de la capacité de généralisation.
Si les SVM (Support Vector Machines, ou Séparateurs à Vaste Marge) sont aujourd'hui reconnus comme l'une des meilleures méthodes d'apprentissage, ils restent considérés comme lents.
Nous avons choisi de coder cet algorithme dans l'environnement Matlab afin de profiter de sa convivialité tout en s'assurant une bonne efficacité.
La comparaison de notre solution avec l'état de l'art dans le domaine SMO (Sequential Minimal Optimization), montre qu'il s'agit là d'une solution dans certains cas plus rapide et d'une complexité moindre.
Nous proposons dans cet article une méthode 3D d'estimation du flot optique conduisant à un recalage non-rigide monomodalité de volumes cérébraux.
La formulation énergétique du problème est enrichie par l'utilisation d'estimateurs robustes.
De plus un schéma d'optimisation efficace, multirésolution et multigrille, est proposé afin d'accélérer la recherche et d'améliorer la qualité de l'estimation.
Les apports de cette méthode sont démontrés sur des données réelles et ses performances sont quantitativement évaluées sur des données simulées.
A partir d'un corpus de logatomes de type CVCVCV où la consonne est [b] ou [m] et la voyelle [a i u], nous avons étudié le comportement des muscles orbicularis oris superior et levator veli palatini.
L'importance de la préplanification globale de la séquence (rôle de la position initiale servant de référence) et du rééquilibrage intrasegmental du timing a étéévaluée dans le cadre d'une théorie de l'encodage moteur basée sur les notions de “sequencing” et de “phasing”.
D'autre part, nos données confirment que la synchronisation musculaire constitue bien une règle de base.
Dans de nombreux domaines (biologie, médecine, psychologie, etc.), des outils de fouille de textes efficaces permettraient d'économiser un temps de travail énorme.
Afin d'avoir un outil utilisable par des spécialistes du domaine, ce dernier doit couvrir les différentes étapes de la fouille de textes.
L'approche que nous proposons dans cet article consiste à extraire les règles d'associations propres au domaine à partir d'un ensemble de textes homogènes spécialisés.
Notre approche est composée de différentes étapes dans lesquelles l'expert du domaine joue un rôle essentiel.
La première étape consiste à extraire les termes dans les textes et à les associer à un concept, c-à-d. un ensemble de termes ayant la même sémantique.
En utilisant cette nouvelle connaissance propre au domaine, le corpus initial est réécrit sous forme matricielle.
La dernière étape de notre approche, consiste à discrétiser la matrice obtenue à l'étape précédente afin d'en extraire les règles d'association propres au domaine.
Cette contribution traite le problème de l'apprentissage par renforcement inverse (ARI), défini comme la recherche d'une fonction de récompense pour laquelle le comportement d'un expert (connu par le biais de démonstrations) est optimal.
Nous introduisons SCIRL, un nouvel algorithme qui utilise la grandeur dénommée attribut moyen de l'expert comme la paramétrisation d'une fonction de score pour un classifieur multiclasse.
Cette approche donne une fonction de récompense pour laquelle la politique de l'expert est (nous le démontrons) quasi optimale.
Contrairement à la plupart des algorithmes d'ARI existants, SCIRL n'a pas besoin de résoudre le problème direct de l'apprentissage par renforcement.
De plus, en utilisant une heuristique, il fonctionne avec uniquement des trajectoires échantillonnées par l'expert.
Nous illustrons cela sur un simulateur de conduite.
Cet article introduit une nouvelle méthode de tatouage pour la protection d'images fixes.
La méthode permet de cacher une signature dans une image, sous la forme de w paquets de r bits.
Le schéma de tatouage est additif, et la marque elle-même est calculée par addition de produits de couples de fonctions orthogonales.
Nous montrerons comment le choix des fonctions orthogonales peut être fait, de façon à rendre le tatouage robuste face à différents types d'attaques.
La structure de base du codeur comporte un prédicteur à long terme qui est implanté comme un dictionnaire adaftatif, tandis qu'un dictionnaire Gaussien creux avec des vecteurs non recouvrants est utilisé pour l'excitation stochastique.
Pour satisfaire les contraintes d'implantation, nous avons adopté plusieurs méthodes de recherche de codes.
Avec ces méthodes, la charge de calcul du codeur de base peut être réduite à 12.4 MIPS, avec un dictionnaire de codes stochastiques à 7 bits.
Nous ètudions une recherche hiérarchique á deux étages dans le dictionnaire adaptatif.
Cette méthode de recherche ŕeduit la charge de calcul, au prix d'une dégradation légére des performances du codeur.
La qualité du codeur CELP est jugée voisine de celle du codeur G.722 à 48 kbit/s.
Sacadeau-Software est un logiciel d'aide à la décision destiné aux agronomes travaillant sur la pollution de l'eau dans les bassins versants et aux personnes en charge de la gestion de ces bassins versants.
Cet outil se focalise sur la maîtrise de la contamination des eaux par les pesticides apportés sur les cultures de maïs.
Il s'appuie sur un modèle incluant la représentation, d'une part, des processus biophysiques de transfert des pesticides à l'échelle d'un bassin versant et, d'autre part, des processus de décision dans le cadre de la culture du maïs.
Sacadeau-Software permet de lancer des simulations des cultures pour toutes les exploitations de l'ensemble d'un bassin versant et d'obtenir le taux de transfert des polluants à l'échelle du bassin versant.
Des règles caractérisant les sous-parties du bassin versant ayant une pollution de l'eau à l'exutoire, et les sous-parties sans pollution, sont inférées automatiquement à partir des simulations effectuées.
Un outil de visualisation permet alors de faire le lien entre les règles apprises et les exemples caractérisés par ces règles.
Enfin, un outil de recommandation d'actions propose, à partir des règles apprises, des actions propres à améliorer une situation de pollution.
Les variations dans la production de parole dues au stress induit contribuent de manière significative à la réduction des performances des systèmes de traitement de parole.
Pour estimer ces variations, une approche consiste à établir une classification objective du stress du locuteur, basée sur le signal acoustique.
Cette étude propose un algorithme pour l'estimation de la probabilité du stress induit.
Le taux de stress prédit par cet algorithme peut être intégré dans des algorithmes de traitement de parole afin d'augmenter leur robustesse dans des environnements difficiles.
Les résultats d'une étude précédente sur la classification du stress sont d'abord utilisés pour sélectionner un ensemble de paramètres de parole relatifs au phonème et au type de stress.
Une analyse des paramètres articulatoires, d'excitation et cepstraux est conduite sur une base de données de parole sous stress (“Speech Under Simulated and Actual Stress” (SUSAS)).
Les paramètres sensibles au stress sont ensuite sélectionnés pour dix conditions de stress (incluant le cockpit d'un hélicoptère Apache, la colère, la parole claire, l'effet Lombard, la voix forte, etc.) et sont incorporés dans un réseau de neurones appris pour classifier le degré de stress.
Sur un ensemble fermé de locuteurs et pour un ensemble ouvert de stimuli de parole, il produit un taux de bonne classification de 91.0%.
Finalement, l'algorithme de classification du stress est incorporé dans un système de reconnaissance de parole où un modèle de Markov est appris pour chaque condition de stress.
Avec cette nouvelle approche de reconnaissance “dépendante du stress”, on obtient une amélioration des performances de 10.1% et de 15.4%, respectivement, par rapport aux systèmes de reconnaissance appris avec de la parole neutre et avec différents styles de parole.
Il est possible de différencier, à l'écoute, la parole lue et la parole spontanée.
La prosodie semble être essentielle pour faire cette distinction.
Dans cet article, on étudie l'importance des distributions et réalisations des frontières prosodiques.
Des monologues spontanés (appelés monologues “d'instruction”), émis par cinq locuteurs masculins, ont été enregistrés.
Les transcriptions de ces monologues ont été lues à haute voix par les mêmes locuteurs.
Une expérience de perception a été menée pour obtenir des scores de classification sur les phrases isolées extraites des énoncés spontanés et lus.
Des transcriptions prosodiques de l'ensemble des monologues spontanés et lus ont permis d'évaluer la distribution et la réalisation des frontières prosodiques sous-jacentes dans les deux types de parole.
La structure prosodique sous-jacente a été estimée en utilisant un systm̀ee automatique de synthèse de parole.
Les différences, entre parole lue et parole spontanée, observées dans la production des frontières prosodiques ont été reliées aux scores de classification perceptive par une analyse de régression multiple.
Cet article décrit un algorithme performant et efficace pour la reconnaissance en parole continue de larges vocabulaires.
II est basé sur un analyseur LR à deux niveaux et utilise des modèles de Markov comme modèles de phonèmes.
Pour améliorer les performances de reconnaissance, il utilise la probabilité des treillis avant et arrière.
Pour améliorer l'efficacité de la recherche, il utilise des fenêtres variables, combine en un seul les candidats qui présentent les mêmes séquences d'allophones et ont le même état grammatical et combine ensuite les candidats au niveau du sens.
Pour évaluer ses performances en mode indépendant du locuteur, cet algorithme a été utilisé dans un système d'accès à un annuaire téléphonique comportant plus de 70 000 noms (environ 80 000 mots).
Pour 8 locuteurs, cet algorithme fournit un taux de 65% de compréhension de la parole spontanée.
Ces résultats montrent que le système est efficace malgré la grande perplexité du vocabulaire.
Cet article décrit également un système de dialogue multi-modal qui utilise ce système de reconnaissance de grands vocabulaires.
L'utilisation de transformations du signal est une étape nécessaire pour l'extraction de traits dans des systèmes de reconnaissance des formes.
Ces transformations doivent prendre en compte le but pricipal de la reconnaissance des formes : la minimisation du taux d'erreur.
Dans cet article, nous proposons une nouvelle méthode pour obtenir des transformations de l'espace des traits, basée sur le critère de minimisation de l'erreur de classification.
Le but de ces transformations est d'obtenir un nouvel espace de représentation dans lequel la distance euclidienne est optimale pour la classification.
La méthode proposée est testée dans le cadre d'un système de reconnaissance de parole utilisant divers types de Modèles de Markov.
La comparaison avec des techniques de prétraitement standard montre que notre méthode fournit une réduction du taux d'erreur dans tous les contextes expérimentaux étudiés.
Nous étudions plusieurs méthodes pour accomplir la deuxième tàche, en insistant sur les avantages et désavantages d'une approche par diphones basée sur la prédiction linéaire.
Les diphones nécessitent plus de mémoire ordinateur afin de representer toutes les transitions possibles eentre couples de phonèmes, mais ils incorporent en grande partie les effets de coarticulation qui sinon devraient étre modélisés lors de la synthèse phonémique.
Les règles d'interpolation sont simples parce qu'au niveau spectral les frontières entre diphones sont similaires.
Cet article traite de la prédiction de la qualité vocale transmise par une ligne téléphonique.
On commence par présenter une définition du terme `qualité' qui prend en compte aussi bien des facteurs de communication que des facteurs relatifs au service téléphonique, et on propose un nouveau schéma de classification.
Ce schéma considère les paramètres d'entrée et de sortie du modèle, les composants du réseau considérés et les domaines d'applications pour lesquels un modèle est utilisé, ainsi que des données psychoacoustiques et de jugement.
Selon ce schéma, les modèles de prédiction de la qualité peuvent être classifiés sous trois classes : les mesures comparatives à base de signaux, les modèles de planification de réseau, et les modèles de surveillance de réseau.
Les mesures comparatives étant discutées amplement dans la littérature, on se limite aux deux dernières types de modèle.
Les bases psychoacoustiques des deux modèles de planification les plus connus (le modèle E et le modèle SUBMOD) sont discutées en détail, et – en combinaison avec d'autres modèles – utilisées pour développer des approches de surveillance.
On discute des extensions potentielles de ces modèles, qui incluent la transmission à large bande, la modélisation de la qualité de la voix transmise, les prédictions pour les perturbations non-stationnaires, ainsi que l'application des modèles à la prédiction des effets de la transmission téléphonique sur la reconnaissance et la synthèse vocale.
Nous proposons une méthode de discrimination non paramétrique conçue pour favoriser l'interprétabilité de la prédiction.
D'une part, l'utilisation d'un modèle additif généralisé permet de représenter graphiquement l'effet de chaque variable d'entrée sur la variable de sortie.
D'autre part, les paramètres de ce modèle sont estimés par vraisemblance pénalisée, où le terme de régularisation généralise la pénalisation h aux fonctions splines.
Cette pénalisation favorise les solutions parcimonieuses sélectionnant une partie de l'ensemble des variables d'entrée, tout en permettant une modélisation flexible de la dépendance sur les variables sélectionnées.
Nous étudions l'adaptation de différents critères de sélection analytiques à ces modèles, et nous les évaluons sur deux jeux de données réelles.
Cet article présente un aperçu des recherches entreprises au Japon, relatives à l'information individuelle véhiculée par l'onde de parole.
Alors que les corrélats physiques des traits perceptifs de l'identité de la voix ont été étudiés du point de vue psychologique, la recherche menée du point de vue de l'ingénieur se rattache à la reconnaissance automatique du locuteur, à la reconnaissance de la parole indépendante du locuteur, et aux algorithmes d'apprentissage en reconnaissance de la parole.
La recherche en reconnaissance du locuteur peut être cataloguée en deux classes, selon que le texte est prédéterminé ou non.
Cependant, il a été mis en évidence que, même si le texte n'est pas prédéterminé, l'information individuelle dépendante du texte peut être utilisée et ce, sur base de la reconnaissance explicite ou implicite du phonème.
Divers exemples de méthodes de reconnaissance du locuteur sont classées ici selon ces variantes et leurs performances sont présentées.
En particulier, cet article éclaire la variabilité à long terme des paramètres intra-locuteur comme l'un des problèmes les plus cruciaux en reconnaissance du locuteur.
En plus, cet article présente une étude des méthodes destinées à réduire les effets de la variabilité spectrale à long terme sur la précision de la reconnaissance.
Cet article présente un algorithme d'apprentissage non supervisé par chaînes de Markov cachées (CMC) et algorithmes génétiques (AG).
Deux des problèmes rencontrés lors de l'utilisation des CMC sont de déterminer les probabilités de la CMC et le nombre d'états de cette chaîne.
Bien souvent, ce nombre d'états est déterminé soit par expériences successives, soit à l'aide de connaissances a priori du domaine.
L'algorithme présenté ici emploie un algorithme génétique afin de déterminer le nombre d'états cachés de la CMC ainsi que les différentes probabilités qui la constituent.
Cet algorithme est couplé à l'algorithme de Baum-Welch qui permet une réestimation efficace des probabilités de la CMC.
Différents algorithmes, hybrides ou non, sont comparés entre eux sur une application d'apprentissage et de reconnaissance d'images représentant des visages.
Les résultats montrent la supériorité de l'approche génétique pour ce type de problème.
Un algorithme hybride de codage, consistant en une prédiction intertrame compensée en mouvement et en une quantification vectorielle adaptative pour le gain et la forme, est proposé pour le codage vidéo à faible débit.
Un système de codage vidéo utilisant l'algorithme proposé et ces caractéristiques de codage sont également décrits.
La prédiction intertrame compensée en mouvement est une technique efficace pour réduire la redondance temporelle contenue dans les images en mouvement.
Dans la méthode proposée, des blocs d'amplitude constante (blocs de niveaux de gris) sont aussi inclus dans les vecteurs cherchés pour augmenter l'efficacité de codage pour des mouvements rapides d'objects ou de changement de scène.
La quantification vectorielle adaptative pour le gain et la forme, avec un livre de code à recherche arborescente, est utilisée pour coder les signaux de différences intertrames compensés en mouvement.
Elle peut correspondre aux changements de statistique de la source car le livre de code est indépendant des composantes de gain des vecteurs d'entrée.
L'algorithme de codage proposé a été évalué par simulation sur ordinateur et apparaît efficace pour la transmission de vidéo à faible débit.
Un système de codage de vidéo à 64 kbit/s basé sur cet algorithme de codage a été implanté.
Le système de codage développé peut transmettre des données numériques, de l'audio et du vidéo multiplexé à la cadence de base du RNIS et peut être appliqué au vidéotéléphone aussi bien qu' à la vidéoconférence.
Cette introduction a pour but d'élaborer le cadre général auquel appartiennent les articles rassemblés ci-dessous.
Elle s'adresse à deux problèmes importants dans l'étude du traitement lexical : l'identification des différentes étapes dans la reconnaissance de mots et la caractérisation des divers types d'influences contextuelles sur ces étapes.
Nous essayons de décomposer les processus de reconnaissance de mots en plusieurs étapes ayant des conséquences aux niveaux théorique.
Nous adoptons également une approche analytique en traitant des influences dues au contexte, en distinguant plusieurs types de contexte (lexical, intra-lexical, syntaxique, sémantique et interprétatif).
Une telle démarche est nécessaire si nous voulons rendre explicite le rapport entre tel ou tel type d'information contextuelle et l'étape oùs'exerce son influence.
Le cadre général de ce travail est celui de l'analyse et de la synthèse de la parole par ordinateur.
Le signal de parole peut être scindé en deux composantes principales : (1) une composante périodique (constituée des éléments quasi-périodiques (ou voisés) produits par une vibration quasi-régulière des cordes vocales) ; (2) une composante apériodique ou de bruit (constituée des éléments de nature aléatoire pouvant survenir durant un son voisé (i.e. bruit fricatif dans le phonème /v/) ou en l'absence de vibration des cordes vocales (i.e. bruit fricatif dans /s/, /t/, etc.)).
Le but de ce travail est d'apporter une contribution à une modélisation précise de cette seconde composante et notamment des signaux de bruits modulés.
Tout d'abord, une méthode de synthèse s'inspirant du bruit de grenaille, est introduite.
Cette technique consiste à utiliser des processus ponctuels aléatoires qui définiront des instants d'occurrence d'événements spectraux (représentés par des Formes d'Ondes Formantiques ou FOF).
Puis, s'appuyant sur un support théorique (représentation de Rice, théorie de la modulation aléatoire), un algorithme d'analyse/synthèse est proposé.
Des tests de perception ont montré que cette méthode permet d'obtenir des signaux synthétiques jugés très naturels.
De plus, cette approche apporte de nombreuses possibilités pour la modification de la qualité vocale (modifications temporelles, effort vocal, etc.).
Notre étude associe une approche linguistique et psycholinguistique du phénomène « métaphore » basée sur des énoncés spontanés d'enfants de 2-4 ans ainsi que sur des productions d'adultes dans des textes scientifiques pour large public.
Nous voudrions montrer en quel sens ces énoncés, généralement considérés soit comme « déviants » soit comme « ordinaires » , peuvent apporter un éclairage à la fois de la structuration du lexique des verbes chez le jeune enfant et l'organisation du lexique des verbes chez l'adulte.
Concernant les énoncés produits par les jeunes enfants, nous développons des arguments qui vont à l'encontre des notions d' « erreur » ou de « métaphore » qui sont quasi systématiquement investies.
On propose aussi une extension de cette formulation afin d'obtenir une modélisation optimale des variations de la prononciation.
Puisque de différents mots n'exposent pas, en général, le même degré de variation de la prononciation, cette méthode permet une représentation des mots par un nombre varié d'entrées lexicales.
La méthode améliore la description d'unités de parole des mots du vocabulaire, chose qui a démontré une amélioration de la performance de la reconnaissance en ce qui concerne la tâche de la DARPA Resource Management.
La reconnaissance automatique de textes saisis à l'aide d'une tablette à digitaliser ouvre la voie à une nouvelle génération d'ordinateurs « nomades » , dépourvus de clavier comme de souris et dialoguant avec l'utilisateur à l'aide d'un outil que ce dernier maîtrise depuis son plus jeune âge : le stylo.
Dans cet article nous étudions différents moyens de coopération entre un analyseur de contexte, et deux expert- classifieurs traitant les données de manières différentes.
Les résultats obtenus sur une base de données de 7 000 lettres et 12 scripteurs sont encourageants puisqu'ils permettent une amélioration globale des performances de 20 %, portant le taux de reconnaissance global à 84,2 % en lettres et 64 % en mots.
Cet article tente d'apporter quelques éléments de réponses à la question de savoir quels procédés formels et sémantiques les Camerounais mettent en œuvre pour créer des appellatifs et quelles fonctions les appellatifs (créés) remplissent dans la gestion des relations sociales.
Parmi les méthodes de débruitage du signal de parole, la règle d'atténuation spectrale d'Ephraı̈m et Malah (EMSR) a prouvé son efficacité à réduire efficacement le niveau du bruit de fond tout en se prévenant d'un artefact couramment rencontré : le bruit musical.
Afin de prendre en considération certains facteurs psychoacoustiques, une réalisation de la méthode EMSR sur une échelle de fréquence perceptuellement pertinente est présentée.
Cette méthode utilise un banc de filtres non-uniforme et sans décimation.
Cependant, le découpage en fréquence réalisé est uniforme sur l'échelle de fréquence ERB.
Une comparaison objective et subjective a été réalisée entre cette méthode, l'implémentation classique de l'EMSR et une implémentation en banc de filtres uniformes avec échantillonnage critique.
De nouvelles techniques sont décrites ayant pour objectif la reconnaissance automatique du locuteur à partir de parole transmise par téléphone.
La reconnaissance se fonde sur une analyse spectrale de phrases-code ayant une durée fixe.
A partir de l'énoncé complet, une séquence de spectres à court terme est extraite de manière à former un spectrogramme évolutif.
Des procédures de normalisation tenant compte des distorsions spectrales introduites par la ligne téléphonique et des variations d'amplitude sont ensuite appliquées.
On aboutit à l'identité du locuteur en comparant le spectrogramme d'un échantillon de l'énoncé avec des spectrogrammes de référence et en calculant une mesure de dissimilitude entre eux.
Pour effectuer ces comparaisons, il est nécessaire de tenir compte des différences de débit et de synchroniser avec précision les phénomènes phonétiques qui se correspondent.
L'alignement temporel s'effectue au moyen d'un algorithme de programmation dynamique qui minimise les différences temporelles entre les segments de parole correspondants.
Pour l'évaluation du système, on a utilisé un ensemble de phrases prononcées par des locuteurs coopératifs et transmises via des lignes téléphoniques conventionnelles.
Différentes versions de la procédure de reconnaissance ont été testées et comparées entre elles.
Tant pour l'identification que pour la vérification des locuteurs, on a obtenu des taux d'erreur de 2% et moins.
On présence ici un panorama des recherches en prosodie menées au Département de Linguistique et Phonétique de l'Université de Lund depuis 1950.
Il montre que la question des accents de mots a été un moteur essentiel dans le dévelopment de ces travaux.
L'intérêt s'est d'abord focalisé sur les configurations de Fo liées aux accents distinctifs et à leurs variations contextuelles et dialectales ainsi qu'aux indices perceptifs permettant leur identification.
Ensuite, l'analyse des aspects plus globaux a conduit à l'élaboration d'un modèle compositionnel de la prosodie du Suédois dans lequel les contours d'accents locaux sont considérés comme superposés à l'intonation globale.
On indique ensuite certains domaines de recherche négligés, comme la perception et les effets possibles de règles générales d'économie.
Cet exposé se termine par un appel à la construction d'un cadre plus unifié pour l'analyse prosodique.
Quelques 550 segments vocaliques ont été extraits d'un texte lu par un locuteur néerlandais à deux débits, normal et rapide.
La durée de chaque segment est mesurée ainsi que les caractéristiques statiques et dynamiques des formants, telles que les fréquences au centre des formants, les descriptions des évolutions de formants utilisant 16 points équidistants par formant ou des fonctions polynomiales de Legendre.
Ces caractéristiques formatiques ont été examinées en fonction de la durée des voyelles mais aucune indication d'“undershoot” dépendant de la durée n'a été mis en évidence.
Par contre, ce locuteur montrait un comportement de coarticulation dépendant des consonnes très consistant et adaptait son style d'élocution au débit de manière à toujours atteindre la même valeur fréquentielle au centre des formants.
Différentes évolutions formantiques (stylisées par des paraboles) ont été synthétisées pour diverses durées, en isolation ou dans des contextes CVC puis présentées pour identification à des auditeurs.
Les décalages nets dans les réponses vocaliques, par rapport aux stimuli stationnaires, ne montraient aucune indication d'“overshoot” perceptif.
Une méthode de movennage pondéré attribuant le plus grand poids aux fréquences formantiques de la partie finale des voyelles rend mieux compte des résultats.
Les tests ont été réalisés sur des signaux obtenus à partir d'environnements acoustiques simulés ou réels et avec différents rapports signal/bruit (SNR).
Le traitement étudié vise à tirer profit des canaux binauraux d'entrée pour effectuer l'annulation du bruit.
Les deux signaux à large bande de fréquence sont d'abord décomposés en sous-bandes distribuées linéairement ou selon une caractéristique cochléaire, et sont ensuite traités selon les caractéristiques des signaux sous-bandes.
Les resultats de tests d'intelligibilité sont discutés dans lesquels les données de parole et de bruit, provenant de conditions simulées ou réelles, sont présentées à des volontaires selon différents rapports signal/bruit, différentes distributions en sous-bande et différents espacements des sous-bandes.
Les résultats obtenus sur des environnements acoustiques simulés ou réels prouvent que l'approche MMSBA améliore de façon significative à la fois le SNR et l'intelligibilité.
L'article décrit une méthode d'acquisition de connaissances morphologiques constructionnelles (dérivationnelles) à partir de dictionnaires de synonymes.
Cette méthode, destinée à la création semi-automatique de bases de données constructionnelles, exploite de différentes manières la structure paradigmatique du lexique.
Elle repose sur l'identification de quadruplets analogiques (morpho-synonymiques) qui permettent de croiser des contraintes sémantiques définies au moyen de relations synonymiques et des contraintes morphographiques.
Elle a été utilisée avec succès pour des dictionnaire de synonymes français et dictionnaires.
Nous proposons en outre un typage des quadruplets morpho-synonymiques qui rend explicite le fait que certains couples de lexèmes sont soumis à davantage de contraintes que d'autres.
Les surfaces de type papier, lorsqu'elles ne présentent pas de pli franc, sont mathématiquement décrites par des surfaces développables.
Ces dernières sont difficiles à paramétrer de manière minimale car le nombre de degrés de liberté significatif dépend de la déformation.
Les modèles existants sont incomplets ou dépendent de grands jeux de paramètres redondants.
Notre première contribution est un modèle génératif contrôlé par un jeu quasi-minimal de paramètres intuitifs.
Le principe est de plier une surface plane autour de règles de guidage.
Notre deuxième contribution est un algorithme d'estimation du modèle proposé à partir de plusieurs images.
Tout d'abord, les caméras et une structure 3D éparse de la surface de l'objet sont reconstruites.
Une paramétrisation 2D de ces points est ensuite calculée par une méthode non-linéaire de réduction des dimensions.
Cette paramétrisation est essentielle pour évaluer la courbure d'une surface passant par les points reconstruits, nécessaire à l'initialisation des paramètres du modèle.
Enfin, un ajustement de faisceaux ajuste les paramètres du modèle afin de raffiner la surface en minimisant l'erreur de reprojection.
Plusieurs expériences ont été produites afin d'étudier et de compenser pour les variations à même un locuteur lors de la vérification du locuteur.
Afin de d'encourager les variations à même un locuteur, un logiciel stimulant un tel comportement parlé a été créé.
À l'aide de ce logiciel, une base de donnée contenant 50 locuteurs avec variations volontaires et involontaires a été enregistrée.
Cette base de donnée a été utilisée pour analyse acoustique et pour tests de vérification automatique du locuteur (VAL).
Les variations volontaires de la parole sont utilisées comme ensemble d'inscription au système de VAL.
Un tel ensemble est appelé entraı̂nement structuré et est comparé à un entraı̂nement neutre consistant seulement de parole normale.
Les deux ensembles contiennent le même nombre de phrases.
Il est montré qu'en testant sur un style de parole mixte, la performance du VAL système est ameliorée sans que la performance de tests avec parole normale ne dégrade.
Cet article décrit le protocole expérimental retenu par les participants au projet européen SAM (Projet ESPRIT no. 2589 : Multilingual Speech Input/Output : Assessment, Methodology and Standardisation) pour l'évaluation d'intelligibilité des synthétiseurs de parole au niveau de la phrase.
Le SUS test consiste à mesurer l'intelligibilité moyenne d'un jeu de “phrases sémantiquement imprédictibles” (Semantically Unpredictable Sentences), générées aléatoirement à partir de quelques lexiques des mots minisyllabiques les plus fréquents dans chaque langue, selon cinq structures syntaxiques élémentaires.
L'avantage de ce corpus de phrases est de ne pas être figé, puisque les mots sont extraits aléatoirement des lexiques pour former de nouvelles phrases à chaque nouveau test.
De nombreux synthétiseurs de parole à partir du texte ont été évalués, au moyen de ce test, dans plusieurs langues.
Les résultats obtenus ont montré que le SUS test est efficace et permet des comparaisons fiables entre synthétiseurs à condition de respecter un certain nombre de contraintes dans la définition et le déroulement du test lui-même.
Ce sont les recommendations issues de l'expérience accumulée pendant le projet SAM et au-delà qui sont présentées ici, de façon à ce que les utilisateurs de ce test puissent se servir d'un outil de mesure “standardisé”.
Cet article aborde le problème de l'extraction automatique de caractéristiques pour la classification de signaux et de textures.
De plus, un nouvel algorithme de résolution est proposé pour traiter ce problème.
Enfin, notre approche a été testée sur des exemples de signaux et de textures et comparée à des méthodes de l'état de l'art avec des résultats compétitifs sur des jeux de données de textures.
Cet article met en question la simple notion d'analogie, entendue comme entièrement basée sur les associations phonologiques, ainsi que l'idée selon laquelle les effets de fréquence sont incompatibles avec l'opération de règles symboliques telles qu'elles sont appliquées dans l'étude des traitements morphologiques.
Il discute les résultats de trois expériences sur le traitement de morphologie inflexionnelle complexe effectué par des adultes russes et des adultes américains apprenant le russe comme langue étrangère - deux expériences sur la génération de verbes nouveaux et une expérience sur la décision lexicale.
Dans cet article on traite de la question de savoir si et comment on peut dire que les grammaires proposées par les linguistes peuvent être actualisées en modèles adéquats de traitement de phrases.
On étudie d'abord les postulas qui guident les expériences s'appuyant sur la théorie dite de complexité dérivationnelle (DTC).
Ces expériences ont été censées montrer que la théorie de la Grammaire Transformationelle (TG) connue comme Théorie Standard n'était que partiellement adéquate pour rendre compte de l'analyse humaine.
En particulier, on a pensé (voir Fodor, Bever et Garrett, 1974) que les expériences DTC démontraient que, tandis que l'analyseur utilisait les descriptions structurelles implicites dans les dérivations transformationnelles, les computations qu'il faisait ressemblaient peu aux transformations proposées par une TG.
Les principales propositions sous-tendant la DTC étaient que 1) le modéle de calcul (ou analyseur) elfectue les opérations de façon linéaire et sérielle et que 2) il incorpore une grammaire plus ou moins représentable sous une forme semblable à une grammaire de compétence.
Si l'on fait l'hypotheses d'une sérialité, stricte, il parait plus facile d'inclure dans le modéle d'analyse une grammaire lexicale étendue telle que celle proposée par Bresnan (1978) comme opposée à une TG.
Cette conjoncture joue un rôle important dans la critique que fait Bresnan à la TG en tant que partie pertinente d'une théorie d'utilisation du langage.
Fodor, Bever et Garrett (1974) ainsi que Bresnan (1978) cherchent à rendre les règles grammaticales compatibles avec les données psycholinguistiques et avec la proposition (1).
Ils proposent des modéles qui limitent la part de traitement actif réalisé en temps réel.
Nous montrons que le calcul en temps réel n'est pas nécessairement associé à une complexité supplémentaire de temps de réaction.
C'est à dire que nous montrons qu'un analyseur qui relie la SP à la SS par des régles de transformation (ou plus précisément par des règles d'analyse de forme très proche des règles d'un modéle transformationnel) peut s'accorder avec les données de la psycholinguistique si l'on fait simplement varier le postulat (1).
Plus précisément nous montrons qu'en enchassant TG dans une architecture de calcul parallèle (qui peut être justifiée comme raisonnable pour l'usage du langage) on peut saisir les différences de complexité dans le calcul des phrases qu'avaient relevées les expérimentateurs DTC
La proposition (2) permet aussi d'évaluer les grammaires candidates pour une théorie de l'utilisation du langage.
On montre d'abord que Bresnan (1978) peut affaiblir cette proposition pour rendre l'Extended Lexical Grammar compatible avec les résultats de la psycholinguistique.
Ensuite on analyse la position de Tyler et Marslen Wilson (1977–1980) selon laquelle leurs expériences montrent qu'on ne peut instancier une TG dans un modèle d'analyse sans changer la proposition (2).
Ceci est lié au fait qu'ils insistent sur le fait que leurs expériences supportent un “mode interactif” d'analyse dont ils pensent qu'il est compatible avec la Thèse de l'Autonomie de la Syntaxe.
On montre que la Thèse de l'Autonomie est sans relation avec ce modèle interactif.
Adopter ce modèle n'empêche donc pas d'inclure directement le TG dans un analyseur.
En outre, nous montrons pourquoi en allant dans le sens de la proposition (2), une condition que nous appellons le TTH n'est pas un critère absolu pour juger de l'utilité d'une théorie grammaticale en vue de construire une théorie d'analyseur.
Nous soutenons que les grammaires ne doivent pas être envisagées comme fournissant directement et de façon transparente (proposition 2 ci-dessus), un algorithme d'analyse.
Cependant, nous insistons sur le fait que la théorie de la grammaire a une place centrale dans le développement d'un mode d'utilisation du langage même si le Type de Transparence est affaibli selon nos suggestions.
Enfin, nous montrons que toutes ces remarques servent à l'évaluation comparative des modèles d'analyses possibles qui incorporent la grammaire transformationnelle, la grammaire lexico-fonctionnelle et les propositions de Tyler et Marslen-Wilson.
Les variations de vitesse d'élocution (ROS) affectent les indices spectraux du signal vocal et la prononciation ; les systèmes de reconnaissance automatique de la parole y sont donc exposés.
Afin de combattre ces effets, nous proposons d'utiliser en parallè le deux groupes de modèles acoustiques et de prononciation, adaptés en fonction de la vitesse d'élocution.
Le choix entre ces deux groupes peut basculer à la frontière des mots afin de rendre compte en cours d'énoncé des variations de cette vitesse, courantes en parole conversationnelle.
Grâce au parallélisme des deux groupes de modèles et à la méthode de décodage basée sur le maximum de vraisemblance, notre approche ne demande pas l'estimation de la vitesse d'élocution avant décision de reconnaissance, ce qui serait difficile à réaliser.
Nous évaluons nos modèles sur une tâche de reconnaissance automatique de la parole téléphonique grand vocabulaire.
Les expériences sur une configuration de développement NIST 2000 Hub-5s montrent que notre modélisation obtient 2,2% d'amélioration du taux de reconnaissance de mots comparé à un système de base ne comportant pas de traitement de la dépendance à la vitesse d'élocution.
Par rapport à un système de base amélioré où la coarticulation et les élisions sont modélisées dans un dictionnaire de multi-mots, notre modélisation dépendante de la vitesse d'élocution obtient 1,5% d'amélioration.
Nous avons de plus introduit une nouvelle modélisation des réductions phonétiques, fréquentes dans la parole à débit rapide, où les phones courts peuvent être omis en tant que segment mais préservés en tant que contexte phonétique pour les phones adjacents.
Cette approche a également permis une légère amélioration s'ajoutant à celle qu'obtient la prise en compte des variations de vitesse d'élocution.
Le débat de l'entre-deux tours de la présidentielle française de 2017 est révélateur des ambiguïtés du fact-checking quand il prétend dénoncer les mensonges propagés par les acteurs publics.
Drapées dans un discours de vérité, les pratiques de fact-checking visent d'abord à identifier le faux plus qu'à dire le vrai.
Elles délèguent l'établissement de la vérité à des sources fiables que le fact-checker pourra mobiliser.
L'analyse révèle toutefois qu'il s'agit d'abord de sources institutionnelles considérées comme collectivement légitimes.
Le fact-checking déploie ainsi une approche potentiellement conservatrice de l'information journalistique qu'il applique ensuite à l'ensemble des propos tenus dans l'espace public.
Cet article propose une méthode pour extraire le signal voulu à partir d'un signal bruité addressant ainsi le problème de ségrégation de deux sources acoustiques comme modèle de ségrégation de sources acoustiques basé sur l'analyse de la scène auditive.
Comme le problème de ségrégation de deux sources acoustiques est un problème mal-inversé il est nécessaire d'utiliser des contraintes afin de déterminer une solution unique.
La méthode proposée adopte les quatres règles proposées par Bregman comme contraintes physiques et utilise comme propriétés des sources acoustiques, l'amplitude instantannée et la phase des composants du signal bruité après son passage par une banque de filtres.
Le modèle proposé peut alors extraire l'amplitude instantannée et la phase du signal voulu.
Des simulations pour la ségrégation du ton harmonique complexe à partir d'un ton harmonique bruité et la comparaison des résultats obtenus lors de l'utilisation de tous ou d'une partie des contraintes ont été effectuées.
Les resultats montrent que la méthode proposée peut effectuer une ségrégation précise du ton harmonique complexe lors de l'utilisation de toutes les contraintes en relation avec les quatres règles de Bregman et une diminution de la précision lors de l'utilisation de seulement une partie de ces contraintes.
Dans cet article, nous étudions un ensemble de problèmes qui ont été identifiés chez des enfants qui éprouvent des difficultés de lecture, et nous essayons de les expliquer en tenant compte des propriétés du système de traitement du langage.
Le fait de considérer les troubles de lecture du point de vue de la structure linguistique et de l'acquisition du langage nous permet de faire des hypothèses spécifique sur leurs causes.
Ces hypothèses sont ensuite examinées à la lumière d'une analyse des exigences de la tâche de lecture et de l'évaluation de l'état du lecteur qui ne parvient pas à satisfaire à ces exigences.
Le reste de l'article étudie plus en détail une proposition quant à la source des troubles de lecture, dans laquelle le système de mémoire de travail joue un rôle central.
Cette proposition est évaluée à la lumière d'investigations empiriques qui ont essayé de séparer le savoir structural et la capacité de mémoire chez des enfants normaux et chez des enfants éprouvant de sérieuses difficultés de lecture.
Nous présentons dans cet article en nouveau modèle de l'intonation en anglais.
L'intonation y est décrite en termes de “descentes” (rise), “montées” (fall) et “lignes de connexion” (connection).
Les accents et les montées de continuation sont représentés par les éléments de “montée” et de “descente” ; les lignes de connexion sont utilisées partout ailleurs.
Un système d'équations permet de reconstruire le contour de la fréquence fondamentale à partir de ces éléments.
Nous décrivons ensuite un système d'étiquetage automatique qui associe à toute phrase une représentation en descentes/ montées/ connexions, sans connaissance a priori, ni analyse descendante.
Une série d'expériences portant sur un corpus de phrases prononcées par 6 locuteurs de langue anglaise et d'accents variés valide ce modèle : l'erreur de modélisation est comprise entre 3.6 et 7.3 Hz.
Par ailleurs, une comparison des étiquetages automatiques et manuels montre une correspondance de 72% à 92%.
Nous concluons par une comparison du modèle proposé avec lex modèles existants et en discutons les applications pratiques.
Ce papier décrit la première étape de notre recherche, un système (que nous appelons VEST) qui reconnaît deux locuteurs de langue espagnole et deux de langue anglaise, et qui est limité à quatre cents mots.
L'idée clé novatrice est que la reconnaissance de la parole et l'analyse de la langue sont intimement liées du fait qu'elles reposent toutes deux sur le même modèle de langage, c'est-à-dire une grammaire augmentée de structures de phrases.
Dans cet article, nous décrivons plusieurs méthodes objectives et subjectives permettant d'évaluer le comportement d'un système d'interaction vocale et de ses composantes.
Nous nous focalisons sur l'évaluation de l'interaction homme-machine notamment celui de recherche d'informations.
Cet article décrit notre contribution à l'effort entrepris par le CEPT pour la définition d'un standard européen pour le radio téléphone numérique cellulaire.
Nous proposons un modèle descriptif des formes de la résolution coopérante de problèmes en dyade, avec les mécanismes d'apprentissage qui y sont associés.
La résolution coopérante de problèmes est analysée selon trois dimensions fondamentales et graduelles : la symétrie, l'alignement et l'accord.
La première dimension renvoie à la distribution de rôles transactionnels, la deuxième à la coordination des actions, et la troisième à la résolution de désaccords.
La combinaison des trois dimensions produit un espace de huit formes principales de coopération, au sein desquelles nous situons " la collaboration ".
Des analyses comparatives de séquences d'interactions illustrent les relations entre les dimensions.
Enfin, nous discutons des mécanismes d'apprentissage associés aux formes de coopération, et nous proposons des extensions du modèle pour analyser des groupes plus étendus.
Les paramètres articulatoires, la forme du conduit vocal et la fonction d'aire sont extraits de spectres de fricatives.
Un modèle de la génération des fricatives a été employé pour déterminer les contraintes acoustiques pour une procédure d'optimisation dans laquelle le travail musculaire est pris pour critère.
Une distance interspectrale a été mesurée en utilisant l'inégalité de Cauchy-Boujakovsky.
Il est nécessaire d'initialiser correctement l'approximation des paramètres articulatoires pour obtenir une solution précise et stable du problème inverse.
Dans le domaine de la recherche en Reconnaissance Automatique de la Parole (RAP), il est devenu habituel de poursuivre en priorité les approches réduisant le taux d'erreurs au niveau du mot.
Les auteurs émettent cependant quelques réserves à l'égard d'une telle stratégie qui, bien qu'apparemment raisonable, conduit souvent à un manque d'innovation.
En effet, d'énormes efforts ont été consentis pendant plusieurs années sur les approches aujourd'hui prédominantes en RAP et celles-ci ont été développées et testées sur des données standards de référence, convergeant donc ainsi vers un minimum local dans l'espace des techniques disponibles.
Dans ce cas, il est clair que pratiquement n'importe quelle nouvelle approche suffisamment différente ne pourra pas se comparer favorablement aux systèmes existants et résultera souvent initialement en une augmentation du taux d'erreurs.
D'un autre côté, il est également probable que les problèmes restant à résoudre nécessiteront de nouvelles approches.
Dans ce papier, nous discutons certaines directions de recherche qui ne conduiront peut-être pas toujours à une diminution immédiate et garantie du taux d'erreurs, alors qu'elles pourraient ultimement s'avérer bénéfiques aux performances de nos systèmes.
Les thèmes qui seront abordés dans se papier concernent notamment : la discrimination entre modèles, le rôle de l'information a priori en reconnaissance de la parole, le couplage du modèle acoustique et du modèle de language, l'extraction des caractéristiques et information temporelle, et quelques procédures de reconnaissance reflétant mieux les propriétés perceptuelles chez l'homme.
L'inférence de réseaux de régulation de gènes s'oriente actuellement vers l'utilisation conjointe d'informations biologiques complémentaires.
Nous utilisons ici des données de marqueurs génétiques en plus des classiques données d'expression dans le cadre des réseaux bayésiens statiques discrets.
Nous comparons les qualités de di_érents scores ainsi que l'impact d'un a priori lié à la connectivité des réseaux.
Nous proposons et comparons deux modélisations aux approches existantes pour l'inférence de réseaux de régulation.
Sur des données simulées, l'un de nos modèles obtient les meilleurs résultats dans le cas d'échantillons de petites tailles.
Nous utilisons ce même modèle sur des données réelles d'Arabidopsis thaliana.
Il est bien connu que la variabilité inter-locuteur liée à l'accent est un facteur important de la dégradation des performances des systèmes de reconnaissance.
Si l'on peut faire une estimation précise de l'accent d'un locuteur, alors un ensemble de modèles de reconnaissance modifiés pour prendre en compte cet accent peut être utilisé pour améliorer les scores de reconnaissance.
Dans cette étude, on traite de la question de l'identification de l'accent en Anglais Américain.
Une base de données d'accents étrangers a été établie : elle comporte des mots et des syntagmes connus pour être sensibles à l'accent.
Des algorithmes de classification d'accent ont ensuite été développés, basés sur des mots isolés ou sur des phonèmes.
L'ensemble des traits considérés comprend les coefficients cesptraux en Mels et l'énergie, ainsi que leurs dérivées du premier ordre.
On montre que la précision de la classification augmente avec la longeur de la phrase test.
Des séquences de 7 à 8 mots isolés donnent lieu à un taux de classification correcte d'accent de 93%, dans un ensemble de quatre type d'accents différents.
Un test d'écoute subjectif a également été mené pour comparer les performances humaines et automatiques sur cette tâche.
Les résultats montrent que la classification automatique donne, de façon cohérente, des performances supérieures à celles des réponses humaines pour cette tâche de classification.
Toutefois, on montre que certains auditeurs sont capables d'égaler les performances des algorithmes pour la détection d'accent.
Enfin, une étude expérimentale a été menée pour étudier l'influence de l'accent étranger sur la reconnaissance.
On montre que la précision de la reconnaissance peut être améliorée de façon notable en faisant un apprentissage séparé des modèles pour chaque accent plutôt qu'en utilisant un modèle unique pour chaque mot.
Le projet de cet article est de rendre compte de l'histoire et de la sociologie d'un groupe de fonctionnaires singuliers – les sténographes parlementaires – dont le métier est intimement lié à l'histoire du parlementarisme dans la plupart des démocraties du monde.
Conduite à partir d'un matériau sociologique et historique original, cette enquête vise à questionner la façon dont les institutions sont façonnées, produites et reproduites par les savoirs et les faire, dont des valeurs s'incarnent, dont les institutions prennent forme.
Au-delà de l'institution de la publicité des débats, l'article s'intéresse aux conditions techniques, matérielles et sociales de définition de l'Assemblée nationale comme institution politique.
Le modèle statistique de propagation de l'erreur d'étape y est donné.
Par itération sur l'ensemble des étapes, cette analyse mène à l'estimation de l'énergie des trois contributions d'erreur — erreurs d'entrée, arithmétique et de coefficients — qui sont comparées à l'expérience.
Ceci est principalement du à la difficulté d'identifier et de catégoriser les facteurs liés à l'émotion dans la parole, puis à les utiliser dans les systèmes de synthèse.
De tels modèles pourraient aussi être utilisés comme des outils efficaces pour la validation de modèles de l'émotion et des autres facteurs de stress altérant la parole.
Atteindre des performances robustes pour un système de reconnaissance vocale est un problème pifficile à résoudre surtout lorsqu'un tel systéme est utilisé comme fonction de composition vocale dans les radiotéléphones mobiles de voiture.
La nécessité de telles fonctions devient primordiale dans la mesure òu l'utilisateur d'un radiotéléphone mobile peut se concentrer sans risques sur la conduite de son véhicule tout en composant le numéro de son correspondant et discuter avec ce dernier en mode “mains-libres”.
Le travail présenté dans cet article pose le problème de la reconnaissance mono-locuteur de most isolés dans un environnement bruité.
Dans ce contexte toute la difficulté réside dans le fait qu'il existe des différences importantes entre les conditions d'apprentissage (généralement dans le silence) et celles de reconnaissance (généralement dans le bruit, lorsque le véhicule roule).
Une nouvelle technique de réduction du bruit est proposée : la Soustraction Spectrale Non linéaire (NSS).
Dans un système de reconnaissance utilisant les Modèles de Markov Cachés (HMM), des estimateurs robustes de variances (lissage) et de densités de probabilités d'observation (projection) sont également introduits et combinés avec la Soustraction Spectrale Non linéaire.
Nous montrons aussi que les limites courantes d'application de la Projection (RSB inférieurs à 0 dB) peuvent être repoussées grâce à l'utilisation de NSS.
Le système de reconnaissance (HMM) voit ses performances s'élever de 56%, sans traitement, à 98%, après réduction du bruit par NSS.
Plus de 3000 mots à reconnaître ont été employés pour l'évaluation des différents systèmes considérés (trois bases de données, deux langues europénnes).
De telles performances ont été atteintes en ayant recours à des techniques robustes d'apprentissage et de reconnaissance ainsi qu'à prétraitement des mots bruités à l'aide de NSS.
Afin d'évaluer les différents algorithmes de propagation d'incertitude, nous avons réalisé des expérimentations en utilisant les algorithmes de propagation développés dans le contexte des réseaux possibilistes basés sur le produit et l'algorithme de propagation pour la logique possibiliste quantitative mis au point dans cet article.
On évalue l'histoire de l'utilisation de la psychologie dans le contrôle social et l'accroissement vraisemblable de son importance dans le futur.
Les effets du financement militaire de la recherche psychologique et les conséquences du chômage sont plus particulièrement étudiés.
La psychologie et les disciplines reliées aux sciences cognitives et aux neurosciences pourraient être plus pertinentes en participant au développement des composantes techniques des pratiques de contrôle social plutôt qu'en fournissant les justifications idéologiques de leur utilisation.
Une nouvelle fonctionnelle pour l'estimation du mouvement d'objets déformables est proposée dans le cadre de l'étude des images de milieux continus où la valeur des pixels est proportionnelle à la densité d'une grandeur.
Cette nouvelle fonctionnelle repose sur le concept de l'énergie de déformation élastique et les principes de conservation des milieux continus.
L'introduction a priori d'un modèle physique de déformation, permet une nouvelle interprétation de la fonctionnelle proposée par Song et Leahy [Song 91].
Des résultats obtenus à partir de simulations montrent la possibilité de prise en compte du caractère compressible ou non de la déformation.
Des résultats sur séquence d'images réelles sont également présentés.
Cet article présente des algorithmes bayésiens pour le démélange d'images hyperspectrales.
Chaque pixel de l'image est décomposé selon une combinaison linéaire de spectres de référence pondérés par des coefficients d'abondances.
Dans un cadre supervisé, nous supposons connus les spectres de références.
Le problème consiste alors à estimer les coefficients du mélange sous des contraintes de positivité et d'additivité.
Une loi a priori adéquate est choisie pour ces coefficients qui sont inférés à partir de leur loi a posteriori.
Un algorithme de Monte Carlo par chaîne de Markov (MCMC) est développé pour approcher les estimateurs.
Dans un cadre semi-supervisé, les spectres participant au mélange sont supposés inconnus.
Nous faisons l'hypothèse qu'ils appartiennent à une bibliothèque spectrale.
Un algorithme MCMC à sauts réversibles permet dans ce cas de résoudre le problème de sélection de modèle.
Enfin, dans un dernier cadre d'étude, les algorithmes précédents sont étendus au démélange non-supervisé d'images hyperspectrales, c'est-à-dire au problème d'estimation conjointe des spectres et des coefficients de mélange.
Ce problème de séparation aveugle de sources est résolu dans un sous-espace approprié.
Dans cet article, nous nous intéressons à la structuration des termes d'un domaine, c'est-à-dire à l'acquisition de relations entre termes.
En effet, les terminologies ne peuvent plus se contenter de recenser les termes et les organiser brièvement sous forme d'une hiérarchie.
Elles doivent également proposer toute une gamme de relations qui reflètent au mieux les connaissances du domaine et répondent de manière adaptée aux besoins des applications.
Nous confrontons la place accordée traditionnellement par la théorie terminologique aux relations avec les besoins réels qui apparaissent lors de la constitution, l'utilisation et la réutilisation de ressources terminologiques.
Nous présentons également un panorama des approches proposées pour l'acquisition de relations entre termes à partir de corpus spécialisés.
La reconnaissance de mots (dans la chaîne parlée) englobe trois fonctions fondamentales : l'accès, la sélection et l'intégration.
L'accès se réfère à l'appareillement de l'onde sonore avec les représentations de formes lexicales ; la sélection, désigne la discrimination du meilleur “pareil” (match) lexical avec le stimulus, et l'intégration recouvre l'appareillement de l'information syntaxique et sémantique avec les niveaux de traitement supérieures.
Cet article décrit comment deux versions d'un modèle (de type “cohorte”) rendent compte de ces processus, en traçant son évolution à partir d'une première version comportant un principe d'interaction partielle où l'accès est strictement autonome mais où la sélection est soumise à des contrôles “de haut en bas” vers une deuxième version (à fonctionnement entièrement “de bas en haut”) où le contexte n'intervient plus dans les processus d'accès et de sélection.
Par conséquent, le contexte n'intervient qu'á l'interface entre les représentations supérieures et l'information générée en temps réel sur les propriétés syntaxiques et sémantiques des membres du cohorte.
Ce nouveau modéle garde intactes les caractéristiques essentielles d'un processus de reconnaissance de type cohorte.
Il intégre les notions d'accés et d'évaluation multiples permettant ainsi un processus de reconnaissance optimal fondé sur le principe de contingence de choix perceptif.
Cet article présente un système de segmentation qui découpe automatiquement un ensemble de mots manuscrits en lettres.
L'objectif de cette opération est de constituer une base d'apprentissage pour un système de reconnaissance en ligne de mots manuscrits.
Les tracés de mots à segmenter ont été saisis sur une tablette à digitaliser puis convertis en une suite de vecteurs.
Enfin, à chacun de ces tracés de mot, est associé le mot alphabétique correspondant.
La description de notre système de segmentation est suivie d'un test expérimental portant sur le découpage en lettres d'une base de 10000 mots qui proviennent de 10 scripteurs différents.
Par simulation informatique, on a engendré des réponses impulsionnelles entre des points d'une pièce rectangulaire et deux points opposés sur une “tête sphérique”.
Des sons ont été convolués avec ces réponses impulsionnelles pour fabriquer des stimuli, en vue d'étudier les effets de la réverbération sur la capacité des locuteurs à utiliser les différences de fréquence fondamentale (Δ Fo) pour discriminer entre des voyelles différentes.
L'expérience 1 a permis de vérifier la validité de la simulation en montrant qu'elle fournissait (i) des perceptions correctes de latéralisation, (ii) une plus grande contribution du temps que de l'amplitude dans la latéralisation due aux différences interaurales, et (iii) un effet nul de la réverbération sur la latéralisation.
Les expériences 2 à 5 ont mesuré les seuils de masquage pour des voyelles “cible” harmoniques synthétiques en présence de sons de masquage.
Dans l'expérience 2 des locuteurs avaient à identifier les cibles en présence d'un masquage par du bruit rose.
Cette expérience a défini une géométrie et un degré de réverbération pour lesquels les auditeurs n'ont plus d'indices binauraux dûs à la géométrie des sources.
L'expérience 3 a montré que la même configuration ne détruisait pas la capacité à utiliser les Δ Fo pour séparer les cibles de sons de masquage de type vocalique, quand les deux ont des contours de Fo statiques ; mais elle empêche les auditeurs d'utiIiser les Δ Fo relatifs à des contours de Fo qui changent de façon cohérente.
L'expérience 4 a montré qu'une largeur de modulation de ±1,45% était suffisante pour empêcher l'emploi des Δ Fo, mais que cet emploi n'est pas éliminé tant que la largeur de la modulation ne dépasse pas les Δ Fo.
On démontre que ces résultats sont compatibles avec les modèles proposés de la capacité à utiliser les Δ Fo pour séparer les voyelles concurrentes, et que la réverbération détruit cette capacité pour des Fo mobiles en mélangeant les harmoniques des sources en concurrence.
Finalement, l'expérience 5 démontre que la réverbération n'a pas d'effet sur la capacité à séparer une voyelle modulée d'un bruit rose.
Donc, dans ces expériences, la réverbération doit exercer ses effets en mélangeant les harmoniques des sons de masquage plutôt que celles des cibles.
Pour finir, les expériences démontrent que les Δ Fo peuvent être des indices plus robustes de séparation de sons concurrents que les indices binauraux.
L'application de ces résultats à la perception de la parole continue naturelle est discutée.
Le premier, constitué de réseaux neuronaux, d'une part, transforme les paramètres cepstraux classiques en un ensemble de valeurs échelonnées attribuées à des traits acoustico-phonétiques pour chaque échantillon, et par ailleurs, divise le signal de parole en segments acoustico-phonétiques.
La sortie de cette première étape, obtenue en combinant les sorties de ces deux traitements, donne une première estimation de la suite des phonèmes composant le message.
La deuxième étape est basée sur un système-expert composé de règles allophoniques, d'un lexique de transcriptions phonétiques des mots appartenant au vocabulaire de l'application choisie, de règles syntaxiques et du système de contrôle général.
Ce système-expert, pour traiter la chaîne des phonèmes provenant de la première étape, utilise un lexique et un système d'analyse grammaticale basé sur le principe des “îlots de confiance”.
Le vocabulaire actuel se compose de 35 mots provenant d'une application de type CAO.
Cet article présente une étude comparative de la relation entre le timing d'un mouvement mélodique montant ou descendant et la syllabe qu'il accentue pour trois langues : le néerlandais, le français et le suédois.
Dans une expérience perceptive, des stimuli de 5 syllabes /mamamamama/ et / a a a a a/ furent synthétisés avec un mouvement mélodique montant ou descendant relativement rapide.
Le timing du mouvement fut systématiquement varié de manière à accentuer la troisième ou la quatrième syllabe.
Il était demandé aux sujets d'indiquer la syllabe perçue comme accentuée.
La frontière d'accentuation (AB) entre la troisième et la quatrième syllabe est définie comme le moment avant lequel plus de la moitié des sujets indiquent la troisième syllabe comme accentuée et après lequel plus de la moitié indiquent la quatrième.
Les résultats montrent qu'il existe des différences significatives entre les trois langues concernant la position de la AB.
En général, pour les mouvements mélodiques montants, les ABs sont clairement définies.
Elles sont situées au milieu de la voyelle de la troisième syllabe pour les sujets français, et plus tard dans la voyelle pour les sujets suédois et néerlandais.
En ce qui concerne les mouvements mélodiques descendants, il est obtenu une AB clairement définie uniquement pour les sujets suédois et néerlandais.
Elle est située à la fin de la troisième syllabe.
Pour les sujets français en revanche, ce type de mouvement ne permet pas de définir une AB.
Cela confirme l'absence d'accentuation par mouvements mélodiques descendants en français.
En faisant varier la durée du mouvement mélodique, il a pu être montré (dans les cas ou la `AB' est clairement définie), que la notion d'accentuation est liée à la perception d'une variation mélodique située au début du mouvement.
Nous développons dans cet article un algorithme de codage linéaire de Walsh (WLC) qui donne une approximation comprimée du spectre de puissance de Walsh dans une fenêtre temporelle.
Cet algorithme est plutôt interpolatif que prédictif.
Il minimise l'erreur d'interpolation moyenne carrée et il donne le modèle de spectre WLC.
Son utilisation pour des systèmes de reconnaissance de parole est explorée.
Deux mesures de distorsion de gain unité sont incorporées dans un système de reconnaissance de mots isolés basé sur une déformation temporelle dynamique et dépendant du locuteur.
Trần Đức Thảo et Jean Piaget partagent une même volonté de révolutionner la méthode de la philosophie de la connaissance en la fondant sur la mise en parallèle de l'évolution de l'intelligence humaine et du développement des compétences cognitives des individus.
Cette combinaison fait écho à la théorie biologique de la récapitulation dont le caractère obsolète et erroné semble les condamner.
Nous montrerons au contraire qu'elle est la marque de l'actualité de leur projet théorique.
Une méthode de simulation du conduit vocal dans le domaine temporel est décrite.
Le modèle adopté comporte une source de pression d'air constante, une section étroite variable dans le temps représentant la glotte et un tube correpondant au conduit vocal couplé à la cavité nasale,
Les équations qui gouvernent la génération et la propagation de l'onde sonore dans le modèle sont transformées en une représentation en variables discrètes par l'application de certaines règles : la règle rectangulaire dans l'espace et la régle trapézoîdale dans le temps.
Discrétiser de cette manière particulière introduit une distortion spectrale due à une déformation fréquentielle.
Une analyse théorique indique que cette déformation peut être interprétée comme une manifestation de la dépendance fréquentielle de la vitesse de phase dans le système discrétisé.
Son amplitude dépend à la fois de la fréquence (f s) d'échantillonnage spatial (X).
Onze voyelles françaises, synthétisées avec f s = 20 kHz et X = 1 cm présentent un haut degré de naturel et d'intelligibilité même si une trace de distorsion fréquentielle est relevée dans la région du 3° formant.
Pour f s = 40 kHz, l'effet de la distortion spectrale devient pratiquement négligeable aux fréquences inférieures à 4 kHz.
Cet article traite d'un module de synthèse de parole qui a été utilisé pour présenter des informations de trafic par radio de voiture, dans le cadre du système RDS-TMC de Contrôle de Messages de Trafic.
Une des idées de base de ce service visant à être pan-européen est de pouvoir fournir des informations de trafic dans la langue maternelle du conducteur, indépendamment de la langue utilisée dans l'endroit géographique oú il se trouve, ou utilisée par la radio-diffusion.
La synthèse de la parole est indispensable pour réaliser ce type de service, pour un ensemble illimité de noms de lieux.
Un prototype a été développé pour l'allemand.
L'ouverture à d'autres pays et d'autres langues est prévue pour 1998.
Cet article prône une approche orientée individu pour la résolution du problème classique des mariages stables.
Selon cette approche, la solution émerge de négociations entre agents.
L'agentification de l'algorithme séminal de Gale-Shapley revient à distinguer deux comportements d'agents (proposant et disposant) qui négocient pour aboutir à une solution stable mais inéquitable.
Le comportement d'agent CASANOVA que nous proposons ici consiste à jouer simultanément ces deux rôles dans une multitude de négociations bilatérales.
Les agents mettent en oeuvre une stratégie de concession minimale maximisant leur bien-être individuel.
Les solutions qui émergent sont équitables et elles ne peuvent pas être atteintes par les méthodes multi-agents existantes.
Un modèle à trois dimensions de la langue a été élaboré à partir des images obtenues par Résonance Magnétique (IRM) sur un sujet prononçant 44 articulations suédoises.
En s'appuyant sur la différence entre les contours de la langue mesurés pour les différentes articulations et une position de référence, six paramètres de contrôle de la mâchoire et la langue ont été déterminés par application d'une analyse factorielle sur des mesures articulatoires.
Les cinq premiers facteurs ont expliqué 88% de la variance des contours de la langue dans le plan sagittal et 78% de la variance tri-dimensionnelle.
Ce modèle à six paramètres est capable de reconstruire les articulations mesurées avec une erreur moyenne de 0,13 cm et peut également prendre en compte les différences latérales et les asymétries des contours de la langue.
En vue de corriger l'hyper-articulation résultant des expositions prolongées durant l'acquisition d'IRM, les valeurs des paramètres ont été ajustées en comparant les contacts linguopalataux virtuels et ceux mesurés par électropalatographie.
Des données de mouvement ont été mesurées pour des séquences voyelle-fricative à l'aide d'un articulographe électromagnétique, afin de déterminer le contrôle cinématique du modèle.
Les systèmes SAS (Sonar à Antenne Synthétique) sont activement utilisés pour l'imagerie du fond marin.
En effet, la haute résolution des images SAS est d'un très grand intérêt pour la détection, la localisation ou encore la classification d'objets présents sur le fond marin.
Mais ces images sont fortement entachées d'un bruit granulaire multiplicatif, connu sous l'appellation de bruit de speckle, qui réduit les résolutions spatiale et radiométrique.
Si bien que l'interprétation automatique de ces images présente quelques difficultés.
Une solution peut consister en un pré-traitement visant à réhausser le signal d'intérêt, sans pour autant altérer la résolution spatiale.
Nous proposons dans cet article d'utiliser conjointement le filtrage adapté stochastique et un filtre moyenneur auto-adaptatif.
Par ailleurs, afin de préserver au mieux la résolution spatiale, le critère utilisé pour mettre en oeuvre le filtrage adapté stochastique est celui de la minimisation de l'écart entre l'une des caractéristiques statistiques du speckle et celle du bruit estimé, entraînant une adaptation sur la taille de la fenêtre glissante.
Des expérimentations sur données réelles sont proposées et les résultats comparés avec ceux obtenus par différentes techniques de débruitage à base de filtrage adapté stochastique.
Le lexique de désignation des procès tarde sur celui de la désignation des entités aux étapes précoces de l'acquisition de la langue première (L1), mais également dans l'acquisition ultérieure de langues étrangères (L2).
Le traitement sémantique des verbes par les enfants et les adultes manifeste une plus grande flexibilité que celui des noms, ce qui est expliqué par le caractère relationnel des verbes, et par le fait que la catégorisation des événements en unités lexicalisées est moins déterminée par la perception que structurée par des schèmes de lexicalisation spécifiques à chaque langue.
Lorsque le lecte de l'enfant (ou de l'apprenant d'une L2) est peu pourvu en verbes, la flexibilité sémantique des verbes sera sollicitée, notamment par des processus analogiques.
La situation actuelle au Japon à propos du développement et des applications de l'électro-palatographie est revue.
Les données sur l'articulation linguale par rapport aux différences individuelles et aux caractéristiques des enfants sont ensuite fournies.
Ces données ont été obtenues en combinant les mesures tri-dimensionnelles des empreintes en plâtre des palais durs de cinquante adultes, de trente enfants et de deux enfants à différentes étapes de leur développement dentaire avec l'observation électro-palatographique des formes de contact lingual des palais durs de sujets adultes et de quatre enfants pendant la prononciation de consonnes japonaises.
Dans cet article, nous proposons une technique de détection et de segmentation de zones de couleur de peau.
La méthode utilise les techniques de data mining pour produire les règles de prédiction, suivie d'une phase de segmentation en régions cohérentes de peau en utilisant les règles déjà produites.
Les expérimentations réalisées sur une base d'images importante montre l'efficacité et la faisabilité de notre approche.
Une méthode est décrite pour l'extraction de vecteurs de caractéristiques robustes aux distortions provenant du type de téléphone utilisé dans des applications de reconnaissance du locuteur.
La technique transforme les vecteurs de caractéristiques tels que le Mel-cepstre, le log-spectre et les caractéristiques basées sur la prosodie, à l'aide de réseau de neurones non-linéaire.
Le réseau de neurones est entraîné de manière discriminante pour maximiser la performance du système de reconnaissance du locuteur, spécifiquement dans des conditions où des types de téléphone différents sont utilisés lors de l'entraînement et de la vérification.
L'algorithme ne requiert, ni enregistrement stéréo de la session d'entraînement, ni étiquettage manuel des types de téléphone utilisés à l'entraînement et à la vérification.
Les résultats sur le corpus 1998 NIST Speaker Recognition Evaluation montrent une amélioration relative atteignant avec les nouvelles caractéristiques basées sur le réseau de neurones. Le système de référence utilise des vecteurs de caractéristiques basés le MEL-cepstre avec soustraction du cepstre moyen ainsi que des modèles d'imposteurs dépendant du type de téléphone.
Dans cet article sont présentés de manière synthétique les résultats du projet ANR DESAM (Décompositions en éléments sonores et applications musicales).
La plupart des aspects abordés dans le projet ont donné lieu à de nouvelles méthodes et algorithmes qui sont regroupés au sein d'une boîte à outils, la DESAM Toolbox.
Celle-ci rassemble un ensemble de fonctions Matlab® dédiées à l'estimation de modèles spectraux très utilisés pour les signaux musicaux.
Les méthodes étudiées dans ce projet peuvent bien sûr être utiles pour la recherche automatique d'informations dans les signaux musicaux, mais elles constituent avant tout une collection d'outils récents pour décomposer les signaux selon différents modèles, avec pour résultat des représentations mi-niveaux variées, pouvant être utiles dans d'autres domaines d'application.
Cet article propose un algorithme de prédiction, sur la base des contours de F0, des frontières de mots et des mots grammaticaux en Hindi.
Il utilise des propriétés des contours de F0 comme la tendance à la déclinaison, la remise à niveau, et les formes de type montant-descendent.
Les unités syllabiques sont identifiées à partir du contour d'énergie, de F0 et du coefficient de prédiction linéaire du premier ordre.
A chaque syllabe est assignée une valeur d'accent L (bas), H ou h (haut) en (i) comparant la valeur de F0 au centre de chaque noyau syllabique avec celle de l'unité syllabique précédente et (ii) en comparant les valeurs de F0 en deux points différents au sein de chaque unité syllabique dans une séquence ayant une valeur d'accent L.
Les frontières de mots sont placées entre les unités syllabiques adjacentes H et L, h et L, L et L, L et h, H et h.
Une évaluation menée sur un corpus de 50 phrases en Hindi lues par cinq locuteurs natifs dans un environment de bureau montre que environ 74% des frontières de mots et environ 28% des mots grammaticaux sont identifiés correctement.
Les résultats de cette prédiction peuvent être utilisés pour améliorer les performances des modules de décodage acoustico-phonétique, d'analyse lexicale ou syntaxique d'un système de transcription parole-texte.
La robustesse de l'algorithme dans le cas de parole bruitée est également discutée.
Cet article décrit PEGASUS, une interface de dialogue oral pour la réservation immédiate de voyages aériens
PEGASUS exploite les technologies de traitement du langage naturel que nous avons développées dans le cadre de ATIS et permet aux utilisateurs du système EAASY SABRE de American Airlines de faire des réservations de vols.
La demande de l'utilisateur est transformée par le système de compréhension de parole en un schéma (frame) qui représente sa signification.
La tâche du module de gestion du systéme consiste à transformer la représentation sémantique en une commande de EAASY SABRE, à la transmettre au module de l'application, à mettre en forme et interpréter l'information résultante et à gérer le dialogue.
Les résultats d'évaluation préliminaires suggèrent que les utilisateurs peuvent apprendre à utiliser PEGASUS de façon productive pour faire des réservations de vols, même si beaucoup de travail reste encore à faire pour rendre le système vraiment convivial.
Les dernières années ont été très intéressantes chez AT&T dans le domaine des applications avancées du vocal en télécommunications : des progrès techniques et des avancés au niveau des processeurs/plateformes ont permis l'identification, le développement et le test d'un ensemble de nouveaux services.
Pendant cette période, et avant la séparation d'AT&T, de Lucent Technologies et NCR, AT&T rassemblait, sous un même `toit', un laboratoire de recherche engagé sur les technologies vocales avancées, des organisations d'affaires construisant des plateformes pour pousser ces technologies dans des applications en télécommunications, et d'autres organisations d'affaires ayant la responsabilité du déploiement de services à technologies vocales pour faciliter l'usage et réduire le coût des services de télécommunications pour les abonnés et les professionnels.
Maintenant que cette période de notre histoire commune prend fin, nous pouvons regarder en arrière pour dresser une vue d'ensemble sur comment les progrès techniques, les avancées au niveau des plateformes et les besoins et occasions au niveau des services centralisés (à travers un réseau téléphonique) ont interagi pour que la technologie vocale devienne une expérience quotidienne pour des millions de gens – et pour décrire quelques leçons que nous avons appris au cours de ce chemin.
Travuas antérieurs ont démontré la possibilité d'entraîner un perceptron à niveaux multiples pour l'estimation de la période fondamentale (Tx) de signaux produits par différents locuteurs parlant dans des conditions de bruit intense.
L'algorithme a été implanté en temps réel et réalisé sur un système de dévloppement TMS320C25.
Un prototype portatif incorporant le logiciel a été construit.
Il servira de base à une nouvelle génération de prothèses auditives à traitement de signal pour les sourds profonds.
Grâce à une très faible consommation, l'autonomie de l'appareil est de 12 heures sans recharge.
L'algorithme a été amélioré afin d'augmenter sa résolution temporelle ce qui le rendra intéressant pour une large série d'autres applications.
Cet article présente ¡'implémentation d'un réseau bayésien dynamique avec variable exogène continue, appliquée à la classification d'évènements discrets irrégulièrement espacés, organisés en séquence.
La modélisation des tables de probabilités conditionnelles, qui sont fonction d'une variable exogène codant un vecteur de distances entre événements, s'appuie sur les mélanges de lois gaussiennes estimés par l'algorithme EM.
La classification de points singuliers des rails du métro de Paris est le cadre applicatif de cette étude.
La parcimonie de l'approche proposée est également démontrée dès que l'ordre du modèle est supérieur ou égal à deux.
Cette implémentation permet, à terme, d'améliorer la décision fournie par un capteur spécifique de défaut de rail.
Récemment, des techniques motivées par la perception auditive, sont appliquées dans de principales technologies courantes de la parole. Il semble y avoir un regain d'intérêt à l'exploitation de plus de connaissance du processus de la parole humaine dans la conception de systèmes de reconnaissance de la parole.
Le papier discute l'expérience de l'auteur dans l'application de connaissances auditives à la reconnaissance automatique de la parole.
Il avance l'idée que la raison d'appliquer des connaissances de la perception auditive humaine à l'ingénierie de la parole devrait être la capacité de la perception à supprimer quelques parties de l'information contenue dans le message de la parole. L'article plaide contre l'exploitation aveugle de connaissance accidentelle dispersée qui peut être non pertinente pour une tâche de reconnaissance de la parole.
Trois propriétés de perception humaine de la parole sont discutées : • reśolution spectrale limiteé, • utilisation de l'information contenue dans des segments de longueur d'une syllabe environ, • possibilité d'ignorer les composantes altérées ou non pertinentes de la parole.
L'auteur montre, en se référant à certains travaux publiés, que l'utilisation sélective de la connaissance auditive optimisée en fonction et dans certains cas provenant de vraies donneés de parole, peut être compatible avec les approches stochastiques actuelles de la reconnaissance automatique de la parole et pourrait avoir des avantages pour des applications pratiques d'ingénierie.
Cette étude concerne la production des consonnes de l'anglais par des locuteurs de langue maternelle italienne.
Les 240 locuteurs adultes de langue maternelle italienne concernés par cette étude ont commencé à apprendre l'anglais quand ils ont émigré au Canada, âgés de 2 à 23 ans.
Les occlusives et constrictives de l'anglais ont étéétiquettées en consonne initiale, médiane et finale de mots puis soumises à un jugement auditif par choix forcé auprès d'auditeurs de langue maternelle anglaise.
Les résultats montrent que l'âge d'apprentissage (AOL) de l'anglais par les sujets de langue maternelle italienne exerce un effet systématique sur leur production des consonnes anglaises, même quand ils vivent au Canada depuis 32 ans en moyenne, et qu'ils parlent anglais davantage qu'italien.
Dans tous les cas sauf deux, au moins un des sous-groupes constitué sur la base de l'AOL a obtenu des résultats significativement différents de ceux du groupe de contrôle (NE) formé de sujets de langue maternelle anglaise.
Pour ce qui concerne l'identification des consonnes et des frontières syllabiques, l'AOL à partir duquel on note une différence entre le premier sous-groupe de sujets de langue maternelle italienne et les sujets du groupe de contrôle (NE) varie.
Les résultats sont discutés par rapport aux hypothèses proposées dans la littérature concernant les erreurs segmentales liées à la production d'une seconde langue.
Pour la reconnaissance des voyelles, la normalisation des fréquences formantiques a souvent été utilisée pour éliminer des différences inter-locuteurs.
Cependant, l'estimation de ces fréquences devient difficile sous certaines conditions, comme la transmission téléphonique par exemple.
Cet article présente une approche de la normalisation des voyelles basée sur un alignement spectral par distorsion fréquentielle.
Une distance normalisée en fréquence entre spectres expérimentaux et spectres de référence est définie sur la base d'une différence quadratique moyenne sur tous les choix possibles de fonctions de distorsion fréquentielle soumises à des contraintes non-linéaires et à des conditions aux limites.
Une fois que les différences au niveau de la pente spectrale dues aux caractéristiques glottiques individuelles sont éliminées adaptivement, la distance spectrale est calculée par programmation dynamique.
Les expériences sur l'identification des voyelles ont été effectuées sur les neuf voyelles de l'anglais américain dans des environnements /hVd/ produites par 12 locuteurs masculins et 12 locuteurs féminins.
Les résultats montrent que la méthode de distorsion spectrale augmente considérablement les identifications correctes pour les voyelles féminines lorsque les voyelles masculines sont utilisées comme références.
Bien que l'amélioration de l'identification soit attribuée principalement à l'alignement fréquentiel linéaire, une amélioration supplémentaire est obtenue pour la voyelle /ae/ grâce à une légère distorsion fréquentielle non-linéaire.
Une application à la normalisation de locuteurs pour la détection de mots dans de la parole continue est aussi discutée.
Parmi les modèles bien adaptés pour classer des séquences figurent les chaînes de Markov d'ordre fixe.
Le lissage de probabilités ou les chaînes de Markov d'ordre variable sont des améliorations de ce modèle qui ont permis d'en augmenter le pouvoir de généralisation pour un coût de stockage moindre.
Dans cet article, nous proposons une autre extension, basée sur un test statistique, qui permet de contrôler la présence de motifs différents dans le modèle sous-jacent à une séquence et dans celui de sa classe d'affectation.
A titre d'illustration, nous comparons les résultats fournis par ces différents modèles sur le benchmark de séquences DNA d'E. coli du répertoire de l'UCI et montrons l'influence du choix des paramètres sur leurs performances.
Cet article propose une nouvelle méthode pour la construction automatique de hiérarchies sémantiques adaptées à la classification et à l'annotation d'images.
L'objectif est de fournir une mesure qui est plus proche de la sémantique des images.
Nous proposons ensuite des règles, basées sur cette mesure, pour la construction de la hiérarchie finale qui encode explicitement les relations hiérarchiques entre les différents concepts.
La hiérarchie construite est ensuite utilisée dans un cadre de classification sémantique hiérarchique d'images en concepts visuels.
Nos expériences et résultats montrent que la hiérarchie construite permet d'améliorer considérablement les résultats de la classification.
Cet article discute quelques points importants de la recherche actuelle en synthèse de la parole.
La modélisation des caractéristiques du locuteur et les émotions sont utilisées comme exemples des nouvelles tendances dans le domaine de la synthèse de la parole.
L'accent est mis sur les relations avec la reconnaissance de la parole.
De nouvelles méthodes, comme l'apprentissage automatique, et l'utilisation de nouvelles techniques sont aussi discutées.
Nous présentons un système de reconnaissance de parole continue multi-locuteurs, appliqué aux occlusives du français dans tous les contextes vocaliques.
L'architecture du système est fondée sur les résultats de plusieurs expériences de perception auditive et d'analyse acoustique du bruit d'explosion.
Ces expériences nous ont permis d'apprécier le pouvoir de discrimination du bruit d'explosion par rapport à la reconnaissance du lieu d'articulation des occlusives, de montrer que la connaissance, à l'avance, de la voyelle permet de meilleurs performances d'identification de l'occlusive.
Par conséquent, nous avons conçu et évalué un décodeur acoustico-phonétique des occlusives qui tient compte des informations sur la nature de la voyelle pour reconnaître l'occlusive.
Des expériences ont été effectuées sur deux corpus et montrent un taux de reconnaissance de 90% sur de nouveaux locuteurs et locutrices.
La plupart des difficultés dans un système de compréhension de la parole proviennent de la grande incertitude propre au signal acoustique.
Comme aucun décodeur phonétique ne semble en mesure de fonctionner parfaitement à plus ou moins breve échéance, nous allons devoir utiliser un maximum de connaissances en provenance des niveaux superieurs.
Nous présentons ici une solution possible pour intégrer certaines de ces informations dans le domaine particulier de l'interrogation d'une base de données administratives.
Tout d'abord, nous exposons comment des connaissances contextuelles peuvent être extraites de représentations locales
Lors du séminaire ESCA-NATO “Speech Under Stress” (Lisbonne, Portugal, Septembre 1995), les discussions ont été centrées sur la définition et la modélisation du stress et de ses effets.
Sur la base de ces discussions, l'article présenté ici essaie de produire une définition du stress et propose plusieurs modèles du stress visant á clarifier certains enjeux du domaine et qui pourraient être adoptés par la communauté de recherche en parole.
La notion de stress est assez vague et est utilisée de manière différente dans différents domaines de recherche : il n'y a donc pas de définition précise de stress.
On propose une séparation plus nette entre les “Stressors” (les causes du stress) et le “strain” (les effets du stress) et des méthodes pour relier les “Stressors” au “dstrain” sont présentées,
Finalement, on propose des axes de recherche future dans le domaine.
Dans cet article, nous nous intéressons à l'extraction automatique des traits de visages (yeux, sourcils, nez, bouche, menton) ainsi qu'à la reconnaissance des six expressions faciales définies par Ekman [19].
Nous exploitons pour cela des versions modifiées du modèle actif d'apparence initialement proposé par Cootes et al. [11] qui permet de représenter à la fois la forme et la texture d'un visage.
L'extraction des traits faciaux est faite à l'aide d'un modèle actif d'apparence hiérarchique, calculé à partir des réponses de visages à des bancs de filtres de Gabor.
Deux modèles d'expressions faciales sont ensuite proposés, calculés à patir du modèle d'apparence standard (non hiérarchique), pour reconnaître puis supprimer ou modifier l'expression d'un visage inconnu.
L'hippocampe et l'amygdale sont deux structures cérébrales intervenant dans plusieurs fonctions cognitives fondamentales.
Leur segmentation, à partir de volumes d'imagerie par résonance magnétique (IRM), est un outil essentiel pour mesurer leur atteinte dans certaines pathologies neurologiques, mais elle est rendue difficile par leur géométrie complexe.
Nous considérons leur segmentation simultanée par une méthode de déformation homotopique compétitive de régions.
Celle-ci est guidée par des connaissances anatomiques relationnelles ; ceci permet de considérer directement des structures atrophiées.
Rapide, l'algorithme donne, pour les deux structures, des résultats comparables à la segmentation manuelle avec une meilleure reproductibilité.
Ses performances, concernant la qualité de la segmentation, le degré d'automatisation et le temps de calcul, sont parmi les meilleures de la littérature.
Le recalage non supervisé d'images médicales volumiques reste un problème difficile en raison de l'importante variabilité et des grandes différences d'information pouvant apparaître dans des séquences d'images de même modalité ou dans des couples d'images multimodales.
Nous présentons dans cet article des méthodes robustes de recalage rigide d'images 2D et 3D monomodales et multimodales, reposant sur la minimisation de mesures de similarité inter-images.
Les méthodes proposées s'appuient sur la théorie de l'estimation robuste et mettent en œuvre des M-estimateurs associés à des techniques d'optimisation stochastique multigrilles rapides.
Ces estimateurs robustes sont évalués à travers le recalage d'images médicales volumiques monomodales (IRM/IRM) et multimodales (IRM/TEMP).
Elles permettent de plus de recaler des couples d'images sur lesquels les méthodes classiques échouent.
Cette étude porte sur l'analyse, la modélisation et la simulation de capacités humaines de planification et d'interaction.
Les protocoles expérimentaux ont été analysés des points de vue de la planification et des interactions.
Les modèles proposés de planification et d'interaction sont intégrés de façon homogène à une nouvelle architecture d'agent appelée BDIggy.
Le modèle de l'interaction humaine 1) s'appuie sur la théorie des actes de langage pour modéliser les énoncés, à l'aide d'un ensemble de performatives appliquées à des états mentaux 2) utilise un modèle du discours, représenté par des automates temporisés, pour décrire la dynamique des conversations humaines.
Dans BDIggy, l'interaction et la planification s'entrelacent grâce aux concepts BDI.
L'architecture BDIggy est validée par un test « à la Turing » .
La télésurveillance médicale est une branche de la télémédecine qui vise à surveiller à distance les paramètres d'un patient.
Cet article décrit les principes de ces dispositifs et cite les projets qui ont bénéficié à des patients.
Cet article s'intéresse à l'étude des propriétés de l'Analyse en Composantes Principales Relationnelle (ACPR) qui analyse un vecteur aléatoire conditionnellement à la réalisation d'un paramètre induisant une relation binaire sur l'espace probabilisé de référence.
Nous détaillons les propriétés de la covariance et de l'espérance relationnelles qui sont à la base de cette technique d'analyse connue mais finalement peu étudiée.
L'article présente quelques illustrations des propriétés que nous mettons en évidence, et qui éclairent les interprétations en ACPR.
Souvent, les processus de prise de décision sont incorporés dans des procédures « officielles » pour prendre en compte le focus dans tous les cas.
Cependant, les procédures mènent souvent à des solutions sous-optimales pour des prises de décisions spécifiques, et les acteurs sont obligés de développer des pratiques pour prendre en compte la spécificité du contexte dans lequel la décision est prise.
Ce fossé entre les procédures et les pratiques est connu dans différents domaines (tâches effectives et prescrites, logique de fonctionnement et logique d'utilisation, etc.).
Nous avons montré quelles sont les différences dans un formalisme basé sur le contexte, nommé les graphes contextuels.
Dans ce papier, nous discutons la possibilité d'arguer de manière explicite l'utilisation de « bonnes » ou « mauvaises » pratiques pour la formation d'apprenti humain, ceci grâce à une acquisition incrémentale de la connaissance et de l'apprentissage de nouvelles pratiques par un système.
Nous discutons ces aspects dans le cadre d'une application de taille réelle, la sécurité routière (modélisation de comportements de conducteurs), mais ces idées peuvent être facilement applicables à d'autres domaines.
Cet article présente un modèle de modulation AM–FM pour l'analyse, la synthèse, et le codage de la parole.
Le modèle AM–FM décrit le signal de parole comme la somme de différents signaux représentant les fréquences formantiques, modulés en fréquence et amplitude.
Un filtrage multibandes et une démodulation basée sur un algorithme de séparation d'énergie sont utilisés pour analyser le signal.
Une analyse par démodulation multibandes (ADM) est tout d'abord employée afin d'estimer la fréquence fondamentale du signal, en se basant sur la fréquence instantanée moyenne comme estimation des harmoniques du pitch.
Cet algorithme de suivi du pitch conduit à une estimation lisse et précise de la fréquence fondamentale.
Un vocoder utilisant une modulation AM–FM est ensuite mis en oeuvre pour modéliser le signal par la somme de ses harmoniques.
Un banc de filtres adaptatif permet d'extraire les bandes de fréquence formantiques et un algorithme fondé sur la séparation d'énergie est utilisé pour démoduler les harmoniques des formats en signaux instantanés modulés en amplitude et en fréquence.
Différents algorithmes sont proposés conduisant à un codage efficace à 4.8–9.6 kbits/sec de l'enveloppe et la fréquence instantanée des résonances formantiques.
Enfin, l'importance perceptive de la modulation des résonances du signal de parole est étudiée et démontre que la modulation d'amplitude ainsi obtenue est indépendante du locuteur et du phonème.
Dans cet article sont présentés les résultats d'une expérience de reconnaissance indépendante du locuteur effectuée sur un corpus de parole continue pour l'italien.
Le système de reconnaissance utilise des unités phonétiques modélisées par des chaînes de Markov cachées multi-gaussiennes.
Différents ensembles d'unités phonétiques ont été évaluées, en allant des modèles indépendants du contexte aux triphones les plus spécifiques : des unités propres aux mots outils ont également été évaluées.
La reconnaissance a été effectuée sans contrainte syntaxique sur un vocabulaire de 979 mots, c'est-à-dire avec un facteur de branchement de 979.
Les résultats confirment l'intérêt des modèles dépendants du contexte, pour lesquels le taux de mots reconnus est proche de 80% sur un ensemble de 300 phrases.
Le problème de l'extraction des règles d'association est un problème majeur en fouille de données.
De nombreuses connaissances sont représentées sous la forme de règles d'association et la recherche exhaustive de ces règles est souvent très coûteuse en temps lorsque les données sont volumineuses et très corrélées.
Nous proposons une nouvelle approche basée sur une recherche non exhaustive des règles d'association dans une base de données.
L'algorithme proposé permet (i) d'extraire les meilleures règles d'association, étant donnée une mesure de qualité, (ii) d'extraire des "pépites" de connaissances dans les données (c-à-d., des règles d'association ayant un faible support et une forte confiance).
Nous passons en revue dans cet article une série de méthodes utilisées pour l'évaluation des systèmes de conversion texte-parole et commentons leurs avantages et désavantages respectifs.
Ce panorama se limite aux méthodes subjectives, à savoir celles utilisant des auditeurs humains.
Nous ne traiterons pas ici des méthodes objectives qui tentent d'évaluer la qualité par des techniques basées sur le traitement de signal.
Dans la section 1, nous discutons de quatre facteurs influençant les caractéristiques de l'évaluation : les composants du système de conversion texte-parole, le niveau du texte, aspects de la parole et la fonction.
Dans lessections 2 et 3, nous présentons des méthodes en relation avec les modules linguistique et acoustico-phonétique.
Enfin, dans la section 4, nous tirons quelques conclusions générales.
Deux expériences concernant la détection de l'accent lexical dans des mots isolés en anglais sont présentées.
Une méthode non statistique basée sur les variation de la fréquence fondamentale est proposée.
Ses résultats sont meilleurs que ceux obtenus par l'utilisation de valeurs statiques du fondamental.
Pour éviter le problème de la segmentation syllabique, on présente deux méthodes qui déterminent des régions d'accentuation dans un mot.
Les régions trouvées peuvent etre considérées comme étant des régions de fiabilité phonétique.
Dans la deuxième expérience, l'accent lexical est analysé dans des mots prononcés avec une montée de la voix.
Le maximum d'énergie est le meilleur indicateur d'accent, suivi par la durée des syllabes.
Enfin, il est proposé que l'étude de l'accent lexical en parole continue tienne compte de l'utilisation générale des variables prosodiques dans les phrases.
Les interfaces cerveau-machine (BMI : Brain-Machine Interface) sont des systèmes de communication directe entre un individu et une machine ne reposant pas sur les canaux de communication standard que sont nos nerfs périphériques et nos muscles.
Dans une BMI, l'activité cérébrale de l'utilisateur est enregistrée, analysée et traduite en commandes destinées à la machine.
Nous présentons enfin un état de l'art des différentes interfaces BMI développées jusqu'alors, en nous attachant plus particulièrement à celles dédiées à l'aide aux personnes atteintes d'un handicap moteur sévère dans leur tâche de communication ou de contrôle de machines.
Tenir compte de l'interdépendance des paramètres articulatoires permet de réduire la taille du dictionnaire de codage et d'améliorer les conditions de recherche optimale.
Les approximations initiales des vecteurs articulatoires pour la résolution du problème d'inversion sont calculées le long des trajectoires des paramètres articulatoires sur des syllabes synthétisées.
La prise en compte d'une transformation linéaire par morceaux de l'espace des paramètres articulatoire en l'espace des paramètres articulatoire ainsi que de la valeur minimale de l'aire sagittale du conduit vocal et du nombre de Reynolds permet d'accélérer le processus d'optimisation d'un facteur supérieur à 100.
Ce papier est consacré essentiellement à notre mesure heuristique, nommée HVS (Heuristique for Variable Selection)[YAC 97], que nous utiliserons pour la sélection de variables.
HVS ne demande que peu de calculs simples, faciles à implémenter.
Nous testerons son efficacité sur un problème de discrimination et un problème de régression, après avoir montré sa capacité de détection et de quantification de pertinence.
Ce papier illustre l'utilisation de techniques de traitement d'image pour segmenter le plan temps-fréquence (et temps-échelle).
Cette étude est appliquée à la séparation d'ondes sismiques.
On considère des données issues d'une rangée de capteurs.
Pour chaque signal enregistré, l'application d'une transformée temps-fréquence décrit l'information dans une image sur laquelle les différentes ondes sont localisées et séparées.
La segmentation par Ligne de Partage des Eaux (LPE) de ces représentations à deux dimensions permet une caractérisation automatique des filtres temps-fréquence menant à la séparation des différentes ondes.
Ensuite, pour appliquer cet algorithme de séparation à l'ensemble des signaux issus des différents capteurs, on utilise la continuité d'un signal à l'autre pour effectuer le suivi des différentes ondes d'une image à l'autre.
Hormis une phase d'initialisation, on obtient ainsi un algorithme automatique.
Cet algorithme est validé et comparé à une méthode classique en sismique sur un jeu de données réelles.
En comparaison, l'algorithme proposé a l'avantage de séparer toutes les ondes simultanément, et sans introduire d'artefacts.
Les limites de l'algorithme sont atteintes lorsque les motifs caractérisant chacune des ondes ne sont plus convenablement séparés dans la représentation temps-fréquence.
Nous présentons un nouvel algorithme qui contribue à étendre le formalisme de l'Apprentissage par Renforcement (RL) aux Processus Décisionnels Markoviens Partiellement Observés (POMDP).
L'idée principale de notre méthode est de construire une extension d'état, appelée observable exhaustif, qui permet de définir un nouveau processus qui est alors markovien.
Nous démontrons que résoudre ce nouveau processus, auquel on peut appliquer les techniques classiques de RL, apporte une solution optimale au POMDP original.
Nous appliquons l'algorithme déduit de ce résultat sur plusieurs exemples pour en tester la validité et la robustesse.
Le Programme Archivage du LACITO (Laboratoire de Langues et Civilisations à Tradition Orale du CNRS) a pour but la pérennisation, l'exploitation et la diffusion de documents linguistiques intégrant texte et son, en particulier les enregistrements faits et transcrits sur le terrain par les chercheurs du laboratoire.
L'annotation (transcription, analyse, gloses interlinéaires, traductions) est balisée selon la norme XML et synchronisé phrase par phrase avec l'enregistrement numérisé, pour donner accès simultanément au texte et au son.
Dans la mesure du possible des outils logiciels génériques et librement disponibles sont utilisés.
Les documents produits sont consultés à l'aide des browsers les plus courants sur Internet.
Le texte balisé est manipulé à l'aide d'outils génériques XML.
Une centaine de documents dans une vingtaine de langues ont été préparés, dont certains sont disponibles sur Internet.
Nous présentons un environnement et des techniques élaborées pour la représentation et le traitement de connaisances acoustiques, phonétiques et lexicales pour la reconnaissance de la parole.
Les outils proposés permettent d'effectuer de manière uniforme, continue et dynamique le codage et le traitement de données de type numérique (signal, paramètres, formes, etc.) et d'informations de nature symbolique (mots, phonèmes, syllables, traits, indices, etc.).
La mise en oeuvre de ces méthodes est décrite à partir d'une application concernant la reconnaissance multi-locuteur des 26 mots correspondant aux lettres de l'alphabet énoncées en français.
Malgré la difficulté bien connue de ce vocabulaire, les résultats obtenus valident parfaitement cette approche du problème, particulièrement dans le cas des mots acoustiquement très proches.
Ce papier va d'abord donner un bref aperçu des recherches de Simon en psychologie, et va ensuite se concentrer sur ses travaux concernant la psychologie des experts.
Dans cet article, nous présentons une nouvelle technique de traitement pour réseaux de microphones ayant pour but la déréverbération aveugle des signaux de parole altérés par l'acoustique de salle.
La méthode repose sur le traitement séparé des composantes en phase-minimale et passe-tout des signaux de sortie des microphones.
Les composantes en phase-minimale sont traitées dans le domaine cepstral, où l'on effectue un moyennage spatial suivi d'un filtrage passe-bas.
Les composantes passe-tout, qui contiennent l'information de position de la source, sont traitées dans le domaine fréquentiel en effectuant une formation de voie suivie de l'extraction d'une composante en phase-minimale.
Puisqu'elle repose sur le traitement spatio-temporel d'un ensemble de trames synchronisées provenant de plusieurs microphones, cette technique peut être utilisée dans des environnements acoustiques variants tels que l'on rencontre en pratique.
Des réponses impulsionnelles de salles synthétisées au moyen d'un ordinateur sont utilisées afin d'évaluer la nouvelle technique et de la comparer à une formation de voie conventionnelle sous des conditions contrôlées.
Les résultats indiquent une augmentation significative du gain d'antenne et un effet de déréverbération marqué.
On présente les résultats d'une recherche cognitive sur les déficits de langage d'un patient avec un aphasie de Wernicke.
La compréhension de ce patient, R.D. est très pauvre en ce qui concerne la langue parlée, sa compréhension de la langue écrite est bonne.
On trouve de nombreux néologismes et paraphrases verbales dans ses productions spontanées ainsi que dans ses lectures à voix haute.
A la suite de Butterworth (1979) on pense que les néologismes découlent de difficultés pour reactiver les spécifications phonologiques dans le lexique de sortie pour la parole. On trouve que la fréquence des mots influe sur le taux de succès dans les recherches tandis que la distinction syntaxique entre mot de la classe fermée et mot de la classe ouverte est sans influence.
R.D. épelle mieux qu'il ne dénomme et peut épeller des mots qu'il est incapable de dire correctement.
Quand il épelle, ses erreurs découlent d'essais vers un mot cible à partir d'une information orthographique partielle (les neologismes semblent dépendre de rappels partiels de l'information phonologique).
Nous pensons que les sujets normaux commettent dans certaines circonstances des erreurs de même type dans le langage parlé et écrit.
Le modèle de Garrett (1982) est utilisé pour discuter de la facon dont on peut interpréter les aphasies avec jargon et d'autres formes de Wernicke.
L'analyse des déficits de R.D., conjointe aux analyses d'autres patients fournis par la littérature, permer d'avancer certaines propositions valables pour les théories de traitement du langage.
En outre, 1) la compréhension et la production de mots écrits familiers n'est pas obligatoirement médiatisée par la phonologie ; 2) les lexiques phonologiques et orthographiques sont distincts ; 3) les morphèmes sont représentés séparément dans le lexique phonologique ; 4) la fréquence d'usage affecte l'aisance et la rapidité de réactivation des items dans le lexique phonologique ; 5) pour les deux lexiques cette reactivation ne se fait pas en tout ou rien.
Les quantificateurs vectoriels ont traditionnellement été utilisés dans les systémes de reconnaissance de parole comme pré-processeurs pour des algorithmes sophistiqués tels que la modélisation markovienne sous-jacente (HMM) ou l'alignement dynamique temporel (DTW).
Récemment, des systèmes plus simples basés plus directement sur la quantification vectorielle (VQ) ont été proposés pour la reconnaissance de mots isolés dans de petits vocabulaires.
Le problème crucial avec de tels systèmes réside dans le manque d'information temporelle dans l'algorithme de reconnaissance.
De nouvelles variantes de systèmes VQ incorporant cette information sont décrites.
La principale nouveauté consiste en une technique d'histogramme conditionnel incluant les probabilités relatives de mots-codes successifs dans la mesure de distorsion utilisée dans l'algorithme de reconnaissance VQ.
Plusieurs algorithmes de ce type sont appliqués à la reconnaissance de lettres énoncées extraites de l'alphabet anglais, sous-ensemble du vocabulaire orthographique d'IBM.
Les résultats de nos simulations éclairent les mérites relatifs de ces algorithmes.
Les systèmes automatiques de dialogue téléphonique contiennent généralement un module de compréhension chargé de traiter les sorties du module de reconnaissance automatique de parole. Ce traitement consiste à extraire non-seulement le type de requête exprimée par l'utilisateur mais aussi les paramètres de cette requête tels que les expressions numériques, temporelles ou bien encore les noms propres.
Ces expressions sont généralement appeleés des Entités Nommées et leurs définitions peuvent être génériques ou bien liées à un domaine d'application particulier.
Détecter et extraire de telles entités dans le cadre d'un système automatique de dialogue téléphonique à initiative mixte tel que How May I Help You ?
sm,tm (HMIHY) est le sujet de cette étude.
Après avoir passé en revue les méthodes habituelles basées sur des grammaires écrites manuellement ou bien sur des étiqueteurs statistiques, nous proposons une nouvelle approche permettant de combiner leurs avantages respectifs.
Nous proposons également une nouvelle architecture, pour les systèmes automatiques de dialogue téléphonique, qui utilize les résultats du module de compréhension afin d'améliorer la transcription des requêtes des utilisateurs.
Toutes les méthodes proposées sont évaluées sur un corpus contenant de réels dialogues entre des utilisateurs et une application mise en service sur une large échelle.
Cet article décrit un système de transcription orthographique-phonétique bidirectionnel basé sur une stratégie combinant des techniques basées sur les données et un formalisme de règles.
Notre approche fournit une analyse hiérarchique des mots, incluant la position de l'accent, sa morphologie et sa structure syllabique.
La génération est réalisée par une technique d'analyse syntaxique probabiliste où les probablilitiés sont apprises à partir d'un lexique.
Nos corpus d'apprentissage et de test sont constitués d'épellations et de prononciations des 10 000 mots les plus fréquents du Brown Corpus.
L'étiquetage phonétique a été enrichi par des marqueurs indiquant la morphologie et l'accentuation.
Les résultats sont fournis sur deux grammaires distinctes, correspondant à deux stades d'évolution du travail.
Notre travail antérieur avec la première grammaire nous a incitéà modifier le formalisme de la grammaire, ce qui a abouti à des contraintes plus fortes avec moins de règles,
Nous avons évalué les performances de notre système tant au niveau du mot entier que du phonème.
Pour le corpus de test, la précision atteinte au niveau du mot est de 69.3% ; elle est de 91.7% au niveau du phonème, en utilisant un répertoire de 52 phonèmes.
Bien que cet article traite essentiellement de la transcription orthographique-phonétique, notre système est également un système de génération dans l'autre sens, comme décrit dans un article antérieur (Meng et al., 1994a).
Nous pensons que notre formalisme sera applicable en particulier pour entrer vocalement des mots inconnus dans un système de reconnaissance.
La méthode proposée repose sur le couplage de contraintes de type “n-grammes” conventionnelles, avec des contraintes grammaticales autorisant certaines déviations, au cours d'une stratégie de décodage multi-passes.
Les segments à grande fiabilité correspondent aux séquences de mots satisfaisant à la fois les contraintes n-grammes et les contraintes grammaticales.
Pour une plus grande efficacité, la grammaire hors contexte exprimant les contraintes grammaticales est approximée au moyen d'un automate à état fini.
Par ailleurs, les possibilités de déviations telles que les insertions, les éliminations, et les substitutions sont considérées lors de l'application de ces contraintes.
En conséquence, l'application des contraintes grammaticales s'avère plus robuste que dans le cas des analyseurs de syntaxe robustes conventionnels qui n'autorisent, comme déviations, que des insertions.
Nos expériences confirment que la méthode proposée permet la reconnaissance partielle d'un énoncé avec une plus grande fiabilité que les méthodes de reconnaissance de la parole continue exclusivement basées sur des contraintes de type n-grammes.
Enfin, nos résultats indiquent que le fait d'autoriser un nombre accru de déviations par rapport aux contraintes grammaticales permet d'améliorer les performances des analyseurs robustes conventionnels.
L'ajustement d'une ellipse sur des données 2D est un très vieux sujet en estimation et en RDF, qui a donné lieu à de nombreuses études [2,6,10,15,16,21] et en suscite encore aujourd'hui [12,17,19,24].
Un peu moins étudiée [5], la représentation polaire de l'ellipse constitue une alternative plus coûteuse car elle nécessite l'optimisation de sa paramétrisation.
D'une représentation nécessitant au plus 5 paramètres à une autre définie par 5 + N (N étant le nombre de données), le choix semble évident.
Cependant, nous proposons dans cet article de nouvelles idées sur la question.
Tout d'abord, nous montrons que l'estimation séparée des paramètres et de la paramétrisation de l'ellipse permet de simplifier le problème en aboutissant respectivement à une inversion directe pour les premiers et à la recherche des racines d'un polynôme du 4ième ordre pour la seconde.
Nous montrons également que la paramétrisation est « porteuse » de l'information dimensionnelle de l'ellipse et qu'en la « perturbant » correctement dans le processus de minimisation il est possible de forcer la solution à rester dans un espace paramétrique préétabli.
Ce résultat nouveau permet de fournir une solution sans biais dimen- sionnel même dans un contexte fortement bruité et incomplet.
Une enveloppe de confiance est ensuite estimée assurant à la fois un encadrement plus large de la solution et le rôle de filtre pour la recherche des segments voisins candidats potentiels pour affiner l'estimation.
Enfin, nous proposons une stratégie de regroupement/ajustement suivie d'une phase de décision floue constituant ainsi un schéma robuste de détection de formes elliptiques dans les images.
Les difficultés d'interaction émotionnelle intrinsèques à l'autisme en font un domaine d'application opportun pour les interfaces homme-machine émotionnelles.
Cet article traite des problèmes liés à l'assimilation d'expressions faciales émotionnelles survenant au cours d'un dialogue.
Nous avons développé une interface homme-machine qui associe des expressions faciales émotionnelles à chaque réplique d'un dialogue.
Les expressions faciales peuvent être représentées dans deux styles graphiques différents, l'un étant plus caricatural que l'autre.
Nous décrivons un protocole expérimental pour évaluer l'apport de cette interface avec 10 adolescents autistes et 10 enfants non autistes.
Les résultats montrent que les sujets sans autisme réussissent mieux que les sujets avec autisme à utiliser conjointement les expressions faciales et la parole pour désambiguïser un dialogue.
Dans l'étude présente, nous avons examiné les unités et les stratégies qui enclenchent l'organisation de l'information phonétique, spécifique aux premiers mots de l'enfant.
Pour ce faire, nous nous basons sur les données transcrites longitudinales, recueillies sur huit enfants unilingues germanophones L1, dans le cadre du projet kielois sur l'acquisition phonique précoce.
Ces données ont été saisies chaque semaine sur des enfants âgés de sept à treize mois au début de l'expérience.
Nous en tirons, après analyse, les résultats suivants :
L'enfant n'emploie au début qu'un répertoire limité de modèles articulatoires, qui peuvent déterminer la structure phonétique de la plupart de ses premiers mots, en tant qu'unités de codage basiques.
Ces modèles sont manifestement construits sur la base des modes articulatoires préférés de l'enfant, et aussi à partir des caractéristiques acoustique et auditive les plus frappantes des mots cibles du langage adulte.
Il y a globalement cinq types d'évolution de ces modèles au cours du temps.
La linguistique de l'énonciation développée par Antoine Culioli est une théorie des opérations prédicatives et énonciatives.
Les deux concepts d'opération et de représentation sont ainsi au cœur du modèle épistémologique et de la méthode d'analyse, qui cherche à démêler les relations en jeu dans la construction des énoncés pour les rapporter à l'activité symbolique de représentation et au processus mental qu'elle présuppose.
Un examen critique du modèle des trois niveaux de représentation nous amène à distinguer la fonction de représenter du mode de représenter.
Le retour à Saussure nous permet de revoir le modèle en y intégrant, au niveau des représentations notionnelles, ce que celui-ci nomme des figures.
Une brève étude de cas illustre, pour finir, le développement théorique.
Cet article présente une étude expérimentale sur le “déplacement d'accent” dans des énoncés ambigus et non-ambigus.
Des séquences ambiguës comme Chinese fan fournissent des caractéristiques phonologiques pouvant correspondre à deux structures différentes.
Si la séquence est un syntagme syntaxique, Chinese étant un adjectif venant modifier le nom fan, alors fan présente une proéminence relative plus grande.
Si Chinese est un nom et que la séquence est un composé, alors fan est déssacentué et c'est Chinese qui présente la plus grande proéminence.
De plus, le fait que Chinese soit un item dont l'accent peut être déplacé laisse supposer que le déplacement d'accent intervient dans l'interprétation syntagmatique.
Les mots à catégories ambiguës avec déplacement d'accent potentiel pourraient donc contenir, plus que les items non-soumis au déplacement d'accent, des indices précoces pour la catégorisation syntaxique, sous la forme de patrons prosodiques modifiés.
Les données de production montrent que les patrons de déplacement d'accent correspondent en effet aux catégories syntaxiques, mais seulement si le deuxième élément de la séquence n'est pas branché à droite.
Une expérience de compréhension avec des énoncés a catégories ambiguës suggère que les patrons de proéminence de syntagmes ou d'expressions composées ainsi que le déplacement d'accent facilitent le traitement syntaxique.
Une deuxième expérience de compréhension a permis de dupliquer cette observation et d'élargir l'analyse à des énoncés non-ambigus comme Torquay College.
Dans les énoncés non-ambigus, de nouveau, I'accent de syntagme apparait comme affectant le traitement, ce qui n'est pas le cas pour le déplacement d'accent.
Cet article traite de la détection de cibles mobiles sur fond de fouillis dans le cadre de radar aéroporté monostatique utilisant trois formes d'antennes : linéaire uniforme (ALU) ; courbée uniforme (ACoU) et circulaire uniforme (ACU), en visée non latérale.
Outre le fait que ces méthodes sont très coûteuses en calculs, leurs performances ne sont pas optimales (l'estimateur de la matrice de covariance des interférences plus bruit est biaisé) dans le cas où les données ne sont pas iid.
C'est le cas, par exemple, pour les radars utilisant une ALU en visée non latérale et ceux utilisant une ACoU ou ACU.
En effet, dans ces configurations, la densité spectrale du fouillis présente une dépendance en distance qui introduit une non stationnarité des données.
Un nouvel algorithme de codage multibandes synchrone au pitch appelé PSMB (“pitch synchronous multi-band”) est proposé.
Pour chaque trame cet algorithme génère un signal représentatif d'un cycle de pitch (PCW) (“pitch-cycle waveform”) en utilisant un modèle d'excitation multibandes MBE (“multi-band excitation”).
Selon si une trame est liée ou pas à la trame précédente, le signal PCW correspondant est quantifié sur deux parmi trois “codebooks”.
Dans le cas contraire, le signal PCW est codé utilisant le même “codebook” stochastique et un “codebook” d'excitation à une impulsion à bande limitée BSPE (“bandlimited single pulse excitation”).
Cette nouvelle approche définit un codage basé sur la période du pitch.
De ce fait, le nouvel codeur réduit les faiblesses de l'algorithme MBE amélioré (IMBE).
Le codeur PSMB développé fonctionne à 4 kbps et produit une meilleure performance que le codeur IMBE Inmarsat à 4.15 kbps.
Les tests d'écoute effectués montrent une légère amélioration au niveau perceptuel par rapport au codeur CELP (“code excited linear predictive”) FS1016 à 4.8 kbps.
Des algorithmes de recherche rapide dans les trois “codebooks” sont développés dans le cadre du codeur PSMB proposé.
Cette recherche rapide a rendu la complexité de calcul du codeur PSMB comparable à celle du codeur FS1016.
Cet article décrit le travail récent effectué au MIT pour développer des systèmes de dialogue oral homme-machine multilingues.
Notre aproche est basée sur l'hypothèse qu'une représentation sémantique commune peut être extraite des énoncés quelle que soit la langue, au moins dans des domaines restreints.
Dans notre conception de ces systèmes, l'information dépendante de la langue est séparée le plus possible du noyau du système et encodée dans des structures de données externes.
Le gestionnaire interne du système, les modules d'analyse du discours et de dialogue, et les bases de données sont tous maintenues dans un état indépendant de la langue.
Dans cet article, nous décrivons plus particulièrement le système de dialoque oral multilingue Voyager qui peut gérer des dialogues concernant la région géographique de Cambridge, MA, aux Etats-Unis.
Le système peut fournir des informations concernant les distances, les durées de trajets ou les directions entre des objets situés dans cette zone (par exemple, les restaurants, hotels, banques, bibliothèques, etc.).
On fournit des résultats d'évaluation des versions en anglais, japonais et italien.
D'autres recherches multilingues annexes sont également brièvement mentionnées.