Nous présentons le projet Papillon dédié la construction d'une base lexicale multilingue linguistiquement riche. Ce projet s'appuie sur le principe de construction collaborative, qui permet à chacun, professionel ou amateur, institution ou individu, de contribuer, dans la mesure de ses moyens, à ce grand chantier. Pour qu'un tel travail collaboratif puisse s'amorcer, il est nécessaire de fournir un ensemble conséquent d'informations lexicales multilingues, sur lesquels les contributeurs pourront s'appuyer. Après avoir présenté l'architectures linguistique, lexicale et informatique du projet Papillon, nous détaillons la méthode utilisée pour créer les informations initiales mises à disposition des contributeurs.
L'article s'intéresse à la formalisation, dans une optique de calcul, des définitions lexicographiques BDéf (Altman et Polguère, 2003). La richesse informationnelle de ces définitions laisse envisager de nombreux calculs utiles aussi bien à la pratique lexicographique qu'à une réflexion plus générale sur la modélisation du sens lexical. De tels calculs ne peuvent toutefois être mis en oeuvre sans une formalisation poussée des définitions, qui se traduira ici par le recours aux structures de traits typées (Carpenter, 1992). En permettant à chaque instant de s'assurer de manière automatique de la cohérence générale du lexique, la formalisation nous permet de mettre en place une nouvelle méthodologie de description lexicale par allers-retours successifs entre les définitions lexicographiques et une métadescription de ces dernières.
Cet article présente un nouvel analyseur syntaxique, nommé SXLFG, qui repose sur le formalisme des Grammaires Lexicales Fonctionnelles (Lexical Functional Grammars, LFG). Nous décrivons l'analyseur non contextuel sous-jacent ainsi que la façon dont les structures fonctionnelles sont efficacement calculées sur la forêt partagée résultant de l'analyse non contextuelle. Nous présentons ensuite les différentes techniques de rattrapage et de tolérance d'erreur que nous avons implémentées pour en faire un analyseur robuste. Enfin, nous donnons des résultats concrets de l'utilisation de SXLFG avec une grammaire du français à large couverture. Nous montrons que notre analyseur, tout en étant un analyseur profond non probabiliste, est à la fois efficace et robuste et permet l'analyse rapide de très gros corpus, bien que la grammaire utilisée pour l'évaluation soit très ambiguë.
Le traitement automatique des langues (TAL) relève à la fois de la démarche scientifique et de la démarche technologique. Dans les deux cas, l'évaluation des systèmes informatiques implémentés est indispensable pour estimer le succès d'une recherche. S'inspirant du cadre ISO pour l'évaluation des logiciels, et utilisant une typologie des systèmes de TAL fondée sur la place de la langue parmi les données d'entrée ou de sortie, cet article analyse le rôle central des métriques d'évaluation à plusieurs étapes du processus de recherche en TAL. L'accent est mis sur les métriques qui comparent un résultat produit avec des résultats corrects attendus. L'analyse de plusieurs situations d'évaluation, en particulier le cas des systèmes de traduction automatique, illustre l'importance d'un choix cohérent des métriques et de l'utilisation conjointe de plusieurs métriques. L'influence du contexte d'utilisation sur le choix des métriques et le cas des systèmes interactifs sont discutés en conclusion.
Le langage XML autorise, par sa souplesse de structuration, des manipulations du contenu qui créent parfois des ruptures arbitraires dans le flot naturel du texte. Ces caractéristiques soulèvent des difficultés lorsque l'on souhaite mettre en oeuvre des techniques d'analyse automatique du contenu des documents XML. Cet article présente cette problématique et y répond, sur le plan théorique, avec l'introduction du concept de contexte de lecture, puis sur le plan pratique, avec une classification automatique des balises XML et la présentation d'un outil générique de gestion des contenus XML.
Cet article d'introduction au numéro pose la problématique de la structuration de terminologie. Il s'agit de mettre en relation des unités terminologiques et donc de construire un réseau de termes. L'article fait le point sur les pratiques et sur l'état d'avancement des travaux en terminologie computationnelle.
Nous présentons une méthode d'analyse de corpus afin de générer une ontologie dans le domaine du e-recrutement. Notre approche combinant statistiques et extraction de n-grammes de mots permet de créer une ontologie en anglais et en français contenant 440 métiers issus de 27 domaines d'activité. Chaque métier est ainsi relié aux compétences nécessaires à sa pratique pour un total d'environ 6 000 compétences différentes. Une évaluation manuelle sur une portion de l'ontologie a été réalisée par un expert du recrutement et a montré des résultats de très bonne qualité.
L'adéquation des alignements en phonèmes fournis par trois systèmes d'alignement automatique pour l'analyse linguistique du schwa est évaluée par comparaison avec un alignement manuel réalisé par deux juges. Les taux et types d'erreurs sont rapportés ainsi que les facteurs linguistiques ayant une incidence sur ces derniers. Les performances des trois systèmes sont influencées en particulier par la nature des consonnes entourant le schwa et par la durée de celui-ci. Des différences sont néanmoins observées entre les alignements des trois systèmes en fonction de la tâche (détection vs placement des frontières) et des caractéristiques structurelles des systèmes. Ces données soulignent l'importance d'un choix réfléchi du système à utiliser et d'une bonne connaissance de ses caractéristiques pour son utilisation à des fins d'analyse phonétique.
Dans cet article, nous décrivons une méthode de classification d'énoncés destinée à la détection d'accord/désaccord dans le dialogue homme-machine. L'étiquetage du dialogue peut être utilisé par le dialogue manager sans avoir à effectuer de parse complexe. Nous proposons une technique de classification à base d'une hiérarchie de classificateurs Support Vector Machines. Une combinaison de trois classificateurs binaires est utilisée pour filtrer les classes pour lesquelles le corpus contient beaucoup d'information et se concentrer sur les classes plus ambigües. Cet article présente une analyse détaillée des traits caractéristiques de classification et propose une amélioration de 2.47% sur l'état de l'art tout en utilisant un modèle de classification plus performant.
Cet article aborde le problème de la représentation formelle des expressions calendaires et en présente une analyse fonctionnelle, fondée sur la représentation catégorique des ordinaux. Il expose ensuite un projet d'aide à la lecture d'un texte biographique long selon une navigation temporelle.
Les connaissances linguistiques évidentes pour l'être humain ne le sont souvent pas pour la machine, et doivent par conséquent être explicitées. Les relations morphosémantiques en sont un exemple. Si l'anglais possède un nombre fini de règles morphologiques pour dériver des noms partant de verbes et inversement, la nature des relations sémantiques des mots dérivés n'est pas évidente pour un système automatique. Nous discuterons ici l'ajout de tels liens à WordNet.
Nous concentrons notre recherche sur le management du dialogue en vue d'activités collaboratives qui impliquent des tâches multiples et simultanées. Le contexte conversationnel pour les activités simultanées est représenté en utilisant un "Arbre de Mouvement de Dialogue" et un "Arbre d'activités" qui soutiennent les fils multiples et " intercalés" du dialogue, au sujet des differentes activités et de leur plan d'exécution. Nous décrivons aussi la sélection cumulative du message, l'agrégation, et la méthode de génération employées dans ce contexte. Le systeme de génération doit aussi être capable de contrôler des contextes où il y a de multiples sujets coordonnés par la conversation. Nous montrons comment utiliser les représentations dynamiques de contexte pour l'interprétation flexible et naturelle de contributions de dialogue dans de telles applications. Nous démontrons que ces techniques sont viables dans un système de dialogue pour les conversations avec des robots semi-autonomes et mobiles.
Dans cet article, nous proposons d'abord une terminologie française pour la présentation des résultats d'évaluation de résumés automatiques. Ensuite, nous décrivons les paramètres expérimentaux devant être précisés lors de la présentation des résultats d'une évaluation. Ces paramètres expérimentaux, identifiés à partir d'une analyse de vingt-deux évaluations dans le domaine du résumé automatique, sont des informations sur le déroulement d'une évaluation. En outre, nous faisons ressortir les tendances et les problèmes méthodologiques associés à ces paramètres et formulons des recommandations pour guider le chercheur souhaitant évaluer des résumés automatiques.
Aujourd'hui, l'approche la plus courante en traitement de la parole consiste à combiner un reconnaisseur vocal statistique avec un analyseur sémantique robuste. Pour beaucoup d'applications cependant, les reconnaisseurs linguistiques basés sur les grammaires offrent de nombreux avantages. Dans cet article, nous présentons une méthodologie et un ensemble de logiciels libres (appelé Reguls) pour dériver rapidement des reconnaisseurs linguistiquement motivés à partir d'une grammaire générale partagée pour le catalan, le français et l'espagnol.
La constitution de corpus est une des premières priorités que rencontrent les langues peu dotées. L'émergence de ressources issues d'Internet, de tailles de plus en plus imposantes et couvrant de nombreuses langues, peut laisser penser que ce point est désormais résolu, ce qui n'est pas le cas. An Crúbadán et W2C. Parallèlement à une évaluation manuelle, nous avons estimé la possibilité d'utiliser un ou plusieurs modules d'identification de langue afin de filtrer le contenu de ces ressources, ce qui s'avère possible mais au prix d'un rappel peu élevé. Pour cette tâche, nous avons testé et réentraîné divers systèmes afin de les adapter au mieux au corse. Ce travail nous permet de mettre à disposition un modèle capable d'identifier le corse ainsi que 17 autres langues européennes.
Sur la base des résultats d'une évaluation de certains des principaux algorithmes connus de génération d'expressions référentielles, cet article explore plusieurs problèmes posés par l'évaluation et présente quelques considérations d'ordre général à prendre en compte lors de l'évaluation des sous-tâches de la génération automatique de textes.
Cet article traite de la parole spontanée, la définissant d'abord sous différents angles et spécificités, puis en envisageant sa transcription de façon diachronique et synchronique. Enfin, par le biais de différentes expériences, réalisées notamment dans le cadre du projet EPAC, nous avons identifié les principaux problèmes que la parole spontanée posait aux systèmes de reconnaissance automatique, et proposons des optimisations en vue de les résoudre.
Dans ce travail, nous nous intéressons à la tâche de prédiction de performance des systèmes de transcription de la parole. Nous comparons deux approches de prédiction : une approche de l'état de l'art fondée sur l'extraction explicite de traits et une nouvelle approche fondée sur des caractéristiques entraînées implicitement à l'aide des réseaux neuronaux convolutifs (CNN). Nous essayons ensuite de comprendre quelles informations sont capturées par notre modèle neuronal et leurs liens avec différents facteurs. Pour tirer profit de cette analyse, nous proposons un système multitâche qui se montre légèrement plus efficace sur la tâche de prédiction de performance
Notre travail s'intègre dans le cadre du projet intitulé « Oreillodule » : un système de reconnaissance, de traduction et de synthèse de la parole arabe. L'objectif de cet article est de présenter l'analyseur sémantique développé pour la compréhension automatique de la parole arabe spontanée. L'évaluation du système montre bien l'efficacité et la robustesse de ce dernier.
S XPipe 2, chaîne modulaire et paramétrable dont le rôle est d'appliquer à des corpus bruts une cascade de traitements de surface. Préalable nécessaire à une possible analyse syntaxique, ils peuvent également servir à préparer d'autres tâches. Développé pour le français mais également pour d'autres langues, S XPipe 2 comprend, entre autres, divers modules de reconnaissances d'entités nommées dans du texte brut, un segmenteur en phrases et en tokens, un correcteur orthographique et reconnaisseur de mots composés, ainsi qu'une architecture originale de reconnaissance de motifs non contextuels, utilisée par différentes grammaires spécialisées (nombres, constructions impersonnelles...). Nous présentons les fondements théoriques des différents modules, leur mise en œuvre pour le français et pour certains une évaluation quantitative.
L'objectif du projet RITEL est de réaliser un système de dialogue homme-machine permettant à un utilisateur de poser oralement des questions, et de dialoguer avec un système de recherche d'information généraliste. Dans cet article, nous présentons la plateforme actuelle et plus particulièrement les modules d'analyse, d'extraction d'information et de génération des réponses. Nous présentons les résultats préliminaires obtenus par les différents modules de la plateforme.
La prédiction de la liaison en français est un problème de modélisation nontrivial. Nous comparons un algorithme d'apprentissage automatique basé sur la mémoire avec un point de comparaison basé sur des règles. L'apprentissage automatique est entraîné à prédire si la liaison se produit entre deux mots consécutifs en évaluant des traits lexicaux, orthographiques, morphosyntaxiques et sociolinguistiques. Notre étude montre que la meilleure performance est obtenue en utilisant uniquement des trais lexicaux et syntaxiques, à savoir les lettres finales et initiales, la catégorie de laision (obligatoire ou optionelle), les étiquettes morphosyntaxiques, le nombre de syllabed par mot et la distance Levenshtein des vingt mots phonologiques les plus proches. Contrairement à nos attentes, inclure des traits sociolinguistiques rend la précision et le rappel plus bas. L'utilisation des traits lexicaux et syntaxiques a mené à une précision de.80 et un rappel de.85. Elle est non seulement plus élevée que le point de comparaison basé sur des règles, mais aussi plus élevée que celle d'IGTree (un algorithme d'arbres de décision) et celle d'une classification naïve bayésienne. Ripper, un algoritme d'induction de règles plus sophistiqué, a pu produire des résultats similaires à l'algorithme basé sur la mémoire, mais Ripper détecte moins d'instances de liaison optionelle, ce qui résulte en un rappel plus bas pour cette catégorie. Il paraît que la possibilité de généralisation des exemples spécifiques en contexte aide la prédiction de la liaison.
Cet article présente CROC (Coreference Resolution for Oral Corpus), le premier système de résolution des coréférences en français reposant sur des techniques d'apprentissage automatique. Une des spécificités du système réside dans son apprentissage sur des données exclusivement orales, à savoir ANCOR (anaphore et coréférence dans les corpus oraux), le premier corpus de français oral transcrit annoté en relations anaphoriques. En l'état actuel, le système CROC nécessite un repérage préalable des mentions. Nous détaillons les choix des traits – issus du corpus ou calculés – utilisés par l'apprentissage, et nous présentons un ensemble d'expérimentations avec ces traits. Les scores obtenus sont très proches de ceux de l'état de l'art des systèmes conçus pour l'écrit. Nous concluons alors en donnant des perspectives sur la réalisation d'un système end-to-end valable à la fois pour l'oral transcrit et l'écrit.
Les mesures d'accord interannotateur sont utilisées en routine par le TAL pour évaluer la fiabilité des annotations de référence. Pourtant, les seuils de confiance liés à cette estimation relèvent d'opinions subjectives et n'ont fait l'objet d'aucune expérience de validation dédiée. Dans cet article, nous présentons des résultats expérimentaux sur données réelles ou simulées qui visent à proposer une interprétation des mesures d'accord en termes de stabilité de la référence produite, sous la forme d'un taux moyen de variation de la référence entre différents groupes d'annotateurs.
Cas particulier parmi les tâches de traitement automatique des langues, l'acquisition terminologique n'a guère fait l'objet d'évaluation systématique jusqu'à présent. Il est cependant nécessaire de conduire des évaluations pour faire le bilan des recherches passées, mesurer les progrès accomplis et les angles morts. Cet article défend l'idée qu'on peut définir des protocoles d'évaluation comparative même pour des tâches complexes comme la terminologie computationnelle. La méthode proposée s'appuie sur une décomposition des outils d'analyse terminologique en fonctionnalités élémentaires ainsi que sur la définition de mesures de précision et de rappel adaptées aux problèmes terminologiques, à savoir la complexité des produits terminologiques, la dépendance aux applications, le rôle de l'interaction avec l'utilisateur et la variabilité des terminologies de référence.
L'idée que nous défendons dans cet article est qu'il est possible d'obtenir des concepts sémantiques significatifs par des méthodes de classification automatique. Pour ce faire, nous commençons par proposer des mesures permettant de quantifier les relations sémantiques entre mots. Ensuite, nous utilisons les méthodes de classification non supervisée pour construire les concepts d'une manière automatique. Nous testons alors deux méthodes de partitionnement : l'algorithme des K-means et les cartes de Kohonen. Ensuite, nous utilisons le réseau bayésien AutoClass conçu pour la classification non supervisée. Pour grouper les mots du vocabulaire en différentes classes, nous avons testé trois représentations vectorielles des mots. La première est une représentation contextuelle simple. La deuxième associe à chaque mot un vecteur de valeurs représentant sa similarité avec tous les mots du lexique. Enfin, la troisième représentation est une combinaison des deux premières.
Cet article présente un modèle de génération de gestes de la langue des signes française qui s'appuie sur une approche de modélisation semi-formelle des gestes, et sur un formalisme de spécification capable de traduire un énoncé en un flux continu de données pour le contrôle du mouvement d'un signeur virtuel. Ce modèle bénéficie à la fois d'une connaissance des structures linguistiques propres à cette langue, et de résultats d'analyse issus de mouvements capturés.
Cet article présente un travail en sémantique lexicale. L'idée est d'explorer un graphe lexical de façon à mettre en évidence la structure du lexique qu'il modélise, et de rendre ainsi l'information qu'il contient accessible à un système automatique. Les outils sont développés et testés sur un graphe de synonymie adjectivale et s'appuient sur la structure petit-monde à invariance d'échelle de ce graphe. Ils sont destinés à être ensuite utilisés dans l'exploration d'autres graphes lexicaux et plus généralement d'autres graphes de terrain.
Cet article présente la cascade de transducteurs CasEN pour la reconnaissance des entités nommées et deux autres cascades qui utilisent le texte étiqueté par CasEN. La première ajoute des informations sur les locuteurs de l'enquête sociolinguistique Eslo et la deuxième met en relation des entités nommées dans un corpus du journal Le Monde. La cascade CasEN est implanté sous le logiciel CasSys de la plateforme Unitex et sera librement mise à disposition des utilisateurs.
Cet article étudie la relation entre le formalisme TT-MCTAG et le formalisme RCG. RCG est connu pour décrire exactement la classe PTIME. Nous montrons comment une forme restreinte de TT-MCTAG peut être convertie en une RCG « simple » équivalente. Le résultat est intéressant pour des raisons théoriques (il montre que la forme restreinte est légèrement sensible au contexte), mais également pour des raisons pratiques (la conversion proposée a été implantée dans un analyseur TT-MCTAG).
Le but de cet article est de dégager quelques aspects de la théorie de la référence et de la dépendance (coréférence) des démonstratifs, dans un cadre qui plaide pour une vision non directement référentielle de l'indexicalité. On rend compte des démonstratifs en tant que quantifieurs sujets à plusieurs restrictions. Ces restrictions instaurent surtout une perspective contextuelle propre pour l'interprétation des énoncés. Afin de confirmer notre proposition, nous nous appuyons sur des exemples de l'espagnol. Notre analyse est formalisée dans le cadre de la théorie des représentations discursives. Enfin, nous nous intéressons au comportement des démonstratifs dans le discours, car ils sont employés dans différents types d'interprétations, dépendantes et non dépendantes.
Cet article présente une tentative pour appliquer des méthodes d'analyse syntaxique performantes, à base de réseaux de neurones récursifs, à des langues pour lesquelles on dispose de très peu de ressources. Nous proposons une méthode originale à base de plongements de mots multilingues obtenus à partir de langues plus ou moins proches typologiquement, afin de déterminer la meilleure combinaison de langues possibles pour l'apprentissage. L'approche a permis d'obtenir des résultats encourageants dans des contextes considérés comme linguistiquement difficiles.
Les moteurs de recherche sur le web et la plupart des systèmes de questions-réponses proposent à l'utilisateur un ensemble de liens et/ou d'extraits de pages web traitant du thème de la requête. Ces réponses sont souvent incohérentes jusqu'à un certain degré (réponses se recouvrant partiellement, contradictoires, etc.). Il est alors difficile pour l'utilisateur de savoir quelle est la réponse correcte. Nous présentons une approche dont le but est de proposer des réponses numériques synthétiques dans le cadre d'un système de questions-réponses. Ces réponses sont générées en langue naturelle et, dans un but coopératif, incluent des explications qui justifient la variation des valeurs numériques lorsque plusieurs valeurs, a priori incohérentes, sont proposées comme réponses potentielles à la question.
Nous décrivons dans cet article le formalisme grammatical des Grammaires de Dépendances Génératives Probabilistes (GDPG) ainsi que les résultats d'expériences d'analyse syntaxique réalisées à l'aide de grammaires extraites automatiquement à partir d'un corpus arboré français. LesGDPG se distinguent des grammaires probabilistes contemporaines tant par leur modèle algébrique que par leur modèle probabiliste. Le premier est un système génératif pour grammaires de dépendances et le second est un processus markovien permettant de conditionner l'établissement de dépendances d'un mot en fonction de certaines autres dépendances définies pour ce mot.
Plusieurs familles de classifieurs ont été entraînés de manière supervisée sur quatorze types de fautes détectées par un correcteur grammatical de qualité commerciale. Huit des quatorze classifieurs développés dans ce travail font partie de la plus récente édition du correcteur, intégré dans un populaire assistant de rédaction. Ce travail mené sur une période de six mois est un exemple réussi de déploiement d'une approche d'ingénierie statistique au service d'une application langagière grand public robuste.
L'idiosyncrasie des collocations rend problématique une représentation efficace et linguistiquement motivée de celles-ci dans des lexiques monolingues aussi bien que multilingues. S'appuyant sur des études récentes en Lexicologie Combinatoire explicative (LEC), nous montrons qu'il existe une corrélation entre le sens des unités lexicales et leurs collations, et qu'il existe des correspondances entre ces corrélations pour différentes langues. Nous proposons une structure pour un lexique collocationnel multilingue qui rend compte de cette structure lexicale.
Dans cet article, nous proposons une méthode pour améliorer les thésaurus distributionnels grâce à un mécanisme d'amorçage : un ensemble d'exemples positifs et négatifs de mots sémantiquement similaires sont sélectionnés de façon non supervisée et utilisés pour entraîner un classifieur supervisé. Celui-ci est ensuite appliqué pour réordonner les voisins sémantiques du thésaurus utilisé pour la sélection des exemples. Nous montrons comment les relations entre les constituants de noms composés similaires peuvent être utilisées pour réaliser une telle sélection et comment conjuguer ce critère, soit de façon précoce, soit de façon tardive, à un critère déjà expérimenté touchant à la symétrie des relations sémantiques. Nous évaluons l'intérêt de ces propositions sur un large ensemble de noms en anglais couvrant un vaste spectre de fréquences. Cet article est une version étendue de (Ferret, 2013 ;
L'article décrit le développement du corpus aligné PROIEL, qui couvre le texte original grec du Nouveau Testament et les traductions latine, gotique, vieux-slave et arménienne. Pour faciliter la création du corpus, nous avons developpé une application web qui permet l'annotation des textes sur plusieurs niveaux : morphologie, syntaxe, alignement de phrases, syntagmes et mots, structure informationelle et sémantique. Dans l'article nous décrivons cette application web ainsi que nos schémas d'annotations. Bien que conçue pour l'étude des phénomènes pragmaticaux, l'annotation très riche des textes a resulté à une ressource importante pour l'étude comparée and historique de la syntaxe et de la pragmatique de l'indo-européen, et le corpus pourra facilement être étendu à d'autres langues indo-européennes.
Alors que la détection d'erreurs simples est aujourd'hui une fonctionnalité standard des traitements de texte un peu évolués, de nombreuses erreurs restent difficiles à repérer. C'est souvent le cas lorsque la forme correcte est remplacée par une autre forme valide et syntaxiquement plausible en contexte. Nous avons utilisé les révisions de Wikipédia pour extraire automatiquement une listes d'erreurs de ce type. Ces données permettent de se faire une meilleure idée de l'utilité réelle des indicateurs standard de conformité contextuelle, qu'ils soient linguistiques ou statistiques. L'ensemble du dispositif expérimental utilisé pour ce travail sera rendu public, ce qui permettra à d'autres chercheurs de reproduire nos expériences et d'approfondir nos résultats.
La normalisation des concepts cliniques consiste à relier les mentions d'entités dans les récits cliniques à leurs concepts correspondants dans des terminologies médicales normalisées. Il peut étre utilisé pour déterminer la signification spécifique d'une mention, faciliter l'utilisation et l'échange efficaces d'informations cliniques et soutenir la compatibilité sémantique des textes. De plus, nous preparons une méthode de référence multi-classes basée sur BERT. Notre méthode de tamisage multi-passes atteint une précision de 82,0% sur le corpus MCN, la plus élevée de toutes les méthodes fondée sur des règles.
Néanmoins, les particularités de cette construction soulèvent des questions qui défient notre compréhension de la syntaxe de cette construction spécifique et, plus largement, de la façon dont le cœur et la périphérie de la Grammaire Universelle (UG) sont organisés (Culicover et Jackendoff, 1999). À ce jour, aucune étude n'a examiné la construction corrélative comparative dans une langue des signes. Le but de cet article est de combler ce manque et de fournir une revue des principales caractéristiques des corrélatifs comparatifs en langue des signes italiennes (LIS).
-six variables qui modélisent diverses caractéristiques lexicales, syntaxiques et sémantiques des textes, ainsi que certaines particularités du contexte FLE. L'article présente également une série de comparaisons entre des techniques de sélection de variables et des algorithmes d'apprentissage automatisé. Il apparaît que notre meilleur modèle, fondé sur les machines à vecteurs de support (SVM), surpasse de manière significative les modèles précédents pour le français. Quant à la contribution des techniques TAL à la lisibilité, nos résultats suggèrent que l'usage de variables TAL au sein des modèles ne produit pas des résultats significativement supérieurs à une approche classique, mais que combiner les deux types d'information conduit à une amélioration significative des performances.
Dans ce papier, nous présentons une plateforme générale pour intégrer des annotations originaires de nombreux outils différents et employant des ensembles d'étiquettes divers. Quand un corpus fait l'objet d'une annotation multi-niveaux, les annotateurs peuvent profiter d'utiliser plusieurs outils experts différents, chacun adapté aux phénomènes ou types d'annotation envisagés. Ces outils employent différents modèles de données (accompagné par de différents méthodes de visualisation), et produisent des formats de sortie distincts. Pour permettre de processer ces sorties d'une manière uniforme, nous avons développé un format pivot, appelé PAULA, et des convertisseurs formats des et aux formats originals des outils. Les annotations ne sont pas integrées seulement au niveau de format, mais aussi au niveau de la représentation conceptionelle. Pour cela, nous introduisons OLiA, une ontologie des annotations linguistiques, qui met en relation les ensembles d'étiquettes alternatifs qui néanmoins recouvrent le même phénomène linguistique. Pour l'exploitation à travers les ensembles d'étiquettes différents, ANNIS est lié à l'ontologie susmentionnée. En outre, la plateforme comprend un composant export dans un environnement d'apprentissage automatique pour soutenir l'annotation semi-automatique.
Dans cet article, je donne une description de ma recherche sur la sémantique à large couverture pour le français et, plus précisément, de la façon dont ces expressions sémantiques sont utilisées pour donner l'interprétation spatio-temporelle des itinéraires. En utilisant une grammaire catégorielle extraite semi-automatiquement du French Treebank et une lexique sémantique construite manuellement, l'analyseur convertit les analyses syntaxiques en DRS (discourse representation structures). Le but principal de cet article est l'application et la spécialisation de cette méthodologie générale au corpus Itipy contenant des récits de voyage du 19ème siècle. La chaîne de traitement complète donne des résultats tout à fait satisfaisants et laisse l'opportunité d'y ajouter des extensions spécialisées, comme un composant qui calculerait les relations discursives entre les parties du texte.
La demande d'interactions naturelles homme-ordinateur et homme-robot plus sophistiquées augmente rapidement, car les utilisateurs s'habituent davantage aux interactions de type conversation avec leurs appareils. Nous présentons VoxWorld, une plateforme de simulation multimodale pour la modélisation des interactions homme-machine.
L'identification de mots apparentés et des correspondances de sons récurrents intervient dans deux des principales tâches de la linguistique historique : démontrer des filiations linguistiques et reconstruire l'histoire des familles de langues. Nous proposons des méthodes de détection et de quantification de trois caractéristiques des mots apparentés : les correspondances de sons récurrents, la ressemblance phonétique et l'affinité sémantique. Le but ultime est d'identifier les mots apparentés et les correspondances directement à partir de listes de mots représentant des paires des langues dont la filiation est connue. Les solutions proposées sont indépendantes des langues traitées et sont évaluées sur des données linguistiques réelles. Les résultats d'expériences impliquant des langues indo-européennes, algonquines et des langues de la famille du totonaque indiquent que nos méthodes sont plus précises que des programmes comparables et d'atteignent une haute précision et un haut taux de rappel sur des ensembles de test. Les résultats suggèrent également que la combinaison de divers types d'indices augmente grandement la justesse de l'identification des mots apparentés.
Ce travail s'intéresse à la notion de contexte lexical qui est au coeur de l'approche fondatrice en extraction de lexiques bilingues à partir de corpus comparables spécialisés. D'une part, nous revenons sur les deux principales stratégies, dédiées à la caractérisation du contexte lexical, qui reposent sur l'exploitation de représentations graphique ou syntaxique. Nous montrons que l'exploitation conjointe de ces deux représentations a un intérêt particulier pour la tâche de construction de lexiques bilingues. D'autre part, nous abordons la difficulté de disposer d'observations significatives du contexte des mots en corpus comparables spécialisés. Pour répondre à cette difficulté, nous proposons de mettre en oeuvre des stratégies de réestimation des observations de cooccurrences de mots par méthode de lissage ou par prédiction. Les différentes contributions associées à ce travail engendrent une amélioration significative de la qualité des lexiques extraits.
Cette introduction au numéro spécial de la revue TAL consacré à la sémantique distributionnelle propose un panorama des thèmes de recherche actuels dans ce champ et fournit un résumé succinct des contributions acceptées.
En dépassant la stratégie didactique des listes de vocabulaire constituées sur des critères de fréquence, notre modèle veut présenter à l'apprenant les indices phonologiques et leur consistance au sein du système graphique dans sa globalité. À la frontière entre les disciplines, notre approche en TAL intègre dans le modèle présenté des propositions en didactique des langues ainsi que des résultats en psycholinguistique et neuro-imagerie.
Les obligations légales concernant l'accessibilité des contenus audiovisuels conjuguées avec l'importance des volumes actuellement produits par diverses sources suscitent un intérêt croissant pour les systèmes de sous-titrage automatique. Traditionnellement, ces systèmes procèdent en enchaînant une étape de reconnaissance de la parole et une étape de « traduction » de la transcription vers les sous-titres. Pour le sous-titrage monolingue, la « traduction » correspond à une simplification et à une segmentation du texte, qui doivent notamment respecter des normes liées à l'affichage, et composer avec les erreurs issues de la reconnaissance vocale. Dans le cas des émissions télévisées, la forme et la teneur du flux audio initial comme des sous-titres à répliquer varient significativement selon les programmes. En prenant inspiration dans la littérature de la traduction automatique, cet article met en place et compare des méthodes d'adaptation aux genres télévisuels pour la production de sous-titres.
La reconnaissance des noms propres en corpus est devenue incontournable dans certaines tâches du traitement automatique des langues, comme l'indexation, la recherche d'information ou la traduction. Nous passons en revue les récents systèmes développés pour la reconnaissance des noms propres et appliqués à l'anglais ou au français écrits : leurs objectifs, leurs méthodes et leurs résultats. Nous montrons que dans cette problématique, un acquis s'est constitué et que les méthodes élaborées ont permis la construction de systèmes aux performances particulièrement intéressantes.
Bien que le changement linguistique ait fait l'objet de nombreuses recherches numériques, les phénomènes diachroniques de renouvellement linguistique et plus spécifiquement la grammaticalisation ont été, semble-t-il, laissés de côté. Motivés par d'autres perspectives, les différents modèles s'appuient sur des représentations qui, comme nous le montrons, ne permettent pas d'aborder efficacement la modélisation de ce type de phénomènes. Nous proposons ici un cadre de représentation visant à décrire le renouvellement linguistique et se prêtant bien à la simulation numérique. Nous l'illustrons par une implémentation particulière mettant en évidence le phénomène de javellisation sémantique.
Nous présentons un système pour déterminer, à partir des données de Twitter, les évènements qui suscitent de l'intérêt d'utilisateurs au cours d'une période donnée ainsi que les dates saillantes de chaque évènement. Un évènement est représenté par plusieurs termes dont la fréquence augmente brusquement à un ou plusieurs moments durant la période analysée. Afin de déterminer les termes (notamment les hashtags) portant sur un même sujet, nous proposons des méthodes pour les regrouper : des méthodes phonétiques adaptées au mode d'écriture utilisé par les utilisateurs et des méthodes statistiques. Pour sélectionner l'ensemble des évènements, nous avons utilisé trois critères : fréquence, variation et Tf*Idf.
Dans le cadre de Morpho Challenge 2009, nous avons mis au point un système d'analyse morphologique non supervisée qui tire profit d'analogies de formes entre mots. A l'issue de notre participation nous avons pris le temps de corriger certaines lacunes de notre système et d'en analyser les caractéristiques principales. Dans cet article, nous mettons l'accent non pas sur les performances de notre système (au demeurant plus qu'encourageantes) à cette compétition, mais plutôt sur les acquis de ces expériences post-compétition. Nous espérons que ce travail contribuera à démontrer que l'analogie formelle est non seulement une alternative viable mais également compétitive à l'analyse morphologique non supervisée.
Toutefois, la compilation des corpus historiques est différente de la compilation des corpus contemporains. Les concepteurs de corpus doivent faire face à des problèmes inhérents aux textes historiques, tels que : l'absence d'une norme orthographique, l'utilisation généralisée des abréviations en plus de leurs variantes orthographiques, le manque d'espace entre les mots, l'utilisation irrégulière des traits d'union, les symbols typographiques non standard.
Cet article s'intéresse aux méthodes de traitement informatique des vidéos en langue des signes (LS). Elle utilise un modèle de couleur de la peau et trois filtres à particules pour suivre les mains et la tête, avec des étapes de rééchantillonnage et de recuit simulé pour améliorer leur robustesse aux occultations et aux grandes variations de dynamique des gestes, fréquentes en LS. Les évaluations des filtres mettent en valeur les améliorations apportées par ces étapes. La seconde partie présente des modèles de la LS pour exploiter ces résultats. Un modèle d'espace de signation et un modèle de construction de cet espace permettent de rendre compte de la structure spatiale de l'énoncé signé. Nous exposons plusieurs pistes pour réaliser une segmentation automatique des signes. Nous concluons sur un ensemble d'applications et de perspectives rendues possibles par ces méthodes.
Cet article présente la modélisation du domaine des noms propres définie dans le projet Prolex. Celle-ci repose sur deux concepts centraux : le nom propre conceptuel et le prolexème. Le nom propre conceptuel ne représente pas le référent, mais un point de vue sur ce référent. Il possède dans chaque langue un concept spéficique, le prolexème, qui est une famille structurée de lexèmes. Autour d'eux, nous avons défini d'autres concepts et des relations (synonymie, méronymie, accessibilité, éponymie, etc.). Chaque nom propre conceptuel est en relation d'hyperonymie avec un type et une existence au sein d'une ontologie.
Nous proposons D-STAG, un nouveau formalisme pour l'analyse automatique de la structure discursive des textes. Les analyses produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours, qui sont compatibles avec les structures de discours produites en SDRT. L'analyse discursive prolonge l'analyse phrastique, sans modifier celle-ci, ce qui rend envisageable la mise en oeuvre d'un analyseur de discours.
Notre article démontre à la fois la création et l'évaluation d'un modèle de sémantique distributionnelle du grec ancien. Tout d'abord nous avons développé un modèle d'espace vectoriel où chaque mot est représenté par un vecteur qui codifie les informations qui concernent ses contextes linguistiques. Ensuite nous avons validé différents modèles d'espace vectoriel en testant leur output par rapport à des références obtenues à partir de trois sources : un savant de l'Antiquité, la lexicographie moderne et la ressource WordNet. Enfin, en vue de démontrer comment le modèle peut être appliqué à une activité de recherche, nous fournissons une étude de cas, à petite échelle, de la variation sémantique dans les formules épiques, à savoir les unités récurrentes qui ont une flexibilité linguistique limitée.
Tant dans le domaine de la psychologie que dans celui du traitement automatique des langues, les normes portant sur des propriétés sémantiques, comme le caractère concret ou abstrait, la polarité ou l'émotionnalité, constituent des ressources importantes. La construction manuelle de ces normes, par l'intermédiaire d'évaluateurs, est coûteuse, d'où l'intérêt de développer des méthodes de construction ou d'extension automatique. Plusieurs méthodes ont été proposées, mais elles portent sur une seule dimension : la polarité. Nous proposons de voir dans quelle mesure l'une d'entre elles peut être étendue à six autres normes, et ce pour le français et l'espagnol. Les expérimentations confirment l'efficacité de la technique non seulement pour étendre une norme, mais également pour mettre en évidence des mots pour lesquels les valeurs attribuées par les évaluateurs sont sujettes à caution.
Nous présentons une méthode de fouille d'erreurs pour détecter automatiquement des erreurs dans les ressources utilisées par les systèmes d'analyse syntaxique. Nous avons mis en œuvre cette méthode sur le résultat de l'analyse de plusieurs millions de mots par deux systèmes d'analyse différents qui ont toutefois en commun le lexique syntaxique et la chaîne de traitement présyntaxique. Nous pouvons ainsi identifier des inexactitudes et des incomplétudes dans les ressources utilisées. En particulier, la comparaison des résultats obtenus sur les sorties des deux analyseurs sur un même corpus nous permet d'isoler les problèmes issus des ressources partagées de ceux issus des grammaires.
Le dialogue est rempli d'énoncés intuitivement complets, mais qui ne sont pas des propositions hors de leur contexte, typiquement les réponses courtes aux questions. Le traitement de tels énoncés non phrastiques (Non Sentential Utterances, NSUs) est connu comme étant difficile, d'un point de vue théorique comme calculatoire. Dans cet article, nous proposons une étude basée sur corpus des NSUs. Nous proposons une classification dans le dialogue complète et théoriquement fondée, sur la base d'une portion du British National Corpus (BNC). Cette étude suggère que l'interprétation des NSUs est susceptible d'être résolue grâce à une grammaire relativement complexe, combinée avec une approche fondée sur la dynamique des énoncés, c'est-à-dire une stratégie qui garde la trace d'un enregistrement fortement structuré du dialogue des entités introduites dans le contexte par les énoncés. Il n'est notamment pas évident qu'un raisonnement complexe et centré sur un domaine spécifique soit nécéssaire.
L'apprentissage structuré est au fondement des méthodes modernes d'apprentissage automatique pour le traitement automatique des langues (TAL). Nous présentons également un panorama des applications de ces techniques en TAL, en discutant les bénéfices découlant de leur utilisation.
L'analyse morphologique peut fournir d'aide intelligente aux étudiants débutants de français. Elle peut servir à les informer du sens grammatical de la morphologie, à leur donner accès à des dictionnaires en ligne, et à leur permettre la comparaison de toutes les occurrences d'un mot qui leur est inconnu. Cet article traitera d'un système dont l'implémentation s'est complètement réalisée et dont le prototype s'est déja avéré un succès dans une première étude de son utilisation.
Nous présentons une approche pour la recherche de réponses à des questions médicales posées en langage naturel, appelée MEANS. Cette approche se fonde sur des techniques de TAL pour l'extraction des entités médicales et des relations sémantiques exprimées dans les questions et les corpus médicaux. MEANS utilise les langages du Web sémantique pour représenter et interroger les informations recherchées par les utilisateurs. Cette caractéristique permet de partager les informations extraites à partir des corpus et de considérer l'acquisition cumulative de connaissances à moyen et long terme. MEANS construit une requête SPARQL initiale et plusieurs requêtes relaxées comme interprétation sémantique formelle de la question. L'évaluation de MEANS sur un jeu de données réelles a abouti à des résultats encourageants en termes de précision et de MRR et a permis de constater les améliorations significatives apportées par sa méthode de relaxation de requêtes.
L'amélioration continue des performances des systèmes statistiques de traduction automatique, comme celle des outils de reconnaissance vocale, ouvrent de nouvelles perspectives pour le développement d'applications de traduction automatique de la parole. En nous appuyant sur notre expérience de développement de systèmes pour des traductions de conférences, nous analysons et essayons de quantifier les difficultés principales auxquelles continuent de faire face les développeurs d'outils de traduction de parole, en tout premier lieu le manque de corpus oraux parallèles, et présentons diverses manières de les contourner.
La croissance du contenu arabe dans les médias sociaux a été causée par les crises dans le monde arabe, évoquant la nécessité d'analyser les réactions du public à l'égard des événements en cours. L'analyse des sentiments de la langue arabe est au centre d'intérêt de plusieurs études en TAL. Avec l'émergence de plusieurs TAL ressources en arabe accessibles au public ainsi que l'émergence de techniques d'apprentissage approfondi, les chercheurs pourraient gérer la nature complexe de la langue arabe plus efficacement. Au cours de la dernière décennie, plusieurs systèmes d'analyse du sentiment dans le contenu arabe (ASA) ont été développés. Cependant, leurs performances n'ont pas été étudiées ou comparées entre elles. Cette enquête couvrirait les travaux de ASA réalisés au cours des cinq dernières années. Nous comparons les résultats, évaluons les performances et donnons un aperçu de la capacité des ressources arabes créées à soutenir la recherche future dans le domaine ASA.
Les domaines spécialisés comme la Médecine utilisent un grand nombre de noms propres dans leurs terminologies. L'objectif de ce travail est d'étudier les critères permettant d'identifier les noms propres dans les termes médicaux, débouchant sur une stratégie pour l'étude de nouvelles terminologies. Les critères étudiés portent sur la casse des caractères, la présence dans plusieurs langues et l'emploi de patrons. Appliquées à deux terminologies médicales, cette méthode d'identification des noms propres permet d'atteindre des valeurs de rappel et de précision de l'ordre de 86% pour une terminologie et de 98% pour l'autre. Les implications stratégiques de ces résultats sont discutées.
Permettant de spécifier davantage les référents des entités nommées par une annotation fine, cette approche constitue également une première étape de désambiguïsation de ces unités.
La recherche de mots inconnus dans un dictionnaire est particulièrement difficile en japonais car elle demande de connaître la prononciation correcte de ces mots. Nous proposons un système qui supplée à une connaissance partielle de la prononciation des mots en permettant à des apprenants du japonais de rechercher des mots selon une estimation peut-être incorrecte de leur prononciation. Cela constitue un progrès notable par rapport aux systèmes existants qui ne proposent aucun traitement en cas de prononciation incorrecte. A cette fin nous calculons d'abord les prononciations possibles de chaque kanji (sinogramme), et les différents types d'alternance phonologique et d'erreur de prononciation qui peuvent se produire et nous leur associons une probabilité. Partant de ces probabilités et de fréquences calculées en corpus, nous calculons une mesure de plausibilité pour chaque prononciation générée pour une entrée de dictionnaire donnée, selon le modèle bayesien naïf. Une hypothèse de prononciation étant entrée par l'utilisateur, le système propose un choix d'entrées candidates trouvées dans le dictionnaire. Ce système est hébergé sur un site web et libre d'accès. Dans une évaluation effectuée sur les données du Japanese Proficiency Test (test de compétence linguistique d'apprenants du japonais) et sur des erreurs relevées en lecture naturelle, le système a réduit de façon significative le nombre de recherches infructueuses dans le dictionnaire dues à des prononciations incorrectes.
Cet article présente SIMDIAL, un paradigme d'évaluation des comportements de Systèmes de Dialogue Homme-Machine (SDHM). Ce paradigme est fondé sur la simulation déterministe d'utilisateurs et permet une évaluation automatique ou semi automatique des SDHM face à des tâches précises.
Dans cet article, nous présentons une méthodologie pour construire des interlangues lisibles et vérifiables, qui combinent à la fois expressivité et simplicité. Comme celles-ci sont représentées dans le même formalisme que celui utilisé pour l'analyse syntaxique et la génération, leur bonne formation peut être décrite par des grammaires d'unification, qui permettent naturellement (1) de générer une glose lisible par l'être humain d'une représentation structurée, (2) pour déterminer si cette dernière est bien formée sémantiquement. Bien plus que de simples pivots pour la traduction, les interlangues participent ainsi directement à l'amélioration de la reconnaissance vocale (en filtrant les phrases asémantiques) et du cycle du développement (qui peut se faire de manière entièrement monolingue, via la glose).
La compréhension des mécanismes du langage nécessite de prendre en compte très précisément les interactions entre les différents domaines ou modalités linguistiques, ce qui implique la constitution et le développement de ressources. Nous décrivons ici le CID (Corpus of Interactional Data), corpus audio-vidéo de 8 heures, en français, constitué au Laboratoire Parole et Langage (LPL). L'annotation multimodale du CID inclut la phonétique, la prosodie, la morphologie, la syntaxe, le discours et la mimogestualité. Les premiers résultats de nos études sur le CID permettent de confirmer l'intérêt d'une annotation multimodale pour mieux comprendre le fonctionnement du discours.
Nous présentons le modèle de langage sous-jacent au moteur de prédiction de mots de la Plateforme de Communication Alternative (PCA), un logiciel d'aide à la communication pour personnes handicapées. Le moteur s'appuie sur un lexique général du français très couvrant qui donne pour chaque entrée la fréquence d'usage du mot et ses traits morphosyntaxiques associés. Il intègre un modèle utilisateur (lexique personnel mémorisant les mots inconnus saisis par l'utilisateur, calcul des fréquences d'usage propres, stockage des phrases produites) et un module de prédiction morphosyntaxique qui pondère les fréquences des mots prédits en fonction du contexte syntaxique de la phrase en cours de composition. L'évaluation du modèle de langage donne des résultats satisfaisants, le taux d'économie de saisies est d'environ 55 % pour une liste de 9 propositions. La contribution dominante provient de la prédiction brute basée sur les fréquences d'usage du lexique général.
Il a été démontré que la détection des propriétés aspectuelles des clauses sous la forme de clauses sémantiques dépend d'une combinaison de caractéristiques linguistiques. Nous explorons cette tâche dans un cadre d'apprentissage sur la base des réseaux de neurones profonds. Nous introduisons un mécanisme d'attention qui identifie le contexte pertinent. Notre modèle permet d'éviter la nécessité de reproduire des caractéristiques linguistiques pour d'autres langues. Nous présentons des expériences pour l'anglais et l'allemand qui atteignent des performances compétitives, et explorons nos résultats d'un point de vue linguistique. Nous présentons une nouvelle approche pour la modélisation de l'information du genre de texte et nous mettons en valeur l'adaptation de notre système d'une langue à l'autre.
Nous proposons un système de traitement automatique de la langue ayant pour but l'identification de phrases candidates tirées de corpus. Le système hybride allie une approche heuristique à des méthodes d'apprentissage automatisé et intègre un nombre de critères de sélection pertinents. Nous nous concentrons sur deux aspects fondamentaux : la complexité linguistique et la dépendance des phrases extraites envers leur contexte d'origine. En plus d'une description détaillée du système, cet article rapporte les résultats d'une évaluation empirique réalisée avec des enseignants de langues et des apprenants portant sur l'utilité du système à des fins éducatives.
Nous présentons les résultats d'un effort visant à permettre aux ordinateurs de segmenter les décisions arbitrales des États-Unis en phrases. Nous avons créé un ensemble de données de 80 décisions de justice de quatre domaines différents. Nous montrons que les décisions juridiques sont plus difficiles pour les systèmes de détection des limites de peines existantes que pour les textes non juridiques. Les systèmes existants de détection des limites de phrases sont basés sur un certain nombre d'hypothèses qui ne sont pas valables pour les textes légaux, leur performance en est donc altérée. Nous montrons qu'un modèle général d'étiquetage de séquence statistique est capable d'apprendre la définition plus efficacement. Nous avons formé un certain nombre de modèles de champs aléatoires conditionnels qui surpassent les systèmes traditionnels de détection des limites de la peine lorsqu'ils sont appliqués aux décisions juridictionnelles.
L'article présente une méthode d'analyse syntaxique en constituants par transitions qui se fonde sur une méthode de pondération des analyses par apprentissage profond. Celle-ci est comparée à une méthode de pondération par perceptron structuré, vue comme plus classique. Nous introduisons tout d'abord un analyseur syntaxique pondéré par un réseau de neurones local et glouton qui s'appuie sur des plongements. Ensuite nous présentons son extension vers un modèle global et à recherche par faisceau. La comparaison avec un modèle d'analyse de la famille perceptron global et en faisceau permet de mettre en évidence les propriétés étonnamment bonnes du modèle neuronal à recherche gloutonne.
Nous présentons une mise en oeuvre du principe du prototypage rapide de ressource terminologique. L'expérience est menée dans le contexte de l'accès au site www.droit.org qui diffuse l'édition Lois et décrets du Journal Officiel de la République française (95 000 documents). Un corpus de référence constitué de 12 Codes essentiels du Droit français est traité par l'analyseur syntaxique de corpus SYNTEX. Celui-ci extrait un réseau de 130 000 noms et syntagmes structuré par des relations de dépendance syntaxique. Le module d'analyse distributionnelle UPERY ajoute des liens de proximité entre les noms et syntagmes apparaissant dans les mêmes contextes syntaxiques (simples ou complexes). L'ontologie résultante est intégrée dans l'interface d'accès au site, et un protocole d'évaluation est mis en place pour valider cette première version de l'ontologie.
Nous prêtons une attention particulière à la modélisation des séquences de questions-réponses (y compris les demandes de clarification), et nous arguons que les métavariables, résultant des unifications issue de la recherche de preuves, jouent un rôle décisif dans la formalisation. Nous montrons que notre système est non seulement adéquat d'un point de vue théorique, mais également d'un point de vue pratique.
Ce projet est réalisé en partenariat avec l'IAI (Institut de Recherche Appliquée à l'Information) et plusieurs compagnies allemandes. Nous décrivons plus particulièrement un outil capable de maintenir une base terminologique et de détecter des variantes de termes dans des textes. Cet outil s'adresse particulièrement aux terminologues et aux auteurs techniques.
Dans ce travail, nous nous intéressons aux problèmes liés au traitement automatique de l'oral parlé dans les médias tunisiens. Cet oral se caractérise par l'emploi de l'alternance codique entre l'arabe standard moderne (MSA) et le dialecte tunisien (DT). L'objectif consiste à construire des ressources utiles pour apprendre des modèles de langage dédiés à des applications de reconnaissance automatique de la parole. Comme il s'agit d'une variante du MSA, nous décrivons dans cet article une démarche d'adaptation des ressources MSA vers le DT. Une première évaluation en termes de couverture lexicale et de perplexité est présentée.
Cet article propose une approche de recommandation fondée sur l'analyse de requêtes. En particulier, nous nous focalisons sur des requêtes dans lesquelles l'utilisateur cherche des similitudes entre des livres, des auteurs ou encore des collections. Notre approche consiste, dans un premier temps, à repérer ces requêtes grâce à un procédé de classification automatique supervisée. Dans un deuxième temps, un analyseur en dépendance est employé afin de dégager les liens syntaxico-sémantiques présents au sein de ce type de requêtes. Une fois ces dépendances extraites, plusieurs stratégies d'expansion de ces requêtes sont mises en place afin de les exploiter comme entrées d'un index puis, via l'utilisation d'un modèle DFR : InL2. Nos expérimentations montrent que ces pistes sont un pas supplémentaire vers la compréhension des besoins utilisateurs exprimés au sein des requêtes longues et détaillées.
Nous présentons dans cet article un système de résumé automatique tourné vers l'analyse de blogs, où sont exprimées à la fois des informations factuelles et des prises de position sur les faits considérés. Notre système de résumé est fondé sur une approche nouvelle qui mêle analyse de la redondance et repérage des informations nouvelles dans les textes ; Le système est évalué sur l'anglais, à travers la participation à la campagne d'évaluation internationale TAC (Text Analysis Conference) où notre système a obtenu des performances satisfaisantes.
Un courant de recherche important réduit la modélisation du dialogue à celle de la « structure intentionnelle » des interlocuteurs qui prennent part à cette activité langagière. Cet article est une présentation critique de cette approche intentionnelle du dialogue. Nous passons en revue les évolutions de cette approche et les différents modèles qui en sont issus ­ en présentant à chaque fois les fondements philosophiques et linguistiques qui les motivent. Nous proposons en conclusion un point de vue général assez critique, et quelques pistes pour remédier à certains des problèmes rencontrés.
Nous avons étudié l'efficacité de l'analyse de beam search et des techniques de l'analyse profonde dans le probabiliste HPSG analyse. D'abord, nous avons examiné le beam thresholding et l'analyse itérative. Ensuite, nous avons examiné trois techniques développées originalement pour l'analyse profonde : quick check, large constituent inhibition, et l'analyse hybride avec la CFG chunk parser. Le quick check, l'analyse itérative et l'analyse hybride contribuaient considérablement à la performance de l'analyse totale. Finalement, nous avons examiné la robustesse et l'extensibilité de HPSG analyse sur le corpus de MEDLINE contenant presque 1.4 milliard de mots. Le corpus entier a été analysé en 9 jours avec 340 CPUs.
Nous présentons une implémentation ﬂexible et originale de la distance d'édition : la composition filtrée, un type particulier de composition de deux machines à états finis au travers d'un filtre qui modélise l'ensemble des opérations d'édition valides. Le filtre est un transducteur pondéré ou une cascade de transducteurs pondérés. Il est obtenu par compilation de règles de réécriture qui profitent d'un nouveau concept défini dans notre bibliothèque de machines à états finis : le marqueur de règles, un symbole qui n'appartient pas à l'alphabet utilisé, mais est inséré dans une règle de réécriture afin d'identifier un phénomène et d'en suivre l'évolution. Les marqueurs désambiguïsent et facilitent l'expression de conditions et de contraintes. La méthode est illustrée dans le cadre de la correction des mots hors vocabulaire.
Cette recherche a permis non seulement de préciser les divers taux de réussite de chacun des étiqueteurs selon les corpus retenus, mais aussi de montrer que ceux-ci constituent de véritables instruments heuristiques pouvant permettre d'améliorer de manière significative la description des corpus.
Ce travail a été mené sur un corpus de dépêches en français et en anglais fourni par l'Agence France Presse (AFP). Nous ne nous intéressons pour le moment qu'à l'extraction des dates et pas aux événements associés.
Algorithmes dans les deux la linguistique computationnelle et en intelligence artificielle nécessitent la capacité de comprendre et de raisonner sur l'information spatiale en langage naturel. Dans cet article, nous discutons de ce que les chercheurs face à des problémes en ce qui concerne ce sujet, mettant l'accent sur la nécessité d'un schéma d'annotation bien développé. Les desiderata d'un tel langage de spécification sont définies avec les mécanismes de représentation sont nécessaires pour la spécification de réussir. Nous examinons ensuite plusieurs schémas d'annotation d'information spatiale, en se concentrant sur la derniére version de la spécification ISO-Space. Enfin, nous nous demandons oú ISO-Space est encore loin et de proposer quelques façons que les spécifications peuvent être enrichis.
Cuxac fait l'hypothèse d'une organisation morphémique plutôt que phonémique des signes lexicaux en LSF. Cet article expose les principes théoriques et les conditions de falsiabilité mises en oeuvre pour inventorier ces valeurs morphémiques et leur assigner un corrélat formel pour, finalement, préciser la part respective des contraintes en jeu dans la structuration des signes. S'ils corroborent l'hypothèse d'une compositionnalité morphémique, ces résultats interrogent la nature des paramètres classiquement retenus, du fait des liens d'interdépendance qu'ils entretiennent.
La notion de connexion est définie comme une classe d'équivalence de combinaisons, ce qui nous permet d'introduire une notion de structure syntaxique qui s'abstrait en partie de la notion d'unité syntaxique. Les conséquences que cela a sur la conception de la structure syntaxique sont présentées.
La linguistique du texte et du discours se tournant de plus en plus vers l'approche corpus, en lien également avec le développement de bases de textes et d'instruments informatiques, de nouvelles formes de convergences se font jour.
Dans ce travail, nous présentons un système d'apprentissage automatique qui identifie le pronom impersonnel it. Plusieurs sortes d'utilisation du pronom impersonnel it sont distinguées pour des déterminer les motifs (patterns) linguistiquement pertinents.
En examinant une série traductive du dixième livre des Métamorphoses d'Ovide (mythe d'Orphée et Eurydice), cet article interroge l'apport des logiciels PHOEBUS et MEDITE pour le traitement de textes proches du point de vue du contenu, mais éloignés au niveau de la structure, du lexique et de l'état de la langue. Notre objectif est de prouver que, après quelques enrichissements du corpus et en faisant coopérer ces deux outils, on peut concevoir une visualisation modulable et lisible par tous. En suggérant des pistes d'évolution pour ces logiciels, nous essayons d'engager un dialogue étroit entre les informaticiens qui développent les outils et les chercheurs littéraires qui les exploitent.
Les systèmes professionnels de traduction offrent à cet effet à leurs utilisateurs la possibilité d'enrichir un lexique de base avec de nouvelles entrées. Récemment, Stroppa et Yvon (2005) démontraient l'intérêt de l'apprentissage analogique pour l'analyse morphologique d'une langue. Dans cette étude, nous montrons qu'il offre également une réponse adaptée au problème de l'enrichissement d'un lexique bilingue et à la traduction d'entrées lexicales inconnues en particulier.
Les dictionnaires électroniques traditionnels permettent de nos jours, grâce à des requêtes évoluées, d'accéder rapidement aux articles et aux informations qu'ils contiennent. En cela consiste leur principale valeur ajoutée. Cependant, celle-ci s'estompe une fois l'utilisateur parvenu à l'article, car il y retrouve la présentation des versions papier et leurs limites. Nous montrons ici, à travers l'exemple du DAFLES, un nouveau dictionnaire pour apprenants du français, comment il est possible d'organiser différemment les articles de dictionnaires, de façon plus dynamique et plus adaptée aux besoins de l'utilisateur. Ceci est rendu possible grâce à une structuration souple et cohérente des données lexicales.
Le problème de l'attribution stylométrique d'auteur est un problème fondamental. L'idée fondamentale derrière cette recherche est que l'on peut déterminer la paternité d'un document sur la base d'un ensemble de trait cognitifs et linguistiques qui permettent d'identifier de manière unique le style d'écriture d'une personne. Dans de nombreux cas, cependant, le bruit présent dans les documents originaux peut rendre cette analyse plus difficile et moins fiable. Nous étudions les erreurs introduites par un processus typique de reconnaissance optique de caractères (OCR). En utilisant des erreurs simulées (aléatoirement) dans un corpus de référence standard, nous évaluons la sensibilité au bruit du processus d'attribution d'auteur. Nos résultats indiquent que, bien que la précision diminue avec un niveau de bruit, cette baisse n'est pas substantielle.
Dans cet article, le dialogue est étudié d'un point de vue analytique, en combinant les apports de la sémantique formelle et de la rhétorique du discours. La sémantique formelle, considérée dans sa version dynamique, nous permet d'étudier l'énoncé en contexte. Elle nous permet par conséquent d'aborder les problèmes du monologue se retrouvant dans le dialogue (phénomènes anaphoriques par exemple). Mais elle autorise aussi la prise en compte des nouveaux aspects propres aux dialogues comme la gestion des références communes. Les représentations sémantiques des tours de parole ainsi obtenues nourrissent des processus d'inférences pragmatiques conduisant à une structure rhétorique du dialogue. Elle permet en outre dans le cas du dialogue, de raffiner les mécanismes d'inférence de l'interprétation (phénomènes conventionnels par exemple). Nous avons choisi de travailler sur des dialogues orientés vers la réalisation d'une tâche (l'explication d'un itinéraire) afin de limiter le domaine de la conversation pour se concentrer sur l'interaction relative à la transmission des informations.
Nous décrivons enfin une implémentation de ce système baptisée WISIGOTH.
Dans cet article, nous présentons une nouvelle approche computationnelle d'analyse et d'annotation de la langue arabe. L'approche proposée est fondée sur la théorie des frames sémantiques de Fillmore (Frame Semantics). Nous abordons la question de l'applicabilité de cette théorie à la langue arabe qui diffère typologiquement de l'anglais, langue sur laquelle la théorie a été fondée à l'origine. Nous allons aussi explorer l'utilisation de cette approche pour l'analyse sémantique de l'arabe, notamment, l'annotation en rôles sémantiques et son application à l'analyse contrastive des événements de mouvement et de déplacement en arabe et en anglais, en utilisant un outil développé spécialement afin de répondre aux spécificités de l'arabe et de satisfaire les principes du projet FrameNet. En dépit des différences entre l'arabe et l'anglais sur plusieurs aspects, allant du type d'écriture à la typologie de langue, les résultats de notre projet confirment, une fois de plus, la nature cross-langues de la théorie des frames sémantiques. Les résultats de ce travail sont fondés sur l'analyse d'un corpus constitué dans sa majorité des expressions du domaine des mouvements et déplacements.
Les relations lexicales typées entre termes sont indispensables pour les tâches réalisées en TALN, mais leur collecte peut s'avérer difficile. L'approche présentée ici consiste à faire participer un grand nombre de personnes à un projet contributif en leur proposant une application ludique accessible sur le Web. À partir d'une base de termes préexistante, ce sont ainsi les joueurs qui vont construire le réseau lexical, en fournissant des associations qui ne sont validées que si elles sont proposées par au moins une paire d'utilisateurs. De plus, ces relations typées sont pondérées en fonction du nombre de paires d'utilisateurs qui les ont proposées. Nous abordons ensuite la question de la détermination des différents sens d'usage d'un terme, en analysant les relations entre ce terme et ses voisins immédiats dans le réseau lexical, avant d'introduire la notion de similitude entre ces différents sens d'usage. Nous pouvons ainsi construire pour un terme son arbre des usages. Enfin, nous présentons brièvement la réalisation et les premiers résultats obtenus.
Cet article présente les principaux avantages du modèle vectoriel pour la sémantique lexicale. Outre une représentation robuste, ce modèle permet l'émergence de relations entre termes, comme celles de synonymie et d'antonymie relatives. Nous décrivons le formalisme vectoriel utilisé, ainsi que les fonctions de base qui permettent de déterminer la notion de proximité thématique. L'extension de la méthode d'indexation d'un terme issu d'un document de spécialité se base en particulier sur une notion de pliage et de dépliage de vecteurs entre espaces vectoriels. Tout terme défini par d'autres termes qui n'appartiennent pas forcément à la terminologie, va imposer l'union des bases génératrices, et donner lieu à un dépliage du vecteur dans une base plus grande. Nous montrons comment la distribution lexicale, l'antonymie et la synonymie agissent comme des révélateurs de structure et réalisent une mise en relation transversale (non liée aux liens ontologiques) et instantanée dans l'espace vectoriel étendu. Les apports des expériences effectuées permettent de nous focaliser sur l'intérêt de ce type de méthode pour les terminologies.
Le traitement automatique de la parole peut concrètement contribuer à éclairer de nombreuses questions concernant la variabilité phonémique à l'oral. L'exploitation de grandes masses de données permet ainsi de dégager de grandes tendances, dont une interprétation plus fine repose ensuite à la fois sur un éclairage linguistique et sur un certain nombre de précautions méthodologiques. Nous allons focaliser l'étude sur la variabilité des voyelles orales en français. Des mesures de durée et de formants, à partir des grands corpus PFC et ESTER utilisés à la fois par les chercheurs en linguistique et en traitement automatique de la parole permettront d'illustrer l'impact de divers paramètres, notamment le style de parole, la position syllabique et l'origine régionale des locuteurs. Enfin la réalisation des voyelles mi-fermées antérieures sera examinée à l'aide de classification automatique de variantes dans un cadre bayésien.
Notre travail se situe dans le cadre de la traduction automatique du français vers la langue des signes française (LSF) avec synthèse de gestes au moyen d'un signeur virtuel. Nous présentons tout d'abord quelques éléments descriptifs et théoriques de la LSF. Après avoir situé notre travail, nous proposons une formalisation de la partie dite standard du lexique de la LSF et de quelques phénomènes morphosyntaxiques. Nous poursuivons par la présentation du système de traduction automatique à pivot interlingue « TiLT » développé à France Télécom R&D et l'adaptation de son module de génération à la LSF. Nous terminons par présenter la technologie avatar également développée à France Télécom R&D et sa prise en charge de la LSF.
Nous présentons une vue d'ensemble du projet de l'Index Thomisticus Treebank (IT-TB). L'IT-TB consiste d'environ 60,000 occurrences tirées de l'Index Thomisticus de Roberto Busa SJ, un corpus de onze millions de mots latins de Thomas d'Aquin. Nous décrivons brièvement les règles d'étiquetage, qui sont en commun avec la Latin Dependency Treebank (LDT). Nous décrivons l'application des parseurs probabilistes dépendanciels sur les données de l'IT-TB et de la LDT. Nous présentons les résultats de l'entraînement et de l'analyse syntaxique sur plusieurs ensembles des données et nous fournissons une évaluation des algorithmes et des techniques d'apprentissage. En outre, nous introduisons le lexique de valence de l'IT-TB tiré de la treebank. Nous reportons les données quantitatives du lexique et nous fournissons quelques mesures statistiques sur les structures de sous-catégorisation.
Cet article présente la constitution et la mise à disposition du corpus oral ESLO. Notre objectif est de montrer quil ne sagit pas seulement de recueillir et rendre disponible des données langagières mais aussi de rendre explicite lensemble de la chaîne de traitement qui permet délaborer un tel corpus. Après avoir présenté le projet et le corpus nous préciserons les problèmes juridiques et méthodologiques qui ont conditionné les opérations de traitement du corpus et notamment les procédures danonymisation indispensables à la libre diffusion de cette ressource. Dans une seconde partie, nous présenterons les différentes annotations effectuées sur les données brutes avec quelques exemples de leurs exploitations. Nous expliquerons la méthodologie suivie qui est toujours guidée par la nature des données et lobjectif final visé : constituer un grand corpus oral variationniste du français. Nous aborderons enfin les questions de mise à disposition du corpus en ligne.
Dans cet article, nous présentons et évaluons une approche permettant de combiner un analyseur fondé sur une grammaire et un analyseur fondé sur des données, en utilisant des méthodes d'apprentissage automatique pour produire des analyses syntaxiques guidées par les deux analyseurs. Nous montrons comment la conversion de la sortie d'un analyseur LFG en une représentation en dépendances permet d'utiliser une technique d'empilement d'analyseurs ("parser stacking"), dans laquelle la sortie de l'analyseur fondé sur une grammaire fournit des caractéristiques utilisables par un analyseur fondé sur les données. Nous évaluons notre approche sur l'anglais et l'allemand, et montrons des améliorations significatives pour les résultats d'analyses syntaxiques complètes qui découlent de l'analyse en dépendances ainsi que des caractéristiques provenant de grammaires. Enfin, nous procédons à une évaluation dédiée à une application, et explorons l'utilisation de cet empilement d'analyseurs comme point de départ pour l'annotation en dépendances d'une nouvelle langue.
Cet article concerne le contrôle des ellipses sémantiques en génération du langage pour le dialogue homme-machine. Pour cela, la structure rhétorique du dialogue est employée, aussi qu'un historique des faits acceptés par le destinataire de l'énoncé. À partir de ces principes, un ensemble d'algorithmes sont proposés. Ensuite, cette méthode est étendue au dialogue à plusieurs locuteurs et illustrée à travers plusieurs exemples.
Dans cet article, nous nous intéressons au problème de la partition des expressions référentielles d'un texte en chaînes de coréférences disjointes. Nous portons une attention particulière à l'identification automatique des chaînes chapeautées par des noms propres, grâce à un algorithme simple (c'est-à-dire nécessitant ni analyse syntaxique complète, ni identification des parties du discours) et partiellement adaptable au type de texte, en utilisant un petit ensemble de noms caractéristiques pour chaque domaine. Pour élaborer cet algorithme, nous avons comparé trois types de textes appartenant à des domaines différents et nous avons utilisé le codage XML pour représenter les données. Aussi, nous nous sommes attaqués à quelques sous-problèmes comme l'identification automatique des syntagmes nominaux (trouver les bonnes bornes gauches et droites) et la sélection des chaînes de coréférences importantes.
Différents paradigmes de systèmes d'analyse des textes se sont succédés ces dernières années. Les systèmes de compréhension des années 1980 visent une analyse profonde et exhaustive du texte, tandis que les systèmes d'extraction d'information du début des années 1990 mettent en œuvre des analyses beaucoup plus locales. L'article dresse un panorama et un bilan rapide de ces approches. Il analyse ensuite le retour d'approches globales (par apprentissage symbolique ou statistique notamment) dans des systèmes essentiellement locaux. La généricité et l'adaptabilité des systèmes sont discutées et illustrées à travers des exemples représentatifs des réalisations du domaine.
L'apprentissage non supervisé et semi-supervisé de la morphologie fournit des solutions pratiques pour le traitement des langues morphologiquement riches et requiert une intervention humaine réduite comparée aux analyseurs traditionnels basés sur des règles. L'évaluation directe des méthodes d'apprentissage utilisant des analyses de référence linguistique est importante pour leur développement, puisque l'évaluation par les applications finales prend généralement beaucoup de temps. Cependant, même l'évaluation linguistique n'est pas simple pour l'analyse morphologique complète, car les identifiants de morphèmes générés par la méthode d'apprentissage peuvent se révéler arbitraires. Nous passons en revue les méthodes d'évaluation existantes pour les tâches d'apprentissage et proposons de nouvelles variations. Afin de comparer les méthodes, nous effectuons une vaste méta-évaluation à l'aide de l'importante base de résultats provenant des compétitions Morpho Challenge.
Nous proposons une méthode permettant d'extraire automatiquement des chaînes de mots relatives à des opinions à partir de corpus étiquetés. Il s'agit dans un premier temps d'améliorer les performances de systèmes de catégorisation automatique utilisés pour retrouver l'opinion (positive, négative ou neutre) rattachée à un texte. Dans un deuxième temps, la visualisation de ces chaînes permet d'avoir un aperçu des critiques fréquemment rencontrées. Cette méthode est testée sur des corpus en français ou en anglais de critiques de jeux vidéo et de films et sur un corpus d'enquêtes téléphoniques de satisfaction clients. Nous présentons des exemples de chaînes de mots extraites et les améliorations observées pour la catégorisation.
Dans cet article, nous présentons une méthode de filtrage permettant de sélectionner à partir d'un ensemble de documents les extraits de textes les plus significatifs relativement à un profil défini par un utilisateur. Pour ce faire, nous mettons l'accent sur l'utilisation conjointe de profils structurés et d'une analyse thématique des documents. Cette analyse permet également d'étendre le vocabulaire définissant un profil en fonction du document traité en sélectionnant les termes de ce dernier les plus étroitement liés aux termes du profil. Tous ces aspects assurent une plus grande finesse du filtrage tout en permettant la sélection d'extraits de documents ayant un lien plus ténu avec les profils mais davantage susceptibles d'apporter des informations nouvelles et donc intéressantes. L'intérêt de l'approche présentée a été illustré au travers du système REDUIT qui a fait l'objet d'une évaluation concernant à la fois le filtrage de documents et l'extraction de passages.
Tous ces modèles intègrent des traits dédiés aux mots composés, dont certains sont calculés à partir de ressources lexicales externes. Nous montrons que l'approche par présegmentation atteint des performances dépassant l'état de l'art, alors que celle par postsegmentation est un peu en dessous de nos espérances. Les différentes expériences menées ouvrent de nombreuses pistes de recherche.
Nous décrivons dans cet article une méthode permettant l'acquisition d'un lexique syntaxique des verbes du français à partir de l'analyse automatique de gros corpus. Nous évaluons cette méthode par rapport à des ressources existantes et nous montrons que notre système produit automatiquement de nouvelles données qui peuvent compléter les lexiques existants. Nous montrons enfin comment la syntaxe peut aider à faire émerger des classes lexico-sémantiques, dans la lignée des travaux de Levin (1993).
Une tâche importante des systèmes d'extraction d'information se focalisant sur des événements est le remplissage de formulaires regroupant, en les caractérisant par leur type, les informations associées à un événement donné à partir d'un texte. Cette tâche peut s'avérer difficile lorsque l'information est dispersée à l'échelle du texte et mélangée à des éléments d'information liés à d'autres événements similaires. Nous proposons dans cet article une approche en deux étapes pour prendre en compte ce problème : d'abord une segmentation du texte en événements pour identifier les phrases relatives à un même événement, puis une méthode de sélection des entités liées à l'événement dans ces phrases. Une évaluation de cette approche sur un corpus annoté de dépêches dans le domaine des événements sismiques montre une F1-mesure de 77 % pour la tâche de remplissage de formulaires.
Les réseaux neuronaux récurrents (RNN) se sont montrés très efficaces dans plusieurs tâches de traitement automatique des langues. Cependant, leur capacité à modéliser l'étiquetage de séquences reste limitée. Cette limitation a conduit la recherche vers la combinaison des RNN avec des modèles déjà utilisés avec succès dans ce contexte, comme les CRF. Dans cet article, nous étudions une solution plus simple mais tout aussi efficace : une évolution du RNN de Jordan dans lequel les étiquettes prédites sont réinjectées comme entrées dans le réseau et converties en plongements, de la même façon que les mots. Nous comparons cette variante de RNN avec tous les autres modèles existants : Elman, Jordan, LSTM et GRU, sur deux tâches de compréhension de la parole. Les résultats montrent que la nouvelle variante, plus complexe que les modèles d'Elman et Jordan, mais bien moins que LSTM et GRU, est non seulement plus efficace qu'eux, mais qu'elle fait aussi jeu égal avec des modèles CRF plus sophistiqués.
Les dictionnaires bilingues ont un grand potentiel comme source de ressources lexicologiques pour l'apprentissage dans les systèmes automatisés tels que OCR, traduction automatique, et recherche documentaire multilangue. Dans cet article, nous décrivons un système pour extraire des lexiques de termes à partir de dictionnaires bilingues imprimés. Notre travail se présente en trois phases - Segmentation de dictionnaire, étiquetage des entrées, et génération. Dans la segmentation, les pages sont divisées en entrées logiques basées sur des caractéristiques structurelles apprises à partir d'exemples choisis. Les entrées extraites liées aux étiquettes fonctionnelles sont passées à un module d'étiquetage qui associe des étiquettes linguistiques à chaque mot ou expression dans l'entrée. Le résultat produit par le système est une structure qui représente les entrées du dictionnaire. Nous avons employé cette approche pour analyser une variété de dictionnaires avec des alphabets latins ou non-latins, et nous démontrons les résultats de la génération de lexiques de termes pour la recherche d'information à partir d'une collection d'articles de journaux français et de requêtes en anglais.
QRISTAL (Questions-Réponses Intégrant un Système de Traitement Automatique des Langues) est un système de questions-réponses utilisant massivement le TAL, tant pour l'indexation des documents que pour l'extraction des réponses. Ce système a été classé en tête lors de l'évaluation EQueR (Evalda, Technolangue2) puis de nouveau en tête lors des évaluations CLEF 2005 et 2006, en français comme en multilingue. Après une description fonctionnelle du système, un exemple de traitement de question permet de détailler le processus d'analyse et d'extraction de la réponse. Les résultats des évaluations et des tests complémentaires permettent de mieux situer l'apport des différents modules de TAL. Les réactions des premiers utilisateurs incitent enfin à une réflexion sur l'ergonomie et les contraintes des systèmes de questions-réponses, face aux outils de recherche sur le web.
La compréhension et le résumé automatique de la narration sont difficiles, en raison de l'incapacité des systèmes automatiques d'identifier le vrai contenu de l'information. Ceci peut avoir comme conséquence le choix inadéquat du contenu aussi bien que l'incohérence du résumé. Cet article présente une nouvelle conception pour le résumé automatique de la narration, qui repose sur l'exploitation d'information temporelle. Cette conception distingue trois niveaux : scène, histoire et intrigue. L'article se conclut par une discussion sur les difficultés à relever dans ce domaine.
L'intelligence artificielle (IA) a connu ces dernières années de grandes avancées qui ont résonné avec des préoccupations sociétales. Des instances ont été créées et ont commencé à structurer les problèmes posés par ces développements. Tant pour la société civile que pour de nombreux scientifiques, le champ de ces instances recouvre les problématiques du traitement automatique des langues (TAL). Dans cet article nous revenons sur certains aspects expliquant la relation entre IA et TAL, mais aussi sur les éléments qui les différencient. Nous revenons sur les problèmes d'éthique pour l'IA et également pour le TAL. Enfin nous argumentons pour ne pas dresser l'éthique comme solution de facto à la réflexion, mais plutôt comme occasion de positionner les recherches dans des perspectives plus globales et nous revenons sur le problème de la relation entre utilisation de modèles numériques et faculté de les interpréter.
Les recherches en extraction lexicale bilingue à partir de corpus comparables ont abouti à des résultats prometteurs pour les corpus très volumineux en utilisant une méthode d'alignement dite directe. Le changement d'échelle induit par des corpus d'une taille plus modeste conduit à l'obtention de résultats plus contrastés. Nous proposons d'introduire la notion de points d'ancrage sur laquelle nous faisons reposer une partie de l'alignement pour augmenter significativement les résultats de l'approche directe sur de tels corpus.
Cet article présente le système KeyGlass : un système de saisie de texte avec ajout dynamique de caractères en fonction de ceux précédemment saisis. Le système de prédiction que nous utilisons pour optimiser notre système est basé sur l'utilisation conjointe d'un arbre lexicographique et d'un système utilisant des bigrammes. Nous présentons dans cet article les différentes étapes qui nous ont conduit vers ce système de prédiction. Enfin nous étudions, au travers de deux évaluations (l'une théorique et l'autre avec des utilisateurs), l'utilité et l'efficacité de notre système lors d'une tâche de saisie de texte. Les résultats montrent une réduction importante de la distance parcourue par le pointeur sur le clavier logiciel. En revanche, les utilisateurs sont un peu moins rapides pour saisir du texte.
Cet article présente une enquête sur l'utilisation de quatre types d'informations contextuelles pour améliorer la précision de la correction automatique de fautes d'orthographe de mots seuls. La tâche est présentée comme un reclassement contextuellement informé. Le contexte local immédiat, capturé par statistique de mot n-grammes est modélisé à partir d'un modèle de langage à l'échelle du Web. La deuxième méthode consiste à mesurer à quel point une correction s'inscrit dans le tissu sémantique local, en utilisant un très grand modèle sémantique distributionnel. La quatrième approche s'attache au contexte au-delà du texte lui-même. Si le sujet approximatif peut être connu à l'avance, la correction orthographique peut être biaisée par rapport au sujet. L'efficacité des méthodes proposées est démontrée avec un corpus annoté de 3 000 travaux d'étudiants des évaluations internationales de langue anglaise. Le document décrit également un système mis en place qui permet d'obtenir une grande précision sur cette tâche.
A partir d'extraits de corpus, nous montrons qu'un modèle d'interprétation de la référence basé exclusivement sur des contraintes imposées par l'expression linguistique (éventuellement accompagnée d'un geste de désignation) est insuffisant dans le cadre du dialogue avec support visuel. Pour exploiter finement ces contraintes, un tel modèle doit nécessairement s'appuyer sur une représentation des contextes intégrant les principes de fonctionnement de la perception visuelle, de la tâche et de la mémoire de l'utilisateur. Nous nous focalisons ici sur les mécanismes de construction et d'exploitation de ces contextes hétérogènes dans un cadre unifié. Nous proposons ainsi un modèle d'interprétation de la référence suffisamment générique pour pouvoir être appliqué à différents types d'interactions et à différents types d'applications.
Comme dans tous les domaines de spécialité, le domaine médical manipule des termes très spécifiques (p. ex., blépharospasme, alexitymie, appendicectomie), qui sont difficiles à comprendre par les non spécialistes. Nous proposons une méthode automatique pour l'acquisition de paraphrases, qui soient plus faciles à comprendre que les termes originaux. La méthode est basée sur l'analyse morphologique des termes, l'analyse syntaxique et la fouille de textes non spécialisés. L'analyse et l'évaluation des résultats indiquent que de telles paraphrases peuvent être extraites et présentent une compréhension plus facile. En fonction de paramètres, la précision varie entre 90 et 7,4 %. Ce type de ressources est utile pour plusieurs applications de TAL (p. ex., recherche d'information, simplification de textes, systèmes de question-réponses).
Les sciences participatives, et en particulier la production participative (crowdsourcing) bénévole, sont un moyen encore peu exploité de créer des ressources langagières pour les langues peu dotées dont suffisamment de locuteurs sont présents sur le Web. Nous présentons ici nos expériences concernant l'annotation en parties du discours pour des langues non standardisées, en l'occurrence l'alsacien et le créole guadeloupéen. Nous détaillons la méthodologie utilisée, montrons qu'elle est adaptable à plusieurs langues, puis nous présentons les résultats obtenus. L'analyse des limites de la plateforme d'origine nous a conduites à en développer une nouvelle, qui, outre l'annotation en parties du discours, permet la création de corpus bruts et d'un lexique de variantes alignées. Les plateformes créées, les ressources langagières, et les modèles de taggers entraînés
La métaphore présente un intérêt indiscutable pour étudier en diachronie l'évolution des idées dans les textes scientifiques relevant des sciences humaines et sociales. Cependant, malgré les différentes méthodes proposées en pour détecter automatiquement les métaphores, très peu de travaux de recherche ont essayé de les appliquer à ce genre de textes. Dans cet article, nous présentons une tentative d'identification des métaphores conceptuelles dans des textes de géographie en français et en anglais qui utilise une méthode reposant sur LDA (Heintz et al.
Cet article présente l'œuvre de Gabriel G. Bès, qui s'étend des années 60 aux années 2000, du structuralisme aux industries de la langue. L'œuvre de Bès est traversée par la question épistémologique et la volonté de pratiquer la linguistique comme une science empirique. On y retrouve comme problématiques centrales l'explicitation des observations sur le réel, la formalisation des hypothèses, et la pratique rigoureuse du test d'adéquation des hypothèses par rapport aux observations.
Cet article fait le point dans le domaine de la recherche d'information sémantique (RIS), qui est à l'intersection de plusieurs disciplines : recherche d'information (RI), ingénierie des connaissances (IC) et traitement automatique des langues (TAL). Nous détaillons le rôle de la phase d'annotation des documents à l'aide d'une ressource, puis la transformation de ces annotations dans l'espace d'indexation. Nous détaillons les trois grandes familles de modèles de recherche d'information et leur prise en compte de ces nouveaux espaces d'indexation.
Les corpus parallèles sont la pierre angulaire de plusieurs technologies de traduction automatique. L'expérience montre que la stratégie visant à réduire l'intervention manuelle dans cet exercice n'est jamais la même d'un corpus à l'autre. Un classificateur est entraîné à décider des documents parallèles et un moteur de recherche d'information est utilisé afin de réduire l'espace de recherche des paires de documents parallèles. Nous montrons l'efficacité de PARADOCS sur de nombreuses tâches avec de nombreuses paires de langues.
Les outils statistiques robustes en TAL, tels que les étiqueteurs morphosyntaxiques et les analyseurs syntaxiques, utilisent souvent des descripteurs "pauvres", qui peuvent être appliqués facilement à n'importe quelle langue, mais ne prennent pas en compte les particularités de la langue. Dans cette étude, nous cherchons à améliorer l'analyse de deux phénomènes en français en injectant des connaissances plus riches : l'étiquetage morphosyntaxique du mot "que" et l'analyse syntaxique de la coordination. Nous comparons plusieurs techniques : la transformation automatique du corpus vers d'autres normes d'annotation avant l'entraînement, l'ajout de descripteurs ciblés et riches lors de l'entraînement, et l'ajout de règles symboliques qui contournent le modèle statistique lors de l'analyse. Nous atteignons une réduction du taux d'erreur de 55 % pour l'étiquetage de "que", et de 37 % pour les structures coordonnées.
Cet article traite de la génération du langage naturel dans le contexte des systèmes de questions-réponses avec une approche non supervisée. L'approche proposée montre des résultats très prometteurs pour l'anglais et pour le français.
Nous soutenons que l'architecture traditionnelle du lexique en Traduction Automatique (TA), bien que sans doute adéquate pour des langues non-apparentées ne permet pas de capturer des générations importantes concernant des langues d'une même famille. Ces généralisations importantes, si elles sont capturées, peuvent aider à produire des systèmes multilingues de Traitement Automatique des Langues Naturelles (TALN) plus robustes. Dans cet article, nous décrivons l'architecture d'héritage hiérarchique pour les lexiques multilingues de langues apparentées initialement proposée par Cahill et Gazdar (1995) et ultérieurement utilisée pour développer un lexique de néerlandais, anglais et allemand.
Différents travaux en TAL font apparaître la nécessité d'articuler, au sein de chaînes complexes, divers traitements mettant en jeu une pluralité d'objets linguistiques et de méthodes d'analyse. La plate-forme LinguaStream propose une architecture et un ensemble d'outils visant à faciliter la mise en œuvre de tels « assemblages » : modèles génériques d'analyse (grammaires, transducteurs, projection de lexiques...), ponts vers des modules externes (analyseurs morphologiques ou syntaxiques...), outils lexicométriques, outil de visualisation des annotations, etc. La conception de LinguaStream et son exploitation dans différents projets, nous ont conduits à formuler différents principes méthodologiques que nous tentons ici d'expliciter. Nous décrivons ensuite succinctement trois applications développées avec LinguaStream, selon cette méthodologie : analyse de cadres de discours, recherche d'information géographique et segmentation thématique par des méthodes de fouille de texte.
Dans cet article, nous présentons la notion d'algorithme local et d'algorithme global pour la désambiguïsation lexicale de textes. Un algorithme local permet de calculer la proximité sémantique entre deux objets lexicaux. L'algorithme global permet de propager ces mesures locales à un niveau supérieur. Nous nous servons de cette notion pour confronter un algorithme à colonies de fourmis à d'autres méthodes issues de l'état de l'art, un algorithme génétique et un recuit simulé. En les évaluant sur un corpus de référence, nous montrons que l'efficacité temporelle des algorithmes à colonies de fourmis rend possible l'amélioration automatique du paramétrage et, en retour, leur amélioration qualitative. Enfin, nous étudions plusieurs stratégies de fusion tardive des résultats de nos algorithmes pour améliorer leurs performances.
Cette étude s'inscrit dans le champ de l'évaluation automatique de textes rédigés en langue étrangère. Elle vise à évaluer d'une manière approfondie l'utilité d'y prendre en compte des mesures automatiques de la compétence phraséologique. Les indices phraséologiques employés sont obtenus par l'analyse de tous les bigrammes de mots présents dans un texte d'apprenants auxquels des scores d'association sont attribués sur la base d'un grand corpus natif. Les analyses effectuées sur trois ensembles de textes indiquent que ces indices apportent une information utile pour estimer la qualité des textes et que celle-ci est différente de celle apportée par des mesures lexicales plus simples. Une validation externe, réalisée en apprenant et en testant le modèle prédictif sur des ensembles de textes très différents, montre que ces indices présentent un degré de généralisation important.
Un problème commun aux systèmes de Question-Réponse, et de recherche documentaire plus généralement, est la présence d'une quantité excessive de données parmi lesquelles chercher l'information pertinente. Ceci apporte un rappel élevé mais se traduit par un risque de faible précision de l'information retournée à l'utilisateur. De plus, ce problème concerne la pertinence des réponses vis-à-vis des besoins des utilisateurs, puisque les questions peuvent être ambiguës et même des réponses extraites de documents au contenu pertinent peuvent être mal reçues si elles sont trop compliquées (ou trop simples) pour les utilisateurs. Nous adressons ce problème en incorporant une composante de modélisation de l'utilisateur afin de personnaliser les résultats d'un système Web de Question-Réponse à domaine ouvert sur la base de son niveau de lecture et de ses interêts.
L'un des composants les plus novateurs de notre réflexion porte sur les apports de la phonologie de corpus, notamment des corpus oraux d'apprenants de L2. Cette réflexion nous conduit à plaider en faveur d'une approche interdisciplinaire plus riche entre didacticiens et ingénieurs de la parole afin d'encourager le développement des systèmes de correction automatique dans les curricula de L2.
Cette recherche est dédiée à la caractérisation et à l'identification d'accents en français. À la fois pour des accents étrangers et régionaux, nous sommes partis d'expériences d'identification perceptive, nous avons mesuré des traits phonétiques qui peuvent caractériser ces accents en utilisant l'alignement automatique en phonèmes, et nous avons hiérarchisé les traits les plus discriminants en utilisant des techniques de classification. Les indices suivants sont perceptibles, et se montrent relativement robustes pour l'identification automatique  : dévoisement des occlusives sourdes, mouvement du /e/ vers [i],confusions /b/~/v/ et /s/~/z/, «  r roulé  » , antériorisation ou fermeture du schwa pour les accents anglais, allemand, arabe, espagnol, italien et portugais en français ; antériorisation du /O/ dans la moitié nord de la France, maintien du schwa et dénasalisation des voyelles nasales dans le sud de la France, pour les accents régionaux du français.
Les bases de connaissances prennent en charge des questions fréquentes et permettent d'apporter une crédibilité supplémentaire à certaines réponses. Enfin, un apprentissage automatique des caractéristiques des questions auxquelles le système a pu répondre lors de la campagne EQueR permet de prédire dans plus de 70 % des cas si le système est capable de répondre à une question donnée.
Cet article présente (i) une classification des noms propres bulgares, (ii) une méthodologie d'analyse et génération automatique des formes des noms propres, (iii) quelques approches de la création automatique d'un dictionnaire des noms propres. Les noms propres bulgares sont divisés en classes. Chaque classe comporte des règles pour la génération du paradigme. Le patron est une représentation lexicologique qui apparie toutes les formes du paradigme. L'analyse morphologique est basée sur l'appariement du nom propre et du patron. Le patron et la classe incorporent des informations sur le paradigme entier d'un nom propre particulier. Un dictionnaire électronique de noms propres est créé. Il contient les paires <patron, numéro de la classe>.
Après un rappel historique et terminologique, nous aborderons le sujet de la référence pour le traitement du langage naturel, de la qualité des annotations, de la quantité des données, des différence entre évaluation de technologie et évaluation d'usage, de l'évaluation des systèmes de dialogue, des standards avant de conclure sur une brève présentation des articles du numéro et quelques remarques prospectives.
Nous présentons une modélisation de la coordination à l'aide des grammaires d'interaction. La notion de polarité permet de définir l'interface d'une structure syntaxique comme sa capacité à se combiner à d'autres structures syntaxiques en termes de besoins et de ressources. La coordination de deux conjoints est alors vue comme la superposition des interfaces de leurs structures syntaxiques. Ces structures sont représentées à l'aide de la notion de description d'arbres et peuvent ainsi intégrer la sous-spécification pour se combiner de façon particulièrement souple.
La ressource présentée dans cet article combine un corpus de noms déverbaux annotés sémantiquement et syntaxiquement, basée sur le French Treebank, et un lexique électronique fournissant des informations d'ordre morphologique, syntaxique et sémantique sur les noms présents dans le corpus.
Nous verrons en effet que selon le type de nom propre envisagé, qu'il soit mono- ou polylexical, il existe des régularités qui permettent de déterminer la démarche de traduction adéquate. Ces instructions sont destinées à être utilisées, entre autres, pour le traitement d'un corpus, afin de réaliser un dictionnaire électronique bilingue des noms propres allemand-français.
Pour cela, nous décrivons un corpus minimal et contrôlé pour mesurer l'intensité de ces biais dans les traductions de l'anglais vers le français et du français vers l'anglais. Grâce à des méthodes de sondage et des interventions sur les représentations internes de l'encodeur, nos expériences montrent que l'information de genre est distribuée sur l'ensemble des représentations des tokens sources et cibles et que la sélection du genre en langue cible résulte d'une multiplicité d'interactions entre les diverses unités impliquées dans la traduction.
L'application de méthodes d'analyse distributionnelle pour calculer des liens de proximité sémantique entre les mots est devenue courante en TAL. Cet article est consacré à la question de l'évaluation d'une ressource distributionnelle, et de son amélioration. Nous envisageons la mise en place d'une procédure d'évaluation comme une première étape vers la caractérisation de la ressource et vers son ajustement, Nous proposons un protocole d'annotation en contexte des voisins distributionnels, qui nous permet de constituer un ensemble fiable de données de référence. Les données produites sont analysées, puis exploitées pour entraîner un système de catégorisation automatique des liens de voisinage distributionnel.
Les modèles probabilistes discriminants permettent de manipuler des représentations linguistiques riches, sous la forme de vecteurs de caractéristiques de très grande taille. Travailler en grande dimension pose des problèmes, en particulier computationnels, qui sont exacerbés dans le cadre de modèles de séquences tels que les champs aléatoires conditionnels (CRF). Sélectionner automatiquement les caractéristiques pertinentes s'avère alors intéressant et donne lieu à des modèles plus compacts et plus faciles à utiliser. Dans cette étude, nous proposons un algorithme d'estimation pour les CRF qui réalise une telle sélection, par le truchement d'une pénalisation L1. Nous présentons également les résultats d'expériences menées sur des tâches de traitement des langues (le chunking et la détection des entités nommées). Nous proposons enfin des pistes pour améliorer l'efficacité computationelle de cette technique.
Au cours des dernières années il y a eu un nombre important de recherches au sujet du résumé automatique. Toutefois, il y a eu comparativement peu de recherche au sujet des ressources computationnelles et composantes qui peuvent être adaptées facilement pour le développement et l'évaluation des systèmes de résumé automatique. Ici on présente un ensemble de ressources spécifiquement développées pour le résumé automatique qui se basent sur la plateforme GATE. Les composantes sont utilisées pour calculer des traits indiquant la pertinence des phrases. Ces composantes sont combinées pour produire différents types de systèmes de résumé tels que résumé de document simple, résumé de document multiple, et résumé basé sur des topiques.
Nous présentons un bilan du Corpus arboré du français, ou French Treebank (FTB) (1996-2016), qui est une ressource lexicale et syntaxique unique en son genre, richement annotée (et validée manuellement) pour les linguistes, et pour le TAL, avec environ 300 utilisateurs dans le monde. Après avoir exposé les principes de construction, et les principaux choix d'annotation, nous présentons l'état final du corpus, ses différents formats, et une première évaluation. Nous présentons aussi quelques ressources dérivées et des exemples d'interrogation.
Dans le domaine biomédical, utiliser des termes spécialisés est essentiel pour accéder à l'information. Cependant, dans beaucoup de langues, ces termes sont des constructions morphologiques complexes qui compliquent cet accès à l'information. Dans cet article, nous nous intéressons à l'identification des composants morphologiques de ces termes et à leur utilisation pour une tâche de recherche d'information (RI). Nous proposons différentes approches reposant sur un alignement automatique avec une langue pivot particulière, le japonais, et sur un apprentissage par analogie permettant de produire des analyses morphologiques fines des termes d'une langue donnée. Ces analyses morphologiques sont ensuite utilisées pour améliorer l'indexation de documents biomédicaux. Les expériences rapportées montrent la validité de cette approche avec des gains en MAP de plus de 10 % par rapport à un système de RI standard.
Cet article est consacré à une grammaire du verbe akkadien utilisant des techniques de machines finies à états. Elle repose sur des techniques innovantes permettant de relier différentes représentations d'une forme (quatre dans cette grammaire) au moyen d'une structure arborescente et de compiler statiquement des structures de traits dans des transducteurs finis.
Cet article présente une description des constructions antithétiques, que l'on a intégrée à une grammaire formelle du français spontané afin d'en permettre une utilisation en TAL. Ces constructions ont pour caractéristique d'être soumises à des contraintes provenant simultanément de domaines divers (lexique, syntaxe, sémantique et pragmatique). Nous commençons ici par présenter une description du phénomène, puis nous illustrons notre proposition par un exemple d'analyse, avant de montrer comment nous avons intégré cela dans la grammaire formelle que nous développons.
Nous montrons que tout ensemble de productions d'une langue peut être appréhendé comme un espace à trois dimensions : l'axe syntagmatique est celui des combinaisons de signes au sein du texte, l'axe paradigmatique est celui des commutations possibles en chaque point du texte et l'axe sémiotique est celui interne aux signes, qui lie le signifié au signifiant, le sens au texte. Toute modélisation formelle d'une langue se doit d'effectuer un recouvrement de cet espace tridimensionnel. De plus, pour capter les régularités de la langue, un modèle cherchera à couvrir les portions les plus importantes possibles de cet espace avec chaque règle, ce qui est complexe en raison de l'interdépendance des trois dimensions. Pour mieux comprendre les enjeux, nous comparerons deux stratégies illustrées par un modèle bien connu, LTAG, et un autre similaire, GUST, qui s'en inspire tout en privilégiant un découpage selon les axes syntagmatique et sémiotique.
Dans cet article, nous discutons les questions importantes qui se posent lors de l'acquisition de lexiques informatiques, sémantiques et multilingues, en partant des lexiques de base jusqu'à la construction de lexiques grande échelle. D'une part, nous montrons que la qualité des lexiques repose essentiellement sur les décisions méthodologiques (par exemple, acquisition orientée corpus versus acquisition orientée thesaurus) et la qualité de la formation des lexicographes. D'autre part, nous présentons différentes méthodes qui facilitent l'acquisition de lexiques à grande échelle (comme les règles lexicales et les ressources en ligne). En guise de conclusion, nous montrons les avantages d'une approche sémantique dans le cadre du multilinguisme.
Nous nous intéressons à la tâche de reconnaissance des entités nommées pour la modalité orale. Cette tâche pose un certain nombre de difficultés qui sont inhérentes au traitement de l'oral. Dans ce travail, nous proposons d'étudier le couplage étroit entre la tâche de transcription de la parole et la tâche de reconnaissance des entités nommées. Dans ce but, nous détournons les fonctionnalités de base d'un système de transcription de la parole pour le transformer en un système de reconnaissance des entités nommées. Ainsi, en mobilisant les connaissances propres au traitement de la parole dans le cadre de la tâche liée à la reconnaissance des entités nommées, nous assurons une plus grande synergie entre ces deux tâches qui se traduit par une augmentation significative de la qualité de la reconnaissance des entités nommées.
Nous montrons qu'il est possible, sans perte d'information, d'exprimer Prolexbase selon une approches sémasiologique, conforme à LMF.
Nous étudions, par des méthodes statistiques sur des corpus français et italiens, le phénomène de réduction des termes complexes dans les langues de spécialité. Il existe deux types de réductions : anaphorique et lexicale. Nous montrons que la réduction anaphorique dépend du type de discours (de vulgarisation, pédagogique, spécialisé) mais ne dépend ni du domaine, ni de la langue, alors que la réduction lexicale dépend du domaine et est plus fréquente dans les domaines techniques à évolution rapide. D'autre part, nous montrons que la réduction anaphorique a tendance à suivre la forme pleine du terme, nous définissons une notion d'arbre anaphorique de terme et nous étudions ses propriétés. Concernant la réduction lexicale, nous tentons de démontrer statistiquement qu'il existe une notion de cycle de vie de terme, où la forme pleine est progressivement remplacée par une réduction lexicale.
Mais ces collections ne peuvent être traitées manuellement avec un coût raisonnable  : seuls les systèmes automatiques apportent une solution viable. Dans ce document, nous traiterons de l'extraction automatique de l'identité du locuteur (prénom et patronyme) présente dans les enregistrements sonores. À partir des résultats d'un système de transcription enrichie, nous présentons une méthode qui vise à extraire l'identité des locuteurs de la transcription et à l'assigner aux différents tours de parole. Le système a été évalué sur des enregistrements radiophoniques provenant de la campagne d'évaluation ESTER 1 phase II.
Cet article se propose de comparer les structures de discours proposées en RST, SDRT et dans les DAG de dépendances prolongeant le niveau sémantique de MTT pour le discours. D'où le terme de « capacité générative forte » , emprunté aux grammaires formelles.
Nous rapportons dans cet article un ensemble de résultats liés à la mise au point d'une base de marqueurs de relations lexicales pour un outil d'aide à la réalisation d'ontologies à partir de textes, CAMÉLÉON. L'évaluation de ces patrons sur huit corpus différents montre une grande variation de leurs performances selon le corpus testé.
Cet article présente un état de l'art des techniques d'ingénierie linguistique développées au cours des vingt dernières années dans le domaine de l'aide à la communication pour les personnes handicapées. Nous concluons en montrant que cette problématique est en réalité plus vaste puisqu'elle intéresse toutes les applications de type entrée de texte sur clavier limité.
Dans la production de documents linéaires, les systèmes de génération automatique du langage ont traditionnellement exploité le rôle structurel des marques textuelles du discours, comme les phrases relationnelles et référentielles. Pourtant, ces marques de la cohérence textuelle ne fonctionnent pas dans des documents non linéaires : des nouveaux dispositifs graphiques sont nécessaires avec des règles d'utilisation guidées par des structures théoriques. Dans cet article nous explorons des nouvelles possibilités pour représenter la cohérence dans des documents hypertextuels.
Cet article décrit une plate-forme de TALN, modulaire et multilingue, enrichie d'un système de contrôle basé sur l'aide multicritère à la décision. La présentation est complétée par une description des données linguistiques utilisées ainsi que des applications basées sur cette technologie.
Cet article présente la recherche que nous avons réalisée en examinant le but de la structure du discours de réponse aux questions du type pourquoi (why-QA). Nous avons développé un système de réponse aux questions pourquoi qui utilise les relations RST dans une collection de documents pré-annotés (le RST Treebank). En appliquant cette méthode, nous obtenons un rappel de 53,3 % avec une Moyenne du Rang Inverse (MRR) de 0,662. Nous soutenons que le rappel maximum qui puisse être obtenu en utilisant les relations RST est de 58,0 %. En supprimant les questions qui requièrent une connaissance du monde, le rappel maximal serait de 73,9 %. Nous concluons que les structures du discours peuvent jouer un rôle important dans la réponse aux questions complexes, mais que l'augmentation du rappel nécessite davantage de sortes de traitements linguistiques.
Cet article présente les derniers développements de TERMINAE, qui en font une plateforme adaptée à la construction de diverses ressources terminologiques à partir de textes, y compris d'ontologies. Désormais, TERMINAE aide à élaborer aussi bien des fiches terminologiques, qu'un réseau conceptuel structurant les concepts associés aux termes ou encore une ontologie formelle sur laquelle des inférences peuvent être effectuées. La démarche méthodologique et l'utilisation de l'outil sont détaillés sur un exemple de construction d'un réseau conceptuel portant sur la fabrication du verre à partir d'un texte décrivant cette fabrication. L'accent est mis sur les modalités d'exploration des textes à l'aide des outils de traitement du langage naturel disponibles sur cette plate-forme.
On y examine la façon de représenter ces différents types d'entités en DRT (sur la base de leurs propriétés linguistiques), et les conséquences qu'ont ces méthodes de représentation sur la représentation du lien anaphorique lui-même. Après un inventaire des matériaux de reprise en jeu, on passe en revue les différents types de reprise, directes ou croisées
L'acquisition terminologique est souvent considérée comme une technologie proche de la maturité. Il existe des outils robustes pour réaliser les tâches centrales de ce processus d'acquisition de ressources à partir de corpus. Cependant, mettre en place un assemblage cohérent d'outils pour supporter un processus aussi étendu reste une entreprise difficile, et les concepteurs ne peuvent guère prendre appui sur des architectures de référence. Pour répondre à ces besoins, nous proposons une architecture de services où chaque application est conçue par réutilisation de services génériques et adaptation de services dédiés à des tâches plus sensibles au contexte de l'application.
Cet article vise la description et le repérage automatique de segments contenant de l'obsolescence dans les documents de type encyclopédique. Nous supposons, malgré le caractère non linguistique de ce phénomène, que des indices discursifs permettent le repérage de ces segments. Nous travaillons sur un corpus annoté manuellement par des experts sur lequel nous projetons des indices repérés automatiquement. Nous utilisons des techniques d'apprentissage automatique pour évaluer le pouvoir prédictif de nos indices. À l'aide de techniques de classification supervisée, nous montrons que nos hypothèses sont pertinentes et permettent d'envisager le déploiement d'une méthode automatique pour l'aide au repérage de segments obsolescents.
La simplification automatique a pour objectif de produire une version de textes plus facile à comprendre à destination d'un public identifié. Le plus souvent, le lexique et les règles de simplification sont acquis à partir de corpus parallèles. Comme de tels corpus n'existent pas en français, nous proposons des méthodes pour les construire à partir de corpus comparables. Notre méthode repose sur une étape de filtrage, destinée à ne garder que les meilleures phrases candidates à l'alignement, et une étape d'alignement considérée comme un problème de catégorisation. Il s'agit de décider si une paire de phrases est alignable ou non. Nous exploitons différents types de descripteurs (essentiellement basés sur le lexique et les corpus) et obtenons jusqu'à 0,97 de F-mesure avec les données équilibrées.
Le but du projet décrit dans cet article est d'implémenter un prototype électronique du dictionnaire monolingue du basque Euskal Hiztegia, désormais eEH. L'accent sera mis sur deux aspects : i) la méthodologie utilisée pour corriger et mettre à jour le contenu du dictionnaire après sa transformation de MRD (Machine Readable Dictionary) en TEI (Text Encoding Initiative) et ii) l'application elle-même, application qui a été développée pour faciliter la recherche et la navigation dans l'information contenue dans le dictionnaire. Notre motivation a été double : 1) obtenir une représentation électronique bien structurée d'une ressource importante de la langue basque, ressource susceptible d être utilisée ensuite comme un outil de base dans nos recherches et développements futurs, et 2) offrir aux utilisateurs un ensemble de fonctionnalités allant au-delà de celles offertes par les dictionnaires conventionnels papier ou par des applications classiques de navigation dans un dictionnaire. L'article décrit brièvement le dictionnaire EH lui-même, puis son contenu et sa structure.
Cet article se situe dans le cadre de l'extraction d'information non supervisée en domaine ouvert en se concentrant sur l'extraction et le regroupement à large échelle de relations entre entités nommées de type non défini a priori. L'étape d'extraction combine l'utilisation de critères simples mais efficaces et une procédure de filtrage à base d'apprentissage. L'étape de regroupement organise quant à elle les relations extraites pour en caractériser le type selon une stratégie multiniveau permettant de prendre en compte à la fois un volume important et des critères de regroupement élaborés. Les évaluations menées montrent que cette approche a la capacité d'extraire des relations avec une bonne précision et de les grouper selon leurs similarités sémantique et thématique.
La décision de le transformer en dictionnaire électronique a été prise alors que le dictionnaire était pratiquement achevé. Cet article présente les différents aspects de cette opération, depuis la transformation du TLF en une structure XML complexe, jusqu à sa diffusion sur Internet. Il présente également les différents modes d exploitation du TLF, ainsi que les bases de données annexes qui ont été réalisées pour en améliorer l'efficacité.
L'une des questions du traitement automatique des langues est de discuter de la réalité de la capacité langagière des formalismes. Au-delà de la modélisation linguistique, la théorie générative de Chomsky et le minimalisme s'intéressent à appréhender le langage humain en tant que processus cognitif, ce qui conduit à introduire le principe de dérivation par phases. Une première formalisation du minimalisme a été proposée dans (Stabler, 1997) afin, notamment, d'en étudier les propriétés computationnelles. L'extension formelle proposée ici, fondée sur les grammaires minimalistes catégorielles, (Amblard, 2011), s'attache à intégrer la notion de phase dans un cadre logique qui permet aussi de définir un calcul sémantique. Les enjeux de cette modélisation nous amènent à discuter de la commutativité et de la non-commutativité dans le formalisme.
Comment faire pour qu'il y ait plus d'applications industrielles utilisant la génération en langage naturel ? Autrement dit, pourquoi est-il intéressant du point de vue pratique d'utiliser ces techniques pour ces applications réelles ? Nous essayons de répondre à ces questions en deux étapes : premièrement, nous étudions les avantages de la génération automatique en général, et deuxièmement, nous nous centrons sur la génération linguistique, dont la spécificité réside selon nous dans la combinaison de la qualité et la variabilité des textes générés, la maintenabilité, et l'adaptabilité du système. Finalement, nous proposons un programme de travail pour la première question : "comment faire pour qu'il y ait plus d'applications industrielles utilisant la génération du langage naturel" ?
Les systèmes de questions-réponses ont pour objectif de donner accès à des informations précises. Dans un système de questions-réponses classique, l'étape qui effectue la mise en relation de la question et des réponses est l'extraction de la réponse, qui consiste dans un premier temps à sélectionner des phrases susceptibles de contenir la réponse. La sélection et le classement des phrases sont cruciaux, car ils déterminent fortement le poids des réponses courtes retournées, et donc le classement de ces réponses. Cet article s'intéresse plus particulièrement à cette étape, en tentant d'évaluer des stratégies de sélection des phrases candidates.
Nous présentons la métagrammaire (MG) : un cadre de description linguistique modulaire, permettant, via compilation, la représentation de TAG Lexicalisées. MG permet l'expression de généralisations syntaxiques et formalise des principes rendant compte de la bonne formation des structures lexicalisées, exprimés en utilisant des fonctions syntaxiques. Un outil implémenté compile une MG en une LTAG, en opérant les croisements valides de phénomènes. Nous décrivons une MG pour le français (MGf) et une pour l'italien (MGi). La MGf a été construite à partir d'une LTAG existante (Abeillé 1991) et a permis d'en augmenter la couverture. La MGi a été construite à partir de la MGf. La compilation automatique fournit deux LTAG parallèles, compatibles pour des applications multilingues.
Les modèles vectoriels utilisés pour l'analyse distributionnelle souffrent de la dispersion des données dans la matrice des contextes et du nombre important de dimensions de cette matrice. Ces limitations rendent difficile leur application aux corpus de spécialité, et les termes ne sont habituellement pas pris en compte alors qu'ils sont essentiels. Dans cet article, nous proposons une adaptation de l'analyse distributionnelle afin de pouvoir l'utiliser efficacement sur des textes de spécialité. L'approche proposée réalise une abstraction des contextes distributionnels pour réduire la dispersion des données et ainsi améliorer la qualité des regroupements tout en y incluant les termes. Nous avons évalué notre approche sur deux corpus médicaux.
Les chercheurs en traitement automatique de la langue des signes française (LSF) sont amenés à étudier et à traiter une langue encore incomplètement décrite, sans forme écrite et à partir de corpus fragmentaires. Ils doivent donc opérer des réductions à tous les niveaux de représentation de la langue, contraindre les conditions d'acquisition des discours en LSF et limiter le champ de leurs applications. Il est important de bien expliciter et justifier ces réductions et d'en connaître les implications. Nous présentons les caractéristiques de la LSF et de son évolution et indiquons les différentes approches permettant de réduire la complexité des traitements tout en conservant l'essentiel des spécificités de la LSF.
L'article décrit les relations énonciatives et sémantiques constitutives des discours rapportés, dégagées à partir de l'analyse des marques linguistiques de l'activité citationnelle dans des médias écrits. Les catégorisations et les ressources constituées sont destinées à différentes tâches d'annotation de textes, notamment dans le contexte du web sémantique.
Les textes scientifiques et techniques regorgent d'assertions (hypothèses, conditions, possibilités) qui nuancent le discours et donnent plus ou moins de certitude aux informations. Nous nous intéressons aux assertions portant sur la relation {patient, problème médical} dans les dossiers patients. Notre objectif est d'identifier automatiquement la certitude et les degrés de certitude dans les textes médicaux, ainsi que leur polarité (positive/négative). Nous proposons d'exploiter un ensemble de marqueurs de types morphologique, contextuel et structurel pour identifier et désambiguïser les assertions. Notre système a été évalué dans le cadre de la compétition internationale I2B2. L'évaluation montre que la détection de certaines catégories d'assertion est satisfaisante alors que d'autres restent difficiles à détecter. Nous étudions également l'apport de chaque marqueur et analysons l'évolution des résultats, ainsi que les performances de la détection de différentes catégories d'assertions.
La plate-forme de traitement linguistique Antelope, en partie basée sur la Théorie Sens-Texte (TST), permet l'analyse syntaxique et sémantique de textes sur des corpus de volume important. Antelope intègre plusieurs composants préexistants (pour l'analyse syntaxique) ainsi que des données linguistiques à large couverture provenant de différentes sources. Un effort d'intégration permet néanmoins d'offrir une plate-forme homogène. Notre contribution directe concerne l'ajout de composants d'analyse sémantique et la formalisation d'un modèle linguistique unifié. Cet article présente la plate-forme et la compare à d'autres projets de référence. Il propose un retour d'expérience d'un éditeur de logiciel vers la communauté du TAL, en soulignant les précautions architecturales à prendre pour qu'un tel ensemble complexe reste maintenable.
La plupart des outils d'analyse de sentiments traitent essentiellement l'arabe standard moderne (ASM), et peu d'entre eux ne prennent en considération les dialectes. À notre connaissance, aucun outil en libre accès n'est disponible concernant l'analyse de sentiments de textes écrits en dialecte algérien. Cet article présente un outil d'analyse de sentiments des messages écrits en dialecte algérien. Cet outil est fondé sur une approche combinant l'utilisation de lexiques ainsi qu'un traitement spécifique de l'agglutination. Nous avons évalué notre approche en utilisant deux lexiques annotés en sentiments et un corpus de test contenant 749 messages. Les résultats obtenus sont encourageants et montrent une amélioration continue après l'exécution de chaque étape de notre approche.
La langue et le droit entretiennent depuis toujours des liens étroits, ce dernier étant d'abord un « discours » , mais le traitement automatique de la langue juridique représente aujourd'hui un enjeu majeur du fait de l'empreinte croissante du droit sur le web, de son ouverture et de sa complexification dans les sociétés contemporaines mondialisées. Par ailleurs, le prisme d'une langue de spécialité, ici la langue juridique, permet de mesurer les progrès du traitement automatique des langues. Ce numéro de TAL vise à attirer l'attention sur les défis et les enjeux du traitement automatique de la langue juridique, à montrer l'intérêt des recherches récentes dans ce domaine, mais aussi, plus largement, à montrer comment différents méthodes d'analyse s'organisent pour une langue de spécialité.
Nous montrons comment parsli, notre modèle de la morphologie flexionnelle, permet de représenter ces phénomènes non canoniques et de les formaliser en vue d'une implémentation. Nous l'illustrons au moyen de données de langues variées.
Nous proposons une définition algébrique de cette notion, valable pour les structures utilisées couramment pour les représentations linguistiques : mots sur un alphabet fini, structures attribut-valeur, arbres étiquetés. Nous présentons ensuite une application à une tâche concrète, consistant à apprendre à analyser morphologiquement des formes orthographiques inconnues. Des résultats expérimentaux sur plusieurs lexiques permettent d'apprécier la validité de notre démarche.
Les noms propres sont souvent indispensables pour comprendre l'information contenue dans un document. Notre travail se concentre sur l'augmentation automatique du vocabulaire d'un système de transcription automatique de la parole (RAP) à partir d'un corpus diachronique. Nous faisons l'hypothèse que certains noms propres apparaissent dans des documents relatifs à la même période temporelle et dans des contextes lexicaux similaires. Trois méthodes de sélection de noms propres sont proposées pour augmenter de façon dynamique le vocabulaire en utilisant des informations lexicales et temporelles. Les méthodes sont fondées sur des statistiques de cooccurrences dans des fenêtres de taille fixe, sur l'information mutuelle et sur le modèle vectoriel. Différents paramètres de sélection de noms propres sont également étudiés afin de limiter l'augmentation du vocabulaire. Les résultats de reconnaissance montrent une réduction significative du taux d'erreur de noms propres en utilisant un vocabulaire augmenté.
Cet article présente notre méthode de production automatique de résumé de textes juridiques qui permet aux juristes de consulter rapidement les idées clés d'une décision juridique pour trouver les jurisprudences pertinentes à leurs besoins. La méthodologie repose sur l'exploitation de la structure thématique afin de constituer automatiquement une fiche de résumé augmentant la cohérence et la lisibilité du résumé. La constitution de la fiche de résumé se fait en quatre étapes : la segmentation thématique qui repère la structure du document en quatre thèmes INTRODUCTION, CONTEXTE, RAISONNEMENT JURIDIQUE et CONCLUSION, le filtrage des unités moins importantes comme les citations des articles des lois, la sélection des unités textuelles saillantes dans les segments thématiques et la production du résumé dans la limite de la taille demandée. La conception des différentes composantes du système, appelé LetSum, est décrite en détail ainsi que son implémentation et le résultat d'évaluations.
Nous présentons la démarche que nous avons adoptée pour mener à bien une évaluation système dans le contexte du développement d'un système d'alignement sous-phrastique, ALIBI. À cet égard, nous examinons trois procédures d'évaluation qui correspondent à des aspects fondamentaux de la mise au point de systèmes de traitement automatique des langues : une évaluation par annotation des sorties du système qui permet d'observer le comportement de chaque composante prise isolément, une évaluation avec des références multicorpus qui permet d'observer le comportement d'un système selon le type de corpus qu'il prend en entrée et une évaluation avec une référence standard disponible publiquement qui permet d'observer son comportement par rapport à des outils de même famille. Nous décrivons chaque expérience d'évaluation et faisons le point sur la nature des résultats qu'elle fournit ainsi que leurs apports.
L'index de fin de livre est un outil traditionnel d'accès à l'information. Il présente les sujets traités dans l'ouvrage sous une forme et dans un ordre différent du document luimême. Nous montrons qu'un tel index présente une vue synthétique du contenu et qu'en cela, il s'apparente aux résumés indicatifs. De nouvelles méthodes sont développées pour construire automatiquement de tels index et il est intéressant de les confronter aux méthodes de résumé automatique. Même s'ils ont des points communs, ces deux types d'outils ne relèvent pas de la même tradition et les méthodes qui permettent de les construire obéissent à des logiques différentes. Leur confrontation permet de mieux comprendre le propre de chacune.
Dans un contexte d'interaction homme-machine, les systèmes de détection des émotions dans la voix doivent être robustes aux variabilités et efficaces en temps de calcul. Cet article présente les performances que nous pouvons obtenir en utilisant uniquement des indices paraverbaux (nonverbaux). Nous proposons une méthodologie pour sélectionner les familles de paramètres robustes, en étudiant trois ensembles de descripteurs testés sur trois corpus de données spontanées collectés dans des contextes d'interaction homme-machine. Le résultat de notre étude met en avant les paramètres perceptifs liés à l'énergie spectrale (énergie par bandes de Bark), en obtenant des performances de détection sur quatre émotions au niveau de l'ensemble des descripteurs de référence du Challenge Interspeech 2009.
Dans cet article, nous proposons une modélisation logique d'une classe de noms d'action en français qui sont lexicalement ambigus entre une interprétation processive et une interprétation résultative dans laquelle ils désignent des objets créés par le processus sous-jacent, des artefacts. Ces travaux, basés sur la notion de type pointé et ses extensions en λ-calcul typé, fournissent un cadre de formalisation satisfaisant une approche compositionnelle du sens et allant dans le sens d'un traitement de l'ambiguïté lexicale en français qui soit à la fois unifié, opératoire et conforme aux propriétés linguistiques observées.
Cette recherche a pour objectif la validation d'une méthodologie pour l'étude de marqueurs de la segmentation dans un grand corpus de textes. Deux indices signalant les ruptures thématiques dans un texte sont proposés. Le premier s'appuie sur la présence de marques de paragraphe et emploie le rapport des chances pour identifier les meilleurs marqueurs. Le second prend en compte la cohésion lexicale par l'entremise d'un indice issu de l'analyse sémantique latente. Ces deux indices ont été appliqués principalement à l'étude des expressions adverbiales temporelles dans des textes littéraires. Les analyses effectuées confirment une série d'hypothèses linguistiques au sujet de la fonction de marqueur de la segmentation de ces expressions.
Les corpus annotés sont toujours plus cruciaux, aussi bien pour la recherche scientifique en linguistique que le traitement automatique des langues. Ce numéro spécial passe brièvement en revue l'évolution du domaine et souligne les défis à relever en restant dans le cadre actuel d'annotations utilisant des catégories analytiques, ainsi que ceux remettant en question le cadre lui-même. Il présente trois articles, l'un concernant l'évaluation de la qualité d'annotation, et deux concernant des corpus arborés du français, l'un traitant du plus ancien projet de corpus arboré du français, le French Treebank, le second concernant la conversion de corpus français dans le schéma interlingue des Universal Dependencies, offrant ainsi une illustration de l'histoire du développement des corpus arborés.
Cet article présente SLAM, un modèle de solutions lexicales automatiques pour métaphores de type « déshabiller* une pomme » . SLAM recherche une solution conventionnelle à ce type de production métaphorique. Pour cela, il croise l'axe paradigmatique du verbe métaphorique « déshabiller* » , sur lequel est rapproché le verbe « peler » , avec l'axe syntagmatique émergeant d'un corpus dans lequel « peler une pomme » est une structure syntaxiquement et sémantiquement régulière. Nous testons le modèle sur DicoSyn, un réseau de synonymes de type petit monde, pour le calcul de l'axe paradigmatique et sur le corpus Frantext.20 pour le calcul de l'axe syntagmatique. Enfin, nous évaluons ce modèle à l'aide d'un corpus expérimental issu de la base de données Flexsem.
Nous présentons une technique efficace pour calculer la probabilité d'une séquence de mots éventuellement discontigus, c'est-à-dire la probabilité que ces mots apparaissent dans un ordre donné, quel que soit le nombre d'autres mots pouvant apparaître entre eux. Notre approche est basée sur une formalisation du problème en une chaîne de Markov particulière, dont nous présentons et exploitons les spécificités afin d'obtenir une complexité compétitive. Nous développons notre approche plus avant afin de calculer la fréquence documentaire attendue d'une séquence donnée. Cet article présente finalement une application de ces travaux : une méthode automatique pour l'évaluation directe de l'intérêt d'une séquence de mots, par le biais de comparaisons statistiques entre leurs fréquences attendues et observées.
VerbNet est une ressource lexicale pour les verbes anglais qui est largement utilisée en TAL du fait de sa bonne couverture lexicale et syntaxique et de son encodage systématique des rôles thématiques. Nous présentons comment nous avons développé Verbenet à partir de VerbNet tout en utilisant au maximum les ressources lexicales existantes du français, et comment sont encodées les différentes alternances du français en mettant l'accent sur les différences avec l'anglais (l'existence de formes pronominales, par exemple). Cet article devrait permettre à un chercheur en TAL une utilisation simple et efficace de Verbenet pour une tâche comme l'annotation en rôles sémantiques.
Parce que l'éthique est un sujet important, mais sous-représenté dans le domaine du traitement automatique des langues et de la parole (TALP) par rapport à d'autres domaines scientifiques, nous nous interrogeons sur les causes de ce relatif désintérêt. Pour cela, nous mettons en contextes historique et sociétal le traitement de l'éthique en TALP. Les articles de ce numéro spécial, bien que ne couvrant pas tous les sujets, présentent un échantillon cohérent pour amener la communauté scientifique au sens large à réfléchir sur ces sujets et, à terme à s'en saisir et à agir. C'est la motivation principale de ce numéro spécial : développer les outils théoriques et pratiques qui nous permettront, en tant que communauté, de ne pas garder sous le boisseau les problèmes éthiques.
Les systèmes d'analyse sémantique reposent le plus souvent sur des analyses locales du texte tandis qu'il devient de plus en plus évident que l'organisation du texte fait sens et doit être exploitée pour fournir un meilleur accès au contenu des documents. Cet article a pour objectif de montrer l'apport d'une approche textuelle au sein d'un cadre applicatif précis  : la modélisation des Guides de Bonnes Pratiques Médicales. Le système a été validé sur trois aspects complémentaires  : utilité, performances et pertinence de la méthode.
Les termes-clés sont les mots ou les expressions polylexicales qui représentent le contenu principal d'un document. Dans cet article nous présentons TopicRank, une méthode non supervisée à base de graphe pour l'extraction de termes-clés. Cette méthode groupe les termes-clés candidats en sujets, ordonne les sujets et extrait de chacun des meilleurs sujets le terme-clé candidat qui le représente le mieux. Les expériences réalisées montrent une amélioration significative vis-à-vis de l'état de l'art des méthodes à base de graphe pour l'extraction non supervisée de termes-clés.
De nombreux travaux se sont attaqués à la reconnaissance des entités médicales à partir de textes. Cependant il n'y a pas eu, à notre connaissance, d'études comparant deux stratégies pour traiter cette tâche : (i) l'extraction en amont des syntagmes nominaux, suivie d'une étape de catégorisation de leur type et (ii) la détermination simultanée des frontières et des types des entités. Nous comparons leur robustesse et aussi leur portabilité en les évaluant sur deux corpus médicaux standards de genres différents. Les résultats obtenus confirment que les méthodes statistiques sont plus robustes que celles à base de règles à condition qu'un nombre suffisant d'exemples soit disponible. À cette contrainte s'ajoute le manque de portabilité des méthodes à base d'apprentissage sur des corpus différents. Les méthodes hybrides combinant les aspects sémantiques et statistiques permettent d'améliorer davantage les performances obtenues par apprentissage.
Les tables du Lexique-Grammaire, dont le développement a été initié par (Gross, 1975), constituent un lexique syntaxique très riche pour le français. Nous présentons dans cet article le travail réalisé afin de formaliser la classification existante pour les verbes distributionnels à l'aide de formules logiques, dans le but d'assurer la maintenance du lexique. Nous détaillons les différents types de propriétés définitoires, ainsi que leur codage dans la table des classes, regroupant l'ensemble des propriétés de toutes les classes de verbes. La définition formelle de l'ensemble des propriétés définitoires pour chaque classe a permis de représenter la classification sous forme d'arbre de décision, afin d'aider à trouver la classe associée à toute nouvelle entrée, ce qui permet de garantir la future alimentation du lexique.
Cet article détaille les résultats d'analyses réalisées sur la transcription d'entretiens avec des patients schizophrènes, aux niveaux de la production orale (disfluences) et du lexique (morpho-syntaxe et lemmes). Le corpus traité contient plus de 375 000 mots, son analyse a donc nécessité l'utilisation d'outils de traitement automatique des langues (TAL) et de textométrie. Nous avons en particulier séparé le traitement des disfluences du traitement lexical, ce qui nous a permis de montrer que si les schizophrènes produisent davantage d'achoppements et de répétitions (disfluences) que les témoins, la richesse de leur lexique n'est pas significativement différente.
L'annotation de relations temporelles demeure encore aujourd'hui un défi : annoter manuellement de façon fiable et cohérente les relations temporelles dans des textes reste difficile et l'est bien plus encore lorsqu'il s'agit d'annotation automatique. Cet article préconise un changement dans la définition du problème en proposant une approche qui, en s'appuyant sur la syntaxe et sur une stratégie de type « diviser pour conquérir » , offre une manière plus élaborée de sélectionner les entités impliquées dans une relation temporelle. La décomposition du problème en de plus petites questions se concentrant sur la syntaxe et l'identification de solutions précises et linguistiquement fondées pour les résoudre favorisent une meilleure compréhension des phénomènes impliqués dans l'établissement de relations temporelles. Nous illustrons le potentiel des solutions linguistiquement fondées dans le cadre de l'identification de relations temporelles en proposant et évaluant une première série de tâches se concentrant sur la syntaxe.
Dans ce travail nous présentons une étude détaillée de la tâche d'acquisition de paraphrases sous-phrastiques à partir de corpus monolingues parallèles. Nous démontrons empiriquement que ces corpus, bien qu'extrêmement rares, constituent le type de ressource le plus adapté pour ce type d'étude. Nos expériences mettent en jeu cinq techniques d'acquisition, représentatives de différentes approches et connaissances, en anglais et en français. Afin d'améliorer la performance en acquisition, nous réalisons la combinaison des paraphrases produites par ces techniques par une validation reposant sur un classifieur automatique bi-classe. Un résultat important de notre étude est l'identification de paraphrases qui défient actuellement les techniques étudiées, lesquelles sont classées et quantifiées en anglais et français.
Nous présentons une expérience d'utilisation d'informations de sous-catégorisation par un analyseur syntaxique pour la résolution d'ambiguïtés de rattachement prépositionnel. Le lexique de sous-catégorisation est constitué de probabilités associées à des couples (mot, préposition). Il a été construit automatiquement à partir d'un corpus de 200 millions de mots. Pour évaluer ce lexique, nous utilisons quatre corpus de test de genres variés.
Cet article retrace une expérience de constitution d'un corpus arboré pour le serbe, conçu dans le but de doter cette langue des instruments nécessaires à l'analyse syntaxique et, plus généralement, de favoriser des recherches plus systématiques aussi bien en TAL (traitement automatique des langues) qu'en linguistique serbe. Au-delà de la description des résultats de ce projet, nous présentons une méthode de confection d'un corpus arboré qui vise à optimiser les ressources, par définition rares, dont on dispose dans le cas d'une langue peu dotée, qu'il s'agisse de moyens matériels (corpus et outils) ou humains. Nous montrons comment tirer au mieux parti de l'existant pour faciliter le travail des annotateurs humains et accélérer l'enrichissement du corpus, tout en garantissant la validité de l'annotation produite. Cette méthode, basée sur des principes transposables à d'autres langues, a vocation à faciliter la création des corpus arborés pour les langues sous-dotées en général.
L'évaluation et l'entraînement d'algorithmes pour le traitement automatique d'anaphores nécessitent le développement de corpus annotés. Dans cet article, nous proposons une méthodologie pour l'annotation des descriptions définies qui permet d'annoter environ 5 000 descriptions définies de façon consistante et utile pour les modules de résolution. Nous présentons ensuite les résultats de cette annotation et discutons de leurs implications pour la résolution automatique.
Les plates-formes d'évaluation sont à l'heure actuelle peu répandues et les enjeux qu'elles représentent sont certainement sous-estimés. Gain de temps, économies de moyens, progression plus rapide des systèmes, rassemblement d'une communauté autour d'un même paradigme d'évaluation ; les avantages sont nombreux. Dans cet article, nous proposons une plate-forme pour évaluer automatiquement des systèmes d'annotation syntaxique et relatons son déploiement en tant que service dans une campagne d'évaluation. Après avoir dressé un état de l'art des plates-formes utilisées pour l'évaluation de systèmes TAL et des outils disponibles pour la mise en place de serveurs Web, nous présentons notre plate-forme d'analyse syntaxique et sa mise en œuvre sous forme de serveur Web dans le projet PASSAGE, nous montrons ensuite l'intérêt de la généralisation d'un tel service à d'autres domaines du TAL.
Dans cet article, nous proposons IrcamCorpusTools, une plate-forme ouverte et facilement extensible pour la création, l'analyse et l'exploitation de corpus de parole. Elle est déjà employée pour la synthèse de la parole par sélection d'unités, les analyses prosodique et phonétique contextuelles, la modélisation de l'expressivité, ainsi que pour exploiter divers corpus de parole en français et autres langues.
Cet article décrit la synthèse de deux types de phrases à extraction en français -- les phrases avec des relatives et des interrogatives indirectes. D'autre part, notre étude apporte un éclairage nouveau sur la formalisation des règles linguistiques de correspondance entre la structure sémantique et la structure syntaxique profonde et sur l'organisation de l'ensemble de ces règles, qui constituent le module sémantique d'un modèle Sens-Texte.
Au-delà des moteurs de recherche généralistes, des outils capables d'interroger des collections documentaires spécialisées doivent être proposés pour répondre à des besoins d'information précis. Cela suppose une analyse sémantique adaptée à la collection de documents et au domaine considérés. C'est l'objectif de la plate-forme d'annotation Ogmios décrite ici. Les performances obtenues pour l'annotation des documents issus du Web sont compatibles avec le rythme de leur récupération. Nous montrons également comment cette plate-forme a été intégrée dans un moteur de recherche spécialisé.
Cet article présente une annotation automatique d'expressions spatiales géolocalisables permettant de contribuer à l'amélioration du processus d'indexation de l'information géographique contenue dans des documents textuels. La méthode proposée combine l'exploitation de relations spécifiques intra-phrastiques et de ressources externes : lexiques de verbes spécialisés, thésaurus généraux, ontologie topographique et gazetiers. Elle repose sur la reconnaissance d'entités nommées spatiales étendues afin, d'une part, d'en déduire une représentation symbolique exprimée en termes de traits sémantiques puis, d'autre part, de leur attribuer au moins une représentation numérique. Notre méthode a été intégrée et expérimentée dans un processus totalement automatisé d'annotation et d'indexation spatiale de documents textuels. Les expérimentations ont été réalisées sur un fonds documentaire réel de type « récit de voyage » .
Cet article décrit MorphoClust et MorphoNet, deux méthodes pour l'apprentissage non supervisé de familles morphologiques. MorphoClust forme des familles par groupements successifs, de manière similaire aux méthodes de classification ascendante hiérarchique. La méthode MorphoNet est quant à elle fondée sur la détection de communautés dans des réseaux lexicaux. Les nœuds de ces réseaux représentent des mots et les liens des règles de transformation morphologique acquises automatiquement à partir de mots graphiquement similaires.
Cet article présente une typologie de facteurs de risque concernant les technologies numériques et plus particulièrement les technologies langagières. Son objectif est d'offrir une grille d'analyse pour une évaluation critique des recherches et applications du TAL dans une démarche éthique.
Une analyse syntaxique profonde et robuste améliore la performance d'un système de question-réponse. Le système utilise l'information contenue dans une version analysée syntaxiquement du corpus des documents. Nous montrerons que l'utilisation de l'information syntaxique améliore certains modules de Joost, comme l'extraction et l'ordonnancement final des réponses, l'acquisition automatique d'information lexicale, l'extraction de faits hors ligne et la recherche de passages.
Le microblogage est une tendance récente dans l'Internet d'aujourd'hui. Les utilisateurs expriment leurs opinions par le biais des plates-formes de microblogage comme Twitter. Dans nos travaux, nous utilisons Twitter comme une source de données multilingues pour collecter un corpus de textes exprimant des sentiments annotés. Avec le lexique issu du corpus collecté, nous construisons un classificateur des sentiments que nous appliquons à trois types de tâches : la classification des sentiments dans des textes courts, la désambiguïsation des adjectifs de sentiments ambigus et la construction de lexiques affectifs pour d'autres langues. Nous les appelons des tâches de microanalyse des sentiments car elles opérent sur des textes courts ou des portions de textes.
Comme pour de nombreuses autres problématiques TAL, la reconnaissance d'entités nommées met en jeu aussi bien des systèmes à base de connaissances que des systèmes guidés par les données. Dans cet article, nous proposons une approche médiane par l'adaptation de méthodes issues de l'extraction de connaissances. Notre système, mXS, intègre des techniques de fouille séquentielle hiérarchique pour la détection des entités nommées. Le système adopte une démarche centrée sur les données pour extraire des motifs symboliques. Il repose par ailleurs sur une stratégie originale qui consiste à rechercher séparément le début et la fin des entités. Cette approche présente l'intérêt de conserver une certaine robustesse par rapport aux bruit et disfluences. À ce titre, mXS a participé à la campagne d'évaluation ETAPE où il a présenté de bons résultats. Cet article présente le fonctionnement de mXS et ses performances sur les jeux de données issus de deux campagnes d'évaluation francophones (ESTER 2 et ETAPE).
Les principaux travaux en fouille textuelle privilégient communément la taille du corpus sur sa qualité. Ainsi dans le cadre de l'alignement lexical à partir de corpus comparables, les meilleurs résultats sont obtenus pour des corpus de grande taille (plusieurs millions de mots). Pour les domaines de spécialité, et pour de nombreuses paires de langues, il n'est pas possible de disposer de corpus textuels aussi volumineux. Dans le cadre de ce travail, nous soutenons l'hypothèse que la qualité des données textuelles peut non seulement suppléer à leur quantité mais garantit aussi celle des ressources lexicales extraites. En particulier, nous montrons l'intérêt de prendre en compte le type du discours lors de la constitution du corpus.
Ce modèle repose sur l'utilisation de IGTREE, un algorithme d'inférence d'arbre de décision capable de traiter à la fois un grand nombre de classes et d'exemples d'apprentissage. À travers une première série d'expérimentations nous montrons que la capacité de prédiction du modèle augmente log-linéairement avec le nombre d'exemples d'entraînement. La taille des arbres inférés croît, elle, linéairement avec la quantité d'exemples d'apprentissage. Lorsque notre modèle est entraîné sur un corpus journalistique de 30 millions de mots, le nombre de mots correctement prédits est de 42.2 % sur des textes journalistiques. Une seconde série d'expérimentations démontre que ce prédicteur générique peut être spécialisé pour traiter des configurations dans lesquelles l'ensemble des mots à prédire se restreint à un petit ensemble. Le modèle spécialisé atteint des meilleurs résultats que le classifieur générique.
Dans cet article, nous passons en revue les principales difficultés que posent les noms propres aux applications de traitement de la parole. Deux applications sont plus particulièrement étudiées, à savoir la reconnaissance de la parole grand vocabulaire et la synthèse de la parole. Pour ces deux applications, nous montrons que la prise en compte des noms propres par les modules de traitement linguistique (modèles syntaxiques pour la reconnaissance, système de conversion graphème-phonème pour la synthèse) implique l'analyse de larges corpus de textes et le développement de volumineux dictionnaires de noms propres. Les méthodes pratiques pour construire de telles ressources sont présentées et discutées.
Dans cet article, nous décrivons un schéma d'annotation pour l'encodage des objets abstraits (propositions, faits et possibilités) associés aux relations de discours et à leurs arguments tels qu'annotés dans le Penn Discourse TreeBank.
La plupart des systèmes de résumé automatique produisent comme résultat une entité indépendante du texte source présumée d'avoir un sens en soi même.
Cet article a pour objet la représentation formelle des lexiques des langues des signes. Les représentations existantes, basées sur la description systématique de paramètres, nous semblent trop rigides. Nous proposons un modèle flexible basé sur une approche géométrique et séquentielle. Nous discutons ensuite de sa représentation sous forme de graphes, permettant une mise en évidence claire de la structure de la description et des dépendances entre ses constituants. Nous présentons les apports possibles du modèle dans deux domaines : la génération automatique et la constitution de bases de données pour l'étude linguistique.
L'article analyse l'impact morphologique des propriétés quantitatives des structures lexicales et sous-lexicales dans la décomposition de mots morphologiquement complexes en faisant appel à une simulation basée sur des états d'activation. Nous observons qu'il existe un noyau complexe de relations extra-sémantiques capable d'activer des représentations proto-morphologiques et de canaliser efficacement la décomposition des mot complexes. Nous proposons un modèle de manipulation de ce noyau d'information dans l'élaboration lexicale de pseudo-mots complexes. Le modèle est testé par rapport à un algorithme de distance d'édition et à des jugements sur la similarité de mots fournis par des locuteurs natifs.
La communauté du TAL développe de nombreux corpus, souvent librement disponibles, disposant d'annotations riches mais difficilement utilisables pour des chercheurs non-informaticiens. Si la communauté du TAL souhaite ouvrir ses corpus annotés à un public plus large, elle doit impérativement concevoir et déployer des interfaces simples, ce qui n'est pas un problème trivial. Dans cet article, nous réfléchissons, dans le cadre du projet Scientext, aux critères ergonomiques et aux méthodes permettant d'élaborer un système de requêtes facile d'accès et soulignons les limites de la plupart des outils existants. Nous présentons la plateforme ScienQuest, conçue pour effectuer sans connaissances techniques préalables des recherches sur les parties textuelles, les parties du discours et les fonctions syntaxiques.
Contrairement à la plupart des langues modernes, le moyen français est une langue dont l'orthographe n'est pas encore stabilisée. Il existe de très nombreuses variantes pour un même mot et en conséquence les méthodes classiques de lemmatisation ne peuvent pas s'appliquer. LGeRM (lemmes, graphies et règles morphologiques) propose une solution qui s'appuie sur une base de formes connues lemmatisées et sur un ensemble de règles graphémiques et morphologiques spécifiques de la langue médiévale. Il permet ainsi de faciliter la consultation d'un dictionnaire, l'interrogation et la lemmatisation de textes médiévaux et trouve des applications dans l'édition électronique de manuscrits et la construction automatique de glossaires. Cet outil polyvalent est accessible sur Internet à l'adresse www.atilf.fr/dmf.
La normalisation de documents en domaine contraint tels que des notices de médicaments peut être poussée plus loin que les technologies actuelles ne le permettent, et sans alourdir significativement la tâche du rédacteur technique. Nous proposons une nouvelle approche de normalisation de documents en domaine contraint qui utilise des représentations du contenu bien formées des documents en termes de buts communicatifs et des textes normalisés associés. Cette approche combine une analyse automatique et une phase de négociation interactive. La création de documents normalisés par les méthodes traditionnelles de saisie du texte est présentée et critiquée, et le paradigme émergent de la création de documents par spécification du contenu, dans lequel s'ancre notre approche, est introduit.
Cet article s'intéresse à la classification d'opinions de textes communautaires par apprentissage supervisé, en vue de les utiliser pour un système de recommandation. Nous comparons différents prétraitements, représentations et techniques d'apprentissage sur des données réelles parlant de films et présentant diverses particularités (textes trés courts en anglais, contenant beaucoup de codes type sms, d'abréviations, de fautes d'orthographe, etc.). Nous étudions en détails les résultats de différents classifieurs ainsi que l'apport des prétraitements sur ce type de données. Pour finir, nous évaluons les résultats du meilleur classifieur à lâaide d'un moteur de recommandation de type filtrage collaboratif.
Cet article propose une analyse de la résolution des anaphores zéro et des pronoms explicites en coréen, tirés du corpus Penn Korean Treebank. Notre algorithme de résolution automatique a été construit en faisant appel au modèle du centrage tel qu'il est développé dans le cadre de la théorie de l'optimalité. Cette méthode nous a permis d'obtenir une précision de 74 %. En outre, nos expériences montrent que deux facteurs doivent être conjointement pris en compte pour la résolution des anaphores pronominale : le rapport avec le topique global et le trait de topicalité des pronoms.
Ces corpus articulent plusieurs niveaux de description linguistique (morphologie, syntaxe, sémantique, etc.), de façon à permettre la construction d'observables associant plusieurs niveaux, ainsi que la description de corrélations entre niveaux de description. Les corpus multiannotés sont complexes à construire, à représenter et à interroger. Les spécificités de CorpusReader tiennent principalement au choix d'opérer une fusion a posteriori des annotations que produisent les outils d'analyse existants, plutôt qu'à organiser leur interopérabilité.
La segmentation thématique de documents a fait l'objet d'un nombre important de travaux dont il n'est pas toujours facile de dégager des conclusions claires, en particulier en ce qui concerne l'utilisation de connaissances. Dans cet article, nous proposons d'examiner deux voies se situant dans le même cadre pour améliorer une méthode de segmentation fondée sur la simple récurrence lexicale. Ces thèmes sont ensuite utilisés pour faciliter l'identification des similarités thématiques entre unités de discours. La seconde réalise le même but en faisant appel à une ressource externe, en l'occurrence un réseau de cooccurrences lexicales construit à partir d'un large corpus. Ces deux approches sont également combinées. Une évaluation de ces deux approches et de leur combinaison est réalisée dans un même cadre et illustre l'intérêt de cette combinaison.
Le programme 'Corpus de la parole' est un projet en collaboration entre le ministère de la culture et le CNRS qui vise à constituer une collection de ressources orales sur le français et les langues de France. Un portail WEB offre un accès éditorialisé à cette collection. Cet article présentera les points principaux de l'organisation de ce programme, de la collecte des corpus aux aspects de pérennisation en passant par l'accès et la diffusion des données numériques.
Cet article rapporte le développement et l'évaluation d'un corpus historique conçu pour des recherches empiriques sur l'interaction entre la structure d'information et la syntaxe dans l'ancien haut-allemand. La création et l'exploration du corpus contribuent à l'investigation du rôle des conditions pragmatiques pour la typologie syntaxique, sa variation et sa mutation dans les langues germaniques.
Les livres d'heures sont le plus grand best-seller de tout le Moyen Âge, avec plus de 10 000 témoins conservés. Incontournables pour comprendre l'univers mental médiéval, leurs textes ont été très peu étudiés. Ils sont très longs et ont une structure complexe correspondant à l'organisation liturgique médiévale et la prière quotidienne de l'office. Cet article décrit les méthodes et les traitements automatiques mis en oeuvre sur les livres d'heures : la reconnaissance de l'écriture manuscrite et la segmentation adaptées à ces manuscrits. L'approche de segmentation semi-supervisée proposée tire profit de la constitution spécifique du manuscrit pour mieux retrouver leur structure malgré le bruit engendré par la reconnaissance de l'écriture.
Cet article décrit la ressource ANNODIS, issue d'un projet financé par l'ANR, corpus de français écrit enrichi à différents niveaux, dont un niveau d'annotation manuelle de structures discursives. Une originalité de la ressource est de proposer un corpus diversifié (plusieurs types de textes sont représentés) et deux annotations fondées sur des approches distinctes de la structuration des discours. La description de la ressource -- objets annotés, textes composant le corpus -- s'accompagne de la présentation des ancrages théoriques sous-jacents aux modèles d'annotation, et des choix méthodologiques qui ont guidé les diverses phases de préparation et d'annotation du corpus. Nous formulons les enjeux d'une telle ressource pour la linguistique et le TAL, et présentons les premières exploitations.
Nous présentons un cadre générique en traduction automatique statistique (TAS) dans lequel des prédictions lexicales, sous forme d'un modèle de langue local à la phrase à traduire, sont exploitées pour guider la recherche de la meilleure hypothèse de traduction, ce qui a pour effet d'opérer une micro-adaptation lexicale. Nous proposons une instanciation de ce cadre qui est évaluée sur trois paires de langues : les prédictions auxiliaires proviennent d'autres systèmes de TAS qui réalisent une triangulation via une langue auxiliaire. Une première configuration met en jeu neuf langues auxiliaires, ce qui permet de mesurer la contribution relative de chaque langue. Nous proposons ensuite d'utiliser simultanément ces neuf systèmes, en les combinant par consensus. Nos résultats montrent qu'il est possible d'augmenter les performances d'un système de TAS de manière entièrement automatique en exploitant des sources auxiliaires.
Cet article présente la problématique de l'alignement monolingue avec recherche de déplacements. Celle-ci est posée par la critique génétique, une discipline d'études littéraires. Les solutions informatiques existantes ne sont pas satisfaisantes pour répondre à ce problème NP-difficile. Nous proposons d'emprunter à la bioinformatique et l'algorithmique textuelle une famille d'algorithmes appelée alignement par fragments. Une adaptation de ce type d'algorithmes pour le TAL est décrite. Une évaluation expérimentale présente les bons résultats obtenus face à d'autres méthodes.
Cet article présente le projet Prolex de traitement automatique des noms propres. En particulier, nous expliquons la constitution d'un dictionnaire relationnel, la découverte de relations entre les noms propres d'un texte et l'étude de la dérivation morphologique d'un toponyme en un nom d'habitants.
Ces travaux abordent la question de l'accès aux textes numériques, et plus particulièrement de l'accès à leur "contenu informationnel" considéré sous l'angle de son ancrage temporel. Il s'agit ainsi à la fois de mettre en oeuvre des systèmes d'interaction avec les utilisateurs et de parvenir à modéliser et représenter formellement la sémantique d'une des unités textuelles qui contribue de façon saillante à ancrer dans le temps les situations décrites dans les textes : les adverbiaux de localisation temporelle. L'étude linguistique vise à montrer que l'ingénierie des langues peut gagner à envisager d'une façon renouvelée l'analyse des "expressions temporelles" dans les textes, en cherchant à formaliser les différentes opérations sémantiques à l'oeuvre dans les adverbiaux de localisation temporelle. Nous montrons que cette démarche permet d'envisager l'élaboration de nouveaux systèmes de recherche d'information en mesure de traiter des requêtes associant un critère calendaire avec un ensemble de mots-clés, comme "les universités au début du XIIe siècle", par exemple.
Les méthodes externes d'évaluation de systèmes de TA définissent des mesures de qualité à partir des résultats de TA et de leur usage. Alors que les systèmes opérationnels sont depuis longtemps le plus souvent évalués par des méthodes fondées sur la tâche, les campagnes d'évaluation des dernières années utilisent (parcimonieusement) des méthodes subjectives assez chères fondées sur des jugements humains peu fiables, et (pour la plus grande part) des méthodes basées sur des traductions de référence, impossibles à utiliser lors de l'utilisation réelle d'un système, d'autant moins corrélées aux jugements humains que la qualité augmente, et totalement irréalistes en ce qu'elles forcent à mesurer les progrès sur des corpus fixes, sans cesse retraduits, et non sur de nouveaux textes à traduire pour des besoins réels. Il y a aussi de nombreux biais introduits par le désir de diminuer les coûts, en particulier l'utilisation de corpus parallèles dans le sens inverse de leur production et l'utilisation de juges monolingues au lieu de bilingues. Nous prouvons cela par une analyse de l'histoire de l'évaluation en TA, des méthodes d'évaluation du « courant dominant » , et de certaines récentes campagnes d'évaluation. Nous proposons d'abandonner les méthodes fondées sur des traductions de référence en évaluation externe, et de les remplacer par des méthodes strictement fondées sur la tâche, en les réservant à l'évaluation interne.
Cet article montre l'intérêt d'utiliser les motifs issus des méthodes de fouille de données dans le domaine du TAL appliqué à la biologie médicale et génétique. Nous proposons une approche pour apprendre les patrons linguistiques par une méthode de fouille de données fondée sur les motifs séquentiels et sur une fouille dite récursive des motifs eux-mêmes. Elle ne nécessite pas de ressources linguistiques autres que le corpus d'apprentissage. Pour la reconnaissance d'entités biologiques nommées, nous proposons une méthode fondée sur un nouveau type de motifs intégrant une séquence et son contexte.
La résolution de métonymie des entités nommées constitue un réel enjeu pour le traitement automatique des langues et bénéficie depuis peu d'un intérêt grandissant. Dans cet article, nous décrivons la méthode que nous avons développée pour la résolution de métonymie des entités nommées dans le cadre de la compétition SemEval 2007. Afin de résoudre les métonymies sur les noms de lieux et noms d'organisations, tel que requis pour cette tâche, nous avons mis au point un système hybride fondé sur l'utilisation d'un analyseur syntaxique robuste combiné avec une méthode d'analyse distributionnelle. Nous décrivons cette méthode ainsi que les résultats obtenus par le système dans le cadre de la compétition SemEval 2007.
Cet article présente SIBYSEM, un moteur de prédiction utilisé pour la complétion de mots dans le système de communication assistée SIBYLLE. SIBYSEM est un modèle adaptatif qui prend en compte les saisies de l'utilisateur et propose une adaptation sémantique au champ sémantique courant du discours basée sur l'analyse sémantique latente. De nombreuses expérimentations rendent compte des capacités d'adaptation du modèle. Nous rendons compte également de sept années d'expérience acquise avec SIBYLLE au cours de sept années d'utilisation quotidienne au sein du centre de rééducation de Kerpape.