The planned Pan-European cellular digital mobile radio (DMR) system envisaged for the early 1990s intends to use medium rate encoding of about 16 kbit/s. At least three phases of experimentation are planned, these being: (1) national pre-selection tests. (2) European selection test, and (3) the final characterisation and verification test with associated channel coding. The first two phases intend to use listening tests only where effects due to the extra processing delay of the codecs will not be tested. An initial maximum delay target of 65 ms was set. The final phase will use conversion tests do evaluate the effects of quality, delay and echo and will be complemented by listening tests which will also be used for codec characterisation. This paper describes subjective testing methodologies adopted to select suitable candidate codecs capable of being used in the proposed DMR system. Each administration participating in the tests had to convince the European Committee (Conférence Européenne des Administrations des Postes et Télécommunications, CEPT) that their candidate codec was better than or at least as good as the speech quality of companded FM analogue 900 MHz systems currently in use in Europe.
A measure was obtained of variability in fundamental frequency (F 0) in citation forms of lexical tones. The language selected for investigation was Thai, a tone language with five lexical tones: mid, low, falling, high and rising. Twenty speakers participited in the experiment: 10 “young” male speakers and 10 “old” speakers, 5 male and 5 female. High-quality tape recordings were obtained of each subject's productions of a minimal set of five monosyllabic words. F 0 contours were extracted by a cepstral analysis. A comparison was made of inter- and intraspeaker variability in the production of the five Thai tones. Results of analysis of variance indicated that the degree of intersubject variability in F 0 was greater than intraspeaker across all five tones, that young and old speakers exhibited the same pattern of variability, and that variability in tone production differed depending on the lexical tone. The falling and rising tones exhibited smaller degrees of variability than the mid, low or high. Findings are interpreted to highlight the nature of F 0 variability, the relationsip of F 0 variability to amount of F 0 movement, and crosslinguistic differences in F 0 variability as a function of prosodic structure.
Previous research has indicated that workload can have an adverse effect on the use of speech recognition systems. In this paper, the relationship between workload and speech is discussed, and two studies are reported. In the first study, time-stress is considered. In the second study, dual-task performance is considered. Both studies show workload to significantly reduce recognition accuracy and user performance. The nature of the impairment is shown to differ between individuals and types of workload. Furthermore, it appears that workload affects the selection of words to use, the articulation of the words and the relationship between speaking to ASR and performing other tasks. It is proposed that speaking to ASR is, in itself, demanding and that as workload increases so the ability to perform the task within the limits required by ASR suffers.
The Synonyma of Isidore of Seville, which were written in synonymic style but whose content is moral, could be used both as a grammar book and as a moral book. However it is this second reading which clearly dominated in the middles ages, as it is proven by the study of the paratext and the manuscript context, of the medieval inventories, of the centos or of the literary posterity.
This publication presents two new approaches of design microstrip antennas array. First is based on the technique of the genetic algorithms inspired from the processes of the evolution of the species and the natural genetics and the second based on the analogy between the resolution of the combinative problems of optimization and the annealing of the solids. These two approaches permits to seek simultaneous the law of optimal feed and the space distribution of the radiant elements so that the radiation pattern is as close as possible to an optimal desired diagram specified from a function or a pattern shape.
The registration of 3-D objects is an important problem in computer vision and especially in medical imaging. It arises when data acquired by different sensors and/or at different times have to be fused. Under the basic assumption that the objects to be registered are rigid, the problem is to recover the six parameters of a rigid transformation. This paper presents a novel iterative method designed for the rigid registration of 3-D objects. Its originality lies in its physical basis: instead of minimizing an energy function with respect to the parameters of the rigid transformation (the classical approach) the minimization is achieved by studying the motion of a rigid object in a potential field. In particular, we consider the kinetic energy of the solid during the registration process, which allows it to “jump over” some local maxima of the potential energy and so avoid some local minima of that energy. In that particular application, we perform the matching process with the whole segmented volumes.
Different modes of prescription are detectable in the lists of terms recommended within the framework of the official terminology process in France, as it has been operating for more than two decades, following the Juppé decree in 1996. Several dynamics are observed in these recommendations, which contain valuable remarks and numerous stylistic usage labels. It attempts to consider these elements of official prescription as within the framework of a combined meta-lexicographic and terminological analysis.
This paper presents first the semantic web, which aims at developing a knowledge level, independently from its linguistic realization. We show that this project is illusory and that a link must be established between formalized knowledge and texts available on the web. The paper presents different approaches to establish this link and deal with the relationship between general knowledge sources and knowledge dynamically acquired from texts.
The French and Vietnamese languages came into contact while Vietnam was under French occupation leading to the phonological assimilation of a large number of French loanwords into the Vietnamese language. The borrowings thus had to adjust to the Vietnamese tonal system. Scientific literature on this issue (as well as on French-Vietnamese linguistic contacts) has been sparse, and is based on a restricted number of borrowed forms. Using a phonological and statistical analysis of 600 Vietnamese words of French origin, the current study tackles the attribution of tones to previously toneless syllables. It counts the occurrences of each tone and reveals some laws determining the tonal integration of the French borrowings.
The study of these songs is based on the classification of these sound units, especially to extract the song theme of the singers in a specific area during a specific season. Recently, some approaches are proposed for automatic classification of these sound units. This paper introduces the sparse coding as a robust unsupervised classifier to generate efficient time-frequency representation of the calls of the whale. Secondly, the subunit shows to be interesting to analyze the evolution ofthe humpback whale songs during two years. It is statistically shown that the shortest units are the most stable (occurring with similar time frequency shape across the two years), while the longest units are evolving from one year to one other.
The pitch filter in a low bit-rate CELP speech coder has a strong impact on the quality of the reconstructed speech. In this paper we propose a pseudo-multi-tap pitch filter with fewer degrees of freedom than the number of prediction coefficients, but which gives a higher pitch prediction gain and a more appropriate frequency response than a conventional one-tap pitch filter. The prediction gain of the pseudo-multi-tap pitch filter is compared to that of conventional one-tap and three-tap pitch filters with integer and non-integer pitch lags. This relaxed test gives better results than a strict stability test. Finally, we have incorporated the pseudo-multi-tap pitch filter into a 4.8 kbit/s CELP speech coder. Both the objective SNR and subjective quality are better than for a conventional one-tap pitch filter.
The Stochastically Excited Linear Prediction (SELP) algorithm for speech coding offers good performance at bit rates as low as 4.8 kbit/s. Linear Predictive Coding (LPC) techniques remove the short-term correlation from the speech. A pitch loop removes long-term correlation, producing a noise-like residual, which is vector quantized. Information describing the LPC filter coefficients, the long-term predictor, and the vector quantization is transmitted. In this paper, we describe improvements to the SELP algorithm which result in better speech quality and higher computational efficiency. In its closed-loop form, the pitch loop can be interpreted as a vector quantization of the desired excitation signal with an adaptive codebook populated by previous excitation sequences. To better model the non-stationarity of speech we extend this adaptive codebook with a special set of candidate vectors which are transform of other codebook entries. The second stage vector quantization is performed using a fixed stochastic codebook. In its original form, the SELP algorithm requires excessive computational effort. We employ a new recursive algorithm which performs a very fast search through the adaptive codebook. In this method, we modify the error criterion, and exploit the resulting symmetries. The same fast vector quantization procedure is applied to the stochastic codebook.
The paper presents a survey of recent finding from the authors, and their collaborators, related to the phase transition problem and relational learning. The most compelling resuit is that commonly used heuristics such as the information gain are inevitably entrapped in the phase transition region. Then, it is unlikely to expect relational learning scaling up to problems larger than the ones solved so far.
The distance measure is of great importance in both the design and coding stage of a vector quantizer. Due to its complexity, however, the spectral distance which best correlates with the perceptual quality is seldom used. Since they are in general mathematically more tractable, these weighted squared Euclidean distance measures are more commonly used. Significant differences can be found in the performance of different distance measures suggested in previous literatures. In this paper, a complete study and comparison of weighted squared Euclidean distance measures is given. This paper also proposes a new weighted squared Euclidean distance measure for vector quantization of Line Spectrum Pairs (LSP) or Cosine of LSP (CLSP) parameters.
Althougn systems for the transcription of speech prosody have existed for a long time, the need to represent prosodic information in large-scale speech databases places new demands on such systems. The ToBI system recently developed in the USA differs in many interesting ways from conventional systems such as the “Standard British” system that has been in use for several decades. This paper discusses the differences in the context of current research on a machine-readable corpus of spoken English, and examines the possibility of converting automatically between the two types of transcription.
Two excised human larynges were used to investigate the effects of changes in supraglottal acoustics on phonation, especially fundamental frequency (F 0). An artificial supraglottal tube was connected to the larynx. Two different sized cylindrical blocks were inserted and moved in the tube to simulate the acoustics of neutral, front and back vowels. The F 0 changes were greatest at the instant when the blocks were inserted in the tube, i.e. when the tube was converted from open to blocked. F 0 changes connected with the use of the larger block did not show any systematical variation. In many cases an F 0 drop was measured. The results are interpreted in terms of an acoustic-mechanical feedback in vocal source-tract interaction. It is concluded that the so-called intrinsic F 0 of vowels cannot be explained on an acoustical basis. The acoustical conditions for the small block were simulated using a theoretical model.
A robust new algorithm for accurate endpointing of speech signals is described in this paper after an overview of the literature. This algorithm uses simple measures based on energy and zero-crossing rate for speech/silence detection. Instead of the usual two-state model, three states including a transitory phase are assumed. The zero-crossing rate measure is used in a special manner in the transitory state to improve the endpointing accuracy. The algorithm is based on classification in context and uses some knowledge-based heuristics for correction of false detections. The approach here is the one used in the visual detection of a waveform embedded in noise. No a priori knowledge of noise is needed and the algorithm is capable of producing good results even in cases where the signal starts with mouth noise. One important feature of this algorithm is its ease of implementation for real-time processing. The algorithm is also adaptive and can cope with varying signal to background noise ratios. The algorithm was originally developed on a database collected in an acoustic chamber. Modifications for its application to telephony speech as well as results of a preliminary test are included.
An investigation of the pharyngealization by analysis/synthesis is realized as a contribution to the development of a synthesis-by-rule system for Arabic. The present paper describes the acoustic parametrization of the Arabic pharyngealized consonants / /, / /, / /, / / using a formant synthesizer. Acoustical studies showed that the usual description of the Arabic vowel system as a set of 3 short vowels and their 3 long counterparts is inadequate. A system of 12 vowels including 6 pharyngealized ones is defined according to phonetic and phonological facts. The rules dealing with the extension of pharyngealization in connected speech are established and validated by synthesis.
We begin by considering some recent studies of institutional history, which seem to give us a better understanding of the hidden face of the elaboration of the norm in France, namely the exclusion of the Parisian Parlement as a model of good usage. In the second half of the article, we address a thorny issue: that of the influence of normative statements on usage, focusing on three case studies from the fields of noun and verb morphology and the lexicon.
The main advances of the R&D Sample Orchestrator project are presented, aiming at the development of innovative functions for the manipulation of sound samples. These features rely on studies on sound description, i.e. the formalization of relevant data structures for characterizing the sounds ' content and organization. This work was applied to automatic sound indexing and to the development of new applications for musical creation - interactive corpus-based synthesis and computer-aided orchestration. The project also included an important part on high-quality sound processing, through several enhancements of the phase vocoder model – processing by sinusoidal model in the spectral domain and automatic computation of the analysis parameters.
The Higher Pole Correction (HPC) function in analog and digital all-pole modelling of speech production is analyzed by comparing all-pole models with a Transmission Line (TL) model. The validity of the TL model, which was chosen as a computational reference system in the study, is tested by comparing its transfer functions to acoustical measurements made on a physical vocal tract model. The variation of effective length of the vocal tract turned out to be an important parameter in modelling the HPC. Even if the frequency responses of the HPC in analog and digital cases differ, the relative changes in the correction, influenced by the variations in the effective length of the vocal tract, are exactly the same in both cases. The work results in all-zero models, which can be used in analog as well as digital all-pole realizations to form a new type of pole-zero model for speech production. This new pole-zero model is related to the PARCAS terminal analog model [Laine, 1982].
In this paper, it is tried to apply neural networks for two kinds of problems concerning articulatory motion. They are estimation of articulatory motion from speech waves and generation of articulator movement for a sequence of phonemic symbols. In the former problem, since estimation of articulatory parameters is regarded as a nonlinear mapping between the acoustic parameters and the articulatory ones, a neural network is expected to be a suitable method. In the latter problem, a nonlinear control system that produces articulatory motion is successfully constructed combining neural networks.
The frame-by-frame variation of tongue profiles derived from X-ray film data is described in terms of the temporal patterns of four articulatory parameters. The temporal variation of each parameter, i.e., movement, is assumed to be the output of a time-invariant auto-regressive filter. Each filter is excited by a sequence of pulses, representing articulatory commands. The filter coefficients, and the position and amplitude of the pulses are determined by applying an MLPC method. The minimum number of pulses is determined by using an acoustic criterion. It depends on the number of the phonetic features, in the sentence, of which their realization is crucially related to their pertinent parameters.
The influence of pitch contour, segmental durations, and spectral features on the perception of two speaking styles was studied. For this purpose two male speakers each spoke “spontaneously” to an interviewer and afterwards read out their own literally transcribed spontaneous text. Pairs of identical spontaneous and read utterances were selected that were fluently spoken in both speaking styles (no false starts, hesitations, etc.). Five test conditions were constructed in which the utterances had: (1) no manipulations; (2) phoneme durations from the opposite speaking style; (3) the pitch contour from the opposite speaking style; (4) a monotonous pitch contour; (5) the original spectral features combined with the prosodic features of the opposite speaking style. The stimuli were presented to 32 subjects in a listening experiment. Their task was to classify each utterance as either “spontaneous” or “read out” speech. All manipulations of the test utterances had a significant effect on the classification of the speaking style. We also analysed the original utterances with respect to several acoustic measures for intonation, duration, jitter and shimmer, and spectral vowel quality. Overall, read speech compared to spontaneous speech had: a lower articulation rate, more F 0 variation, more F 0 declination, less shimmer, and less vowel reduction. However, none of these acoustic features by itself can clearly discriminate between the two speaking styles. Above all it became clear that the performance of the speakers and the listeners varied enormously.
This paper proposes an augmented chart data structure with an efficient word lattice parsing scheme in speech recognition. The augmented chart and the associated parsing algorithm can represent and very efficiently parse, without changing the fundamental principles of chart parsing, a lattice of lexically highly ambiguous word hypotheses in speech recognition. Every word lattice can be mapped to the augmented chart, with the ordering and link between the word hypotheses being well preserved in the augmented chart. A jump edge is defined in order to link edges representing word hypotheses physically separate, but connectable from a practical point of view. Preliminary experimental results show that with augmented chart parsing, all the possible constituents of the input word lattice can be constructed and no constituent needs to be built more than once. This significantly reduces computational complexity, especially when serious lexical ambiguity exists in the input word lattice as in the case of many speech recognition problems. This augmented chart parsing is thus very a useful and efficient approach to language processing problems in speech recognition.
Problems that arise when we are comparing the vibro-acoustic signatures of high-voltage apparatus in electrical substations have led to the discovery of a new time-warp algorithm. Such signature comparisons are made in order to monitor trends in the health of the equipment. However, the signatures collected contain a sequence of transients generated by electromechanical events which appear with a slightly different time frame from one switching operation to the other, depending on the temperature and load, among other factors. Despite being small, this time distortion results in a significant difference between the instantaneous amplitudes of the signatures. The magnitude of the time difference between the last signature and a reference has a diagnostic use. The proposed time warp algorithm makes it possible to identify the time relationship between the events of two signatures, even in the presence of discontinuities in the time frame. We show that these discontinuities are not treated appropriately by the dynamic time warping (DTW) algorithm currently employed in speech processing. The paper explains the way these two algorithms work and presents results that bear witness to the increased accuracy of the multi-scale correlation. In part, it is the interpolation of the warp trace that makes such accuracy possible. The warp trace is a function describing the time deviation between the signatures and even including the presence of inversions in the order of appearance of the transients.
A great number of application fields is concerned with multidimensional signal processing and the new possibilities of hardware allow the development of device operating to those signals. After a presentation of the studied domain, we give some elements of terminology and present the essential models of multidimensional signals. We present then a synthetic approach of the estimation (or measurement) methods used in the multidimensional signal characterization. We conclude in a presentation of the principal operator (filters) used in ND signal processing in connexion with 1 D signal operators.
Nowadays, neural networks are largely used in signal and image processing. We propose a new neuron model that uses a special coding for its output, which we will call «Scalar Distributed Representation» (SDR). This representation is based on the idea of representing the neuron's output by a function, and not only by a scalar. We show that SDR produces a non-linear behaviour of connections between neurons. The SDR is described in general and then adapted on practical considerations. We consider the use of SDR for a Multi-Layer Perceptror and we propose a learning algorithm. Finally, we validate the model on two applications: dimensionality reduction, and prediction. In both cases, an important benefit is obtained over the classical model.
The optimisation of a parallel distributed detection system with N sensors always leads to a set of 2 N + N non linear equations, which is only solved in particular cases, assuming statistical independence of the local observations and for systems constituted by very few sensors. Usually, the number of equations to solve increases very quickly with the number of sensors. The computations become unfeasible. In this paper, a selection procedure of sensors relevant for the decision process based on the use of Shannon's conditional entropy is developed. Then, these systems are optimized via en entropy based method. This one determines the local thresholds and constructs a decision tree which minimises the decision error probability. Due to the fact that the performances of distributed systems are lower than the centralised ones, the previous optimisation techniques can be applied in the distributed quantification problem taking into account a compromise between the information flow to broadcast to the fusion center, and the performances of the decision system.
A system has been developed that enables a wanted speech signal to be extracted from a background of unwanted speech and other interference under real-life conditions. Using only two microphones 25 cm apart it exploits directional and harmonicity cues in a hybrid algorithm which takes advantages of both. The output of the algorithm has been tested subjectively by human listeners, and objectively by a speech recognition system. These tests show improved intelligibility of the wanted speech signal. In one set of tests, for example, the performance of a speech recogniser after segregation was similar at a signal to noise ratio of 6 dB as without segregation at a signal to noise ratio of 20 dB.
Estimating a non-uniformly sampled function from a set of learning points is a classical regression problem. Kernel methods have been widely used in this context, but every problem leads to two major tasks: optimizing the kernel and setting the fitness-regularization compromise. This article presents a new method to estimate a function from noisy learning points in the context of RKHS (Reproducing Kernel Hilbert Space). We introduce the Kernel Basis Pursuit algorithm, which enables us to build a L-i-regularized-multiple-kernel estimator. The general idea is to decompose the function to learn on a sparse-optimal set of spanning functions. Our implementation relies on the Least Absolute Shrinkage and Selection Operator (LASSO) formulation and on the Least Angle Regression Stepwise (LARS) solver. The computation ofthe full regularization path, through the LARS, will enable us to propose new adaptive criteria to find an optimal fitness-regularization compromise.
This communication proposes a new Prony model with time-varying poles for modélisation of nonstationary signals. This new model is based upon a linear combination of time-varying exponentials. This method leads to an extension of several techniques of stationary spectral estimation to the nonstationary case: amplitude and phase are time-varying. We show and justify a method and the corresponding algorithm to estimate the complete new parameter vector. The determination of the time-varying parameters requires five steps: estimation of the time-varying AR parameters, estimation of the right poles of the linear time- varying system, modélisation of these poles, computation of the new poles, and least-square estimation of the amplitudes. To validate this model, a simulation signal composed of two chirps is chosen.
Many research works combine learning from demonstration and policy improvement methods to learn the controller of a robot along a specific trajectory. Nevertheless, a capability to learn in the whole reachable space of this robot is missing in these works. In this paper we propose a method that consists in learning a reactive near-optimal feedback controller in two steps. Second, the feedback controller is optimized further with direct Policy Search methods. As a result, we obtain a controller that is executed 20 000 times faster than the original controller for a similar performance. Our work is evaluated in simulation.
This paper presents the Dial-Your-Disc (dyd) system, an interactive system that supports browsing through a large database of musical information and generates a spoken monologue once a musical composition has been selected. The paper focuses on the generation of spoken monologues and, more specifically, on the various ways in which the generation of an utterance at a given point in the monologue requires modeling of the linguistic context of the utterance.
The existing texture classification methods are generally based on a parameter extraction stage followed by a classifier stage. Using this kind of method for an operational application requires to take into account the risk of classes mixture in the parameters space. We show that a connectionnist classifier is able to deal efficiently with these parameters.
A system for the automatic recognition of sonorant consonants extracted from continuous speech is described. The system is based on fuzzy rules. After performing a precategorial classification in which feature extraction is carried out by modules organized in a hierarchy of levels, the speech message is segmented in pseudo-syllabic nuclei and hypotheses about vowels and consonants are emitted. The rules which improve the classification of Italian liquids and nasals in most contexts were inferred after experiment, and account for coarticulation effects. Results obtained for four male and two female speakers are presented together with an acoustic-phonetic motivation of the approach used. These results show that this method gives substantially better performances than previous approaches.
Models of word recognition differ with respect to where the effects of sentential-semantic context are to be located. Using a crossmodal priming technique, this research investigated the availability of lexical entries as a function of stimulus information and contextual constraint. To investigate the exact locus of the effects of sentential contexts, probes that were associatively related to contextually appropriate and inappropriate words were presented at various positions before and concurrent with the spoken word. The results show that sentential contexts do not preselect a set of contextually appropriate words before any sensory information about the spoken word is available. Moreover, during lexical access, defined here as the initial contact with lexical entries and their semantic and syntactic properties, both contextually appropriate and inappropriate words are activated. Contextually effects are located after lexical access, at a point in time during word processing where the sensory input by itself is still insufficiently informative to disambiguate between the activated entries. This suggests that sentential-semantic contexts have their effects during the process of selecting one of the activated candidates for recognition.
Malicious programs, such as viruses and worms, are frequently related to previous programs through evolutionary relationships. Discovering those relationships and constructing a phylogeny model is expected to be helpful for analyzing new malware and for establishing a principled naming scheme. Matching permutations of code may help build better models in cases where malware evolution does not keep things in the same order. We describe methods for constructing phylogeny models that uses features called n-perms to match possibly permuted codes. An experiment was performed to compare the relative effectiveness of vector similarity measures using n-perms and n-grams when comparing permuted variants of programs. The similarity measures using n-perms maintained a greater separation between the similarity scores of permuted families of specimens versus unrelated specimens.
We present a computational model that generates listening behaviour for a virtual agent. It triggers backchannel signals according to the user's visual and acoustic behaviour. The choice of the type and the frequency of the backchannels to be displayed is performed considering the agent's personality traits. We link agents with a higher level of extroversion to a higher tendency to perform more backchannels than introverted ones, while linking neuroticism to less mimicry production and more response and reactive signals sent. We run a perceptive study to test these relations in agent-user interactions, as evaluated by third parties.
A large-scale Japanese speech database has been described. The database basically consists of (1) a word speech database, (2) a continuous speech database, (3) a database for a large number of speakers, and (4) a database for speech synthesis. Multiple transcriptions have been made in five different layers from simple phonemic descriptions to fine acoustic-phonetic transcriptions. The database has been used to develop algorithms in speech recognition and synthesis studies and to find acoustic, phonetic and linguistic evidence that will serve as basic data for speech technologies.
The aim of this paper is to call attention to the role played by prosodic structure in continuous word recognition. First we argue that the written language notion of the word has had too much impact on models of spoken word recognition. Next we discuss various characteristics of prosodic structure that bear on processing issues. Then we present a view of continuous word recognition which takes into account the alternating pattern of weak and strong syllables in the speech stream. A lexical search is conducted with the stressed syllables while the weak syllables are identified through a pattern-recognition-like analysis and the use of phonotactic and morphonemic rules. We end by discussing the content word vs. function word access controversy in the light of our view.
Back-propagation has been used to train a small network for the prediction of syllable-level duration in a text-to-speech system. Both input and output are in the form of analog values, and the net performs a multiple regression function.
Recent studies have shown that the sub-band based speech recognition approach has the potential of improving upon the conventional, full-band based model against frequency-selective noise. A critical issue towards exploiting this potential is the choice of the method for combining the sub-band observations. This paper introduces a new method, namely, the probabilistic-union model, for this combination. The new model is based on the probability theory for the union of random events, and represents a new method for modeling partially corrupted observations given little knowledge about the corruption. The new model has been incorporated into a hidden Markov model (HMM) and tested for recognizing a speaker-independent E-set, corrupted by various types of additive noise. The results show that the new model offers robustness to partial frequency corruption, requiring little or no knowledge about the noise statistics.
We present a new algorithm for planar (XA) and tomographic (MRA) images registration. The 2D/3D registration is defined as determining the optimal rigid transformation which enables to register the entire dataset coming from both modalities in a common three-dimensional referential. Interests of the described method can be observed through two points. Firstly, the use of an anatomical based registration offers the possibility of less constraining exam, more suitable for patients. Then, after an anatomical referential has been interactively selected, registration procedure may be steered independently since the initialization step is automatic. The tomographic dataset is used to construct a three-dimensional structure, which defined the common referential for the two modalities. Then, the optimal structure position is obtained in the planar imaging reference through a multi-scale analysis and optimization procedures. Finally, since the position of the anatomical structure is known in both MRA and planar imaging referential, it is possible to obtain a three-dimensional matching under condition of having at least two incidences for the planar imaging.
Low structure, scattered information, strongly symbolic representation are some of the characteristics that define this textual document type. Then, supervised learning techniques are used in order to produce targeting models from this reduced space. Interesting results (86% of recall and 88% of precision) are obtained by induction trees and discriminant analysis. Even if the validation results (55% of recall and 60% of precision) could be better, this approach offers interesting perspectives, based on content, for the automatic exploitation of CV.
This paper proposes two methods for creating a phoneme- and speaker-independent model that greatly reduce the amount of calculation needed for similarity (or likelihood) normalization in speaker verification. For each input speech, these methods only need to calculate the likelihood against a single model instead of against the models of all the reference speakers as in the conventional methods. In addition, the new methods perform better than or equally as well as the conventional methods. Speaker verification is tested by using separate populations of customers and impostors in order to evaluate performance under practical conditions. The speaker (and text) verification error rates are roughly 1.5 times higher than when the same population is used for both customers and impostors. Using 15 customers and a separate group of 15 impostors, a speaker verification error rate of 1.8% for text-independent verification and a speaker-and-text verification error rate of 1.1% for text-prompted verification were obtained after normalization. The latter error rate was about half of that achieved by the original method.
In this paper, we review the current state of the art in stochastic modeling for spoken dialogue system design. We discuss acoustic modeling of speech units for automatic speech recognition and language modeling of linguistic units for natural language processing. We point out some of the emerging stochastic modeling techniques and show the similarity between language modeling and acoustic modeling. Finally, we address search and decision issues related to the integration of knowledge sources for automatic speech recognition and natural language processing.
This article is devoted to the problem of the explanation of the result given by a decision tree (DT) when it is used as a decision aid system, to classify input data and to provide the output class as a result. At now the end-user can rely mainly on some estimation of the error-rate or on the trace of the classification, that is the path run through the DT. We propose here two new methods to qualify the result given by the DT for each particular case. These methods are based on a geometric study of the inverse image of the different classes (the decision surface). First we identify the tests in the tree that are the most relevant to explain the final class, by the way of sensitivity analysis: the vector of initial data is projected onto the decision surface.
In these days of multimodal systems and interfaces, many research teams are investigating the purposes for which novel combinations of modalities can be used. Based on the study of particular applications, empirical investigations of speech functionality address points in a vast multi-dimensional design space. At best, solid findings yield low-level generalisations which can be used by designers developing almost identical applications. Furthermore, the conceptual and theoretical apparatus needed to describe these findings in a principled way is largely missing. This paper argues that a shift in perspective can help address issues of modality choice both scientifically and in design practice. Instead of empirically focusing on fragments of the virtually infinite combinatorics of tasks, environments, performance parameters, user groups, cognitive properties, etc., the problem of modality functionality is addressed as a problem of choosing between modalities which have very different properties with respect to the representation and exchange of information between user and system. Based on a study of 120 claims on speech functionality from the literature, it is shown that a small set of modality properties are surprisingly powerful in justifying, supporting and correcting the claims set. The paper analyses why modality properties can be used for these purposes and argues that their power could be made available to systems and interface designers who have to make modality choices during early design of speech-related systems and interfaces. Using hypertext, it is illustrated how this power may be harnessed for the purpose of predictively supporting speech modality choice during early systems and interface design.
Previous studies have shown that the discriminability of a pair of stop consonants is determined by the particular consonant pair and by its position in a CVC syllable. The hypothesis that these differences in discriminability are related to differences in the auditory representations of the stimuli, following frequency analysis by the peripheral auditory system, was tested in two experiments. Second, the auditory representation of each consonant in the syllable-initial and syllable-final position was measured by means of a simultaneous-masking technique. Two cues to a consonant's identity are (a) the instantaneous spectrum at the onset (syllable initial)_or offset (syllable final) of the formant transition, and (b) the formant transition pattern. The discriminability of six pairs of consonants (three initial and three final) was correlated with both the difference between their masking patterns at onset or offset (ϱ −0.89) and with the difference between their auditory transition patterns (ϱ= 0.94). The strength of these correlations indicates that the variations in consonant discriminability which have been reported may be predictable from a knowledge of their auditory representations at a peripheral level, with relatively little contribution from phonetic, linguistic, or cognitive factors.
The proposed framework combines machine learning techniques and Riemannian geometry-based shape analysis. We represent facial surfaces by collections of radial curves and iso-level curves, the shapes of corresponding curves are compared using a Riemmannian framework. We select the most discriminative curves using the well known AdaBoost algorithm. The experiment involving FRGC v2 dataset demonstrates the effectiveness of this feature selection by achieving 98.02 % as rank-1 recognition rate.
A classification of different methods used for the assessment of TTS (Text-To-Speech) systems, according to the demands placed on the listener, is proposed and discussed. The classification is made according to the four traditional scale levels: the Nominal, Ordinal, Interval and Ratio level. A fifth level, the Supra-Nominal, including memory processes, is proposed. The methods are divided into qualitative, non-metric methods and quantitative, metric methods. The outcome is that the highest metric assessment level (Ratio) is not necessarily the level that places the highest demands on the listener. Quite to the contrary, the Nominal level, supporting a non-metric qualitative approach, places even higher demands on the listener.
This article is concerned with the fusion of stacked images, in order to restore the 2D image of an object. To perform this fusion, we have to estimate the transfer function between the sensitivity functions of the films on which these images have been taken. In our context, this estimation is difficult because these images are degraded by widespread stains. We show that it is possible to estimate both the stains and the transfer function to implement the restoration.
There are many situations where non-real-time speech enhancement is required. For such applications, employing any available a priori knowledge can lead to more effective enhancement solutions. In this study, a novel text-directed speech enhancement algorithm is developed for usage in non-real-time applications. In our approach, the text of the intended dialogue is used to partition noisy speech into regions of broad phoneme classifications. Classes considered include stops, fricatives, affricates, nasals, vowels, semivowels, diphthongs and silence. These partitions are then used to direct a new vector quantizer based enhancement scheme in which phone-class directed constraints are applied to improve speech quality. The proposed algorithm is evaluated using both objective as well as subjective quality assessment techniques. It is shown that the text-directed approach improves the quality of the degraded speech over a broad range of noise sources (i.e., flat communications channel noise, aircraft cockpit noise, helicopter fly-by noise, and automobile highway noise) and over a broad range of signal-to-noise ratios (i.e., 10, 5, 0 and −5 dB). In each case, the proposed method is shown consistently to exhibit improved objective quality over linear and generalized spectral subtraction, as well as the Auto-LSP constrained iterative enhancement method using the Itakura-Saito measure and a 100-sentence evaluation speech corpus. Subjective quality assessment was conducted in the form of an A-B comparison test. Results of these evaluations demonstrate that, for wideband noise distortions, the proposed algorithm is preferred over the unprocessed noisy speech more than 2 to 1, while the proposed algorithm is preferred over spectral subtraction by more than 3 to 1.
In probabilistic speech recognition it is often interesting to evaluate the contribution of the language model and that of the acoustic model. We propose an information theoretical approach which takes into account the interaction between the two sources of information. Experimental results are presented concerning the IBM prototype real-time recognizer of the Italian language based on a 20,000-word vocabulary.
In the framework of a phonosyntax model [1–4], the prosodic structure of the sentence is encoded by melodic contours located on stressed vowels. These contours are described by the phonological features [±Extreme], [±Rising], and [±Ample]. Acoustic analyses of French read sentences show that, except for the final contour, intensity and duration do not play a role in differentiating the contours, which contrast effectively by their rising or falling slope and by the relative amplitude of F 0 variation.
This paper proposes a local, cooperative and real-time multi-agent approach to build adaptive and incremental profiles where a user is supposed to be represented by a set of textual documents. These documents are sequentially parsed, which leads to the construction of a temporary terminological network (TTN). Preliminary results of the built system as well as perspectives are then presented.
Dutch consonants, spoken in lists of two-syllable nonsense words of the type CVCVC which were embedded in short carrier phrases, were identified by listeners under various acoustic disturbance conditions. The 28 conditions were a mixture of four reverberation times, five signal-to-noise rations, and five different noise spectra. The identification results were summated over the six talkers and five listeners. In this way we achieved 28 confusion matrices per constant position (initial, medial, and final). These sets of matrices were processed by individual differences multidimensional scaling programs, and more specifically by TUCKALS (Kroonenberg and de Leeuw, [9]). The resulting three-dimensional stimulus configuration for the initial consonants is very stable and can be represented as a tetrahedron with /z, s/, /m, n/, /p, t, k, b, d/, and /f, v, χ/ at the four corner points and /l, r, w, j, h/ in the centre. This consonant configuration is discussed with respect to its relevance to the Dutch language given the experimental conditions. The representation of the 28 conditions turns out to be almost exclusively one-dimensional despite the three different aspects (reverberation time, noise level, noise spectrum) of the acoustic disturbances.
Two experiments are reported on the perception of the distinction between /ba/ and /pa/ which investigate perceptual cues to the onset of voicing. Neither the onset of periodic excitation nor a change in spectral balance appear to be the dominant cue for voicing onset. In both experiments the overall intensity level provided the best metric for explaining the position of subjects' boundaries.
The second half of the seventeenth century is often depicted, due to the intense prescriptive activity, as a time when the arguments made were accepted and followed by effect. In this paper, we focus on the possible limits of this activity. After some terminological clarifications on what may be called norms and prescriptions, we study the foundations of “anti-prescriptivism” in the seventeenth century, especially in “remarks on language”, and we show how this current of resistance to prescriptions was taken up by the worldly culture of the end of the century. In total, we consider that, far from being considered only as a period of “adjustment”, the second half of the seventeenth century, on the contrary, can be seen as a conflictual and polemical moment, characterized by an alteration of the traditional normativity of grammar, and by the establishment of social opposition that made it difficult to establish a single standard supported by a clear prescription.
Situations, the semantic interpretations of context, provide a better basis for selecting adaptive behaviours than context itself. The definition of situations typically rests on the abilityto define logical expressions and inference methods to identify particular situations. In this paper we extend this approach to provide for efficient organisation and selection in systems with large numbers of situations having structured relationships to each other. We apply lattice theory to define a specialisation relationship across situations, and show how this can be used to improve the identification of situations using lattice operators and uncertain reasoning. We demonstrate the technique against a real-world dataset.
This essay is an attempt to define the features and the value of authors' manuscripts in Middle French through the archeological and philological study of the original manuscripts of four texts by Chistine de Pizan (Cent Ballades, Debat de deux amans, Mutacion de Fortune, Advision). Three stages in the publishing process are considered (determining the model; copying from a fair copy; revising the transcription) in order to underline the fact that an autograph transcription, certainly privileged, is less important than the initial authorization of the transcription from the exemplar, written and rewritten by the author's hand, and the final act of the manuscript's «visite», which is visible in the text corrections, the authorial modifications and the customization of certain components of the codex. Finally, the article examines what's at stake from a methodological and editorial point of view in such a study of autograph publications (author's intention, choice of the base manuscript and relevance of an esthetical and genetic research).
The goal of this paper is to present an embedded calculator for image processing SYMPHONIE and the methodology used for its realization. For this application, low size and low consumption will be very important and an ASIC of a million gates has been developped. To succeed in this realization we used a VHDL model allowing simulations on the full system. Then we used VHDL synthesis methods for the conception of the ASIC. We will conclude this paper by a presentation of some perfomances of the system in some applications fields.
Most theories of language acquisition implicitly assume that the language learner is able to arrive at a segmentation of speech into clausal units. The present studies employed a preference procedure to examine the sensitivity of 7-10-month-old infants to acoustic correlates of clausal units in English. The infants oriented longer to the samples segmented at the clause boundary. A second experiment confirmed that these preferences depended on where the pauses were inserted in the samples. These findings have important implications for understanding how language is learnable. The prelinguistic infant apparently possesses the means to detect important units such as clauses, within which grammatical rules apply.
This is an attempt to formulate an outline of the properties of the human voice source in connected speech. Six aspects of the production process are considered: (1) reference data for a particular speaker; (2) segment specific values and source-tract interactions; (3) coarticulation of glottal gestures and interpolation at boundaries; (4) basic F 0 dependencies; (5) the influence of stress, accents and voice intensity; (6) the phrasal contour of source variations. The parameterization of source data is based on the transformed LF-model and frequency domain correspondences (Fant, 1995) which allows for a maximal specificational power with a limited number of parameters.
A self-learning speaker and channel adaptation technique based on the separation of speech spectral variation sources is developed for improving speaker-independent continuous speech recognition. Statistical methods are formulated to remove spectral biases at the acoustic level and to adapt parameters of Gaussian mixture densities at the phone unit level. The spectral bias is estimated in two steps using unsupervised maximum likelihood estimation: the probability distributions of the speech spectral features are first assumed uniform for severely mismatched channels, and the spectral bias is then reestimated using Gaussian phone models. The task vocabulary size was 853; the grammar perplexity was 105; the test speech data were collected under mismatched recording conditions with each test set containing 198 sentences. Depending on the speakers and channel conditions, the two-step spectral bias removal yielded relative error reductions (RER) of 3% to 11% compared to the conventional cepstral mean removal; the USPA yielded RER of 12% to 26% after the two-step bias removal; the IPA further yielded RER of 8% to 19% after the USPA.
The structure of a synthesis system is described that uses prominence as a central parameter. A definition of prominence suitable for this application is given. For the empirical foundation the reliability of prominence ratings by human listeners is assessed. These ratings were compared with acoustic data on F 0 and duration. A linear relationship between ratings and parameter values was found. Two algorithms to transform prominence values to prosodic parameters are briefly described and evaluated. The application of prominence to the synthesis of focal accents is demonstrated. The results indicate the validity of the prominence based approach as an interface between linguistics and acoustics.
Multi-agent based modelling is about considering that the global behavior of a system comes from interactions which take place between micro level entities. Consequently, how the micro and macro levels dynamics are linked to each other is a crucial issue. In this paper, we focus on how the Ferber and Müller's Influence/ Reaction model can help in considering such an issue. We show that it represents an interesting solution and we then propose an adaptation of this model which is more suited to simulation: the IRM4S model (an Influence Reaction Model for Simulation). This model clarifies the Influence/Reaction principle and highlights its capacity to actually join the micro level modeling concerns with those of the macro level.
Experts of roads and public works have been interested for a long time in the bumps on the road ways. Such road defects with regard to a flat surface, are called the roughness of the road. The longitudinal profile analysor (LPA) was made by the «Laboratoire Central des Ponts et Chaussées» in Nantes, in order to measure the roughness. The signal given by this plant can be considered as the output of a linear system whose input is the unknown longitudinal section of the road. We present in this paper two methods for solving this problem: the first one is determinist and uses a back filtering by the transfer function of the LPA. The second one is stochastic and uses Kalman filtering. At first, we modelize the LPA by a fifth order transfert function built with a description of its différents mechanic and electronic components and by an experimental frequency analysis. Then the double filtering technic eliminates the phase distortions of the LPA signal, so we obtain a pseudo profile reproducing the exact profile with an attenuation for frequencies outside the analysor band pass. The second method uses a LPA model obtained by parametric identification (maximum likelihood method) and a model of the profile type Wiener signal. After eliminating polynomial components and low frequencies, the reconstructed signal follows accurately variations of the roughness road. Results obtained from measurements made on a test bed and an experimental way are presented.
In this paper, we generalize relations between clean and noisy speech signal using vector Taylor series (VTS) expansion for noise-robust speech recognition. We use it for both the noisy data compensation and hidden Markov model (HMM) parameter adaptation, and apply it for the cepstral domain directly, while Moreno used it to estimate the log-spectral parameters. Also, we develop a detailed procedure to estimate environmental variables in the cepstral domain using the expectation and maximization (EM) algorithms based on the maximum likelihood (ML) sense. To evaluate the developed method, we conduct speaker-independent isolated word and continuous speech recognition experiments. White Gaussian and driving car noises added to clean speech at various SNR are used as disturbing sources. Using only noise statistics obtained from three frames of silence and noisy speech to be recognized, we achieve significant performance improvement. Especially, HMM parameter adaptation with VTS is more effective than the parallel model combination (PMC) based on the log-normal assumption.
A new hybrid higher-order cepstrum (HOC) and functional link network (FLN)-based blind equaliser (HOCFLN) is presented. The system initially uses the complex cepstrum of the 1-D slice of the fourth-order cumulants of the unknown received signal to partially estimate the equaliser coefficients, then it switches to an FLN adaptive equaliser operating in the decision directed mode (DDM) to further improve the mean-squared error (MSE) convergence. In this system two nonlinearities are used; one on the input data where the HOC is used and the other one in the FLN equaliser filter. It is shown that in the new HOCFLN system multiple nonlinearities give significant performance improvement with less computational complexity, particularly in severe channel distortion compared to the conventional equalisation algorithms. This method can accommodate both nonminimum phase MA and ARMA channels. Performance results for channels exhibiting abrupt characteristic changes are also shown.
The description of 3D objects independently of their position and orientation, is an important and difficult problem in pattern analysis. In this paper, we deal with this problem by a pseudo-Fourier transform on the group of motions of the 3D Euclidean space, which we denote by M(3). This transform allows us to define 3D gray-levels object descriptors which are invariant and stable with respect to M(3). This method is applied to human bones automatic classification and description.
It is attempted to reduce the phonetic quality of vowels to the positions of the peaks in their tonotopical spectra relative to the other peaks, simultaneous or preceding in context. Synthetic two-formant vowels were identified by speakers of languages that differentiate richly among high vowels (Swedish, Turkish). The parameters F 1 (204–801 Hz) and F′2 (509–3702 Hz) were systematicallyvaried in steps of 0.75 critical bandwidths. f0 was kept close below F 1 in all vowels. These were presented in two orders with subsequently rising or falling F′2. Most subjects heard predominantly close vowels. It is speculated that this second reference point might represent a default position of the third formant or the like.
In order to study methods of detecting a lack of vocal efficiency due to pathological changes of the larynx and of quantifying this lack at the acoustic level, we defined an acoustic parameter related to the efficiency of phonation and tested its performance in terms of its ability to discriminate between normal and dysphonic speakers. We extracted a so-called vocal efficiency feature (VEF) from the autocorrelation function of the steady portion of the isolated French vowel /a/. During the first stage we established by simulation the relationship between the VEF and the shape and duty cycle of the glottal waveform. The simulation showed that the lower the VEF values were, the smaller was the glottal closure quotient and the more symmetric the waveform. The proposed VE measure permitted discrimination between normal and dysphonic speakers. The relevance of the resulting classification was illustrated by its interpretation in the framework of a theoretical model [20].
Research is reviewed that addresses itself to human language learning by developing precise, mechanistic models that are capable in principle of acquiring languages on the basis of exposure to linguistic data. Such research includes theorems on language learnability from mathematical linguistics, computer models of language acquisition from cognitive simulation and artificial intelligence, and models of transformational grammar acquisition from theoretical linguistics. It is argued that such research bears strongly on major issues in developmental psycholinguistics, in particular, nativism and empiricism, the role of semantics and pragmatics in language learning, cognitive development, and the importance of the simplified speech addressed to children.
This article develops the theory of misunderstandings by applying it to the analysis of the conversational organisation of a misunderstanding as it appears in a sequence from a tutorial dialogue. Interlocutionary analysis (Trognon and Brassac, 1992) of the sequence containing the misunderstanding reveals that the conversational organisation of misunderstanding can take complex forms. In the literature, the conversational organisation of misunderstanding rests on a structure with three elements: T1, the content of the turn bearing the misunderstanding, T2, the content of the turn revealing the misunderstanding, and T3, the content of the turn resolving the misunderstanding. In the sequence analyzed, these three moments (T1, T2, and T3) do not follow each other in direct succession but are built into a complex hierarchical structure. These three moments, more particularly T2 and T3, are reproduced several times in a row. Furthermore, our sequence is unique as it illustrates a resolution of the misunderstanding without involving intersubjectivity.
The use of three types of vocabularies (cockpit-control words, digits, and initial consonants) was compared for the assessment of five speech recognizers. The goal of this study is to compare various assessment methods from application oriented to carefully controlled laboratory situations. It was found that the discrimination between various recognizer (input) conditions is improved for more difficult vocabularies. Confusions between stimuli and responses of testwords can be used as a diagnostic tool for prediction of performance and developments.
In this paper we report our experience at LIMSI-CNRS in developing and porting a stochastic component for natural language understanding to different tasks and human languages. The domains in which we test this component are the American ATIS (Air Travel Information Services) and the French MASK (Multimodal-Multimedia Automated Service Kiosk) applications. The study demonstrates that for limited applications, a stochastic method outperforms a well-tuned rule-based component. In addition we show that the human effort can be limited to the task of data labeling, which is much simpler than the design, maintenance and extension of the grammar rules. Since a stochastic method automatically learns the semantic formalism through an analysis of these data, it is comparatively flexible and robust.
The problems that participants in conversation have, it is argued, are really joint problems and have to be managed jointly. The participants have three types of strategies for managing them. (1) They try to prevent foreseeable but avoidable problems. (2) They warn partners of foreseeable but unavoidable problems. And (3) they repair problems that have already arisen. Speakers and addressees coordinate actions at three levels of talk: (1) the speaker's articulation and the addressees' attention to that articulation; (2) the speaker's presentation of an utterance and the addressees' identification of that utterance; and (3) the speaker's meaning and the addressees' understanding of that meaning. There is evidence that the participants have joint strategies for preventing, warning about and repairing problems at each of these levels. There is also evidence that they prefer preventatives to warnings, and warnings to repairs, all other things being equal.
Our research thematic deals with the representation quality and the outlier detection in supervised learning. The prediction of a continuons value is referred to as regression learning. In this case, once constructed the neighbourhood graph resulting from the predictors, we proposed in a recent work to evaluate the representation quality using neighbourhood autocorrelation coefficient, like Moran spatial coefficient. Extending the analogy with spatial analysis, we suggest in this paper to detect abnormal values of the variable to predictfrom local components of global neighbourhood autocorrélation coefficient and Moran scatterplot. Experimentation led on varions bases from UCI Machine Learning repository has given interesting results.
This paper aims at giving an overview of recent advances in the domain of Speech Recognition. The paper mainly focuses on Speech Recognition, but also mentions some progress in other areas of Speech Processing (speaker recognition, speech synthesis, speech analysis and coding) using similar methodologies. Special emphasis centers on the improvements made possible by Markov Models, and, more recently, by Connectionist Models, resulting in progress simultaneously obtained along the above different axes, in improved performance for difficult vocabularies, or in more robust systems. Some specialised hardware is also described, as well as the efforts aimed at assessing Speech Recognition systems.
Over the centuries Zaydīs have been called upon to respond to a series of challenges from within and without to the internal cohesion of their tradition. Noting that Zaydīs did not commonly follow the legal opinions of their eponym Zayd b. ʿAlī (d. 112/740), Sunnī critics challenged them to justify their adoption of the label Zaydī. The classical response was provided by the Yemeni imam al-Manṣūr ʿAbd Allāh b. Ḥamza (d. 614/1217), who explained affiliation to Zayd in theological and political terms. Within Zaydism itself, however, disagreement among leading imams on questions of law occasioned dissent among their followers. To counter this threat to unity within their ranks, Zaydī jurists widely adopted the theory that all qualified legal experts (muğtahids), including the Zaydī imams, were equally correct. More technical were the questions that came to surround the character of the legal school (maḏhab) that became dominant among Yemeni Zaydīs. These concerned both the source of the legal opinions that made up the doctrine of the school and the related question of the school's structure of authority. These historical and theoretical questions acquired a particular urgency from the 11th/17th century and were popularized with the circulation of Isḥāq b. Yūsuf's (d. 1173/1760) short poem, ʿUqūd al-taškīk, which directly challenged Yemeni Zaydīs to clarify their legal identity. Equally challenged was the structure of authority of all the Sunnī maḏhabs, and therefore issues raised here pertain to Islamic law more generally. This poem evoked a variety of responses in prose and verse, including a short treatise by the poem's author, al-Tafkīk li-ʿUqūd al-taškīk. While several respondents sought to affirm the viability of the legal school, others, notably Ibn al-Amīr al-Ṣanʿānī (d. 1182/1769) and Muḥammad b. ʿAlī al-Šawkānī (d. 1250/1834), argued that it could not be saved. Their objections to traditional legal authority (taqlīd) within Zaydism were widely disseminated by 19th- and 20th-century Muslim reformers interested in undermining the Sunnī schools of law and continue to enjoy great currency.
One of a listener's major tasks in understanding continuous speech is segmenting the speech signal into separate words. When listening conditions are difficult, speakers can help listeners by deliberately speaking more clearly. In four experiments, we examined how word boundaries are produced in deliberately clear speech. Durational measurements were taken of the pre-boundary syllable, and of pausing (if any) at the boundary, in baseline utterances and in deliberately clear repetitions. We found that speakers do indeed attempt to mark word boundaries in clear (though not in normal) speech; moreover, they differentiate between word boundaries in a way which suggests they are sensitive to listener needs. Previous research has suggested that in English, listeners apply heuristic segmentation strategies which make word boundaries before strong syllables easiest to perceive. When conditions for the listener are difficult, however, speakers pay more attention to marking word boundaries before weak syllables, i.e. they mark just those boundaries which are otherwise particularly hard to perceive.
Weighted Medians with integer weights are widely used nonlinear operators in signal processing. Weighted median filters belong to the class of stack filters. Since each stack filter is uniquely specified by its corresponding positive Boolean function, the latter can be used to analyze the behavior of the filter. On the other hand, weighted medians are usually represented by a given set of weights. In this paper, weighted medians are first extended to include weighted median operators with positive real-valued weights. It is shown that any real weighted median is equivalent to an integer weighted median. As a consequence, a sensitivity analysis can always be performed to represent real weighted medians by finite precision weighted medians. This result proves very useful if one realizes that optimal adaptive weighted medians are usually real-valued. Conversion algorithms are presented to find the positive Boolean function representing a given weighted median and a set of weights of a weighted median representing a given positive Boolean function if feasible.
Approximate Value Iteration (AVI) is a method for solving a large Markov Decision Problem by approximating the optimal value function with a sequence of value representations V n processed by means of the iterations $ {V}_{n+1}=\mathcal{AT}{V}_n$ where $ \mathcal{T}$ is the so-called Bellman operator and $ \mathcal{A}$ an approximation operator, which may be implemented by a Supervised Learning (SL) algorithm. Previous results relate the asymptotic performance of AVI to the L ∞ -norm of the approximation errors induced by the SL algorithm. Unfortunately, the SL algorithm usually perform a minimization problem in L p -norms (p ≥ 1), rendering the L ∞ performance bounds inadequate. In this paper, we extend these performance bounds to weighted L p -norms. This enables to relate the performance of AVI to the approximation power of the SL algorithm, which guarantees the tightness and pratical interest of these bounds. We illustrate the tightness of the bounds on an optimal replacement problem.
Variable selection and regularisation are two widespread methods for improving generalisation abilities in neural network models. In this article, we show that using the L1-norm of the parameters as a regulariser allows for a simultaneous pruning ofunnecessary parameters. Formally halfway between classical variable selection and weight-decay-style regularisation, this method reconciles both. The result is parsimonious models with well-controlled parameters. We first illustrate the properties of this regulariser with a one parameter analytical study. We then present results obtained on neural networks for time series prediction. Finally, we discuss practical aspects like parameter estimation, as well as links with a similar method proposed in the statistics literature in the context of linear regression.
Acoustic-phonetic decoding of speech recognition constitutes a major step in the process of continuous speech recognition. This paper reminds the difficulties of the problem together with the main methods proposed so far in order to solve it. We then concentrate on the different complementary approaches that have been investigated by our group: expert system based on spectrogram reading, recognition by phonetic triphones, connectionist model based on the cortical column unit and stochastic recognition without segmentation.
In this article, we present some development results of a system that performs mosaicing of panoramic faces. Our objective is to study the feasibility of panoramic face construction in real-time. This led us to conceive of a very simple acquisition system composed of 5 standard cameras and 5 face views taken simultaneously at different angles. Then, we chose an easily hardware-achievable algorithm: successive linear transformation, in order to compose a panoramic face of 150° from these 5 views. The method has been tested on hundreds of faces. In order to validate our system of panoramic face mosaicing, we also conducted a preliminary study on panoramic faces recognition, based on the «eigenfaces» method. We also are considering applying our system to other applications such as human expression categorization using movement estimation and fast 3D face reconstruction.
In this paper, we present an application of the evidence theory for the classification of physiological states in a bioprocess. We are particularly interested by the relevance of the data sources which are here biochemical parameters measured during the bioprocess. The evidence theory, and more particularly the notion of conflict is used to evaluate the relevance of each data source. An other measure of conflict, based on a distance, is also used, and provides in some cases, better results than the classical notion of conflict of the evidence theory. Results are presented for two kinds of bioprocesses: batch process (which corresponds to a supervised classification) and fed-batch process (which corresponds to an unsupervised classification).
A new approach to construct phonemic transcriptions of spoken utterances is described. The Self-Organizing Feature Maps by Kohonen are first applied to vector-quantize speech into a sequence of phoneme labels a centisecond apart. This code sequence is converted into a phoneme string using a multi-layered feed-forward network trained with error back propagation. The trained network acts as a filter removing undesired transitional and coarticulatory effects from the code sequence. This makes it almost a trivial task to convert the code sequence into a phoneme sequence. The need for any statistical speech models, such as Hidden Markov Models, is thus eliminated. The new approach is compared to an existing one being used in a speech recognition system, in which simple durational rules are used for the same transformation task. The accuracy of produced phonemic transcriptions is 4.8 per cent units better using the proposed multi-layered network approach (88.4% opposed to 83.6%).
Human movement modeling can be of great interest for the design of pattern recognition systems relying on the processing of fine neuromotricity, like on-line handwriting recognition, signature verification as well as in the design of intelligent systems involving in a way or another the processing of human movements. So far, many models have been proposed to study human movement production in general and handwriting in particular: models relying on neural networks, dynamics models, psychophysical models, kinematic models and models exploiting minimization principles. Among the models which provide analytical representations, the Kinematic Theory of rapid human movements and its delta-lognormal model have been considered as very promising. However, although numerous studies have shown that such a paradigm could explain most of the basic phenomena constantly reported in classical studies dealing with fine motor control, many problems, both theoretical and technical, have postponed its direct or indirect integration in the design of pattern recognizers. In this paper, we overview these problems and report on various projects conducted by our team to overcome these difficulties. First, we present a brief recall of the different models in the field and focus on the family of models involving lognormal functions. Then, from a practical perspective, we describe two new parameter extraction algorithms suitable for the reverse engineering of single strokes as well as complex handwriting signals. We show how the resulting representation can be used to improve electromyographic signal processing, opening a windows on new applications for handwriting processing, particularly in biomedical engineering and in some fields of neurosciences. We briefly conclude by listing various potential applications of the Kinematic Theory, particularly in the fields of handwriting recognition, signature verification and biomedical signal processing.
In this article, the possible use of Kohonen's neural network to vector quantize images is investigated. Some theoretical results on convergence of the training process are first given. Then, results obtained for various codebook sizes and input dimensions are compared. Tests are then performed with the best parameter values, using several images to design codebooks. This approach is based on the concurrent use of five networks where the effect of various relevant parameters is studied, such as number of classes of vectors, vectors dimension, and number of vectors used for coding.
This article describes the architecture and the operation of the DIRA (Integrated Dialogue and Automatic Recognition) continuous speech recognition system in its present stage of development. The DIRA system is a supervised multi-expert system. The supervisor dynamically arranges the tasks of its expert modules, which are each attached to one of the subdomains of the speech recognition problem, i.e. the acoustic)phonetic, the lexical-, the syntactic)semantic-, the prosodic- and finally the pragmatic domain. A blackboard serves as message interchange medium between these expert modules, as well as long-term memory for the speech recognition process as a whole. The supervisor is an opportunistic planner: it reasons on the data present at the blackboard and «calculates» the best strategy (a scheme for the activation the expert modules) to resolve the current problem. The operation of the individual expert modules is also addressed in this bases represented as rules controlling the transitions in ATN's (Augmented Transition Networks), the linguistic analyzers using the same A TN concept and the principle of functional lexical grammars, the comprehensive analyzer founded on the principle of lexical priming and finally the rule- based prosodic analyzer. The operation of the speech recognition system is commented, while providing examples and test results.
In this paper, we present our work on the design, adjustment and registration of a 3D articulated model of the hand on monoscopic images sequences, without markers. After a review of the state of the art, we describe our 3D generic model of the hand and how it is adjusted to the operator's morphology on an image of his open hand. Then, we compare various cost functions and optimisation methods to register the 3D model of the hand. Finally, we show results on image sequences. Gesture capture by artificial vision could be a valuable help for applications such as human-machine interaction by gesture, for virtual humanoid animation, low bit rate coding of gesture for telepresence, or sign language recognition.
An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training.
A new characterization of agrammatism is suggested, based on new data from Hebrew speaking agrammatic aphasics, and a reexamination of data from Russian and Italian. This characterization is formed in relation to linguistic levels of representation. It is then shown that agrammatic performance in a variety of tasks (including comprehension) is explained naturally as a consequence of this condition.
In the framework of phonemic speech recognition using Hidden Markov Models (HMMs) together with codebooks trained by Learning Vector Quantization (LVQ), a novel way to model context-dependencies in speech is presented. We use LVQ to map acoustic contextual data into context independent phonemic form. This mapping eliminates the need to employ context dependent phonemic, for example, triphone HMMs, and the difficulties associated therein. Instead, simpler context independent discrete observation HMMs suffice. We report excellent results for a speaker dependent task for Finnish.
This paper describes an approach for defining what individual strategies in constituting and decoding messages may be, in order to better understand and eventually automatically account for the variability of the speech signal. The work presented here concentrates on the phonological level of French. Two rule-based modules representing general phonological variants have been developed. Results of tests on them are given, and the errors produced are examined. The exceptions to the rules, taken from the rule-building data, as well as the assessment data, are examined in search of regularly produced events, common to several speakers, which are found to have common causes, for example dialect. Speakers may then be characterised as belonging to a class of individuals who, as a result of a common cause, all make certain phonological choices in the same manner.
Speech is a natural error-correcting code. The speech signal is full of rich sources of contextual redundancy at many levels of representation including allophonic variation, phonotactics, syllable structure, stress domains, morphology, syntax, semantics and pragmatics. The psycholinguistic literature has tended to concentrate heavily on high level constraints such as semantics and pragmatics and has generally overlooked the usefulness of lower level constraints such as allophonic variation. It has even been said that allophonic variation is a source of confusion or a kind of statistical noise that makes speech recognition that much harder than it already is. In contrast, I argue that aspiration, stop release, flapping, palatalization and other cues that vary systematically with syllabic context can be used to parse syllables and stress domains. These constituents can then constrain the lexical matching process, so that much less search will be required in order to retrieve the correct lexical entry. In this way, syllable structure and stress domains will be proposed as an intermediate level of representation between the phonetic description and the lexicon. My argument is primarily a computational one and will include a discussion of a prototype phonetic parser which has been implemented using simple well- understood parsing mechanisms. No experimental results will be presented.
This article focusses on the automated synthesis of agents in an uncertain environment, working in the setting of Reinforcement Learning, and more precisely of Partially Observable Markov Decision Processes. The agents (with no model of their environment and no short-term memory) are facing multiple motivations/goals simultaneously, a problem related to the field of Action Selection. We propose and evaluate various Action Selection architectures. They all combine already known basic behaviors in an adaptive manner, by learning the tuning of the combination, so as to maximize the agent's payoff. The logical continuation of this work is to automate the selection and design ofthe basic behaviors themselves.
Image is often considered as the fundamental perceptual unit of a visualization. In this paper, we suggest using one color image to allow an immediate and synthetic visualization of data. The color permits to exhibit the main structures of dataset. After reducing the dimensionality of the dataset, we generate color pixel using a transformation deduced from the work of Ohta et al. The last step consists in sorting and arranging pixel into a squared image to provides the final color image that summurizes initial data.
The large-scale (3–3.5 Bark) spectral integration (LSI) theory derived from the work of Chistovich and colleagues and supposed to provide a basis for the computation of the F 2 parameter is not in fact supported by an actual proof, since all presumed evidence can be understood without this theory. We believe that LSI existence cannot be demonstrated in a matching test, since if a subject is instructed to deliberately match a “complicated” spectrum with a “simplified” one, he will do it, whatever the way. Evidence for a specific LSI mechanism can only be found if we are able to find a “trace” of it in an experiment in which no “simplification” of the spectrum is explicitly required from the subject. In order to find a “trace of F 2” in memory, two sets of experiments (identification experiments and discrimination experiments with increasing memory load) were conducted using two sets of 4-formant synthetic vowels (around the [i]-[y] and the [e]-[ø] phonetic boundaries, respectively). The main results of this study are the following: • - significant differences observed in the discrimination scores cannot be explained without assuming that an LSI representation is in fact computed; • - this LSI representation is more robust in the auditory memory than the classical critical band (i.e. 1-Bark) reprensentation. The key argument was that stimuli associated with ambiguous F 2 determinations in matching experiments were also associated with ambiguous traces a memory — as revealed by a particularly high number of false alarms — in discrimination experiments. These results provided us with the “trace of F 2 in memory” that we were looking for.
In this approach, the MLP is trained for phoneme classification, and then the output values of the MLP are used as the state-dependent weightings. Applying the MLP outputs to the state-dependent weightings improves the performance of the conventional HMM without state-dependent weightings. However, in order to further improve the discriminability of competing classes, the discriminative training of the state-dependent weightings is performed by computing the gradient of the optimization criterion for the state-weighted HMM with respect to the MLP parameters. The proposed algorithm reduces the error rate considerably as compared with the conventional HMM in speaker-independent continuous speech recognition.
A segmental probabilistic model based on an orthogonal polynomial representation of speech signals is proposed. Unlike the conventional frame based probabilistic model, this segment based model concatenates the similar acoustic characteristics of consecutive frames into an acoustic segment and represents the segment by an orthogonal polynomial function. An iterative algorithm that performs recognition and segmentation processes is proposed for estimating the segment model. This segment model is applied in the text independent speaker verification. Tests were carried out on a 20-speaker database. With the best version of the model, an equal error rate of 0.59% can be reached, for test utterances of 10 digits. This corresponds to a relative error rate reduction of more than 50%, compared to the conventional frame based probabilistic model.
To classify objects located in their environment, underwater mobile robots use sequential sensory data (sonar). These pieces of information are imperfect, that means imprecise, uncertain and incomplete. Incompleteness is defined as the unavailability of some parameters which makes some classification criteria impossible to compute and which delays the decisions. The paper proposes to model data in the framework of possibility theory, and to apply fuzzy calculus to evaluate criteria even in the case of incompleteness. Results are sequentially fused by a dissymmetric combination process. The different dissymmetric fusion rules are reviewed and a specific dissymmetric operator is proposed to solve the incompleteness problem.
This paper presents a comparative study of transformations used to compute the area of cross-sections of the vocal tract from the mid-sagittal measurements of the vocal tract. MRI techniques have been used to obtain both mid-sagittal distances and cross-sections of the vocal tract for French oral vowels uttered by two subjects. The measured cross-sectional areas can thus be compared to the cross-sectional areas computed by the different transformations. The evaluation is performed with a jackknife method where the parameters of the transformation are estimated from all but one measurement of a speaker's vocal tract region and evaluated on the remaining measurement. This procedure allows the study of both the performance of the different forms of transformation as a function of the vocal tract region and the stability of the transformation parameters for a given vocal tract region. Three different forms of transformation are compared: linear, polynomial and power function. The estimation performances are also compared with four existing transformations.
This paper provides a renewed perspective on the manufacturing of organizational decisions. We approach decision making as performative praxis – a set of activities whereby actors simultaneously produce decisions and turn theories of decision-making into social reality. We extend the performative praxis perspective – that combines the processes of conventionalization, engineering and commodification – to account for actors' direct and indirect mobilization of theoretical representations when making decisions.
This paper deals with Pseudo-QMF sub-band coding of color TV signals based on DPCM schemes with scalar quantization. The proposed methods are based upon predictive coding system and scalar quantization and the objective is to get an excellent visual quality of the decoded images. First, we proposed three sets of adaptive prediction functions and compared them. Then we developed three adaptive quantization schemes with differents complexity of adaptation.
The low level of computer programs in go is not only due to the combinatorial complexity of the game but also to the difficulty to build a sound and complete evaluation function. This paper highlights the numerous spatial concepts of a go evaluation function ; the most important are grouping, splitting, cercling and aggregating. For such reason, computer go is an excellent example of the application spatial reasoning theories. For each spatial concept, this paper shows the mathematical tools used for computer simulation (mathematical morphology, topology, Hausdorff distance, qualitative spatial reasoning). This study is based on computer experiments which lead to the achievement of the international go playing program Indigo.
In this paper, we describe an efficient method for obtaining word classes for class language models. The method employs an exchange algorithm using the criterion of perplexity improvement. The novel contributions of this paper are the extension of the class bigram perplexity criterion to the class trigram perplexity criterion, the description of an efficient implementation for speeding up the clustering process, the detailed computational complexity analysis of the clustering algorithm, and, finally, experimental results on large text corpora of about 1, 4, 39 and 241 million words including examples of word classes, test corpus perplexities in comparison to word language models, and speech recognition results.
In this paper we present an adaptive microphone array with adaptive constraint values to suppress coherent as well as incoherent noise in disturbed speech signals. We use a generalized sidelobe cancelling (GSC) structure implemented in the frequency domain since it allows a separate handling of determining the adaptive look-direction response to suppress incoherent noise and adjusting the adaptive filters for cancellation of coherent noise. The transfer function in the look-direction is an adaptive Wiener-Filter which is estimated by using the short-time Fourier transform and the Nuttall/Carter method for spectrum estimation. The experimental results demonstrate that the proposed method works well for a large range of reverberation times and is therefore able to operate independently of the correlation properties of the noise field.
Designing a visual aid for speech training of the hearing-impaired presupposes the solution of problems in various fields such as phonetics, automatic speech recognition, speech perception and production, learning theory, ergonomics, system development and computer programming. The success of the eventual visual aid will depend on how well these problems are solved in a coherent fashion. This paper proposes a structural description of the formal aspects of a visual aid for speech training, and presents an account of the type of problems met at different stages of developing such a device. Together this leads to a framework for the construction of the Visual Speech Apparatus described in the second part of the paper.
The modification methods described in this paper combine characteristics of PSOLA-based methods and algorithms that resynthesize speech from its short-time Fourier magnitude only. The starting point is a short-time Fourier representation of the signal. In the case of duration modification, portions, in voiced speech corresponding to pitch periods, are removed from or inserted in this representation. In the case of pitch modification, pitch periods are shortened or extended in this representation, and a number of pitch periods is inserted or removed, respectively. Since it is an important tool for both duration and pitch modification, the resynthesis-from-short-time-Fourier-magnitude-only method of Griffin and Lim (1984) and Griffin et al. (1984) is reviewed and adapted. Duration and pitch modification methods and their results are presented.
The digitization of mobile radio involves high demands on the system components. It is, above all, the speech digitizer which requires particular attention. The data rate used for the speech transmission largely determines the frequency bandwidths of the speech channels and, hence, the frequency economy of the entire system. As a consequence, the data rate must be kept as low as possible. In addition, it is necessary to use apropriate error protection codings in order to sufficiently compensate for disturbing influences of the radio channel on the speech quality. Both requirements result in complex coder structures with different functional blocks complementing each other. The structure discussed in this paper combines an optimum source coding algorithm based on Vector Quantization (VQ) with a modified multi-pulse procedure. The paper shows possibilities of how to apply VQ to different parameter sets. Particular attention is paid to the application of VQ to log-area coefficients split up into groups.
In this paper we present a review of some fluid mechanical phenomena involved in bilabial plosive sound production. As a basis for further discussion, firstly an in vivo experimental set-up is described. The order of magnitude of some important geometrical and fluid dynamical quantities is presented. Different theoretical flow models are then discussed and evaluated using in vitro measurements on a replica of the lips and using numerical simulations.
This paper addresses the automatic cartography of sea-bottom by means of high resolution sonar images. Many texture analysis methods have been developed since now, based on statistical, geometrical or spectral modeling [14, 45, 7, 44]. Nevertheless, few of them are robust toward image rotations. Indeed, the property of rotation invariance is essential in our framework, particularily for obtaining good classification rates. We present in this article five methods for the automatic classification of rotating images, corresponding to four classes of sea-floor: “sand”, “ridge”, “dune” and “wreck”. The first one is an extension of a circular AutoRegressive method, initially proposed by Kashyap et Khotanzad [19], which allows to estimate a reduced number of rotation invariant parameters. The four other methods are based on an original approach, consisting to apply a mathematical transform to a set of parameters describing texture features. Two of them consist in computing the Log-Polar transform to autoregressive (AR) or correlation (COR) parameters. The two others consist in estimating the Zernike moments of autoregressive (AR) or correlation (COR) parameters. Classification rates obtained on sonar images and on Brodatz album are presented and allow to compare the performances of each approach.
We present a context-dependent, phoneme and function word based, Hidden Control Neural Network (HCNN-CDF) architecture for continuous speech recognition. The system can be seen as a large vocabulary extension of the word-based HCNN system proposed by Levin in 1990. Initially, we analysed context-independent HCNN modeling principle in the framework of the Linked Predictive Neural Network (LPNN) speech recognition system and found that it results in a 6% increase of the word recognition accuracy at perplexity 402. Significant savings compared to the LPNN in the resource requirements and computational load for the HCNN implementation can be achieved. In speaker-dependent recognition experiments with perplexity 111, the current versions of the LPNN and HCNN-CDF systems achieve 60% and 75% word recognition accuracies, respectively.
Between the end of the chain of waveform coders, which reaches from linear PCM to adaptive predictive coders (APC), and the class of (especially: predictive) vocoders, a “coding gap” of roughly 32-2.4 kbit/s is shown to actually define “medium-rate” speech coding. The fundamental approaches trying to close the gap are exposed, working either in time or frequency-domain. It is discussed, how their weaknesses may be overcome in advanced predictive or generalized “Filter-Bank” Coding (FBC) systems. The basic ideas and problems of vector quantization (VQ) are reviewed. Combinations of VQ with other schemes are addressed as well as other composite coding systems.
The negative peak amplitude of the differentiated glottal flow (d peak) is known to correlate strongly with the sound pressure level (SPL) of speech. Therefore, the function between d peak and SPL has been conventionally modeled as a single line. In this survey, the linearity of the function between d peak and SPL is revisited by analyzing glottal flows that were inverse filtered from speech sounds of largely different intensities. It is shown that SPL–d peak-graphs can be modeled more accurately by using two linear functions, the first of which models soft phonation, and the second of which models normal and loud speech sounds. For all of the analyzed SPL–d peak-graphs, the slope of the modeling line matching soft phonation was larger than the slope of the line for normal and loud speech. This result suggests that vocal intensity is affected not only by the single amplitude domain value of the voice source, d peak, but also by the shape of the differentiated glottal flow near the instant of the negative peak.
This paper focuses on the partial emphasis or “prominence” of parts of Japanese sentences. Four sets of 43 read sentences uttered by two speakers including various types of prominence (172 sentences in total) are analyzed. This analysis shows that in 88% of the sentences prominence is produced by enhancing F 0 and increasing power. No examples of lengthening of phoneme duration are observed in the emphasized parts of the sentences except for some special cases. One exception is lengthening accompanied by pause insertion as a mark of prominence, and another slowing total speech rate. The prosodic features of read natural speech are then used to develop rules for changing a reference sentence to produce prominence for rule-based speech synthesis. Listening test results using 10 subjects do not show any significant difference in expressibility between prominence synthesized by rule (rate of correct expression: 76.9%) and prominence in natural speech (79.9%) at the 5% level. To further improve prominence expressibility, listening tests for 10 subjects are used to clarify the conditions under which prominence expressibility becomes optimal. These tests show that the prosodic control parameters increase the expressibility of prominence by about 20%. Finally, prosodic features of spontaneous conversational speech are analyzed and compared with those of read sentence speech. Speech-rate reduction in parts where prominence is placed is more conspicuous in spontaneous conversational speech.
Speaker variability in the coarticulation of the vowels /a,i,u/ was investigated in /C 1,VC2∈/ pseudo-words, containing the consonants /p,t,k,d,s,m,n,r/. These words were read out in isolation by fifteen male speakers of Dutch. The formants F 1–3 (in Bark) were extracted from the steady-state of each vowel /a,i,u/. Coarticulation in each of 1200 realisations per vowel was measured in F 1–3 as a function of consonantal context, using a score-model based measure called COART. The largest amount of coarticulation was found in /u/ where nasals and alveolars in C 1-position had the largest effect on the formant positions, especially on F 2. Coarticulation in /a,u/ proved to be speaker-specific. For these vowels the speaker variability of COART in a context was larger, generally, if COART itself was larger. Studied in a speaker identification task, finally, COART improved identification results only when three conditions were combined: (a) if COART was used as an additional parameter to F 1–3; (b) if the COART-values for the vowel were high; (c) if all vowel contexts were pooled in the analysis. The two main conclusions from this study are that coarticulation cannot be investigated speaker-independently and that COART can be contributive to speaker identification, but only in very restricted conditions.
In this paper, we tackle the problem of the extraction of non-redundant association rides using seniantics based on the Galois connection. The presented approach is of strong interest for the visualization of the extracted rules. The experiments carried out on real-life databases show the usefulness of the approach.
The performance levels of most current speech recognizers degrade significantly when environmental noise occurs during use. Such performance degradation is mainly caused by mismatches in training and operating environments. During recent years much effort has been directed to reducing this mismatch. This paper surveys research results in the area of digital techniques for single microphone noisy speech recognition classified in three categories: noise resistant features and similarity measurement, speech enhancement, and speech model compensation for noise. The survey indicates that the essential points in noisy speech recognition consist of incorporating time and frequency correlations, giving more importance to high SNR portions of speech in decision making, exploiting task-specific a priori knowledge both of speech and of noise, using class-dependent processing, and including auditory models in speech processing.
This article proposes an alternative rhythmic unit to the syllable: the inter-perceptual-center group (IPCG). This group is delimited by events which can be detected using only acoustic correlates (Pompino-Marschall, 1989). The rhythmic patterns for French are described using this characterisation: we show that realisation of accents is gradual over the trailed accentual group and that this gradual lengthening is needed for perception.
Identification experiments were performed to assess the relative importance of Fourier phase versus amplitude for intervocalic stop consonant perception. In the first experiment, three types of stimuli were constructed from VCV signals: (1) Swapped stimuli, a swapped stimulus has the amplitude spectra of its consisting segments from one VCV signal and its corresponding phase spectra from another; (2) Phase-only stimuli; and (3) Amplitude-only stimuli. It was shown that the perception of intervocalic stop consonants varies from amplitude dominance to phase dominance as the Fourier analysis window size increases. The crossover lies somewhere between 192ms and 256ms. The influence of phase at smaller window sizes was elaborated in the second experiment. This experiment demonstrated that phonetically different signals can be constructed by combining the same short-time amplitude spectra with different phase spectra, so the short-time amplitude spectra displayed on a spectrogram cannot exclusively specify a stop consonant. In both experiments, the perception of voicing in stops was found to rely strongly on phase information while the perception of the place of articulation was mainly determined by amplitude information.
We present a speech transmission system operating at a low bit rate (about 1000 bits/s), which is based on the following principle. The speech signal is analysed using a LPC technique and, for each frame, the global parameters of the input signal (energy, pitch, voicing) on the one hand, and secondly the filter coefficients are separately coded. Studies have been mainly focused on the last point. We have compared several representation spaces (autocorrelation, cepstrum, LPC analysis without and with preemphasis), in order to choose the most suitable representation for a good vector quantization. The latter has been performed by a very simple algorithm, which we have compared to the “LBG” method. We have shown that, in our experimental conditions, the simplicity of our algorithm is an advantage, and that it performs a good vector quantization of the spectral space. The second part of the study is oriented towards the use of the codebook obtained as described above. We compute it from several speakers, for a total of about 30000 frames (about six minutes) of speech: it consists of more than 1500 vectors. We have studied how to obtain a fast coding of a vector with this codebook, losing the optimality of the nearest neighbour coding. We have shown that the distortion is only slightly increased by using clustering techniques on the codebook, leading to a hierarchical coding decision, which allows a very fast coding of any new vector. In conclusion, the simplification of the codebook construction (associated with a correct choice of the spectral representation space), and a fast (but suboptimal) method of coding with such a codebook lead to a system whose performances are only slightly degraded compared to reference spectral vector quantization systems for speech transmission.
In this paper we propose a new transform domain coding technique called transform trellis coded quantization (TTCQ) that is based on the trellis coded quantization technique recently proposed where Ungerboeck's amplitude modulation trellises and set partitioning ideas are used for source coding. We combine the proposed technique with a transform domain formulation for small frame sizes obtaining a transform domain scheme suitable for low-delay speech coding.
Bellcore's CallManager system is an experimental prototype of a call screening and `follow-me anywhere' service platform that makes use of a personal appointment calendar and a personal phonebook (accessed through the customer's wireline or wireless PDA) to determine the customer's current location and the importance of each caller to the customer. This is an example of a future class of network-provided services that utilize advanced speech processing and customer specific data to greatly enhance the value of the telephone network to the customer. Such services allow important calls to reach the customer while screening out unwanted or unimportant calls. These services can potentially also proactively provide information to a customer `as it happens' wherever the customer may happen to be.
The article focusses on how Jean Lemaire de Belges reflected on poetic as well as authorship issues during a period that required to take a stand against the medieval heritage - that is, all acquired knowledge from the Antiquity to the Middle ages. In the Épîtres de l'Amant vert, the subordinate relationship between the poet and his patron results in a self-deprecating parody of the patronage tradition. The political and moral allusions of the Illustrations de Gaule et singularitez de Troye bring into question the legitimacy of poetic discourse as well as its rhetoric, which are both influenced by their close relationship with the theme of love. The Concorde des deux langages begins with the assumption of a possible reconciliation in the persona of the author, between the poet armed with the seductiveness of his eloquence, and the cleric submitted to the requirements of “labeur historien” (the historian's task).
We describe a model for the transduction of the basilar membrane displacements into activity of the auditory nerve fibers. The model consits of two parts: a one-dimensional model of basilar membrane motions and a model of sensory receptors. The first part is a linear functional model which gives the electrical analog of the displacement patterns of the basilar membrane. The second part is a non-linear physical model which is mainly based on the assumption that information issued from the two kinds of hair cell receptors interacts electrically. The model is shown to reproduce the excitation patterns of single auditory nerve fibers in response to simple input signals (one and two-tone stimuli) or to inferred trapezoidal displacements of the basilar membrane. A second set of experiments is described to show how useful such a model can be for the understanding of complex sound coding. We present results for a speech-like single formant input sound paying special attention to the fundamental frequency representation in the discharge patterns of our simulated fibers.
Moving away from constrained parametric to unconstrained flexible non parametric models is a deep trend in image processing that constant increase of computing resources makes each day more possible. Deformable models offer a good illustration: from rigid parametric transformation to non parametric diffeomorphic matching, we show how the matching problem can be seen as search of optimal deformation paths between two objects, or equivalently determination of a minimal geodesic in a shape manifold.
This paper introduces recent research activities on speech recognition, ranging from acoustic processing to linguistic processing, at NTT (Nippon Telegraph and Telephone Corporation) Laboratories. These include the proposal of ΔLSP parameters, hierarchical Δcepstral parameters, a new method of utilizing pitch information, automatic speaker adaptation techniques, robust HMM phoneme models, new training algorithms for neural networks, linguistic processing that uses syntactic and semantic knowledge, implementation of prototype continuous speech recognition systems, and an efficient text-independent speaker recognition algorithm.
We present a real-time approach for circular and polygonal road signs detection in still images, regardless of their pose and orientation. Object detection is performed using a Hough like transform based on accumulation on pairs of points with their gradients. Circular and polygonal (non triangular) traffic signs are detected by the so-called Bilateral Chinese Transform BCT. Triangular traffic signs are processed by the Vertex Bisector Transform VBT. These two approaches are tested on several databases of urban scene.
The visual data flow environment called V4Miner simultaneously uses a set of interactive techniques, visualization methods and machine learning algorithms to discover knowledge in an intuitive, interactive and retroactive way. A data mining task is easily expressed by a graphical visual data flow where a method is represented by a graphic JavaBeans component, the communication between methods is based on a data bus. The user can create and fully control the discovery process without any programming. The visual data flow makes it possible on the one hand to reduce the complexity of an analysis task and on the other hand to improve quality and users ' comprehensibility of the obtained results.
This paper describes a rule-based data-driven (DD) method to model pronunciation variation in automatic speech recognition (ASR). The DD method consists of the following steps. First, the possible pronunciation variants are generated by making each phone in the canonical transcription of the word optional. Next, forced recognition is performed in order to determine which variant best matches the acoustic signal. Finally, the rules are derived by aligning the best matching variant with the canonical transcription of the variant. Error analysis is performed in order to gain insight into the process of pronunciation modeling. This analysis shows that although modeling pronunciation variation brings about improvements, deteriorations are also introduced. A strong correlation is found between the number of improvements and deteriorations per rule. This result indicates that it is not possible to improve ASR performance by excluding the rules that cause deteriorations, because these rules also produce a considerable number of improvements. Finally, we compare three different criteria for rule selection. This comparison indicates that the absolute frequency of rule application (F abs) is the most suitable criterion for rule selection. For the best testing condition, a statistically significant reduction in word error rate (WER) of 1.4% absolutely, or 8% relatively, is found.
Jean Miélot's Contemplations sur les sept heures de la Passion, composed in 1454 for Philip the Good, duke of Burgundy, are preserved in a single copy, Paris, BnF, fr. 12441. Miélot's main source was the anonymous Meditatio de passione Christi per septem diei horas, but a comparison of the two texts shows that he significantly modified the Latin text in order to adapt it for a lay reader.
This paper describes the design and implementation of the MATE workbench, a program which provides support for the annotation of speech and text. It provides facilities for flexible display and editing of such annotations, and complex querying of a resulting corpus. The workbench offers a more flexible approach than most existing annotation tools, which were often designed with a specific annotation scheme in mind. Any annotation scheme can be used with the MATE workbench, provided it is coded using XML markup (linked to the speech signal, if available, using certain conventions). The workbench uses a transformation language to define specialised editors optimised for particular annotation tasks, with suitable display formats and allowable editing operations tailored to the task. The workbench is written in Java, which means that it is platform-independent. This paper outlines the design of the workbench software and compares it with other annotation programs.
A two-part experiment on protrusion movements of the lower lip for the vowel /u/ was conducted to investigate: (1) the production of 'troughs', i.e., diminution of lip protrusion during the intervocalic consonant in /uCu/ utterances, and (2) 'time locking', that is, movement onset at a fixed interval prior to onset of voicing of /u/, regardless of the number of 'neutral' consonants in /VunroundedC n u/ utterances. Two speakers of French, a speaker of Spanish and two speakers of English produced similar sets of nonsense utterances. A bite block was used to eliminate mandible movements as a source of lower lip movements. A detailed examination was made of relationships among events in movement and acoustic signals for each token of multiple repetitions of the test utterances. Results suggest that the production of troughs may be due in part to specific articulatory targets for the intervocalic consonants /s/ and /t/. Specific consonantal targets prevented the direct investigation of time-locking. Experimental limitations preclude firm conclusions, but the nature of the results suggests that a complex of factors, including “co-production of adjacent segments” and “context-dependent adjustments of articulatory targets” may underlie observed variation in articulatory behaviour.
This paper reports a set of studies of some phonetic characteristics of the American English represented in the TIMIT speech database. First we describe some relevant characteristics of TIMIT, and how we use the non-speech files on the TIMIT CD with a commercial database program. Two studies are then described: one using only the non-audio parts of TIMIT (segmental transcriptions and durations, and speaker information), and one using the audio signal for acoustic analysis. Results of such studies should be useful not only to linguistic phoneticians but also for speech recognition lexicons and text-to-speech systems.
We present in this paper a character recognition system using many classifiers. Each classifier gives an answer and the final result is selected by majority-vote. The system uses six classifiers built around first and second order hidden Markov models (TIMM) as well as nearest neighbor considerations. The majority-vote is chosen so as to give better results than each of the other systems applied individually. The recognition process is followed by a post-processing which employs combinations of stochastic and dictionary verification methods forword recognition and error-correction.
In this tutorial on time-scaling we follow one particular line of thought towards computationally efficient high quality methods. We favor time-scaling based on time–frequency representations over model based approaches, and proceed to review an iterative phase reconstruction method for time-scaled magnitude spectrograms. The search for a good initial phase estimate leads us to consider synchronized overlap-add methods which are further optimized to eventually arrive at WSOLA, a technique based on a waveform similarity criterion.
Complex emotions in real-life context have not been studied much. We are exploring how to represent and automatically detect a subject's emotional state in humanhuman spoken interaction. In contrast to most previous studies, conducted on artificial data, this paper addresses some of the challenges faced when studying real-life non-basic emotions. Real-life spoken dialogs from call-center services have revealed the presence of many blended emotions. A soft emotion vector is used to represent emotion mixtures. This representation enables to obtain a much more reliable annotation and to select the part of the corpus without conflictual blended emotions for training models. A correct detection rate of about 80% was obtained with SVMs and decision trees models between Negative and Neutral emotions using paralinguistic cues on a corpus of 20 hours of recording in a Medical call-center.
This paper describes a policy-based approach to firewall management. The Policy-Based Networking (pbn) architecture proposed by the Policy Framework Group of Internet Engineering Task Force (ietf) is analysed, together with the communication protocols, policy specification languages, and the necessary information models. An overview of policy specification languages applicability topbn architecture is presented paying particular attention to the specification of security policies through Security Policy Specification Language (spsl). The Common Open Policy Service protocol (cops) and its variant,cops for Policy provisioning (cops-pr), both used for the transport of policy information, are also presented. The paper continues with a description of an application of thepbn architecture to firewall management. The proposed architecture is presented and its implementation issues are analysed with some usage examples. The paper concludes with the evaluation of the policy-based approach to firewall management.
Two sets of experiments were performed to test the perceptual benefits of enhancing consonantal regions which contain a high density of acoustic cues to phonemic contrasts. In the second set, corresponding regions in natural semantically-unpredictable sentence (SUS) material were annotated and enhanced in the same way. Both sets of stimuli were combined with speech-shaped noise and presented to normally-hearing listeners. These results demonstrate the benefits gained from enhancement techniques which use knowledge of acoustic cues to phonetic contrasts to improve the intelligibility of speech in the presence of background noise.
This study highlights a hitherto neglected trope of Muslim apocalyptic literature—namely, that in a region known as al-Ṭālaqān there awaits the future Mahdī a great treasure that will gain him a mighty army to aid him fight the final battle against evil. Tracing the trope's origin in Zoroastrian apocalypticism and its subsequent dissemination in a wide array of Muslim apocalyptic traditions, this paper argues that this apocalyptic trope ultimately entered into Muslim apocalypticism, in particular Šīʿite apocalypticism, during a Zaydī revolt against the ʿAbbāsids led by the Ḥasanid Yaḥyā b. ʿAbd Allāh in the year 176/792. The paper then explores how the revolt of Yaḥyā b. ʿAbd Allāh shaped the function of the 'treasures of al-Ṭālaqān' trope in Muslim apocalypticism and how Yaḥyā's personality and the revolt he inspired continued to leave an indelible imprint on Imāmī apocalypticism thereafter.
The development of dative objects by children acquiring Hebrew was investigated. A large corpus of maternal input speech was examined, and the dative verb category was found to be extremely skewed for frequency as well as varied for semantics. Children's development was explored through the analysis of 30 longitudinal observations. The very first verbs produced with dative complements were indeed core verbs. Semantic facilitation demonstrated transfer-based learning. The effects of input frequency and obligatoriness were much smaller for the second wave of verbs learned. The results reveal that two sequentially ordered processes operate in the development of a syntactic category which immediately recreate its structure: establishment of the core, and similarity-based transfer-learning of the periphery.
In a wide variety of applications, where the data X E A that must be processed characterize instances to which binary labels Y E {-1, +1} are randomly assigned, the goal of statistical learning does not reduce to find the likeliest label for a given instance but consists in ranking all the instances x E A in the same order as the one induced by the probability a posteriori y (x) = P {Y = +1 | X = x},, ranking rules being evaluated through ROC analysis. In contrast to the majority of procedures used in practice, based on a preliminary estimation of the function y (x), the results described in this article propose an extension of the concept of decision tree to the ranking problem in order to optimize the ROC curve directly.
Combining the properties of both probability theories and graph theory, bayesian networks have become very popular during the past decade. Determining the network's structure from a database of cases remains, however, a major issue. We have developed a genetical algorithm defining a structure while ignoring the limitations usually imposed upon the search (limited number ofparents per node, knowledge of an ordering over the variables). The algorithm searches the space ofdirected acyclic graphs and defines a set oflocal optima, eventually returning the best local optimum it has found.
This article describes a new approach for parametric analysis and synthesis of speech. It is based on the frequency domain modelling of the residual signal of an LPC analysis, including an harmonic structure and pitch extraction, and a sub-band analysis in order to determine a «voiced-unvoiced ratio» variable with frequency. This residue parametric representation is added to the classical LPC parameters for transmission or memorisation. Our system presents of course the flexibility of parametrical systems and is well adapted to frame to frame transition problems encountered.in text-to-speech applications.
This paper describes and analyses algorithms for hands-free telephony which use an acoustic echo canceller combined with an additional FIR filter in the sending path of the hands-free telephone. We describe two different methods to adapt the additional FIR filter (called “the echo shaping filter”) which are highly effective and easy to implement. It is shown that these algorithms allow to reduce the order of the compensator significantly and still provide high echo attenuation and only little distortions of the near speech signal during double talk.
In this paper we present some issues of knowledge engineering in the field of life sciences and, as an illustration, a data integration system opened on the Web, called ONDINE (Ontology based Data INtEgration), which proposes a complete workflow to extract, to semantically annotate and to query data from tables found in scientific documents from the Web. The core and key element of ONDINE is an Ontological and terminological Resource (OTR) allowing the modeling of n-ary relations; concepts from this OTR are used to annotate tables. First we present the OTR model, then the semi-automatic method for semantic annotation of tables guided by this OTR, and ﬁnally our software system, @Web (Annotating Tables from the Web), designed to semantically annotate tables.
SIGNAL is a language (which definition is in progress at IRISA), intended to be the algorithms description language of a CAD system for real time signal processing applications. Main characteristics of SIGNAL are issued from data flow principles: a program describes an oriented graph the nodes of which are processes (i. e. arithmetical and temporal primitive functions) connected through their input and output ports. SIGNAL is able to describe and run any real time algorithm. The presentation is illustrated by the way of an adaptive gradient lattice algorithm, treated as an example.
In an isolated testimony, Apollonius Dyscolus argues that some simple words are formed from words compounded with alpha privative, by means of the deletion of the alpha (ἀέκητι → ἕκητι). The way Apollonius discusses this formation is paradoxical. Indeed, the grammarian regards this formation as a pathos (morphological change). However, a corrupted word-form, according to Apollonius himself, should always retain the meaning of the original form, whereas a simple word necessarily means the opposite of its privative compound. This article aims to show what this conception of Apollonius, which challenges the very principle of pathology, teaches us about the particular status of the privative compound and its place in ancient grammatical theory.
Most of these systems work well only when the reference and test speech utterances are recorded under the same relatively noise-free conditions. The goal of this study was to develop a speaker verification system based on an orthogonal linear prediction model, that adopts a unified approach by utilizing only one set of reference parameters irrespective of whether the test speech data is of high quality or noise corrupted. However, for reference and test utterances spoken over the telephone, satisfactory verification is realized only when all the orthogonal parameters are utilized in distance computation.
This paper presents a study on finite-register-length effects in a Hidden Markov Model speech recognizer. A statistic model is utilized to approximate the distribution of the score differences. The range of recognition rate due to quantization noise on HMM parameters is calculated by using the statistic model. Then the relation between the recognition rate and the quantization noise is derived. This provides the information for determining the register length in the hardware realization of a HMM speech recognizer.
In this paper, we present a new writer independent system dedicated to the automatic recognition of on-line hand-printed texts. This system uses a very large French lexicon (200000 words), which covers numerous fields of application. The recognition process is based on the activation-verification model proposed in perceptive psychology. A set of experts encodes the input signal and extracts probabilistic information at several levels of abstraction (geometrical and morphological). We experiment several strategies of self-supervised writer-adaptation on this system. The best one, called “dynamic self-supervised adaptation”, modifies the recognizer parameters continuously. It gets recognition results close to supervised methods. These results are evaluated on a database of 90 texts (5400 words) written by 38 different writers and are very encouraging as they reach a recognition rate of 90%.
This paper describes recent advances in glottal source modeling for speech synthesis. In particular two procedures for modeling the glottal excitation waveform are described and applied to voice conversion. One model uses a polynomial to represent the glottal excitation waveform for one pitch period. The coefficients of the polynomial model form a vector that is used to design a glottal excitation code book with 32 entries for voiced excitation. The codebook is designed and trained using two sentences spoken by different speakers. In addition to the glottal excitation codebook, we use a stochastic codebook with 256 entries for unvoiced noise excitation. Analysis techniques are described for constructing both codebooks. The GELP synthesizer, which resynthesizes speech with high quality, provides the speech scientist with a simple speech synthesis procedure that uses established analysis techniques, that is able to reproduce all speech sounds, and yet also has an excitation model waveform that is related to the derivative of the glottal flow and the integral of the residue. Another approach uses the LF glottal volume-velocity waveform to model the characteristics of three voice types: modal, breathy, and vocal fry (creaky). We then convert a modal voice to sound like a breathy or vocal fry voice using the vocal tract characteristics for modal voice and the glottal volume-velocity waveform model for breathy and vocal fry voices as the excitation.
Speaker verification has been the subject of active research for many years, yet despite these efforts and promising results on laboratory data, speaker verification performance over the telephone remains below that required for many applications. This experimental study aimed to quantify speaker recognition performance out of the context of any specific application, as a function of factors more-or-less acknowledged to affect the accuracy. Some of the issues addressed are: the speaker model (Gaussian mixture models are compared with phone-based models), the influence of the amount and content of training and test data on performance; performance degradation due to model aging and how can this be counteracted by using adaptation techniques; achievable performance levels using text-dependent and text-independent recognition modes. These and other factors were addressed using a large corpus of read and spontaneous speech (over 250 hours collected from 100 target speakers and 1000 imposters) in French, designed and recorded for the purpose of this study. On these data, the lowest equal error rate is 1% for the text-dependent mode when two trials are allowed per verification attempt and with a minimum of 1.5 s of speech per trial.
Bayesian Belief Networks are a powerful tool for combining different knowledge sources with various degrees of uncertainty in a mathematical sound and computationally efficient way. Surprisingly they have not yet found their way into the speech processing field, despite the fact that in this science multiple unreliable information sources exist. The present paper shows how the theory can be utilized in for language modeling. Using these extensions a language model for speech recognition based on a context-free framework is constructed. In this model, sentences are not parsed in their entirety, as is usual with grammatical description, but only “locally” on suitably located segments. The model was evaluated over a text data base. In terms of test set entropy the model performed at least as good as the bi/tri-gram models, while showing a good ability to generalize from training to test data.
In this paper we describe a probabilistic fusion approach based on entropic criteria, which aims at reducing the combination space by explicitly representing the notions of source redundancy and source complementarity. This modelling is particularly interesting to optimize the choice of measurements provided by sources in order to combine in multi-sources fusion system. It is in agreement with the preoccupation to perform efficiently fusion processing and to minimize the hardware resources in information fusion problem. To answer that, we made a study of the parallelization of the entropy fusion algorithm developed for its parallel implementation in the framework of an application to mobile robotics. The algorithm specification exhibiting potential parallelism is then implemented on a network of workstations running in mode MIMD-NORMA using the parallel/distributed programming environments SynDEx which support AA-A methodology and PVM which support Hoare's CSP concept.
A simple neural network for isolated word recognition constructed under consideration of neurobiological and psychoacoustical observations is described. The biologically motivated preprocessing of the speech signals includes transforming frequency to critical band-rate and power to loudness, contrasting the spectrograms and extracting temporal and spectral features. It is shown that the different stages of preprocessing of the speech signal increase recognition rates significantly and are essential to achieve faultless recognition of a small vocabulary. In addition, the network is able to recognize simultaneously spoken words without any change of its architecture. Thus, it represents a concept to solve one of the most difficult figure-ground-problems in speech research without using conventional techniques like directional separation of stereophonically recorded speech or fundamental frequency tracking.
In this paper, we present the problem of noisy images recognition and in particular the stage of primitives selection in a classification process. This selection stage appears after segmentation and statistical describers extraction on documentary images are realized. We describe precisely the use of decision tree in order to harmonize and compare it with another less studied method based on a concept lattice.
This paper describes the IBM approach to Broadcast News (BN) transcription. Typical problems in the BN transcription task are segmentation, clustering, acoustic modeling, language modeling and acoustic model adaptation. This paper presents new algorithms for each of these focus problems. Some key ideas include Bayesian information criterion (BIC) (for segmentation, clustering and acoustic modeling) and speaker/cluster adapted training (SAT/CAT).
Alphasic children with serious language deficits were taught to communicate via a system of visual symbols originally devised by David Premack (1969) for use with chimpanzees. The subjects, lacking normal language, readily learned to express several language functions in this way (word, sentence, class-concept, question, negation). The linguistic status of 'Premackese' is questioned, and it is suggested that it is better viewed as a communication system. It may, therefore, be that the aphasic children lack some specifically linguistic ability.
In this paper, we apply a general reinforcement learning method to automatically design the behavior of non player characters of the Counter-Strike© first person shooter computer game. The result of the learning process is a set of decision trees that represents compactly and easily readable a model of the problem itself and the decision policy of characters. Beyond this example, we discuss the potential benefits of our method to design the decision architecture of non player characters in commercial computer games.
In the case of a “novel word” absent from a text-to-speech system's pronouncing dictionary, traditional systems invoke context-dependent letter-to-phoneme rules to produce a pronunciation. A proposal in the psychological literature, however, is that human readers pronounce novel words not by using explicit rules, but by analogy with letter-to-phoneme patterns for words they already know. In this paper, a synthesos-by-analogy system is presented which is, accordingly, also a model of novel-word pronunciation by humans. It employs analogy in both orthographic and phonological domains and is applied here to the pronunciation of novel words in British (Received Pronunciation) English and German. In implementing the system, certain detailed questions were confronted which analogy theory is at present inadequately developed to answer. Thus, a major part of this work concerns the impact of implementational choices on performance, where this is defined as the ability of the system to produce pronunciations in line with those given by humans. The size and content of the lexical database on which any analogy system must be based are also considered. The better performing implementations produced useful results for both British English and German. However, best results for each of the two languages were obtained from rather different implementations.
In this paper, we analyze the possibility of the reduction of smoothing levels in 3-D adaptive lower-upper-middle (LUM) smoother based on the fixed threshold control (FTC). Besides the excellent noise attenuation capability with the simultaneous signal-detail preservation, recently introduced LUM FTC filter with a window size N is characterized by a relatively complex structure, where an estimate is formed according to (N +1)/2 decision rules. This fact can constrain its possible hardware filter implementation in real motion video applications. In order to simplify the filter complexity, however, to retain the excellent filter performance simultaneously, we propose two approaches such as linear reduction of smoothing levels and optimal reduction based on genetic algorithm.
Although most parameters in a speech recognition system are estimated from data by the use of an objective function, the unit inventory and lexicon are generally hand crafted and therefore unlikely to be optimal. This paper proposes a joint solution to the related problems of learning a unit inventory and corresponding lexicon from data. On a speaker-independent read speech task with a 1k vocabulary, the proposed algorithm outperforms phone-based systems at both high and low complexities.
We propose a new region-based segmentation of textured sonar images with respect to seafloor types. We characterize sea-floor types by a set of empirical distributions estimated on texture responses to a set of different filters and we introduce a novel similarity measure between sonar textures in this attribute space. Our similarity measure is defined as a weighted sum of Kullback-Leibler divergences between texture features. Second, we add an additional weighting, evaluated as an angular distance between the incidence angles of the compared texture samples, to cope with the problem related to the sonar image acquisition process that leads to a variability of the backscattered (BS) value and the texture aspect with the incidence angle range. Our segmentation method is stated as the minimization of a region-based functional that involves the similarity between region texture based statistics and prototype ones and a regularization term that imposes smoothness and regularity on region boundaries. The proposed approach is implemented using level-set methods, and the functional minimization is done using shape derivative tools.
A hidden Markov model isolated word recogniser using full likelihood scoring for each word model can be treated as a recurrent 'neural' network. The units in the recurrent loop are linear, but the observations enter the loop via a multiplication. Training can use back-propagation of partial derivatives to hill-climb on a measure of discriminability between words. The back-propagation has exactly the same form as the backward pass of the Baum-Welch (EM) algorithm for maximum-likelihood HMM training. The use of a particular error criterion based on relative entropy (equivalent to the so-called Mutual Information criterion which has been used for discriminative training of HMMs) can have derivatives which are interestingly related to the Baum-Welch re-estimates and to Corrective Training.
A morphological method for the color segmentation of cytological images is presented. This method is mainly based on watershed whose potential function blend local and global informations. The method uses a priori informations for the frame of the method. The paper is based on three parts. In a first part, the frame of a morphological segmentation method is recalled. Finally, the usefulness of the segmentation method is illustrated on images from serous cytology
The domain of the speech recognition and dialog system EVAR is train time table inquiry. We observed that in real human-human dialogs when the officer transmits the information, the customer very often interrupts. Many of these interruptions are just repetitions of the time of day given by the officer. The functional role of these interruptions is often determined by prosodic cues only. An important result of experiments where naive persons used the EVAR system is that it is hard to follow the train connection given via speech synthesis. In this case it is even more important than in human-human dialogs that the user has the opportunity to interact during the answer phase. Therefore we extended the dialog module to allow the user to repeat the time of day and we added a prosody module guiding the continuation of the dialog by analyzing the intonation contour of this utterance.
Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX.
We analyzed spontaneous speech production in semi-standardized interviews conducted with 10 patients suffering from moderate senile dementia of the Alzheimer type (SDAT), 5 Wernicke's aphasics, and 5 elderly controls without brain damage. Data analysis revealed in both patient groups a reduction of sentence length but absence of systematic paragrammatic symptoms on the part of the demented patients. A relatively selectively diminished use of nouns was striking in the production of both patient groups, whereas word finding ability was surprisingly well preserved in the SDAT patients. Both patient groups exhibited marked deficits but different patterns of pathological behaviour on the discourse level of responding to the interviewer's questions. Results are interpreted within a proposed neurolinguistic language production model. It is argued that the formulation process may be preserved in demented patients but is disturbed in aphasia. Language-related disturbances in senile dementia are assumed to result from pre-linguistic disorders in the formation of the conceptual structure of the intended speech act.
Articulatory models of speech production are generally controlled by kinematic parameters that govern the position and movement of tissue and air. A four parameter model of the glottis is described with similar kinematic parameters to complement this approach. Parameters are nondimensional zed quotients that control the static and dynamic shape of the glottis. The model can stimulate glottal flow, glottal area, and vocal contact area simultaneously which identical parameters. Specific features of these waveforms are discussed in terms of the glottal configuration that was used to produce them. In particular, a detailed description of the contact area waveshape is included. The model is expected to be applied to vocal fold imaging techniques that rely on signals gathered noninvasively on the surface of the body. For synthesis, the model provides an alternative to flow pulse modeling because it can include some source-system interactions with relatively little computational overhead.
Psycholinguists strive to construct a model of human language processing in general. But this does not imply that they should confine their research to universal aspects of linguistic structure, and avoid research on language-specific phenomena. First, even universal characteristics of language structure can only be accurately observed cross-linguistically. This point is illustrated here by research on the role of the syllable in spoken-word recognition, on the perceptual processing of vowels versus consonants, and on the contribution of phonetic assimilation phonemena to phoneme identification. In each case, it is only by looking at the pattern of effects across languages that it is possible to understand the general principle. Second, language-specific processing can certainly shed light on the universal model of language comprehension. This second point is illustrated by studies of the exploitation of vowel harmony in the lexical segmentation of Finnish, of the recognition of Dutch words with and without vowel epenthesis, and of the contribution of different kinds of lexical prosodic structure (tone, pitch accent, stress) to the initial activation of candidate words in lexical access. In each case, aspects of the universal processing model are revealed by analysis of these language-specific effects. In short, the study of spoken-language processing by human listeners requires cross-linguistic comparison.
This research evaluated Grodzinsky's (1984, 1986a) syntactic loss and Kolk and van Grunsven's (1985) working memory impairment explanations of syntactic comprehension deficits in agrammatic aphasics. Four aphasic patients were evaluated who showed different patterns of impairment on morphological and structural aspects of production. The comprehension tasks compared performance on full and truncated passive sentences. The syntactic loss hypothesis predicted worse performance on truncated than full passives, while the working memory deficit hypothesis predicted the reverse. Neither hypothesis was supported, as the patients performed at a similar level on both types of passives. In addition, there was little relation between the patients' production indices and their comprehension level. The results argue against any global theory of agrammatism that attempts to attribute all agrammatic speech and co-occurring syntactic comprehension deficits to the same source.
In this paper the results of a study of objective quality measures for a broad range of coding systems are presented. These objective measures take the linear and the nonlinear distortions of the coder into account. A correlation analysis was performed, in order to find out those measures which are most effective in predicting perceivable parametric attributes of speech quality. Furthermore, we describe the test signal we have used in our study, which was not natural speech but a speech-model process.
This paper deals with flexible information retrieval. A method of gradual relevance feedback is presented. Users give gradual preference to documents retrieved by the system. They can express their preferences by 'this document is averagely relevant' or 'one document is very relevant'.
When designing multimodal systems, the designer faces the problem of the choice of modalities to optimize the system usability. Based on modeling at a high level of abstraction, we propose an evaluation of this choice during the design phase, using a multicriteria principle. The evaluation focuses on several points of view simultaneously, weighted according to the environment and the nature of the task. It relies on measures estimating the adequacies between the elements involved in the interaction. These measures arise from a fine decomposition of the interaction modalities.
Some considerations in the normalisation of tone are discussed, and their application demonstrated on the fundamental frequency data of seven speakers of a variety of Wu Chinese. The derivation of a possible Linguistic-Phonetic representation for comparison with other varieties is also illustrated.
The acoustic echo canceler using an adaptive transversal filter and the least mean-square (LMS) algorithm is the most effective technique to reduce acoustic echoes in a hands-free telephone system. However, the requirement of a very high order filter for each microphone results in difficulties in convergence and hardware implementation. In this paper, the performance of the finite length adaptive filter is studied. A formula which relates the echo cancellation to the filter size, N, is established. Detailed analysis shows that this finite filter length will have better performance using speech than white noise.
Second order methods in signal processing for spatial analysis are limited by the correlation between sources. In fact, the sources must emit independent signals in order to satisfy the exigences of theses methods as well as to obtain high resolution. The method proposed here, increases the rank of the matrix that contains the vectors used to reform the source sub-space basis. This is like a source decorrelation. The advantage of this new method is that it continues to separate with a high resolution sources that are totally correlated unlike the popular method of spatial smoothing. This article shows how the DEESE algorithm use translational invariance and directional invariance of the linear arrays for the recuperation of an orthonormal source sub-space basis. A realistic simulations are also presented to confirm the correct position of source estimations and high resolution factors.
This paper attempts to compare the performance of two popular speech coders, namely regular pulse excitation with long-term predictor (RPE-LTP) and code-excited linear prediction (CELP), in both random and burst error environments. In simulation, the bit stream generated by each coder is corrupted with error patterns obtained from the burst error model. The burst error model consists of two parts: a Rayleigh fading envelope and an M-ary in differential phase shift keying reciever model in which the signal level is varied according to the Rayleigh fading envelope. The performance of each coder is evaluated both objectively and subjectively. The simulation results indicate that the RPE-LTP coder provides more consistent performance in the burst error environment. This paper also discusses the comparison of four channel coding techniques designed for the CELP coder.
There are no major contradictions between Henri Ey's great plea for a “psychic body” or a “becoming conscious process” and the efforts cognitivist theoricians are displaying nowadays. Organodynamism may be considered as the source of the latter's attempts as well as cognitivist theoricians may claim to take their inspiration from it. This can offer advantages and enrich the two fields if the thread of history is maintained taught.
Detailed data on voicing source characteristics are of interest in both analysis and synthesis of speech. In this study, a time-domain based method of inverse filtering was used to analyze male and female utterances produced with several voice qualities. Fourteen mono-syllabic utterances (ten in normal voicing style, and 2 each in creaky and breathy voice) by each of 8 speakers (4 men, 4 women) were filtered by a set of zero pairs corresponding to the measured formants. The resulting differentiated flow waveforms were analyzed in the time domain and in the frequency domain at voicing onset, near the middle of the vowel, and near the end of voicing. Though there was variability among subjects of the same sex, the women tended to have shorter closed quotients and longer return quotients (the period between maximal rate of change of flow and minimal flow). In the spectral domain, there tended to be less energy at higher frequencies in the middle of the vowel for the women compared to the men. The magnitudes of the male-female differences are similar to those observed for the creaky-normal voicing differences and breathy-normal differences. These differences may arise from a combination of biological, sociological and acoustical effects.
This paper presents a neural network methodology called «yprel networks». After relating the main characteristics of the approach, we shall detail the incremental learning methodology used to improve the performances, which is based on the relearning phases from the classification errors. The results obtained on a characters recognition problem are then discussed.
Among the extensive correspondence of Timothy I, Catholicos of the Church of the East, are two letters which refer to his collobaration in a translation of Aristotle's Topics into Syriac and Arabic, commissioned by the Caliph al-Mahdī. An annotated English translation of both letters is provided.
To increase the believability and life-likeness of Embodied Conversational Agents, we introduce a behavior synthesis model for the generation of expressive gesturing. A small set of dimensions of expressivity is used to characterize individual variability of movement. We empirically evaluate our implementation in two separate user studies. Interaction effects between different parameters need to be studied further.
This article aims at providing a synthesis of the musical applications of digital signal processing, of related research issues, and of future directions that emerge from recent works in that field. After introducing preliminary notions related to the music technical system and to the analysis of different digital representations of music information, it focuses on three main function types: audio synthesis and processing, sound spatialization and audio indexing and access technologies.
The Perceval printed in 1530, which collects under a single title the prose adaptations of five works related to the Conte du Graal of Chrétien de Troyes, is based on a manuscript that has since been lost. The aim of this article is to verify, by the means of a survey of the entire text, the possible relationship with any of the existing manuscripts of the verse romances. Although neither of these constitutes «the» source of the prose writer, the 1530 adaptation does shed light on the poems' textual tradition as well as suggest some variants which may stem back to the «originals».
This article states of the art about tools and techniques for images documents analysis and recognition.
The experience demonstrated that the implementation of an efficient knowledge management pass by the setting up of a knowledge cartography. Thus, the cartography of knowledge is a means of «cognitive» navigation to reach resources of a knowledge heritage of an organization, that it is implicit or explicit. Besides, it permits to have a fine understanding, by an analysis of criticity, of knowledge domains on which must be efforts made in term of capitalization, sharing or innovation. We present in this article a methodology and tools offered to build such a cartography.
The paper describes a method for automatically segmenting a database of isolated words as required for the purpose of speech synthesis. The phoneme-like units in the phonetic transcription of the utterances are represented by dedicated hidden Markov models (HMMs) and segmentation is performed by aligning the speech signal against the sequence of HMMs representing the words. The specific advantage of the method presented here is that it does not need manually segmented speech material to initialize the training of the HMMs. Therefore, it can be regarded as an improved variant of established techniques for automatic segmentation. The problem of proper initialization of the HMMs without resorting to manually segmented material is solved by a hierarchical approach consisting of three successive steps. In the first step a segmentation in broad phonetic classes is realized that provides anchor points for the second stage, consisting of a sequence-constrained vector quantization. In this stage each broad phonetic class is further segmented into its constituent phonemes. The result is a crude phonetic segmentation which is then used as initialization of the HMMs in the last stage. Fine-tuning of the models is realized via Baum-Welch estimation. The final segmentation is obtained by Viterbi alignment of the utterances against the HMMs.
Our basic representation of the data is a Galois lattice, i.e. a lattice in which the terms of a representation language are partitioned into equivalence classes w.r.t. their extent (the extent of a term is the part of the instance set that satisfies the term). We propose here to simplify our view of the data, still conserving the Galois lattice formal structure. For that purpose we use a preliminary partition of the instance set, representing the association of a type to each instance. By redefining the notion of extent of a term in order to cope, to a certain degree (denoted as a), with this partition, we define a particular family of Galois lattices denoted as Alpha Galois lattices.
In this paper we present a solution to the nonlinear spectral estimation problem for speech enhancement. We start from a rather simple statistical model (log-normal) for the short time spectral estimates of speech and noise. By empirical data generation and curve fitting approaches we are able to get explicit, though simple, expressions for the MMSE estimator in function of input level and the model parameters for each frequency component. The great advantage of our approach is that it has a sound theoretical foundation, is general by the choice of its parameters, and almost as simple to use as classical spectral subtraction. Moreover, using a neural network as function approximator, which is found to be the best for our curve fitting problem, other model based MMSE estimators can be readily implemented with the proposed approach.
We introduce an estimate of the L2-probabilistic dependence measure constructed with the generalized Fourier series which is able to realize a linear vector feature dimensional reduction in the discriminate multi-class problem. We compare the proposed algorithm with the well known linear discriminate analysis (LDA) and with a generalized version of a multi class recursive linear extractor based on the L2-probabilistic dependence measure (R1D L2-PMD). For vector Gaussian mixtures such comparison is done in the mean of the probability error of classification which is estimated by a multivariate Kernel probability density function. The corresponding smoothing parameters are optimized analytically in the sense of the Mean Integrated Square Error (MISE). The non Gaussian case is evaluated with the error of the к nearest neighborhood classifier. Finally we will illustrate the importance of the proposed method by testing it in the context of the face recognition
Most biologists and some cognitive scientists have independently reached the conclusion that there is no such thing as learning in the traditional “instructive” sense. This is, admittedly, a somewhat extreme thesis, but I defend it herein the light of data and theories jointly extracted from biology, especially from evolutionary theory and immunology, and from modern generative grammar. I also point out that the general demise of learning is uncontroversial in the biological sciences, while a similar consensus has not yet been reached in psychology and in linguistics at large. Since many arguments presently offered in defense of learning and in defense of “general intelligence” are often based on a distorted picture of human biological evolution, I devote some sections of this paper to a critique of “adaptationism,” providing also a sketch of a better evolutionary theory (one based on “exaptation”). Moreover, since certain standard arguments presented today as “knock-down” in psychology, in linguistics and in artificial intelligence are a perfect replica of those once voiced by biologists in favor of instruction and against selection, I capitalize on these errors of the past to draw some lessons for the present and for the future.
In this paper we propose two forms of hybridization between a metaheuristic (Ant Colony Optimization or Genetic Algorithm) and an exact method (ILP) for the solution of the car sequencing problem. We examine whether such hybridizations can improve solution quality over what can be obtained from the alternative approach of using local search within one of these metaheuristics. The results presented show that the use of hybrid approaches provides a significant improvement and that the performance of the heuristic or metaheuristic that the hybridization is based on greatly influences the quality of the results, however, computations time are long. We conclude that diversification in a population-based metaheuristic plays an important role in the hybridization.
The prevailing approach to the acoustic-phonetic description oescription of the diphthong is based (1) on the two lowest, vocal-tract resonance (or formant) frequencies (F 1 and F 2) considered either individually or jointly in the F 1−F 2 plane, and (2) on a very sparse representation of the temporal course of these frequencies. While this time-honoured approach has been particularly useful for characterising the initial and final vowels of the diphthong, there appears to be very little progress beyond the F 2−F 2 plane, as a parametric framework for elucidating the dynamic nature of the vowel-to-vowel transition. By contrast, a more accurately spectro-temporal description of a subset of the Australian English diphthongs () is obtained in this work by considering a detailed, temporal representation of the three lowest formant-frequencies (F 1, F 2 and F 3). In particular, certain nonlinearity features of the densely-sampled contour of the F 3 are highlighted, which appear to have hitherto been either unknown or considered inconsequential to the specification of the diphthong. This finding is shown to contribute a new, three-dimensional (F 1−F 2−F 3) perspective on the acoustic characteristics of the vocalic transition of the diphthong.
In the history of the enunciative theories of the twentieth century, Charles Bally is the author of the first “general theory of enunciation”, although his name has rarely been mentioned by the theorists of the field. In the context of the study of the relations between representations and operations among several enunciation theorists, in a more or less close relationship with the image of Saussure, we plan to shed light on the most revealing points that led Bally from stylistics to the theory of enunciation, pursuing a thread that goes from the distinction between “impressions” and “idées pures” (Précis 1905) to the affective / intellectual opposition (Traité 1909), then to the treatment of expressiveness as a mechanism (Le langage et la vie 1926), to arrive at the theory of enunciation, where he defines modality. In this course, we discuss also his original interpretation of Saussurean concepts and conceptualizations, as well as Saussure's position on the status of the affectivity in linguistics.
An account is given for the evolution of strong-weak (trochaic) stress on disyllabic English nouns and weak-strong (iambic) stress on disyllabic English verbs. This explanation draws on two claims: (1) Language users adjust the stress patterns on words so that alternations between strong and weak beats are created (the principle of rhythmic alternation) and (2) Nouns and verbs tend to appear in different rhythmic contexts, such that verbs are more likely than nouns to be biased toward iambic stress. Analyses of spoken and written samples of English revealed that disyllabic verbs were more likely than disyllabic nouns to receive an inflection that adds a syllable onto the word. Because such syllables are weakly stressed, rhythmic alternation would be created if the disyllabic word received stress on the second syllable (e.g., “suggesting”) rather than the first (“promising”). Two experiments showed that stress assignments on pseudowords such as “cortand” are in fact varied depending on the syllabic nature of inflections added to the words. In addition, the text analyses and experiments can account for specific subpatterns within the noun-verb stress asymmetry as well as the general asymmetry itself. Implications of these findings for theories of word stress are discussed, as well as the more general point that patterns of language change can be understood in terms of language processing at the level of the individual speaker or listener.
A 16 kbit/s speech codec with low complexity and low signal delay is presented which is a special version of the Regular-Pulse Excitation LPC approach (RPE-LPC). This proposal is the basis for the codec standard which will be used in the future Pan-European digital mobile telephone system. An experimental hardware model is described.
One way to lower the coding rate of CELP coders is to lengthen the excitation analysis frame size. For enhanced speech quality in such a case it is desirable to have the CELP excitation peaky (or sharpened). Based on this observation, we propose a new adaptive source in which samples of the source have different gains according to their amplitudes by a two-tap pitch predictor. Simulation results show that peaky pulses at voiced onset and a burst of plosive sound are clearly reconstructed, and that in voiced sound the excitation has the desirable peaky pulse characteristic and the pitch periodicity is well reproduced.
A recursive algorithm (the recursion is on the order of model) related to the estimation of the coefficients of an autoregressive model based on maximozation of likelihood is presented. The estimated coefficients are used to obtain the soatial power densities by means of maximum of entropy method. Lastly, simulation results are presented for comparison with Durbin-Levinson's algorithm.
While endmembers are often extracted using a geometric approach, the abundances are usually estimated by solving an inverse problem. In this paper, we bypass the problem of abundance estimation by using a geometric point of view. The proposed framework shows that a large number of endmember extraction techniques can be adapted to jointly estimate the abundance fractions, with no additional computational complexity. This is illustrated in this paper with the N-Findr, SGA, VCA, OSP, and ICE endmember extraction techniques. A nonlinear extension is also proposed, using non linear dimension reduction methods such as MDS, LLE and ISOMAP. These strategies maintain the geometric unmixing algorithms unchanged, for endmember extraction as well as abundance fraction estimation. The relevance of the proposed approach is illustrated through experiments on synthesized data and real hyperspectral image.
We propose two strategies for experiment selection in the context of batch mode reinforcement learning. The first strategy is based on the idea that the most interesting experiments to carry out at some stage are those that are the most liable to falsify the current hypothesis about the optimal control policy. We cast this idea in a context where a policy learning algorithm and a model identification method are given a priori. The second strategy exploits recently published methods for computing bounds on the return of control policies from a set of trajectories in order to sample the state-action space so as to be able to discriminate between optimal and non-optimal policies. Both strategies are experimentally validated, showing promising results.
A Two-Level Time-Delay Neural Network (TLTDNN) technique has been developed to recognize all Mandarin Finals of the entire Chinese syllables. The first level discriminates the vowel-group based on (a, e, i, o, u, v) and the nasal-group based on nasal ending (-n, -ng, -others). The nasal-group discriminator is used to further split the large /a/ subgroup produced by the vowel-group discriminator. The two groupings in the first level produce 8 subgroups in the second level. Further discrimination in the second level enables the identification of all 35 Mandarin Finals. The technique was thoroughly tested using 8 sets of 1265 isolated Hanyu Pinyin syllables, with 6 sets used for training and 2 sets used for testing. The overall result shows that a high recognition rate of 99.4% on the training datasets and 95.6% on the test datasets, is achievable. The top 4 recognition rate attained on the test datasets is as high as 99.1%.
In this paper, we develop different mathematical models in the framework of the multi-stream paradigm for noise robust automatic speech recognition (ASR), and discuss their close relationship with human speech perception. Largely inspired by Fletcher's “product-of-errors” rule (PoE rule) in psychoacoustics, multi-band ASR aims for robustness to data mismatch through the exploitation of spectral redundancy, while making minimum assumptions about noise type. Previous ASR tests have shown that independent sub-band processing can lead to decreased recognition performance with clean speech. We have overcome this problem by considering every combination of data sub-bands as an independent data stream. After introducing the background to multi-band ASR, we show how this “full combination” approach can be formalised, in the context of hidden Markov model/artificial neural network (HMM/ANN) based ASR, by introducing a latent variable to specify which data sub-bands in each data frame are free from data mismatch. This enables us to decompose the posterior probability for each phoneme into a reliability-weighted integral over all possible positions of clean data. This approach offers great potential for adaptation to rapidly changing and unpredictable noise.
When several different acoustic cues contribute to the perception of a phonetic distinction, a trading relation among the cues can be demonstrated in an identification task as long as the speech stimuli are phonetically ambiguous. The present experiments address the question of whether the cues trade also in unambiguus stimuli, using a fixed-standard AX discrimination task with stimuli either from the vicinity of the phonetic category boundary or from within a phonetic category. The results suggest that four of the five trading relations examined are tied to the perception of phonetic constrasts; they disappear or reverse within categories. The one predicted exception represents a trading relation presumed to originate at a psychoacoustic level. The data severely restrict psychoacoustic explanations for these effects and also suggest that within-category discrimination is not achieved in a phonetic mode of perception, thus affirming a dual-process view of speech discrimination.
The Multi-Layer Perceptron (PMC in French) is one of the neural networks the most widely used, particularly for supervised classification. First, existing results on general representation capabilities enjoyed by the PMC architecture are surveyed, independently of any learning algorithm. Then it is shown why the minimization of a quadratic error over the learning set seems an awkward optimization criterion, though some asymptotic properties are also proved. In a second stage, the bayesian approach is analyzed when learning sets of finite size are at disposal. With the help of certain density estimators whose basic properties are emphasized, it is possible to build a feed-forward neural network implementing the bayesian classification. This technique of direct discrimination seems to perform better than the classical MLP in all respects despite of the similarities of the architectures.
Research has been conducted in the area of voice processing for over six decades but it has only been in the past few years that the impact of the years of research is starting to be seen in modern telecommunications systems. Virtually every area of voice processing, including speech coding, speech synthesis, speech recognition, and even, to a small extent, speaker verification, has left the research laboratory and now appears in a product or service that is in daily use out in the marketplace, often by millions of customers per day. This revolution in voice processing in telecommunications is fueled by algorithmic advances (which improve the quality of the voice processing systems), hardware advances (which provide high processing power and memory at low cost), and networking advances (which provide high bandwidth pipes to the home, office, and throughout the telecommunications network). In this paper we illustrate the impact of voice processing on modern telecommunications by showing the diverse ways in which speech coding, speech synthesis, speech recognition and speaker verification have become embodied in new products and services.
This paper presents our interactive tool for knowledge acquisition. This device defines operational means to locate, acquire and develop knowledge in an image processing context. Our pragmatic approach consist in using a terminology based on expertises to extract knowledge units. This linguistic analysis resuit on the development of a consensual model of scenario and the development of an interface. Then, we propose a dynamic interaction, to acquire and backup image processing knowledge in a scenarios base. Finally, the exploitation, of aiready played scenarios, hy the naive is made in an intuitive way. Thanks to a hyperholic graph, non-experts extract knowledge which are usefui to solve a new problem..
Among the main lip-geometry measures such as: • - frontal: width, height, area; • - lateral: upper, lower and corner protrusions; lip horn opening and depth; some relations are of interest for acoustic, articulatory and practical modeling. In order to maximize the proper movements of the lips themselves, a corpus was designed to explore the maximum lip manoeuvre space for French vowels (5 speakers) using naturally close jaw conditions (consonantal [s] and [∫] contexts). Proposals to calculate are from width or height alone are thus discarded. Profile protrusions are naturally strongly correlated, and so is lip horn depth with corner protrusion. So we can choose corner as the pivot in lip horn profile modeling. But, for such a modeling, the most interesting relation between front and profile views is the inverse correlation between width and protrusion (on condition that corner protrusion is chosen).
This paper gives an overview of a system for phoneme-based large-vocabulary continuous-speech recognition. The system provides the speaker dependent recognition component in the speech understanding system spicos that is designed to recognize and understand database queries spoken in natural German language. The recognition problem then amounts to an efficient search through a huge state space such that purely local decisions can be avoided and globally optimal decisions can be taken. The size of this state space depends primarily on the type of language model being used. Three types of language models are studied: no language constraints, finite state network, stochastic trigram model based on word categories. For each of the three language models, recognition experiments have been carried out on a 917-word task and 4 speakers. For each speaker, 200 sentences totalling 1391 words had to be recognized.
In this paper we report on progress made at LIMSI in speaker-independent large vocabulary speech dictation using newspaper-based speech corpora in English and French. The recognizer makes use of continuous density HMMs with Gaussian mixtures for acoustic modeling and n-gram statistics estimated on newspaper texts for language modeling. Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models. For English the ARPA Wall Street Journal-based CSR corpus is used and for French the BREF corpus containing recordings of texts from the French newspaper Le Monde is used. Experiments were carried out with both these corpora at the phone level and at the word level with vocabularies containing up to 20,000 words. Word recognition experiments are also described for the ARPA RM task which has been widely used to evaluate and compare systems.
Many Linear Prediction (LP) vocoder systems use the LP residual energy method for gain matching. Large gain errors occur however when a low frequency formant is in resonance with voiced excitation impulses. In this paper, the errors are evaluated and their fundamental causes are discussed. It is demonstrated that large errors are due to excessive spectral line suppression of the LP error signal, and to discrepancy between analysis windowing and continuous excitation in the synthesis. Finally, an existing improvement of the method is discussed in the context of the statements.
We present a new image sequence analysis method for automatic and real-time extraction of transitory and complex motions in natural scenes. We show how to extract these motions as multidimensional point clusters obtained from the temporal embedding of grey level variations, in five successive steps: embedding, fractal indexing, point chaining, cluster identification and data extraction. We develop the two main algorithms: fractal space filling indexing and chaining in order to access directly to the relevant information. To illustrate our method, we present an automatic system for early smoke source detection through the processing of landscape images by extracting fugitive and various movements within a small spot of pixels affected by the smoke. We show how to modify the embedding technique used to obtain the data points coordinates to produce many other applications for the fractal embedding method, for example the recognition of complex moving or varying shapes objects.
While reporting on work done to elucidate the structuring-up process of acoustic space vs. time data-information with in the cochlea, the present article stresses the paramount importance of the phase effect within the ear. This work is based on results secured with a peripheral-hearing model previously tested for reliability. Here, this model is broken up into mutually independent parts for the specific function each fulfils between the external ear and the nerve fiber endings. This model is described through a set of equations that apply to acoustic wave propagation, to the mechanicl vibration of the basilar membrane and to electro-mechanical transduction in hair cells and nerve fibers. Representing information that runs inside the cochlea becomes a crucial problem when trying to detect pertinent acoustic cues to be used in signal analysis. Moreover, for an improved perceptual quality, (e.g. in speech synthesis), phase is a definitely non-negligible factor. Therefore, while the sonagram should be retained as an indispensible instrument in monitoring intensity evolution, it is equally imperative to secure a mode of sample-by-sample visualization of other relevant phenomena.
Mathematical morphology is based on the concept of ordering. With color image process, write a valid order relation requires to using distances from standard color spaces CIELAB or CIELUV. Since the first recommendations of the CIE (International commission on illumination), several colors distances have been proposed. The aim of this paper is studying the impact of each color distances in the context of color mathematical morphology. The results are developed for a new construction of morphological operators based on color distances in CIELAB space. A criterion to evaluate methods of color ordering is then proposed to compare the main approaches in mathematical morphology with those based on a distance function.
Island-driven parsers have interesting potential applications in Automatic Speech Understanding (ASU). Most of the recently developed ASU systems are based on an Acoustic Processor (AP) and a Language Processor (LP). AP computes the a priori probability of the acoustic data given a linguistic interpretation. LP computes the probability of the linguistic interpretation. This paper describes an effort to adapt island-driven parsers to handle stochastic context-free grammars. These grammars could then be used as Language Models (LM) by LP to compute the probability of a linguistic interpretation.
The intelligibility of initial and final consonants in monosyllabic CVC utterances was measured for rule-synthesized speech (dyadic concatenation). In order to evaluate the intelligibility results, two additional speech conditions were tested PCM-coded speech (12 bits, 10 kHz sampling rate), and LPC-resynthesized speech (12 coefficients). The stimulus set consisted of all possible initial and final consonants appearing in nine different CVC contexts, namely with three vowels /i, u, /, and with three nonneighboring consonants /p, t, k/. The subjects had to identify the initial or final consonant in each word and could choose any single consonant as a response. The overall percentage correct scores, averaged over 33 listeners, for the initial and final consonants, were 93% and 92.8% for PCM-coded speech, 86.7% and 85.4% for LPC-synthesis, and 58.2% and 73.5% for rule-synthesized speech. Apart from these scores also the actual confusions are discussed in some detail. More important than the scores achieved for this particular, meanwhile improved, version of the rule-synthesis system, was the insight gained in how to evaluate and improve such systems.
This paper presents a novel algorithm which generates three-dimensional face point trajectories for a given speech file with or without its text. These codebooks consist of both acoustic and visual features. Acoustics are represented by line spectral frequencies (LSF), and face points are represented with their principal components (PC). During the synthesis stage, speech input is rated in terms of its similarity to the codebook entries. Based on the similarity, each codebook entry is assigned a weighting coefficient. If the phonetic information about the test speech is available, this is utilized in restricting the codebook search to only several codebook entries which are visually closest to the current phoneme (a visual phoneme similarity matrix is generated for this purpose). Then these weights are used to synthesize the principal components of the face point trajectory. The performance of the algorithm is tested on held-out data, and the synthesized face point trajectories showed a correlation of 0.73 with true face point trajectories.
In a real situation, the choice of the best representation for the implementation of a signature verification system able to cope with all types of handwriting is a very difficult task. This study is original in that the design of the integrated classifiers is based on a large number of individual classifiers (or signature representations) in an attempt to overcome in some way the need for feature selection. In fact, the cooperation of a large number of classifiers is justified only if the cost of individual classifiers is low enough. This is why the extended shadow code (ESC) used as a class of shape factors tailor-made for the signature verification problem seems a good choice for the design of integrated classifiers E(x).
A sub-band multisensor structure using intermittent adaption is proposed for speech enhancement. The convergence of the proposed method is compared with conventional LMS and frequency domain LMS and a dramatic increase in convergence rate is shown using both simulated and real data. Preliminary investigation of sub-band filter order is also reported.
The analysis of the acoustic parameters which best summarize the cues to phone discrimination for the language under consideration should be a previous step in acoustic-phonetic decoding, regardless of the methodology to be used. The Spanish language has not been widely analyzed from this point of view. This work deals with the acoustic discrimination of Spanish stop consonants. Our main goal was to find a reliable and reduced set of parameters for place of articulation identification of Spanish unvoiced stops. On the basis of the obtained parameters, two automatic classifiers were developed and tested. Only the acoustic features of the burst segment, automatically segmented from the speech waveform, were considered in the parameter estimation. The analysis of these features was carried out in both the time and frequency domains over a CV context corpus uttered by 6 speakers. In the first case, the classifier was designed as a procedural form. Alternatively, in the second case a statistical classifier was obtained from a previous automatic discriminant analysis of the parameters. Both classifiers were tested over a CV context corpus uttered by 40 new speakers not included in the analysis corpus, which resulted in a good rate of identification.
The proposed algorithm computes the coarse likelihood score for each word in a lexicon using the observation probabilities of speech spectra and duration information of recognition units. With the proposed approach we could reduce the computational amount by 74% with slight degradation of recognition accuracy in an 1160-word recognition system based on the phoneme based hidden Markov modeling (HMM). Also, we observed that the proposed coarse likelihood score computation algorithm is a good estimator of the likelihood score computed by the Viterbi algorithm.
Jitter measures are known to discriminate between normal and dysphonic speakers. Two comparative studies were carried out. The first showed that as far as inter-vowel quality differences were concerned, all significant differences could be related to the idiosyncratic behaviour of several preprocessing schemes with reference to vowel quality. Intrinsic differences were canceled out by normalizing absolute jitter by the average fundamental period. As a rule of thumb preprocessing routines were more successful, the further F 0 and F 1 were apart. In a second experiment, jitter values extracted from connected speech did not discriminate between normal and dysphonic speakers any more efficiently than values calculated from sustained vowels. As far as our corpora were concerned, no intrinsic superiority in the discrimination performance of connected speech as opposed to sustained vowels could be found. In the case of running speech absolute microperturbation values appeared to be higher during inter-segment transitions and during voice onset and offset.
This paper presents a theoretical framework for environment normalization training and adaptation in the context of mixture stochastic trajectory models. The presented approach extends, to segment based models, the currently successful technique of environment normalization used in adapting Hidden Markov models. It also adds to the environment normalization framework a novel method for representing and combining different sources of variability. In our approach the normalization and adaptation are performed using linear transformations. When applied to speaker and noise adaptation in a continuous speech recognition task, our method led to up to 34% improvement in the recognition accuracy for speaker adaptation compared to unadapted models. For noise adaptation the technique outperformed environment dependent models for some of the tested cases. It was also observed that using environment normalization training in conjunction with transformation adaptation outperforms conventional MLLR.
In systems theory, pure anticipation seems the best mathematical solution for the problem of control, whatever the inputs are known a priori (case of the systems of regulation), or not (case of tracking systems). From a formal point of view, this is obtained by putting the system to be controlled in cascade with its opposite system. However, this solution is not realizable physically, because the model of the system is rarely complete. Moreover, the opposite system is often unstable. Lastly, the disturbances observed, either at the output, or at the input, make difficult the access to a satisfactory solution. Last aspect: to be exploitable, anticipation has to be made in real-time. The method described here tries to bring a fast and robust solution to the problem of prediction. This method uses at the same time the geometrical properties of the signal to be predicted at the considered moment (local procedure), and a learning base constituted by past observations (global procedure). The performances of this predictor are next evaluated on several significative examples and are compared with those of others predictors.
This paper addresses the impact of telephone transmission channels on automatic speech recognition (ASR) performance. A real-time simulation model is described and implemented, which allows impairments that are encountered in traditional as well as modern (mobile, IP-based) networks to be flexibly and efficiently generated. The model is based on input parameters which are known to telephone network planners; thus, it can be applied without measuring specific network characteristics. It can be used for an analytic assessment of the impact of channel impairments on ASR performance, for producing training material with defined transmission characteristics, or for testing spoken dialogue systems in realistic network environments. In the present paper, we present an investigation of the first point. Two speech recognizers which are integrated into a spoken dialogue system for information retrieval are assessed in relation to controlled amounts of transmission degradations. The measured ASR performance degradation is compared to speech quality degradation in human-human communication. It turns out that ASR shows a different behavior than expected human quality judgments for some impairments. This fact has to be taken into account in both telephone network planning as well as in speech and language technology development.
Results are presented which show that performance using the variable frame rate technique and triphone models can be better than that obtained using triphone models and full frame rate data. The variable frame rate technique requires considerably less processing time.
The inverse problem for the vocal tract is under consideration from the viewpoint of the ill-posed problem theory. The proposed approach, which permits overcoming the difficulties related to ambiguity and instability, is based on the variational regularization with constraints. The work of articulators is used as a functional of regularization and a criterion of optimality for finding an approximate solution. The measured acoustical parameters of the speech signal serve as external constraints while the geometry of the vocal tract, the mechanics of the articulation, and the phonetic properties of the language play the role of internal constraints. An effective numerical implementation of the proposed approach is based on a local piecewise linear approximation of the articulatory-to-acoustics mapping and a polynomial approximation of the discrepancy measure. A heuristic method named the “calibrating curves method” is applied for estimating the accuracy of the obtained approximate solution. It was shown that in some cases the error of the inverse problem solution is weakly dependent on the errors of formant frequency measurements. The vocal tract shapes obtained by virtue of the proposed approach are very close to those measured in X-ray experiments.
Knock is a well-known problem for spark-ignition engine manufacturers. Knock detection helps achieve the best compromise between increasing engine efficiency, fuel consumption and present requirements with regard to exhaust emission legislation. The ignition timing is usually controlled so that knock never occurs even with fuel quality changes. The advantage of a knock detection method is to work within close range of knocking conditions but avoid its occurrence. The purpose of our study consists in highlighting several knock intensities from block vibration signals provided by an accelerometer. Our aim is to differentiate three kinds of engine cycles: absence of knock, increasing knock and heavy knock. The developped diagnostic approach deals with fuzzy pattern recognition. The method, experimented on a learning set, leads to several diagnoses that cooperate.
This paper investigates the impacts of standardisation and vehicularizationon linguistic diversity. We demonstrate in the light of Tɔŋúgbe in south-eastern Ghana and vehicular Fula in Northern Cameroon that natural languages are social objects in motion, which can be institutionally and/or socially subjected to promotion or demotion. In the first part, we show that Tɔŋúgbe, one of the dialects of the Ewe language, brings to the fore data that is not captured by standard Ewe even though such data is critical to the understanding and study of not just the Ewe language, but also, the whole Gbelanguage cluster. We concentrate on the definite article in the dialect and demonstrate that, contrary to what pertains in standard Ewe, NP determination with the definite article in Tɔŋúgbe is an intersection between syntax and phonology. In the second part we argue that vehicularizationas in the case of Fula Adamawa, appears to be a double- egged sword which on one hand promotes a language or a variety among others and, on the other hand, causes the demotion of minority languages or varieties.
An optimum coding of the parameters of a formant speech synthesizer is proposed. The optimisation of these control parameters coding is based on statistical and subjective criteria. The synthesizer used is a parallel synthesizer capable of synthesizing high quality speech whether voiced or unvoiced. The utterances chosen for experimentation are groups of high quality synthetic French CVCV which represent French speech faithfully. The first group of utterances consists of voiced stops (b, d and g with the vowels a, u and i) while the second group consists of voiced fricatives (З and z with the vowels a, u and i). The proposed procedure consists of four steps. The first one is a statistical study carried out on the control parameters of the synthetic utterances in order to find the optimum effective dynamic range of each parameter. During the second step, the minimum number of bits necessary for quantizing each parameter independently (with no noticeable degradation) is found. The third step finds the minimum number of bits when all the parameters are quantized simultaneously. This is done by regrouping the parameters in sub-groups and finding the minimum number of bits when applying the quantization of the parameters of the sub-group simultaneously. Then, we regroup the sub-groups until we find the optimum number of bits when all parameters are applied simultaneously. The final step is to find the optimum sampling rate of each interval of each utterance. A variable sampling interval is proposed that depends on the nature of speech events.
Applications that rely on mobile devices for user interaction must be mindful of the user's limited attention, which will typically be split between several competing tasks. We propose a more dynamic context-driven approach to content delivery. We demonstrate our approach using Scatterbox, a pervasive computing application we have developed which performs sensor fusion to derive a user's current situation. Based on the user's level of interruptibility, Scatterbox prioritises and forwards relevant messages to their mobile phone. We draw conclusions from a preliminary evaluation of the system.
This article presents several applications of the clustering criterion «K-products» introduced in [10]. This criterion is applied to the unsupervised mixture estimation, the straight lines extraction in binary images and the blind channel estimation in radiocommunications. Some optimization algorithms are proposed and comparisons with other known methods are presented.
From a number of linear prediction parametric representations each of which furnishes equivalent information about the linear predictor, the cepstral coefficients representation is known to provide the best speech recognition performance. Since the cepstral coefficients of a stable all-pole filter are inversely proportional to their quefrencies, these coefficients are multipled by their respective quefrencies. The quefrency-weighted cepstral coefficients (also known as the root-power sums) are studied as to their effectiveness in a vowel recognition experiment and found to perform better than the cepstral coefficients with a Euclidean distance measure.
Our research project aims at elaborating hybrid Organizational Semantic Webs (O-SWs), based on a strong coupling between a Knowledge Base (KB) and a Documents Base (DocB). The capitalized knowledge is hence distributed among the KB and the DocB. The interest of modeling pieces of knowledge is that it allows the OSW to reason about this knowledge in order to assist users in their knowledge management tasks. In counterpart the distribution of knowledge among heterogeneous sources renders its access more complicated. In order to overcome these difficulties, we propose, on the one hand, to introduce a model of the information contained in the OSW, while making abstraction ofits mode ofspecification, and, on the other hand, to couple this model with a mechanism ofdynamic generation ofpresentations oftargeted information whose contents is adapted to the user. In this paper, we present an overview ofthis approach.
Aerodynamic simulations of /aCa/ utterances were made using a low-frequency model for upper vocal tract airflow and a two-mass model for the voice source. These simulations helped increase insight into the results of an empirical study of flow during running speech. The various sources of flow, including wall compliance, were examined for their contributions to total flow from the mouth. The two-mass model was modified to allow for more natural glottal flow during abduction and adduction. Even with modifications the two-mass model was not sufficient to model source variations during running speech.
By means of a space-diversity technique which consists in using several identical subarrays (e.g. belonging to a single linear arrays composed of equispaced sensors), the propagator can be extracted from a single short data record and then performs goniometry of remote sources. Thus the cases of non-stationary situations and fully correlated incident wavefronts can be handled, given that only the vector of the received signals is exploited, not their cross-spectral matrix. In this method, a SNR per source at least equal to 0 dB is required. Nevertheless, if a certain temporal coherence exists for the sources during a time interval corresponding to about thirty of forty snapshots, then the robustness can be greatly improved with respect to noise, using a technique which consists in applying an exponential memory to the vector of the received signals. This superdirective method provides the phase of the incident wavefronts which can be used to estimate Doppler shifts.
The NOISEX-92 experiment and database is described and discussed. NOISEX-92 specifies a carefully controlled experiment on artificially noisy speech data, examining performance for a limited digit recognition task but with a relatively wide range of noises and signal-to-noise ratios. Example recognition results are given.
Research on spoken languages has shown that the durations of silent pauses in a sentence are strongly related to the syntactic structure of the sentence. A similar analysis of the pauses (holds) in a passage in American Sign Language reveals that sequences of signs are also interspersed with holds of different lengths: long holds appear to indicate the ends of sentences; shorter holds, the break between two conjoined sentences; and the shortest holds, breaks between internal constituents. Thus, pausal analysis is a guide to parsing sentences in ASL.
In this paper we describe a technique that we developed for enhancing speech signals degraded by additive non-stationary noise. The performance of the technique is evaluated in the context of a speech recognition task on connected digits corrupted by different types of noise representative of military environments. The algorithm is based upon spectral amplitude estimation of the speech signal given state-dependent parametric speech and noise models. The spectral analysis is performed by a resonator based frequency interpolation filterbank whose parameters are selected according to the nature of the noise process. The models are ergodic hidden Markov models (HMMs) with Gaussian multivariate distributions trained on noise and speech samples.
In this paper we propose a scheme for developing a voice conversion system that converts the speech signal uttered by a source speaker to a speech signal having the voice characteristics of the target speaker. In particular, we address the issue of transformation of the vocal tract system features from one speaker to another. Formants are used to represent the vocal tract system features and a formant vocoder is used for synthesis. The scheme consists of a formant analysis phase, followed by a learning phase in which the implicit formant transformation is captured by a neural network. The transformed formants together with the pitch contour modified to suit the average pitch of the target speaker are used to synthesize speech with the desired vocal tract system characteristics.
This paper presents the adaptation of Fujisaki's quantitative model to the analysis of German intonation and its application to F 0 synthesis by rule. The parameter values of the model are determined by an automatic approximation of naturally produced F 0 contours. The potential sources of variation of the parameter values are examined using statistical methods. A set of rules is formulated that capture the effects of both linguistic and speaker-dependent features. The rules generate artificial intonation contours which in turn can be related to linguistic features such as sentence mode or word accent. Acceptability of the rule-generated intonation patterns as well as the adequate modelling of linguistic prosodic properties are evaluated perceptually by both phonetically trained subjects and prosodically “naive” listeners. In general, utterances resynthesized with rule-generated F 0 contours are judged highly acceptable and natural by both groups of listeners.
The line spectrum pair (LSP) frequency representation has recently been proposed as an alternative linear prediction (LP) parametric representation. In the context of speech coding, this representation shows better quantization properties than the other LP parametric representations. In the present paper, the LSP representation is studied for speech recognition. Several distance measures based on this representation are investigated on a steady-state vowel recognition task. The weighted LSP distance measure is found to result in the best performance. The performance of the weighted LSP distance measure is compared with that of the other popular LP distance measures (such as the Itakura, cepstral, weighted cepstral, root-power-sum, log area ratio and reflection coefficient distance measures). The weighted LSP distance measure is found to perform significantly better than these popular LP distance measures.
The clustering methods are data mining tools, which aim at identifying groups of similar objects compared to the values they take on different attributes. The methods known as conceptual clustering associate with the partition an interpretation of the classes. Such a structuring can be described by a pair of partitions, called bipartition, composed of a partition of objects and a partition of attribute value pairs. This article presents a study of the use of local search methods to produce bipartitions maximizing a quality measure defined by Goodman and Kruskal [GOO 54, GOO 59]. We propose an algorithm based on a stochastic neighborhood. The stopping criterion is redefined in a statistical way to guaranty the quality of the obtained solution with respect to a confidence level. Finally, a variational study of the function to be optimized leads to a complementary reduction of the computational complexity.
This contribution aims at evaluating the use of pronunciation variants for different recognition system configurations, languages and speaking styles. To measure the need for variants we have defined the variant2+ rate which is the percentage of words in the corpus not aligned with the most common phonemic transcription. This measure may be indicative of the possible need for pronunciation variants in the recognition system. Pronunciation lexica have been automatically created so as to include a large number of variants (overgeneration). In particular, lexica with parallel and sequential variants were automatically generated in order to assess the spectral and temporal modeling accuracy. We first investigated the dependence of the aligned variants on the recognizer configuration. Then a cross-lingual study was carried out for read speech in French and American English using the BREF and the WSJ corpora. A comparison between read and spontaneous speech was made for French based on alignments of BREF (read) and Mask (spontaneous) data. Comparative alignment results using different acoustic model sets demonstrate the dependency between the acoustic model accuracy and the need for pronunciation variants. The alignment results obtained with the above lexica have been used to study the link between word frequencies and variants using different acoustic model sets.
When identifying pairs of simultaneous steady-state vowels, listeners perform well even when the two vowels start and stop at the same time, are presented monaurally, have the same fundamental frequency (f 0), and have approximately equal intensities. The sensation described by listeners is of one dominant vowel “coloured” by a second or non-dominant vowel. A small difference in f 0 improves performance and typically results in a sensation of two voice sources rather than of one voice coloured by another. Dominance may reflect cognitive “decision” strategies in addition to spectral masking at the level of the peripheral auditory system.
An algorithm is proposed which automatically estimates the local signal-to-noise ratio (SNR) between speech and noise. The feature extraction stage of the algorithm is motivated by neurophysiological findings on amplitude modulation processing in higher stages of the auditory system in mammals. It analyzes information on both center frequencies and amplitude modulations of the input signal. This information is represented in two-dimensional, so-called amplitude modulation spectrograms (AMS). A neural network is trained on a large number of AMS patterns generated from mixtures of speech and noise. After training, the network supplies estimates of the local SNR when AMS patterns from “unknown” sound sources are presented. Classification experiments show a relatively accurate estimation of the present SNR in independent 32 ms analysis frames.
Like all products of human activity, handwriting exhibits extreme variability. We are analysing this variability before initiating a text recognition process, a first degree of characterization for handwriting. In the case of handwriting consisting of few words, such as the literal amount of cheques, this first degree can be obtained for each word independent of its semantic signification by extracting a small number of measures. Based on the analysis of 989 handwritten amounts from cheques, it is shown that these measures are weakly correlated and define a variability space of non-uniform density, which suggests the possibility of classification of handwriting into a set of several families.
In this paper a new variant of HMM, named Multiple VQ HMM (MVQHMM), is presented. Its main characteristic is the use of a separate codebook for each model. Procedures for training and probability evaluation of these models are described. The evaluation procedure combines the quantization distortions of the vector sequences with the discrete HMM generation probabilities. Furthermore, the multiple VQ hidden Markov models seem to be more robust than the discrete and semi-continuous ones in relation to the inter-speaker variability of the recognition system.
A novel Content-Based Video Retrieval (CBVR) framework is presented in this paper: its purpose is to find similar video sub-sequences in videos. By introducing temporal flexibility in the description of video sub-sequences, this framework makes the use of flexible, but slow, distance measures (such as Dynamic Time Warping) optional. As a consequence, real-time retrieval of similar video sub-sequences, among hundreds of thousands of examples, is now possible. The proposed method is adaptive; a fast training procedure is presented. Performances have been successfully assessed on a dataset of 1,707 video clips (> 800,000 sub-sequences). Ultimately, we plan to design a real-time alert (and/or recommendation) generation system for computed-aided video-guided surgery.
Does knowledge of language consist of mentally-represented rules? Rumelhart and McClelland have described a connectionist (parallel distributed processing) model of the acquisition of the past tense in English which successfully maps many stems onto their past tense forms, both regular (walk/walked) and irregular (go/went), and which mimics some of the errors and sequences of development of children. Yet the model contains no explicit rules, only a set of neuronstyle units which stand for trigrams of phonetic features of the stem, a set of units which stand for trigrams of phonetic features of the past form, and an array of connections between the two sets of units whose strengths are modified during learning. Rumelhart and McClelland conclude that linguistic rules may be merely convenient approximate fictions and that the real causal processes in language use and acquisition must be characterized as the transfer of activation levels among units and the modification of the weights of their connections. We analyze both the linguistic and the developmental assumptions of the model in detail and discover that (1) it cannot represent certain words, (2) it cannot learn many rules, (3) it can learn rules found in no human language, (4) it cannot explain morphological and phonological regularities, (5) it cannot explain the differences between irregular and regular forms, (6) it fails at its assigned task of mastering the past tense of English, (7) it gives an incorrect explanation for two developmental phenomena: stages of overregularization of irregular forms such as bringed, and the appearance of doubly-marked forms such as ated and (8) it gives accounts of two others (infrequent overregularization of verbs ending in t/d, and the order of acquisition of different irregular subclasses) that are indistinguishable from those of rule-based theories. In addition, we show how many failures of the model can be attributed to its connectionist architecture. We conclude that connectionists' claims about the dispensability of rules in explanations in the psychology of language must be rejected, and that, on the contrary, the linguistic and developmental facts provide good evidence for such rules.
The author discusses some recent contributions to the question of the syntactical function of the wayyiqtol form (the so-called impf. cons.) which is characteristic for Biblical Hebrew narratives (Waltke/O'Connor and Joüon/Muraoka on the one hand and Niccacci on the other). He considers particularly the linguistic paradigms that underlie the contributions (»comparative linguistics«and»textual linguistics«). He criticizes the etymological explanation offered by Waltke/O'Connor, at the same time arguing that Niccacci's textual linguistic model should be modified. Finally, he tentatively suggests using the predominantly paratactic syntax of the LXX narratives as a model for modern bible translations. Accordingly, the monotonous chains of wayyiqtol initial clauses in the Hebrew narratives should be rendered by coordinated clauses in the modern target languages.
This paper deals with the extension of the classical problem of discovering rules of the form «if a then almost b» to the search of generalized rules of the form R R' where R and R' can be rules themselves. In the context of the statistical implicative analysis initially developed by Gras [GRA 79], [GRA 96], we recently proposed a first formalization based on the concept of “implicative hierarchy”. Inspired from the classical hierarchical classification, an agglomerative algorithm amalgamates rules with an incremental process. Here, we propose a new formalization which highlights the structures of the model. And, we justified the used of the term “hierarchy” by associating it with a height which satisfies the ultrametric inequality. The approach is illustrated on a survey from the French Public Education Mathematics Teacher Society.
This paper describes a spoken document retrieval (SDR) system for British and North American Broadcast News. The system is based on a connectionist large vocabulary speech recognizer and a probabilistic information retrieval (IR) system. We discuss the development of a real-time Broadcast News speech recognizer, and its integration into an SDR system. Two advances were made for this task: automatic segmentation and statistical query expansion using a secondary corpus.
One of a listener's major tasks in understanding continuous speech is segmenting the speech signal into separate words. When listening conditions are difficult, speakers can help listeners by deliberately speaking more clearly. In four experiments, we examined how word boundaries are produced in deliberately clear speech. In an earlier report we showed that speakers do indeed mark word boundaries in clear speech, by pausing at the boundary and lengthening pre-boundary syllables; moreover, these effects are applied particularly to boundaries preceding weak syllables. In English, listeners use segmentation procedures which make word boundaries before strong syllables easier to perceive; thus marking word boundaries before weak syllables in clear speech will make clear precisely those boundaries which are otherwise hard to perceive. The present report presents supplementary data, namely prosodic analyses of the syllable following a critical word boundary. More lengthening and greater increases in intensity were applied in clear speech to weak syllables than to strong. Mean F 0 was also increased to a greater extent on weak syllables than on strong. Pitch movement, however, increased to a greater extent on strong syllables than on weak. The effects were, however, very small in comparison to the durational effects we observed earlier for syllables preceding the boundary and for pauses at the boundary.
In conversation, speakers and addressees work together in the making of a definite reference. In the model we propose, the speaker initiates the process by presenting or inviting a noun phrase. Before going on to the next contribution, the participants, if necessary, repair, expand on, or replace the noun phrase in an iterative process until they reach a version they mutually accept. In doing so they try to minimize their joint effort. The preferred procedure is for the speaker to present a simple noun phrase and for the addressee to accept it by allowing the next contribution to begin. We describe a communication task in which pairs of people conversed about arranging complex figures and show how the proposed model accounts for many features of the references they produced. The model follows, we suggest, from the mutual responsibility that participants in conversation bear toward the understanding of each utterance.
When speaking to interactive systems, people sometimes hyperarticulate — or adopt a clarified form of speech that has been associated with increased recognition errors. The goals of the present study were (1) to establish a flexible simulation method for studying users' reactions to system errors, (2) to analyze the type and magnitude of linguistic adaptations in speech during human-computer error resolution, (3) to provide a unified theoretical model for interpreting and predicting users' spoken adaptations during system error handling, and (4) to outline the implications for developing more robust interactive systems. A semi-automatic simulation method with a novel error generation capability was developed to compare users' speech immediately before and after system recognition errors, and under conditions varying in error base-rate. Matched original-repeat utterance pairs then were analyzed for type and magnitude of linguistic adaptation. When resolving errors with a computer, it was revealed that users actively tailor their speech along a spectrum of hyperarticulation, and as a predictable reaction to their perception of the computer as an “at risk” listener. During both low and high error rates, durational changes were pervasive, including elongation of the speech segment and large relative increases in the number and duration of pauses. During a high error rate, speech also was adapted to include more hyper-clear phonological features, fewer disfluencies, and change in fundamental frequency. The two-stage CHAM model (Computer-elicited Hyperarticulate Adaptation Model) is proposed to account for these changes in users' speech during interactive error resolution.
Within the framework of the Dempster-Shafer theory of evidence, data fusion is based on the building of single belief mass by combination of several mass functions resulting from distinct information sources. This combination, called Dempster's rule of combination, or orthogonal sum, has several interesting mathematical properties, like commutativity or associativity. Unfortunately, it badly manages the existing conflict between the various information sources at the normalization step. The management of conflict is a major issue, especially during the fusion of many information sources. Indeed, the conflict increases with the number of information sources. That is why a strategy of conflict redistribution is essential. In this paper, we define a formalism to describe a family of combination operators. We propose to develop a generic framework in order to unify several operators. We introduce, within this generic framework, traditional combination operators used within the evidence theory. We propose other operators allowing a less arbitrary redistribution of the conflicting mass on the propositions. These various combinations operators were tested on sets of synthetic belief masses and real data.
This paper examines the emergence of the qualifications langue morte (dead language) and langue vivante (living language) in France in the 17th century. It is based on an extensive study of metalinguistic sources (grammars, dictionaries, collections of remarks, various treatises). A first part deals with the evolution of the notions used to qualify Latin, from a representation in terms of alteration and corruption, in use since the 16th century, to those of life and death. The second part shows how the development of the theory of usage is concomitant with a new valorisation of living languages. The third part shows how dictionaries of the end of the century record the opposition between langue vivante and langue morte, paving the way for a new representation of languages that would later spread to educational contexts. Through the study of this emerging paradigm, the question of the representation of languages as having regulated grammars or as changing vectors of human expression is addressed.
Local spectral distortion measures are commonly used to measure the similarity (or spectral distance) between two given short-time spectra. In this study we compared several different spectral distortion measures including the Itakura-Saito distortion measure, the log likelihood ratio (LLR) distortion measure, the likelihood ratio (LR) distortion measure, the cepstral (CEP) distortion measure, and two proposed perceptually based distortion measures, the weighted likelihood ratio (WLR) and the weighted slope metric (WSM) distortion measures, in terms of their effects on the performance of standard dynamic time warping (DTW) based, isolated word, speech recognizer. Two modifications of the basic forms of each measure were also investigated, namely a Bark-scale frequency warping and the incorporation of suprasegemental energy information. All distortion measures and their modifications were tested on an alpha-digit vocabulary, 4-talker, telephone recording data base. The results can be summarized as: (1) All LPC-based distortion measures performed reasonably well. The log likelihood ratio and weighted slope metric distortion measures gave the highest recognition accuracy, while the Itakura-Saito distortion measure gave the lowest score; (2) Whereas the addition of suprasegmental energy information helped the recognition performance, the use of gain and absolute loudness degraded the performance; (3) Bark-scale frequency warping did not, at least for the highly bandlimited telephone data base we tested, performed as well as its unwarped counterpart; (4) The weighted likelihood ratio distortion measure did not perform as well as its unweighted counterpart.
A proteomic approach offers a powerful and complementary tool to genomics. It allows to index and characterize proteins, and, for example, to compare their levels of expression between healthy and pathological states. In mass spectrometry, the detector noise, the electronic and chemical noise, sometimes the small amount of peptides that has to be treated and finally the spectrum reduction noise (due to bad filtering and/or thresholding), can induce Parasitic Mass Peaks (PMP) and/or hide some Useful Mass Peaks (UMP) of low intensities. The immediate consequence is that the presence of the PMP and the absence of the UMP will be detrimental to the protein identification quality. In this article, we propose an original algorithm eliminating the PMP, detecting and amplifying those which are useful. The preprocessing principle uses a multi-scale analysis technique coupled to a fuzzy thresholding (multi-scale fuzzy thresholding), a local amplification of the UMP, and finally an adaptive Base Line Correction. The algorithm principle consists of dividing the frequential pass bandwidth of each masses spectrum into two subbands, a Low and High Frequency (LF,HF) subband, then each subband is in turn divided into two subbands etc. The HF subbands are then thresholded according to the minimization criterion of the Shannon fuzzy entropy, and then amplified locally; the base line is calculated in an adaptive way and subtracted from reconstructed spectrum. To evaluate the quality of this algorithm, we present a comparison of the results obtained by our algorithm, and those obtained by the DataExplorer software.
From a discourse perspective, utterances may vary in at least two important respects: (i) they can occupy a different hierarchical position in a larger-scale information unit and (ii) they can represent different types of speech acts. Spoken language systems will improve if they adequately take into account both discourse segmentation and utterance purpose. An important question then is how such discourse-structural features can be detected. Analyses of monologues and human-human dialogues have shown that a good indicator of these factors is prosody, defined as the set of suprasegmental speech features. This paper explores whether speakers also use prosody to highlight discourse structure in a particular type of human-machine interaction, viz., information query in a travel-planning domain. More specifically, it investigates if speakers signal (i) the start of a new topic by marking the initial utterance of a discourse segment, and (ii) whether an utterance is a normal request for information or part of a correction sub-dialogue. The study reveals that in human-machine interactions, both discourse segmentation and utterance purpose can have particular prosodic correlates, although speakers also mark this information through choice of wording. Therefore, it is useful to explore in the future the possibilities of incorporating prosody in spoken language systems as a cue to discourse structure.
In this paper two methods for obtaining criteria for the perceptual evaluation of voiced/unvoiced detectors are tested and compared. The first experiment consists of a perceptual scanning task in which subjects are asked to categorize short speech segments of 30 ms as “voiced” or “unvoiced”. The judgments which are obtained in this task appear to be very reliable although the judgments on the onset of voiced segments are more reliable than those on the offset. In the second experiment different versions of resynthesized speech are presented to a panel of listeners with the request to judge their relative quality. The versions only differ in the location of the V/U-transitions. It appears that the quality of speech in which voiced segments are determined by the SIFT pitch extraction algorithm but subsequently shortened by 20 ms at both ends is considered to be the most acceptable. The acceptability of the versions in which the V/U-transitions are determined by listeners in a perceptual scanning task is only slightly less.
In the early 1990s, the availability of the TIMIT read-speech phonetically transcribed corpus led to work at AT&T on the automatic inference of pronunciation variation. This work, briefly summarized here, used stochastic decision trees trained on phonetic and linguistic features, and was applied to the DARPA North American Business News read-speech ASR task. More recently, the ICSI spontaneous-speech phonetically transcribed corpus was collected at the behest of the 1996 and 1997 LVCSR Summer Workshops held at Johns Hopkins University. A 1997 workshop (WS97) group focused on pronunciation inference from this corpus for application to the DoD Switchboard spontaneous telephone speech ASR task. We describe several approaches taken there. These include (1) one analogous to the AT&T approach, (2) one, inspired by work at WS96 and CMU, that involved adding pronunciation variants of a sequence of one or more words (`multiwords') in the corpus (with corpus-derived probabilities) into the ASR lexicon, and (1+2) a hybrid approach in which a decision-tree model was used to automatically phonetically transcribe a much larger speech corpus than ICSI and then the multiword approach was used to construct an ASR recognition pronunciation lexicon.
We present here a Demand Maximizing Circuit Problem, which involves time elastic demands, and which is related to applications of Network Synthesis to the design of urban public transportation systems. This problem consists in the optimization, on some irregular domain, of some quantity whose computation involves heavy computational costs. We propose a specific metaheuristic Pursuit scheme, based upon the application to the original problem of a multiform rewriting process. We present and discuss various interpretations of this scheme together with practical experimentations.
A theoretical overview and supporting data are presented about the control of the segmental component of speech production. Findings of “motor-equivalent” trading relations between the contributions of two constrictions to the same acoustic transfer function provide preliminary support for the idea that segmental control is based on acoustic or auditory-perceptual goals. The goals are determined partly by non-linear, quantal relations (called “saturation effects”) between motor commands and articulatory movements and between articulation and sound. Since processing times would be too long to allow the use of auditory feedback for closed-loop error correction in achieving acoustic goals, the control mechanism must use a robust “internal model” of the relation between articulation and the sound output that is learned during speech acquisition. Studies of the speech of cochlear implant and bilateral acoustic neuroma patients provide evidence supporting two roles for auditory feedback in adults: maintenance of the internal model, and monitoring the acoustic environment to help assure intelligibility by guiding relatively rapid adjustments in “postural” parameters underlying average sound level, speaking rate and the amount of prosodically-based inflection of F0 and SPL.
In this paper, we propose a new parameter smoothing method in the hybrid time-delay neural network (TDNN)/hidden Markov model (HMM) architecture for speech recognition. In the hybrid architecture, the TDNN and the HMM are combined using the activations from the second hidden layer of TDNN as the outputs of a fuzzy vector quantizer (FVQ). The HMM algorithm is modified to accommodate these FVQ outputs. To improve the performance of the hybrid architecture, a new smoothing method has been proposed. The average values of the activation vectors from the second hidden layer of the modular TDNN are used to generate the smoothing matrix from which smoothed output symbol observation probability is obtained. With this proposed approach, our simulation results performed on speaker-independent Korean isolated words show the reduction of the error rate by 44.9% as compared to the floor smoothing method.
This paper describes a method of adapting a continuous density HMM recogniser trained on clean cepstral speech data to make it robust to noise. The technique is based on parallel model combination (PMC) in which the parameters of corresponding pairs of speech and noise states are combined to yield a set of compensated parameters. It improves on earlier cepstral mean compensation methods in that it also adapts the variances and as a result can deal with much lower SNRs. The PMC method is evaluated on the NOISEX-92 noise database and shown to work well down to 0 dB SNR and below for both stationary and non-stationary noises. Furthermore, for relatively constant noise conditions, there is no additional computational cost at run-time.
In this article, we propose a generic method for the automatic localisation and recognition of numerical fields (phone number, ZIP code, etc.) in unconstrained handwritten incoming mail documents. The method exploits the syntax of a numerical field as an a priori knowledge to locate it in the document. A syntactical analysis based on Markov models filters the connected component sequences that respect a particular syntax known by the system. We show the efficiency of the method on a real incoming mail document database.
This paper presents a comprehensive study of continuous speech recognition in Spanish. We have developed a semicontinuous phone-class dependent contextual modelling. Using four phone-classes, we have obtained recognition error rate reductions roughly equivalent to the percentage increase of the number of parameters, compared to baseline semicontinuous contextual modelling. We also show that the use of pausing in the training system and multiple pronunciations in the vocabulary help to improve recognition rates significantly. The actual pausing of the training sentences and the application of assimilation effects improve the transcription into context-dependent units. Multiple pronunciation possibilities are generated using general rules that are easily applied to any Spanish vocabulary. With all these ideas we have reduced the recognition errors of the baseline system by more than 30% in a task parallel to DARPA-RM translated into Spanish with a vocabulary of 979 words. Our database contains four speakers with 600 training sentences and 100 testing sentences each. All experiments have been carried out with a perplexity of 979, and even slightly higher in the case of multiple pronunciations, to be able to study the acoustic modelling power of the systems with no grammar constraints.
Handling non-native speech in automatic speech recognition (ASR) systems is an area of increasing interest. The majority of systems are tailored to native speech only and as a consequence performance for non-native speakers often is not satisfactory. One way to approach the problem is to adapt the acoustic models to the new speaker. Another important means to improve performance for non-native speakers is to consider non-native pronunciations in the dictionary. The difficulty here lies in the generation of the non-native variants, especially if various accents are to be considered. Traditional approaches to model pronunciation variation either require phonetic expertise or extensive speech databases. They are too costly, especially if a flexible modelling of several accents is desired. We propose to exclusively use native speech databases to derive non-native pronunciation variants. Furthermore we combine this approach with online, incremental weighted MLLR speaker adaptation. Using the enhanced dictionary and the speaker adaptation alone improved the word error rate of the baseline system by 5.2% and 16.8%, respectively. When both methods were combined, we achieved an improvement of 18.2%.
Corporate terminology lies at the heart of numerous applications including corporate knowledge base, "know-how" & knowledge management, technology surveys, documentation research, etc. Terminology acquisition is a difficult task and no unanimously accepted generic method exists. This article demonstrates that an ontological model for the representation and meaning of terms guarantee these properties. Such an approach defines the "Ontological Terminology", between the "Textual Terminology" and the "Conceptual Terminology".
We present a unifying framework for exact and approximate inference in Bayesian networks. This framework is used in “ProBT”, a general purpose inference engine for probabilistic reasoning and incremental model construction. This paper is not intended to present ProBT but to describe its underlying algorithms mainly the “Successive Restrictions Algorithm ” (SRA) for exact inference, and the "Monte Carlo Simultaneous Estimation and Maximization" (MCSEM) algorithm for approximate inference problems. The main idea of ProBT is to use “probability expressions ” that can be “exact” or “approximate” as basic bricks to build more complex models incrementally.
In this article, a new method of classification of remote sensing images is described. Usually, these images contain voluminous, complex, and sometimes erroneous and noisy data. In our approach, classification rules are discovered by evolution-based process instead of applying a priori chosen classification algorithm. During the evolution process, classification rules are created using raw remote sensing images, the expertise encoded in classified zones of images, and statistics about related thematic objects. The discovered rules are simple to interpret, efficient, robust and noise resistant. In the paper, the evolution-based approach is detailed and validated on remote sensing images covering not only urban zones of Strasbourg, France, but also vegetation zones of the lagoon of Venice.
The segmental quality has been and still is an important area to study and improve. Evaluation of this aspect has been carried out at regular intervals. We have settled for a basic VCV (vowel-consonant-vowel) test with the consonants in a symmetric vowel context. All consonants in Swedish can occur in this position, and since we use an open response, nonsense word format, all confusions are possible. In the most recent system, new parameter control strategies were explored. Previously, most parameters in our system had been specified by smoothed step functions. In the new system, the general smoothing algorithm has been replaced by a linear interpolation algorithm. The development of the new system involved a major revision of the older rule-set. Error rates have progressively decreased: 41.7% (1983), 26.1% (1987) and 12.8% (1989). However, a concentrated effort on some of the consonants is necessary to improve the system further.
The use of Markov Random Field (MRF) models within the framework of global bayesian estimation has recently brought new powerful solutions to standard image analysis problems. These models are generally associated with greedy relaxation algorithms. This is the reason why multiresolution methods, well known in Computational Mathematics, are widely used to speed up the convergence rate of these algorithms. But for the moment there is no real mathematical framework which associates in a simple and efficient way multigrid strategies and markovian models: most previous multiresolution markovian models have been defined using various heuristics, especially as far as the adjustment of parameters over scale is concerned. The models we consider here are both mathematically consistent and computationally tractable and are related to a multiscale exploration of the set of solutions. We detail the application of these new models to two basic issues in motion analysis from an image sequence: motion detection and 2D-motion estimation. We show the advantages of the new approach: it allows the relaxation schemes to converge faster than those associated with standard multiresolution approaches, and toward better estimates (i.e. estimates of lower energy).
A new method for 2D/3D registration, applied to Magnetic Resonance Imaging (3D) and to X-Ray angiography (2D), has been adapted and used for planning treatment in radiosurgery. The imaging flow needed for planning radiosurgery is considerable and using registration technique would make lighter the imaging protocol without restricting planning. We describe the preliminary results of the evaluation giving criteria to compare registration technique and localization using stereotactic frame, which is the gold standard method. Preliminary results obtained during this first step in validating registration put forward which kind of MRI sequence are more suitable to registration.
By now it should not be surprising that high performance speech recognition systems can be designed for a wide variety of tasks in many different languages. This is mainly attributed to the use of powerful statistical pattern matching paradigms coupled with the availability of a large amount of task-specific language and speech training examples. However, it is also well-known that such a high performance can not be maintained when the testing data do not resemble the training data. The speech distortion usually appears as a combination of various acoustic differences but the exact form of the distortion is often unknown and difficult to model. One way to reduce such acoustic mismatches is to adjust speech features according to some models of the differences. Another method is to modify the parameters of the statistical models, e.g. hidden Markov models, to make the modified models characterize the distorted speech features better. Depending on the knowledge used, this family of feature and model compensation techniques can be roughly categorized into three classes, namely: (1) training-based compensation, (2) blind compensation, and (3) structure-based compensation. This paper provides an overview of the capabilities and limitations of the compensation approaches and illustrates their similarities and differences. The relationship between adaptation and compensation will also be discussed.
The range of terminology based products required to satisfy emerging needs for document and knowledge management is significantly broadening. In this paper, we assert that each application type implies to set up dedicated terminological products from texts or other domain resources. We propose a unifying methodological framework as well as a set of software tools the use of which makes it easier to acquire knowledge from texts and to model adapted resources for a specific scope. We mention in parallel some of the basic problems that arise and, when they exist, some technical or theoretical solutions to be considered. We rely on three case- studies where we built up terminologies from texts for very distinct applications.
From this point of view, graph kernels provide a nice framework combining machine learning and graph theory techniques. Among methods based on graph kernels, an major family is based on a decomposition of a graph into substructures. In this paper, we present two extensions of a kernel previously based on unlabeled sub structures to labeled substructures and cyclic information. We also propose selection methods which allow us to weight the set of considered sub structures in order to improve prediction accuracy.
Speaker verification experiments using discrete and semi-continuous HMMs with telephone quality isolated digits are reported.
A finite linear combination of Independent Identically Distributed (i. i.d) non Gaussian variables cannot be Gaussian. In the case of an infinite linear combination, the Gaussian assumption is often considered by applying the limit central theorem. The output ofARMA (possibly AR) filters is an infinite sum of independent input samples. The aim of this paper is to study the output law of these filters and more precisely its «tendency» to the Gaussian law.
This paper addresses the issue of speaker verification system assessment. The objective of this work is to develop a way of characterising speaker verification systems in a concise and meaningful manner. Here a performance profile is suggested that encompasses the important aspects of a system under test, namely the actual verification performance, model storage requirements, confusability of the speakers in the test set, quality of the speech data used and the duration of speech data available for enrolment and testing. Results are presented that show how this profile of measures can be used to give a more meaningful representation of a given system. The aim of publishing this work is not to be prescriptive about a particular method of assessment, but rather to highlight the issues involved. In general, better definitions of widely used terms are required before meaningful comparisons can be drawn between different speaker verification systems. Widespread adoption of a set of standardised measures, along the lines of those suggested here, would significantly improve the value of such comparisons.
Jean Miélot plays an important part in the renewal of the knowledge of Antiquity in the late Middle Ages, conveying several texts of Italian humanism to the Burgundian court and translating Cicero. In these circumstances, it seemed interesting to study especially the vocabulary of ancient Rome he used. According to a very utilitarian conception of translation, the less the text is explicitly historical, the more the transpositions are numerous. The corpus shows no intention to create or improve the vocabulary of ancient Rome. Instead, Jean Miélot lies in the tradition of illustrious predecessors such as Simon de Hesdin and Nicolas de Gonesse.
Among the sources of information used in legal identification, fingerprints and genetic data seem to provide a high degree of reliability. It is possible to evaluate the probability of confusing two individuals who might possess the same fingerprint characteristics or the same genetic markers, and to quantify the risk of a false alarm. By their very nature, these data do not vary significantly over the course of time, and they cannot be modified by a suspect. The erroneous metaphoric term “voiceprint” leads many people (not only the general public) to believe that the voice is as reliable as the papillary ridges of the fingertips. According to present evidence, certain magistrates in France attach far too much importance to analyses of the voice which, along with other indices, should not be used except to help in directing an investigation. In this communication, the author will detail the conditions under which, in France, voice analyses are carried out in the course of an investigation undertaken by the law, and will attempt to define the limits of this protocol, and the difficulty (and impossibility) of producing a reliable statistical test. A historical review will then be presented of the discussions initiated by and position statements adopted by the French speech community since 1900. Finally some ideas and proposals will be put forward in conclusion, which might be discussed by specialists in speech in collaboration with the police, the gendarmerie, and the magistrature, on a national, European, and international level, to advance the search for legal proof of identification within a scientific framework, and to end up with well-defined protocols.
The paper is concerned with the approach developed within the ANR Project StaRAC, and it gives an overview of its main results. The objective was to reconsider the concept of stationarity so as to make it operational, allowing for both an interpretation relatively to an observation scale and the possibility of its testing thanks to the use of time-frequency surrogates, as well as to offer various extensions, especially beyond shift invariance.
Today, the need for variable bit rate coding exists to an increasing extent for, such as videoconferencing, audioconferencing, packet circuit multiplication systems and mobile radio applications. This article extends the concept of embedded coding to include multi-stage CELP and VSELP coding at variable bit rates. Then, the resulting orthogonalized gains are used in the derivation of a new algorithm, which is implemented off-line in order to ensure the optimization of successive codebooks in embedded multi-stage CELP/VSELP coding. Finally, subjective test results are presented, which illustrate that 24 and 32 kbit/s embedded CELP/VSELP wideband coders provide speech quality close to that of the embedded SB/ADPCM G722 coders at 56 and 64 kbit/s.
This paper reviews and evaluates three recent stage theories of reading acquisition (Marsh, Friedman, Welch, & Desberg; Frith; Seymour) and also discusses the relationships between phonological awareness and reading, especially the direction of causality in such relationships. Data from a longitudinal study of reading acquisition are then reported. This study included assessments of phonological skills in children before they had begun to learn to read. The results of the study suggest that (a) even if learning to read is conceptualised as a sequence of stages, not all children pass through the same sequence of stages, (b) phonological awareness and reading acquisition have a reciprocal interactive causal relationship, not a unidirectional one, and (c) phonological skills can play a role in the very first stage of learning to read among phonologically adept children. Hence, it is incorrect to claim that the first stage of learning to read always involves such non-phonological procedures as “logographic” processing.
The aim of this work is to build up a common framework for a class of discriminative training criteria and optimization methods for continuous speech recognition. A unified discriminative criterion based on likelihood ratios of correct and competing models with optional smoothing is presented. The unified criterion leads to particular criteria through the choice of competing word sequences and the choice of smoothing. Analytic and experimental comparisons are presented for both the maximum mutual information (MMI) and the minimum classification error (MCE) criterion together with the optimization methods gradient descent (GD) and extended Baum (EB) algorithm. A tree search-based restricted recognition method using word graphs is presented, so as to reduce the computational complexity of large vocabulary discriminative training. Moreover, for MCE training, a method using word graphs for efficient calculation of discriminative statistics is introduced. Experiments were performed for continuous speech recognition using the ARPA wall street journal (WSJ) corpus with a vocabulary of 5k words and for the recognition of continuously spoken digit strings using both the TI digit string corpus for American English digits, and the SieTill corpus for telephone line recorded German digits. For the MMI criterion, neither analytical nor experimental results do indicate significant differences between EB and GD optimization. For acoustic models of low complexity, MCE training gave significantly better results than MMI training. The recognition results for large vocabulary MMI training on the WSJ corpus show a significant dependence on the context length of the language model used for training. Best results were obtained using a unigram language model for MMI training. No significant correlation has been observed between the language models chosen for training and recognition.
This paper describes experiments on automatic speech recognition using demisyllbles as segmentation units and the consonant clusters contained therein as decision units for classification. As compared to the large number of different demisyllables, the use of consonant clusters reduces the class inventory considerably. In order to test the method, three experiments dealing with isolated German words were carried out. In the third experiment a complete 1000-word recognition system was developed which performed the segmentation, the classification of consonant clusters and vowels, and a correction of recognition errors by use of a phonetix lexicon. Demisyllable segmentation and processing have proved suitable, especially for large vocabularies.
Many problems in natural language processing (NLP) can be formulated as classification problems. The complexity of the natural language make it difficult to select the discriminating attributes for a given task, the reliability of attribute values is often dubious and vary with the domain of the corpus. This article argues that the bayesian networkformalism is well adapted for the classification of this type of data. We have estimated the benefit brought by the bayesian classifior on a real NLP application: the distinction between the impersonal and anaphoric occurrences of the english pronoun it.
This paper describes a framework for time-dependent modelling of nonstationary signals, based upon autoregressive or ARM A models with time-varying coefficients. Previous works by the author are summarized here: starting from ideas proposed by Rao, Mendel and then Liporace, it is assumed that the coefficients of the model are expressed as linear combinations of known time functions. A class of estimators is described in this text, comprising autoregressive models, moving average models, and also lattice filters, parametrized through reflection coefficients or through Log Area Ratios. Speech synthesis, which is one of the applications for which these models are efficients, concludes the paper.
In this paper, we describe the voice-activated interactive system MAIRIEVOX which has been developed at the CNET. Finally, we describe some of the French industrial applications, developed from this experimental system MAIRIEVOX.
We address the definitions and synthesis of stochastic processes which possess warped scaling laws that depart from power law behaviors in a controlled manner. We define warped infinitely divisible cascading (IDC) noise, motion and random walk. We provide a theoretical derivation of the scaling behavior of the moments of their increments. We provide numerical simulations of a warped log-Normal cascade to illustrate these results. Algorithms for synthesis and Matlab functions are available from our web pages.
The participles of compound tenses have been the subject of special treatment among grammarians of Romance languages since the Renaissance. Such forms raise a fairly specific issue since their properties do not appear to be compatible with the class of participles as the Latin tradition defines it. Some suggest recategorizing these words by giving them a new designation or assigning them to a new class with more suitable properties. This study brings together a series of texts (15th-18th centuries) in which theoretical options are proposed, and it highlights the importance of recategorization for dealing with data that challenge the Latin descriptive model. Furthermore, the recurrence and commensurability of theoretical solutions in various traditions show the interest of going beyond the framework of national histories.
We present a performance evaluation of usual workstations and supercomputers for the simulation of synchronous Boltzmann machines during relaxation. We consider the impact of several parameters, such as the network architecture, the type of relationship between layers, the coding of the neurons states, the arithmetic and the architecture of the hardware. We give a method to forecast the average performance of different computers. Finally, we conclude that the real time simulation of medium size Boltzmann machines will be achieved by the next generation of microprocessors.
Comprehension failures in agrammatic aphasics, as well as their difficulties in sentence construction, have been attributed to an underlying deficit involving the retrieval of syntactic structure. In this study we show that four agrammatic patients display a remarkable sensitivity to structural information, as indicated by their performance on a grammaticality judgment task. These results indicate significant sparing of syntactic knowledge in agrammatism, and suggest that the sentence comprehension disturbances in these patients do not reflect loss of the capacity to recover syntactic structure. In particular, accounts of the comprehension deficit in agrammatism that implicate a failure to exploit information carried by the closed class (function word) vocabulary are called seriously into question. Alternative explanations of the comprehension problem in agrammatism are explored.
This paper describes four experiments which have been carried out to evaluate the speech output component of the INSPIRE spoken dialogue system, providing speech control for different devices located in a “smart” home environment. The aim is to quantify the impact of different factors on the quality of the system, when addressed either in the home or from a remote location (office, car). Factors analyzed in the experiments include the characteristics of the machine agent during the interaction (voice, personality), the physical characteristics of the usage environment (acoustic user interface, background noise, electrical transmission path), as well as task-related characteristics (listening-only vs. interaction situation, parallel tasks). The results show a significant impact of agent and environmental factors, but not of task factors. Potential reasons for this finding are discussed.
The Prolog III programming language extends Prolog by redefining the fundamental process at its heart, unification. Prolog HI integrates into this mechanism, refined processing of trees and lists, number processing, and processing of complete propositional calculus. The capabilities thus acquired by the language are illustrated by various examples.
This article concerns the presentation of classifiers systems built to learn interactions. Different kinds of classifiers systems are presented, using a chronological approach introducing new challenges or concepts approached by every model. We describe used concepts: genetic algorithms and reinforcement learning. The article presents basics systems such as ZCS or XCS, systems for anticipation, as well as hierarchicals or heterogenous classifiers.
We postulate in this paper that highly structured speech production models will have much to contribute to the ultimate success of speech recognition in view of the weaknesses of the theoretical foundation underpinning current technology. We present two probabilistic speech recognition models with the structure designed based on approximations to human speech production mechanisms, and conclude by suggesting that many of the advantages to be gained from interaction between speech production and speech recognition communities will develop from integrating production models with the probabilistic analysis-by-synthesis strategy currently used by the technology community.
The aim of this research is to analyze the main durational changes occurring in the spoken string when a reader produces an emphatic accent (“didactic accent”) on target-words included in texts, and to verify whether these changes are organized into structured prosodic forms. Three experiments address the following questions: (1) Are the durational changes concentrated in the immediate vicinity of the target (last syllable and pause preceding the target, duration of enunciation of the target, pause following the target)? (2) Are there intercorrelations among the durational variations observed? (3) Does the typographical realization of the target produce different effects on time changes? (4) Does the semantic weight of the targets change the way in which speakers produce didactic accents? Two speakers had to read 16 different texts (each text once with and once without target-words). Pre-target variables are intercorrelated, but post-target pauses vary independently. Speakers are insensitive to typographical and semantic determinants. The results are compatible with the hypothesis that mental representations of prosodic forms govern the temporal structure of speech in loud reading, and they show the importance of cognitive determinants during continuous reading.
This article gives a state of the art of temporal neural networks and a comparison of three recurrent neural network which are most representative for applications of dynamic monitoring and prognosis. The criteria of selection of these networks are at two levels: a temporal criterion and an architectural criterion. Following the application of these criteria, three recurrent networks seem relevant: the RRBF, the R2BF and the DGNN. Tests using a benchmark of dynamic monitoring and a benchmark of prognosis enable us to evaluate the performances of the three temporal networks in term of computing and processing capacity time.
Different levels of affects are expressed in different levels of speech processing: expressions of emotions linked to an involuntarily triggered control, expressions of the speaker's attitudes and intentions and meta-linguistic expressive strategies. C-Clone is presented as an interactive cognitive architecture of the expressive communication. An auto-annotated subset of emotional expressions made possible the validation of the modeling of affective prosody into gradient contours. Moreover perceptive experiments show that no prosodic dimension is specific of an emotional value.
This paper introduces a new topological clustering formalism, dedicated to categorical data arising in the form of a binary matrix or a sum of binary matrices. The proposed approach is based on the principle of the Kohonen's model (conservation of topological order) and uses the Relational Analysis formalism by optimizing a cost function defined as a Condorcet criterion. We propose an hybrid algorithm, which deals linearly with large data sets, provides a natural clusters identification and allows a visualization of the clustering result on a two dimensional grid while preserving the a priori topological order of the data. The proposed approach called CRT was validated on several data sets and the experimental results showed very promising performances.
The dynamic processing of speech in the auditory system is apparently performed in parallel to spectral shape analysis and detection of spectral change. This formulation, closely paraphrased on Chistovich et al. (1982), is a background assumption against which we examine the auditory role of diphthongs as an example of spectral change over time. When we do this, we find a clear conflict in the literature concerning the relative perceptual weight of different components of a diphthong. Does the listener attend to a diphthong's endpoints, or to its spectral rate-of-change? This paper undertakes further experiments to try to resolve the conflict on this issue. As a result of our findings, a simple cumulative model of auditory processing of the speech signal turns out to be inadequate. So too do those models which rely on the rate-of-change itself as a trigger for the spectral comparison process. Instead, the role of spectral change in diphthongs seems to be (a) as a flag commanding extra perceptual weighting by virtue of the fact that change (of some kind) occurs, and (b) as a pointer to adjacent temporal regions of the signal which are important and should be sampled more densely.
This paper describes a pole-zero (ARMA) modeling of speech using a recursive-least-squares (RLS) fast transversal filter (FTF) algorithm. This ARMA FTF algorithm can estimate unknown input excitation and the estimated input is used to determined the parameters of the pole-zero model. This algorithm is derived using geometric projections. The geometric projection approach gives insight and useful interpretation of various filters that form the algorithm. We give a performance evaluation of the proposed algorithm by applying to synthetic and natural speech spectral estimations. This algorithm accurately represents spectral peaks and valleys of speech and requires less computations than RLS lattice filters and ARMA FTF algorithm of Ardalan and Faber (1988). Additionally, this algorithm can also be applied to other signal processing areas where the input is unknown.
For a large-vocabulary speech-recognition system, such as Dragon Systems' 30,000 word DragonDictate recognizer, an efficient approach to training is to use “phonemes-in-context” (PICs) which are triphones supplemented by a code to describe prepausal lengthening. Each PIC is in turn represented by a sequence of one to six “phonetic elements” (PELs). For each phoneme, there may be thousands of different PICs, but there are no more than 63 PELs. Initially all PICs and PELs are trained from a database of about 16,000 tokens recorded by a reference speaker. When the recognizer is used by a new speaker, each word that is recognized is immediately used to adapt the PELs in its Markov models. After about a thousand words have been recognized, most PELs have been adapted to the new speaker, so that even models for words that have not yet been spoken are appropriate for the new speaker. The recognizer was tested with two texts that differed greatly in vocabulary and style. Three speakers dictated each text: the reference speaker, a new male speaker and a new female speaker. After adaptation on 1,500 words, performance for all three speakers was better than the performance for the reference speaker on unadapted models. With an active vocabulary of 25,000 words, the fraction of words recognized correctly was 86%, with an additional 8% on a “choice list” of eight words.
This paper summerizes tools coming from estimation's theory to evaluate and design experimental device. Theses tools are applied to the "Whale Anti Collision System" dedicated to localise Sperm Whales and to avoid collision with ships. Relying on theoretical tools, our approach tries to be as close as possible of reality by using true sperm clicks, realistic measurement noise by allowing sensors and acoustic environmental missknowledges. Without any acoustic environmental assesment, WaCs system shows to be a good tool for biology study but may not be enougth accurate to be included in a anti-collision network.
In this paper, a new nonlinear predictive control scheme is proposed for a five-link planar under-actuated biped walking robot. The basic feature in the proposed strategy is to use on-line optimization to update the tracked trajectories in the completely controlled variables (actuated coordinates) in order to enhance the behavior and the stability of the remaining indirectly controlled ones (unactuated coordinates). The whole framework is illustrated through simulation case-studies. To attest the efficiency of the proposed scheme, robustness against model uncertainties and ground irregularities are investigated by simulation studies.
The use of the Time-Domain Pitch Synchronous OverLap-Add (TD-PSOLA) algorithm in a Text-To-Speech synthesizer is reviewed. Its drawbacks are underlined and three conditions on the speech database are examined. In order to satisfy them, a previously described high quality resynthesis process is developed and enhanced, which makes use of the well-known Multi-Band Excited (MBE) model. An important by-product of this operation is that optimal Pitch Marking turns out to be automatic. A temporal interpolation block is finally added. The resulting Multi-Band Resynthesis Pitch Synchronous OverLap Add (MBR-PSOLA) synthesis algorithm supports spectral interpolation between voiced parts of segments, with virtually no increase in complexity.
In this paper the design of accurate Semi-Continuous Density Hidden Markov Models (SC-HMMs) for acoustic modelling in large vocabulary continuous speech recognition is presented. Two methods are described to improve drastically the efficiency of the observation likelihood calculations for the SC-HMMs. First, reduced SC-HMMs are created, where each state does not share all the – gaussian – probability density functions (pdfs) but only those which are important for it. It is shown how the average number of gaussians per state can be reduced to 70 for a total set of 10 000 gaussians. Second, a novel scalar selection algorithm is presented reducing to 5% the number of gaussians which have to be calculated on the total set of 10 000, without any degradation in recognition performance. Furthermore, the concept of tied state context-dependent modelling with phonetic decision trees is adapted to SC-HMMs. In fact, a node splitting criterion appropriate for SC-HMMs is introduced: it is based on a distance measure between the mixtures of gaussian pdfs as involved in SC-HMM state modelling. This contrasts with other criteria from literature which are based on simplified pdfs to manage the algorithmic complexity. On the ARPA Resource Management task, a relative reduction in word error rate of 8% was achieved with the proposed criterion, comparing with two known criteria based on simplified pdfs.
This paper argues that neural networks are good vehicles for automatic speech recognition not simply because they provide non-linear pattern recognition but because their architecture allows the incorporation and exploitation of existing knowledge about speech. The first part of the paper argues that the definition of the speech recognition problem implies that prior knowledge of linguistic analysis is essential for its solution, and suggests that the currently poor exploitation of such knowledge is a consequence of contemporary pattern recognition architectures. Criticism is made of the current emphasis on syntctic pattern recognition algorithms operating at the level of the phonetic segment. The second part of the paper demonstrates that a network architecture for the lexicon provides a mechanism for the incorporation and exploitation of a range of phonological analyses. Furthermore, through the explicit separation of phonological representations from phonetic ones, there exists the possibility of constructing a front-end phonetic component on purely pattern recognition principles. Through normalisation of speaker and environment, the phonetic component may be interfaced to the network lexicon to provide a complete recognition architecture which avoids compromise in the exploitation of speech knowledge.
This paper presents the design methodology behind the floating point verification procedure for the Low-Delay Code-Excited Linear Prediction speech coder, recently selected by the CCITT as Recommendation G.728. This procedure is based on a non bit-exact specification, which is different from previous CCITT speech coder verification procedures. This approach gives additional freedom for the implementor of the algorithm, and will allow for more efficient implementations on various kinds of hardware. However, this flexibility also means that different implementations will respond slightly different to verification test signals. To cope with this, explicit objective measurements of such deviations are used in the verification process. These measurements are simple weighted and unweighted signal-to-noise ratios (WSNR and SNR). In addition to the objective measurements, certain restrictions had to be placed on the test sequence design. In spite of the input restrictions, a set of test sequences giving reasonably good coverage of the LD-CELP algorithm and state space has been found. Evaluation experiments are reported, showing that these sequences have a satisfactory error detecting capability. A final discussion concludes that the chosen verification approach is indeed feasible as an implementor s tool.
Among data mining tasks, characterization does not attract much attention from researchers,in comparison to classification. It seems to us an interesting task, since it does not require négative examples, which may be a strong requirement for real applications. In this paper. we present a general framework for the characterization of a target set of objects by means of their own properties, but also the properties of objects linked to them. According to the kinds of objects. various links can be considered. In the case of geographic databases, spatial relations express links between geographic objects. We propose some algorithms for mining characterization rides, and we show how they have been applied to a real goography application provided by BRGM.
French towns were very deeply transformed in a short century near 1850-1930. Old city dwellers no longer recognized the urban life they had discovered. The young countrymen were confused. Perhaps integration was made easier by the development of show business. Laughing and smiling in the city facilitated coexistence, which later became an urban identity.
We propose a model for a statistical representation of the conceptual structure of a restricted subset of spoken natural language. The model is used for segmenting a sentence into phrases and labeling them with concept relations (or cases). The model is trained using a corpus of annotated transcribed sentences. An understanding system is being built around this model, allowing for unconstrained spoken input in a database retrieval task. The scope of this paper is to give details and results concerning the new language representation model. To that aim, the model was implemented and tested allowing a text input. While the model parameters were estimated using 547 training sentences, the results on a test set of 148 sentences showed that almost 97% of the concepts were correctly detected and labeled by the automatic concept labeling procedure; eventually, 65% of the sentences were correctly understood.
Several studies have recently tried to provide artificial agents with cognitive capacities and psychological features. However these studies focused on procedural approaches, which are difficult to track, and have handled only small parts of the psychological domain. We present here a generic approach to the implementation of a principle stating that personality traits have an actual influence over the rational decision making process of cognitive agents.
Research on speech and emotion is moving from a period of exploratory research into one where there is a prospect of substantial applications, notably in human–computer interaction. Progress in the area relies heavily on the development of appropriate databases. This paper addresses four main issues that need to be considered in developing databases of emotional speech: scope, naturalness, context and descriptors. The paper shows how the challenge of developing appropriate databases is being addressed in three major recent projects––the Reading–Leeds project, the Belfast project and the CREST–ESP project. From these and other studies the paper draws together the tools and methods that have been developed, addresses the problems that arise and indicates the future directions for the development of emotional speech databases.
The problem of real-time automatic speech recognition in an adverse environment is addressed. Though much research has been performed in the area of speech recognition, only limited success has been demonstrated for real-time recognition in noisy stressful environments. The primary reason for this is that the performance of present day recognition algorithms are predicated on the assumptions of the environmental settings in which the algorithms have been formulated and implemented. The speech recognition system incorporates direct processing steps to address the effects of additive noise on the speech signal and stress on the speech production system. Performance evaluations showed an improvement in speech feature representation under stressed speaking conditions, with an average improvement in recognition rate of +17.28% across eleven noisy stressful speaking conditions.
In automatic speech recognition, the signal is usually represented by a set of time sequences of spectral parameters (TSSPs) that model the temporal evolution of the spectral envelope frame-to-frame. Those sequences are then filtered either to make them more robust to environmental conditions or to compute differential parameters (dynamic features) which enhance discrimination. In this paper, we apply frequency analysis to TSSPs in order to provide an interpretation framework for the various types of parameter filters used so far. Thus, the analysis of the average long-term spectrum of the successfully filtered sequences reveals a combined effect of equalization and band selection that provides insights into TSSP filtering. Also, we show in the paper that, when supplementary differential parameters are not used, the recognition rate can be improved even for clean speech, just by properly filtering the TSSPs. To support this claim, a number of experimental results are presented, both using whole-word and subword based models. The empirically optimum filters attenuate the low-pass band and emphasize a higher band so that the peak of the average long-term spectrum of the output of these filters lies at around the average syllable rate of the employed database (≈3 Hz).
First, the classification algorithms can be separated into two main categories: discriminative and model-based approaches. While, the first approach tries to minimize the first type of error, but cannot deal effectively with outliers, the model-based approaches make the outlier detection possible, but are not sufficiently discriminant. Thus, we propose to combine these two different approaches in a two-stage classification system embedded in a probabilistic framework. In the first stage we pre-estimate the posterior probabilities with a model-based approach and we re-estimate only the highest probabilities with appropriate Support Vector Machine (SVM) in the second stage. Another advantage of this combination is to reduce the principal burden of SVM: the processing time necessary to make a decision. Finally, the first experiments on the benchmark database MNIST have shown that our dynamic classification process allows to maintain the accuracy of SVMs, while decreasing complexity by a factor 8.7 and making the outlier rejection available.
A method has been proposed for the use of optical processing techniques in the analysis and recognition of speech signals. It was realized as an optical processor consisting of a Helium-Neon laser, optical lenses, photographic film plates and diffusers. A personal computer system was further introduced for the total system of optical processing to manage the experimental data. Since the optical processing method has an inherent advantage for high-speed and parallel processing of two-dimensional patterns, the frequency-time pattern of a one-dimensional signal can be obtained without shifting a window along the time axis and the template matching for speech recognition can be conducted in a short period. Utterances of vowels and syllables were analyzed using the processor and the results showed a close aggrement with those obtained by the computer simulation. Nonlinear warping of the time axis, indispensable for spoken word recognition, was shown to be accomplished by controlling the transmittance function of the windowing plate. Template matching of vowel sounds gave a correct recognition of the Japanese five vowels. These results indicate the validity of the optical processor. The use of a liquid crystal plate was also proposed as a windowing plate, and an experiment was conducted on the analysis of vowel sounds. A near-real-time template matching with non-linear time warping is possible by electrically controlling the liquid crystal plate using the result of template matching as a feedback signal.
For several voiced speech signals, the measurement of the instant of glottal closure was get by using the smoothed pseudo Wigner- Ville distribution and the Wong method. Both methods give similar results when measurements are easy. For some difficult cases, for which the Wong method becomes unaccurate, the smoothed pseudo Wigner-Ville distribution can still be used
Our research aims to instrument informal learning activities in the context of museum visits. Various work, based on semantic characterization of artworks, have been proposed to deliver cultural content in mobility. However, those systems take little account of the situated nature of museum visits. We thus propose an artwork and a context model allowing to enrich the classic use of semantic distance with a contextualization step, in order to deliver document better suited to the user and his situation.
The scores returned by support vector machines are often used as a confidence measures in the classification of new examples. However, there is no theoretical grounds sustaining this practice. Thus, when classification uncertainty has to be assessed, it is safer to resort to classifiers estimating conditional probabilities of class labels. Here, we focus on the ambiguity in the vicinity of the boundary decision. We propose an adaptation of maximum likelihood estimation, instantiated on logistic regression. The model outputs proper conditional probabilities into a user-defined interval and is less precise elsewhere. The model is sparse, in the sense that few examples contribute to the solution. The computational efficiency is thus improved compared to logistic regression. Furthermore, preliminary experiments show improvements over standard logistic regression with performances similar to support vector machines.
As is well known, the acquisition of speech skills by humans involves the “simultaneous” learning of speech perception and of speech production in an environment of speakers who have already acquired these skills. By contrast in speech processing by machines, speech recognition and speech synthesis are studied and implemented separately (and different methodologies have been developed for each). The present paper puts forward a structure for the acquisition of speech by machines, asm, in which both recognition and synthesis are trained “simultaneously” from human training speech. The structure consists of a synthesis chain in which a synthesiser is driven by a trainable neural network controller from a synthesis state vector and of a recognition chain comprising a trainable neural network recogniser which produces a recogniser state vector. The recogniser alternately receives training speech from a human speaker and speech from the synthesiser. A coupled minimisation is set up which trains the recogniser network and the synthesiser state and network necessary to classify or recognise human input speech and to produce synthetic speech which is recognised to be of the same class as the human speech. The algorithm is demonstrated for the acquisition of steady state vowels and simple isolated words.
This paper deals with decentralized decision making in cooperative and autonomous teams of robots. Even if DEC-MDPs describe an expressive framework for cooperative multiagent decision making, they suffer from a high complexity and fail to formalize properties of multi-robot missions. Our purpose is therefore to propose a model that can deal with more complex time and action representations, and to develop an algorithm that efficiently solves large problems. Thus, the OC-DEC-MDP model is defined and a polynomial algorithm is presented. Experiments show that our approach can deal with real-world multi-robot decision problems.
Knock is a well-known problem for spark-ignition engine manufacturers. Knock detection helps achieve the best compromise between increasing engine efficiency, fuel consumption and present requirements with regard to exhaust emission legislation. The ignition timing is usually controlled so that knock never occurs even with fuel quality changes. The advantage of a knock detection method is to work within close range of knocking conditions but avoid its occurrence. The purpose of our study consists in highlighting several knock intensities from block vibration signals provided by an accelerometer. Our aim is to differentiate three kinds of engine cycles: absence of knock, increasing knock and heavy knock. The developped diagnostic approach deals with fuzzy pattern recognition. The method, experimented on a learning set, leads to several diagnoses that cooperate.
This paper introduces ALTO, an interactive-graphic computer system designed to facilitate the development of routing algorithms for transportation vehicles. This system embodies a “general heuristic”, that is, a set of templates that are instantiated by an expert user with his/her own formulas in order to create specific algorithms. By this mean, ALTO can be used to reproduce a broad class of routing algorithms already documented in the literature or can be used to devise new resolution strategies for complex problems. An application for a real mail pick-up problem is presented at the end of the paper, in order to emphasize the flexibility of the system.
In this paper we focus on the adaptation of boosting to grammatical inference. We aim at improving the performances of state merging algorithms in the presence of noisy data by using, in the update rule, additional information provided by an oracle. This strategy requires the construction of a new weighting scheme that takes into account the confidence in the labels of the examples. We prove that our new framework preserves the theoretical properties of boosting. Using the state merging algorithm RPNI*, we describe an experimental study on various datasets, showing a dramatic improvement ofperformances.
Autoregressive analysis regularisation is considered as a variational problem solved by calculus of variations where the autoregressive polynomial is regarded as a transformation of the unitary complex circle into a parametric closed orientated curve embedded in the complex space. We proove that the Euler-Lagrange equation of this problem is equivalent to the classical regularized Yule-Walker equation. Then, this regularization problem is formulated, by an intrinsic geometrical approach, as a geodesic distance minimization with respect to a metric defined by the data fitting criteria. Then, Calculus of Variations provides, after a recall of complex function curvature definition, a «Mean Curvature Flow» Partial Differential Equation (PDE). Its discretization by Z transform leads to a PDE acting on the vector of autoregressive parameters. This second approach allows to set regularization free from the optimization of the additional hyperparameter, classically introduced in the Tikhonov approach, simply by stoping PDE when its evolution speed decreases. The second advantage lies in the fact that the PDE numerical scheme is naturally adapted for on-line continuous estimation at the rate of data flow. Extension of the way the previous problem is formulated for the estimation of Cepstrum, whose the associate distance as well as the group delay distance performances are accepted to be very efficient for signal processing applications, shows that the differential cepstrum is exactly identifiable with the Hopf-Cole transform of the autoregressive polynomial and then induces an associate according to Burgers equation with respect to data. We conclude by using Polya's interpretation of complex function integration by means of vectors field flux and work to illustrate regularization as a process that tends to make non-divergent and non-rotational the conjugate autoregressive vectors field along the unitary complex circle.
This study aims at determining the effect of the orientation of the constraint conditions as a support for making web sites easier to use. Study results showed that the UCC helped designers, since professionals and novices took into account an important number of user constraints without hindering the consideration of client constraints. It was not the case for designers dealing with the MCC. Although these results are encouraging, designers introduced again an important number of usability problems in their web sites. The paper concludes with a presentation of studies conducted with professional and novice designers and ways to help them make web sites easier to use.
This paper describes a speech spectrum transformation method by interpolating multi-speakers' spectral patterns and multi-functional representation with Radial Basis Function networks. The interpolation is carried out using spectral parameters between pre-stored multiple speakers' utterance data to generate new spectrum patterns. The multiple functions' outputs are weighted-summed, using weighting values given by RBF networks. The parameters of this multi-functional transformation are adapted by the gradient descent method. Using ten training words, the reduction rate increased to 48% by the multi-functional transformation.
This paper presents a study in the asymmetrical semi-supervised learning framework, where only positive and unlabeled data are available, and an application to a bio-data processing problem. We show that under very mild assumptions, the Naive Bayes classifier can be identified from positive and unlabeled data. From this study, we derive algorithms that we experiment on artificial data. Lastly, we present an application of this work to the problem of the extraction of local affinities in proteins for the prediction of disulfide connectivity.
We consider the automatic classification framework, using a Markov model. We address the problem of estimating the number of classes and the associated class parameters. We propose a method using the contextual information inherent in images to discriminate different classes in the case of mixture distributions with strongly mixed classes. This method is validated theoretically and practically, using synthetical images and real data. We prove that the proposed method has a validity domain larger than the methods based on a histogram analysis. We then discuss the shape of the data driven potential induced by the detected classes in a Markovian framework. Results are obtained by using two priors: the Potts model and the Chien-model.
This article presents an overview of several measures for speaker recognition. These measures relate to second-order statistical tests, and can be expressed under a common formalism. Alternative formulations of these measures are given and their mathematical properties are studied. In their basic form, these measures are asymmetric, but they can be symmetrized in various ways. All measures are tested in the framework of text-independent closed-set speaker identification, on 3 variants of the TIMIT database (630 speakers): TIMIT (high quality speech), FTIMIT (a restricted bandwidth version of TIMIT) and NTIMIT (telephone quality). Remarkable performances are obtained on TIMIT but the results naturally deteriorate with FTIMIT and NTIMIT. Symmetrization appears to be a factor of improvement, especially when little speech material is available. The use of some of the proposed measures as a reference benchmark to evaluate the intrinsic complexity of a given database under a given protocol is finally suggested as a conclusion to this work.
We present in this paper our works on the classification of industrial parts based on «Support Vector Machine» method. We present the practical frame in which are made the operations, flaws types to detect as well as feature extraction techniques. Then we introduce the three classification techniques we implemented. We explain our learning method and how we obtain optimum classifier parameters. We compare the results obtained using feature space based on a priori knowledge and on space extracted from sequential selection algorithm.
This contribution deals with how Bar Hebræus borrowed the notion of transitivity from the grammarian of Arabic, Zamaḫšarī, and how he reformulated it within his grammar of Syriac. I proceed by translating and commenting his text and comparing it with the text by Zamaḫšarī. His chapter is organised into four sections: 1. First section: concerning examples of intransitive and transitive verbs; 2. Second section: on the causes of transitivity; 3. Third section: concerning the failure of the causes of transitivity; 4. Fourth section: concerning verbs which are both transitive and intransitive. The difference between the two grammarians is manifest in the final two sections in which it appears that although Bar Hebræus borrowed the concept of transitivity from Zamaḫšarī, his treatment goes far beyond what is found in his source. Indeed, the only concern of the grammarian of Arabic is to ensure that all the complements are in the accusative and to identify the causes of transitivity. Finally, Bar Hebræus studies in depth the labile verbs which are both transitive and intransitive.
This paper examines air flow patterns at vowel-consonant and consonant-vowel transitions. Oral air flow was recorded in six speakers of American English producing reiterant speech. The air flow signal was inverse filtered to obtain an estimate of the glottal pulse. Measurements were made of peak and minimum flow, open quotient, pulse area and fundamental frequency. The results show that at the transitions between vowels and voiceless consonants the pulse properties show large variations. In particular, the source is characterized by a breathy mode of phonation. Breathiness was indexed by large values of peak and minimum flow, and an open quotient close to 1. The observed variations can be accounted for by the laryngeal adjustments that are made for voiceless consonants, in particular the glottal opening movement and its phasing with the oral articulatory events. Individual differences suggest that speakers vary in their use of the longitudinal tension of the vocal folds in controlling voicelessness.
This paper presents a new matrix formulation of the basic concepts governing discrete Hidden Markov Models (HMM). Using this formulation, we show that symbol and state probabilities are exponential functions of the transition matrix of the model. Furthermore, based on the eigenanalysis of the transition matrix, a closed form relationship is derived between the eigenvalues of this matrix and the symbol probabilities at different instants. The matrix formulation provides a useful tool for the physical interpretation of the learning and decision process through HMM. A better insight is obtained, and tools are also given for a design with improved learning characteristics.
Secondly, we use a set of linguistic information in the form of reduced models, based on linguistic models of texts. In this area we aim to evaluate if using linguistic information and analysis can improve the performance of a filtering system. Indeed, as well as using lexical characteristics, we use a range of indicators based on structure and content of the messages. This information is independent to the application domain and reliability depends on the learning operation. In order to evaluate the feasibility of our approach and its reliability, we have experimented with a corpus of1200 messages. We present here the results of a set of evaluation experiments.
This paper presents some developments in query expansion and document representation of our spoken document retrieval system and shows how various retrieval techniques affect performance for different sets of transcriptions derived from a common speech source. Modifications of the document representation are used, which combine several techniques for query expansion, knowledge-based on one hand and statistics-based on the other. Taken together, these techniques can improve Average Precision by over 19% relative to a system similar to that which we presented at TREC-7. These new experiments have also confirmed that the degradation of Average Precision due to a word error rate (WER) of 25% is quite small (3.7% relative) and can be reduced to almost zero (0.2% relative). The overall improvement of the retrieval system can also be observed for seven different sets of transcriptions from different recognition engines with a WER ranging from 24.8% to 61.5%. We hope to repeat these experiments when larger document collections become available, in order to evaluate the scalability of these techniques.
This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of process called 'visual routines' to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations.
The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed. In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion. This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication. Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker's emotional state, the listener's attribution, and the mediating acoustic cues). In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research.
This article presents an original formalism, the interac-DECPOMDP, in which agents can directly interact. On the basis of this formalism, this article proposes a decentralized learning algorithm based on a heuristic distribution of rewards during interactions. Experiments have validated its ability to automatically build collective behaviors. The presented techniques could then constitute a mean to operationalize self-organization in order to solve problems.
Motion estimation from image sequences is based on two assumptions ; the brightness conservation assumption and the assumption of spatial, temporal or spatio-temporal continuity (i.e. smoothness constraint) of the apparent velocity field. The latter assumption holds locally, within the objects, but it results in blurring the boundaries between the projections, onto the image plane, of objects undergoing different motions. These boundaries are called motion discontinuities. The main subject of this paper is an overview of the existing techniques designed to estimate the apparent velocity fields while preserving the motion discontinuities. The first part deals with the methods based on an assumption which states that the motion discontinuities spatially coincide with image brightness boundaries. The second part reports the motion segmentation methods. These discontinuities are preserved by means of a line process, a robust estimator or within an anisotropic diffusion scheme. The last part is devoted to the occlusions. The estimation errors in the occlusion areas are due to the violation of two basic assumptions: the continuity assumption and particularly the conservation assumption.
The major objective pursued with the deinterleaved data in as many distinct flows as emission sources is to enable the analysis of laws governing the parameters describing each radar pulse train. However, because of the great number of disturbances and the pulse parameter evolution law complexity, the task is extremely difficult. We show that the sequential Monte-Carlo methods are well suited to produce an adapted solution to this extraction problem, that can also be formulated like a data association problem. The fusion of the various pulse parameters, on condition that the fusion process is realized by taking in account the model specificities, allows us to extend the algorithm application field to environments where impulse density is important.
Breathiness is used to form linguistic contrasts in some languages, but also characterizes speakers as individuals and, to an extent, gender. Henton and Bladon (1985) claimed that breathiness diminishes intelligibility. The experiments described in the present paper used synthetic speech to determine the effect of adding a noise source to a modal voice source and to determine the effects of the different acoustic consequences of breathiness on the intelligibility of isolated words. No significant effects were found.
This paper describes the main retrieval problems when facing blogs. Using the classical tf idf vector-space model together with three probabilistic and one statistical language model, we evaluate them using a TREC test-collections composed of 100 topics. Using two performance measures, we show that ignoring a stemming approach results in a better performance than other indexing strategies (light or Porter's stemmer). Taking account of the presence of two search words in the retrieved documents may significantly improve the retrieval performance.
This paper, based on three presentations made in 1998 at the RLA2C Workshop in Avignon, discusses the evaluation of speaker recognition systems from several perspectives. Overall performance results of this evaluation are presented by means of detection error trade-off (DET) curves. These show the performance trade-off of missed detections and false alarms for each system and the effects on performance of training condition, test segment duration, the speakers' sex and the match or mismatch of training and test handsets. Several factors that were found to have an impact on performance, including pitch frequency, handset type and noise, are discussed and DET curves showing their effects are presented. The paper concludes with some perspective on the history of this technology and where it may be going.
In this article, we devise a fidelity criterion for quantifying the degree of distortion introduced by a speech coder. An original speech and its coded version are transformed from the time-domain to a perceptual-domain using an auditory (cochlear) model. This perceptual-domain representation provides information pertaining to the probability-of-firings in the neural channels. The introduced cochlear discrimination information (CDI) measure compares these firing probabilities in an information-theoretic sense. In essence, it evaluates the cross-entropy of the neural firings for the coded speech with respect to those for the original one. The performance of this objective measure is compared with subjective evaluation results. Finally, we provide a rate-distortion analysis by computing the rate-distortion function for speech coding using the Blahut algorithm. Four state-of-the-art speech coders with rates ranging from 4.8 kbit/s (CELP) to 32 kbit/s (ADPCM) are studied from the view-point of their performances (as assessed by the CDI measure) with respect to the rate-distortion limits.
It is 50 years since Stuart Piggott excavated the prehistoric complex at Cairnpapple. At that time there were few excavated parallels in Scotland, and interpretation inevitably relied heavily on sites excavated in southern Britain. Much more locally relevant data are now available and the sequence at Cairnpapple can now be reassessed its regional context. Piggott identified five Periods, commencing with a stone setting, 'cove' and cremation cemetery of 'Late Neolithic date' around 'c. 2500 B.C.'. Period II was a henge monument, consisting of a 'circle' of standing stones with ceremonial burials in association, and an encircling ditch with external bank – 'Of Beaker date, probably c. 1700 B.C.' Period III comprised the primary cairn, containing two cist-burials 'Of Middle Bronze Age date, probably c. 1500 B.C.' Period IV involved the doubling of the size of the cairn, with two cremated burials in inverted cinerary urns. Of final Middle Bronze Age or native Late Bronze Age date, probably c. 1000 B.C. Period V comprised four graves 'possibly Early Iron Age within the first couple of centuries A.D.' The present paper, using comparable material from elsewhere in Scotland, argues for a revised phasing: Phase 1, comprises the deposition of earlier Neolithic plain bowl sherds and axehead fragments with a series of hearths. This is comparable to 'structured deposition' noted on other sites of this period. Phase 2 involved the construction of the henge – a setting of 24 uprights – probably of timber rather than stone, probably followed by the encircling henge ditch and bank. The 'cove' is discussed in the context of comparable features in Scotland. Phase 3 saw the construction of a series of graves, including the monumental 'North Grave', which was probably encased in a cairn. Piggott's 'Period III' cairn was then built, followed by the 'Period IV' cairn. The urn burials seem likely to have been inserted into the surface of this mound, which may have covered a burial (since disturbed) on the top of the Period III mound, or may have been a deliberate monumentalising of it. The four graves identified as Iron Age by Piggott seem more likely to be from the early Christian period. The reassessment of Piggott's report emphasises the value of the writing of a clear, and sufficiently detailed account. While no report can be wholly objective it can be seen that Piggott's striving for objectivity led him to write a paper that is of lasting value.
In this paper we describe a probabilistic fusion approach based on entropic criteria, which aims at reducing the combination space by explicitly representing the notions of source redundancy and source complementarity. This modelling is particularly interesting to optimize the choice of measurements provided by sources in order to combine in multi-sources fusion system. It is in agreement with the preoccupation to perform efficiently fusion processing and to minimize the hardware resources in information fusion problem. To answer that, we made a study of the parallelization of the entropy fusion algorithm developed for its parallel implementation in the framework of an application to mobile robotics. The algorithm specification exhibiting potential parallelism is then implemented on a network of workstations running in mode MIMD-NORMA using the parallel/distributed programming environments SynDEx which support AA-A methodology and PVM which support Hoare's CSP concept.
This article examines the extent to which word recognition is influenced by lexical, syntactic, and semantic contexts in order to contrast predictions made by modular and interactive theories of the architecture of the language comprehension system. We conclude that there is strong evidence for lexical context effects, mixed evidence for semantic context effects and little evidence for syntactic context effects. We suggest that top-down feedback effects in comprehension are primarily limited to situations in which there is a well-defined part-whole relationship between the two levels and the set of lower-level units that could receive feedback from a higher level is restricted.
The vowel sub-component of a speaker-independent phoneme classification system will be described. The architecture of the vowel classifier is based on an ear model followed by a set of Multi-Layered Neural Networks (MLNN). MLNNs are trained to learn how to recognize articulatory features like the place of articulation and the manner of articulation related to tongue position. Experiments are performed on 10 English vowels showing a recognition rate higher than 95% on new speakers. When features are used for recognition, comparable results are obtained for vowels and diphthongs not used for training and pronounced by new speakers. This suggests that MLNNs suitably fed by the data computed by an ear model have good generalization capabilities over new speakers and new sounds.
Annotating images using a fixed number of concepts is a fundamental task for content based image retrieval and classification. In practice, several modalities (visual, text...) provide information about the content of images. We are specifically interested in the tags associated with images, usually resulting from folksonomy, that provide imperfect and partially relevant information. We propose two tag models bearing such imperfections, one modeling the phenomenon through a threshold which is automatically set according to semantic similarity with visual concepts, and the other improving the coding scheme of bags-of-words, inspired from recent work carried out in image classification. All this work is validated on several publicly available databases commonly used in the field of image annotation. The experimental results show that the proposed methods are beyond the state of the art while remaining less computationally expensive.
This paper presents a POMDP approximation method, called RTBSS, which is based on a look-ahead search in order to plan in a real-time dynamic environment. The basis of our approach is to avoid computing full policies in POMDP problems. Our approach is especially motivated by real-time environments where the state space is too large to consider traditional offline algorithms. We then proceed with an online approach to find at each step, the action that maximizes the agent expected utility. To this end, we present the formalism behind our approach. Then, we present how the approach was applied on three different environments. Let us mention finally that this approach was successfully implemented for the RoboCupRescue 2004 international competition where we finished second.
A speech recognition system based on synthetic generation of reference prototypes is described. The vocabulary and grammar are described in a finite-state phoneme network. In the transformation from symbolic to spectral representation, reduction rules modify the initial phoneme target values and a coarticulation module inserts interpolated transition states at phoneme boundaries. The phoneme templates are specified in terms of control parameters to a seriel formant synthesiser. At each state, a 16-channel filter bank section is computed from the synthesis parameters. The recognition process uses a time-synchronous dynamic programming technique to find the path in the network that minimises the accumulated spectral distance to the input utterance. A technique for dynamic adaptation to the speaker's voice source spectrum is performed during recognition. Without adaptation, the average recognition for ten male speakers was 88% on an isolated-word task using a 26-word vocabulary. Adding voice source adaptation raised the performance to 96%. On a vocabulary of 3 connected digits, the adaptation technique improved the recognition rate for six male speakers from 87.7% to 92.8%. The improvement was largest for subjects with low initial recognition rate, indicating the benefit of the voice source adaptation technique for certain voices. Changing the voice source model and optimising the adaptation time constant raised the recognition rate further to 96.1%. Current work is directed towards speaker adaptation of phoneme parameters and modelling of the variability of the parameter dynamics at phoneme boundaries.
The effects of voice onset time (VOT), voice tail (the time between oral closure and voice offset), duration and range of formant transitions, and frication noise intensity on the perception of voicing in Dutch two-obstruent sequences (C 1 C 2) was tested in four separate experiments, employing synthetic stimuli. The results showed that VOT and voice tail were strong cues, and that frication noise intensity was a weaker cue to the perception of voicing in C 1 C 2 sequences. Formant transitions did not have a significant effect. Moreover, the VOT and voice tail data showed that cues from temporally rather distant portions of the acoustic signal are integrated into a perceptual unit, and that a particular cue has a similar effect on the perceived voicing status of both component consonants.
Two experiments on unaided and cued recall of sentences presented in context are reported. Key nouns in the sentences were arranged to have uniform surface functions, but to vary independently in deep syntactic category and semantic function. Cued recall for sentences in which the semantic function of actor and recipient coincided with the syntactic function of deep subject and object, respectively, was better than for sentences which did not have this normal semantic-syntactic coincidence. Unaided recall was not different for the two types of sentences. Models of sentence processing may have to represent both types of information as available to the language user.
In this communication we apply an Information Retrieval model for the writer identification task. Queries are handwreitten document images projected on a suitable feature set. The handwritten document database is indexed according to the vector space model originaly used for textual information. The approach uses both the image and textual description of handwritten documents. Identified documents are then processed by the verification stage. We use a mutual information criterion so as to verify that each identified document can have been written by the writer of the query. Decision operates using an hypothesis test. The approcah is evaluated on two different database and proves to be robust to the variability of handwriting. Perspectives are oriented towards the use of large handwritten document database
We propose a new method to compute an approximate policy of a Dec-POMDP which outperforms state of the art approaches including PBDP and MBDP. Our approach is based on an estimation of the probability distribution of beliefs reachable for a given horizon. This estimation is done by simulating the execution of an heuristic policy of the Dec-POMDP. This probability distribution over beliefs is then used to choose the candidate policy trees for the given horizon using a simple criterion which tries to minimise the error induced by pruning.
A novel acoustic modeling algorithm that generates non-uniform unit HMMs to effectively cope with spectral variations in fluent speech is proposed. The algorithm is devised for the automatic iterative generation of long-span units for non-uniform modeling. This generation algorithm is based on an entropy reduction criterion using text data and a maximum likelihood criterion using speech data. The effectiveness of the non-uniform unit models is confirmed by comparing likelihood values between long-span unit HMMs and conventional phoneme-unit HMMs.
In descriptions of Tibetan grammar it is common to treat -las and -nas together in the discussion of case marking, signaling merely that -las is capable of forming comparisons whereas -nas is not. Similarly, in the discussion of comparison most authors make no distinction between the suffixes -bas and -las. A look at a few examples of these three morphemes demonstrates that they have quite distinct syntax and semantics.
This paper describes a development of a spoken dialogue travel guidance system, TARSAN. TARSAN uses commercial CD-ROM guidebooks as its knowledge source, containing a large amount of travel information. To deal with this amount of information, a large vocabulary has to be accepted by a speech recognizer without reducing its performance. Thus, we propose two steps of active/non-active word control methods: (1) a word/grammar prediction strategy, and (2) unknown word re-evaluation algorithm. The word/grammar prediction strategy dynamically changes a recognition network according to a conversation situation by making use of results retrieved from the CD-ROMs. This strategy makes users to access almost all data on the CD-ROMs using a small vocabulary speech recognizer. This algorithm enhances the ability of the word/grammar prediction. In the experiment without Garbage Models, 80.9% of the utterances were correctly understood. In the unknown word re-evaluation experiment using the Garbage Models, 86.4% were correctly re-evaluated, while the false alarms of 5% were found.
We combine for Monte-Carlo exploration machine learning atfour different time scales: online regret, through the use of bandit algorithms and Monte-Carlo estimates; transient learning, through the use of rapid action value estimates (RAVE) which are learnt online and used for accelerating the exploration and are thereafter neglected; offline learning, by data mining of datasets of games; use of expert knowledge coming from the old ages as prior information. The resulting algorithm is stronger than each element separately. We finally emphasize the exploration-exploitation dilemna in the Monte-Carlo simulations and show great improvements that can be reached with a fine tuning of related constants.
Research in large vocabulary speech recognition has been intensively carried out worldwide, in the past several years, spurred on by advances in algorithms, architectures and hardware. In the United States, the DARPA community has focused efforts on studying several continuous speech recognition tasks including Naval Resource Management, a 991 word task, ATIS (Air Travel Information System), a speech understanding task with an open vocabulary (in practice on the order of several thousand words) and a natural language component, and Wall Street Journal, a voice dictation task with a vocabulary on the order of 20,000 words. Although we have learned a great deal about how to build and efficiently implement large vocabulary speech recognition systems, there remain a whole range of fundamental questions for which we have no definitive answers.
Stimuli made as trains of alternating one-formant and two-formant pulses, and of two-formant stimuli with variable A 1/A 2 ratio, were used in vowel identification experiments. Two main facts were obtained: 1) the increase in proportion of one-formant pulses in a train and the increase in amplitude of the corresponding formant in two-formant stimulus affect the identification in just the same way; 2) large variations in amplitude difference between one-formant and two-formant pulses in the train have no effect on the identification. Neither the hypothesis of running phonemic classification nor the average spectrum hypothesis is compatible with both of these facts. It seems possible that simple “single-peak” and “pair of peaks” patterns are used as components in spectrum shape analysis.
A series of experiments is reported in which subjects describe simple visual scenes by means of both sentential and non-sentential responses. The data support the following statements about the lexicalization (word finding) process. (1) Words used by speakers in overt naming or sentence production responses are selected by a sequence of two lexical retrieval processes, the first yielding abstract pre-phonological items (L1-items), the second one adding their phonological shapes (L2-items). (2) The selection of several L1-items for a multi-word utterance can take place simultaneously. (3) A monitoring process is watching the output of L1-lexicalization to check if it is in keeping with prevailing constraints upon utterance format. (4) Retrieval of the L2-item which corresponds with a given L1-item waits until the L1-item has been checked by the monitor, and all other L1-items needed for the utterance under construction have become available. A coherent picture of the lexicalization process begins to emerge when these characteristics are brought together with other empirical results in the area of naming and sentence production, e.g., picture naming reaction times (Seymour, 1979), speech errors (Garrett, 1980), and word order preferences (Bock, 1982).
Early diagnosis is the most efficient way to struggle against cancer. Among all the existing techniques, optical methods (photodiagnosis from NUV to NIR) show important characteristics required by the physicians: high sensitivity non-ionising radiations and non-traumatic measurements. They are particularly well suited to the detection of cancers in hollow organs, that are usually superficial and hardly visible with classical endoscopy. This paper describes a methodological approach based on the use of tissue autofluorescence, applicable in clinical endoscopy, and leading to the definition of diagnosis indicators from the spectral parameters. Following a state-of-the-art on autofluorescence spectroscopic (LIFS) and endoscopic imaging methods, we present the efficiency of fibered LIFS in terms of sensitivity and specificity for the diagnosis of esophagus cancerous lesions (clinical study over 25 patients). We then present the technological characteristics of an autofluorescence endoscopic imaging prototype developed in our labs as well as its calibration. A second part is devoted to endoscopic image registration and mosaicing and to optics aberration correction in perspective of the automatic construction of a panoramic image (cartography) of the organ's explored areas. Finally, exploiting the fluorescence data provided by the imager, the feasibility of the superimposition of spatial and spectral information is validated with a phantom.
This paper presents a set of rules to predict phoneme durations for synthesis applications in French. The rules use a speaker-independent Intrinsic Duration for each phoneme and a lengthening/shortening coefficient reflecting the effects of context and speaking style. The model can thus yield different sets of phoneme durations as produced by different speakers. The validity of the model was tested on 2 speakers. For the test-corpora, the mean differences between predicted and measured durations were less than 18 ms.
Zhuang is the current designation for the northern and central Tai languages spoken in Guangxi in southern China. The implications of this typology for the study of writing systems, and the Chinese writing system in particular, would seem to be considerable.
In the design of low-bit-rate (LBR) speech coding algorithms, language variability is often considered to be of secondary importance in comparison with other operational factors such as speaker variability and noise. Given that languages differ extensively in the composition of the spectral envelope and that the quantised spectral envelope of speech represents an important part of the bit allocation in speech coding, it is surprising to find that no comprehensive studies have ever been carried out on the role of language in spectral quantisation. This paper addresses this through a series of performance studies of spectral quantisation carried out across a set of language families typical of global mobile telecommunications. The study considers factors of quantiser design such as the size and structure of codebooks, and the quantity of monolingual data used in codebook training. This study found that quantisation distortion is not uniform across languages. It is shown that a significant difference exists in the behaviour of spectral quantisation across languages, in particular the behaviour of high distortion outliers. Detailed analysis of the spectral distortion data on a phonetic level revealed that the nature of the distribution of spectral energy in phonemes influenced the behaviour of monolingual codebooks. Some explanations for codebook performance are presented as well as a set of recommendations for codebook design for multi-lingual environments.
On se souvient de la distinction faite par Guiette entre une poésie «formelle», qui réinvente perpétuellement ses règles, et une poésie «formaliste», qui se contente de remplir des formes fixées d'avance.
The evaluation of processed and synthesized speech is closely related to the auditory perception of complex sounds. An understanding of the perception of complex sounds is therefore helpful to improve the quality of processed sounds. The perceptual study of speech sounds in this paper is mainly concerned with auditory masking. Unlike most such studies, the targets in our experiment are narrowband noise signals and the maskers are wideband harmonic complex sounds. We show that the detection of targets at low frequencies is mainly determined by the spectral properties of the maskers. At high frequencies, the detection of targets is predominantly determined by the temporal behaviour of maskers. The relative contributions of spectral and temporal analysis strongly depend on the fundamental frequency of the masker. Better temporal resolution is associated with a higher masker level.
A body-worn hearing aid has been developed with the ability to estimate formant frequencies and amplitudes in real time. These parameters can be used to enhance the output signal by “sharpening” the formant peaks, by “mapping” the amplitudes of the formants onto the available dynamic range of hearing at each frequency, or by resynthesizing a speech signal that is suited to the listener's hearing characteristics. The aid can also be used in a “frequency response tailoring” mode similar to a conventional hearing aid. Initial evaluations of the peak sharpening mode produced small improvements in speech perception for three groups of subjects: (a) Five severely-to-profoundly hearing-impaired people scored 7% higher on average when using the formant-based hearing aid combined with a multiple-electrode cochlear implant compared with their implant and their conventional hearing aid together. (b) A hearing aid user with a severe hearing loss scored 11% higher with a “peak-sharpened” signal than with his own conventional hearing aid. (c) Four normally hearing listeners showed a mean improvement of 19% in the perception of vowels and a decrease of 5% for consonants in background noise when the signal was processed. These preliminary results illustrate some potential effects of formant-based processing.
This paper presents a novel approach to sinusoidal coding of speech which avoids the use of a voicing detector. The proposed model represents the speech signal as a sum of sinusoids and bandpass random signals and it is denoted hybrid harmonic model in this paper. The use of two different sets of basis functions increases the robustness of the model since there is no need to switch between techniques tailored to particular classes of sounds. Sinusoidal basis functions with harmonically related frequencies allow an accurate representation of the quasi-periodic structure of voiced speech but show difficulties in representing unvoiced sounds. On the other hand, the bandpass random functions are well suited for high quality representation of unvoiced speech sounds, since their bandwidth is larger than the bandwidth of sinusoids. The amplitudes of both sets of basis functions are simultaneously estimated by a least squares algorithm and the output speech signal is synthesized in the time domain by the superposition of all basis functions multiplied by their amplitudes. Experimental tests confirm an improved performance of the hybrid model for operation with noise-corrupted input speech, relative to classic sinusoidal models which exhibit a strong dependency on voicing decision. Finally, the implementation and test of a fully quantized hybrid coder at 4.8 kbit/s is described.
This paper proposes a method to extract the emotional impact of images based on accurate and low level features. We supposed their accuracy could also implicitly encode high- level interesting or discriminant information for emotional impact extraction. Using this statement, our tests have been done on a new image database composed of low semantic diversified images. The complexity of emotion modeling was considered in classification process through psycho-visual tests. For the nature of the emotion they had the choice between "Negative", "Neutral" and "Positive" and the power ranged from "Low" to "High". With the nature ofemotions, we made a classification in three classes of emotions.
Spoken word recognition consists of two major component processes. First, at the prelexical stage, an abstract description of the utterance is generated from the information in the speech signal. We review evidence which suggests that positive (match) and negative (mismatch) information of both a segmental and a suprasegmental nature is used to constrain this activation and competition process. We then ask whether, in addition to the necessary influence of the prelexical stage on the lexical stage, there is also feedback from the lexicon to the prelexical level. In two phonetic categorization experiments, Dutch listeners were asked to label both syllable-initial and syllable-final ambiguous fricatives (e.g., sounds ranging from [f] to [s]) in the word–nonword series maf–mas, and the nonword–word series jaf–jas. These lexical effects became smaller in listeners' slower responses, even when the listeners were put under pressure to respond as fast as possible. Our results challenge models of spoken word recognition in which feedback modulates the prelexical analysis of the component sounds of a word whenever that word is heard.
In the scope of gray-level image processing and understanding, thinning is certainly a central shape descriptor for image analysis and pattern recognition. Enhancement is also an essential tool in facilitating the visual interpretation and understanding of images, especially for noisy and blurry ones. The lack of general unified frameworks necessitates the investigation of these problems in a coherent fashion, using partial differential equations. In this paper, we present a method for thinning and enhancing images by using a shock filter derived from our previously work introduced on enhancement. This new filter incorporates specific diffusion fields and since each such field is characteristic of a given application, it brings a new degree of freedom to the shock filters, in order to address problems of greater practical interests. Probative results on handwritten documents illustrate the performance and efficiency of our model.
Empiricist heritage and a combinatory conception of linguistic facts intermingle in Saussure and his immediate predecessors and this made it difficult to construct an adequate theory of meaning (Bedeutung), which was mostly understood as resulting from gathering mental representations (Vorstellungen). As concerns this difficulty, the Saussurean sign appears to practically beg the question. Some contemporaries however choosed another approach that rather focused on Darstellung (that is to say 'representation' as a symbolic technique, and no more as a cognitive phenomenon). Although the grammatical phenomena they analysed were very different, these attempts share a common postulate concerning the existence of organizational properties in the sign itself, which provides arguments in favour of sign motivation. They are however semantic oriented just as the theories focused on Vorstellung, and this feature delimits the scope of such a semiotic.
This work is in the context of the methodological development of a multispecialist architecture called MESSIE. This paper presents the system specifications for the interpretation of man-made structures in the field of aerial imagery. The main difficulty of such a system is the knowledge expression necessary for the interpretation: among them, strategy, scene and objects knowledges. MESSIE is a blackboard architecture organized with four types of knowledge bases that schematically are grouped in two hierarchical levels. The first level corresponds to the Scene and Strategy level and the second corresponds to the specialists (one for each objet). Each level works only with certain points of view.
Errors in child speech show that some children initially formulate tense-hopping and subject-auxiliary inversion as copying without deletion. Other errors suggest that some children may formulate other movement rules as deletion without copying. A claim about the nature of the language acquisition device is made on the basis of our analysis of these errors: the language acquisition device formulates hypotheses about transformations in terms of basic operations.
Jean Froissart's Melyador belongs to the tradition of courtly chivalric romances. In the most faithful fashion, this romance stages four large tournaments upon which rest the whole work's architecture and love intrigues. Studying these conventional pieces shows that they replicate and take on the theme's inherent share of stereotypes, although they also transcend these stereotypes and turn them into a mode of aesthetic renewal. The stereotypical nature of the tournament episodes is indeed both generalized and reduced to its essence, while it is very stylized to the point of bestowing lyrical and circular dimensions to the episodes. Thus, in Melyador, the time of the chivalric performance meets up in a very original fashion with the poetical dimension of the work.
Artificial neural networks (ANNs) have found applications in large spectrum of fields. Satisfactory results are obtained particularly in classification problems. In speech recognition context, the use of ANNs is hard, this is essentially due to the absence of the temporal aspect in their structure. On the other hand, assuming a speech recognition task, a word could be recognized and well categorized or recognized and badly categorized ; so the explanation of the decision is very important. In this paper, we address two limitations of ANNs: the lack of explicit knowledge and the absence of temporal aspect in their implementation. STN: is a model of a specialized temporal neuron, which includes both symbolic and temporal aspects. To illustrate the STN utility, we consider a system for speech recognition ; we underline in this paper the explanation aspect of the system.
The use of digital signal processors is not yet commonly widespread despite their obvious advantages. In this paper we present a certain number of ideas helping ease their use. These ideas have been put to work by conceiving the architecture of a processor which is both optimal and easy to use.
It is our belief that speech recognition algorithms can best be handled through the effective an efficient cooperation of multiple knowledge sources. For the development of various types of such algorithms, also called hybrid speech algorithms and, more generally, for speech processing, we need some advanced architectures and speech processing environments. There is also a need to manipulate speech knowledge, through the use of abstract data structures, to process data bases, and to help the modeling, simulation, and evaluation of automatic speech recognizers. To tackle these problems, we propose the new concept of an artificial laboratory for speech processing. Such a system simulates a real laboratory and allows analysis of data. It also provides a large range of computing facilities which can be used with ease to perform modeling and simulation. In this correspondence, the main concepts of the system are briefly described.
Ensemble methods have been successfully used as a classification scheme. The reduction of the complexity of this popular learning paradigm motivated the appearance of ensemble pruning algorithms. This paper presents a new efficient ensemble pruning method which not only highly reduces the complexity of ensemble methods but also performs better than the non-pruned version in terms of classification accuracy. This algorithm consists in ordering all the base classifiers with respect to their entropy which exploits a new version of the margin of ensemble methods. Confrontation with both the naive approach of randomly pruning base classifiers and another ordered-based pruning algorithm turned out convincing in an extensive empirical analysis.
In earlier research the perception of voicing in Dutch two-obstruent sequences (C1C2) was shown to be affected by each of the following parameters: closure duration of the second consonant in the sequence, voice onset time (VOT), voice termination time (VTT — the period between oral closure and voice offset), duration of the preceding vowel resonance, position of stress, and frication noise intensity. The aim of the experiment reported here was twofold: to establish whether the effects of the six cues were additive or interactive and to assess the relative perceptual importance of the six cues. The results (measured in terms of voiced-voiced, voiceless-voiced, voiceless-voiceless, and voiced-voiceless responses) indicated that the effects of the parameters are additive and that, although presence/absence of periodicity (VOT and VTT) is the most important determinant of perceived voicing, perception is also to a large extent affected by “C2”-duration and “preceding vowel” duration.
Middle Indian languages belong to the same linguistic family as Sanskrit. But their grammarians offer a surprising contrast: literary Prakrits are described by grammarians who use Sanskrit, the most famous prescriptive model, which is thus extended. Pali, on the other hand, the language of Theravāda Buddhist scriptures, is described in grammars that make use of Pali. Possible reasons for this difference are considered here. Is the choice of Pali more than a superficial difference? Does the choice of Sanskrit prevent from taking into account features of linguistic reality? An attempt is made to answer these questions through the instance of verb-description and the treatment of the verb-root in grammars of the Middle Indian languages. How do the grammarians negotiate between the powerful Sanskrit model and the reality of verbal paradigms that tend to be based on the present stem and formed on a regular basis?
Four classification techniques - nearest neighbour, Gaussian mixtures, multi-layer perceptron and Kohonen self-organizing networks - are run on a simplified “real-world” problem and the results discussed. The problem chosen is the identification of utterances of “yes” and “no” from many speakers, represented as simple vectors. The results show that the multi-layer perceptron outperforms the other methods, and has a low runtime complexity.
This paper presents high performance speaker identification and verification systems based on Gaussian mixture speaker models: robust, statistically based representations of speaker identity. The identification system is a maximum likelihood classifier and the verification system is a likelihood ratio hypothesis tester using background speaker normalization. The systems are evaluated on four publically available speech databases: TIMIT, NTIMIT, Switchboard and YOHO. The different levels of degradations and variabilities found in these databases allow the examination of system performance for different task domains. Constraints on the speech range from vocabulary-dependent to extemporaneous and speech quality varies from near-ideal, clean speech to noisy, telephone speech. Closed set identification accuracies on the 630 speaker TIMIT and NTIMIT databases were 99.5% and 60.7%, respectively. On a 113 speaker population from the Switchboard database the identification accuracy was 82.8%. Global threshold equal error rates of 0.24%, 7.19%, 5.15% and 0.51% were obtained in verification experiments on the TIMIT, NTIMIT, Switchboard and YOHO databases, respectively.
The approach exploits the ability of neural networks for non-linear projection of multidimensional data, and their advantages over traditional methods. An updating rule for this network, based on the Conjugate Gradient Algorithm is used. The main advantage of this algorithm is the speedup of the convergence rate.
This paper outlines a modeling technique for digital images which relies on Markov random fields proposed by Pickard for the purpose of representing fuzzy contextual concepts such as “the uniformity of a region” or “the continuity of a contour”. We develop a maximum likelihood estimation technique which is a straightforward generalization of an approach which is used quite extensively in speech recognition circles.
Links are needed to bridge the gap between the analysis of speech as a set of discrete, ordered but durationless linguistic unit and analyses of the continuously changing acoustic signals, defined along a time axis. Current recognition and synthesis devices do not make good use of the structure imposed by speech production processes on the mapping between an allophone sequence and the many possible associated speech signals. A quantitative, flexible articulatory time framework has been developed as a contribution to the new kinds of phonetic descriptions needed. Units of articulation for allophones of the phonemes of British English and methods for linking adjacent allophones are proposed. Tentative specifications for a sub-set are offered, based on a review of published findings for natural speech. Articulatory schemes are taken to be organised with reference to particular events The two events may relate to inter-articulator coordination between two different quasi-independent articulators or to the durational extent of a statically maintained state for a single articulator. The coordination between the two events is expressed through the duration D of the time interval between them. Six examples are given of the construction of a complete articulatory time plan for an English sequence. This forms the first stage for a computer-implemented model of the articulatory, aerodynamic and acoustic processes of speech production. The synthetic speech output from the model is given acoustic variations intended to mimic those arising in natural speech due to a speaker's choice of options, including a change in rate of speech. This is achieved in the modelling by altering one or more D values in the articulatory time plan and by dispensing with some optional actions. The variability of multiple repetitions by a real speaker can be introduced into the synthetic speech by perturbing the D values. The model needs to be matched to specific real speakers in order to assess the extent to which it is realistic in its simulation of the variation and variability of acoustic pattern features for natural speech and the extent to which covariations can be predicted with it.
This paper explores whether a speaker's voice quality, defined as the perceived timbre of someone's speech, changes as a function of variation in speech melody. Analyses are based on several productions of the vowel `a', provided with different intonation patterns. It appears that in general fundamental frequency covaries with the `strength relationship' between the first two harmonics (H1–H2). That relationship determines the voice quality to some extent, and is often claimed to reflect open quotient. However, correlating the H1–H2 measure to parameters of the LF-model reveals that both the open quotient and the skewness of the glottal pulse have an impact on the lower part of the harmonic spectrum.
The objective of our current experiments is to examine the effect of duration on loudness in CV syllables. A natural [ta] was manipulated with the aid of a computer so as to generate a reference stimulus of 250 ms at 70 dB SPL and a series of test stimuli with duration of 75, 100, 150, 200, and 250 ms. The stimulus interval was 200 ms in experiment I, and 600 ms in experiment II. A computer-assisted method of adjustment was set up. The measured level difference in dB between the reference stimulus and the adjusted test stimulus is less than 2 dB for the shortest duration, and is equal to 0.2 dB for the longest one. In experiment III, the Up-Down-Transformed-Response procedure was used with the same stimuli as in experiment II. Resulting level differences values, when plotted, had the same loudness contour as in experiment I and II, but were slightly higher.
A new algorithm for the extraction of the fundamental frequency time-varying information is described. The algorithm is based on the iterative use of a linear filter with zero phase and monotonically decreasing frequency response (low pass). The results show that the method is both efficient and robust in noisy environments, providing an estimate for the locations of the closure and opening of the vocal chords.
In this paper, we present a novel approach to text mining that helps to build intelligent user interfaces for recommender and information retrieval systems. The main problem for the user in information retrieval is that he must have almost perfect knowledge of the domain and the domain terminology. Our approach eases this burden by showing a way how to encode domain knowledge so that an information retrieval system can transform the user's way to talk about the domain in the expert's way to do that. After that transformation the system can search its data bases for appropriate information. We demonstrate the practicability of our approach in a case study on a TV recommender system.
This critical operation consists of extracting most of the redundant information present in the signal, to produce a reduced set of symbols. These compressed symbols being extremely vulnerable to errors, which are likely to occur on the physical channel, the channel encoder is present to protect the compressed information stream and guarantee a sufficient level of quality to allow the reconstruction of the signal by the source decoder. These contents are characterized by a large redundancy in the data stream. Different types of redundancy can be distinguished depending on the type of the considered content: spatial, temporal or statistical redundancy [170], and two different classes of source coders can then be employed. More details on these source coding techniques and relative standards, as well as of channel coding principle and the corresponding Shannon theorems are presented in Section 2. Robust joint source channel coding techniques, aim at modifying the encoder in order to introduce redundancy in the coded binary stream [49], to include in particular the addition of markers [106], unequal error protection solutions [105][78][126][21], or even a more general cross-layer approach [42][127].
Biometrics refers to technologies for measuring and analysing human body characteristics in person authentication applications. The computational power available in today's computer and embedded systems (e.g. mobile phones, l aptop and personal digital assistant) allows the biometrics market to grow with the aim of replacing PIN codes or taped password in control access. Among the different biometric technologies that have emerged in the last decade, automatic iris verification systems are recognised as the most reliable. This is followed by a software implementation of the proposed EMD-based iris images processing on ARM920T core-module which demonstrates the feasibility of embedding the iris technology on future multimedia mobile platform.
To decrease the hazards of using mobile phones while driving, voice processing provides several tools that simplify their use: echo cancellation allows comfortable hands-free conversation, feedback and user guidance by voice allow to operate the phone in eyes-busy situations, and last not least speech recognition frees from keypad data entry to operate the telephone. A comprehensive view of a device incorporating the above mentioned technologies, which has been realized as an add-on for the Philips car telephone family, will be presented. Emphasis is placed on the speech recognition algorithms. Robustness of the algorithms to changing acoustic environment was improved by estimating and subtracting the long-term spectrum. We will show that, if this operation is done recursively, it is equivalent to the high-pass filtering or RASTA (Relative Spectral Approaches) methods recently proposed in the literature.
The technologies of ISDN teleconferencing, CD-ROM multimedia services, and High Definition Television are creating new opportunities and challenges for the digital coding of wideband audio signals, wideband speech in particular. In the coding of wideband speech, an important point of reference is the CCITT standard for 7 kHz speech at a rate of 64 kbit/s. Results of recent research are pointing to better capabilities — higher signal bandwidth at 64 kbit/s, and 7 kHz bandwidth at lower bit-rates such as 32 and 16 kbit/s. The coding of audio with a signal bandwidth of 20 kHz is receiving significant attention due to recent activity in the ISO (International Standards Organization), with a goal of storing a CD-grade monophonic audio channel at a bit-rate not exceeding 128 kbit/s. Prospects for accomplishing this are very good. As a side result, emerging algorithms will offer very attractive options at lower rates such as 96 and 64 kbit/s.
The study described here investigates the perceived emotional content of “affect bursts” for German. Affect bursts are defined as short emotional non-speech expressions. This study shows that affect bursts, presented without context, can convey a clearly identifiable emotional meaning. The influence of the segmental structure on emotion recognition, as opposed to prosody and voice quality, is investigated. Agreement between transcribers is used as an experimental criterion for distinguishing between reflexive raw affect bursts and conventionalised affect emblems. A detailed account of 28 affect burst classes is given, including perceived emotion and recognition rate in listening and reading perception tests as well as a phonetic transcription of segmental structure, voice quality and intonation.
In placing the attempts to teach language to nonhuman species in both a cultural and philosophical context, we consider evolutionary claims about the origins of human language, and Quine's indeterminacy of translation thesis. In contrasting the natural language acquired by humans with the artificial language taught nonhumans, we propose treating the human mind as a blend of learning, hard-wiring and cognition. We discuss the nature of each of the components, suggest how they may interact, and compare the three-component human system with the two-component system of other species.
We recently proposed an input-output model of the glottal pulse. Mathematically speaking, the pulse is broken down into a cosinusoidal input signal and a pair of nonlinear shaping functions. The pulse is recovered when the cosinusoid is put through the shapers. In this article, it is shown that the cycles of a speaker's glottal waveform can be synthesized with the shaping functions of a small number of reference cycles. Indeed, nonlinear systems are not described by a transfer function. Therefore, it may be assumed that the nonlinear shaping functions of a glottal pulse are less variable than the shape of the pulse itself. Two experiments were carried out to test this assumption. In a first, the output static waveforms from a two-mass model of the vocal folds were copied. In a second, the glottis signal that was obtained from a logatome [ama] spoken by a male speaker was analyzed and synthesized. Each pulse was characterized by its peak amplitude, period and form factor. In both experiments, the features of all the glottal pulses could be copied by calculating the shaper coefficients of just two reference pulse and by adjusting the control parameters of the driving cosinusoid till the output of the shaper exhibited the desired feature values.
An object-oriented analysis-synthesis coder is presented which encodes objects instead of blocks of N × N picture elements. The objects are described by three parameter sets defining the motion, shape and colour of an object. The parameter sets are obtained by image analysis based on source models of either moving 2D-objects or moving 3D-objects. Known coding techniques are used to encode the parameter sets. An object-depending parameter coding allows to introduce geometrical distortions instead of quantization errors. Using the transmitted parameter sets an image can be reconstructed by model-based image synthesis. Experimental results achieved with a first implementation of the coder are given and are discussed.
We propose a statistical dialogue modeling method based on the information theory and the speech act theory. The dialogue model consists of a trigram of utterances classified by their speech act. It can be used to rule out erroneous speech recognition candidates that are syntactically and semantically correct, but contextually incorrect, by examining whether the utterance candidates form a natural local discourse in terms of speech act sequencing. Since it is based on the information theory, we can define objective measures for the quality of the dialogue model, such as discourse perplexity. We show that the dialogue model can predict the speech act type of the next utterance by experiments on 100 keyboard dialogues, that include 2,722 utterances and 38,954 words. It achieves 39.7% prediction accuracy for the top candidate and 61.7% for the top three candidates, when 90 dialogues were used for training and the remaining 10 dialogues were used for testing. We also show that we can make a better language model by combining the dialogue model with a sentence model. The word perplexity of word bigram with speech act type trigram is 7.27, while that of simple word bigram is 11.6, when the word perplexity of the language models is computed using the 100 keyboard dialogues.
In order to construct a robust natural language processing system, it is necessary to incorporate dialogue management. The design of an appropriate manager requires a well-founded understanding of human-human conversation. This paper presents an attempt to articulate the notion of conversation as collaboration in light of this need. The argument that conversation is inherently collaborative is made based on syntactic phenomena from spontaneous English conversation. In this paper, “collaboration” implies the simultaneous co-production of a conversation by the participants involved, not merely the construction of conversational meaning through alternating discrete contributions made by conversants. The syntactic structures discussed in support of this view are list structures, echo questions, short answers, joint productions and what are here called “parallel structures” and “accomodations”. Suggestions are made concerning the impact of this view on conversation analysis and on the design of human-machine interfaces.
The paper is concerned with the statistical assessment of the description of contingency tables by induction trees. It focuses on the special case ofcategorical data. Three topics are successively considered. i) The nature of the fit in supervised learning where we stress the distinction between fitting individual values andfitting their cross-tabulated synthetic representation. ii) The description of contingency tables provided by induction trees which is compared with the log-linear modeling used in statistics. iii) The adaptation of the goodness-of-fit measures and statistics used in log-linear modeling to the case ofinduction trees. The discussion is completed with an application to the Titanic data set.
A representation of the speech signal as a sum of elementary waveforms (Elementary Waveform Speech Model or EWSM) is introduced and some of its features for modifying localized time-frequency events are demonstrated. The elementary waveforms model the local spectro-temporal maxima of energy within the speech signal thanks to the use of simple mathematical functions. An automatic analysis-synthesis system allows for waveforms parameters estimation, using frame-by-frame processing: spectral modelling and segmentation using short-time Fourier transform and LPC spectrum, Fourier filtering according to this segmentation, waveform spotting in each channel, waveform modelling using simple functions. The classical theory of speech production proves the validity of the EWSM parameters; their modifications yield well-localized time-frequency transformations, including frequency compression/expansion, pitch, formant and noise modification.
The educational system and the Académie française, a specialized institution, are part of this third role that relies on texts (government circulars, programmes) defining this organizational action. On these three levels, the state's implication in the production of linguistic norms remains indirect and scarce ―despite resistant myths to the contrary. The above brings out an ideological component focusing on the concept of legitimacy, along with the ideas of consensus, norm and normalization, linguistic insecurity, etc. These debates concern the nature of the state and the cohesion of the political body.
We propose, in this paper, an original approach in a statistical framework, for fully automatic delineation of kidneys (healthy and pathological) in 2D CT images. Our approach has two main steps: a localisation step followed by a delineation step. The localisation step is guided by a statistically learned prior spatial model in one hand and a grey level prior model in a second hand. The second step, utilizes the localisation results in order to precisely delineate the kidney's regions using a set of learned IF-THEN rules. The proposed approach is tested on clinically acquired images and promising results are obtained.
This is an overview of some recent studies of voice source acoustics and glottal flow analysis and modelling performed at the KTH. Time and frequency domain aspects of the production process are discussed with a view of relating glottal flow parameters from inverse filtering and vocal tract transfer functions to formant amplitudes and bandwidths. Alternative methods of determining the time constant Ta = 1 (2πFa) in the return phase of glottal flow derivative after the instant of excitation, and thus of spectral tilt, are discussed. Selective inverse filtering, removing all but one formant, is potentially useful for this purpose. The influence of uncertainties in quantifying the vocal tract transfer function is exemplified by a calculation of the effects of introducing a finite baffle effect of the human head adding a high-frequency emphasis above the standard + 6 dB/octave. Particular attention has been paid to temporal variations within an utterance as derived from continuous inverse filtering. Aspects of breathy voicing and female-male differences in voice production are discussed. It is demonstrated that the temporal profile of the excitation amplitude, E e (t), within an utterance derived from a male speaker can be approximated by the envelope of the negative part of the speech wave.
This work investigates some acoustic and perceptual characteristics of focal and postfocal accents in questions of Neapolitan Italian. In this variety, yes/no question pitch accents are characterized by a rise–fall configuration, with a very conspicuous peak (L*+H). When intended focus is early, a postfocal accent is produced, which aligns with the last stressed syllable of the intonation phrase (!H*). Results from a perception study suggest that the postfocal!H* is not the nuclear accent of the intonation phrase, despite being final. The phonetic and phonological nature of the focal L*+H and the postfocal!H* are also investigated in production through a set of yes/no questions varying in intended focus scope and focus placement. The results of this study support the hypothesis that focal and postfocal accents are structurally different, in that postfocal accents are acoustically much reduced. Finally, we explore the temporal alignment and melodic values of the initial rise and final fall in focus constituents varying in size. The results suggest an effect of “tonal repulsion” (Silverman and Pierrehumbert, The timing of prenuclear high accents in English, in: J. Kingston, M.E. Beckman (Eds.), Papers in Laboratory Phonology: Between the Grammar and the Physics of Speech, Cambridge University, Cambridge, 1990, pp. 71–106) on the temporal location of the L*+H peak as well as “seeming truncation” of the focus constituent final fall in one-word focus constituents.
Multi agent based simulations (MABS) have been successfully exploited to model complex systems in different areas. Nevertheless a pitfall of MABS is that their complexity increases with the number of agents and the number of different types of behaviours considered in the model. For average and large systems it is impossible to validate the trajectories of single agents in a simulation. The classical validation approaches, where only global indicators are evaluated, are too simplistic to give enough confidence on the simulation's model. It is then necessary to introduce intermediate levels of validation. In this paper we propose the use of data clustering and automated characterization of clusters in order to build, describe and follow the evolution of groups of agents in simulations. The description of clusters is used to generate profiles of agents that are reintroduced in simulations in order to study the stability of the descriptions and structures of clusters over several simulations and decide their capability to describe the modelled phenomena. These tools provides the modeller with an intermediate point of view on the evolution of the model. They are flexible enough to be applied both offline and online, and we illustrate it with both a NetLogo and a CSV-simulation log example.
Dominant models of auditory word recognition emphasize the lexical access component of the word identification problem. They thus cast the recognition process as a simple operation matching the input against stored lexical representations or direct activation of those representations. Given this characterization of the problem, it is entirely unclear how the perceiver's knowledge of language structure could facilitate the recognition of words. Yet it may be necessary to appeal to grammatical knowledge to solve many of the outstanding problems in theories of word recognition, e.g., segmentation, coping with variation in the acoustic instantiation of words, and recognizing novel words. The current paper takes seriously the possibility that grammatical knowledge participates in word recognition. It investigates what kinds of information would be helpful for what purposes. It also attempts to sketch in the outlines of the general sort of recognition system which could take advantage of the kinds of regularities found in natural languages.
We present a flexible environment for the generation of word hypotheses in continuous speech. After describing the interface to the other modules of our speech understanding system a verification algorithm and a word spotting technique both based on HMM will be discussed. The generation of reference models for the matching procedures is done automatically using the standard pronunciation of a word and a set of phonological rules about intra word assimilation. These alternative pronunciations are represented by graphs with labeled edges. Some preliminary results for the matching procedures are also given.
An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried out. In a framework of continuous density hidden Markov model (CDHMM), Bayesian learning serves as a unified approach for parameter smoothing, speaker adaptation, speaker clustering and corrective training. The goal is to enhance model robustness in a CDHMM-based speech recognition system so as to improve performance. Our approach is to use Bayesian learning to incorporate prior knowledge into the training process in the form of prior densities of the HMM parameters. The theoretical basis for this procedure is presented and results applying it to parameter smoothing, speaker adaptation, speaker clustering and corrective training are given.
In the present study 20 Dutch male speakers were asked to read aloud 47 test words in a word list and in short sentences. Part of this word set was also named by them through the presentation of pictures. A group of 20 listeners was asked to identify an unstressed vowel in all of these test words. The vowel responses of listeners were recoded into two broad categories: “full vowel” and “schwa”. Our aims were (1) to find out to what extent listeners are able to unambiguously distinguish between these two categories, (2) to investigate the influence of the frequency of occurrence of words on the classification of the test vowels, (3) to investigate the influence of speaking styles on the classification of the test vowels by comparing the speech conditions “word list”, “pictures” and “sentences”. The experimental results showed that (1) listeners often could not unambiguously classify the test vowels, especially if these occurred in interstress position, (2) the number of schwa responses was much higher for vowels in words with a relatively high frequency of occurrence, (3) the number of schwa responses increased in a more casual speaking style. Acoustic measurements on the test vowels revealed a clear relation between the perceptual results and the acoustic features of the vowels. Although the preconditions for the sound change “full vowel → schwa” in several Dutch words are excellent, the actual completion of the sound change is in our view to a large extent blocked by the rather close correspondence between Dutch vowel sounds and their orthographic representations.
Recently, a number of researchers reported quantitative results about the acoustic changes between normal and Lombard speech. These results highlighted that the nature of the Lombard reflex is highly speaker-dependent. In this paper, after briefly discussing the influence of acoustics on speech production, we summarize some important characteristics of the Lombard reflex. Then, we review some experimental results showing how the Lombard reflex varies with the speaker gender, the language, and the environment (type of noise). Finally, we briefly discuss the use of relational features as a way to reduce the influence of the Lombard reflex on automatic speech recognizers.
In this paper we apply a study of the structure of the English language towards an automatic syllabification algorithm and consequently an automatic foreign accent identification system. Any word consists of syllables which can in turn be divided into its constituents. Elements within the syllable structure are defined according to both their position within the syllable and the position of the syllable within the word structure. Elements of syllable structure that only occur at morpheme boundaries or that extend for the duration of morphemes are identified as peripheral elements; those that can occur anywhere with regard to word morphology are identified as core elements. All languages potentially make a distinction between core and peripheral elements of their syllable structure, however the specific forms these structures take will vary from language to language. In addition to problems posed by differences in phoneme inventories (a detailed analysis of comparative phoneme inventories across the languages treated here is outside the scope of this paper), we expect speakers with the greatest syllable structural differences between native and foreign language to have greatest difficulty with pronunciation in the foreign language. In this paper, we will analyze two accents of Australian English: Arabic whose core/periphery structure is similar to English and Vietnamese, whose structure is maximally different to English.
We propose a new approach to pheneme-based continuous speech recognition when a time function of plausibility of observing each phoneme is given. We introduce a criterion for best sentence, related to the sum of plausibilities of individual symbols composing the sentence. Based on the idea of making use of a high plausibility region to reduce the computation load while keeping optimality, our method finds the most plausible sentences relating to the input speech, given the plausibility, μ a,n of observing each phoneme a at each time slot n. Two optimization procedures are defined to deal with the following embedded search processes: (1) find the best path connecting peaks of the plausibility functions of two successive symbols, and (2) find the best time transition slot index for two given peaks. Dynamic programming is used in these two procedures. Since the best path finding algorithm does not search slot by slot, the recognition is highly efficient. Experimental results with the VINICS system show that the method gives a better recognition precision while requiring about 1/20 computing time, compared to traditional DP based methods. The experimental system obtained a 95% sentence recognition rate on a speaker-dependent test.
We present a new search algorithm for very large vocabulary continuous speech recognition. Continuous speech recognition with this algorithm is only about 10 times more computationally expensive than isolated word recognition. We report preliminary recognition results obtained by testing our recognizer on books on tape using a 60 000 word dictionary.
Based on the covariance method, we have developed an analysis/synthesis system which is capable of independent manipulation of the formant frequencies and bandwidths for voiced speech. The analysis is performed pitch synchronously and is based on the local minimum of the normalized squared error. Once the formant frequencies and their bandwidths have been estimated, modifications are performed by altering the predictor coefficients, so that the modified formants and/or bandwidths are the solution to the new polynomial equation. This system has applications in voice modification and speech perception as a tool for investigating voice quality and personality.
Since a few years, neural networks analysis rouses great interests. According to this approach, the study of postulated functions in the nervous system demands some powerful simulation tools. Taking inspiration from general features of signals processing and from present tendances toward parallelism in computer architecture, we propose an efficient array processor architecture for recursive adaptive networks analysis and more generally for data (signal, image) analysis: it's the processor named CRAS Y (a systolique calculator for adaptive networks).
We present in this paper a fast classification operator suitable for image processing, the performances of this operator as well as its implementation in the form of an ASIC. In image segmentation and classification in view of defect detection, it is often impossible to find a reduced set of pertinent characteristic parameters which allows to distinguish the classes. We propose herein a geometric classification method by stress polytop training which allows the use of a great number of parameters and ensures a high decision speed. The decision operator associated with the classification has been implemented in Standard Cell and Full Custom.
In this paper, an image segmentation algorithm based on credal labelling is presented. The main contribution of this work lies in the way in which the images are modelled by belief functions in order to represented uncertainty inherent in the labelling of a voxel to a class. For each voxel, the basic belief assignment is derived from intrinsic features of the regions in the image. In order to control the uncertainty in the labelling step, a decision threshold is decreased in a progressive way throughout an iterative process until its stabilization. The methodology is applied for volumes segmentation on computed tomography images.
This study explores the intellectual legacy of the Kūfan jurist Muḥammad b. Sulaymān al-Kūfī (d. early 4th/10th century) who eventually settled in Yemen and was part of the intellectual circle surrounding al-Hādī ilā l-Ḥaqq Yaḥyā b. al-Ḥusayn (d. 298/911). It offers an analysis of his most important work, Kitāb al-–Muntaḫab, through a singular ritual law case study that focuses on the use of the basmala in the daily prayer. The conclusion points towards the Muntaḫab's value as a possible conduit for accessing a stream of Kūfan jurisprudence which has not survived into the modern period.
Advances in transducer technology, signal processing and computing make possible high-quality sound capture from designated spatial volumes under adverse acoustic conditions. The techniques of multiple beamforming and matched filtering are applied to two- and three-dimensional arrays of sensors. Array performance is assessed in a preliminary way from computer simulations of rooms and from image characterization of the multipath environment. The results suggest that high-quality signals can be retrieved from spatially-selected volumes in severely reverberant enclosures. Reciprocally, the same techniques can be applied to spatially-selecteve sound projection.
Grammatical and semantic constraints are effective for interpreting or understanding linguistic expressions. However, they appear to be inadequate for selecting among several candidates, all of which may be relatively correct or inadequate grammatically or semantically. Clearly, we humans interpret a linguistic expression contextually even if there are many potential interpretations. This method performs calculations for similarity scores between linguistic expressions and for likelihood scores to select the most suitable expression. Both illocutionary force-based and morpho-syntactic classifications are considered, along with the frequencies of existing sets of neighboring linguistic expressions stored in an example database. An experimental processing unit which performs such local context analysis has been implemented in a bidirectional (English and Japanese) translation prototype system, and has shown its applicability to the selection of context-dependent translation candidates. This local context analysis mechanism can be used with conventional translation systems without contextual processing to raise translation accuracy.
In this article, we show how the issue of grammar acquisition can be approached from the standpoint of learning heuristic rules of the language under consideration. We describe our GASRIA system consisting of: An inductive learning module for grammar inference based on a novel method capable of parsing sentences not parsable by existing methods, called Partial Parsing Algorithm (PPA), a first-order logic environment, a knowledge base (KB) consisting of a rule base using variables.
We present a new 3D interactive method for visualizing multimedia data (numeric, symbolic, sounds, images, videos, Web sites) with Virtual reality. We use a 3D stereoscopic display in order to let the expert easily visualize and perceive the numerical attributes. Navigating through the data is done with a 3D sensor with six degrees of freedom that simulates a virtual caméra. Interactive request can be formulated by the expert with a data glove that recognizes the gestures. We show how this tool bas been applied to a real world application in the context of human skin analysis.
A single-board on-line system for speaker-independent isolated word recognition is described. The system consists of preprocessing hardware which is a simplified model of peripheral auditory processing, and a microprocessor system. By several tests the influence of digital word length, dynamic range and filter channel configuration on recognition performance was explored. The results indicate that a dynamic range of 30 dB seems to be adequate and three or four bits per channel are sufficient to encode the amplitude information. In addition, the influence of varying the filter parameters was investigated.
We present a new method for recognising handwritten characters based on pseudo-ID Markov models. This method belongs to a class of techniques which attempt to recover the prototype shape of a character starting from a distorted image. By contrast with popular approaches in this field, our method does not need any explicit feature extraction and it outputs recognition scores which are estimates of Bayes probabilities. It also allows an automatic training. One of its distinctive characteristics is the use of a truely bi—dimensional but causal model for the estimation of probabilities. Lastly, we show how synthetic images of characters fitted to the statistical distribution of shapes in the training set can be generated by this model.
This paper addresses typical problems encountered with hands-free equipment in the context of GSM radiotelephony. We first summarise some important characteristics of the noise field in moving vehicles, and we also describe the acoustical echo phenomenon. We show that, in order to provide sufficiently high speech quality, these hands-free equipments should include noise reduction (NR) and acoustic echo control (AEC) devices. We then describe two possible structures combining noise reduction and acoustic echo control. As a conclusion, we raise the fact that the choice of a particular structure, among those proposed, is conditioned by the performance of the adaptation algorithm of the AEC solution.
A new approach to adaptive Semantic-Language modelling has recently been proposed which allows automatic learning of all the acoustic and syntactic-semantic models that are required for a given Continuous Speech Recognition task. Recognition or Understanding is seen as a Formal Transduction procedure that exploits the set of acoustic and linguistic constraints that have been captured in the learned models to directly input raw acoustic signals and output the semantic messages that are conveyed by these signals. In this paper, the proposed approach is reviewed and new improvements are presented. Also, preliminary results with a large semantic-space continuous speech task (Spanish numbers in the one-million range) are presented showing the currently achieved capabilities of this approach.
A system is described which adds simulated emotion effects to synthetic speech. The control parameters of a speech synthesizer are controlled by rule in order to simulate the features of emotion expressed in the human voice. The system can simulate six vocal emotions and was evaluated with naïve listeners. The results indicated that the system was producing recognizable vocal emotions, with perception rankings similar to those found by previous research on human emotional speech. This system has been developed for use in voice prosthesis systems for non-vocal disabled persons, although it could be used to enhance any application which uses rule-based synthetic speech.
A phonetic classification scheme based on a feed forward recurrent back-propagation neural network working on audio and visual information is described. Some results will be given for various speaker dependent and independent phonetic recognition experiments regarding the Italian plosive consonants.
In this paper, we present, in an as complete as possible way, an approach for the design of complex adaptive systems, based on adaptive multi-agent systems and emergence. For this, in the first place, we introduce the Amas theory (Adaptive Multi-Agent Systems). This theory gives local agent design criteria so as to enable the emergence of an organization within the system and thus, of the global function of the system. Then, we describe an application in e-education built using this theory by explaining its technical working and some experiments, which are thereafter discussed. Other applications (flood prediction, ecommerce, telephonic routing) are also described, illustrating other domains where the theory has also been successfully applied. Finally, we characterize the emergent phenomena in these applications, and position our theory in relation with others.
Statistical modeling of the speech signal has been widely used in speaker recognition. The performance obtained with this type of modeling is excellent in laboratories but decreases dramatically for telephone or noisy speech. Moreover, it is difficult to know which piece of information is taken into account by the system. In order to solve this problem and to improve the current systems, a better understanding of the nature of the information used by statistical methods is needed. This knowledge should allow to select only the relevant information or to add new sources of information. The first part of this paper presents experiments that aim at localizing the most useful acoustic events for speaker recognition. Finally, the potential of dynamic information contained in the relation between a frame and its p neighbours is investigated. In the second part, the authors suggest a new selection procedure designed to select the pertinent features. Conventional feature selection techniques (ascendant selection, knock-out) allow only global and a posteriori knowledge about the relevance of an information source. However, some speech clusters may be very efficient to recognize a particular speaker, whereas they can be non-informative for another one. Moreover, some information classes may be corrupted or even missing for particular recording conditions. This necessity for speaker-specific processing and for adaptability to the environment (with no a priori knowledge of the degradation affecting the signal) leads the authors to propose a system that automatically selects the most discriminant parts of a speech utterance. The proposed architecture divides the signal into different time–frequency blocks. The likelihood is calculated after dynamically selecting the most useful blocks. This information selection leads to a significative error rate reduction (up to 41% of relative error rate decrease on TIMIT) for short training and test durations. Finally, experiments in the case of simulated noise degradation show that this approach is a very efficient way to deal with partially corrupted speech.
This study compared the effectiveness of three acoustic supplements to speechreading: the low-pass-filtered output of an electroglottograph, a variable-frequency sinusoidal substitute for voice fundamental frequency (FO), and a constant-frequency sinusoidal substitute that served as a representation of voicing. Both sinusoidal signals were synthesized at constant amplitude during periods of voicing. The sinusoidal signals were prepared off-line by a combination of automatic and manual estimation of the FO contours of video-recorded sentences. These signals were then resynchronized with the audio portions of the original recording. In 12 normally-hearing adults, the electroglottograph signal and the variable-frequency sinusoidal FO substitute both increased the number of words recognized in sentences of known topic by between 30 and 35 percentage points. The magnitude of this effect was greater for longer sentences but independent of basic speechreading ability. The constant-frequency substitute provided a 13 percentage point increase, suggesting that approximately one-third of the FO speechreading enhancement effect could be accounted for by voicing detection alone.
In this paper, we present a general framework for supervised classification. This framework only needs the definition of a generalisation operator and provides ensemble methods. For sequence classification tasks, we show that grammatical inference has already defined such learners for automata classes like reversible automata or k-TSS automata. Then we propose a generalisation operator for the class of balls of words. Finally, we show through experiments that our method efficiently resolves sequence classification tasks.
Most published adaptation research focuses on speaker adaptation, and on adaptation for noisy channels and background environments. In this paper, we present a study of task adaptation, where the speech recognition models are adapted to a specific application or task, giving significant performance gains. We explore several new questions about adaptation which have not been studied before, and present novel solutions to these problems. For example, we show that adaptation can result in increased out-of-grammar error rates. We present an automatic confidence score mapping algorithm to correct this problem. We show that grammar-dependent acoustic adaptation gives improved performance. In addition, we show that in-grammar acoustic adaptation gives significantly better results. We study acoustic and grammar task adaptation, and show that the gains are additive. Finally we show that adaptation improves both accuracy and speed, where traditional studies have been more focused on accuracy alone. We also study traditional adaptation modes such as supervised and unsupervised adaptation, the use of confidence thresholds for unsupervised adaptation, and the effect of the amount of data on task adaptation.
In this paper we aim to identify the underlying causes that can explain the performance of different channel normalization techniques. To this aim we compared four different channel normalization techniques within the context of connected digit recognition over telephone lines: cepstrum mean subtraction, the dynamic cepstrum representation, RASTA filtering and phase-corrected RASTA. We used context-dependent and context-independent hidden Markov models that were trained using a wide range of different model complexities. The results of our recognition experiments indicate that each channel normalization technique should preserve the modulation frequencies in the range between 2 and 16 Hz in the spectrum of the speech signals. At the same time, DC components in the modulation spectrum should be effectively removed. With context-independent models the channel normalization filter should have a flat phase response. Finally, for our connected digit recognition task it appeared that cepstrum mean subtraction and phase-corrected RASTA performed equally well for context-dependent and context-independent models when equal amounts of model parameters were used.
In articulatory phonetics speech is described as a sequence of distinct articulatory gestures, each of which produces an acoustic event that should approximate a phonetic target. Due to the overlap of the gestures these phonetic targets are often only partly realized. Atal (1983) has proposed a method for speech coding based on so-called temporal decomposition of speech into a sequence of overlapping target functions and corresponding target vectors. The target vectors may be associated with ideal articulatory positions. The target functions describe the temporal evolution of these targets. This method makes no use of specific articulatory or phonetic knowledge. We have extended and modified this method to improve the determination of the number and the location of the target functions and to overcome the shortcomings of the original method. With these improvements temporal decomposition has become a strong tool in analysing speech, from which researchers working on speech coding, recognition and synthesis may profit.
Telephone companies in the United States handle over 6 billion Directory Assistance (DA) calls each year. Automation of even a portion of DA calls could significantly reduce the cost of DA services. This paper explores two factors affecting successful automation of DA: (a) the effect of directory size on speech recognition performance, and (b) the complexity of existing DA call interactions. Speech recognition performance for a set of 200 spoken names was measured for directories ranging from 200 to 1.5 million unique names. Recognition accuracy decreased from 82.5% for a 200-name directory to 16.5% for a 1.5 million name directory. In part because high recognition accuracy is not easily achievable for these very large, low-context directories, it is likely that initial implementations of DA automation will focus on a small percentage of calls, requiring a smaller vocabulary. To maximize potential savings, listings that are most frequently requested constitute the optimal vocabulary. To identify critical issues in automating frequent DA requests, approximately 13,000 DA calls from an office near a major metropolitan area in the United States were studied. In this sample, 245 listings covered 10% of the call volume, and 870 listings covered 20% of the call volume.
This paper describes a unified framework for continuous speech recognition (CSR) under grammatical constraints, where trellis calculations and parsing are performed by the same simple fundamental operations, namely multiplication and addition of likelihood matrices. The matrix parser is shown to be a generalization of the CYK parser. It also facilitates explicit supra-segmental duration control for all grammatical categories. Preliminary results showed that improved duration control on the mora level raised the recognition accuracy for a phrase recognition task from 86.7% to 88.5%.
An original form of poetical debate is elaborated in the 12th and 13th century in relation to court lyricism. Under the appellation of “jeux-partis” in “oil” tongue, they meet some success in the urban frame of the “puy d'Arras”. As they formulate a sophistry of love, they intersect a number of different formalisations such as the poetical, juridical and scholastic ones. What is at stake in the debate is expressed on the dilemmatic mode. The argumentation is worked out at large through three basic enunciations: maxims, proverbs and images. The maxim, inscribed as it is in the lyrical discourse that it lays down as being axiomatical for love, has an ambivalent function: on one hand it is the enunciation one intends to dispute, on the other hand it is the form taken by the demonstration. As it combines with a syntax of demonstration it only brings out the illusion of dialectics incidentally revealing the reduddant tautology of “le jeu-parti”. Out of a number of 120 poems, there are no less than 80 proverbial expressions which articulate themselves on the context differently — although they do so in majority through assertive formulations — yet disrupting with its isotopy in so much as they illustrate and enunciate the rule at the same time. The process of examplarisation, in the form of imaged enunciations operate other alterations. If the proverb, descending from the empirical universe, universalizes the situation it refers to, the image alone proceeds inversely: from a general theme it gives an example of one or several anecdotical situations out of which the universalness of the rule emerges. Now, none of those enunciations proceeds from a demonstrative or even properly argumentative logic. They are enclosed in themselves. The interlocutors do not resume what has just been said, unless it be in a blunt form of refutation. The true formalisation is polemical. It consists in a discourse which handles irony, lightly touches insult and seeks after effect rather than reasoning. It builds up its own truth — contradictorily though — as a game played on an audience whose complicity is to be grasped and then requested — for truthfulness does lie right in the midst of the debate and asserts itself unendlessly, such as it is in “les disputes”, those contests between scholars. “Disputes” and “jeux-partis” promote a logic of controversy. They are “argumentations-spectacles”. The elaboration of truth lies elsewhere, in the “Summae” for example. Le “jeu-parti” is aporhetical. If ever there exists an answer to its questioning, it can be found in love poetry the form of which includes the “sic et non” of “la joy”, mirth and play on love.
Among the various methods proposed to improve the accuracy and the robustness of automatic speech recognition (ASR), the use of additional knowledge sources is a successful one. This paper describes a method we have developed for adaptive integration of acoustic and visual information in ASR. Each modality is involved in the recognition process with a different weight, which is dynamically adapted during this process mainly according to the signal-to-noise ratio provided as a contextual input. We tested this method on continuous hidden Markov model-based systems developed according to direct identification (DI), separate identification (SI) and hybrid identification (DI+SI) strategies. Experiments performed under various noise-level conditions show that the DI+SI based system is the most promising one when compared to both DI and SI-based systems for a speaker-dependent continuous-spelling of French letters recognition task. They also confirm that using adaptive modality weights instead of fixed weights allows for performance improvement and that weight estimation could benefit from using visemes as decision units for the visual recogniser in SI and DI+SI based systems.
There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family – the 'Cross-Entropy Method' and 'Covariance Matrix Adaptation - Evolutionary Strategy' – at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for “Path Integral Policy Improvement with Covariance Matrix Adaptation”. PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically. We illustrate this advantage on a non-trivial simulated robotics experiment.
In the last decade, there has been much interest in the development of context- aware systems but few logic-based formal theories of contexts have emerged. The use of constructive type theory with Dependently Typed Records (DTR) allows for a partial knowledge and dynamic reasoning to take place while assuming an Open World Assumption.
The paper describes experiments on noisy speech recognition, using acoustic models based on the framework of Stochastic Trajectory Models (STM). We present the theoretical framework of 4 different approaches dealing with speech model adaptation: model-specific linear regression, speech feature space transformation, noise and speech models combination, STM state-based filtering. Experiments are performed on a speaker-dependent, 1011 word continuous speech recognition application with a word-pair perplexity of 28, using vocabulary-independent acoustic training, context independent phone models, and in various noisy testing environments. To measure the performance of each approach, recognition rate variation is studied under different noise types and noise levels. Our results show that the linear regression approach significantly outperforms the other methods, for every tested noise types at medium SNRs (between 6 to 24 dB). For the Gaussian noise, with an SNR between 6 to 24 dB, we observe a reduction of the word error rate from 20% to 59% when the linear regression is used, compared to the other methods.
This paper deals with the problem of learning radial basis function neural networks to approximate non linear L 2 function from R d to R. Hybrid algorithms are mostly used for this task. Unsupervised learning techniques are used to estimate the center and width parameters of the radial functions and supervised learning techniques are used to estimate the linear parameters. Supervised learning techniques are generally based on the least squares (LS) estimates (or criterion). This estimator is optimal when the training set (z i,y i) i =1,2,.., q is composed of noisy outputs y i, i = 1,.., q and exactly known inputs z i, i = 1,.., q. However, when collecting the experimental data, it is seldom possible to avoid noise when measuring the inputs z i. The use of least squares estimator produces a biased estimation of the linear parameters in the case of noisy input output training data set, which leads to an erroneous output estimation. This paper proposes the use of an estimation procedure based on the error in variables model to estimate the linear parameters (for supervised learning) when the training set is made up of input and output data corrupted by noise. The geometrical interpretation of the proposed estimation criterion is given in order to illustrate its advantage with respect to the least squares criterion. The improved performances in non linear function approximation is illustrated with a simulation example.
The subject of this paper is the automatic partially immersed sea target detection using infrared images (Band III). These data exhibit different resolutions (depending on the camera field of view), different signal to noise ratios (depending on the sea state) and different contrasts (depending on the temperature difference between the target and the sea). In a very classical way, pattern recognition involves two steps: a pre-processing phase followed with a decision phase (detection, possibly classification). We can qualitatively conclude that the wavelet techniques are particularly robust. The decision consists in a correlation process which is a rough (the decision results from a threshold of the correlation plane) but very simple (a filtering) operation. The references are preprocessed in the same manner as the scene for each data series.
We propose an algorithm for estimating the finite-horizon expected return of a closed loop control policy from an a priori given (off-policy) sample of one-step transitions. This algorithm, named Model-free Monte Carlo (MFMC) estimator, averages cumulated rewards along a set of "broken trajectories " made of one-step transitions selected from the sample on the basis of the control policy. Under some Lipschitz continuity assumptions on the system dynamics, reward function and control policy, we provide bounds on the bias and variance of the estimator that depend only on the Lipschitz constants, on the number of broken trajectories used in the estimator, and on the sparsity of the sample of one-step transitions.
The limited number of data points usually available does not allow us to distinguish with certainty true underlying regularities from spurious ones when the data are described using nurnerous attributes. In this paper, a recoding method inspiredfrom sparse coding and frequent itemsets is presented and applied to the classification of natural scenes. A subset of 1,0(X) 1% itemsets is stochastically selected, and used to code each image into a boolean vector.
The aim of this work is to guide a reinforcement learning agent with some a priori knowledge we have about a given environment. We propose a procedural formalism which allows to design a program to introduce this knowledge. The basic idea of our method is to propose two sets of actions for a given state; a constrained one which is used first and a less constrained one, which will be used later. The initial constraints reduce the state space and so, reduce the learning time. But because the initial constraints can be too tight, we define a relaxation mechanism which will gradually increase the search space. This way of relaxing the initial constraints allows us to prove, for a wide class of programs, that the policy learnt by the agent is as good as if there were no constraints.
This paper reports on a research project concerning prosody in spontaneous spech. Two questions inaugurate the project. The first one concerns prosodic differences between spontaneous speech and read speech. Evidence from Swedish shows that these differences are not fundamental. The second question concerns the relationship between prosody and discourse categories. A methodology has been developed in order to study this relationship. Four different kinds of analyses are applied: (1) analysis of the discourse structure of the speech corpus without specific reference to prosodic information, (2) auditory analysis in the form of a prosody-oriented transcription, (3) acoustic-phonetic analysis and (4) analysis-by-synthesis. Part of this analysis is illustrated with exemplification from a persuasive monologue in French political rhetoric. Focal accent and contrast in pitch range seem to account for typical prosodic means used during political debate.
We present a learning model for categorization of structured documents that takes into account both structural information and textual information. We first define a generative model of structured documents using belief networks. Then we transform the generative model into a discriminative one using the Fisher kernel. Finally, we describe an instance of this model applied to the categorization of HTML documents. The experimental application to a classical corpus shows that the use of structural information outperforms other classical models.
This brief study presents some acoustic phonetic characteristics that reflect both the voice characteristics and motor speech behaviour of 20 pre-adolescent (6-, 8- and 10-year olds) boys and girls, and 9 adults in speech data that were elicited via a picture-naming task. The acoustic phonetic characteristics that were investigated included formant frequency values, coarticulation (or gestural overlap) and temporal patterns. Both voice characteristics and motor speech behaviour presented evidence of age and sex differences, and age by sex interactions. In addition there were significant correlations between formant frequencies and their associated formant frequency changes (or excursions). There was also evidence of individual differences in the patterns of maturation, which did not conform to chronological age. These data are presented and discussed with reference to the sexual dimorphism of the vocal apparatus, the development of vocal characteristics, and motor speech development and behaviour.
Fault tolerance in computer systems can look back at nearly thirty years of theoretical and experimental results. Since the 80's an important effort has been brought to integrate generic fault-tolerance mechanisms into reusable distributed middleware.
The presence or absence of a context sentence, varied across the two experiments, allowed an estimate of between-sentence effects on local and global processes.
Two experiments are reported in which subjects made rapid lexical decisions about inflected nouns preceded by inflected adjectives or pseudoadjectives that did or did not agree grammatically. Both adjectives and pseudoadjectives were shown to affect lexical decision times for nouns, suggesting that the priming of inflected nouns by inflected adjectives occurred at the level of the inflections. Inflected pseudonouns, however, were not affected similarly, suggesting that lexical factors were contributing to the priming in addition to grammatical factors. This instance of grammatical priming is described as an effect that arises post-lexically, based on the outcomes of relatively independent lexical and syntactical processors.
A tutorial review of the use of discrete time transversal Volterra filters (TVF) in detection, estimation and narrow-band array processing is presented, when the data are either real or complex valued. A unique representation of such filters allows to cope simultaneously with a wide field of problems ranging from maximizing the contrast in Hilbert spaces spanned by the outputs of TVF, to the non linear least squared estimation of an unknown stochastic parameter. The general extended normal equations, giving the optimal TVF for all the above problems, are at first derived in a unique form and afterwards applied to each issue of concern. Finally, a few ideas are presented on adaptive TV filtering algorithms.
This paper describes a procedure of multi-step anonymization that was developed on French narrative clinical records in cardiology. Our approach is based upon a combination of several methods, using a rule-based approach followed by the application of a machinelearning system. The combination of the two outperformed each of them taken separately (0.881 F-measure), with a recall (0.912) higher than precision (0.851).
We introduce an extension of the notion of Shannon conditional entropy to a more general form of conditional entropy that captures hoth the conditional Shannon entropy and a similar notion related to the Gini index. The proposed family of conditional entropies generates a collection ofmetrics over the set of partitions affinite sets, which can be used to construct decision trees. Experimental results suggest that by varying the parameter that defines the entropy it is possible to obtain smaller decision trees for certain data bases without sacrificing accwracy.
Over the past 2 decades, the spatial analysis methods developed were based upon restricting hypotheses that are not fulfilled in practical cases and, therefore, often yield disappointing results. This is particularly true when uncorrelated sources, plane wavefronts and identical sensors hypotheses are introduced. In a classical approach, source parameters estimates are obtained using the information contained in the eigenvalues and eigenvectors of the received signals cross-spectral matrix. Making use of the properties of this matrix, an alternate version of the eigensystem method based upon the linear dependency between the source-matrix rows is shown to exist. A deep insight into the causes of performance loss clearly shows an interest for an estimation as complete as possible of the distorted wavefronts. Letting the modules of the sources-vectors elements being equal, a new algorithm which permits the estimation of their phases is finally adressed.
This study describes a method for the automatic detection of two-consonant clusters in French. Four corpora were used, consisting of 603 different consonant clusters and single consonant combined with the 3 vowels /i, a, ã/, in disyllabic and trisyllabic words. The CCV, VCC, CV and VC syllable structures were studied. Stimuli were recorded 5 times in an anechoic room by 10 subjects. Word length, syllable length and syllable-component length were measured by means of a signal editor. More than 90% of the consonant clusters were correctly classified on the basis of the values of the duration parameters extracted from our data base. Seventeen rules were used to output all the macro-classes of the consonant clusters in the test corpus.
Current algorithmes on multiagent learning are for almost limited since they cannot manage the multiplicity of Nash equilibria and thus converge to the Pareto-optimal. We present experimental results showing convergence of such learning mechanism. We then extend our approach to the case of non-stationarity of agents which is another important aspect of multiagent systems. Finally, we tackle the question of non stationarity in multiagent environments in its generality and we present in this context some research avenues which can lead to improve our preliminary results on adaptation.
This article reviews current research on neural network systems for speaker recognition tasks. We consider two main approaches, the first one relies on direct classification and the second on speaker modelization. The potential of connectionist models for speaker recognition is first presented and the main models are briefly introduced. We then present different systems which have been recently proposed for speaker recognition tasks. We discuss their respective performances and potentials and compare these techniques to more conventional methods like vector quantization and Hidden Markov models. The paper ends with a summary and suggestions for further developments.
Any realistic account of language acquisition must take into account the manner in which the child passes from pre-speech communication to the use of language proper. For it can be shown that many of the major organizing features of syntax, semantics, pragmatics, and even phonology have important precursors and prerequisites in the prespeech communicative acts of infants. Illustrations of such precursors are examined in four different domains: The mother's mode of interpreting the infant's communicative intent; the development of joint referential devices en route to deixis; the child's developing strategy for enlisting aid in joint activity; the transformation of topic-comment organization in prespeech to predication proper. Finally, the conjecture is explored whether the child's knowledge of the requirements of action and interaction might provide the basis for the initial development of grammar.
The obtained sitemap is both usable by the internet user and webmaster, but it is especially adapted to visual handicapped person or impaired memory capacities. In this paper, we present our method of sitemap generation which uses artificial ants to generate clusters of similar pages, then we use Prim algorithm on each cluster generated by ants.
They represent important steps in the constitution of a monolingual lexicography that, in varying measures and for certain aspects, thanks to the use and elaboration by humanists such as Cristoforo Scarpa, Giovanni Tortelli, Nestore Dionigi Avogadro and Niccolò Perotti, anticipate modern lexicography with its models and uses. The paper aims at illustrating these aspects by evaluating certain Latin words whose treatment in the entries of the abovementioned works from the Middle Ages to Humanism can suggest a typological classification which can also reflect the different ways in which the relationship between grammar and lexicon has been conceived and brought to fruition.
We review some of the significant themes of research being pursued by speech researchers in Australia and which are not adequately covered in the other papers of this issue. In addition we examine some of the local work on the development of tools and resources for speech research.
For applications needing orientation analysis, Gabor functions provide a well-known and frequently used wavelet decomposition. Localised band-pass low frequency filters, if implemented through direct convolution, lead to costly orientation image decompositions. To counteract this effect, corrective action must be taken during the generation of the convolution kernels. Two examples of pyramidal decomposition illustrate the efficiency of our Gabor filter implementation.
The UMLS® (Unified Medical Language System®) is an extremely rich terminological product which was pragmatically built and can be construed in multiple ways. We sketch the nature of the UMLS and stress two of its potentially antithetic aspects: its relation to ontologies and its relation to language.
We compare a linguistic/psycholinguistic approach of the organisation of the mental lexicon with a computational approach of the implicit lexical organisation in dictionaries taken as graphs and whose structure is a “small world”.
In this paper, we propose a complete system of analysis of images, which includes the whole sequence of treatments from the low level until the interpretation. It uses neural networks as well as a rule-based system. We show that the implementation of an expert-system gives useful information for the conception of the neural nets. The mixed realisation allows us to use at best the specificities of each approach. We also show how to make a neural network learn locally contradictory configurations.
A review of research on speech-motor perceptual and auditory processing, based particularly on studies at the I.P. Pavlov Institute of Physiology in Leningrad by V. Kozhevnikov and L. Chistovich. The use of spoken responses to speech stimuli, i.e. shadowing and mimicking, is given particular emphasis. Early versions of a motor theory of speech perception are also presented.
We study an approximate calculus of the Fourier Transform. Subject to specified conditions, we define, the calculus of the different components of the spectra does not require any multiplication. Thus, the calculus, can be done in real time on an ordinary micro-computer. Moreover, the calculus of the different components of the spectra is done independently of one another. We tested this method on a signal whose spectra is well known, and on a real speech signal. The precision is of a few percent if the signal is oversampled.
In this paper, we present an on-line handwritten character recognition system which is based on structured and logical modeling of handwriting using Hidden Markov Models. After some specific preprocessing, we extract two different classes of primitives which represent the two main aspects of handwriting: the dynamic aspect for the notion of trajectory of the pen tip and the static aspect for the notion of global geometry of the letter. We make an initial training to adjust the probabilities of each Hidden Markov Model. Then, the recognition system computes the probabilities of generation by each model of the letter to be interpreted. This performs a clustering process based on similarity.
Two different Artificial Neural Network (ANN) classifiers have been compared with a traditional closest-mean-classifier, a VQ and a Kohonen-network with respect to classification of vowels extracted from continuous speech. The result shows that the best performance is achieved with the two ANN-classifiers and indicates that a Kohonen map does not deteriorate the information presented to the second layer in the network and hence can be used instead of a first hidden layer.
This study is the first of a series investigating the relation between speech errors and intelligibility of deaf speech. The role of temporal errors is investigated by artificially correcting temporal deviations present in spoken utterances and assessing how different corrections influence intelligibility. In a corpus of 30 sentences spoken by ten deaf children, age 12–14 years, the temporal structure was manipulated using digital signal processing techniques (incorporating L.P.C.-analysis), such that 6 different temporally corrected versions were obtained. These versions differed mainly in the extent to which their temporal structure approximated that of norm sentences spoken by hearing children. All versions, including the original unmanipulated one, were subjected to intelligibility tests. These tests show that for the majority of sentences temporal correction results in a small but significant improvement (of 4.5% to 6%) in intelligibility. Elimination of pauses generally reduces intelligibility. The results obtained disqualify expectations based on correlational studies suggesting a much larger effect of temporal corrections, but corroborate the results of Osberger & Levitt (1979), who employed the same method as we did. Besides replicating Osberger & Levitt's work, the present study goes further in presenting detailed information about the temporal characteristics of deaf speech and in testing finer temporal distinctions.
This paper presents a synthesis of the subspace-based methods for direction of arrival or frequency estimation which do not require the eigendecomposition of the data covariance matrix. These methods, referred to as linear methods because they only use linear operations on the data covariance matrix, have a potential interest for real time applications because of their low complexity and their possible adaptive implementation. While presenting the methods which are referred to as BEWE, the Propagator Method (MP) and SWEDE, we establish the relationship between the different versions of these methods. The complexity of each method is established and discussed. BEWE then appears as the less costly of the linear methods. As the asymptotical performances (for an infinite number of data) of BEWE and SWEDE has already been obtained in the literature, we here propose the derivation of the asymptotical performances of a particular version of the MP, referred to as the Propagator Method with noise elimination (MPEB). We then show that MPEB has the best performance of the linear methods and has the same performance as MUSIC. Simulations are given to strengthen the theoretical results established in the paper and to illustrate the comparaison between all the different methods.
In order to fit properly the a priori information about the observation process, the implementation of any linear quadratic adaptive filter requires to estimate the influence of the third order moments, by the means of the skew and the kurtosis. If these moments are vanishing, the proper structure of such algorithms is uncoupled, that is to say, two different algorithms update on the one hand the linear kernel and on the other hand the quadratic. If these same moments are not equal to zero, the suited structure is coupled: a unique algorithm jointly updates both linear and quadratic kernels. These results stems from the fact that any adaptive algorithm may be viewed as a stochastic estimate of a deterministic gradient type or Newton type procedure. Since Volterra filtering is still linear with respect to the parameters, the properties of the linear quadratic LMS algorithm are very similar to these of the classical. In particular, the approach of the convergence based on the «Mary» indépendance theory remains mainly unchanged. The mismatch in the structure of the LMSLQ algorithm is modelled, when the third order moments are not vanishing, by a noise called the mismatch noise. The explicit derivation of its variance exhibits how the steady state behavior of the algorithm may be damaged by an unfitness of the structure.
We describe a bottom-up acoustic and phonetic decoding system which produces phonetic lattices by simultaneously locating and identifying the units by means of various types of spectral distances adjusted according to the phonemes, the context, and the speaker's characteristics. The results — both for isolated words and for continuous speech — give all the phonemes that have been pronounced, with an efficiency particularly interesting for a bottom-up selection of restricted sets of elements in a large vocabulary.
Starting with a comparison between the first big French thesaurusof Lafaye which distinguishes between two lexicaly close items by opposing different co-occurences within the sentences in which these items are usaully uttered, thus pioneering the Lexicon-grammar analyses inittiated by Maurice Gross, and the first big French analogical dictionary of Boissière which indeed helps finding unknown words but with the cost of squeezing grammatical and semantic differences between lexicaly close items, the author suggests a calculus of the degree of analogy between two items through an analytic and defining description refering to the matrix from which stemmed the defining matrixes of the two compared items.
In this paper, assuming that the score of a speech utterance is a weighted sum of hidden Markov model (HMM) log state-likelihoods, we propose a new method of finding discriminative state-weights recursively using the generalized probabilistic descent method. Compared with the previous approaches, this method does not increase complexity and can be implemented with minor modification of the conventional parameter estimation and recognition algorithms by constraining the sum of the state-weights to the number of states in a recognition unit, and further it can be applied to continuous speech recognition as well as isolated word recognition. To evaluate the performance of the state-weighted HMM recognizer, we perform two kinds of experiments with phoneme-based and word-based state-weights using various kinds of speech databases. Experimental results showed that the recognizers with phoneme-based and word-based state-weights achieved 20% and 50% decrease in word error rate, respectively, for isolated word recognition, and 5% decrease for continuous speech recognition. Our approach yields recognition accuracies comparable to those of the previous approaches for continuous speech recognition, but it is much simpler to implement than others.
Today the computer is changing from a big, grey, and noisy thing on our desk to a small, portable, and ever-networked item most of us are carrying around. This new found mobility imposes a shift in how we view computers and the way we work with them. When interaction can occur anywhere at any time it is imperative that the system adapts to the user in whatever situation the user is in. The second part offers automatic situation assessment through Case-Based Reasoning. We demonstrate a multi-agent system for supplying context-sensitive services in a mobile environment.
We propose a learning method from examples situated at the junction of statistical methods and those based on Artificial Intelligence techniques. Our modelisation is based on automatic generation of classification rules and on an original use of approximate reasoning. The proposed learning method based on linear correlation search among the components of the training set vectors is multi-features. The rule uncertainty is managed in the learning phase as well as in the recognition one. A tool called SUCRAGE was implemented and confronted with a real application in the field of image processing. The obtained results validate our approach and allow us to consider other application fields. Those results also confirm our imperfections hypothesis: the approximate reasoning as well as the intraclasses correlation search can appropriately improve the results.
A noisy environment usually degrades the intelligibility of a human speaker or the performance of a speech recognizer. Over the last few years, special emphasis has been placed on analyzing and dealing with the Lombard effect within the framework of Automatic Speech Recognition. Thus, the first purpose of the work presented in this paper was to study the possible common tendencies of some acoustic features in different phonetic units for Lombard speech. Another goal was to study the influence of gender in the characterization of the above tendencies. Extensive statistical tests were carried out for each feature and each phonetic unit, using a large Spanish continuous speech corpus. The results reported here confirm the changes produced in Lombard speech with regard to normal speech. Nevertheless, some new tendencies have been observed from the outcome of the statistical tests.
We present a theory of belief reconstruction to be embedded in an agent's communication model, which accounts for both belief persistence and revision. We analyse Cohen and Levesque (1990a)'s theories, highlight problems which arise, and show that our theory does not have these problems. The starting point of our theory is called the observation principle. It accounts for a distinction between what an agent observes from another agent, and the action the latter has really performed. The theory is couched in an autoepistemic logic used objectively, along the same lines as in (Levesque, 1990). When applied to a communication context, it is shown that it correctly predicts the changes in an observer's beliefs in test cases such as sincere assertion and (detected or non-detected) lie. Such test cases highlight the ability of the theory to handle not only normal dialogue situations but also those where problems arise due to erroneous perception, such as misrecognition in spoken communication.
Simultaneous recording of vocal fold vibrations and speech signals were performed with three patients having diplophonia using a high-speed digital image recording system developed by the present authors. All three cases studied (1 case of unilateral paralysis of the recurrent nerve; 2 cases of unilateral paralysis of external branch of the superior laryngeal nerve) showed a difference in the vibratory frequency between the left and right vocal folds. The phase difference between the vocal cords varies with time. When it reaches a certain threshold, the phase difference is reset and the vocal cord movements resumes synchrony. When the movements of the vocal cords are in phase, glottal closure is complete and the excitation pattern in the speech waveform is strong, whereas when the movements are out of phase, glottal closure is incomplete and the excitation pattern is weak, resulting in a quasi-periodic vibration in speech waveform.
The aim of this study is to determine the acoustic properties of hiatuses (vowel-vowel sequences) and diphthongs (glide-vowel sequences) in Spanish and to observe how these properties are modified depending on communicative factors. To do this, two groups of data were used: speech samples gathered from conversations between two speakers participating in the execution of a map task, in which the corpus items corresponded to toponyms, and the reading of the same sequences at a normal speaking rate. The comparison was done phonetically and phonologically: first, diphthongs and hiatuses were analyzed acoustically, studying their duration and spectral dynamics, and later, an inventory of diphthongizations and monophthongizations was made. Results show that hiatuses and diphthongs differ in the temporal and frequential domain: hiatuses have a longer duration and a greater degree of curvature in the F2 trajectory than diphthongs. We have also found that vowel-vowel and glide-vowel sequences behave differently in the way they are phonetically reduced: a reduction axis can be drawn in which hiatuses become diphthongs, and diphthongs vowels. It is concluded that hiatus and diphthong are two phonetic categories which can be described on the basis of their acoustic characteristics and are subject, like any other phonetic category, to modifications due to a change in the communicative situation.
Japanese speech synthesis techniques based on composite phoneme units are surveyed. Japanese is an open-syllable language, and there are no consonant clusters in its phonemic system. Japanese is therefore structured with simple and basic syllables of a CV-type, and these are widely used in Japanese speech synthesis-by-rule instead of single phonemes. Other composite syllabic units such as VCV or CVC are also used in Japanese speech synthesis to achieve coarticulatory characteristics. In this paper, a speech synthesis method using CVC units with excitation waveform elements is described as an attempt to improve the quality of synthetic speech.
In spoken dialogue systems, natural language understanding is a difficult problem for which robust parsing methods are required. Most of the systems achieve very specific tasks: understanding is founded on detection ofkey-words or patterns in order to identify values ofpredetermined semantic frames. LOGUS, the understanding system we are presenting in this paper, uses logical formalisms, categorial grammars and conceptual graphs, outside their usual application field. The parsing is incremental; it builds a logical formula by gradually composing the recognized constituents of the sentence. The paper describes and compares the first two versions of Logu S. Their evaluations yield promising results; they show the good robustness of the parsing and its quite good ability to reconstruct the meaning of the utterances. Future studies must be led in order to take into account the context more widely and to manage dialogue.
The performance of isolated-word speech recognizers is typically measured using error rates. The Effective Vocabulary Capacity (EVC) is the maximum vocabulary that a recognizer can in principle handle at a given error rate. It relies on measures that are relatively independent of the challenge vocabulary and that require only tens or hundreds of test utterances. The EVC algorithm is tested with both synthetic and real recognizer data.
Usual visualization methods for multidimensional data sets, do not scale well to high numbers of dimensions.
This paper presents a region-based segmentation algorithm which can be applied to various problems since it does not require a priori knowledge concerning the kind of processed images. The splitting algorithm works independently on each sector, and uses a homogeneity criterion based only on grey levels. The merging is then achieved through assigning labels to each region obtained by the splitting step, using extracted feature measurements. We modeled exploited fields (data field and label field) by Markov Random Fields (MRF), the segmentation is then optimally determined using the Iterated Conditional Modes (ICM). Input data of the merging step are regions obtained by the splitting step and their corresponding features vector. The originality of this algorithm is that texture coefficients are directly computed from these regions. These regions will be elementary sites for the Markov relaxation process. Thus, a region- based segmentation algorithm using texture and grey level is obtained.
Relations between emotions and multimodal behaviors have mostly been studied in the case of acted basic emotions. In this paper, we describe two experiments studying these relations with a copy-synthesis approach. We start from video clips of TV interviews including real-life behaviors. A protocol and a coding scheme have been defined for annotating these clips at several levels (context, emotion, multimodality). The first experiment enabled to manually identify the levels of representation required for replaying the annotated behaviors by an expressive agent. The second experiment involved automatic extraction of information from the multimodal annotations. Such an approach enables to study the complex relations between emotion and multimodal behaviors.
Models of visual word recognition differ in assumptions about the extent to which phonological information is used, and the processes by which it becomes available. These issues were examined in two studies of word recognition in two writing systems, English and Chinese, which are structured along different principles (alphabetic and logographic, respectively). The results indicate that in each writing system, a large pool of higher frequency words is recognized on a visual basis, without phonological mediation. Phonology only enters into the processing of lower frequency words. Thus, although there may be other differences among writing systems which influence processing, differences in the manner in which they represent phonology are not relevant to the recognition of common words. The results are consistent with a parallel interactive model of word recognition in which orthographic and phonological information are activated at different latencies.
How to determine the set of relevant features according to a fixed task? Neural Feature Selection try to solve the problem during the neural network learning. Some frequently used methods are derived from a pruning technique, OBD, proposed in 1990 by LeCun and al. This article review these methods and propose some enhancements by using some simple rules. A study will then compare the previous methods and other classical ones.
At first we deal with the reasons why it is necessary to separate, within a signal, the purely deterministic parts from the non deterministic parts. Then we present the Prony s method and give application examples for computed processing signals
When using hidden Markov models for speech recognition, it is usually assumed that the probability that a particular acoustic vector is emitted at a given time only depends on the current state and the current acoustic vector observed. In this paper, we introduce another idea, i.e., we assume that, in a given state, the acoustic vectors are generated by a continuous Markov process. Indeed, the time evolution of the acoustic vector is inherently dynamic and continuous, and sampling only occurs for the purpose of computation. This allows us to assign a probability density to the time trajectory of the acoustic vector inside the state, reflecting the probability that this particular path has been generated by the continuous Markov process associated with this state. Roughly speaking, it measures the “adequacy” of the observed trajectory with respect to an ideal trajectory, which is modelled by a vectorial linear differential equation. As usual, the segmentation can be obtained by sampling the continuous process, and by applying dynamic programming to find the best path over all the possible sequences of states and all the possible durations.
We describe the use of spectral transformation to perform speaker adaptation for HMM based isolated-word speech recognition. The paper describes and compares three methods, namely, minimum mean square error (MMSE), canonical correlation analysis (CCA) and multi-layer perceptrons (MLP), to compute the transformations. Using isolated words from the TI-46 speech corpus, we found that CCA offers the best adaptation performance. Three HMM training and adaptation strategies are also discussed. In the “no-retraining” approach, the spectral transformation is computed from a small amount of adaptation data, and may be used, essentially, for on-line adaptation. The “training-after-adaptation” approach computes transformations prior to off-line HMM training, but produces a better set of models. The third approach is a novel two-stage combination of these approaches which has been found to achieve good adaptation performance while maintaining fast adaptation. Our experiments show that, on average, only around 10% of a new speaker's training data is required for adaptation in order to achieve better recognition accuracy than that obtained using the speaker-dependent models of that new speaker, when the CCA spectral transformation estimation method is used with this two-stage approach.
This article questions the implantation of Jean Miélot and his works in the environment of the court of Burgundy. I start from the observation that in spite of the enormous variety of interesting texts that Miélot has left us, it seems that he did not meet much success during his own life time. Many of his texts survive only in one or two copies and few contemporary libraries contained his books. There are two questions to be asked: in the first place if we should see Miélot as an isolated author or as a writer who was well integrated in an environment of book producers at the Burgundian court, and in the second place if his works did indeed not meet much success or if they were more widely read than we think. Some investigations in the field of the handwriting and the decoration of the Miélot manuscripts confirm the idea that he was not an isolated writer, but on the contrary well integrated in a network of producers of books for the Burgundian court. We can suppose close ties with colleagues, especially with David Aubert. An overview of the fifteenth-century owners of manuscripts containing Miélot's texts shows that his readers were limited to a very small circle of the ducal family and of several members of the high nobility at the Burgundian court. His texts were indeed as good as not spread to other social groups, nor to other geographical areas. Jean Miélot is an original and interesting fifteenth-century figure and combined codicological, paleographical, textual, and stylistic research can offer us new insights in his place and his work.
This article presents a new algorithm used in order to convert the speech of one speaker so that it sounds like that of another speaker. This algorithm flexibly converts voice quality using two major technical developments. Firstly, the modification of formant frequencies and spectral intensity using piecewise linear voice conversion rules. Secondly, this algorithm provides the ability to produce speech with the desired formant structure by controlling formant frequencies, formant bandwidths and spectral intensity. Speech is iteratively modified in order to achieve the specified formant structure. Listening tests prove that the proposed algorithm converts speaker individuality while maintaining high speech quality.
This article outlines a theoretical framework for the understanding of the neural basis of memory and consciousness, at systems level. This proposal rejects a single anatomical site for the integration of memory and motor processes and a single store for the meaning of entities of events. Meaning is reached by time-locked multiregional retroactivation of widespread fragment records. Only the latter records can become contents of consciousness.
We are currently in the midst of a revolution in communications that promises to provide ubiquitous access to multimedia communication services. In order to succeed, this revolution demands seamless, easy-to-use, high quality interfaces to support broadband communication between people and machines. In this paper we argue that spoken language interfaces (SLIs) are essential to making this vision a reality. We discuss potential applications of SLIs, the technologies underlying them, the principles we have developed for designing them, and key areas for future research in both spoken language processing and human–computer interfaces.
This paper presents hierarchical self-organizing map (som) models for phoneme classification. The hierarchical som method uses a non supervised learning and a spatial organization of data. This classification approach extends the Kohonen map by introducing the principle of multiple prototype vectors by means of an enrichment auxiliary information method in a map. The case study of hierarchical som classification models is phoneme recognition in continuous speech and speaker independent context. The proposed som models serve as tools for developing intelligent systems and pursuing artificial intelligence applications.
Whether closed-class words use the same lexical access route(s) as open-class words has been intensely debated recently. Differences in frequency sensitivity have been suggested as one manifestation of separable access routes. We did not find evidence to support the view that closed-class words have a different or special access route. Neither word class showed any appreciable frequency effect for Kućera-Francis frequencies of 400/million or greater, on either reaction time or error analyses. We did find open-class words to have somewhat faster responses than comparable closed-class words, but this may contradict some explanations of the reported word class effect (Bradley et al., 1980). Moreover, our data also show what may be word-specific influences on lexical decision times—effects which may be impossible to factor out of the word class effect in English. In order to accommodate the frequency insensitivity that we found, logogen-based models of lexical access have to be amended to include a floor on threshold settings. Resonance models (Gordon, 1983), already predict this frequency insensitivity. It should be possible to distinguish between these two accounts by comparing masked and routine lexical decisions, but the unexpected word-specific effects prevented us from doing so.
We suggest that Apollonius' system can be explained by his understanding of the noun, that is said to signify both the substance and the quality of the referent and is therefore related to the signification of the hyparxis / ousía: naming a referent implies its existence at least at linguistic level.
This article deals with rereading activity, mostly in its relationship to temporality. After a few definitions, it analyses successively textual «programming» of rereading, various modalities of rereading of either fictional or non fictional texts, the main reasons of rereading. It thus allows a synthesis of the rereading phenomenon in its links to temporal flow, both on poetical, pragmatical and anthropological level.
This paper examines the problem of shape-based object recognition, and proposes a new approach, the alignment of pictorial descriptions. The first part of the paper reviews general approaches to visual object recognition, and divides these approaches into three broad classes: invariant properties methods, object decomposition methods, and alignment methods. The second part presents the alignment method. In this approach the recognition process is divided into two stages. The first determines the transformation in space that is necessary to bring the viewed object into alignment with possible object models. This stage can proceed on the basis of minimal information, such as the object's dominant orientation, or a small number of corresponding feature points in the object and model. The second stage determines the model that best matches the viewed object. At this stage, the search is over all the possible object models, but not over their possible views, since the transformation has already been determined uniquely in the alignment stage. The proposed alignment method also uses abstract description, but unlike structural description methods it uses them pictorially, rather than in symbolic structural descriptions.
Designing the behaviour of non player characters (NPC), in role-playing video games, is a hard problem both from the programming point of view and from behaviour modelling considerations. This engine is combined with an action selection mechanism based on motivations. This mechanism provides means to define several NPC behaviours by tuning the motivation parameters without new programming code. Our proposition constitutes a behavioural engine dedicated to the modelling of situated character behaviour. Thanks to its genericity, this engine can be used in various environment (ie games) and for various agents, since it adapts to individual specificities and abilities while proposing a diversity in designed behaviours.
Two models for simulating the time-varying vocal tract by means of wave digital filters (WDF) are described. In one model, the coefficients of the WDF are merely updated to simulate area changes. In the other, a “time-varying WDF” is used which allows a physically consistent description of the time-varying vocal tract. In order to test if the first model is sufficient for speech synthesis, the two models are compared one to another. Furthermore, three different models for simulating a glottal source are described. One model is based on an ordinary WDF, the second on a time-varying WDF, and the third on a model in which the glottis is assumed to be purely resistive. The signals of these models and their interactions with the vocal tract are compared one to another. It is shown that the differences between the two models of the vocal tract are very small in the simulation of speech, and not audible in hearing experiments. The differences are larger between the three glottis models, and also audible in hearing experiments. Simple simulation by means of a purely resistive source produces effects, in the same way as the other models do.
LPC speech production models often assume a two real pole function for the glottal volume velocity waveform. This paper discusses the properties of this glottal model with respect to key time domain features and concludes that it is physiologically enrealistic for normal waveform representation. Implications for the performance of LPC analysis schemes, and the perceptual quality of vowel sounds synthesized using this model, are discussed.
This article presents the performance results of a low-bit rate image codec for videophone and B/2B channel ISDN applications. At the receiver, the missing frames are constructed by linear interpolation using coded frames and respective motion vectors. The effectiveness of the codec is evaluated by employing two video sequences as input. The resulting video sequences indicate that for videophone applications, an acceptable quality of image is obtained. Therefore, this codec can be a good candidate for a videophone in ISDN network. Finally, the implementation considerations of this codec are also given.
This paper summarizes results from recent studies on the role of long-term memory in speech perception and spoken word recognition. Experiments on talker variability, speaking rate and perceptual learning provide strong evidence for implicit memory for very fine perceptual details of speech. Listeners apparently encode specific attributes of the talker's voice and speaking rate into long-term memory. Acoustic-phonetic variability does not appear to be “lost” as a result of phonetic analysis. The process of perceptual normalization in speech perception may therefore entail encoding of specific instances or “episodes” of the stimulus input and the operations used in perceptual analysis. These perceptual operations may reside in a “procedural memory” for a specific talker's voice. Taken together, the present set of findings are consistent with non-analytic accounts of perception, memory and cognition which emphasize the contribution of episodic or exemplar-based encoding in long-term memory. The results from these studies also raise questions about the traditional dissociation in phonetics between the linguistic and indexical properties of speech. Listeners apparently retain non-linguistic information in long-term memory about the speaker's gender, dialect, speaking rate and emotional state, attributes of speech signals that are not traditionally considered part of phonetic or lexical representations of words. These properties influence the initial perceptual encoding and retention of spoken words and therefore should play an important role in theoretical accounts of how the nervous system maps speech signals onto linguistic representations in the mental lexicon.
In this article, we present a multisensorial solution for road obstacle detection and tracking. This solution is based on a mixed camera/3D sensor mounted on the front of an experimental vehicle. The multisensor is described. The calibration step enables the matching of the heterogeneous data. Two capabalities of the sensor have been developped: the controlled perception making possible the acquisition of depth data in an area defined in the intensity image; the visual servoing carrying out the focusing of the laser beam on a moving target detected in the intensity image. These two capabalities allow a feedback control on the acquisition mode of the sensor according to the environment. The perception strategy is based on the selection of the best sensor for a given goal. The obstacle detection is based on the segmentation and interpretation of depth data which are well suited in this context. However, the rate of acquisition of these data is too slow in order to extract the kinematic state of the obstacle. So, the tracking process is based on the collaboration between intensity image processing which ensures the tracking itself and a 3D process which returns the obstacle model size to search in the image. This algorithm of heterogeneous data fusion, associated with a Kalman filtering, permits to compute the state of obstacles. This work fits into the european project PROMETHEUS. Experimental results have been validated in real situation on the Prolab vehicle.
This paper describes a monosyllabic corpus for use in testing the consonant intelligibility of synthesized speech. It differs from those used in other tests in that it spans a wide variety of English sounds and is thus useful for diagnosis as well as for comparative assessment. Some “standard” tests of intelligibility use restricted phonetic material, which is possibly easier to understand than a representative sample of English; thus, the results from those tests may not reflect the intelligibility of a wider sample of speech. For illustration, we present the results of a telephone comparison between a demisyllable synthesizer currently being developed at Bellcore (“Orator”), a commercially available phoneme-based synthesizer, and natural speech obtained from 2 talkers. The natural speech data can be used by other laboratories wishing to compare the consonant intelligibility of other synthesis systems to natural speech.
A Voice Oriented Interactive Computing Environment (VOICE) has been implemented in the Hindi language. The system provides in interactive facility for visual and voice feedback. The 200 isolated word recognition system is designed around a railway reservation enquiry task and uses acoustic-phonetic segments as the basic units of recognition. Frame level classification into broad acoustic-phonetic categories is accomplished by a maximum likelihood classifier and segmentation by hierarchical clustering of the frame level likelihood vectors by use of explicit duration semi (Hidden) Markov Models. A more detailed classification of a few categories (vowels, voice bar and nasals in the first instance) is performed by neural nets. String matching using dynamic programming accomplishes lexical access, or conversion of the phonetic category symbol strings into words. Distributed processing of the word recognition task enables recognition at four times real time. A language processor disambiguates between multiple choices given by the recognizer for each word and even corrects some acoustic level recognition errors. This, the first system working in any Indian language, gives a recognition performance of 85% at the word level. For comparison, a purely HMM based word level recognizer has also been implemented. The performance is expected to improve further as there is still substantial scope for refinement.
Several efficient Variable-Bit-Rate MR) methods suitable for low-delay Delayed-Decision Tree-Code (DDTC) and CELP speech coders are presented in this paper. These methods are based on modifying the existing codebook(s) by means of block or lattice codes. To achieve a reduced rate operation of the DDTC coder a novel technique based on a parity code is developed and up to 2 dB SNR improvement uver conventional methods is demonstrated. For extended rate operation of the LD-CELP coder an approach based on a lattice code is used. The modified or additional lattice codebook has a geometrical (algebraic) structure, allowing the search procedure to be performed efficiently and with minimal additional memory. Simulation results of these VBR methods for a 16 kbit/s DDTC coder and for the 16 kbit/s LD-CELP coder proposed for the CCITT recommendation (G.728) are presented.
A method is proposed to allow the retrieval of the identity of the writer of a non-constraint handwritten text by matching it with some reference handwritten documents. The matching is based on a metric computed on the distributions of the allograph of the letters featuring a unique writing style. An automatic system segments the text into characters and assigns a partial membership to the different representative prototypes of the considered letter of the Roman alphabet. Two different datasets are used to assess this system. Online handwriting is considered by this system.
Two experiments use a procedure developed by Carter and Bradshaw (Speech Communication Vol. 3 (1984) pp. 347–360) to examine the role of syllable structure in speech production. In the procedure, subjects exchange phonological segments in corresponding positions of a pair of visually-presented words or nonwords and to produce the resulting words or nonwords as quickly as possible. Carter and Bradshaw have shown that the pattern of latencies mirrors that of frequencies of exchange errors in natural speech. The first experiment of the present study shows that initial consonant exchanges are promoted by phonetic similarity of the exchanging consonants and they reflect a bias for producing real words. With these influences controlled, Experiment 2 replicates and extends the finding of Carter and Bradshaw that initial consonant exchanges are made more rapidly than final consonant exchanges. The discussion relates the latency difference between these conditions to a difference in the “cohesiveness” of initial and final consonants with their vowel. In particular, in Experiment 2, more than one-third of errors made on final-consonant or vowel exchanges are exchanges of the whole syllable rhyme (VC), whereas just 10% of errors made on initial consonant or vowel exchanges are exchanges of the initial CV of the word. Various explanations for the difference in cohesiveness are examined in post hoc analyses.
This paper proposes a review of thirty years of the development of demosaicing algorithms used in digital camera for the reconstruction of color image. Most recent digital camera used a single sensor in front of a color filter array is placed. This sensor sample a single chromatic value per spatial position and an interpolation algorithm is needed for the definition of a color image with three components per spatial position. This article shows that the whole signal and image processing technics have been used for solving this problem. Moreover, a new method proposed recently by the author and collaborators is decribed. This method based on a model of chromatic sampling by the cones in the retina highlights the nature of spatio-chromatic sampling in digital camera with single sensor.
Nearly four decades of research in speech perception have failed to untangle the relation of sound and phone. Is this because speech sound discrimination is so complex or unique that it resists study? Or is it because the right questions have not been asked? This article reviews some of the more recent speech research at the Pavlov Institute in Leningrad which suggests an affirmative answer to the second question. This interesting possibility derives from an unique modulation-analyzing model of speech perception. Wider consideration of the modulation approach might provide some challenging alternatives to current, spectrum-oriented models for solving the “invariance” problem. Following a general introduction, the model is considered in the context of representative experiments and results.
In this paper, we provide an analysis of Genetic Programming (GP) from the Statistical Learning Theory viewpoint in the scope of symbolic regression. Firstly, we are interested in Universal Consistency, i.e. the fact that the solution minimizing the empirical error does converge to the best possible error when the number of examples goes to infinity, and secondly, we focus our attention on the uncontrolled growth of program length (i.e. bloat), which is a well-known problem in GP. Results show that (1) several kinds of code bloats may be identified and that (2) Universal consistency can be obtained as well as avoiding bloat under some conditions. We conclude by describing an ad hoc method that makes it possible simultaneously to avoid bloat and to ensure universal consistency.
Neural networks are shown to be a class of non-linear adaptive filters, which can be trained permanently with a possibly infinite number of time- ordered examples ; this is an altogether different framework from the usual, non-adaptive training of neural networks.
This paper presents experimental results obtained with an original architecture that can do generic learning for randomly observable factored Markov decision process (ROFMDP). First, the paper describes the theoretical framework ofROFMDP and the working of this algorithm, in particular the parallelization principle and the dynamic reward allocation process. Then, the architecture is applied to two navigation problems (gridworld and New York Driving). The tests show that the architecture allows to learn a good and generic policy in spite of the large dimensions of the state spaces of both systems.
In this theoretical paper, we compare the “classical” learning techniques used to infer regular grammars from positive examples with the ones used to infer categorial grammars. To this aim, we first study how to translate finite state automata into categorial grammars and back. We then show that the generalization operators employed in both domains can be compared, and that their result can always be represented by generalized automata, called “recursive automata”. The relation between these generalized automata and categorial grammars is studied in detail. Finally, new learnable subclasses of categorial grammars are defined, for which learning from strings is nearly not more expensive than from structures.
Recently proposed connectionist models of acquired linguistic behaviors have linguistic rule-based representations built in. Similar connectionist models of language acquisition have arbitrary devices and architectures which make them mimic the effect of rules. Connectionist models in general are not well-suited to account for the acquisition of structural knowledge, and require predetermined structures even to simulate basic linguistic facts. Such models are more appropriate for describing the formation of complex associations between structures which are independently represented. This makes connectionist models potentially important tools in studying the relations between frequent behaviors and the structures underlying knowledge and representations. At the very least, such models may offer computationally powerful ways of demonstrating the limits of associationistic descriptions of behavior.
The «Traité chinois des particules et des principaux termes de grammaire», included in Stanislas Julien's Syntaxe nouvelle de la langue chinoise (1869), represents the first translation of Wáng Yǐnzhī's Jīngzhuàn shìcí 經傳釋詞 (Explanation of Particles in the Classics and the Commentaries, 1819) in a Western language. If Wáng's work can be considered the most important dictionary of grammatical particles in the Chinese philological tradition, its translation constitutes a remarkable example of presentation of Chinese philological methodologies and linguistic terminology for a European readership. This article presents and compares the two works, analyzing the ways of translating the entries as well as the ways of transposing linguistic categories and terminology.
This study used a multi-talker database containing intelligibility scores for 2000 sentences (20 talkers, 100 sentences), to identify talker-related correlates of speech intelligibility. We first investigated “global” talker characteristics (e.g., gender, F0 and speaking rate). Findings showed female talkers to be more intelligible as a group than male talkers. Additionally, we found a tendency for F0 range to correlate positively with higher speech intelligibility scores. However, F0 mean and speaking rate did not correlate with intelligibility. We then examined several fine-grained acoustic-phonetic talker-characteristics as correlates of overall intelligibility. We found that talkers with larger vowel spaces were generally more intelligible than talkers with reduced spaces. In investigating two cases of consistent listener errors (segment deletion and syllable affiliation), we found that these perceptual errors could be traced directly to detailed timing characteristics in the speech signal. Results suggest that a substantial portion of variability in normal speech intelligibility is traceable to specific acoustic-phonetic characteristics of the talker. Knowledge about these factors may be valuable for improving speech synthesis and recognition strategies, and for special populations (e.g., the hearing-impaired and second-language learners) who are particularly sensitive to intelligibility differences among talkers.
The development of a high accuracy (about 99%) text-independent speaker recognition system is discussed in this paper in two stages. The first stage deals with the evaluation of the speaker selectivity characteristics of the various parameter sets that characterize human speech. The second stage utilizes any two parameter sets of the first stage tests and combines these logically to obtain a significantly higher recognition accuracy than is possible with any single-speaker-sensitive parameter set. The algorithm utilizes additional utterances to resolve contradictory decisions from the two parameter sets resulting from the first test utterance. For completeness of discussion a brief review of relevant literature in the field is also presented.
A two-level scheme for speaker identification is proposed. The first classifier level is based on the self-organizing map (SOM) of Kohonen. The PDMs are the input for the second classifier level. The second level consists of multilayer perceptron (MLP) networks for each speaker. The first level of the classifier is a preprocessing procedure for the second level, where the final classification is made. The goal of the proposed approach is to combine the advantages of the two type of networks into one classification scheme in order to achieve higher identification accuracy. The experiments show an increased accuracy of the proposed two-level classifier, especially in the case of noise-corrupted signals.
We define prescription as any intervention in the way another person speaks. Long excluded from linguistics as unscientific, prescription is in fact a natural part of linguistic behavior. We seek to understand the logic and method of prescriptivism through the study of usage manuals: their authors, sources and audience; their social context; the categories of “errors” targeted; the justification for correction; the phrasing of prescription; the relationship between demonstrated usage and the usage prescribed; the effect of the prescription. Our corpus is a collection of about 30 usage manuals in the French tradition. Eventually we hope to create a database permitting easy comparison of these features.
A topic of importance in reinforcement learning is online value function approximation. Related algorithms should exhibit some features such as sample efficiency, tracking the solution rather than converging to it (especially because control and learning are interleaved) and maintaining an uncertainty information about approximated values. A Kalman-based Temporal Differences framework is introduced to deal with all these aspects at the same time. A form of active learning which uses the available uncertainty information is also introduced, and the proposed framework is compared to state-of-the-art algorithms on classic benchmarks.
Dynamic elastography using ultrasound radiation force is an imaging technique of biological tissues elastic properties. In a mechanical point of view, biological tissues are supposed isotropic, so their properties are independent of the reference axis. In these mediums, the tensor of elastic constants can be expressed as a function of two independent constants: the elastic bulk modulus K (which is linked to the compression wave propagation) and the elastic shear modulus μ (which is linked to the shear wave propagation). The development of some cancers can result in weak variations of the bulk elastic modulus, but can considerably modify the shear elastic modulus. Measurement of m can then help for the diagnosis of this type of tissue pathology. A judicious mean to measure this parameter is the use of a non-linear effect called ultrasound radiation force. This force is proportional to the attenuation and the intensity of the ultrasound beam emitted by the imaging system. This stress source essentially generates a shear wave that propagates with a velocity proportional to the shear modulus and with a purely transverse polarisation in the far-field (far from the stress source). Measurement of the medium displacements induced by shear wave propagation can allow to calculate the shear modulus of the medium (inverse problem resolution). We performed these measurements from the radio-frequency (RF) lines obtained with an imaging ultrasound transducer. This work describes precisely the signal processing realized on the RF lines. This processing is based on the use of a delay estimation method to measure temporal delays between RF lines during the shear wave propagation. Influence of different parameters (length of the analyse window, Signal to Noise Ratio of RF lines, sampling frequency, ultrasound transducer characteristics…) on the measurement precision has been studied. We present displacement curves as a function of time obtained after optimisation of processing parameters. Experimental results have been favourably compared to a physical model and allowed us to calculate the shear modulus of the medium.
To estimate the orientation angle, we propose to compute a time-frequency representation of the analytic signal x a (t) the centered squared root of the projection histogram x(t) of the document. The projection angle corresponding to the histogram with the highest maximum value of its time-frequency representation is considered as an estimation of the document orientation. The experiments were prepared after a manual orientation of the documents into different angles ranging from −75° to +90°. We found that the Wigner-Ville distribution reaches the highest estimation rate (100%).
A new method for automatically acquiring Fragments for understanding fluent speech is proposed. The goal of this method is to generate a collection of Fragments, each representing a set of syntactically and semantically similar phrases. First, phrases observed frequently in the training set are selected as candidates. Each candidate phrase has three associated probability distributions: of following contexts, of preceding contexts, and of associated semantic actions. The similarity between candidate phrases is measured by applying the Kullback–Leibler distance to these three probability distributions. Candidate phrases that are close in all three distances are clustered into a Fragment. Salient sequences of these Fragments are then automatically acquired, and exploited by a spoken language understanding module to classify calls in AT&T's “How may I help you?” task. These Fragments allow us to generalize unobserved phrases. For instance, they detected 246 phrases in the test-set that were not present in the training-set. This result shows that unseen phrases can be automatically discovered by our new method. Experimental results show that 2.8% of the improvement in call-type classification performance was achieved by introducing these Fragments.
Articulatory trajectories of an articulatory model were recovered by means of a genetic algorithm from acoustic information using a task-dynamic model of speech articulation. Tests on simulated utterances / əbæ/ and / ədæ/ show that the method can recover most of parts of an original trajectory, but it has trouble in obtaining precise timing. For the recovery of articulation, formant frequency trajectories should be supplemented by additional acoustic information, such as RMS amplitude.
The distinction between what is planned and what is done is well known. One contrasts the prescribed task to the effective task, the logic of functioning to the logic of use, procedures to practices, etc. Indeed, there is a solution, thanks to a uniform representation of elements of reasoning and of contexts, called Contextual Graphs. We propose in this paper a notion of contextualized task model that is an operational intermediate between prescribed and effective tasks. Using such contextualized task models would lead to developing more robust procedures as shown in four applications.
Subjects naturally integrate auditory and visual information in bimodal speech perception. To assess the robustness of the integration process, the relative onset time of the audible and visible sources was systematically varied. In the first experiment, bimodal syllables composed of the auditory and visible syllables /ba/ and /da/ were present at five different onset asynchronies. The second experiment replicated the same procedure but with the vowels /i/ and /u/. The results indicated that perceivers integrated the two sources of information at all asynchronies. Cluster responses (for example, /bda/ given visual /ba/ and auditory /da/) occurred primarily for the consonants but not for the vowels. In addition, cluster responses require that both the visual and the auditory information be reasonable compatible with the physical properties of a cluster articulation. For both vowels and consonant-vowel syllables, information from the auditory and visual sources is continuous, independent and combined in a three-stage process of feature evaluation, integration and decision.
Most of object recognition schemes fail in case of illumination changes between the color image acquisitions. One of the most widely used solutions to cope with this problem is to compare the images by means of the intersection between invariant color histograms. Unlike the classical invariant color histograms approach which independently analyzes each image, we consider each pair constituted by the query image and one of the target images constructed during the retrieval. In this paper, we propose a new approach based on color histograms which are adapted to each pair constituted by the query image and one of the target images. These adapted color histograms are determined so that their intersection is high only when the objects contained in the two images are similar. The adapted color histograms processing is based on an original model of illumination changes based on the rank measures of the pixels within the color component images.
This paper describes a resource designed for the general study of spontaneous speech under the stress of sleep deprivation. It is a corpus of 216 unscripted task-oriented dialogues produced by normal adults in the course of a major sleep deprivation study. The study itself examined continuous task performance through baseline, sleepless and recovery periods by groups treated with placebo or one of two drugs (Modafinil, d-amphetamine) reputed to counter the effects of sleep deprivation. The dialogues were all produced while carrying out the route communication task used in the HCRC Map Task Corpus. Pairs of talkers collaborated to reproduce on one partner's schematic map a route preprinted on the other's. Controlled differences between the maps and use of labelled imaginary locations limit genre, vocabulary and effects of real-world knowledge. The designs for the construction of maps and the allocation of subjects to maps make the corpus a controlled elicitation experiment. Each talker participated in 12 dialogues over the course of the study. Preliminary examinations of dialogue length and task performance measures indicate effects of drug treatment, sleep deprivation and number of conversational partners. The corpus is available to researchers interested in all levels of speech and dialogue analysis, in both normal and stressed conditions.
In this paper, we describe different multi-microphone noise reduction techniques as front-ends for a speaker-independent isolated word recognizer in an office environment. Our focus lies on examining the recognition rate if the noise source is not Gaussian and stationary, but a second speaker in the same room. In this case, standard noise reduction techniques like spectral subtraction fail, whereas multi-microphone techniques can raise the recognition rate by using spatial information. We compare the delay-and-sum beamformer, superdirective beamformers, and two post-filter systems. A new adaptive post-filter for superdirective beamformers (APES) is introduced. Our results show that multi-microphone techniques can increase the recognition rate significantly and that the new APES system outperforms related techniques.
This paper explores the possibility of using automatic speech recognition as a front end to a computer for Chinese character processing. A speech recognition experiment has been performed with the complete inventory of second-tone syllables of Standard Chinese. It is shown that the distribution of intrasyllable distances and the distribution of intersyllable distances overlap considerably for the full inventory of 260 second-tone syllables. The recognition rate was determined as a function of the syllabary size and is 47.3% for the complete syllable inventory.
Vowel formant target frequencies from different talkers depend on the details of the vocal tract, sex, regional accent, speaking habits and other factors. Good vowel recognition and studies of vowels from different talkers require an accurate method for compensating for speaker differences in these frequencies. Various methods of compensating for speaker variation in formants were studied. Bark scaled formants and subtraction of Bark fundamental frequency from the first formant was tried first. In spite of recent published papers on the efficacy of this technique, it was found inadequate. The transformations were incapable of improving the clusters of the cardinal vowels, for example. A modification of the Gerstman technique, determining the speaker's formant range and then transforming into an “ideal” talker's range, was found to account for most of the variance due to different talkers given a small amount of training data. This technique was applied to vowel in context studies on American English. Formant ranges were studied for 125 talkers of General American English. Plots of formant ranges for males and females showed interesting patterns. The lower limit of the second formant was not very different, while the lower limit of the first formant was lower for males. Both the first and second formant maxima were larger for females. The modified Gerstman transformation was able to superimpose the formant targets for the same vowel in the same context from different talkers into the same region of F1, F2 space. There remained some residual variance between male and female, even after the transformation. These trends are shown in a series of plots of vowel target frequency data.
The gain portion of a shape-gain quantizer is made adaptive, yielding a vector quantizer that can adjust itself to the time-varying amplitude of a speech signal. The adaptive version requires negligible increase in computational effort. Design methods for optimizing the noise-to-signal or arithmetic segmented noise-to-signal ratio are presented; both yield comparable results. The quantizer performs best on low-frequency portions of speech, due to the smoothness of the waveform there. Overall performance is comparable to the backward-adaptive quantizer of Chen and Gersho at rate one. and slightly inferior at rate two.
Very small spectral irregularities may be detected by the auditory system, which suggests the existence of a kind of frequency derivation-like mechanism. On the other hand, the “center of gravity” phenomenon suggests the existence of an integration-like mechanism. Our goal here is to study these two facts through the determination of psychophysical estimates of the internal representations of static harmonic spectra. It is commonly agreed that the pulsation threshold technique provides estimates of the results of the peripheral spectral analysis of such signals, accounting for the lateral suppression phenomena. The first part of this study consisted of the determination of a good experimental procedure for such a task. We report some observations we made about the pulsation threshold test and the solutions adopted. Then we present a preliminary set of results showing how the internal representation of a one-formant static sound is modified by the emergence of a second formant or by the value of the lateral slopes of the formant. The main conclusion is that smoothing of the spectrum due to non-infinite frequency selectivity dominates the lateral suppression effects in both cases.
Word preselection is achieved by segmenting and classifying the input signal in terms of 6 broad phonetic classes. In the second pass, word verification, a detailed representation of the phonemic structure of word candidates is used for estimating the most likely words. Each word candidate is modeled by a graph of subword Hidden Markov Models. Again, a tree-structure of the whole word subset is built online for an efficient implementation of a beam-search Viterbi algorithm that estimates the likelihood of the candidates. The results show that a complexity reduction of about 73% can be achieved by using the two pass approach with respect to the direct approach, while the recognition accuracy remains comparable.
COMPOST consists of a specialised programing language for text-to- speech synthesis. Each text-to-speech system describes the different steps which convert a running text towards its acoustic and/or visual synthesis by means of a main program called a scenario. The ideas presented in this article are enlighted by concrete examples extracted from a text-to-speech system for French developped using COMPOST.
Previous research has shown that, in a phoneme detection task, vowels produce longer reaction times than consonants, suggesting that they are harder to perceive. Another way of accounting for the findings would be to relate them to the differential functioning of vowels and consonants in the syllabic structure of words. In this experiment, we examined the second possibility. Targets were two pairs of phonemes, each containing a vowel and a consonant with similar phonetic characteristics. Subjects heard lists of English words had to press a response key upon detecting the occurrence of a pre-specified target. This time, the phonemes which functioned as vowels in syllabic structure yielded shorter reaction times than those which functioned as consonants. This rules out an explanation for response time difference between vowels and consonants in terms of function in syllable structure. Instead, we propose that consonantal and vocalic segments differ with respect to variability of tokens, both in the acoustic realisation of targets and in the representation of targets by listeners.
An amplitude-domain quotient for parametrization of the glottal source computed by inverse filtering is presented. The new quotient, AQ, is determined as the ratio between the amplitude of the AC-flow of the glottal waveform and the amplitude of the minimum of the flow derivative. This quotient can be used even though absolute flow values are not given by the recording equipment. The behaviour of AQ was compared to conventional time-based quotients by analysing voices produced by different phonation types. It was shown that phonation types can be quantified effectively when parametrization of the glottal flow estimated by inverse filtering is based on AQ.
The ability of two bottlenosed dolphins (Tursiops truncatus) to understand imperative sentences expressed in artificial languages was studied. One dolphin (Phoenix) was tutored in an acoustic language whose words were computer-generated sounds presented through an underwater speaker. The second dolphin (Akeakamai) was tutored in a visually-based language whose words were gestures of a trainer's arms and hands. The words represented agents, objects, object modifiers, and actions and were recombinable, according to a set of syntactic rules, into hundreds of uniquely meaningful sentences from two to five words in length. The sentences instructed the dolphins to carry out named actions relative to named objects and named modifiers; comprehension was measured by the accuracy of response to the instructions and was tested within a format that controlled for context cues, for other nonlinguistic cues, and for observer bias. The successful processing of either a left-to-right grammar (Phoenix) or of an inverse grammar (Akeakamai) indicated that wholly arbitrary syntactic rules could be understood and that an understanding of the function of words occuring early in a sentence could be carried out by the dolphin on the basis of succeding words, including in at least one case, nonadjacent words. The comprehension approach used was a radical departure from the emphasis on language production in studies of the linguistic abilities of apes; the result obtained offer the first convincing evidence of the ability of animals to process both semantic and syntactic features of sentences. The ability of the dolphins to utilize both their visual and acoustic modalities in these tasks underscored the amodal dependency of the sentence understanding skill. Some comparisons were given of the dolphins' performances with those of language-trained apes and of young children on related or relevant language tasks.
Many researches focus on the study of automatic sign language recognition. Many of them need a large amount of data to train the recognition systems. Our work addresses the annotation of sign language video corpus in order to collect training data. We propose a robust tracking algorithm for hands and head, a method to segment hands during occlusions and an approach to segment gestures using motion and hand shape features. In order to show the advantages and limitations of the proposed approaches, we have evaluated each one using international corpus. The full sign segmentation approach shows promising results.
Therefore, in a continuous speech recognizer (CSR) it would appear profitable to train separate models for the stressed and unstressed variants of each vowel. In the experiments reported on here, we applied stress modeling in both training and testing of the recognizer. Recognition experiments on an independent test set showed that recognition rates did not improve by this use of stress in our CSR. However, if we swapped the stress markers in the recognition lexicon the recognition rates did significantly deteriorate. This demonstrated that the acoustic models for the stressed and unstressed variants of the vowels were different. A pitfall in this experiment was that lexical stress information and phonemic context were possibly confounded. In a follow-up experiment we controlled for context by using generalized context-dependent models. In this experiment the recognition results were not improved either, although the vowel models were better tailored to capture lexical stress-related information. We conclude that the mapping of lexical stress to the acoustic surface of fluent speech is not sufficiently straightforward to be of direct benefit for CSR, due to interaction of lexical stress with rhythm and sentence accent in real speech.
In this paper, we present a new approach towards high performance speech/music discrimination on realistic tasks related to the automatic transcription of broadcast news. In the approach presented here, an artificial neural network (ANN) trained on clean speech only (as used in a standard large vocabulary speech recognition system) is used as a channel model at the output of which the entropy and “dynamism” will be measured every 10 ms. These features are then integrated over time through an ergodic 2-state (speech and non-speech) hidden Markov model (HMM) with minimum duration constraints on each HMM state. For instance, in the case of entropy, it is indeed clear (and observed in practice) that, on average, the entropy at the output of the ANN will be larger for non-speech segments than speech segments presented at their input. In our case, the ANN acoustic model was a multi-layer perceptron (MLP, as often used in hybrid HMM/ANN systems) generating at its output estimators of the phonetic posterior probabilities based on the acoustic vectors at its input. It is from these outputs, thus from “real” probabilities, that the entropy and dynamism are estimated. The 2-state speech/non-speech HMM will take these two-dimensional features (entropy and dynamism) whose distributions will be modeled through multi-Gaussian densities or a secondary MLP. The parameters of this HMM are trained in a supervised manner using Viterbi algorithm. Although the proposed method can easily be adapted to other speech/non-speech discrimination applications, the present paper only focuses on speech/music segmentation. Different experiments, including different speech and music styles, as well as different temporal distributions of the speech and music signals (real data distribution, mostly speech, or mostly music), illustrate the robustness of the approach, always resulting in a correct segmentation performance higher than 90%. Finally, we will show how a confidence measure can be used to further improve the segmentation results, and also discuss how this may be used to extend the technique to the case of speech/music mixtures.
In this paper, an algorithm dedicated to image restoration and edge detection is addressed. Its principle is based on synchronous stochastic relaxation and resistive fuses. The adequacy to both image enhancement and the implementation on a cellular architecture are considered. Results of simulations are given and demonstrate the efficiency of our algorithm even on very noisy images.
A synthesis-based method for pitch extraction of the speech signal is proposed. The method synthesizes a number of log power spectra for different values of fundamental frequency and compares them with the log power spectrum of the input speech segment. The average magnitude (AM) difference between the two spectra is used for comparison. The value of fundamental frequency that gives the minimum AM difference between the synthesized spectrum and the input spectrum is chosen as the estimated value of fundamental frequency. The voiced/unvoiced decision is made on the basis of the value of the AM difference at the minimum. For synthesizing the log power spectrum, the speech signal is assumed to be the output of an all-pole filter. The transfer function of the all-pole filter is estimated from the input speech segment by using the autocorrelation method of linear prediction. The synthesis-based method is tried out on real speech data and the results are discussed.
In automatic speaker recognition tasks a situation may occur in which it cannot be assumed that a voice to be recognized belongs to a known set of voice classes (a closed set). Thus, a problem arises with respect to working out a recognition algorithm that could operate in open sets of speakers, i.e. without assuming that a speech sample from an unknown speaker must belong to one of the speakers of a given set. Two “key words” were examined, as were different parameter sets and large and small populations of speakers. The methodological assumptions and experimental results show that the proposed open set voice recognition method is very flexible and makes it possible to adjust global characteristics, i.e. α and β errors, to the strategy adopted by the recognition system. For a given set of voice patterns, it is always possible to optimize recognition by a proper selection of approximations of the ground class distribution, i.e. by an appropriate selection of decision thresholds.
We are interested in providing automated services via natural spoken dialog systems. By natural, we mean that the machine understands and acts upon what people actually say, in contrast to what one would like them to say. There are many issues that arise when such systems are targeted for large populations of non-expert users. In this paper, we focus on the task of automatically routing telephone calls based on a user's fluently spoken response to the open-ended prompt of “How may I help you?”. We first describe a database generated from 10,000 spoken transactions between customers and human agents. We then describe methods for automatically acquiring language models for both recognition and understanding from such data. Experimental results evaluating call-classification from speech are reported for that database. These methods have been embedded within a spoken dialog system, with subsequent processing for information retrieval and formfilling.
Visualization methods do not scale well with high number of features. We present an approach using a consensus theory based feature selection (CTBFS) algorithm, clustering for sampling and visualization for weight assignment in order to aggregate multivariate and multidimensional datasets. We use datasets available in UCI and Kent Ridge Bio Medical Dataset Repositories in order to evaluate the performance of our new approach.
Over the last few years, the DARPA-sponsored Hub-4 continuous speech recognition evaluations have advanced speech recognition technology for automatic transcription of broadcast news. In this paper, we report on our research and progress in this domain, with an emphasis on efficient modeling with significantly fewer parameters for faster and more accurate recognition. In the acoustic modeling area, this was achieved through new parameter tying, Gaussian clustering, and mixture weight thresholding schemes. The effectiveness of acoustic adaptation is greatly increased through unsupervised clustering of test data. In language modeling, we explored the use of non-broadcast-news training data as well as the adaptation to topic and speaking styles. We developed an effective and efficient parameter pruning technique for backoff language models that allowed us to cope with ever increasing amounts of training data and expanded N-gram scopes. Finally, we improved our progressive search architecture with more efficient algorithms for lattice generation, compaction, and incorporation of higher-order language models.
Decision rules resulting from a knowledge discovery in databases process are filtered by means of automated processes. This important stage of the analysis aims at reducing the number of rules which are presented to the expert for a "close" évaluation and is based on the use of indexes which give a quantitative évaluation of the quality of the knowledge. The search ofa good knowledge représentation is directly linked to the search and the use of good indexes. In this article, we present the problem of the relevant choice of an index by an expert user who has some preferences on the characteristics of the index. We show how it is possible tofind a good compromise by means of a "multicriteria decision aiding" approach.
The relationship between different levels of sentence representation and vocabulary types was investigated. The experiment used a word-monitoring task, which varied the target's word class (open and closed) as well as the functional role of different closed class elements (lexical prepositions, obligatory prepositions, and verb particles). The presence of different context sentence (semantically related/unrelated) was used to estimate the effect of preceding semantic and/or syntactic information on the different types of items. The combined results of normal and agrammatic subjects provide evidence for a computational distinction of different vocabulary types, and consequently, their attribution to different levels of sentence processing. Furthermore, they suggest that lexical and non-lexical information is generally processed at different levels, even if both types of information are carried by one item.
We propose a new approach of contrast in digital image using a multiresolution framework. Contrast enhancement is the main application by mean of an iterative process. Moreover, we show that when iterated, this process ends up with a simplified, e.g. binary, image from any initial image. Several examples are presented all over the paper showing the performance of our algorithm on synthetic as well as real scenes.
The semantic networks that are built today are called ontologies. This choice rests not only on the tradition of Logic, but also on Metaphysics. Unfortunately, the vocabularies of natural languages are not shaped like ontologies: semantic relations are in fact much more complex and varied than ontologic relations. To build a " de-ontology ", we have to take into account the diversity of discourses and genres, because a unique ontology remains illusory ; we have to insist on the problem of semiotic heterogeneity of texts, on complex correlations beetween meanings and expressions, on the role of context. These are some requirements to fulfil characterization tasks, especially in corpus linguistics.
Region oriented image representation offers several advantages over block-oriented schemes, e.g. adaptation to the local image characteristics, or object motion compensation as opposed to block-wise motion compensation. For the task of image data compression, i.e. image coding, new algorithms are needed which work on arbitrarily shaped image regions, called segments, instead of rectangular image blocks. Based on a generalized moment approach, the luminance function inside the segment is approximated by a weighted sum of basis functions, for example polynomials. A set of basis functions which is orthogonal with respect to the shape of the segment to be coded can be obtained using orthogonalization schemes. This results in the derivation of a generalized shape-adapted transform coder. Suitable coder and decoder structures are introduced which do not necessitate the transmission of the basis functions for each segment. Finally an application of the derived algorithms to image sequence coding at low data rates is shown, which is based on a segmentation of the motion compensated prediction error image.
This paper describes the test methodology employed for the performance evaluation of the CCITT 16 kbit/sLD-CELP coder (Recommendation G.728) with non-voice signals. This methodology is sufficiently general that it can be employed for the evaluation of other low transmission-rate digital speech coders. The types of non-voice signals considered in this paper include voiceband data, network signaling, circuit continuity tones and dual-tone multi-frequency signaling.
In this study a new scaling technique is presented which makes it possible to estimate the voice source including its amplitude values by inverse filtering the speech pressure waveform without applying a flow mask. The new technique is based on adjusting the DC-gain of the vocal tract model in inverse filtering to unity. The performance of the new method is tested by analysing correlation between the minimum peak amplitude of the differentiated glottal flow given by the new technique and the sound pressure level of speech. The results show that the new method yields reliable information of the amplitude values of the glottal source without applying a flow mask.
The mismatch between the acoustic conditions during training and recognition often causes a performance deterioration in practical applications of speech recognition systems. Two important effects are the presence of a stationary background noise and the frequency response of the transmission channel from the speaker to the audio input of the recognizer. The original contribution of this work are two signal processing schemes for the estimation of the actual noise spectrum and the difference of the frequency responses between training and recognition. The estimated noise components are taken to adapt the cepstral parameters of the recognizer's references which are described by hidden Markov models (HMMs). The adaptation process is based on the parallel model combination (PMC) approach (M.J.F. Gales, Model based techniques for noise robust speech recognition, Dissertation at the University of Cambridge, 1995). For speaker independent connected or isolated word recognition considerable improvements can be achieved in the presence of just one type of noise as well as in the presence of both types together. Furthermore this adaptation scheme is integrated as part of a complete dialogue and recognition system which is accessible via the public telephone network. The usability and the gain in recognition performance is shown for this application in a real telecommunication scenario under consideration of all real-time aspects.
The SPELL workstation is intended to be a teaching device aimed at intermediate ability foreign language learners. Audio and visual aids will be used to help students improve their general intelligibility within a basic teaching paradigm called DELTA (Demonstrate, Evaluate Listening, Teach and Assess). Prosodic analysis will apply to the features of intonation, stress and rhythm. A phonological approach is used for intonation which provides a well-structured system of contrasting units that correlate with discrete linguistic functions. A more limited approach to the prosodic phonology of stress and rhythm will be taught in the SPELL system by manipulating the relatively simple acoustic features of vowel quality and segmental duration. The micro feature analysis will focus on the segmental class of vowels. A distinctive feature approach is used to characterize non-native vowel pronunciation. Acoustic properties are sought which will be speaker-independent.
This study begins to explore the importance of the physiological domain in voice transformation. A general approach is outlined for transforming the voice quality of sentence-level speech while maintaining the same phonetic content. Transformations will eventually include gender, age, voice quality, emotional state, disordered state, dialect or impersonation. In this paper, only a specific voice quality, twang, is described as an example. The basic question is: relative to pure signal processing, can voices be transformed more effectively if biomechanical, acoustic and anatomical scaling principles are applied? At present, two approaches are contrasted, a Linear Predictive Coding approach and a biomechanical simulation approach.
Mining knowledge from structured data has been extensively addressed in the few past years. Structure of these objects is irregular and it is clever to think that a query on documents structure is almost as important as a query on data. Moreover, manipulated data is not static because new updates are constantly realised. Problem of maintaining such substructures is then prior on researching them because, every time data is updated, found substructures could become invalid. In this paper we propose a system, called AUSMS. (Automatic Update Schema Mining System), allowing data retrieval, researching frequent sub-structures and maintaining extracted knowledge after sources evolutions.
In this article I examine the correspondences found between Western Old Japanese high vowels and Eastern Old Japanese midvowels in light of the recent hypotheses concerning the Proto-Japonic vowel system. Correspondences in both the morphology and the lexicon are established and then comparative evidence from several modem Japanese and Ryukyuan dialects is adduced to show that these are instances of retention of Proto-Japonic *e and *o.
This paper addresses the problem of regression in the case of non-uniform sampled signals. Our method is based on supervised learning theory, we propose to use L2 estimation with wavelet kernels combined with L1 multiscale regularization. The use of Least Angle Regression as solver enable us to propose new solutions to set the regularization parameter.
In heterogeneous databases, images often provided from different sources and belong to different topics, hence there is a need for a large description to ensure efficient representation of their content. However, extracted features are not always adapted to the considered image database. In this paper we propose a new image recognition approach based on two innovations, namely adaptive feature selection and Multi-Model Classification Method (MC-MM). The adaptive selection considers only the most adapted features with the used image database content. The MC-MM method ensures image recognition using hierarchically selected features. Experimental results confirm the effectiveness and the robustness of our proposed approach.
Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).
The authors propose a novel way to implement a speech dialogue system. The method, called Simultaneous Understanding, accomplishes speech recognition, understanding and action simultaneously with the user's utterance. This makes the dialogue system more interactive, because of the following advantages: the user does not have to wait for the system response, unless he wants to see the action results on the screen. If he sees a mis-recognized entry, he can correct it before the database is accessed. The authors implemented two ticket reservation speech dialog systems, using an existing speech recognition system. One of them was based on the above method, which can accept new inputs while simultaneously analyzing the previous utterance, and the other accepts new inputs after analyzing the previous utterance. It was found that the above method improved the total average utterance accuracy by 5.4% and the total time spent to solve tasks by 4.7%. This shows that the new method is promising for increasing the interactiveness for the speech dialogue system.
Can supra-normal spectral contrast in an acoustical speech signal compensate for the degraded frequency selectivity accompanying hearing losses of cochlear origin? Subjects were adults either with audiometrically-normal hearing or with moderate hearing impairments of cochlear origin. For both groups of listeners, accuracy of identification decreased as formant bandwidth increased. In the mean data of both groups for syllable-final identification, there was a tendency for accuracy to increase when format bandwidths were set to half their nominally normal values. Psychoacoustical measures of frequency selectivity correlated robustly with accuracy of identification, but again only for final consonants. These results suggest that factors in addition to reduced frequency resolution and reduced absolute sensitivity set limits on the accuracy of speech identification in cases of cochlear hearing loss, particularly for syllable-initial consonants. Possible candidates, not explored here, are reduced temporal resolution and an increased susceptibility to backward masking. The very limited benefits for speech identification which resulted from reducing formant-bandwidths may reflect the inability of that transformation to compensate for impairments to temporal auditory processing, as well as for extreme impairments to frequency resolution.
Although Genevan research has provided a detailed analysis of cognitive structures, our knowledge of cognitive processes remains fragmentary. The focus is now not only on macro-development but also on changes occurring in children's spontaneous action sequences in micro-formation. A series of experiments designed to study goal-oriented behavior is in progress. This paper describes the action sequences of 67 subjects between 4;6 and 9;5 years in a block balancing task. It is not a study of children's understanding of a specific notion in physics, but an attempt to pave the way towards understanding the more general processes of cognitive behavior. The results also suggest certain functional rather than structural analogies between the acquisition of physical knowledge and the acquisition of language.
In automatic speech understanding, division of continuous running speech into syntactic chunks is a great problem. Syntactic boundaries are often marked by prosodic means. For the training of statistical models for prosodic boundaries large databases are necessary. For the German Verbmobil (VM) project (automatic speech-to-speech translation), we developed a syntactic–prosodic labelling scheme where different types of syntactic boundaries are labelled for a large spontaneous speech corpus. This labelling scheme is presented and compared with other labelling schemes for perceptual–prosodic, syntactic, and dialogue act boundaries. The main advantage of the rough syntactic–prosodic labels presented in this paper is that large amounts of data can be labelled with relatively little effort. The classifiers trained with these labels turned out to be superior with respect to purely prosodic or syntactic labelling schemes, yielding recognition rates of up to 96% for the two-class-problem `boundary versus no boundary'.
Simple articulatory contrasts for a phonological opposition generate a multiplicity of acoustic cues. Knowledge of the covariations of acoustic pattern features within and across speakers is needed for automatic speech recognition. A model of speech production processes was used to generate stimuli along an articulatory continuum, the degree of abduction of the vocal folds for the fricatives in English words “hiss” and “his”. Transition times imposed strong constraints on the articulatory plans devised as inputs to the model. Acoustic segment durations output from the model covaried in the same way as those produced by 5 real speakers; there was good quantitative agreement in most cases. Listeners' responses suggest that the articulatory dimension synthesised is a suitable one for natural speech. Future directions for the modelling of multiple articulatory dimensions and for mapping from speaker-specific patterns of articulation and their perturbations onto the stability of particular acoustic cues are discussed.
This paper describes a method for automatic annotation of prosodic events in speech, using segmental duration information. It details a way of differentiating prominence-related lengthening from boundary-related lengthening, using durational clues alone, and discusses an anomaly in the phrasing characteristics of four speakers' readings of 200 phonetically-balanced sentences. An algorithm is described that uses syllable-level differences in normalised segmental duration measures to detect prosodic boundaries in a speech signal. Tests with read-speech data from four British-English RP speakers show high agreement between speakers with respect to the number of boundaries detected and the length of the phrases delimited by each pair of boundaries, but the correlation between speakers on actual boundary locations is low. There is particular disagreement between speakers in the case of a single function word linking two groups of content words. This discrepancy can be resolved if the boundary is taken to be at the function word location itself, rather than at one or other side of the word. These results are taken to indicate some freedom in the placement of prosodic boundaries in such cases, sometimes being cued by a syntactic boundary, and sometimes by a rhythmic one.
Native speakers of Japanese may be unable to correctly identify the phonemes /l/ and /r/ in spoken English. Nevertheless, in perceiving English utterances, they, like native speakers of English, respond to the different acoustic patterns which convey /l/ and /r/ as if they are sensitive to differences in the vocal tract movements that convey /l/ and /r/. Support for this conclusion is provided by a study in which native speakers of Japanese and native speakers of English labelled stimuli along a synthetic /da/-/ga/ continuum when the stimuli were preceded by natural tokens of /s/ or /∫/, /al/ or /ar/. Each pair of precursors had contrasting effects on the location of the category boundary between /da/ and /ga/, and neither the direction nor the extent of contrast depended on native language experience. Significantly, /al/ gave rise to more /ga/ percepts than /ar/ for Japanese and English speakers alike, regardless of their ability to identify /al/ and /ar/, as such. Interpretation of these results rests on previous observations that the contrasting perceptual effects of /al/ vs. /ar/ and /s/ vs. /∫/ find parallels in the acoustic structure of natural utterances of /al-da/, /ar-da/ etc., due to coarticulation of the vocal tract movements that convey the preceding consonant and those that convey the following /da/ or /ga/. Apparently, native speakers of Japanese can be sensitive to the acoustic consequences of coarticulating /l/ or /r/ with /d/ or /g/ while being unable to categorize /l/ and /r/ as different phonemes. Preceding a language-specific level of perception where speech sounds are represented in accordance with the constraints of a given phonological system, there may exist a universally-shared level where the representation of speech sounds more closely corresponds to the articulatory gestures that give rise to the speech signal.
An assessment of a target-based control model of speech production using Feldman's Equilibrium Point Hypothesis is presented. It consists of simulations of articulatory movements during Vowel-to-Vowel sequences with a 2D biomechanical tongue model. In the model the main muscles responsible for tongue movements and tongue shaping in the mid-sagittal plane are represented. The elastic properties are accounted through a Finite-Element modeling, while force generation principles are implemented according to the non-linear force-length Invariant Characteristics proposed by Feldman. Movement is produced through control variable shifts at rates that are constant throughout each transition. The external contours of the model are adjusted to approximate X-ray data collected on a native speaker of French, and it is inserted in the vocal tract contours of the speaker. Emphasis is put on the realism of synthesized formant trajectories, and on the potential influence of biomechanical tongue properties on to measurable kinematic features.
This paper provides a `snapshot' of telephony based speech technology research within European telecommunications companies.
Previous research (Pind, 1986, 1995a) has shown that the ratio of vowel to rhyme (vowel + consonant) duration is a major cue for quantity in Icelandic and serves as a higher-order invariant which enables the listener to disentangle those durational transformations of the speech signal which are due to changes in the speaking rate from those which involve a change of phonemic quantity. For the listener to be able to calculate this ratio, both segments, vowel and consonant, need to be present in the acoustic waveform. This paper reports two perceptual experiments using edited natural speech where a closure following the vowel is either audible or not (in the latter case the closure is unreleased). Results support the hypothesis that the surrounding context will have a greater effect on the location of the phoneme boundaries for vowel quantity in the unreleased syllables since in that case the listener will not be able to calculate the vowel to rhyme ratio.
An experiment has been performed where various two-formant models reported in the literature were assessed as to their ability to predict the formant frequencies obtained in a vowel identification task. An alternative model is proposed in which the auditory processing of vowel sounds is assumed to take place in two stages: a peripheral processing stage and a central processing stage. In the peripheral stage the speech spectrum is transformed to its auditory equivalent and the formant frequencies are extracted from this spectrum using a peak-picking mechanism. The central stage performs a two-formant approximation on the results of the first stage operation, and it is this formant pair that vowel identification is taken to operate on during vowel perception. The first and second formant frequencies of this two-formant model are taken to be equal to the first and second formant frequencies extracted at the first stage plus a perturbation term which accounts for the interaction effects of the neighbouring formants. The perturbation caused by each of these neighbouring formants is inversely proportional to its separation from the main formants. This model compares favourably with previous models in its prediction of the formant frequencies obtained from the vowel identification task.
This paper is concerned mainly with the choice of a figure of merit for representing the performance of connected-word recognisers when DP word-symbol sequence matching is used for the scoring. Properties of the DP scoring method are discussed. Experimental tests using data from the DARPA Resource Management Task confirm a prediction made from random number simulations that DP scoring overestimates substitution errors and underestimates insertion and deletion errors. As a result, the commonly used total error measure has a particularly large bias. The use of an alternative measure, percent correct, results in lower bias but ignores insertion errors. A new figure of merit, weighted total errors, takes all three kinds of errors into account and minimises bias. Finally, some more sophisticated figures of merit are discussed briefly.
The usual request in this framework is the computation of an optimal policy that defines an optimal action for every state of the system. In this paper, we propose a method for refining near optimal policies via online search techniques, by developping from each current state a randomly sampled look-ahead tree. We show on a navigation problem modeled as a stochastic shortest path that this online search strategy provides good “anytime” profiles.
The method exposed in this paper represents a new edge-detection tool of a grey-level image by the cooperation of two technics: wavelet decomposition and neural networks. The first part recalls the necessary background on mono and bidimensional wavelet decomposition and their main properties. The difficult phase of the algorithm lies in the optimal recomposition of different resolutions, in the aim to obtain thin and noiseless edges. This work is given to a neural network which constitutes the object of the second part. The main interest of this new method is to give good results with images whose caracteristics are completly different, without to modify any parameters.
This paper explains how visual information from the lips and acoustic signals can be combined together for speech segmentation. The psychological aspects of lip-reading and current automatic lip-reading systems are reviewed. The paper describes an image processing system which can extract the velocity of the lips from image sequences. The velocity of the lips is estimated by a combination of morphological image processing and block matching techniques. The resultant velocity of the lips is used to locate the syllable boundaries. This information is particularly useful when the speech signal is corrupted by noise. The paper also demonstrates the correlation between speech signals and lip information. Data fusion techniques are used to combine the acoustic and visual information for speech segmentation. The principal results show that using the combination of visual and acoustic signals can reduce segmentation errors by at least 10.4% when the signal-to-noise ratio is lower than 15 dB.
In the framework of an ANN/HMM hybrid system for phone recognition three specialized ANNs were designed and evaluated. One of these ANNs detects the manner of articulation. The other two ANNs describe the speech signal in terms of place of articulation. The design of these networks was inspired by acoustic-phonetic knowledge. Input parameters, ANN topology and desired output representation have been optimized for the specific task of the network. Experiments are reported for the TIMIT database. Frame classification errors of 17.7% with the manner ANN (5 broad classes), 25.4% with the plosive and nasal ANN (10 phones), and 25.2% with the fricative ANN (11 phones) were obtained on a set of 616 sentences from 77 new speakers. Experiments for a prototype ANN/HMM hybrid system are also reported. We developed an algorithm for the global optimization of this hybrid system. The network for the manner of articulation and one network for the place of articulation were merged to a single ANN which outputs were modeled by an HMM. With this globally optimized hybrid system we achieved a recognition accuracy of 86% on an 8 class recognition problem (7 plosives and one class corresponding to all other phonemes).
In this article, we present a new method for the selection of the most pertinent variables to set at the input of a MLP (Multi Layer Perceptron) or RBF (Radial Basis Function) neural network. This method relies on the statistical analysis of the derivatives of the outputs of the network with respect to the inputs. When all the derivates of the outputs with respect to a given input are statistically zeros, the influence of this input is neglectable ans this input may be eliminated. Illustrations will be provided on several simulated or real temporal series prediction models.
This study brings a contribution to non-gaussian textures classification thanks to signatures extracted from high order correlations and/or spectra. The first part exhibits that most of the textures present a non gaussian statistic, well represented for the third order. The chosen normality tests, the Skewness test and the Kurtosis test, have been calibrated for this study. To lower the complexity of algorithms and yet to reach low estimation variances, we choose to focus on third order correlations with the stationary hypothesis: bicorrelation, bispectrum and bicorspectrum. A Row/columns representation of images has been selected. We propose in the second part characterization based on representations describing third order spatial correlations. The last part concerns textures classification using the previous features. Discrimination performances on Mine Brodatz textures are presented and comparison with the results of a coocurence matrix method are provided.
To date most theories of reading ability have emphasized a single factor as the major source of individual differences in performance. However there has been little agreement on what that factor is. However, candidates have included visual discrimination, phonological and semantic recoding, short-term memory, and utilization of linguistic knowledge and context. The single- factor theories are summarized. It is concluded that more complex, multifactor models of reading ability are required, and some recent attempts to collect data conducive to such a model are described. Two methods of conducting component skills analysis are presented, and it is recommended that they be used as converging operations. Finally, the results of a component skills analysis are used to construct a tentative example of a class of hierarchical models of reading ability that can be pursued developmentally.
Pioneering research by Chistovich and her colleagues used speech shadowing to study the mechanisms of immediate speech processing, and in doing so exploited the phenomenon of close shadowing, where the delay between hearing a speech stimulus and repeating it is reduced to 250 msec or less. The research summarised here began with an extension of Chistovich's findings to the close shadowing of connected prose. Twenty-five percent of the women tested were able to accurately shadow connected prose at mean delays ranging from 250 to 300 msec. The other women, and all the men tested, were only able to do so at longer latencies, averaging over 500 msec. There are called distant shadowers. A second series of experiments established that close, just as much as distant shadowers, were syntactically and semantically analysing the material as they repeated it. This was reflected in the ways their spontaneous errors were constrained, and in their sensitivity to disruptions of the syntactic and semantic structure of the materials they were shadowing. A third series of experiments showed that the difference between close and distant shadowers was in their output strategy. Close shadowers are able to use the products of on-line speech analysis to drive their articulatory apparatus before they are fully aware of what these products are. This means that close shadowing not only provides a continuous reflection of the outcome of the process of language comprehension, but also does so relatively unaffected by post-perceptual processes. In this sense, therefore, close shadowing provides us with uniquely privileged access to the properties of the system.
Recent advances in artificial intelligence have changed the fundamental assumptions upon which the progress of computer-aided process engineering (modeling and methodologies) during the last 30 yr has been founded. Thus, in certain instances, numerical computations today constitute inferior alternatives to qualitative and/or semi-quantitative models and procedures which can capture and utilize more broadly- based sources of knowledge. In this paper it will be shown how process development and design, as well as planning, scheduling, monitoring, analysis and control of process operations can benefit from improved knowledge-representation schemes and advanced reasoning control strategies. It will also be argued that the central challenge coming from research advances in artificial intelligence is “modeling the knowledge”, i.e. modeling: (a) physical phenomena and the systems in which they occur; (b) information handling and processing systems; and (c) problem-solving strategies in design, operations and control. Thus, different strategies require different forms of declarative knowledge, and the success or failure of various design, planning, diagnostic and control systems depends on the extent of actively utilizable knowledge. Furthermore, this paper will outline the theoretical scope of important contributions from AI and what their impact has been and will be on the formulation and solution of process engineering problems.
In this article a new shape descriptor - based on minimal graphs - is proposed and its properties are checked through the problem of graphical symbols recognition. Recognition invariance in front shift and multi-oriented noisy object was studied in the context of small and low resolution binary images. The approach seems to have many interesting properties, even if the construction of graphs induces an expensive algorithmic cost. In order to reduce time computing an alternatively solution based on image compression concepts is provided. The recognition is realized in a compact space, namely the Cosine Discrete space. The use of blocks discrete cosine transform is discussed and justified. The experimental results led on the GREC2003 database show that the proposed method is characterized by a good discrimination power, a real robustness to noise with an acceptable time computing.
The book raises several questions about the very notion of punctuation that is developed in it. This critical review draws attention to several confusions noted in some of the contributions. It also proposes a counter-study on a paleographic problem concerning the question mark and, finally, because the term “punctuation” seems to have been used in a too broad sense, wishes to add nuances to what appears to be a terminological drift.
This paper describes an automatic formant tracking algorithm incorporating speech knowledge. It operates in two phases. The first detects and interprets spectrogram peak lines in terms of formants. The second uses an image contour extraction method to regularise the peak lines thus detected. Speech knowledge served as acoustic constraints to guide the interpretation of peak lines. The proposed algorithm has the advantage of providing formant trajectories which, in addition to being sufficiently close to the spectral peaks of the respective formants, are sufficiently smooth to allow an accurate evaluation of formant transitions. The results obtained highlight the interest of the proposed approach.
There is a view that the fundamental processes involved in word recognition might somehow be different for speech and print. We argue that this view is unjustified, and that the models of lexical access developed for the written form are also appropriate for speech, provided that we allow for obvious differences due to the physical characteristics of speech signals. Particular emphasis is given to the role of word frequency in the recognition process, since this places restrictions on the types of models that can be considered (e.g., the cohort model). We reject the view that there are no frequency effects in spoken word recognition, and we also reject the view that frequency effects in printed word recognition can be relegated to the minor status of a post-access decision effect.
In this paper, we address a characterization problem coming from plant biology. The multiple characterization problem consists of entities (represented by a Boolean assignment) belonging to several groups to find a characterization for each group using propositional logic. It implies that this characterization must be exact and the main difficulty is to compute the minimal one. We show that this problem is W[2]-Complete. We also propose a reformulation to a linear program and describe different approaches for the resolution. We experiment on random and real instances whose solutions are used to design a patent-protected diagnostic test.
A method of detecting the pitch frequency of voiced speech is described, which exploits the harmonic structure of the short-time spectrum. The technique is to form a harmonic histogram. Pitch is then detected as the frequency of the largest component. Both linearly and logarithmically weighted histograms are considered, and it is shown that the log weighted histogram gives the clearest indication of pitch. The spectra used in this paper are derived from a digital filter bank having 187 channels between 50 Hz and 3200 Hz, on a log frequency scale. It is shown that a clear indiction of pitch can still be obtained with 'short' spectra covering the band 200–3200 Hz, i.e. in the telephone bandwidth. The influence of the first formant of certain vowels on harmonic amplitudes is shown to prevent a correct indication of pitch. A method of harmonic enhancement is proposed which effectively overcomes this problem. Finally, pitch contours by this method are presented, which show that pitch can be successfully tracked through nasals, vowels and voiced fricatives.
We present the first generalized heuristic search formalism that is able to solve decentralized POMDPs of both finite and infinite horizon. We present a framework that is based on classical heuristic search on the one hand, and on decentralized control theory on the other hand. We prove that our approach is able to generate optimal deterministic controllers, and we study its performance on examples from the literature.
Microphone array systems can be effective in combating the detrimental effects of acoustic noise and reverberation in hands-free telecommunication. This paper discusses classical delay-sum beamformers as well as the more general filter-sum beamformers. Filter-sum beamformers add the ability to control the array beampattern as a function of frequency and a new design method for a constant beamwidth filter-sum beamformer is presented. The delay-sum and filter-sum beamformers require array sizes that are comparable to the acoustic wavelength. These designs can result in arrays that are large in size. For applications that are space constrained, differential microphone array systems are presented. Finally, two types of adaptive beamformers are presented: a broadside array and a two-element differential microphone.
We describe some experiments in voice-to-voice conversion that use acoustic parameters from the speech of two talkers (source and target). Transformations are performed on the parameters of the source to convert them to match as closely as possible those of the target. The speech of both talkers and that of the transformed talker is synthesized and compared to the original speech. The objective of this research is to develop a model for (1) creating new synthetic voices, (2) studying factors responsible for synthetic voice quality, and (3) determining methods for speaker normalization.
Because of the enormous amounts of rules that can be produced by automatic extraction algorithms, knowledge validation is still one of the most problematic steps in an association rule discovery process. It is necessary to help the user appropriate this bulk of rules and to support him in his search for the relevant knowledge by organizing a real rule exploration. In order to do so, the virtual reality techniques can be very profitable, because they combine a strong and intuitive interactivity with immersive 3D representations capable of integrating a large amount of information while remaining intelligible. In this article, we propose a dynamic graphical representation for the association rules, which is based on virtual reality metaphors and supports the user in his rule rummaging task. A first prototype implementing this new representation has been developed in order to validate our approach.
In this paper, we propose an alternative keyword detection method relying on confidence measures and Support Vector Machines (SVM). Confidence measures are computed from phone level information provided by a Hidden Markov Model based speech recognizer. We use three types of average techniques, arithmetic, geometric and harmonic, to compute a confidence measure for each word. We also use support vector machines which are a classification technique developed from the theory of structural risk minimization. The acceptance/rejection decision of a word is based on the confidence measure vector which is processed by a SVM classifier. The performance of the proposed SVM classifier is compared with those of other methods based on the averaging of confidence measures.
Polynomials having only roots with négative real parts and those having only roots inside the unit circle can be characterized in many ways. The criterias introducing a pair of other polynomials with roots alternating on the imaginary axis or on the unit circle are extended to the complex case. An algorithm is established for testing polynomials with real or complex coefficients having all of their roots in a given half-plane, or in a sector defined by two straight lines passing through the origin. A complete proof of the Routh's criterion for testing the continuous-time linear system stability is proposed in both the real and complex cases.
There are familiar terms such as “contour” and “trajectory” to refer to a vowel formant frequency as a function defined on the time axis, but there is no readily understood term for the analogous idea of how a formant behaves on the “vowel axis”. For this we introduce the concept of a vowel-formant ensemble (VFE) as the set of values realized for a given formant (e.g., F2) in going from vowel to vowel among a speaker's vowel phonemes for a fixed time frame in a fixed CVC context. The VFE affords a simple description of our development: we observe that D.J. Broad and F. Clermont's [J. Acoust. Soc. Am. 81 (1987) 155] formant-contour model is a linear function of its vowel target and that as a consequence all its VFEs for a given speaker and formant number are linearly scaled copies of one another. To show how this question can be addressed, we use F1 and F2 data on one male speaker's productions of 7 Australian English vowels in 7 CVd contexts, with each CVd repeated 5 times. Our hypothesized scaling relation gives a remarkably good fit to these data, with a residual rms error of only about 14 Hz for either formant after discounting random variations among repetitions. The linear scaling implies a type of normalization for context which shrinks the intra-vowel scatter in the F1F2 plane. VFE scaling is also a new tool which should be useful for showing how contextual effects vary over the duration of the syllable's vocalic nucleus.
Speech produced by two speakers was manually segmented, generating two databases with 18,000 and 6,000 vowel segments.
Automatic treatment of the specialyzed texts becomes more and more necessary due to their increasy number. Term extraction, i.e. the extraction of groups of words significant for the field, is an information commonly required in the specialized domains. In this paper, we propose a method for an automatic extraction of spécifie tenns. Our input is a corpus of specialized texts, upon which we carry ont pretreatments: cleaning and lahelling. Next, we are using classical association measures to e.xtract the terminology for the field. Our main contribution is adding varions parameters to improve the research for terms.
We investigate the enhancement of speech corrupted by unknown independent additive noise when only a single microphone is available. We present adaptive enhancement systems based on an existing non-adaptive technique [Ephraim, Y., 19992a. IEEE Transactions on Signal Processing 40 (4), 725–735]. This approach models the speech and noise statistics using autoregressive hidden Markov models (AR-HMMs). We develop two main extensions. The first estimates the noise statistics from detected pauses. The second forms maximum likelihood (ML) estimates of the unknown noise parameters using the whole utterance. Both techniques operate within the AR-HMM framework. We have previously shown that the ability of AR-HMMs to model speech can be improved by the incorporation of perceptual frequency using the bilinear transform. We incorporate this improvement into our enhancement systems. We evaluate our techniques on the NOISEX-92 and Resource Management (RM) databases, giving indications of performance on simple and more complex tasks, respectively. Both enhancement schemes proposed are able to improve substantially on baseline results. The technique of forming ML estimates of the noise parameters is found to be the most effective. Its performance is evaluated over a wide range of noise conditions ranging from −6 to 18 dB and on various types of stationary real-world noises.
A novel speech analysis method which uses several established psychoacoustic concepts is applied to the analysis of vowels. This perceptually based linear predictive analysis (PLP) models the auditory spectrum by the spectrum of the low-order all-pole model. The auditory spectrum is derived from the speech waveform by critical-band filtering, equal-loudness curve pre-emphasis, and intensity-loudness root compression. We demonstrate through analysis of both natural and synthetic speech that psychoacoustic concepts of spectral auditory integration in vowel perception, namely the F1, F2′ concept of Carlson and Fant and the 3.5 Bark auditory integration concept of Chistovich, are well modeled by the PLP method. A complete speech analysis-synthesis system based on the PLP method is also described in the paper.
This paper presents the evaluations of the field trials of two information inquiry systems that use different speech technologies and different human–machine interfaces. The first system, RAILTEL, uses isolated word recognition and system driven dialogue. The second system, Dialogos, understands spontaneous speech and implements a mixed initiative dialogue strategy. Both systems allow access to the Italian Railway timetable by using the telephone over the public network. RAILTEL and Dialogos were tested in extensive field trials by inexperienced subjects. Moreover, a comparative trial with a limited number of subjects was performed in order to gain some insights on the impact of the different speech technologies on users' behaviour.
We propose a set of characteristics to identify these gradual rules, and a classification into “direct” rules and “modulation” rules. In neurobiology, pre-synaptic neuronal connections lead to gradual processing and modulation of cognitive information. We propose in the field of connectionism the use of “Sigma-Pi” connections to allow gradual processing in AI systems. In order to represent as well as possible the modulation processes between the inputs of a network, we have created a new type of connection, “Asymmetric Sigma-Pi” (ASP) units. These models have been implemented within a pre-existing hybrid neuro-symbolic system, the INSS system.
We study the adaptation of all existing language-identification system to new languages using a limited amount of training data. The platform used for this study is the system recently developed (Yan and Barnard 1995a, b) to exploit phonotactic constraints based on language-dependent phone recognition. Using the proposed language model re-estimation technique based on probabilistic gradient descent, two new approaches and their combination are proposed and tested. These approaches all modify the phonotactic language models, so that they no longer equal the conventional maximum-likelihood estimate. The difference of these methods can be viewed as different information resampling on the same amount of data. Experiments were conducted using the standard OGI_TS database (Muthusamy et al., 1992). For comparison, the baseline system (with traditional model estimation) was also subjected to the same set of tests. Systems trained with different amounts of training data in the new languages were evaluated. Compared with the conventional model estimation, the results demonstrate that the new methods improve adaptation to new languages. The success of the discriminative model shows that conventional model estimation is not optimal for language identification, so that improvements can be obtained by modifying the maximum-likelihood estimates of the language models.
This paper reports acoustic-phonetic data on three speaking styles: Informal conversational speech, “clear” speech and Baby Talk. These sets of observations illustrate the fact that a speaker's pronunciation of a given linguistic form can undergo rather drastic physical transformations, particularly in the wide range of contexts presented by spontaneously produced speech. Despite their extensive variations, vowel formant measurements showed a high degree of predictability. The findings bring to the fore a classical issue of speech research: signal variability and phonetic invariance. While the present results do not conclusively preclude the possibility that the investigated speech samples are organized around a core of signal invariants, the extent as well as the systematicity of the variability observed lend support to a different perspective. The paper proposes that the variegated acoustic pattern of speech be seen as products of adaptation. According to this interpretation, phonetic gestures and signals are modulated and tuned adaptively in accordance with on-line communicative and socio-linguistic demands (e.g., controlling the “social distance” between speakers, preserving intelligibility, performing “phatic” and “emotive” functions, etc). Furthermore, it is argued that the linguistic task of the phonetic signal is not to encode invariants but to complement information already available to the speech processing system of the listener. Accordingly, intra-speaker phonetic variations need not be seen as invariants embedded in linguistically irrelevant variability. They rather represent genuine behavioral adaptations that may jeopardize or demolish signal invariance but that transform speech patterns in essentially principled ways.
The two main visual sensors in underwater robotics are sonar and video. In a first part, we present the fundamentals of acoustic imagery. If some technics are well known, others, like synthetic aperture antenna, interferometry and parametric array are still research topics. In a second part, acoustic image processing techniques are presented. They are mainly applied to sea bottom characterization and robot navigation. The third part addresses video technology and processing. This sensor is complementary to sonar, due to its high resolution and the ease of interpretation of the images.
This paper approaches the articulatory-to-acoustic speech production inverse case. A framework based on an explicit combination of vocal-tract morphological and acoustic constraints is proposed. The solution is based on a Fourier analysis of the vocal-tract log-area function: the relationship between the log-area Fourier cosine coefficients and the corresponding formants is used to formulate an acoustic constraint. The same set of coefficients is then used to express a morphological constraint. This representation of both acoustic and morphological constraints in the same parameter space allows an efficient solution for the inverse problem. The basis of the acoustic constraint formulation was first proposed by Mermelstein (1967). However, at that time, the combination with morphological constraints was not realized. The method is tested for some vowels. The results confirm the validity of the method, but they also show the need for dynamic constraints.
We are interested by under-determined inverse problems, and more specifically by source localization in magneto and electro-encephalography (M/EEG). Although there is a physical model for the diffusion (or “mixing'') of the sources, the (very) under-determined nature of the problem leads to a difficult inversion. The need for strong and physically relevant priors on the sources is one of the challenge. For M/EEG classical sparsity prior based on the ℓ1 norm is not adapted, and gives unrealistic results. We propose to take into account a structured sparsity thanks to the use of mixed norms, especially a mixed norm with three indices. The method is then applied on MEG signals obtained during somesthetic stimulation. When stimulated, hand fingers activate separate regions of the primary somatosensory cortex. The use of a three level mixed norm allows to take this prior into account in the inverse problem in order to correctly recover the organization of associated brain regions. We also show that classical methods fail for this task.
The problem of sources discrimination, very classical in Signal Processing field, is also an actual problem in biological systems. Biological sensors are sensitive to many sources, so the Central Nervous System processes typically multidimensional signals, each component of which is an unknown mixture of unknown sources, assumed independent. With regard to adaptation rules used in adaptive filtering, here the adaptive increment is achieved necessarily by the product of two non-linear functions. Some experimental results, in Signal Processing and Image Processing fields, show the efficiency of this adaptive algorithm. We prove also the possible generalization of this algorithm in the case of more complex (non-linear, degenerated, etc.) mixtures. This algorithm points out a new concept of Independent Components Analysis, more powerful than this one of Principal Components Analysis, and applicable in the general flame of data analysis.
In this paper, we propose an original method for agreement error detection and correction that we apply to Arabic. The detection is based on a global syntactic analysis of the sentence. It consists of an “extended syntagmatic analysis” that can first locate the syntagm boundaries and, second, regroup all the sentence components that have an agreement relationship. The correction is based on a multicriteria analysis aiming to rank the correction alternatives in order to choose the best one. This method has the advantage of reducing the dominated alternatives and ranking the remaining ones according to different evaluation criteria.
Articulatory paths have been analysed for the frictive [s] produced by a woman speaker of General American English in connected speech and also in phonotically controlled speech-like sequencues at a shower speech rate. Aerodynamically derived traces indicating the acoustically relevant parameter of cross-section area of the constricted region of the vocal tract suggested that, contrary to related data for the movements of the solid structures, some portions of the traces for this vocal tract indicator had the same path shape across some different vowel contexts and across different speech styles. In the connected speech, the whole cross-section area path seemed to be invariant across different stressed vowel contexts. The acoustic pattern features associated with the invariant portions of vocal tract articulation, in combination with appropriate respiratory and laryngeal articulations, are discussed.
In recent work on the aphasic syndrome of agrammatism, Kean (1980a) has argued that a description of the impaired versus retained elements can be provided only at the linguistic level of representation that mediates between syntactic and phonological structures.
The burāq is the beast on which the Prophet Muḥammad is said to have ridden on his night journey from Mecca to Jerusalem (the isrāʾ) and occasionally on his ascension through the heavens (the miʿrāğ). This article examines the notion of the burāq within both Muslim and Western non-Muslim sources. It shows the evolution of the relatively simple descriptions of the burāq as found in the early Muslim sources to the later embellished and colourful accounts. It also investigates the role the burāq plays in more general Muslim discussions concerning the isrāʾ and the miʿrāğ. As for Western non-Muslim approaches, the article demonstrates how the burāq was initially used in anti-Islamic polemic, subsequently became an object of fascination, and in later times has become a topic of more impartial academic study.
The cue-trading relations between the intensity of aspiration and the intensity of the following vowel for the /d/-/t/ distinction were investigated at auditory, phonetic, syllable, word and sentence levels. The results demonstrate that the higher the linguistic level of cue processing the less categorical were the identifications and the more effective was the cue trading. The stronger cue trading at the syllable, word, and sentence levels seems to suggest that the speech mode of perception can be regarded as a special mode. Nevertheless a certain degree of cue trading is still evident at the auditory and phonetic levels. The results also seem to support the interactive model of speech processing.
In general, the inter-word dissimilarity measure supplied by Dynamic Time Warping algorithms can not be assumed to be a metric because it does not fully satisfy all the required properties (the triangle inequality in particular). In this paper, however, empirical evidence of loose satisfaction of these properties with real speech will be presented, allowing the assumption of a “loose metric space” structure in the set of parametric representations of words in a given vocabulary. Based on this structure, a search algorithm will be introduced which eliminates the need of computing the DTW-distance between the test word and many of the prototypes in the dictionary for Isolated Word Recognition. Experiments with vocabularies of different characteristics, have proved that the algorithm finds the nearest prototype by performing DTW computations with an average of only 30% of the words in the dictionary. This figure has been observed to decrease with increasing dictionary size, and also if the test word is close to the corresponding prototype (“well uttered” word).
We present in this paper the integration of a analytical speech recognition system to the control of a sonar console by a human operator. This application is really useful since it does correspond to a pratical need of the operator who has his eyes busy looking at the sonar screen. The DIAPASON system presents two original features: the acoustic-phonetic decoding part of the system is not based on a classical phoneme labeling process but it yields for each segment of speech a set of acoustic phonetic labels that describe this segment very precisely. This phonetic labelling is associated with a special procedure for lexical access and sentences recognition ; the DIAPASON system is moreover a genuine man-machine dialogue system and not only a system capable of understanding a single sentence as case. The history of the dialogue is used as a special knowledge source during the understanding process. Pragmatic knowledge is thus intimately associated with the analysis of sentence ; this point highly encreas as the overall performance of the system. The paper presents the architecture of DIAPASON and its various components. It is also discusses experimental results obtained in a multispeaker mode and compares the voice dialog system with the without voice system for a real sonar console.
This paper deals with speech enhancement for hands-free audio terminals, including two major problems: noise reduction and acoustic echo cancellation. Our objective is to combine a noise reduction system and an acoustic echo canceller to get a near-end speech signal with a minimum distortion and low levels of echo and noise. We present four structures (using one or two microphones and one loudspeaker) where the operation of echo cancellation comes before that of noise reduction. Experimental results are presented. Finally, in the mono-channel situation, an optimized structure controlled by an echo detector is proposed and tested.
This paper describes a microphone array for speech recording in car environments. The array is designed for hands-free radiotelephone, and is also used as a front-end for an automatic speech recognition system (this study has been realised within the european ESPRIT project ARS “adverse environment recognition of speech”). We first summarise the adaptive beamforming techniques that we have used. We then describe several aspects of the implementation of the array (configuration, design of fixed beamformers, adaptation, complexity reduction). In the last section, we evaluate the performance of the array. Two measures of performance have been retained, one is the signal-to-noise ratio, and the other is the score obtained with the speech recognition system.
In binocular stereovision, the accuracy of the 3D reconstruction depends on the accuracy of matching results. Consequently, matching is an important task. Our first goal is to present a state of the art of matching methods. We define a generic and complete algorithm based on essential components to describe most of the matching methods. Occlusions are one of the most important difficulties and we also present a state of the art of methods dealing with occlusions. Finally, we propose matching methods using two correlation measures to take into account occlusions. The results highlight the best method that merges two disparity maps obtained with two different measures.
In recent years, many methods of analysis and classification of data based on reproducing kernel Hilbert spaces have been developed. Most of these methods incorporate the fundamental principle dictated by Vapnik et al. in Support Vector Machines, which consists in extending linear algorithms to the non-linear case by using kernels. Kernel Fisher Discriminant (KFD) is one of these nonlinear methods which provides interesting results in many practical cases. However, the use of KFD requires storage and processing of matrices whose size equals the number of available data. This paper presents a sequential KFD algorithm which does not require the manipulation of large matrices. Sequential algorithms that fulfil the same requirements as KFD are also presented to perform Kernel Principal Component Analysis(KPCA) and Kernel Generalized Discriminant Analysis (KGDA).
When vowels are synthesised by means of a source-filter model, a delta-pulse train is often used as a source signal. Although breathiness can to some extent be simulated by using a sophisticated glottal-source model, a more natural simulation of breathiness requires the addition of aspiration noise. When stationary noise is used, however, the noise is to a large extent perceived as coming from a separate sound source which hardly contributes to the breathy timbre of the vowel. This problem can be solved by using noise with a temporal envelope of the same periodicity as the pulse train. In a simple source-filter model, a combination of lowpass-filtered pulses and synchronous highpass-filtered noise bursts of equal energy was used as a source signal. In this way, the noise was no longer perceived as a separate sound, but integrated perceptually with the strictly periodic part of the signal. It will be shown that this integration consists of both a reduction of the loudness of the separate noise stream and a timbre change in the breathy vowel.
In this paper, we present a novel likelihood measure which extends the adaptation mechanism to the shrinkage of covariance matrix and the adaptation bias of mean vector. A set of adaptation functions is proposed for obtaining the compensation factors. Experiments indicate that the likelihood measure proposed herein can markedly elevate the recognition accuracy.
A dysgraphic patient is described whose deficit is hypothesized to arise from selective damage to the Graphemic Buffer. The patient's roughly comparable difficulties in oral and written spelling and comparable spelling difficulties in written naming, delayed copy and spelling-to-dictation rule out the hypothesis of selective damage to either input or output mechanisms. More importantly, the nature of the errors produced by the patient and the fact that these errors were distributed virtually identically for familiar and novel words were taken as strong evidence for the hypothesis that L.B:'s spelling disorder results from selective damage to the Graphemic Buffer. Various aspects of the patient's performance are discussed in relation to a functional architecture of the spelling process and in terms of the processing structure of the Graphemic Buffer.
Although less applied, the Henri Ey's thinking is well-known in Romania. The translations of Ey's works (“The Consciousness” and “Psychiatric Studies”) into Romanian, as well as the writings of the Professor Pamfil, who was one of the Ey's students, contributed to propagate his theories. The paper's author considers the organodynamic conceptualization as a premise on his own conception of the rehabilitation of the psychiatric patient, as described in his Textbook of psychiatry, published in Bucharest in 1997. The original contribution of Henri Ey is the theory of the mind dominated by the structures of the ego (reality reflection, self-consciousness and identity). Therefore, Ey unifies the psychoanalysis, the psycho-organic theory and psychogenetics. Henri Ey was a real anthropologist.
Considered in the article is a model of auditory perception by man, represented as a multilevel hierarchical automaton. A number of heirarchical levels are defined more exactly, the conceptual aspects of each level are considered, and the procedure of signal processing on the input/output route is described. The system of speech signal features based on the psychoacoustic masking effect is described. Modelling the masking effect makes it possible, without loss of information, to withdraw from each speech signal spectrum all the spectral components masked by more powerful adjoining components. Verification of the feature system so obtained was carried out on vocabularies up to 200 words with 5 speakers participating. Recognition reliability for one speaker was about 100% and for different speakers between 90 and 99% at most.
Our goal is to obtain high dialog success rates with a very open interaction, where the user is free to ask any question or to provide any information at any point in time. In order to improve performance with such an open dialog strategy, we make use of implicit confirmation using the callers wording (when possible), and change to a more constrained dialog level when the dialog is not going well.
In source-filter based speech coding for low bit rates an efficient representation of excitation pulses is required to attain high quality of the synthetic speech. In this paper, we discuss a pulse waveform representation by a codebook populated with pulse shapes. The codebook is designed from glottal derivative pulses obtained by a linear predictive inverse filtering technique. Pulses are extracted and normalized in time and amplitude to form prototype pulses. Design methods and performance evaluation of the codebooks are investigated in a vector quantization (VQ) framework. The quantization gains obtained by exploiting the correlation between pulses are studied by theoretic calculations which suggest that about 2 bits per vector (in a budget of 7–10 bits) can be gained when exploiting the correlation. Memory based VQ is a generic term for quantization schemes which utilizes previous quantized pulses. We study traditional memory based VQ methods and an extension of memory based VQ with memoryless VQ, denoted a safety-net extension. The experiments show that performance improves when extending memory based VQ with a safety-net. It is found that, at the designated bit rates, a safety-net extended memory based VQ can gain about 1.5–2 bits in comparison with memoryless VQ.
Content-based image retrieval in large image data sets is a tiresome task considering the high rate of heterogeneity even within the same class as well as the high dimensionality of the descriptors space. For that, we propose to guide the research by low level descriptors of reduced size based on sub-bands of wavelet relating to the most significant regions in each image after a fuzzy segmentation step. Moreover, we propose a negative relevance feedback technique based on region weights. Experiments and comparative study with other similar approaches prove the robustness of the proposed approach in terms of semantic contribution thanks to the use of the wavelet sub-band and the negative relevance feedback.
In this paper, a color image segmentation method based on a new approach called bimarginal is proposed. To overcome the drawbacks of the classical marginal approaches, color components are considered in pairs in order to have a partial view of their inner correlation. Working with color images, the three possible combinations are considered as three independant information sources. Each pairwise component combination is firstly analyzed according to an unsupervised morphologic clustering which looks for the dominant colors of a 2D histogram. This leads to obtain three segmentation maps combined by intersection after being simplified. The intersection process itself producing an over-segmentation of the image, a pairwise region merging is done according to a similarity criterion with the Dempster-Shafer theory up to a termination criterion.
The word analogy has many interconnected senses, each of which is in turn connected with such contiguous concepts as “expression” and “metaphor”. In this study, we aim at analyzing both the different concepts covered by the word analogy and their manifold relations with expression and metaphor. As far as the relation between analogy and expression and metaphor is concerned, regressive analogies motivate the creation and circulation of conventional metaphors, whose expression is purely instrumental, whereas projective analogies, made possible by linguistic expression, can only be conceived of as the aim of creative interpretations of conflictual complex meanings.
The automatic récognition of typical pattern sequences (scenarios), as they are developing, is of crucial importance for computer-aided patient supervision. However, the construction of such scenarios directly from medical expertise is unrealistic in practice. Starting from the monitored data and clinical information available, our objective is to extract typical abstracted pattern sequences and then construct scenarios, eventually validated by clinical experts as representative of a class of situations to recognize. In this paper, we present a methodology for data abstraction that gradually allows the construction of such scenarios.
Over the years there has been widespread controversy over the relative merits of the cascade and parallel connections of formant generators in a speech synthesizer. This report shows that the theoretically less attractive parallel connection is able to produce closer approximations to the properties of real speech signals than is generally possible for a cascade synthesizer, both for vowels and for consonants. However, to achieve this result it is necessary to take care over the phase characteristics of the formant generators, and to appropriately shape the skirts of the formant response curves. Although extra amplitude information is needed for a parallel synthesizer, the form of the acoustic specification is in consequence directly related to properties of human speech that can be easily measured from a spectrum display.
This method is based on the asymptotic normality of a certain function of the Haar wavelet and scaling coefficients called the Fisz transformation. Some asymptotic results such as normality and decorrelation of the transformed image samples are extended to the 2D case. This Fisz-transformed image is then treated as if it was independent and gaussian variables. Then we apply a novel Bayesian denoiser that we have recently developed. It clearly outperforms the other denoising methods especially in the low-count setting. Combining the Fisz transform and the BKF Bayesian denoiser yields the best performance.
A Code-Excited Linear Predictive (CELP) coder is developed for transmission purposes and its connection with other predictive coding schemes as well as vector quantization is clarified. At a rate of one bit per sample a codebook composed of 2 L waveforms of L samples is specified taking their values in {+1, −1}. Each waveform i deduced from the L bits index by a one-to-one correspondence between each bit of the index and each sample of the waveform. By construction the codebook is shown to have an inherent robustness against transmission errors and its internal algebraic structure leads to efficient and fast algorithms for selecting the optimum excitation. Both objective and subjective results confirm the high level of performances obtained by the 16 kbit/s CELP coder in different realistic transmission conditions as transmission with errors and ambient noise.
The ability to synthesize pathological voices may provide a tool for the development of a standard protocol for assessment of vocal quality. An analysis-by-synthesis approach using the Klatt formant synthesizer was applied to study 24 tokens of the vowel /a/ spoken by males and females with moderate-to-severe voice disorders. Both temporal and spectral features of the natural waveforms were analyzed and the results were used to guide synthesis. Perceptual evaluation indicated that about half the synthetic voices matched the natural waveforms they modeled in quality. The stimuli that received poor ratings reflected failures to model very unsteady or “gargled” voices or failures in synthesizing perfect copies of the natural spectra. Several modifications to the Klatt synthesizer may improve synthesis of pathological voices. These modifications include providing jitter and shimmer parameters; updating synthesis parameters as a function of period, rather than absolute time; modeling diplophonia with independent parameters for fundamental frequency and amplitude variations; providing a parameter to increase low-frequency energy; and adding more pole-zero pairs.
Fuzzy inference systems are likely to play a significant part in system modeling, when data and expert knowledge integration is important. The aim of this paper is to set up some guidelines for this kind of modeling, based on practical experience in the fields of Agronomy and Environment. We dicuss fuzzy system assets, their ability for data and expert knowledge integration in a common framework, and their position relatively to other models. The open source FisPro implementation is presented and the approach is illustrated through two detailed case studies.
The verification of the Triangle Inequality (TI) by Dynamic Time Warping (DTW) Dissimilarity Measures (DM) seems to be a fairly important prerequisite for the application of some new and promising methods recently proposed for reducing the number of DTW computations in Isolated Word Recognition. The degree of satisfaction of this property for real speech samples has been empirically studied by various authors, who have reported rather controversial results for the different DTW and DMs speech data utilized by each of them. Therefore, a systematic study seemed to be necessary of the impact of the different factors upon which the DTW-based DMs depend. DTW productions, local metrics and speech data taken into account; moreover, this property was always observed as being “loosely” satisfied. In view of these results, a prospective study of some possible causes of TI violation was carried out. The conclusions suggest the use of time-compressing preprocessing techniques and the application of suboptimal DTW procedures (and especially if gross end-point detection errors are involved) as the most likely causes of the TI dissatisfaction rates reported elsewere.
Jitter is the small fluctuation from one glottis cycle to the next in the duration of the fundamental period of the voice source. Analyzing jitter requires measuring glottal cycle durations accurately. Generally speaking, this is carried out by sampling at a medium rate and interpolating the discretized signal to obtain the required time resolution. In this article we describe an algorithm which solves the following two signal processing problems. Firstly, signal samples obtained by interpolation are only estimates of the original samples, which are unknown. The quality of the reconstruction of the signal therefore has to be evaluated. Secondly, small variations in cycle durations are easily corrupted by noise and measurement errors. The magnitude of measurement errors therefore has to be gauged. In our algorithm, the quality of reconstruction by signal interpolation is evaluated by a statistical test which takes into account the distribution of the corrections (which are brought about by interpolation) to the positions of the signal events which mark the beginnings of the glottal cycles. Three different interpolation methods have been implemented. Measurement errors are controlled by estimating independently the cycle durations of the speech and the electroglottographic signals. When the series obtained from both signals agree, we may then conclude that they reflect vocal fold activity and that they have not been unduly corrupted by errors or noise. The algorithm has been tested on 77 signals produced by healthy and dysphonic subjects. Its performance was satisfactory on all counts.
The behaviour of the voice source characteristics in connected speech was studied. Voice source parameters were obtained by automatic inverse filtering, followed by automatic fitting of a glottal waveform model to the data. Consistent relations between voice source parameters and prosodic features were observed.
This paper serves a double purpose: to review the coding methods which have been introduced during the past decade in the 4.8–9.6 kbps range, and to discuss the most recent research trends. The bulk of the paper is devoted to CELP-based coding, a mandatory method which is at the basis of several emerging standards. The rest consists of a brief review of an alternative class of coders based on sinusoidal modelling of speech. The comparison between these opposite techniques will enable us to draw some conclusions and identify areas for future research.
This paper describes a technique whereby a pole/zero function can be fitted to a system frequency response by solving a set of simultaneous equations. The order of the pole/zero functions is defined by twice the number of spectral maxima in the response. The function is constrained to fit the maxima and minima of the response curve. The fitting technique permits the shape of the slopes between maxima to be varied via a single coefficient. The problem of obtaining a good approximation to the vocal tract frequency response from short-time spectra of speech, is solved by a spectral smoothing method. The harmonic structure in the short-time spectra obtained from a digital filter bank is removed by a cepstral truncation process. A synthetic vowel is used as an example to illustrate the technique and indicate the degree of approximation involved.
More precisely, we propose a scheme for image classification optimization, using a joint visual-text clustering approach and automatically extending image annotations. The proposed approach is derived from the probabilistic graphical model theory and dedicated for both tasks of weakly-annotated image classification and annotation. We consider an image as weakly annotated if the number of keywords defined for it is less than the maximum defined in the ground truth. Thanks to their ability to manage missing values, a probabilistic graphical model has been proposed to represent weakly annotated images. We propose a probabilistic graphical model based on a Gaussian-Mixtures and Multinomial mixture. The visual features are estimated by the Gaussian mixtures and the keywords by a Multinomial distribution. Therefore, the proposed model does not require that all images be annotated: when an image is weakly annotated, the missing keywords are considered as missing values. Besides, our model can automatically extend existing annotations to weakly-annotated images, without user intervention. The uncertainty around the association between a set of keywords and an image is tackled by a joint probability distribution (defined from Gaussian-Mixtures and Multinomial mixture) over the dictionary of keywords and the visual features extracted from our collection of images. Moreover, in order to solve the dimensionality problem due to the large dimensions of visual features, we have adapted a variable selection method. Results of visual-textual classification, reported on a database of images collected from the Web, partially and manually annotated, show an improvement of about 32.3% in terms of recognition rate against only visual information classification. Besides the automatic annotation extension with our model for images with missing keywords outperforms the visual-textual classification of about 6.8%. Finally the proposed method is experimentally competitive with the state-of-art classifiers.
The purpose of this paper is to propose a new scheme for image compression. First, we use a wavelet transform in order to obtain a set of orthonormal subclasses of images. The wavelet functions are well localized both in the space and frequency domains. The original image is decomposed on this orthonormal basis with a pyramidal algorithm architecture. The wavelets coefficients of each classes are then vector quantized. A separate optimal codebook is designed for each given resolution and direction using a training sequence and a MSE distortion measurment. Then the input vector is classified (resolution and direction) and only the appropriate subclass of the codebook is then checked using the usual MSE.
The answers to L.J. Boë and P. Perrier's comments are organised under three headings - acoustics, articulation and phonetics. An introductory rundown emphasises the model's features in order to avoid any misunderstandings as to its nature.
In this paper, we address the possibilities offered by hybrid harmonic/stochastic (H/S) models in the context of wide-band text-to-speech synthesis based on segment concatenation. After a brief recall of the hypotheses underlying such models and a comprehensive review of a well-known analysis algorithm, namely the one provided by the multi-band excited (MBE) analysis framework, we study how H/S models allow to modify the prosody of segments and how segment concatenation can be organized, in the purpose of minimizing mismatches at the border of segments. In this context, we introduce an original concatenation algorithm which takes advantage of some analysis errors. Speech synthesis algorithms are then described, including an original synthesis technique based on judiciously prepared IFFTs, and the final segmental quality, in TTS synthesis, is the ability of a synthesizer to produce natural-sounding speech sounds. More particularly, we examine the differences in the quality obtained when using the model in a narrow-band speech coding context and in a wide-band, concatenation based synthesis context. We study three possible causes for these differences: the choice of an analysis criterion, the inadequacy of the model due to pitch variatons, and the effect of coarticulation on phases.
Information retrieval systems generally return a list of ranked documents, in which only the title and possibly a snippet that contains the words of the request allow a user to evaluate the document relevance relative to her initial request. This kind of result leads the user to browse throught a lot of documents before satisfying her information need. In order to improve information retrieval, we have studied text visualisation: which information has to be shown and how? Our system RÉGAL (RÉsumé Guidé par les Attentes du Lecteur) automatically extracts the visualised information from texts by applying a thematic analysis that does not require a pre-existing structuring or a formatting of the texts, and is based on the combination of two criteria: lexical cohesion and cue phrases.
We propose a method for automatically generating a pronunciation dictionary based on a pronunciation neural network that can predict plausible pronunciations (realized pronunciations) from the canonical pronunciation. This method can generate multiple forms of realized pronunciations using the pronunciation network. For generating a sophisticated realized pronunciation dictionary, two techniques are described: (1) realized pronunciations with likelihoods and (2) realized pronunciations for word boundary phonemes. Experimental results on spontaneous speech show that the automatically derived pronunciation dictionaries give consistently higher recognition rates than a conventional dictionary.
This paper is concerned with the use of a commercial large-vocabulary speech recognition system by a team of mainstream users in the course of their everyday work. In particular, it describes use by translators working in four languages in a multilingual environment at the European Commission. The paper begins by describing some of the differences between the point-of-view of a typical researcher in speech recognition and that of a typical mainstream user. It points out some of the psychological barriers that must be overcome if speech recognition is to gain really widespread acceptance, and it concludes that such acceptance will depend at least as much on the sharing of experience between users as on technical advances. The overall results of the trials at the European Commission were encouragingly positive, but several unexpected problems were encountered, many of them related to the multilingual environment. The paper describes how most of these problems are being addressed.
This paper describes a speech technology project called SPELL (Interactive System for Spoken European Language Training), whose main aim is to design an automated system for improving the pronunciation of foreign languages by learners of English, French and Italian. The project has just completed a two-year feasibility study which has created a prototype vehicle incorporating teaching modules in intonation, rhythm and vowel quality. The paper highlights the speech signal processing techniques, similarity metrics and user interfaces which have been integrated to produce an initial demonstration system. A preliminary evaluation by a group of language teaching professionals suggests that the SPELL system is an appropriate tool for exploring the automated teaching of pronunciation.
Legal fictions prevailed in Middle French Literature. How and why did the Ballades by Charles of Orleans follow this cultural fashion? Legal metaphors frame his lyric collection from Retenue to Departie d'Amour. The poet however changes the tradition of loyalty in courtly love into a legislation. By doing so, he throws light on the specific communication with his lady: love in exile and prison implies paradoxically acts, especially official documents. The two lovers, committing themselves to each other, are bound to respect their desire as a law. The Ballades' imaginative world often embodies the lyrical I and the allegories as lawyers, judges or attorneys. The fictional staging evolves, from places where legal actions take place, to courts of justice, where trials and inner debates never end. Legal fictions transform the dialogues with other poets into role-plays, games where the prince can be both judge and jury.
In the present paper, we study these weighted Burg methods for pitch-synchronous analysis of short segments (duration less than one pitch period) of non-nasalized voiced speech and make their comparative performance evaluation. Errors in estimating the power spectrum, formant frequencies and formant bandwidths are used as criteria for performance evaluation. It is shown that the weighted Burg method of Kaveh and Lippert results in the best performance. These methods are also compared with the autocorrelation and covariance methods and the results are discussed.
Les modélisations utilisateurs étudiées par les informaticiens ou les psychologues cognitivistes manipulent souvent des données très abondantes et utilisent des outils de fouille visuelle.
This paper presents a theory and a computational implementation for generating prosodically appropriate synthetic speech in response to database queries. Proper distinctions of contrast and emphasis are expressed in an intonation contour that is synthesized by rule under the control of a grammar, a discourse model and a knowledge base. The theory is based on Combinatory Categorial Grammar, a formalism which easily integrates the notions of syntactic constituency, semantics, prosodic phrasing and information structure. Results from our current implementation demonstrate the system's ability to generate a variety of intonational possibilities for a given sentence depending on the discourse context.
Many text-to-speech (TTS) systems under development in Europe and elsewhere — we discuss in particular the system under development at Edinburgh University's Centre for Speech Technology Research (CSTR) — generate intonational properties of synthetic utterances on the basis of an intermediate abstract phonological representation of prosodic features that is quite independent of any acoustic realisation. For evaluating certain aspects of synthetic prosody (notably accent placement and division into domains), this abstract representation is a more appropriate object of evaluation than the final acoustic output of a system, just as word stress and grapheme-to-phoneme conversion are appropriately evaluated in terms of symbol strings rather than acoustic output. By way of illustration we present the results of an evaluation exercise carried out on the sentence-accent assignment rules of the CSTR system, based on just such an abstract representation, which has been useful in improving our rules.
In this paper, we propose an original methodology which allows the detection and the recognition of multi-oriented and multi-scaled patterns. The supports on which the method is applied are technical documents representing the network of the French Telephone operator France Telecom. The adopted technique, based on the Fourier-Mellin Transform (FMT) is integrated in a global strategy that solves ambiguous situations, through the providing of contextual information. The strategy which is applied to solve the character and symbol classification problem can be divided into two stages. The first one consists in computing a set of invariant descriptors for each isolated pattern belonging to a characters layer detected thanks to a connected components extractor. The second stage, based on a filtering scheme, consists in detecting and recognising the shapes which are either interconnected or connected to any other object. The results of the application of this technique are very encouraging since the classification rate reaches excellent scores in comparison with classical techniques.
In this paper, we present a different approach to the problem of estimating the angle of arrivals (AOA's) of D targets in Frequency- Hopped signaling sensors array for active systems, with D smaller than the number of sensor elements, L This method is based on the application of the Maximum Likelihood Estimator for a new proposed model of received data available in different channels. The simulation results show that this approach improves the resolution in the estimation of the angle of arrivals compared with Monotone-Frequency signaling case. Its drawback, however, is that when the Signal-to-Noise Ratio, SNR, is low the performance deteriorates and a large number of snapshots is required.
Before next-generation human language technology can be designed to function successfully in actual field settings, interface techniques will be needed that can guide users' language to coincide with current system capabilities. The present study examines how input modality and presentation structure influence the linguistic complexity observed in people's spoken and written input to an interactive system. Using a semi-automatic simulation technique, language was collected during speech-only, writing-only and combined pen/voice exchanges, and using presentation formats that either were structured or unconstrained. Results indicate that both modality and presentation format substantially influence linguistic complexity, although the specific nature of their impact differs. A comprehensive analysis is provided of how both factors affect people's observed language in terms of total words, disfluencies, utterance length, lexical variability, perplexity, syntactic ambiguity and semantic integration. Users' preferences for modalities and formats also are analyzed, and implications are discussed for channeling people's language in a transparent way. The long-term goal of this research is to develop interface techniques for managing difficult sources of variability in people's language, so that robust processing of human language technology can be achieved.
Our work aims at comparing and merging points of view of Madagascar farmers on their territory. Our modelling approach is based on conceptual graphs to represent farmers' spatial knowledge, and on formal concept analysis to organize the fusion of these representations. The results interpretation relies both on a interest measure over the obtained formal concepts and onfield expertise. We show the relevance of the selected formal concepts for analysing the way farmers organize their territory regarding forest conservation constraints.
In order to achieve better understanding of the articulatory-acoustic relationships, more data are still very much needed. The two-fold aim of the present study was thus (1) to provide a set of coherent midsagittal functions, area functions and formant frequencies, for a small corpus of vowels and fricative consonants produced by one subject, and (2) to derive a midsagittal profile to area function conversion model optimised for this given subject. Simultaneous tomography and sound recording were available for the subject, as well as some complementary data such as lip geometry or casts of the hard palate. The model is based on Heinz and Stevens' A = αd β area function model, modified so that α varies continuously along the vocal tract midline as a function of the midsagittal distance. The coefficients of the model have been determined with the help of an optimisation algorithm based on a gradient descent technique. The gradient of the error between actual and desired formant values was computed through a back-propagation network implementing both sagittal-to-area conversion and acoustic wave propagation. The fact that the model should work for sounds as different as vowels and consonants and be coherent at both midsagittal and acoustic levels ensures the reliability of the area functions determined in such a way.
In this paper, the training of HMMs has been considered a general optimization problem with linear constraints. A gradient projection method for nonlinear programming with linear constraints has been introduced and presented to solve for “optimal” values of the model parameters. When this classic method is applied to train HMMs of discrete or Gaussian mixture observation densities, a very simple formulation can be derived due to the special structure of the constraints on the HMM parameters. This kind of classical gradient-based optimization methods can offer an opportunity for more flexible modeling of speech signals and more sophisticated training of model parameters for speech recognition.
In this paper, we address the problem of speaker-based segmentation, which is the first necessary step for several indexing tasks. It aims to extract homogeneous segments containing the longest possible utterances produced by a single speaker. In our context, no assumption is made about prior knowledge of the speaker or speech signal characteristics (neither speaker model, nor speech model). However, we assume that people do not speak simultaneously and that we have no real-time constraints. We review existing techniques and propose a new segmentation method, which combines two different segmentation techniques. This method, called DISTBIC, is organized into two passes: first the most likely speaker turns are detected, and then they are validated or discarded. The advantage of our algorithm is its efficiency in detecting speaker turns even close to one another (i.e., separated by a few seconds).
The segment-based speech recognition algorithms that have been developed over the years can be divided into two broad classes. On the one hand those using the conditional segment modeling formalism (CSM), which requires the computation of the likelihood of the sequence of acoustic vectors, conditioned on the sub-word unit sequence and corresponding segmentation. On the other hand those using the posterior segment modeling formalism (PSM), which requires the computation of the joint posterior probability of the unit sequence and segmentation, conditioned on the sequence of acoustic vectors. The latter probability can be written as the product of a segmentation probability and a unit classification probability. In this paper, we focus on the role of the segmentation probability. After having shown that the segmentation probability is not required in the CSM formalism, we motivate its importance in the PSM formalism. Next, we describe its modeling and training. Experiments with two PSM-based recognizers on several speech recognition tasks demonstrate that the segmentation probability is essential in order to obtain a high recognition accuracy. Moreover, the importance of the segmentation probability is shown to be strongly correlated with the magnitudes of the unit probability estimates on segments that do not correspond with a unit.
This article is concerned with the learning by analogy in the world of sequences, based on the resolution of analogical equations. It presents a definition of an analogical relation, based on the edit distance and studies the solving of an analogical equation on sequences. It presents a construction with finite-state trandsducers which computes all the solutions of this equation, reducing the problem to that of solving analogical equations on a finite alphabet. It studies also what is analogy on alphabets and describes two algebraic structures which are compatible with the computation of solutions on sequences. Finally, it presents a direct suboptimal algorithm to compute a solution to an analogical equation on sequences.
This paper presents an operational Pitch Synchronously Excited (PSE) Formant based TtS system for the Greek language, developed at WCL. The system uses a thesaurus comprised of 140 speech segments including the Consonant (C), Vowel (V), CV and CCV type. Particular attention is paid to the concatenation scheme applied to these segments, as well as on their context-sensitive duration and the coarticulation rules written for the Greek language. The formant synthesizer runs on a DSP32C board.
This article studies the notion of verbal agency and valency in contemporary French. A trivalent construction in which a verb has a subject and both a direct and an indirect object is at the core of the reflection. A study of such constructions as found in newspaper Le Monde allows to set apart theoretical expectations and actual properties of usage.
This paper presents a signature authentication method based on handwriting temporal feature. We propose here a method which, after signal normalization, extracts dissimilarity measures between signatures. Those measures are then processed by a neural network. As we do not always have forgeries, this problem is a statistical one-class problem and the use of classical learning algorithms is forbidden. This drawback is eliminated by the introduction of a new learning algorithm.
A parser for continuous speech has to deal with lattices where the word hypotheses of the correct sentence are not usually perfectly aligned and short function words may be missing. To cope with these problems, a two-way interaction between the recognition module and the parser, called feedback verification procedure (FVP), has been investigated. The best scoring solution is finally selected by the parser. Results on a 787-word, speaker-independent, telephone-bandwidth continuous speech recognition task are presented.
Brutal and massive environmental changes, generally affecting large areas, have to be localized as rapidly as possible in order to manage the immediate impact of this type of events on ecosystems and prevent related risks. Therefore, it is necessary to develop efficient methods for change mapping. A quasi-unsupervised region-based method for change detection in high resolution satellite images is proposed. An automatic feature selection optimizes image segmentation and classification via an original calibration-like procedure. A binary classification enables then to separate altered from, intact areas thanks to a new spatio-temporal descriptor based on the level of fragmentation of obtained regions. Both segmentation and classification involve a mean shift procedure. The method was assessed on forest environment using a Formosat-2 mul- tispectral satellite image pair acquired before and after a major storm to identify and map the damages.
This paper addresses the problem of how the forms of information derived by the visual system can be translated into forms useable by the language capacity, so that it is possible to talk about what one sees. The hypothesis explored here is that there is a translation between the 3D model of Marr's (1982) visual theory and the semantic/conceptual structure of Jackendoff's (1983) theory of natural language semantics. It is shown that there are substantial points of correspondence through which the encoding of physical objects and their locations and motions can be coordinated between these two levels of representation, and that both of these levels are deeply involved in visual as well as linguistic understanding.
A test of consonant perception presented with lipreading alone, and with lipreading supplemented by the aid, showed improvements in the perception of voicing and manner of articulation in the aided condition. Testing at the word and sentence level showed differing results for the subjects completing the tasks. A congenitally deaf subject with a history of non-hearing-aid use showed no improvements in the aided conditions whereas another subject with a history of very successful hearing aid used evidenced improvements in the aided condition for both sets of materials. Testing at the level of connected discourse revealed improvements in the aided condition for two subjects but not for a subject with limited hearing aid experience. The significance of the results and future research directions are discussed.
A sentence matching task is used to assess the processing effects of the ungrammaticality produced by violations of constraints on movement rules. Surprisingly, no effects of ungrammaticality were observed either for violations of the Specified Subject Constraint or the Subjacency constraint. These 'overgenerated' sentences could apparently be processed with the same fluency as fully grammatical controls, despite the fact that other types of ungrammaticality produced marked increases in matching times. It is proposed that the matching task utilizes a level of mental representation at which overgenerated sentences are indistinguishable from fully grammatical sentences. This implies a close correspondence between formal derivational mechanisms and features of the operation of the language processor.
Appearance models yield a compact representation of shape, pose and illumination variations. The probabilistic appearance model, proposed by Moghaddam et al. (Moghaddam and Pentland, 1997; Tipping and Bishop, 1997b), has recently shown excellent performances in pattern detection and recognition, outperforming most other linear and non-linear approaches. Unfortunately, the complexity of this model remains high. In this paper, we introduce an efficient approximation of this model, which enables fast implementations in statistical estimation-based schemes. Gains in complexity and cpu time of more than 10 have been obtained, without any loss in the quality of the results.
As indicated by Bourlard et al. (1996), the best and simplest solution so far in standard ASR technology to implement durational knowledge, seems to consist of imposing a (trained) minimum segment duration, simply by duplicating or adding states that cannot be skipped. We want to argue that recognition performance can be further improved by incorporating “specific knowledge” (such as duration and pitch) into the recognizer. This can be achieved by optimising the probabilistic acoustic and language models, and probably also by a postprocessing step that is fully based on this specific knowledge. The widely available, hand-segmented, TIMIT database was used by us to extract duration regularities, that persist despite the great speaker variability. Two main approaches were used. In the first approach, duration distributions are considered for single phones, as well as for various broader classes, such as those specified by long or short vowels, word stress, syllable position within the word and within an utterance, post-vocalic consonants, and utterance speaking rate. The other approach is to use a hierarchically structured analysis of variance to study the numerical contributions of 11 different factors to the variation in duration. Whether this specific use of knowledge about duration in a post-processor will actually improve recognition performance still has to be shown. However, in line with the prophetic message of Bourlard et al.'s paper, we here consider the improvement of performance as of secondary importance.
This paper presents statistics relating to various phoneme guessing algorithms. The N-phoneme statistics were obtained by exhaustive analysis of a lexicon of 96,998 phonetic words. The results show that by incorporating detailed phonotactic knowledge, coupled with broad phonetic knowledge, an algorithm can be formulated which successfully guesses the correct phoneme with mean success rates of up to 67%. This implies that, as far as the computer is concerned, spoken English is, at a minimum, 67% redundant. The results also show that the ability to guess correctly depends on word length and position; phoneme type and the number of unknown phonemes in the word have very little effect on the final results.
Temporal decomposition of a speech utterance results in a description of speech parameters in terms of overlapping target functions and associated target factors. The former may correspond to articulatory gestures and the latter to ideal articulatory positions. Although developed for economical speech coding, this method also provides an interesting tool for deriving phonetic information from acoustic speech signals. The speech parameters used by Atal (1983) is proposing this method were the log-area parameters. Our modified temporal decomposition method (Van Dijk-Kappers and Marcus, 1987, 1989) also works with log-area parameters as input. However, the method is not restricted to these; in principle, most commonly used parameter sets can be used. In this paper we compare the results obtained with nine different sets of speech parametes, including log-area parameters, formants, reflection coefficients and band-filter parameters. The main criterion for good performance will be correspondence between target functions and phonemes or sub-phonemes. The phonetic relevance of the target vectors will also be considered, but in less detail. Speech signal resynthesis supplies yet another criterion; for those parameters sets which are transformable into the same parameter space, a reconstruction error will be defined and evaluated. From these experiments it can be concluded that log-area parameters from the most suitable parameter set available for temporal decomposition. In some respects band-filter parameters yield better results, but this set is not classified as the best due to properties related to resynthesis.
The prosodic structure of speech is the result of complex interactions within and between several different levels of organization. The intonative hierarchy, which is essentially manifested by the nature of the prosodic markers, is the product of complex interactions and constraints within and across organizational levels. Presented here is a model for predicting and interpreting the prosodic organization of spontaneous speech utterances. This model is a hierarchical system composed of six modules: (1) sematic-pragmatic, (2) syntactic, (3) phonotactic, (4) accentual, (5) semantic adjustment, and (6) rhythmic. For a given utterance, the system determines (i) the levels of the boundaries and prosodic markers on the basis of semantic information, and the syntactic structure as defined by the X-bar theory, and (ii) the accentual and rhythmic structures based on phonotactic constraints. The phonetic step, which should transform the abstract labelling into acoustic values, is not presented here. This model can and should be further developed. Future enhancements will pertain to (a) the nature of the rules, (b) different aspects of conversation, and (c) theoretical considerations. Concerning the latter point, current fruitful developments in X-bar theory are likely to lead to positive modifications in the prosodic model, which should enable it to account for certain unexplained phenomena. However, even in its current state, the model produces highly convincing results, since it predicts the number and hierarchy of intonative and stress units in an utterance with a high accuracy rate.
The various rules found have led to the formulation of a computer program labelled PHONEMIA, which is the first module of a complete text-to-speech transcription project. This program has also been used in dealing with the diphones encountered in Greek.
This paper discusses the nature of the data that form the input to linguistic descriptions, particularly with regard to the greater or lesser artificiality of such data. The paper argues in favour of a much stronger emphasis in linguistic work on natural speech data even including spontaneous speech. A major part of the paper is devoted to a general discussion of some preliminaries to the study of linguistic variation in natural speech.
We present a biologically motivated method for assessing the intelligibility of speech recorded or transmitted under various types of distortions. The method employs an auditory model to analyze the effects of noise, reverberations, and other distortions on the joint spectro-temporal modulations present in speech, and on the ability of a channel to transmit these modulations. The effects are summarized by a spectro-temporal modulation index (STMI). The index is validated by comparing its predictions to those of the classical STI and to error rates reported by human subjects listening to speech contaminated with combined noise and reverberation. We further demonstrate that the STMI can handle difficult and nonlinear distortions such as phase-jitter and shifts, to which the STI is not sensitive.
There are two types of classification methods using a Galois lattice: as most of them rely on selection, recent research work focus on navigation-based approaches. In this paper, we define the structural links between decision trees and dichotomic lattices defined from the same table of data described by binary attributes. Under this condition, we prove both that every decision tree is included in the dichotomic lattice and that the dichotomic lattice is the merger of all the decision trees that can be constructed from the same binary data table.
The efficiency of pattern recognition algorithms is highly conditioned to a proper definition of the patterns assumed to structure the data. The multigram model provides a statistical tool to retrieve sequential variable-length regularities within streams of data. In this paper, we present a general formulation of the model, applicable to single or multiple parallel strings of data having either discrete or continuous values. The model is first assessed to derive an inventory of variable-length sequences of letters from text data, where all spaces between the words have been removed. It turns out that the sequences of letters inferred during this fully unsupervised procedure clearly relate to the morphological structure of the text. The model is then used to infer a set of variable-length acoustic units, directly from speech data. Speech files containing examples of acoustic units are provided along with this paper in order to illustrate their consistency from an auditory point of view. We also report experiments using these acoustically defined units for continuous speech recognition.
Bayesian networks are very good tools for representing uncertainty thanks to their clear graphical representation and the conditional probability values defined on that graph. The structure and the probability values are usually given by an expert. In this paper, we are interested in learning the structure of such networks from databases. We have adopted a Bayesian approach, which allows to find an adequacy between the structure and the data. Our starting point was two algorithms: K2 of Cooper and Herskovits and B of Buntine. We have generated algorithm K2B that on one side benefits from the advantages of the two previous algorithms but on the other side reduces their drawbacks. The implementation of K2B gave birth to Alexso, which is able to find a compromise between representativity and simplicity of the structure.
The aim of this paper is to present an outline of a theory of semantics based on the analogy between natural and computer programming languages. A unified model of the comprehension and production of sentences is described in order to illustrate the central “compile and execute” metaphor underlying prodecural semantics. The role of general knowledge within the lexicon, and the mechanism mediating selectional restrictions, are re-analyzed in the light of the procedural theory.
This paper reports the results of three projects concerned with auditory word recognition and the structure of the lexicon. The first project was designed to experimentally test several specific predictions derived from MACS, a simulation model of the Cohort Theory of word recognition. Using a priming paradigm, evidence was obtained for acoustic-phonetic activation in word recognition in three experiments. The second project describes the results of analyses of the structure and distribution of words in the lexicon using a large lexical database. Statistics about similarity spaces for high and low frequency wordswere applied to previously published data on the intelligibility of words presented in noise. Differences in identification were shown to be related to structural factors about the specific words and the distribution of similar words in their neighborhoods. Finally, the third project describes efforts at developing a new theory of word recognition know as Phonetic Refinement Theory. The theory is based on findings from human listeners and was designed to incorporate some of the detailed acoustic-phonetic and phonotactic knowledge that human listeners have about the internal structure of words and the organization of words in the lexicon, and how they use this knowledge in word recognition. The theory relies on several novel techniques to formalize strategies for search space reduction from large vocabularies when only partial information about the phonetic content of a word is available. Taken together, the results of these projects demonstrate a number of new and important findings about the relation between speech perception and auditory word recognition, two areas of research that have traditionally been approached from quite different perspectives in the past.
The study of memory is a great challenge, perhaps the greatest in biological sciences. Memory involves changes in a tiny fraction of an extremely large pool of elements, a conclusion that makes the task of finding those changes using current technologies formidable. What can be done about this roadblock to neurological investigations of learning? One response that has become particularly productive in recent years is to study learning or learning-like phenomena in relatively simple “model” systems. The idea is to extract basic principles from these models in which molecular and anatomical details can be studied and then to use these in analyzing learning in higher regions of the brain. In this article we discuss current progress and emerging concepts derived from the simple system approach using animal models.
We describe a stochastic component for spoken natural language understanding in an application for train travel information retrieval in French. Another important issue concerns the iterative semantic labeling of large data amounts used for the component training. The parser has been evaluated on both corrected and uncorrected speech recognizer output transcriptions.
The sidescan sonar records the energy of an emitted acoustical wave backscattered by the seabed for a large range of grazing angles. The statistical analysis of the recorded signals points out a dependence according grazing angles, which penalizes the segmentation of the seabed into homogeneous regions. To improve this segmentation, classical approaches consist in compensating artifacts due to the sonar image formation (geometry of acquisition, gains, etc.) considering a flat seabed and using either Lambert's law or an empirical law estimated from the sonar data. The approach chosen in this study proposes to split the sonar image into stripes in the swath direction; the stripe width being limited so that the statistical analysis of pixel values can be considered as independent of grazing angles. Two types of texture analysis are used for each stripe of the image. The first technique is based on the Grey-Level Co-occurrence Matrix (GLCM) and different Haralick attributes derived from. The second type of analysis is the estimation of spectral attributes. The starting stripe at mid sonar slant range is segmented with an unsupervised classifier based on the SOFM (Self-Organizing Feature Maps) Kohonen algorithm. The study made in this work is validated on real data acquired by the sidescan sonar Klein 5000. Segmentation performances of the proposed algorithm are compared with those of conventional approaches (K-means).
One of the main challenges faced by the video game industry is to give life to believable Non-Player Characters (NPCs). Research shows that emotions play a key role in determining the behavior of individuals. In order to improve the believability of NPCs' behavior, we propose in this article a model of the dynamics of emotions taking into account the personality and the social relations of the character. First, we present work from the literature on emotions, personality and social relations in Computer Science and in Human and Social Sciences. We focus on the influence of personality on the trigerring of emotions, and of emotions on the dynamics of social relations. Based on this work, we propose a dynamic model of the socio-emotional state and its implementation as part of a tool for game programmers aiming at the simulation of the evolution of emotions and social relations of NPCs based on their personality and role.
This paper introduces some recent studies on voice quality control and conversion technologies. After briefly summarizing some basic scientific findings on the acoustic correlates of speech individuality, we review the latest developments in speech technologies related to voice control and speaker characteristic copying. The main focus is on a survey of non-parametric methods for spectral segmental characteristics mapping between speakers, introducing some different types of spectral mapping methods that have evolved in relation to the speaker adaptation techniques being developed in speech recognition research.
This paper is devoted to Array Processing with fourth-order statistics. After a brief review of the cumulant properties, we introduce an algebraic formalism for easy handling of higher-order multivariate statistics. We define the «quadricovariance», which is an exhaustive representation of the fourth-order cumulants, and its orthogonal decomposition into «eigen- matrices». In this notation, the strong formal analogy between the quadricovariance and the usual covariance is an indication that second- order methods may be directly extended to fourth-order statistics. We also show that the source detection limit is higher with fourth-order statistics. It goes from N − 1 sources with 2-MUSIC to 2(N − I) with 4-MUSIC operating on a linear equispaced array. Using a non uniform linear array, it goes up to N(N − 1) sources. The notion of eigenmatrix is then shown to provide a direct algebraic solution to the «blind source separation problem» which may be seen as an Array Processing problem where no information is available about the array manifold.
For assessing the synthesized speech output component in a complex application system, application-oriented evaluation methods and methodologies are needed which are not supplied by standardized test batteries so far. Many standardized tests analyze synthetic speech mainly with regard to its form (surface structure), and only to a less degree with regard to the meaning that is assigned to it (deep structure). In turn, in order to obtain a valid assessment focus for an application system, the functional aspect of speech (which depends on its deep structure) has to be taken into account. In the paper two case studies are presented which focus on the acceptability of the synthesis component and its constituent dimensions in different application scenarios. In the first one synthetic speech in a car navigation and traffic information system is assessed. The second study relates to synthetic speech in a dialogue system. The assessment is limited to laboratory experiments and avoids costly field tests. It turns out that different dimensions contribute to a variable degree to overall acceptability, differently depending on the application scenario. Application-oriented testing is thus required to identify the application-specific dimensions. It is discussed which characteristics of the application have to be modeled in the assessment, and examples are given for both applications.
The standard problem of classifying cars based on acceleration and speed measures is addressed here. An optimal discrimination strategy is methodically devised by considering 4 decision levels: the choice of the search space (variables selection through heuristics or evolutionary algorithms, discriminant analysis), the choice of the discrimination criterion (probabilistic or maximum margin), the choice of the classification algorithm complexity and the tuning of other algorithm parameters.
In the production of stop consonants by hearing-impaired subjects, voicing errors are far more frequent than place of articulation errors. The problem arises from the phonetic complexity of some of the various kinds of homorganic plosives. The English-speaking deaf encounter special difficulties in the pronounciation of the aspirated/ptk/ which require a precise temporal delay between oral closure and laryngeal opening. The data presented in this paper show that the French-speaking deaf encounter similar difficulties in the production of the prevoiced/bdg/ which require a precise temporal delay between the onset of laryngeal vibrations and the release of oral closure. The rapid sequential delivery of these two articulatory gestures enables the speaker to sustain the voice up to the end of the closure, which has decisive importance for voicing perception. Our data also show that the speakers who correctly produce prevoicing can generally perceive the voicing feature. Although the place of articulation of the stops is not better perceived than their voicing category, most of the moderately-deaf subjects in this experiment can produce the place distinctions perfectly. The role of perceptual feedbacks in the mastering of articulatory gestures is discussed.
Specifically, it will be argued that principles of structural nature and principles of perceptual nature are in conflict in languages of the SOV type, because of the relative clause construction. The way in which a relative clause is structured in an SOV language is an obstacle to its effective perceptual processing. It will be argued that this conflict is one of the major factors determining the diachronic change of a language from an OV to a VO typology.
It is a totally interactive and open system which allows easy integration of new parameters.
A unifying presentation of source separation methods based on higher order statistics is derived. The approach starts from the systematic scanning of the characteristic parameters of all the methods, like: the statistical hypotheses about the data ; the different kinds of models assumed, (standard or doubly orthogonal) ; the separation criteria, (indépendance), that lead to the sources restoration, written with moments or cumulants ; the effective principles of the separation, (direct trails based on a restoration matrix, indirect or global trails that estimate other characteristics of the sources, amplitudes etc…, before the true separation). From a general standpoint, it is shown that, these methods yield a restoration of elements that belong simultaneously to two classes, more than a unique set of sources. The first class, named second order class is associated with all random vectors that have the same covariance matrix. The rationale of the second class, considered as a class of indépendance, stems from the invariance of mutual indépendance by any permutation operation.
The problem addressed in this article is that of automatically designing autonomous agents having to solve complex tasks involving several -and possibly concurrent- objectives. We propose a modular approach based on the principles of action selection where the actions recommanded by several basic behaviors are combined in a global decision. In this framework, our main contribution is a method making an agent able to automatically define and build the basic behaviors it needs through incremental reinforcement learning methods. This way, we obtain a very autonomous architecture requiring very few hand-coding. This approach is tested and discussed on a representative problem taken from the “tile-world”.
In current speech research, there is a need for large databases to be able to test production and perception models at different linguistic levels. There are considerable problems in administering databases, both to label the speech and to easily access stored material. In order to alleviate some of the problems we have created a speech analysis system. Speech data are stored in sentence-sized files. These files are segmented and transcribed semi-automatically given a phonetic transcription of the utterance. This transcription is generated by the letter-to-sound rules of our text-to-speech system. The emphasis on the database is the use for acoustic-phonetic research rather than the use in e.g. evaluation of speech recognizers. This makes demands on flexible and linguistically specified retrieval patterns. Our unorthodox solution to this is to use the synthesis rule structure, similar to the notation used in generative phonology, for accessing the data. By a brief rule statement, speech segments meeting the specified contextual conditions can be identified. Durational data can be collected directly during the database search. Spectral analysis programs operating with a variety of spectral representations have also been created that display the result, typically as a mean/standard deviation spectrum or as a contour histogram spectrum.
The recognition rate for fluently spoken speech achieved with beam search algorithms depends on factors as the perplexity and the recombination structure of the language model, the difficulty of the vocabulary, the quality of the acoustic phonetic decoder, and the chosen pruning strategy. In this paper a statistical model taking these factors in account is developed. An iterative algorithm to calculate the score distribution of beam search paths and the resulting sentence error rate is presented.
It is therefore hardly surprising that one can ultimately wonder what kind of problem it is to which the central presence of the concept of representation in the linguistics of Gustave Guillaume is meant as a response. If “to promote language to existence is to promote it to representation − without which nothing exists for the mind”, it is not because of the nature of language that the linguist resorts to the concept of representation, but because there is a certain way of being a linguist, which consists in posing language as an object for the mind, an attitude which is distinct, among many other solutions, from that consisting, for example, in seeing it as an object or a “parameter” of social life.
This paper describes the development and evaluation of the grapheme-to-phoneme sub-system of a complete real-time synthesis system under development at Macquarie University. Evaluation and development of this system has been facilitated by using weighted statistics which reflect the frequency of occurence of each word in the LOB and Brown corpora of English. These statistics are derived from a test word database which includes all acceptable Australian pronunciations (as defined by the Macquarie Dictionary) of each word, as well as their LOB and Brown frequency counts. These scores facilitate decisions to be made about which alterations to the rules or lexicon will have the greatest effect on total system accuracy in ordinary running text (as reflected by the corpora frequencies).
The focus in automatic speech recognition (ASR) research has gradually shifted from isolated words to conversational speech. Consequently, the amount of pronunciation variation present in the speech under study has gradually increased. Pronunciation variation will deteriorate the performance of an ASR system if it is not well accounted for. This is probably the main reason why research on modeling pronunciation variation for ASR has increased lately. In this contribution, we provide an overview of the publications on this topic, paying particular attention to the papers in this special issue and the papers presented at 'the Rolduc workshop'. Subsequently, the issues of evaluation and comparison are addressed. Particular attention is paid to some of the most important factors that make it difficult to compare the different methods in an objective way. Finally, some conclusions are drawn as to the importance of objective evaluation and the way in which it could be carried out.
This paper argues that the feature specification of a Swiss French deficient ça accounts for its syntactic distribution. Contrary to the ordinary accusative clitics le, la, les, this pronominal form lacks number, gender and Case feature, but has a temporal-aspectual locative feature. The arguments that support ça's caselessness are based on the following diagnostic: 1. (i) the impossibility of doubling the deficient ça, 2. (ii) the impossibility of ça being an enclitic and 3. (iii) its interaction with topicalisation and right dislocation which differs from what can be observed with ordinary clitics in this context. Another distinction in the feature make-up of ça vs. ordinary clitics is the ambiguous categorical status of ça as both D and DP. As a result of its feature composition, ça requires geric event quantification. This interpretation is always available with transitive eventive verbs but with unaccusatives and transitive statives, the reading is blocked in the present tense. It is demonstrated that the aspectual non-ambiguity of the Swiss French deficient ça, i.e. the fact that it can only appear in contexts of generic event quantification, is responsible for ungrammatical sequences such as ∗Je ça aime ('I that like') in this grammar.
The goal of this paper is to investigate various language model smoothing techniques and decision tree based language model design algorithms. For this purpose, we build language models for printable characters (letters), based on the Brown corpus. We consider two classes of models for the text generation process: the n-gram language model and various decision tree based language models. In the first part of the paper, we compare the most popular smoothing algorithms applied to the former. We conclude that the bottom-up deleted interpolation algorithm performs the best in the task of n-gram letter language model smoothing, significantly outperforming the back-off smoothing technique for large values of n. In the second part of the paper, we consider various decision tree development algorithms. Among them, a K-means clustering type algorithm for the design of the decision tree questions gives the best results. However, the n-gram language model outperforms the decision tree language models for letter language modeling. We believe that this is due to the predictive nature of letter strings, which seems to be naturally modeled by n-grams.
`Linguistic annotation' covers any descriptive or analytic notations applied to raw language data. The basic data may be in the form of time functions – audio, video and/or physiological recordings – or it may be textual. The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, `named entity' identification, coreference annotation, and so on. While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem. Proposed standards, to the extent they exist, have focused on file formats. This paper focuses instead on the logical structure of linguistic annotations. We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph. This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and file formats.
In visual representations of the acoustical signal of speech, such as the waveform or spectrogram, speech appears as a series of concatenated sequences of acoustical events, which vary in spectrum, amplitude and duration. The results of a variety of psychoacoustical experiments, from auditory fusion to temporal masking to studies of streaming, can be interpreted as relevant to discovering the auditory capabilities used in listening to these speech sequences. A sampling of such results serves to illustrate the connections between the psychoacoustics of speech and nonspeech, and to suggest guidelines for future work on non-speech temporal patterns, with the goal of a more complete psychophysics of complex sounds.
Within the framework of a research on cellular networks of radio communication, it is essential to be able to predict the area which would be covered by transmitters. To study a transmitter, the standard method consists in applying an electromagnetic wave propagation model to various positions defined according to a constant spatial step. Yet, that method leads to a considerable computation time which might become unexploitable in complex geographical environments. There have already been some researches studying how to reduce that computation time. They consist in the simplification of the propagation model used. The processes in our article is complementary to them. Indeed, our technique is independent of the model. The idea is to reduce the number of calculation points of the model. The method presented here is based on an hypothesis which needs two elements to be confirmed: the segmentation of the signals measured by a mobile receiver ; a software used for the electromagnetic analysis of the geographic studied area. Thus, the purpose is to segment the received signal into intervals corresponding to particular combinations of physical phenomena. To do that, a representation suggested by Mallat and Zhong called “Wavelet Maxima Representation” is studied. That decomposition allows the study of the derivative of a function at different scales. We shall present a method of signal segmentation based on the maxima chaining through the scales of the decomposition. The chaining helps us select the largest discontinuities of the signal and thus segment it.
Considering 2D trajectories is attractive since they form computable image features which capture elaborated spatio-temporal information on the viewed actions. If they are embedded in an appropriate modeling framework, high-level information on the dynamic scene can then be reachable. We aim at designing a general trajectory classification method that does not exploit strong a priori information on the scene structure, the camera set-up, the 3D object motions, while taking into account both the trajectory shape (geometrical information related to the type of motion and to variations in the motion direction) and the speed changes of the moving object on its trajectory (dynamics-related information). Appropriate local differential features combining curvature and motion magnitude are defined and robustly computed on the motion trajectories. A robust enough non-parametric feature extraction framework is also proposed since local differential features computed on the extracted trajectories are prone to be noise corrupted.
This article describes how the performance of a Dutch continuous speech recognizer was improved by modeling pronunciation variation. We propose a general procedure for modeling pronunciation variation. In short, it consists of adding pronunciation variants to the lexicon, retraining phone models and using language models to which the pronunciation variants have been added. First, within-word pronunciation variants were generated by applying a set of five optional phonological rules to the words in the baseline lexicon. Next, a limited number of cross-word processes were modeled, using two different methods. In the first approach, cross-word processes were modeled by directly adding the cross-word variants to the lexicon, and in the second approach this was done by using multi-words. Finally, the combination of the within-word method with the two cross-word methods was tested. The word error rate (WER) measured for the baseline system was 12.75%. Compared to the baseline, a small but statistically significant improvement of 0.68% in WER was measured for the within-word method, whereas both cross-word methods in isolation led to small, non-significant improvements. The combination of the within-word method and cross-word method 2 led to the best result: an absolute improvement of 1.12% in WER was found compared to the baseline, which is a relative improvement of 8.8% in WER.
Hierarchical clustering techniques have been shown to be a powerful tool in building speaker-independent reference models for Dynamic Time Warping (DTW) based speech recognition systems. In this paper, we introduce a clustering algorithm based on the standard KMEANS procedure that generates reference models for continuous density Hidden Markov Model (HMM) based systems by simultaneously considering spectral and duration information. Improved speech recognition performance using clustering is demonstrated on a digit recognition task using the TI/NBS studio quality connected digit database.
This paper retraces, in an informal way, some of the history of speech synthesis and speech research from Von Kempelen's speaking machine to linear prediction.
In this paper, we propose a preliminary framework for accounting for certain surface F 0 variations in speech. The framework consists of definitions for pitch targets and rules of their implementation. Pitch targets are defined as the smallest operable units associated with linguistically functional pitch units, and they are comparable to segmental phones. The implementation rules are based on possible articulatory constraints on the production of surface F 0 contours. Due to these constraints, the implementation of a simple pitch target may result in surface F 0 forms that only partially reflect the underlying pitch targets. We will also discuss possible implications of this framework on our understanding of various observed F 0 patterns, including carryover and anticipatory variations, downstep, declination, and F 0 peak alignment. Finally, we will consider possible interactions between local and non-local pitch targets.
We recall the enhancement by polynomial filtering principle of the numerical Braille relief image. We describe the recognition method based on two orthogonal axis projection of each Braille character. The five recognition steps are developed taking account of form defaults of relief and using a maximum likehood method. The position axis dispersion of Braille characters permits to calculate estimated theoretical error. The error rate, verified in practice for manual made Braille reliefs is about 1 %.
The description of endangered languages is crucial for linguistic typology, as a possible source of information about types of structures not attested in the languages described so far. Conversely, good information about the state of knowledge on linguistic typology is particularly useful for linguists dealing with under-described languages, which is generally the case for endangered languages. However, typologists must be conscious that, in the search for typological generalizations, putting on a par well-alive and well-documented languages, and languages whose unique description cannot be checked or completed anymore, may lead to distorted conclusions. Moreover, it is not possible to work on moribund languages in the same way as on languages still used by a community as its usual means of communication, and one may have doubts on the representativity of data collected in such conditions, in particular in domains such as syntax.
In this paper we introduce a novel concept: the cube transversals on the cube lattice of a categorical database relation. The problem of finding cube transversals is a sub-problem of hypergraph transversals since it exists an order-emhedding from the cube lattice to the power set lattice of binary attributes. Based on this fact, we propose a levelwise algorithm for mining minimal cube transversals. We applied this concept by introducing a new OLAP functionality; the emerging version space. It is the difference between two uni-compatible datacubes or the most frequent elements in the difference. Finally we propose a merging algorithm mining the boundary sets of emerging version spaces without computing the two related datacubes.
Subtle variations in voice (phonatory) quality may reveal aspects of the speaker's mood and attitude, and are thus an important aspect of speaking style. This paper illustrates current research being carried out by the authors on the voice source correlates of a range of such quality differences. The voice qualities looked at include modal, breathy, whispery, tense, lax and creaky voice, following the description of Laver (1980) and analyses presented here focus on a word extracted from a propose passage read with these qualities. The principal method used for analysing the voice source involved inverse filtering of the speech wave. In order to quantify the source characteristics, a four parameter voice source model (the LF-model) was matched to the inverse filtered waveform. Frequency domain analyses of the speech waveform, based on narrow band spectral sections and on spectral averaging, were also carried out. Detailed comparisons of the data measured directly from the glottal waveform and those measured from the speech output yield insights which could not be inferred from either alone. Results suggest a number of important differences between the qualities as well as considerable dynamic variation within a single quality. The data should also prove useful for resynthesis, which is an important tool for testing perceptual aspects of voice quality; e.g., the attitudinal and emotional colouring that may be associated with particular voice qualities.
This article gives a simple method to accelerate the k nearest neighbours algorithm. Despite a sensible increase in the error rate, this method should be useful in the applications that need a classification algorithm with a speed constraint.
This paper presents a new theoretic tool based on Information Theory, the main interest of which is to acutely evaluate the classification tools. The particular nature of real-world objects recognition involves us to design systems based on multi-points of view approaches. We show that neural networks allow to learn the fusion function, optimized to the data and the structure of the composite system. The performance of a composite recognition system is closed to the partition of the available information on each classification tools. A Genetic algorithm is designed to adapt the parameters space partition with the set of classification tools among the quality of the composite system, genetic algorithm.
We present “Transcriber”, a tool for assisting in the creation of speech corpora, and describe some aspects of its development and use. Transcriber was designed for the manual segmentation and transcription of long duration broadcast news recordings, including annotation of speech turns, topics and acoustic conditions. It is highly portable, relying on the scripting language Tcl/Tk with extensions such as Snack for advanced audio functions and tcLex for lexical analysis, and has been tested on various Unix systems and Windows. The data format follows the XML standard with Unicode support for multilingual transcriptions. Distributed as free software in order to encourage the production of corpora, ease their sharing, increase user feedback and motivate software contributions, Transcriber has been in use for over a year in several countries. As a result of this collective experience, new requirements arose to support additional data formats, video control, and a better management of conversational speech. Using the annotation graphs framework recently formalized, adaptation of the tool towards new tasks and support of different data formats will become easier.
This article explores further the question of when the Muslims began to use suffixed invocations of God's curse on the Franks (Crusaders), a topic that we have addressed previously in this journal. The common use of such invocations took some time to develop after the Franks' arrival in the Levant, and previously we argued that this was primarily due to the suffixed invocation used by our earliest source failing to “catch on” with contemporaries, while later forms spoke more directly to the needs of the people of the time. Since the publication of that article we have had the opportunity to work with the original manuscript of our earliest source, which has revealed that the author did use a form of suffixed invocation that became popular later, but the manner in which he did so was sufficiently ambiguous to hinder its adoption by other writers.
If articulatory movements can be estimated, then the articulatory parameters which represent the motion of the articulatory organs would be useful for speech recognition. This paper discusses an effective method of estimating articulatory movements and its application to speech recognition. Firstly, what is described is a method of estimating articulatory parameters known as the model matching method, and various spectral distance measures are evaluated for this method. The results show that the best in average is the higher order cepstral distance measure, which is one of the peak weighted measure. Secondly, articulatory parameters are utilized for the recognition of vowels uttered by unspecified speakers. It is shown that the adaptation of the model by the estimated mean vocal tract length is effective to normalize speaker difference. Thirdly, the motor commands to move the articulatory organs are estimated considering articulatory dynamics, and the continuous vowels are recognized by means of these estimated commands. It has been found that a considerable part of the coarticulation effects can be compensated for by this command estimated, and the method is useful for continuous speech recognition.
In accordance with the principles of the Tone Sequence Model the F0 contour is analyzed as a series of discrete target values that are connected by means of transitional functions. The set of example utterances can be obtained at the address ftp kiwi.nmt.edu (or internet address ftp 129.138.1.82). More detailed instructions can be found in the ToBI-Guidelines or in the README file. It is also possible to get an audio tape of the utterances and a printed paper copy of the labelling guide and F0 tracks at this address: ToBI Labelling Guide, c/o Mary Beckman, Ohio State University, Linguistics Department, 222 Oxley Hall, 1712 Neil Avenue, Columbus, OH 43210-1298, USA. Utterances from ToBI and the Boston Radio News Corpus were used for the evaluation of the generation rules: root mean squared error (RMSE) and correlation between generated and original contour were determined, and in a perception test native speakers assessed the quality of the resynthesized contours which, in general, were judged to sound natural and show few differences to the corresponding originals.
A robust variational setting is proposed for 1D signal registration and applied to the computation of shape geodesics for shape classification issues. This approach is extended to be applied for matching images of shape sequences. This geometric approach is mainly addressed to poorly contrasted images where the intensity-based registration fails. For validation purposes, experiments are carried out on real signals and images issued from marine biological archives which depict a high interindividual variability such that registration-based approaches are of particular interest.
The purpose of this work was to investigate the speech comprehension of four deaf Hebrew-speaking patients implanted with a cochlear prosthesis, the Nucleus 22-channels (N-22) system. Experiments were performed under two conditions: The speech tests (isolated vowels, bisyllabic words and fluent speech in closed and open sets) were first conducted using the Default Frequency Boundaries (DFBs) of the cochlear implant's speech processor. The Default Frequency Boundaries of each electrode which are specified by the computer program of the system, are assumed to be selected on the basis of English. The patients were then retested using the same speech material, and the results were compared with those previously obtained. As a result of the Modified Frequency Boundaries, improvements in the patients' comprehension of the speech elements were noted. The differences in performance between the two sets of frequency boundary distributions suggest that better speech comprehension could be achieved by implanted patients, at least partly, by adjusting the frequency-to-electrode mapping of the N-22 speech processor on a language basis.
Standard articulation tests are not always sensitive enough to discriminate between speech samples which are of high intelligibility. One can increase the sensitivity of such tests by presenting the test materials in noise. In this way, small differences in intelligibility can be magnified into large differences in articulation scores. We used both a more conventional articulation test and a monosyllabic adaptive speech interference test (MASIT) to evaluate the intelligibility of nine different speech-coding techniques. We found different patterns of responses for the articulation test and MASIT. These differences can be explained by the fact that different speech-coding schemes code different acoustic-phonetic properties of the speech signal. Some of these properties are more liable to masking by interfering noise than others. Our results show that, in the case of synthetic speech, differences in intelligibility are not always magnified by adding interfering noise: they may even disappear.
Studied from the Antiquity, the art of arguing was in turn a central topic of teaching, then object of many criticisms. It was finally rehabilitated as a fundamental support for recent developments of sciences and technology of communication. Indeed, these last decades, the emergence of dialogue systems, of natural language generation systems, and of multi-agent systems around a core of argumentation as well as developments of modelling based on both formal and informal approaches of the argumentation representation has supported this rehabilitation. This article proposes a synthesis of this work. It attempts to measure the adequacy of various approaches to the modeling of the natural argumentation and underlines the difficulties which remain to be surmounted. The assistant planning system for argumentation APLA developed by the authors with an aim of attenuating these difficulties is also presented in this article.
Speech analysis shows that the second formant transitions in vowel–vowel utterances are not always of the same duration as those of the first formant transitions nor are they always synchronised. Moreover the formant transitions often move initially in a different direction from their final target. In order to investigate whether these deviations from linearity and synchrony are perceptually significant a series of listening tests have been conducted with the vowel pair /a/–/i/. It was found that delays between the first and second formant transitions of up to 30 ms are not perceived, nor are differences in duration of up to 40 ms if the first and second formants start or end simultaneously. If the second formant transition is symmetric in time with respect to the first formant differences of up to 50 ms are tolerated. Excursions in second formant transition shape of up to about 500 Hz are also not perceived. These results suggest that most of the deviations from linearity and synchrony found in natural vowel–vowel utterances are not perceptually significant.
Transformational methods have made of the acceptability test the instrument of measure and classification par excellence in linguistics. Of the various a priori possible parameters which can influence the result of this test, it is especially the factor of semantic interpretation which is considered here. We would like to show the necessity of dissociating an extralinguistic part of this interpretation (including the universe of discourse adopted by the informant, the culture to which he belongs, and the importance of certain factors of knowledge — or of ignorance — of the world) from a linguistically pertinent hypothetical interpretation which correlates very closely with questions of syntax or morphology. Extra-linguistic semantic concepts are 'natural', come immediately to the mind and screen the research of much more abstract, unfamiliar and probably unconscious linguistic semantic concepts. This is a boundary which it is necessary to draw in semantics between what is linguistic and what is not, i.e., between what must and what cannot appear in a generative grammar. We insist here on the necessity in linguistics of controlling, and thus of varying, the universes of discourse which can modify the acceptability of a given sentence. The argument centres around several uses of the verb planter ('to plant').
In the field of scene analysis for computer vision, a trade-off must be found between the quality of the results expected, and the amount of computer resources allocated for each task. Using an adaptive vision system provides a more flexible solution as its analysis strategy can be changed according to the information available concerning the execution context. We describe how to create and evaluate a visual attention system tailored for interacting with a computer vision system so that it adapts its processing according to the interest (or salience) of each element of the scene. We propose a new set of constraints named PAIRED to evaluate the adequacy of a model with respect to its different applications. We justify why dynamical systems provide good properties for simulating the dynamic competition between different kinds of information. We present different results that demonstrate that our results are fast and highly configurable and plausible.
The closed-class hypothesis asserts that function words play a privileged role in syntactic processes. In language production, the claim is that such words are intrinsic to, identified with, or immanent in phrasal skeletons. Two experiments tested this hypothesis with a syntactic priming procedure. In both, subjects tended to produce utterances in the same syntactic forms as priming sentences, with the structures of the self-generated sentences varying as a function of differences in the structures of the primes. Changes in the closed-class elements of the priming sentences had no effect on this tendency over and above the impact of the structural changes. These results suggest that free-standing closed-class morphemes are not inherent components of the structural frames of English sentences.
In everyday communicative situations not all parts of the spoken message are pronounced equally clear. Especially words bearing a high load of semantic information are put in focus by the speaker. The question of how this is realized in natural spontaneous and read speech, and whether resulting knowledge can be applied in synthetic speech to improve naturalness and acceptability, is subject of this study. By introducing a “peak-and-level” model we examined spectral and temporal aspects in focus and non-focus words from spontaneous speech material and from the same texts, read out after orthographic transcription. Audio recordings were made of a professional male speaker, whose voice and pronunciation also served as a model for the diphone-based component of the Dutch national speech synthesis program. For a number of acoustic parameters it can be concluded that there is a clear difference, both in “peak values” and in “level values”, between the two natural speech styles, but that the peak values display comparable contrasts to the level values in both styles. The results of our measurements in natural speech were compared to the data of the same texts synthesized by the Dutch diphone text-to-speech system. In a pilot experiment, varying temporal aspects in the synthesized speech, listeners were asked to judge the naturalness and intelligibility in order to determine the starting-point for future evaluation of text-to-speech synthesis including peak-and-level contrasts.
A simple formant-estimating speech processor has been developed to make use of the “hearing” produced by electrical stimulation of the auditory nerve with a multiple-channel cochlear implant. Thirteen implant patients were trained and evaluated with a processor that presented the second formant frequency, fundamental frequency, and amplitude envelope of the speech (F 0 F 2). Nine patients were trained and evaluated with a processor that presented the first and second formant frequencies, fundamental frequency, and first and second formant amplitudes (F 0 F 1 F 2). The F 0 F 1 F 2 group performed significantly better in discrimination tasks and word and sentence recognition through hearing alone. The F 0 F 1 F 2 group also showed a significantly greater improvement when hearing and lipreading was compared with lipreading alone in a speech tracking task. A study of spondeerecognition in noise with hearing alone indicated that the added first formant information produced an improvement that was equivalent to a 5 dB increase in the signal-to-noise ratio.
The problem of binary classifier combination is adressed in this article. This approach consists in solving a multi-class classification problem by combining the solutions of binary sub-problems. We consider two strategies in which each class is opposed to each other, or to all others. The combination is considered from the point of view of the theory of evidence. The classifier outputs are interpreted either as conditional belief functions, or as belief functions expressed in a coarser frame. They are combined by computing a belief function that is consistent with the available information. The performances of the methods are compared with those of other techniques and illustrated on various datasets.
It is well-known that complexities exist in the mapping between the acoustic information in the speech signal and the phonetic categories of adult language users. We investigated whether the same complexities exist in the mapping between the speech signal and the forerunners of these categories in infants. For two classes of complexity, we found that the manner in which the categorization of information for speech occurs was virtually identical in infant and adult listeners. These findings indicate that the infant possesses finely tuned linguistically-relevant perceptual abilities, which undoubtedly facilitate and shape the task of language acquisition.
An algorithm for recognition of connected words has been adapted to an application for mobile radio telephony. For this purpose, several manners of generating feature vectors were evaluated using two databases collected in a small car moving at about 120 km/h. The databases contain digits and digit strings uttered via handset and in hands-free mode for each of 10 speakers. In the first phase, experiments were done using signal analysis resulting in linear scaled magnitude filterbank coefficients in the feature vector. This approach achieved a digit error rate of 6.0% on 7-digit strings collected via handset and employing isolated word training. The second approach replaced the magnitudes of the filterbank coefficients by coefficient energy, with the energy being scaled logarithmically. The results derived in the simulation environment were verified in a car using the product hardware.
In this article, we propose to study a speech coding method applied to the recognition of phonemes. The proposed model (the Neural Predictive Coding, NPC) and its two declinations (NPC-2 and DFE-NPC) is a connectionist model (multilayer perceptron) based on the non linear prediction of the speech signal. We show that it is possible to improve the discriminant capacities of such an encoder with the introduction of signal membership class information as from the coding stage. As such, it fits in with the category of DFE encoders (Discriminant Features Extraction) already proposed in literature. In this study we present a theoretical validation of the model in the hypothesis of unnoised signals and gaussian noised signals. NPC performances are compared to that obtained with traditional methods used to process speech on the Darpa Timit an Ntimit speech bases. Simulations presented here show that the classification rates are clearly improved compared to usual methods, in particular regarding phonemes considered difficult to process. A small vocabulary word recognition experiment is provided to show how NPC features can be used in a more conventional speech ANN-HMM based system approach.
As was reported at IVTTA-94, the NYNEX VoiceDialingSM service was first deployed in the NYNEX region in mid 1993. It has since been deployed in several other Bell Operating Companies' regions and substantial new developments have taken place. One of them was a transition, starting in 1995, from the hardware implementation of a DTW-based recognition algorithm to the implementation, on a general-purpose DSP, of a continuous density multi-Gaussian mixture HMM-based algorithm. As a result, it has been possible to expand the service beyond speaker-dependent name recognition to speaker-independent continuous digit recognition (SICDR) and voice-activated network control (VANC) command recognition. This paper highlights major efforts in this transition and provides a more detailed description of the system's speech recognition components.
A new language model is proposed to cope with the scarcity of training data. Each multiple class is assigned by a grouping process based on the left and right neighboring characteristics. Furthermore, by introducing frequent word successions to partially include higher order statistics, multi-class N-grams are extended to more efficient multi-class composite N-grams. In comparison to conventional word tri-grams, the multi-class composite N-grams achieved 9.5% lower perplexity and a 16% lower word error rate in a speech recognition experiment with a 40% smaller parameter size.
It was found that the F 2 locus-nucleus differences were smaller in spontaneous speech than in reference words, in non-prominent syllables than in prominent syllables, and in given words than in new words. These results were interpreted as reflecting differences in coarticulation, both an anticipatory effect of vowel on the preceding consonant and/or formant undershoot. F 2 locus-nucleus patterns appeared to depend on duration, confirming Lindblom's model (1963), they may also be influenced by other factors such as speaker adaptations to the communicative situation.
In voiced speech segments, natural noise is the sum of modulation noise due to the irregularly vibratory patterns of the vocal folds, and additive noise due to the air stream in a glottal chink. The noise content can be evaluated by the signal-to-noise ratio (SNR). The first section of the paper reviews the various time and frequency domain SNR measurement techniques. In the second section, a new SNR measurement technique is introduced, which approximates the harmonic content of speech wave spectra by means of a spectral synthesis-by-analysis algorithm. In the final section, the technique presented is employed to perform SNR measurements on synthetic vowels, and normal and pathological voice signals. The SNR measurement only allows a rough screening with respect to the disorder. The problems involved in the measurement of noise are discussed.
We describe a method for an automatic character string understanding from scanned maps. This method takes into account oriented strings that occur frequently on maps. The application is split in three main modules: connected components analysis, string construction, connected character detection. Results seem sufficient to allow an operational application if syntactic rules are added to ameliorate the warning process
This paper describes Elsag's Large Vocabulary Isolated Word Recognition system Dspell. The system makes use of a diphone-based speech model and an extremely efficient word decoding algorithm, and is implemented on Elsag's multiprocessor emma-2.∗ dspell requires a very convenient training session and features a high recognition performance and extremely fast response on lexicons of up to 10,000 words.
This paper deals with the enhancement of noisy speech signals recorded in a car for mobile radio applications. Our concern is the signal estimation when 1 or 2 observations are available; each one is composed of a speech signal, s i, and an additive noise n i (i = 1, 2). In the case of one microphone, we consider new techniques that capitalize on the aspect of speech perception by focusing on enhancing only the short-time spectral amplitude. A “modified spectral subtraction” method is proposed; it uses a frequency-dependent over-estimate of the noise and can be combined with segmentation of the observation. When two microphones are available, in the case of uncorrelated (or slightly correlated) noises, we introduce a new technique based on the coherence function which is used to filter the observations or to determine a speech/noise classification algorithm. Finally, listening tests have been conducted to compare the simplest methods. In the case of stationary noise, the modified spectral subtraction is very promising and in the case of non-stationary and decorrelated noises, the method based on the coherence is more attractive.
Twenty semantically unpredictable sentences were generated in each of the five selected syntactic structures. These basic structures were defined as a crosslinguistic methodology for corpus generation in a European environment. Responses from twenty listeners during five sessions are also analysed. The distribution shows a strong relationship between the proportion of correct sentences (p s) and of correct words (p w). The ratio r = Log(p s)/Log(p w) seems to be a powerful index for measuring the complexity of a spoken message. Data replotted from the literature confirm the hypothesis that the higher the contextual (semantic, syntactic, etc.) information in a sentence, the lower this index r. In that perspective, the index r could be related to the number of decision units listeners must deal with when listening to a sentence. Speech synthesizers distort the comprehension of sentences. The distribution of omissions and mistakes does not obey the binomial law that would be expected from a simple model, where all input units have the same independent probability to be correctly identified. Analysis of the discrepancy between the experimental distribution of word errors and the binomial distribution obtained from the simple model provides a fruitful explanation of the fact that the linguistic relations between words allow a correction of “theoretically misunderstood” words and a distortion of “theoretically understood” words. Such a phenomenon mainly depends on linguistic content of the sentences that may be quantified by means of the suggested index r. It also shows second order variations due to other factors such as subjects' compentence and training, or the acoustic degradation of the message.
In most speaker identification or verification systems it is assumed that all speakers have the same feature covariance matrices. This assumption simplifies the classification algorithm since it yields a linear classifier. Speakers, however, differ not only in their mean feature vector, but in their covariance matrix as well. The use of a speaker's individual covariance matrix results in a quadratic classifier. If a common covariance matrix is assumed, a common optimal feature space can be determined. With an individual covariance matrix, each speaker is optimally recognized in an individual feature space. The recognition scheme therefore requires the matching of an unknown speaker with the templates defined over different feature spaces. The use of the quadratic classifier together with the individual feature space is shown to drastically improve recognition accuracy while the added memory requirements are shown to be negligible. The suggested quadratic classifier, with individual optimal feature vectors, has been tested using a speaker identification system with six male speakers. In terms of a given separation measure, the quadratic classifier yields improvements of about 2 times over the conventional method.
Cross-linguistic comparisons may shed light on the levels of processing involved in the performance of psycholinguistic tasks. For instance, if the same pattern of results appears whether or not subjects understand the experimental materials, it may be concluded that the results do not reflect higher-level linguistic processing. In the present study, English and French listeners performed two tasks - click location and speeded click detection - with both English and French sentences, closely matched for syntactic and phonological structure. Clicks were located more accurately in open- than in closed-class words in both English and French; they were detected more rapidly in open- than in closed-class words in English, but not in French. The two listener groups produced the same pattern of responses, suggesting that higher-level linguistic processing was not involved in the listeners' responses. It is concluded that click detection tasks are primarily sensitive to low-level (e.g. acoustic) effects, and hence are not well suited to the investigation of linguistic processing.
This study investigated the temporal coordination of the articulators involved in French Cued Speech. Cued Speech is a manual complement to lipreading. It uses handshapes and hand placements to disambiguate series of CV syllables. Hand movements, lip gestures and acoustic data were collected from a speaker certified in manual Cued Speech uttering and coding CV sequences. Experiment I studied hand placement in relation to lip gestures and the corresponding sound. The results show that the hand movement begins up to 239ms before the acoustic onset of the CV syllable. The target position is reached during the consonant, well before the vowel lip target. The results show that the handshape formation gesture takes a large part of the hand transition. Both experiments therefore reveal the anticipatory gesture of the hand motion over the lips. The types of control for vocalic and consonantal information transmitted by the hand are discussed in reference to speech coarticulation. Finally the temporal coordination observed between Cued Speech articulators and the corresponding sound was used as rules to control an audiovisual system delivering Cued Speech for French CV syllables.
Biometrics, which refers to identifying an individual based on his/her physical or behavioral characteristics, has gained in popularity among researchers in signal processing during recent years. It has also focused the attention of medias since the tragic events of September 11th, 2001. We first introduce the notion of biometrics. Then, we describe the architecture of biometric systems and the metrics used to evaluate their performances. We briefly discuss the most common biometrics and the different ways to combine them to obtain multimodal systems. Finally, we present applications of biometrics.
This paper explores how conversants co-ordinate their use and interpretation of language in a restricted context. It revolves around the analysis of the spatial descriptions which emerge during the course of 56 dialogues, elicited in the laboratory using a specially designed computer maze game. Two types of analysis are reported. The first is a semantic analysis of the various types of description, which indicates how pairs of speakers develop different language schemes associated with different mental models of the maze configuration. The second analysis concerns how the communicants co-ordinate in developing their description schemes. The results from this study would suggest that language processing in dialogue may be governed by local principles of interaction which have received little attention in the psychological and linguistic literature to date.
A new combination of coding methods for a 64 kbit/s transmission system for typical videophone situations is investigated. The codec structure is based on a standard hybrid discrete cosine transform (DCT) codec with temporal prediction. The picture is divided blockwise into changed and unchanged areas. One motion vector with subpel accuracy is computed and transmitted for each block of the changed area. For the forward analysis, the prediction error is calculated in the whole picture. Only the blocks with the highest prediction errors are updated by a DCT with a perception adaptive quantization. The number of DCT update blocks depends on the remaining bits after the transmission of the overhead information. The codec is controlled by a forward analysis of the prediction error and is not based on a buffer control. The spatial resolution of the source signal is reduced in two steps to prevent a codec overload caused by too much activity between two frames.
It is proposed that All F are G is often given a 'structure-neutral' interpretation, as All, F, G, lacking a subject-predicate distinction. In the first experiment, children aged 6–7–8, and 11–12, and adults, acted out instructions like “Make a building in which all the yellow blocks are square”. The experiment demonstrated the dominance in children and decline with age of structure-neutral interpretations. In a second experiment, with the same age groups, propositions of the form All F are G, varying as to the factual inclusion relations expressed, were presented as the major premises of syllogistic items. The results indicated the presence of structure-neutral interpretations under some circumstances in adults as well as children, and also demonstrated the existence in all subjects of a 'pragmatic processing' mode that becomes less obligatory with age. In pragmatic interpretations, meaning is determined by previously known factual relations between the things which the words represent, rather than by grammatical relations between the words themselves.
This paper deals with quality assessment of color images dedicated to quantization algorithms. Two methods are described in details, one using the baker's transform and the other using the matrix of local pallets. In this assessment campaign, the results of the described techniques are compared to those of standard algorithms such as median cut, octree and split & merge. Both objective and subjective assessment are performed. The need of subjective evaluation comes from the fact that the usual metrics do not integrate the HVS (Human Visual System). While the color is more considered as perceptual property than a quantitative data, these standard metrics fail in describing image distortion. The preliminary results of the assessment campaign show that the described methods give good results with regards to the former ones.
This article reports on a study of the capacity of prosody to predict upcoming discourse boundaries. More specifically, it is investigated whether the approaching end of a route description can be pre-signalled by melodic and temporal characteristics. Experiment 1 brings to light that listeners are able to estimate on the basis of such prosodic properties how far a given utterance is situated from the end of a description. However, the scope of this prosodic prediction is relatively restricted as listeners can only estimate the absolute discourse position of the last two utterances of the monologue analyzed. Experiment 2 is run in order to explore systematically, by means of a test with synthetic speech, to what extent melodic and durational properties are sufficient to influence finality judgments.
In recent years, both automatic speech recognition (ASR) and text-to-speech (TTS) conversion systems have attained quality levels that allow inclusion in everyday applications. One remaining problem to be solved in both these types of applications is that alleged phone inventories of specific languages are commonly expanded with phones from other languages, a problem that becomes more acute in an increasingly internationalized world where multilingual automatic speech-based services are a desideratum. This paper investigates the nature of phone set expansion in Swedish. The status of these phones is discussed, and since such added phones do not have a phonemic (or allophonic) function, the term `xenophones' is suggested. The results show that very few subjects resort to full rephonematization and that xenophonic expansion is the rule, although there is an uneven distribution depending on particular phones, spanning from phones produced by most subjects, to phones produced by almost no subjects. Of the possible explanatory factors analyzed – regional background, gender, age and educational level – the latter is by far the most important.
In a dialogue, there are at least two sorts of boundaries between discourse units. One type of boundary signals the end of a topical unit; another type of boundary the end of a turn at talk. These two do not necessarily coincide, as a speaker may wish to a new topical unit without wanting to be interrupted by his interlocutor. In order to test whether prosodic cues can differentiate unambigously between topic and turn boundaries, a series of production experiments was set up in which topic-finality and turn-finality were varied independently, and in which visual and non-prosodic verbal cues could not be used. In the most complex condition, the speaker had to give clear cues for topic finality, while not prematurely losing the floor. In this condition, speakers avoided using low tones at turn-internal topical boundaries, reserving them to signal turn-final topic boundaries. When listeners were confronted with portions of the description taken out of their contexts, they could reliably differentiate between turn-final and non-turn-final topical units. Interestingly, when the final parts of a topical unit were removed, listeners could still discriminated between turn-final and non-turn-final expressions, apparently basing themselves on other, more global, prosodic cues. This holds similarly for both minimally and maximally incomplete units.
In the past, educators relied on classroom observation to determine the relevance of various pedagogical techniques. Automated language learning now allows us to examine pedagogical questions in a much more rigorous manner. We can use a computer-assisted language learning (CALL) system as a base, tracing all user responses and controlling the information given out. We have thus used the Fluency system [Proceedings of Speech Technology in Language and Learning, 1998, p. 77] to answer the question of what voice a language learner should imitate when working on pronunciation. In this article, we will examine whether there should be a choice of model speakers and what characteristics of a model's voice may be important to match when there is a choice.
In this paper, first, we present the problem of Non Cooperative Target Recognition (NCTR) as a supervised classification problem. KNN algorithm has been executed initially on CPU with Matlab and then on GPU. Arithmetic operations and memory access pattern has been studied to get the best parallelization. Finally, we conclude with a discussion about possible perspectives for the proposed method especially by focusing on other representation spaces or other classification methods.
Current enterprise modelling approaches describe generic task designs that do not capture actual work practices. However, several improvement areas in organisations are related to the particular ways that individuals have of implementing tasks. This research seeks to enrich enterprise modelling with a methodological approach to capture and model work practices, using a model centered on organizational agents and contexts. The approach allows the identification and analysis of personal and inter-personal work patterns. Context- based representations are acquired from action repositories. The approach is illustrated with a case study. Results on the automatic discovery of personal contexts using clustering are reported.
This paper deals with the problem of learning radial basis function neural networks to approximate non linear L 2 function from R d to R. Hybrid algorithms are mostly used for this task. Unsupervised learning techniques are used to estimate the center and width parameters of the radial functions and supervised learning techniques are used to estimate the linear parameters. Supervised learning techniques are generally based on the least squares (LS) estimates (or criterion). This estimator is optimal when the training set (z i,y i)i=1,2,..,q is composed of noisy outputs yi, i = l,..,q and exactly known inputs z i, i = 1,.., q. However, when collecting the experimental data, it is seldom possible to avoid noise when measuring the inputs z i. The use of least squares estimator produces a biased estimation of the linear parameters in the case of noisy input output training data set, which leads to an erroneous output estimation. This paper proposes the use of an estimation procedure based on the error in variables model to estimate the linear parameters (for supervised learning) when the training set is made up of input and output data corrupted by noise. The geometrical interpretation of the proposed estimation criterion is given in order to illustrate its advantage with respect to the least squares criterion. The improved performances in non linear function approximation is illustrated with a simulation example.
An important area of speech recognition is automatic recognition of connected digit strings (i.e., sequences composed of the digits zero through nine, and oh). Applications of this technology include credit card authorization, catalog ordering, dialing of telephone numbers, and data entry. For the past two years AT&T has experimented with a system for automatic recognition of 10 digit merchant identification codes, and 15 digit customer credit card numbers, for the purpose of authorizing purchases charged to a credit card. Our evaluation used data collected from about 1000 customers who provided 2000 connected digit strings over 800-based dialed up telephone connections. The recognizer correctly recognized 97% of the digit strings with no rejections using constraints on the validity of both merchant identifications and credit card numbers. Several schemes for applying these task constraints in a practical implementation are discussed in this paper. Also, recognition of the dollar amounts of the transaction are presented with some preliminary results.
With a cochlear implant, deaf people are able to understand speech under good listening conditions. Problems occur in adverse conditions, e.g. in reverberant and/or noisy environments. Experiments were carried out to improve the intelligibility of noisy speech using two different single-channel noise suppression techniques. This was realized by preprocessing, i.e. by applying the resynthesized speech signal to the cochlear implant system. Intelligibility tests were carried out in cooperation with the medical department.
Instabilities of the human voice source appear in normal voices under certain conditions (newborn cries, vocal fry, creaky voice) and are symptomatic of voice pathologies. Vocal instabilities are intimately related to bifurcations of the underlying nonlinear dynamical system. We analyse in this paper bifurcations in 2-mass models of the vocal folds and study, in particular, how the incorporation of the vocal tract affects bifurcation diagrams. A comparison of a simplified model (Steinecke and Herzel, 1995) with an extended version including vocal tract resonances reveals that essential features of the bifurcation diagrams (as e.g. frequency locking of both folds and toroidal oscillations) are found in both model versions. However, vocal instabilities appear in the extended model at lower subglottal pressures and even for weak asymmetries.
Document Analysis and Recognition consist in translating their images into an electronic form that can be reusable. The analysis extracts the document layout structure from its image, and the recognition assigns to the layout structure components their logical functions in the document. In this article, we present our work on recognition of a category of documents in which the logical structure is based on typographical tagging such as table of contents. We propose a perceptual approach that extracts these typographical tagging directly from document images. However, the structures of such documents are complex and variable. Our goal is to consider the document structure recognition problem even though these difficulties occur. We developed a automatic recognition system based on a hybrid model combining a bayesian classifier and a probabilistic automaton. The classifier is responsible of drawing a correspondence between text blocks extracted from document images and basic logical entities, while the automaton deals with grouping these entities into a hierarchical logical structure. This hybrid model is built by semi-supervised learning based on knowledge provided by the user on the one hand, and the typographical properties of our documents, on the other hand. This system has been experimented for automatic indexing of tables of contents in periodicals and journals. The complexity and the variability of these documents allow us to show the efficiency of the approach.
Preemphasis of the speech signal at higher frequencies is a preprocessing step employed in various speech processing applications. In the present paper, the effect of preemphasis on vowel recognition performance is studied. Preemphasis of the speech signal is achieved by the first-order differencing of the speech signal. Cepstral coefficients derived through linear prediction analysis are used as recognition parameters. A minimum distance classifier is used for vowel recognition and the recognition performance is studied for four different distance measures: Euclidean distance measure, correlation distance measure, Mahalanobis distance measure and Itakura distance measure. It is shown that preemphasis of the speech signal brings about a deterioration in the vowel recognition performance. Implications of this result for isolated word recognition are also discussed.
The paper presents a study of syllabic structures and their variation in a large corpus of French radio interview speech. A further aim is to show how automatic speech recognition (ASR) systems can serve as a linguistic tool to consistently explore virtually unlimited speech corpora. Automatically selected subsets can be manually checked to accumulate knowledge on pronunciation variants. To focus on sequential variants, a methodology has been set up using descriptions at the phonemic, syllabic and lexical levels. This study reports on a radio corpus composed of 30 one-hour shows of interviews. Less well described phenomena are also observed, such as other vowels (/u/, /ε/, /i/ and /a/) being deleted in a non-final (unstressed) position. Unstressed CV syllables, when preceded by an open syllable, are likely to undergo syllabic restructuring: vowel deletion together with backward onset-coda transfer. Complex syllables tend to be simplified: liquid consonants have a tendancy to be deleted, more often in coda than onset position. The most deletion-prone consonant is /v/, in both onset and coda positions. Finally, a substantial percentage of word-final schwa syllables may completely disappear and short function words are deletion prone whatever the vowel identity.
This article deals with the problem of affective states recognition from physical and physiological wearable sensors. Given the complex nature of the relationship between available signals and affective states to be detected we propose to use a statistical learning method. We begin with a discussion about the state of the art in the field of statistical learning algorithms and their application to affective states recognition. Then a framework is presented to compare different learning algorithms and methodologies. Using the results of this pre-study, a global architecture is proposed for a real time embedded recognition system. Instead of directly recognizing the affective states we propose to begin with detecting abrupt changes in the incoming signal to segment it first and label each segment afterwards. The interest of the proposed method is demonstrated on two real affective state recognition tasks.
Two central assumptions of current models of language acquisition were addressed in this study: (1) knowledge of linguistic structure is “mapped onto” earlier forms of non-linguistic knowledge; and (2) acquiring a language involves a continuous learning sequence from early gestural communication to linguistic expression. The acquisition of the first and second person pronouns ME and YOU was investigated in a longitudinal study of two deaf children of deaf parents learning American Sign Language (ASL) as a first language. Personal pronouns in ASL are formed by pointing directly to the addressee (YOU) or self (I or ME), rather than by arbitrary symbols. Thus, personal pronouns in ASL resemble paralinguistic gestures that commonly accompany speech and are used prelinguistically by both hearing and deaf children beginning around 9 months. This provides a means for investigating the transition from prelinguistic gestural to linguistic expression when both gesture and language reside in the same modality. The results indicate that deaf children acquired knowledge of personal pronouns over a period of time, displaying errors similar to those of hearing children despite the transparency of the pointing gestures. The children initially (ages 10 and 12 months) pointed to persons, objects, and locations. Both children then exhibited a long avoidance period, during which one function of the pointing gesture (pointing to self and others) dropped out completely. During this period their language and cognitive development were otherwise entirely normal, and they continued to use other types of pointing (e.g., to objects). When pointing to self and others returned, it was marked with errors typical of hearing children; one child exhibited consistent pronoun reversal errors, thinking the YOU point referred to herself, while the other child exhibited reversal errors inconsistently. Evidence from experimental tasks conducted with the first child revealed that pronoun errors occurred in comprehension as well. Full control of the ME and YOU pronouns was not achieved until 25–27 months, around the same time when hearing children master these forms. Thus, the study provides evidence for a discontinuity in the child's transition from prelinguistic to linguistic communication. It is argued that aspects of linguistic structure and its acquisition appear to involve distinct, language-specific knowledge.
We have developed a spontaneous speech dialogue system TOSBURG II, employing keyword-based spontaneous speech understanding and multimodal response generation, with adaptive speech response cancellation. Since in multimodal interaction, the user understands the system's response by a visual output before its speech response is completed, the user often interrupts the system's speech response. Therefore, our adaptive speech response cancellation serves to facilitate natural human-computer interaction by allowing the user's interruption. We have also developed an evaluation environment for dialogue data collection and the performance of TOSBURG II. Unlike conventional data collection systems, TOSBURG II collects in this environment not only speech data and the final results of speech understanding but also its intermediate results as dialogue data, to use them for the evaluation and improvement of the system.
Speech recognition was used to automate directory assistance in a 6-month trial with Bell Canada's public customers. The bilingual application gave the caller a choice at the beginning of the dialog to continue in English or French. Over 89% of calls were either partially or fully automated. Customer and operator reactions to the system were positive. Bell-Northern Research's flexible vocabulary recognizer, using a vocabulary of 1,700 city names and synonyms, performed well under real world conditions. Economically significant operator work time savings were demonstrated.
This article describes a speech-based user interface to a wide range of entertainment, navigation and communication applications in mobile environments by means of human-machine dialogues. The system has been developed in the framework of the EU-project SENECA. It uses noise reduction, speech recognition, and dialogue processing techniques. One interesting aspect relies in the fact that low speech recognition confidence and word-level ambiguities are compensated by engaging flexible clarification dialogues with the user. The SENECA system demonstrator has been evaluated by means of user tests. With speech input, road safety, especially for complex tasks is significantly improved. Compared to manual input, the feeling of being distracted from driving is less important with speech.
This paper presents the results of a speaker-independent, isolated word speech recognition system developed for information access over Australian public switched telephone network (PSTN). The recognition system is based on Continuous Density Hidden Markov Modelling (CDHMM). The speech database was collected over the PSTN from a large variety of speakers and different geographical locations. The database contained a vocabulary of 55 words consisting of 41 country names and their variations plus a few control words. The recognition performance, tested on 100 other speakers (50 males and 50 females) with no grammar constraint, resulted in an overall recognition rate of 97.3%. This paper describes the HMM training methodology, which consisted of three stages: hand segmented seed model training, automatic word segmentation and reestimation. To facilitate the future implementation of the recognition system in a DSP environment, a fast frame synchronous Viterbi algorithm was implemented with no degradation in recognition performance. The end-point detection is performed by the combination of the silence/noise model with the word models. For confusable word pairs, sub-word models are used to improve the recognition rate. A post-processing approach is used to enhance the performance of the recognition system, in which all ranked candidates from the Viterbi decoding are subject to the tests of the minimum word duration and the likelihood difference between the first candidate and the second candidate.
For practical applications speech recognition systems need to be insensitive to differences between training and test acoustic conditions. Differences in the acoustic environment may result from various sources, such as ambient background noise, channel variations and speaker stress. These differences can dramatically degrade the performance of a speech recognition system. A wide range of techniques have been proposed for achieving noise robustness. This paper considers one particular approach to model-based compensation, predictive model-based compensation, which has been shown to achieve good noise robustness in a wide range of acoustic environments. The characteristic of these schemes is that they combine a speech model with an additive noise model, a channel model and, in the general case, a speaker stress model, to generate a corrupted-speech model. The general theory of these predictive techniques is discussed. Various approximations for rapidly performing the model combination stage have been proposed and are reviewed in this paper. The advantages and the limitations of such a predictive approach to noise robustness are also discussed. In addition, methods for combining predictive schemes with schemes which make use of speech data in the new environment, adaptive schemes, are detailed. This combined approach overcomes some of the limitations of the predictive schemes.
The growing size of the web and the heterogeneity of accessible information made relevant information extraction (IE) more and more complex from web pages. In the context of information gathering on restricted domains of the web, this work is focused on web page adaptive IE (AIE) systems that can be adapted to new domains through training on annotated corpora as input. WEPAIES performances are evaluated on three standard corpora, more or less structured, and compared to performances of other adaptive information extraction systems.
Using a vocabulary of 12 nonsense words, the authors have developed a method for the objective study of the auditory performance of patients fitted with cochlear implants. The random repetition of the 12 words in the vocabulary produced a list of 129 items which were read out to the patients. Each reading simultaneously tested several features in the vocabulary. The statistical analysis of the results covered the recognition of the words, consonants and basic features; several recognition percentage associations were also considered. The results are given for 10 sessions held with two patients fitted with Chorimac cochlear prostheses. They show that: (1) the different tests are not equivalent (even for the associations of percentages). (2) some important features of the Chorimac prosthesis were met such as a good discrimination of the plosive-fricative opposition and a bad distinction of voicing, and (3) in this work, there is no evidence of help being derived from lip reading; this leads to a discussion of the sensitivity of the evaluation method.
The topic of this paper is the approximation of the Value Function in Reinforcement Learning. We present a method for modeling the Value Function that allocates memory resources while the agent explores its environment. The model of Value Function is based on a Radial Basis Functions Network, with Gaussian units. The model is build incrementally by creating new units on-line, when the system enters unknown regions of the space. The parameters of the model are adapted using gradient descent and the Sarsa(X) algorithm. The method do not require a model of the environment neither does it involve the estimation of a model. The performance of the proposed method is demonstrated on two well-known benchmark problems: the Acrobot and the Bioreactor. Both problems are simulated in real-valued state space and discrete time.
English undergraduate students, studying in London, consistently misspelled Gandhi as Ghandi despite intensive exposure to the correct spelling of the name. This is an exemplary misspelling in a number of ways that we detail in this paper. We conclude that, even with minuscule lexical knowledge of 'Indian' words and names, English readers use 'rules' in such tasks. This gives us hope that, once a correct spelling has been achieved it will be maintained, for new rules (presumably) replace old ones.
In this paper we propose an agent communication approach based on social commitments and arguments. In this approach, agents must use their reasoning capabilities to reason about their mental states before acting on commitments or on their contents in a relevant way. In order to enable them to choose the most relevant arguments at each step of the dialogic interaction, this approach allows the agents to use two types of reasoning: strategic and tactical. The strategic reasoning allows an agent to choose its global communication plan in terms and to select the constraints which it decides to satisfy. In addition, tactical reasoning allows agents to locally choose, at each turn, the most relevant argument according to the adopted strategy.
The performance of speech enhancement algorithms deteriorates rapidly with decreasing signal-to-noise ratio (SNR). At a low SNR, high-intensity phonemes such as vowels are therefore more likely to be enhanced than low-intensity speech segments such as many consonants. Although the selective enhancement of vowels enhances transitional cues for consonant recognition, it simultaneously degrades relative amplitude cues. Experiments with normal-hearing subjects were performed to determine the overall effect of selective enhancement of vowels on the intelligibility of consonants in consonant–vowel–consonant utterances. In quiet, a 12-dB enhancement of the vowels did not significantly reduce consonant intelligibility compared with an unenhanced control condition at 65 dB (A). When unenhanced utterances were presented in background noise with an average SNR of −6 dB at the vowel segments, 50.1% of the consonants were correctly identified while 69.8% of consonants were recognised in a condition where the consonant SNR remained unchanged but where the vowels were selectively amplified by 12 dB. Equal enhancement of the vowels and consonants by 12 dB, however, led to 91.5% consonant recognition. We conclude that speech enhancement algorithms should enhance all speech segments to the greatest possible extent, even if this leads to selective enhancement of some phoneme categories over others.
In order to evaluate this faithfulness, we propose to visualize any measure associated to the data by coloring the corresponding Voronoï cells in the projection space, and we define specific measures. We experiment these techniques with the Principal Component Analysis and the Curvilinear Component Analysis applied to artificial and real databases.
The present tutorial paper is addressed to a wide audience with different discipline backgrounds as well as variable expertise on intonation. The paper is structured into five sections. In Section 1, “Introduction”, basic concepts of intonation and prosody are summarised and cornerstones of intonation research are highlighted. In Section 2, “Functions and forms of intonation”, a wide range of functions from morpholexical and phrase levels to discourse and dialogue levels are discussed and forms of intonation with examples from different languages are presented. In Section 3, “Modelling and labelling of intonation”, established models of intonation as well as labelling systems are presented. In Section 4, “Applications of intonation”, the most widespread applications of intonation and especially technological ones are presented and methodological issues are discussed. In Section 5, “Research perspective” research avenues and ultimate goals as well as the significance and benefits of intonation research in the upcoming years are outlined.
Spectral analysis of non-stationary signals calls for specific tools which permit one to describe a time evolution of frequency characteristics. Such tools, referred to as time-frequency representations, can be defined in an objective way when imposing a priori constraints. Within a stochastic and non-parametric framework, two main approaches are offered, which either emphasize a doubly orthogonal decomposition, or preserve the usual concept of frequency. After having established the corresponding definitions and emphasized the importance of the Wigner-Ville transform, estimation problems are addressed and a discussion is provided for supporting the usefulness of time-frequency representations in processing operations which go beyond a mere description.
Several translations of the Speculum humanae salvationis have been written and copyied in the Burgundy during the second half of the fifteenth Century. In this context, the translation of Miélot, ordered by the Duke Philip the Good, and its two manuscripts, in which we count a minute, has been thought as a failure. The comparison of the textual and codicological features of the Miélot's translation with the three other contemporary french translations in prose permits to arise the specificity of Miélot's art of translation and of mise en livre.
This paper illustrates the advantages of using the Discrete Cosine Transform (DCT) as compared to the standard Discrete Fourier Transform (DFT) for the purpose of removing noise embedded in a speech signal. The derivation of the Minimum Mean Square Error (MMSE) filter based on the statistical modelling of the DCT coefficients is shown. Also shown is the derivation of an over-attenuation factor based on the fact that speech energy is not always present in the noisy signal at all times or in all coefficients. This over-attenuation factor is useful in suppressing any musical residual noise which may be present. The proposed methods are evaluated against the noise reduction filter proposed by Y. Ephraim and D. Malah (1984), using both Gaussian distributed white noise as well as recorded fan noise, with favourable results.
Cooperation between agents, whatever their nature may be, requires a set of representations to be implemented. According to its own capacity in perceiving and possibly reasoning, each agent constructs a more or less rich representation of the situation in which he or she is led to cooperate. The decisions he makes strongly depend on the quality of this representation. It is shown how perception, reasoning and action are dependent within the general framework of the cooperative activity. For that, we observe the relation between the co-operative activity and the individual one. Then, we examine the case of the man/machine co-operation seen as particular case of the co-operation between agents, for which the machine must be equipped with adapted representations and must make it possible to the user to build powerful representations. Modelling for the design of knowledge based systems brings a base ofprimarily conceptual representation. Two points of view must indeed cohabit in co-operative systems; that of the organization in which the co-operation intervenes and that of the user who cooperates with the system to carry out a task. In addition to the conceptual model of the application, we will thus find in the co-operative systems a model of co-operation (point of view of the organization) and a model of user.
Many writers have argued that dialogue should be regarded as a joint activity (see for example (Clark and Wilkes-Gibbs, 1986; Grosz and Sidner, 1990; Schegloff, 1981; Suchman, 1987)), something that agents do together, rather than simply as a product of the interaction of plan generators and recognizers working in synchrony and harmony, as plan-based theories propose. Such plan-based approaches do not explain why addresses ask clarification questions, why they confirm, or even, why they do not walk away. Rather, the joint action model claims that both parties to a dialogue are responsible for sustaining it. Participating in a dialogue requires the conversants to have at least a joint commitment to understand one another. The key questions to be answered include how to formalize such general commitments precisely, and to show how they predict the fine-grained synchrony so apparent in ordinary conversation. To begin to answer these questions, we sketch here how a formal theory of joint action explains confirmations that arise in task-oriented telephone dialogues. A more formal account is given in (Cohen and Levesque, 1991a). Then we argue that extensions of this analysis to dialogue more generally will be difficult. In particular, it will force us to give up our simplistic analyses of propositional content and literal meaning.
The system fits into a 386-based personal computer and runs under MS-DOS. Among its most appealing features are the very quick and application-independent procedure for training the system to the user's voice and the easy way in which application dictionaries can be created. What makes short training possible is the fact that the system extracts target spectral distributions of phonemes from the training words and combines this speaker-dependent information with speaker-independent information concerning other aspects of the speech signal such as energy profile, phoneme durations, and phoneme similarity. The system accepts natural language without any restriction and can be used to dictate documents of any kind, provided the dictionary of the application domain is specified. Dictation, however, is not the only application we envisage. In this paper we describe our approach to speech recognition and provide some information on the actual implementation and performance of our system.
We describe a new stochastic model for generating speech signals suitable for coding at low bit rates. In this model, the speech waveform is represented as a zero mean Gaussian process with slowly-varying power spectrum. The optimum innovation sequence is obtained by minimizing a subjective error criterion based on properties of human auditory perception. Each block of 40 samples (representing 5 ms of the speech signal sampled at 8 kHz) of the innovation signal is coded into one out of 1024 randomly generated Gaussian sequences of length 40. The chosen sequence minimizes a spectrally weighted error criterion. The innovation signal is thus encoded at 2 kbits/s. A time-varying linear filter whose parameters are determined directly from the speech signal is used to produce the desired power spectrum. Even at this low bit rate the resynthesized speech is barely distinguishable from the original.
A new explanatory and formal theory in speech production is presented. This theory is founded on perturbation theory and supported by computer simulation. Sensitivity functions which relate variations in formant frequency to small variations in area functions are revised in a manner that allows for their application to large variations in area functions. Available speech data are shown to confirm this theory, and known phonetic, articulatory and acoustic phenomena can be explained and embedded in a simple, global and formal model. The theory provides new insights into some principles of articulatory-acoustic relations and the application of these principles to phonetic theory. In this paper, several implications are studied: the articulatory-acoustic relation is simplified, the quantal nature of speech is confirmed, phonetic universals and phonetic systems are considered from a new point of view, formant transitions are explained and normalized, and an easy to control 9-parameter model proposed. We also consider other speech phenomena as interpreted by this theory such as compensatory and symmetry effects. The theory can also be used to formulate and calculate basic speech dynamic patterns. Several applications and research perspectives are proposed.
A familiar aspect of English pronunciation is the occurrence of syllabic consonants. It is common to treat consonantal syllabicity as a consequence of vowel elision, implying that, for example, the pronunciation of “button” as /bλtn/ is the realisation of underlying /bλtən/ (or even /bλt⊃n/). Since elision is a phenomenon that is subject to the influence of speaking style, it would seem to follow that in rapid or casual speech we should expect to find more cases of syllabic consonants and fewer cases of unstressed vowels followed by continuant consonants. This paper sets out to show that the phenomenon is not this simple: we look at problems that confront our attempts at the automatic recognition of syllables and other sub-word units, and consider phonotactic and phonetic factors that may help to resolve them.
This article presents a critical edition and study of a 17th/18th-century poetry collection that had previously been mistaken for al-Ṯaʿālibī's lost Kitāb al-Ġilmān. It provides a codicological analysis of Berlin MS Wetzstein II 1786 in which the poetry collection is contained and also explains and corrects long-held misconceptions regarding al-Ṯaʿālibī's connection with the text.
This paper describes the Speech Synthesis Markup Language, SSML, which has been designed as a platform independent interface standard for speech synthesis systems. The paper discusses the need for standardisation in speech synthesizers and how this will help builders of systems make better use of synthesis. Next the features of SSML (based on SGML, standard generalised markup language) are discussed, and details of the Edinburgh SSML interpreter are given as a guide on how to implement an SSML-based synthesizer.
The performance of a speech recognizer is often degraded by noise. Part of the reason for this performance degradation is due to the fact that there is often a strong mismatch between the training and the testing conditions, i.e. the recognition features used in the training case are vastly different from the features used in the testing condition because of the effect of the noise. One way to circumvent this mismatch problem is to use features which are less susceptible to changing noise conditions. In this paper, we propose the use of a family of signal limiters for recognition of noisy speech. The signal limiter, when properly scaled, is equivalent to performing an arcsin transformation on the autocorrelation functions of the original signal. The effect of using the signal limiter as preprocessor is to reduce the variability of the feature vector, so that the mismatch between training and testing conditions in noise is reduced. Testing on a 39-word English alpha-digit vocabulary, in a speaker trained mode, indicates that the recognition performance of a template-based, dynamic time-warping (DTW) recognizer can be significantly improved in noisy conditions when the robust signal limiter is used as a pre-processor to reduce the variability of the features in strong mismatch conditions.
Microarrays allow monitoring of thousands ofgenes over time periods. However, due to the low number of time points of the gene expression series, taking the temporal dependences into account when clustering the data is an hard task. Moreover, classes very interesting for the biologist, but sparse with regard to all the other genes, can be completely omitted by the standard approaches. We propose a Bayesian approach for this problem. A mixture model is used to describe and classify the data. The parameters of this model are constrained by a prior distribution defined with a new type of model that expresses our prior knowledge. These knowledge allow to take the temporal dependences into account in natural way, as well as to express rough temporal profiles about classes of interest.
We review factors that have affected the synthesis of high-quality speech by analysis-synthesis. The influence of a selected subset of these factors on the quality of synthesized speech was evaluated through listener preference judgements by comparing natural speech to the synthetic speech of two synthesizers: linear prediction coding (LPC) and formant. Several synthesizer excitation waveforms were considered. These waveforms included critical parameters that replicated selected glottal timing events, e.g., the instants of glottal closure and glottal opening. In addition, identifying voiced/unvoiced/mixed excitation and silent intervals in the speech waveform and measuring the fundamental frequency of voicing contributed to the synthesis of high-quality speech. A two-channel approach to speech analysis is recommended to aid the automatic processing of speech, where one channel is the conventional acoustic signal, while the other channel is the electroglottogram (EGG).
To obtain an accurate phone sequence from a continuous speech signal, we suggest a novel approach consisting of tightly coupled bottom-up and top-down processing. The bottom-up path consists of segmentation, recognition and labeling. Also the top-down path consists of labeling, speech generation and segmentation. In this manner, the four processes form a closed feedback loop achieving an optimal interpretation efficiently for a given noisy observation of speech signal and a priori knowledge. The major goal of this paper is to identify the system model using both the stochastic estimation theory and the mean field theory. Experimental results are obtained in terms of the TIMIT database. As a result, the overall system can transform the incoming continuous signal into one of the 61 phone classes at the rate of 73.7%.
One should notice here that the system forces the user to provide 5 signatures that are roughly of the same style (with similar total length and total duration). In fact, we used clustering algorithms and we studied the impact on the performances of the system of: the number of classes; the clustering feature space; the algorithm used. The process to determine the classes consists in using a clustering algorithm on a learning dataset that contains the biometric profiles of several signers. In fact, observing the result of the clustering, it seems that for too many signers, the different signatures in their profile end up in different classes, not in a unique one, which indicates that the clustering is not stable, despite our constraint on the signatures of the profile.
An algorithm is presented which is designed to yield reference pitch contours of speech signals for a comparative evaluation of pitch determination algorithms. For the task of pitch determination the question of error analysis in general and measurement accuracy in particular is discussed. It is shown that the inaccuracy of the algorithm must be less than 0.5% in order to render fine measurement errors inaudible. The algorithm uses the output signal of a laryngograph which measures the laryngeal vibrations via the changes of the electric conductance of the larynx. The point of inflection during the rapid rise of the laryngogram is taken as the estimate for the instant of glottal closure. Using a local interpolation filter the quantization error of the measurement is kept below 0.5%.
This paper argues that human cognition employs two modes of learning, s-mode and u-mode. S-mode learning takes place by means of abstract working memory and is delective and reportable. U-mode learning occurs outside abstract working memory and is unselective and unavailable for verbal report. Three experiments are described, employing two tasks which have previously been shown to give rise to two modes of learning consistent with the conceptualisations of s-mode and u-mode learning. The experiments explore the effects of introducing a secondary verbal task on learning, performing and relearning these tasks. The secondary task can be expected to interfere with s-mode learning. It was found that adding the secondary task interfered with performing and relearning one of the tasks, the one for which subjects could verbalise what they had learnt. In contrast, for the other task, for which subjects could not verbalise their learning, performance and relearning were facilitated by adding the secondary task. These data argue for two modes of learning, only one of which involves the verbal system.
The basic hypothesis is that cry vocalizations of hearing-impaired infants differ from those of their counterparts with normal hearing abilities due to the lack of auditory feedback. The listening experiment shows that it is possible for experts to auditorily classify cries for both infant groups, based on the voice related and melodic cry features. The cries of profoundly hearing-impaired infants are different regarding their perceived sound, rhythm and melody. The sound may well be correlated to spectral characteristics, and melodic and rhythmic parameters are extracted which differ significantly for the two infant groups. The findings are discussed in the context of a cry production model. The extracted signal parameters enable an automatic classification of the cries by means of topological feature maps, which may later be used as the basis for an early supplementary diagnostic tool.
In this paper we address the segmentation problem in a Bayesian framework. Of the three stages (modelling, estimation, optimisation), we consider modelling and optimisation. We consider modelling by Markov random fields. We demonstrate the limitations of the Potts model currently employed, and propose a new model (the chien model) which allows us to control the boundary length and lines in the segmented images. We also preserve fine structures in the data. Then, we compare the MPM and MAP criteria when used with the algorithms discussed above. Results are presented on synthetic images and SPOT data. The classification problem is tackled in a second part.
Hypermedia such as web sites and CD-Rom are very rich but poorly structured resources where a user may easily get lost. It is then necessary to propose a user-adapted help system that allows a user to use efficiently such hypermedia. This is a problem of user modelling where we want to model the user's browsing behaviour, that is related to his goals. This paper deals with learning and recognition of user behaviours in hypermedia through the use of Markov models.
A crucial step in processing speech audio data for information extraction, topic detection, or browsing/playback is to segment the input into sentence and topic units. Speech segmentation is challenging, since the cues typically present for segmenting text (headers, paragraphs, punctuation) are absent in spoken language. We investigate the use of prosody (information gleaned from the timing and melody of speech) for these tasks. Results show that the prosodic model alone performs on par with, or better than, word-based statistical language models – for both true and automatically recognized words in news speech. Across tasks and corpora, we obtain a significant improvement over word-only models using a probabilistic combination of prosodic and lexical information. Inspection reveals that the prosodic models capture language-independent boundary indicators described in the literature. For example, pause and pitch features are highly informative for segmenting news speech, whereas pause, duration and word-based cues dominate for natural conversation.
Automatic speech recognition of real-live broadcast news (BN) data (Hub-4) has become a challenging research topic in recent years. This paper summarizes our key efforts to build a large vocabulary continuous speech recognition system for the heterogenous BN task without inducing undesired complexity and computational resources. These key efforts included: • automatic segmentation of the audio signal into speech utterances; • efficient one-pass trigram decoding using look-ahead techniques; • optimal log-linear interpolation of a variety of acoustic and language models using discriminative model combination (DMC); • handling short-range and weak longer-range correlations in natural speech and language by the use of phrases and of distance-language models; • improving the acoustic modeling by a robust feature extraction, channel normalization, adaptation techniques as well as automatic script selection and verification. The starting point of the system development was the Philips 64k-NAB word-internal triphone trigram system. On the speaker-independent but microphone-dependent NAB-task (transcription of read newspaper texts) we obtained a word error rate of about 10%. Now, at the conclusion of the system development, we have arrived at Philips at an DMC-interpolated phrase-based crossword-pentaphone 4-gram system. This system transcribes BN data with an overall word error rate of about 17%.
This paper shows how an articulatory model, able to produce acoustic signals from articulatory motion, can learn to speak i.e. coordinate its movements in such a way that it utters meaningful sequences of sounds belonging to a given language. This complex learning procedure is accomplished in four major steps: (a) a babbling phase, where the device builds up a model of the forward transforms i.e. the articulatory-to-audio-visual mapping; (b) an imitation stage, where it tries to reproduce a limited set of sound sequences by audio-visual-to-articulatory inversion; (c) a “shaping” stage, where phonemes are associated with the most efficient available sensori-motor representation; and finally, (d) a “rhythmic” phase, where it learns the appropriate coordination of the activations of these sensori-motor targets.
This paper introduces a new tool for web usage mining that relies on an automatic analysis of the web user activity and on an interactive session visualization. The analysis step summarises the information before the visualization step. It clusters the web user logs using the bio-mimetic relational clustering algorithm Leader Ant and produces a set of representative profiles for each cluster, whose definition relies on the computation of typicality degrees. The proposed tool was tested and evaluated on real web log files from the museum of Bourges website, showing that it can easily produce meaningful visualizations of typical user navigation.
Automatic Selection of clinical Trial based on Eligibility Criteria (ASTEC) project is to automate, so as to make it systematic, the search of cancer clinical trials, by reusing the patient data contained into an oncologic electronic health record. ASTEC project tackles two major scientific challenges for medical informatics: 1) the syntactic and semantic interoperability between information systems. The oncologic electronic medical records and the recruitment decision system must be interoperable. The ASTEC project proposes a framework of syntactico-semantic interoperability based on international standards. Generic methods of mediation and reasoning based on ontologies are developed to match data from the electronic medical records to the inclusion/exclusion criteria of clinical trials; 2) a decision support system for recruitment. We have developed inference methods on the electronic medical records adapted to the data structure as well as the eligibility criteria. this paper, we present and justify our choices, concerning the medical process in oncology and the scientific and technical aspects.
This paper gives an overview of the Philips research system for phoneme-based, large-vocabulary, continuousspeech recognition. The system has been successfully applied to various tasks in the German and (American) English languages, ranging from small vocabulary tasks to very large vocabulary tasks. Here, we concentrate on continuousspeech recognition for dictation in real applications, the dictation of legal reports and radiology reports in German. We describe this task and report on experimental results. We also describe a commercial PC-based dictation system which includes a PC implementation of our scientific recognition prototype. In order to allow for a comparison with the performance of other systems, a section with an evaluation on the standard Wall Street Journal task (dictation of American English newspaper text) is supplied. The recognition architecture is based on an integrated statistical approach. We describe the characteristic features of the system as opposed to other systems: 1. the Viterbi criterion is consistently applied both in training and testing; 2. continuous mixture densities are used without tying or smoothing; 3. time-synchronous beam search in connection with a phoneme look-ahead is applied to a tree-organized lexicon.
Vowels occurring in a constant context and spoken by male and female adults were analysed and transformed to obtain perceptually weighted spectral representations. The amount of talker, talker-sex and phoneme information in the spectra was investigated by means of machine classifications under various conditions using appropriate average spectra as classificatory categories. The high percentages of correct classifications obtained indicate that a single vowel token is not as ambiguous with respect to phoneme identity as formant measurements suggest. The results agree with those of experiments investigating the identifiability of the vowels of unknown talkers which show that prior familiarity with a talker's voice is not crucial to correct identification. Some degree of calibration to the talker may nevertheless be necessary, especially as regards talker sex, but it is shown that much of the information need for this normalization is available in the spectrum of the token to be normalized. A preliminary algorithmic normalization procedure is presented which is based on spectral information contained in the token.
This theoretical article aims to draw up an inventory of the latest advances in medical knowledge engineering in the specific area of ontologies and knowledge based systems design. Echoing the debates that animated the landscape of Artificial Intelligence (AI) from the 1970s under the impetus of Dreyfus HL, it aims to show that most of the difficulties currently faced by medical knowledge engineering are inherent in the nature of AI, whose project is the mechanization of cognitive activity. As such it promotes the idea that only a fair understanding of what machines can do, given their machinic character itself, and remains, despite its cognitive finitude, a property of human being, may offer to balancing the scales between tasks that can be allocated to machines and those that have to be left in charge of humans.
Current, state-of-the-art speaker-independent continuous-speech recognizers are able to achieve word recognition rates in excess of 94 percent using lexicons of 1000 words or less and grammars or language models with perplexity 60 or less. Performance of these systems decreases rapidly as the perplexity of the grammar increases. As we allow users the flexibility to speak naturally, using constructions of their own choosing, perplexities increase more than an order of magnitude. Fortunately, knowledge of the domain and of communicative and problem solving behaviors can be used to dynamically decrease perplexity and allow more natural interaction given the current state of speech recognition technology. The perplexity reduction from knowledge results in speech performance equal to that demonstrated by speech recognizers using an equivalently low perplexity language model in the same or different domains. This paper addresses how knowledge of domain semantics, dialog, communication conventions and problem solving behavior are used to enhance automatic speech recognition and understanding. Included is a discussion of the system's basic principles and descriptions of the important knowledge sources and heuristics employed by the minds system. This is followed by a brief analysis of some of the heuristics which do not have to be reimplemented across domains. Specifically addressed are why the heuristics are effective, and how much each can be expected to reduce entropy and average branching factor in any possible application domain.
In real-life applications, errors in the speech recognition system are mainly due to inefficient detection of speech segments, unreliable rejection of Out-Of-Vocabulary (OOV) words, and insufficient account of noise and transmission channel effects. In this paper, we review a set of techniques developed at CNET in order to increase the robustness to mismatches between training and testing conditions. These techniques are divided in two classes: preprocessing techniques and Hidden Markov Models (HMM) parameters adaptation. The results of several experiments carried out on field databases, as well as on databases collected over PSN and GSM networks are presented. The main sources of errors are analyzed. We show that a blind equalization scheme significantly improves the recognition accuracy regarding both field and GSM data. Speech detection allows a system to delimit the boundaries of the words to be recognized. We also use preprocessing techniques to increase the robustness of such detectors to noisy GSM speech. We show that spectral subtraction improves speech detection under noisy GSM conditions. Our experiments show an equivalent performance obtained with both Bayesian and linear regression adaptation of HMM parameters. The results obtained also prove that HMM adaptation and preprocessing techniques can be advantageously combined to improve Automatic Speech Recognition (ASR) robustness.
This paper presents the work done at the CNET in speech recognition during the last few years. The authors present the recent generation of speaker-independent systems, based on statistical modeling using the Markov models (PHIL86 software). Several applications of these systems in the Telecommunications area are described, as well as the lessons drawn from them.
In this paper, we describe our progress during the last four years (1995–1999) in automatic transcription of broadcast news from radio and television using the BBN Byblos speech recognition system. Overall, we achieved steady progress as reflected through the results of the last four DARPA Hub-4 evaluations, with word error rates of 42.7%, 31.8%, 20.4% and 14.7% in 1995, 1996, 1997 and 1998, respectively. This progress can be attributed to improvements in acoustic modeling, channel and speaker adaptation, and search algorithms, as well as dealing with specific characteristics of the real-life variable speech found in broadcast news. Besides improving recognition accuracy, we also succeeded in developing several algorithms to achieve close-to-real-time recognition speed without a significant sacrifice in recognition accuracy.
In this paper, we are dealing with the problem of facial features segmentation (mouth, eyes and eyebrows). A specific parametric model is defined for each feature, each model being able to take into account all the possible deformations. In order to initialize each model, some characteristic points are extracted on each image to be processed (for example, the corners of the eyes, mouth and eyebrows). In order to fit the model with the contours to be extracted, a gradient flow (of luminance or chrominance) through the estimated contour is maximized because at each point of the searched contour, the gradient (of luminance or chrominance) is normal. The advantage of the definition of a model associated to each feature is to be able to introduce a regularisation constraint. However, the chosen models are flexible enough in order to produce realistic contours for the mouth, the eyes and eyebrows. This facial features segmentation is the first step of a set of multi-media applications.
This communication addresses multilingual aspects in speech recognition and tries to link them to the concept of interoperability. After a tentative definition of multilingual interoperability, the speech recognition components are discussed with a view towards separating language-specific from language-independent elements. An overview gives examples of previous multilingual speech recognition research and developments across different speaking styles (read, prepared and conversational). The problem of adaptation across languages is addressed. In particular there exist language-independent and cross-language acoustic modeling techniques to port recognition systems from one language to another without language-specific acoustic data. However these data remain valuable for acoustic model adaptation. At this time pronunciation dictionaries and text material appear to be the most crucial language-dependent resources. In our opinion fast porting, enabled by the existence of these language-dependent resources, is a step towards multilingual interoperability. On-going efforts to produce multilingual pronunciation dictionaries and to collect multilingual text corpora including speech transcripts, should be extended to the largest possible number of languages. These efforts could be shared with initiatives aiming at the support of minority languages.
CAD softwares are intended to become real tools for physical objects design aid. But the preliminary design remains a widely open research field. This survey tries to show that a constraint approach of design process is unavoidable to reach that goal. Design process can be seen as an elaboration and constraint satisfaction cyclic process. The associated conceptual model is a geometric constraints object model. It is a multilevel model, integrating geometric and non geometric knowledge. Following are the different approaches of geometric modeling: parametric, variational, feature-based and declarative modeling. It ends up with a brief presentation of solving and decomposition methods of CAD constraints systems.
In this paper, in the case of spatially uncorrelated (or slightly correlated) noises, we introduce a new technique based on the coherence function which is used to determine a speech/noise classification algorithm. We combine it with a noise reduction technique based on the spectral subtraction and evaluate its influence. We report on results obtained on the performance of the algorithm and conclude that they are quite comparable to those obtained using a manual labelling.
This paper presents a review on the use of time-frequency representations in the fields of speech analysis and automatic speech processing. Three main groups of methods are considered: speech production based methods, general signal analysis methods, auditory-based methods. After this review, some short conclusions on their current use, and on some possible future evolutions are proposed.
This paper presents a multi-classifier system design controlled by the topology of the learning data. Our work also introduces a training algorithm for an incremental self-organizing map (SOM). This SOM is used to distribute classification tasks to a set of classifiers. Thus, the useful classifiers are activated when new data arrives. Comparative results are given for synthetic problems, for an image segmentation problem from the UCI repository and for a handwritten digit recognition problem.
We have developed a multi-channel pitch determination algorithm (PDA) that has been tested on three speech databases (0 dB SNR telephone speech, speech recorded in a car and clean speech) involving fifty-eight speakers. Our system has been compared to a multi-channel PDA based on auditory modelling (AMPEX), to hand-labelled and to laryngograph pitch contours. Our PDA is comprised of an automatic channel selection module and a pitch extraction module that relies on a pseudo-periodic histogram (combination of normalised scalar products for the less corrupted channels) in order to find pitch. Our PDA excelled in performance over the reference system on 0 dB telephone and car speech. The automatic selection of channels was effective on the very noisy telephone speech (0 dB) but performed less significantly on car speech where the robustness of the system is mainly due to the pitch extraction module in comparison to AMPEX. This paper reports in details the voiced/unvoiced, unvoiced/voiced performance and pitch estimation errors for the proposed PDA and the reference system while utilising three speech databases.
This paper describes a framework for optimising the structure and parameters of a continuous density HMM-based large vocabulary recognition system using the Maximum Mutual Information Estimation (MMIE) criterion. To reduce the computational complexity of the MMIE training algorithm, confusable segments of speech are identified and stored as word lattices of alternative utterance hypotheses. An iterative mixture splitting procedure is also employed to adjust the number of mixture components in each state during training such that the optimal balance between the number of parameters and the available training data is achieved. Experiments are presented on various test sets from the Wall Street Journal database using up to 66 hours of acoustic training data. These demonstrate that the use of lattices makes MMIE training practicable for very complex recognition systems and large training sets. Furthermore, the experimental results show that MMIE optimisation of system structure and parameters can yield useful increases in recognition accuracy.
In this paper, we present an algorithm designed for the stéréovision matching problem and 3D objetcs identification. We use a simulated Hopfield neural network to solve the problem of matching a pair of stereoscopic images. This model is helpful in optimization and it can be implemented on parallel machines easily.
Ageing at home is nowadays a matter of public health in western societies. The presented work proposes to process the huge amount of data collected at elders homes to provide a consolidated view which describes their general behaviors. To achieve this processing, several classification algorithms are presented. These algorithms rely on a multi-agent paradigm and deal with the heterogeneity among the collected data. Based on the resulting profiles, changes in health status can be anticipated for a specific elder but it is also possible to use these profiles to detect some global events such as an epidemic or the impact of high temperature on an ageing population.
Since simultaneous optimization of source and channel coding is often not practical for speech-coding algorithms, it is useful to develop channel codes optimal for a particular speech coding algorithm operating under a specified range of channel-error conditions. Such source-dependent channel-error codes can be obtained by minimizing an appropriate speech distortion criterion. To carry out this minimization, we use a simulated-annealing procedure. The resulting channel codes are efficient, since they provide error correction of non-uniform accuracy (highly probable quantization levels receive more accurate correction) and/or non-uniform error detection (errors which greatly impact the speech quality are more likely to be detected). An optimal trade-off between error correction and error detection can be obtained. Source-dependent channel codes aimed at counteracting low, random error rates (up to 2%) are applied to the CELP algorithm and the resulting performance is reported. It is found that a small allocation of codewords (equivalent to less than one bit) for the protection of a particular parameter often results in a large performance improvement.
This article presents an overview of the research activities carried out in the European CAVE project, which focused on text-dependent speaker verification on the telephone network using whole word Hidden Markov Models. It documents in detail various aspects of the technology and the methodology used within the project. In particular, it addresses the issue of model estimation in the context of limited enrollment data and the problem of a posteriori decision threshold setting. Experiments are carried out on the realistic telephone speech database SESP. State-of-the-art performance levels are obtained, which validates the technical approaches developed and assessed during the project as well as the working infrastructure which facilitated cooperation between the partners.
Our study deals with the parameter estimation problem of Hidden Markov Chain models and with unsupervised Bayesian image segmentation. We propose two new estimation algorithms obtained from Iterative Conditional Estimation (ICE) and Stochastic Estimation Maximisation (SEM), denoted by MICE and MSEM respectively, and show their competitiveness with respect to the Estimation Maximisation (EM) algorithm in different situations of chain homogeneity and noise. We then study three unsupervised chain restauration algorithms, obtained by adding EM, MICE and MSEM respectively to the Maximum Posterior Mode (MPM) restauration method. The transformation of bi-dimentional process to mono-dimentional ones using Peano curves makes possible the application of these three methods to the problem of unsupervised statistical image segmentation. Doing so, we obtain faster methods than those obtained by models using hidden Markov random fields and we show that the loss of effectiveness, due to the poorer adequacy of the model, is acceptable in general. On the other hand, the flexibility of our modeling allows the conception of numerous unsupervised spatio-temporal segmentation methods. We propose three of them and present results showing their application to the segmentation of a sequence of real images.
This paper introduces a Recurrent Radial Basis Function network (RRBF) for nonlinear system prognosis. The training process is divided in two stages. In the second stage, a multivariable linear regression supervised learning technique is used to determine the weights of the connections between the hidden and output layer. The FuzzyMinMax technique makes the K-means more stable.
Time-scale and, to a lesser extent, pitch-scale modifications of speech and audio signals are the subject of major theoretical and practical interest. Applications are numerous, including, to name but a few, text-to-speech synthesis (based on acoustical unit concatenation), transformation of voice characteristics, foreign language learning but also audio monitoring or film/soundtrack post-synchronization. To fulfill the need for high-quality time and pitch-scaling, a number of algorithms have been proposed recently, along with their real-time implementation, sometimes for very inexpensive hardware. This contribution reviews frequency-domain algorithms (phase-vocoder) and time-domain algorithms (Time-Domain Pitch-Synchronous Overlap/Add and the like) in the same framework. More recent variations of these schemes are also presented.
Automatic speech recognition by computers can provide the most natural and efficient method of communication between humans and computers. While in recent years high performance speech recognition systems are beginning to emerge from research institutions, scientists unequivocally agree that the deployment of speech recognition systems into realistic operating environments will require many hours of speech data to help us model the inherent variability in the speech signal. This database is particularly valuable as a source of spontaneous utterances elicited in a realistic goal-oriented environment.
The relationship between speech and language processing is an important problem to be solved in order to achieve continuous large-vocabulary speech recognition for a speech translation system or the human interface of a man-machine system. For the recognition of large-vocabulary continuous speech, first the phonemes are recognized by HMM (Hidden Markov Model). A generalized LR parser is introduced to predict next words/phonemes. The Japanese utterance is successfully recognized by the combined HMM-LR parser (HMM-LR). Many phrase candidates are filtered out of the speech recognition system through the use of linguistic information. An experimental system which translates spoken Japanese into English (SL-TRANS) has been implemented. The translation method consists of analysis, transfer and generation processes. A new method incorporated in the system analyzes the expressions of linguistic intention meanings in utterances.
This paper presents a brief overview of current uses, in 1994–1995, for CNET speech recognition and text-to-speech technologies in Interactive Voice Response Services (IVR) in France. It describes several operational and experimental services, and analyzes field evaluations of some of them. Finally, this paper summarizes recent developments in the CNET speech recognition and text-to-speech technology.
These methods can not be applied directly to speech analysis where the input signal (the excitation source of the vocal tract) is unknown. In this paper, after clarifying the mechanism, in the case of unknown input signal, that limits the estimation precision, we describe a solution based on the impuls detection and suppression in the residue. This approach can improve the parameter estimation accuracy mainly for the case where the residue is close to an impuls train. It has been shown that this method improves the spectrum estimation of speech signals.
In this paper, we propose a new synthesis unit learning method aiming at multi-lingual speech synthesis and describe its application to English speech synthesis. The method termed Multi-Layered Context Oriented Clustering (ML-COC) is a generalized framework of the COC method which has been applied to Japanese speech synthesis. The conventional COC method produces a set of phonetic context dependent units through a cluster splitting process. In ML-COC, the notion of context is generalized and the factors other than phonetic context, such as stressing and syntactical boundaries, are taken into account to capture the richer phoneme variations of English. A synthesis unit generation experiment shows that ML-COC produces about three times as many synthesis units as the conventional COC (Single-Layered COC: SL-COC) method, and the average intra-cluster variance of ML-COC units is 20% lower than that of SL-COC. These results suggest that the ML-COC synthesis units reflect the phonological structure of English much more appropriately than do the SL-COC units. To validate the effectiveness of the ML-COC method, we conducted preference experiments using synthesized speech. The preference test exposed 10 subjects to 52 sentences. The ML-COC method was preferred over the conventional SL-COC method by a score of 70% to 30%.
This work deals with the reconstruction of Positron Emission Tomographic (PET) three-dimensional (3D) images for the detection of small tumors and metastases in oncology. In PET, tumors appear as areas of hyperfixation of the injected tracer compared to regions with normal uptake. We model the 3D distribution of activity by a mixture of laws, which describes the fact that each point in the 3D volume contains either a normal or a high activity concentration. We solve this model using a Maximum Entropy on the Mean (MEM) approach. Results obtained with our approach are compared with those obtained using two methods that are conventionally used for 3D PET reconstruction. Using simulated data, results obtained with the MEM approach are significantly better than those obtained using the two other methods, when considering an evaluation criterion which characterizes the quality of reconstructed images in terms of lesion detectability. The feasibility of the method is also illustrated on clinical data.
In conformal radiotherapy, beam set up and dose calculations are achieved using images obtained from computed tomography (CT) or magnetic resonance (MR). These images are taken before the treatment which is performed on several sessions on several weeks. At the beginning of each session, the patient has to be positioned on the treatment couch under the linear accelerator in the same position as during MR or CT imaging and planning, and the organs are assumed to be in the same place. Currently, the methods used for this repositioning are based on the external anatomy of the patient and suppose an immobility of the internal structures. In this study, we present a new approach, suited to the clinical practice, for the automatic repositioning of patient in prostate cancer radiotherapy. It is based on localisation by ultrasound images and optical stereolocalisation and on a matching with images regenerated in the planning volume. The method exploits a statistical model of the prostate to automatically extract its contours. The first tests in conditions of a radiotherapy session show that the method is able to obtain a patient setup with an accuracy of about 1.4 mm.
Stop consonants are produced by forming a closure in the vocal tract, building up pressure in the mouth behind this closure, and releasing the closure. At the consonantal release, these components of the sound include an initial transient, a burst of frication noise, and an interval in which there is a sound source at the glottis and transitions in the formants. The models predict the absolute levels of these components for different places of articulation for the consonants.
This paper presents a new approach to equalize the telephone line effects in the transmitted signal aiming at improving the performance of speech recognition systems. Measurements carried out on actual telephone data confirm that telephone lines introduce disturbing convolved components in speech signals. Line effects are almost constant for a given call but vary with the calls. The proposed adaptive filtering of the telephone line effects is compared to two conventional techniques, namely subtraction of the long-term cepstrum and highpass filtering of cepstral trajectories. Recognition experiments are carried out on several telephone databases in a speaker-independent mode. The results show that reducing the channel effects significantly improves the recognition performance. Regarding the obtained error rates, the proposed adaptive filter yields better performance than the conventional highpass filters. However, this adaptive filtering is not as good as the off-line cepstral subtraction technique where the long-term cepstrum is estimated on several recordings of a call. Experiments were also conducted to measure the amount of speech data necessary to obtain a reliable estimate of channel effects. Averaging cepstra vectors on a few seconds of speech produces a reliable estimate of the constant convolved perturbation.
In the context of a discussion on Petrarchism as a process of imitatio within different literary genres, we study the modalities of transmission of petrarchist schemes and images by comparing the preliminary sonnets (I and II) of Pontus de Tyard's 1549 Erreurs amoureuses, which are analysed in parallel with Voi ch'ascoltate in rime sparse il suono and with Ronsard's Amours first sonnet. The two first sonnets of the Erreurs amoureuses represent a splitting of the traditional preliminary apostrophe, addressed to a public that is at once a spectator and an actor of the love suffering. These sonnets take from Petrarch the theme of the giovenile errore, all the whilst interpreting the polysemic erreur in the light of its two principal meanings: erreur as a journey (wandering), and erreur as an error. As far as Pontus de Tyard is concerned, Petrarch is quite evidently the fundamental model, both in terms of the structure and of the vocabulary. In fact, it is indeed the lexical level that provides information in this field: the use of the same lexemes errore/erreur demonstrates to what extent the tendency toward variatio of the petrarchist archetype becomes first and foremost a rhetorical fact.
'Root' and 'affix' as linguistic terms, and later 'suffix' and 'prefix', do not belong to the large lexical technical lore inherited from the Greco-Latin world. Like 'scheva, shewa', they appear during the Renaissance, when grammarians from the Latin tradition discover Hebrew grammar. For the first time, 'Latin grammarians' have to tackle with a foreign but prestigious tradition. The history of these new words is here described with some detail, because they also reflect a new look at languages. The confrontation between the two grammatical traditions will result in the development of new concepts and a new, more analytical, approach to language morphology.
This article presents a method of formant-to-area mapping consisting of the direct calculation of the time derivatives of the cross-sections and length of a vocal tract model so that the time derivatives of the observed formant frequencies and the model's eigenfrequencies match. The vocal tract model is a concatenation of uniform tubelets whose cross-section areas and lengths can vary in time. Time derivatives of the tubelet parameters are obtained by solving a linear algebraic system of equations. The derivatives are then numerically integrated to arrive at cross-section and length movements. Since more than one area function is compatible with the observed formant frequencies, pseudo-energy constraints are made use of to determine a unique solution.
We study heuristic methods for a classification problem encountered in cork industry. More precisely, we want to optimize the parameters of the classification rule that is daily used to classify corks. We experimented several metaheuristics and compared their performance on a real test case. The obtained results improve the current classification rate. We propose also, from our observations, new directions that may lead to further improvement for a still better classification.
We present an automatic recognition system applied to handwritten numeral check amounts which is based on a segmentation-by-recognition probabilistic model This system is descridedfrom the amount field localization on the document image to the generation of hypotheses. An explicit segmentation algorithm determines potential cuts between characters and provides a spatial representation of segmented components. The best path for the segmentation is determined by the combination of recognition scores, of segmentation likelihoods and of a priori probabilities of amounts. The robustness of the system was assessed on a database of 10,000 real cheques images.
This paper describes the RailTel system developed at LIMSI to provide vocal access to static train timetable information in French, and a field trial carried out to assess the technical adequacy of available speech technology for interactive services. The data collection system used to carry out the field trials is based on the LIMSI Mask spoken language system and runs on a Unix workstation with a high quality telephone interface. The spoken language system allows a mixed-initiative dialog where the user can provide any information at any point in time. Experienced users are thus able to provide all the information needed for database access in a single sentence, whereas less experienced users tend to provide shorter responses, allowing the system to guide them. The RailTel field trial was carried out using a common methodology defined by the consortium. 100 novice subjects participated in the field trials, each calling the system one time and completing a user questionnaire. Of the callers, 72% successfully completed their scenario. The subjective assessment of the prototype was for the most part favourable, with subjects expressing an interest in using such a service.
We consider adaptive prediction with an HR moving average (MA) part, when controlled either by the recursive LMS algorithm or by an extended LS (ELS) algorithm based on a posteriori errors. The predictor input is either the sum of band-pass components or a nonstationary speech sentence. We show on one hand that using the a posteriori error algorithm smoothes the oscillations due to the selfstabilization phenomenon, compared to the LMS algorithm case. On the other hand we show that with a nonstationary input the LMS algorithm can be unstable due to power jumps in the speech signal. Finally the a posteriori error algorithm ensures BIBO stability (Bounded Input-Bounded Output) even for a nonstationary input.
Text-dependent speaker identification performance is investigated for small groups of speakers in which each speaker in a group is assigned the same sentence-long password utterance. Several model construction conditions are studied. Baseline maximum likelihood estimate (MLE) models are constructed from three same-session training utterances. Minimum classification error (MCE) models are constructed using the training utterances of all speakers in a group. In addition, models are constructed using additional test utterances from speakers in the group or additional utterances from speakers outside the group. Results show that error rates approximately double from 5-speaker groups to 10-speaker groups. MCE models provide about 25% improvement in closed- and open-set identification error rates, but less improvement, about 10% in imposter accept rates. The greatest improvements are obtained, for both MLE and MCE models, when customer test utterances augment the training utterances. For MCE models closed-set identification error rates are approximately 0.4% and 0.6% for 5- and 10-speaker groups, respectively, while imposter accept rates are approximately 4% and 10%, respectively, when customer reject rates are 5%.
This paper presents a microphone array adaptive beamformer with a dual function. The noise enhanced output is suited to transmission as well as to use as input to speech recognition systems. The areas of use envisaged are the car. the factory floor and noisy offices. The underlying structure is a steered Griffiths-Jim beamformer, with an added speech detection switch for the selective adaptation of both sections. This beamformer is effective in suppressing both stationary and non-stationary interference and is therefore a preprocessor for a wider range of speech recognition applications than any single channel noise suppression scheme could handle. Experiments were performed in a reverberant room with a 4-microphone array. Typical SNR improvements for communication purposes range from 4 to 12 dB. The effective SNR improvement for speech recognition purposes ranges from 4 to 8 dB.
Speech synthesis now profitably automates services by speaking information from computer databases. Some of these services provide driving directions, traffic and timetable information, stock quotes and related financial services, and catalog ordering. One particularly successful telecommunication service, Automated Customer Name and Address (ACNA), sometimes called Reverse Directory Assistance (RDA), requires synthesis with high intelligibility and name pronunciation accuracy, both of which are achieved by current synthesis technology. However, even the best of current technology is not good enough to mindlessly `drop' into complex services. Customized directory preprocessing is necessary to transform listing data, which commonly contains unconventional abbreviations, acronyms unknown to the synthesizer, and scrambled word ordering, into a sentence suitable for synthesis. We describe our directory preprocessing programs used in successful implementations of synthesis in two major US telephone companies. It is also necessary that locality terms, which have considerable geographical variability, are pronounced in accordance with local customs; otherwise, the service will have an outsider's feel to the customers. We also describe an experiment that determined whether the naturalness of recorded speech for prompts and other fixed messages offsets the undesirable discontinuity between natural and synthesized utterances.
In this paper, we present a multi-stream approach for off-line handwritten word recognition. The multi-stream formalism presents many advantages: it can combine several kinds of independent features. Significant experiments have been carried out on two publicly available word databases: IFN/ENIT benchmark database (Arabic script) and IRONOFF database (Latin script). Moreover, the proposed recognition system provides significant results comparable to the best results reported in the literature on both databases.
This paper discusses recent advances in and perspectives of research on speaker-dependent-feature extraction from speech waves, automatic speaker identification and verification, speaker adaptation in speech recognition, and voice conversion techniques. Speaker-dependent information exists both in the spectral envelope and in the supra-segmental features of speech. This individual information can be further classified into temporal and dynamic features. Speaker identification/verification methods can be divided into text-dependent and tect-independent methods. Although text-dependent speaker verification techniques have almost reached the level suitable for practical implementation, text-independent techniques are still in the fundamental research stage. Both supervised and unsupervised speaker adaptation algorithms for speech recognition have recently been proposed, and remarkable progress has been achieved in this field. Improving synthesized speech quality by adding natural characteristics of voice individuality, and converting synthesized voice individuality from one speaker to another, are as yet little exploited research fields to be studied in the near future. Research on speaker-dependent information is one of the most important future directions for achieving advanced speech information processing systems.
Interethnic communication in Cameroon is sometimes characterized by a derogatory discourse on the ethnicity of others. This discourse generally appears in the form of insults, jokes, teases, etc. built into narrations, songs, fiction and telecasts, and is transported from one generation to the other. This article describes some of the strategies used to denigrate the ethnic identity of others in Cameroon. The analyses, based on data (questionnaires, participant-observations, interviews) collected in Yaoundé and other regions of the country, show how Cameroonians use borrowings, nominal compositions, metaphors, semantic shifts, metonymies, etc. to denigrate, downgrade, dehumanize, or demonize members of certain ethnic groups and/or to erase, minimize or contest the ethnicity of others and their ethnic groups.
The recognition of Mandarin syllables is a key problem in large vocabulary Mandarin speech recognition. Conventionally, the tone and base syllable corresponding to a syllable are separately recognized by using a tone recognizer and a base syllable recognizer, respectively. In this paper, we propose a framework for Mandarin syllable recognition based on the classification of sub-syllabic units such as initials, finals and transitions. The final units are classified in accordance with the variations of tones to enhance the capability of tone discrimination. By using hidden Markov models (HMM) based on LPC-derived cepstral parameters, we develop a Mandarin syllable recognizer in which base syllables and their corresponding tones are jointly recognized. Experimental results indicate that the proposed syllable recognizer yields higher recognition rates than the conventional syllable recognizer does when sufficient amount of training data is used. We also show that the performance of the proposed syllable recognizer can be further improved with the incorporation of a tone recognizer.
This paper first surveys and classifies applications of voice humancomputer dialogue. The advantages and limits of speech as a means of communication between users and software are then considered. A major problem in the development of user interfaces with a voice component, besides the choice of appropriate applications, is speech recognition, especially continuous speech recognition. As approaches differ according to the type of application, we first summarize problems and techniques specific to voice data input ; as an example, we briefly describe the speech recognition approach that we have adopted for the dictation machine that we are developing in our Laboratory. Then, the case of voice dialogue understanding and management is considered. To illustrate the discussion, the architecture and functionalities of some prototypes that we have implemented are presented: for instance, an E-mail system and a Sonar control software. Finally, we present a dialogue manager DIAL capable of helping ¡guiding the user in complex cognitive activities that we are currently developing and implementing.
I present and discuss the SAPHO (Segmentation by Acoustico-Phonetic knowledge) model implemented in Awk language under the Unix system on a MASSCOMP computer. The system is devised as a speaker independent ASS (automatic speech segmentation), by a previous recognition of the phonetic articulation manner. In all the ASR systems the phonetic knowledge is at least implicitely used. It has to be explicitely referred to. The phonemic units cannot be directly built from the acoustic signal and are not available at the output of SAPHO. According to the Level Building procedure SAPHO supplies a hierarchized set of acoustic properties and segments, and phonetic properties and segments which fit the phonetic parsing of the acoustic wave. The amenability of this system is entailed by its modularity which allows a possible further architecture as distributed tasks. The suitability and the reliability of SAPHO are corroborated by the accuracy of the results.
400-channels constant bandwidth (12.5 Hz) Long Term Spectra (LTS) delivered by a BK 2033 analyser have been drawn from French utterances. The cross-correlation coefficient was used to investigate LTS residual intra-speaker variability both in inter- and intra-text conditions. Significant subject-dependent differences habe been revealed in both conditions. They indicate voice coherence variability in the speakers. Correlations lowering in inter-text condition moreover reveals the content-dependent nature of LTS. The need for basic LTS research prior to further application is therefore emphasized.
The pervasive use of clinical categories of aphasia in neurolinguistic and cognitive neuropsychological research reflects the assumption that these patient groupings represent disruptions of the normal language processing system along theoretically significant lines. This premise is examined here with particular reference to the status of 'agrammatism'. It is argued that there are compelling reasons to question the coherence of agrammatism as a psychological entity. To overcome these objections, the clinical intuitions on which this aphasic category is based must be replaced by objective criteria for selecting a theoretically significant patient grouping. To reach this goal, it is especially important that a theoretically motivated distinction be made between within-and across-category variation. It is argued that in the case of categories like agrammatism, there are serious methodological obstacles which make such goals unreachable. It is argued, therefore, that our theories should not take categories like agrammatism as psychological givens, especially if the purpose of our research is to reach an understanding of the mechanisms of language processing or of individual aphasic deficits themselves.
Our conception of what it is that the speech production mechanism is attempting to implement in speaking comes from linguistics. But linguistics first developed its methods for other purposes. In the early 19th century linguistics produced a method for tracing the family relationship between suspected cognate words and their constituent sounds. This, the comparative method, involved establishing an optimal path between these forms via a reconstructed parent form. 20th century structuralist linguistics (including generative phonology), essentially grafted the same method onto the task of finding the underlying phonemic constituents of words. Although the underlying structure found in this way may be a good hypothesis as to the mental elements determining actual spoken utterances, there are reasons to suspect that it is too simple. Too much emphasis is placed on the simplicity of the system and on the purely lexical (as opposed to the demarcative and attidudinal) function of the elements in speech. This paper presents some initial attempts to differentiate between phonetic variants in speech which stem from single underlying forms as opposed to those which arise from separate underlying forms (though they may have had a common source historically).
Data or information is only useful when it answers one's information needs, and when it is delivered in a way that facilitates understanding and use. We have all experienced trying to find information about a specific topic, issuing a number of queries on the web using a search engine in an attempt to obtain appropriate links, and, finally, trying to pull together the information we wanted from all the results. In this paper, we present the Virtual Document Planner, a platform designed to support tailored information delivery. The VDP addresses the problem of delivering information that may come from a variety of sources, and that needs to be delivered in a form that facilitates comprehension and use. To illustrate that approach, we describe an application where the VDP is applied to the generation of information about a specific topic in the context of a company web site. We also report on a preliminary study investigating the effect of such information delivery.
This paper presents a comparison of sequential selection processes in order to identify the relevant indices for the early syncopes prediction during a diagnosis test. An hybrid approach, combining a sequential process and a genetic algorithm, has allowed to increase the prediction performance, to reduce the number of selected variables and also to reduce the total number of evaluated subsets during the selection process. Finally, the obtained results allow to predict the apparition of syncope with a sensitivity of 100% and a specificity of 94%.
There is controversy over the role of auditory scene analysis in speech perception and in particular whether listeners form perceptual streams of formants. The role of vowel formant frequencies in the perception of synthetic vowel–nasal syllables and the importance of formant continuity between vowel and nasal were examined in three experiments. When no explicit transitions were present between the vowel and nasal, the perception of each nasal changed from /m/ to /n/ as the vowel F2 increased. Introducing explicit formant transitions removed this effect, and listeners heard the appropriate percepts for each nasal prototype. However, if the transition and the nasal prototype were inconsistent, the percept was determined by the transition alone. In each experiment, therefore, the target frequency of the vowel F2 transition into the nasal consonant determined the percept, taking precedence over the formant structure of the nasal prototype. The results do not show strong evidence for formant streaming and are more consistent with pattern matching processes.
This paper presents an adaptation method of speech hidden Markov models (HMMs) for telephone speech recognition. Our goal is to automatically adapt the HMM parameters so that the adapted HMM parameters can match with the telephone environment. In this study, two kinds of transformation-based adaptations are investigated. One is the bias transformation and the other is the affine transformation. A Bayesian estimation technique which incorporates prior knowledge into the transformation is applied for estimating the transformation parameters. Experiments show that the proposed approach can be successfully employed for self adaptation as well as supervised adaptation. Besides, the performance of telephone speech recognition using Bayesian adaptation is shown to be superior to that using maximum-likelihood adaptation. The affine transformation is also demonstrated to be significantly better than the bias transformation.
A theoretical account of stuttering is presented in which an inadequacy of neuronal resources for sensory-motor information processing is seen as the basis of the disorder. It is proposed that stutterers are deficient in the processing resources normally responsible for determining and adaptively maintaining the internal models which subserve speech production. A general description of such computational processes is detailed in the form of circuitry for an adaptive controller which can calibrate itself to control any variable, nonlinear, dynamic, multiple input, multiple output system.
There are often sufficient cues which allow the auditory system to determine whether sound components continue through such occlusions. This paper reviews the situations where an assumption of continuity is warranted and demonstrates how the principles governing the so-called “continuity illusion” can be used within a computational system for segregating acoustic sources.
To deal with large lexica (more than 2000 words) automatic speech recognition systems (ASR) use an internal phonetic representation of the speech signal and phonemic models of pronunciation from the lexicon to search for the spoken word chain or sentence. Therefore it is possible to model different pronunciations of a word in the lexicon. In German we observed that individual speakers pronounce words in a typical way that depends on several factors as sex, age, place of living, place of birth, etc. Our goal is to enhance speech recognition by automatically adapting the models of pronunciation in the lexicon to the unknown speaker. Another method presented in this paper is speaker adaptation by re-estimating the a posteriori probabilities of the phonetic units used in a “bottom up” ASR system. A word hypothesis is evaluated by the product of the a posteriori probabilities of the phonetic units produced by the classification to the phonetic units belonging to the word hypothesis. Normally these probabilities are estimated during the training of the ASR system and stay fixed during the test. We propose an algorithm which observes the typical confusions of phonetic units of the unknown speaker and adapts the a posteriori probabilities continuously.
Underwater magnetic signals are affected by a nonstationnary noise from different sources. In order to detect the distortion in the magnetic field caused by the displacement of a ferromagnetic mass we try to eliminate as much of the noise as possible by measuring the magnetic field with four sensors when the transient has been received by one of them.
We propose an interactive graphical tool, CA Viz, which allows to visualize and to extract knowledge from FCA results on images. Originally the FCA is for the analysis of contingency tables. For adapting FCA on images, we first define the "visual” words in images. These words are constructed from local descriptors (SIFT, Scale Invariant Feature Transform) in images. CAViz projects clouds ofpoints in factorial plans and allows viewing and extracting interesting information such as: characterizing words, important factors using FCA relevant indicators (representation quality and contribution to the inertia). An application to the Caltech4 database shows the interest of CAViz for image mining.
Mryati et al. use the properties of the sensitivity functions of a cylindrical tube to divide it into eight specific regions associated to the eight possible combinations of the sensitivity of the first three resonances. Bringing out acoustic properties of symmetry and compensation, they claim that the production of vowels and consonants is based on these geometric and acoustic properties, since the eight regions can be linked to morphological and articulatory properties of the vocal tract. The authors formulate a new vowel production theory and they propose a universal phonological system for consonants. We are critical of the New Theory on several counts: • - the limitations of sensitivity functions have been overlooked; • - the generalizations are anthropomorphically weak; • - the predictions fail to match known acoustic facts; • - the universal classification is in contradiction with basic phonetic knowledge. More generally this kind of approach seems intrinsically very limited: the vocal tract is not a series of tubes whose dimensions can be manipulated independently, without referring to an underlying articulatory model integrating articulatory constraints. The New Theory will retain all of its value once it has been returned to its natural context i.e. the simple acoustic description of the vocal tract around the neutral position and as a tool for speech synthesis.
As part of a system for the automatic recognition of isolated words in a large vocabulary on the basis of an analytical approach, we considered the automatic speaker-adaptation of the system. This was carried out by means of an automatic learning procedure of the speakers' reference patterns, and by automatically adjusting the parameters of the system. This learning relies on a time alignment algorithm using acoustic-phonetic features which are little speakerdependent. The learning session was successfully tested on 18 speakers out of 20 (10 women and 10 men) and the reference patterns thus obtained yielded good results during the recognition phase. We have now undertaken an analysis of the vowels uttered by 15 speakers based upon descriptive statistics and statistical interpretation in order to design procedures of normalization and of automatic generation of a speaker's vowel reference patterns.
In the STAP domain, modeling the interferences as an autoregressive (AR) process with the detector called Parametric Adaptive Matched Filter (PAMF), provides an estimation of the clutter-rejection filter with few training data. The main difficulty of this approach is the estimation of the AR matrices by using the training data. Thus, we propose an on-line estimation based on the Kalman filter and its variants. A comparative study is carried out and illustrates the relevance of such approaches with data provided by the DGA.
This paper presents a software architecture, based on web services, that enables the creation and evaluation of interactive visual applications. Web services are a standard for data exchange in a distributed system, such as the web. They are mainly used for data publishing (via API), but can also be used for data processing. We show that web services composition permits the creation of data visualization, by reconstructing the reference model. Generated visualizations can be made interactive once coupled with a program (such as a web browser) to let the user perform visual exploration and data analysis tasks. We also present a service composition interface, and applications to graph and word cloud visualization. Finally, we show how generated server-side logs allow the representation and evaluation of users' activity.
Designing a Voice-Activated Typewriter in French necessitates a study both on how to design the acoustic level recognition, and on how to obtain a model of the French language. Such a project was initiated at LIMSI 15 years ago. This paper presents the different steps that have been completed since the beginning of this project. First, a study on the phoneme-to-grapheme conversion, for continuous, error-free phonemic strings, using a large vocabulary and a natural language syntax was completed in 1979. The corresponding results were then improved, with attempts to convert phoneme strings containing (simulated) errors, while the methodology was adapted to the case of stenotype-to-grapheme conversion. In the ESPRIT project 860 “Linguistic Analysis of the European Languages”, our approach for language modeling was compared with other approaches on 7 different European languages. The link between the acoustic recognition and the language model resulted in a complete system (“Hamlet”), for a limited vocabulary (2,000 words), pronounced in isolation, which was then extended to a vocabulary of 5,000 words, taking advantage of a specialized DTW chip (MuPCD), also designed at LIMSI. This study resulted in the conclusion that dictation in an isolated mode was not acceptable. A speaker-independent continuous speech recognition system is now developed for vocabularies of 5 to 20 KWords.
Steganography has been known and used for a very long time, as a way to exchange information in an unnoticeable manner between parties, by embedding it in another, apparently innocuous, document. This usually leads to very high dimensional spaces for which many problems arise (in comparison to low dimensional spaces): mainly, the required number of images to have an appropriate filling of the space in which the classifier is trained, is never reached. In this article, some of the problems encountered because of the high dimensionality of the problem usually met in steganalysis, are presented, along with possible solutions. With this sufficient number of images, feature selection is then performed, with a forward algorithm, in an attempt to decrease the dimensionality and also to gain interpretability over which features have been reacting the most.
In this contribution, a new system for voice conversion is described. The proposed architecture combines a PSOLA (Pitch Synchronous Overlap and Add)-derived synthesizer and a module for spectral transformation. Prosodic modifications are applied on the excitation signal using the TD-PSOLA scheme; converted speech is then synthesized using the transformed spectral parameters. Two different approaches to derive spectral transformations, borrowed from the speech-recognition domain, are compared: Linear Multivariate Regression (LMR) and Dynamic Frequency Warping (DFW). Vector-quantization is carried out as a preliminary stage to render the spectral transformations dependent of the acoustical realization of sounds. A formal listening test shows that the synthesizer produces a satisfyingly natural “transformed” voice.
Hands-free interaction represents a key-point for increase of flexibility of present applications and for the development of new speech recognition applications, where the user cannot be encumbered by either hand-held or head-mounted microphones. When the microphone is far from the speaker, the transduced signal is affected by degradation of different nature, that is often unpredictable. Special microphones and multi-microphone acquisition systems represent a way of reducing some environmental noise effects. Robust processing and adaptation techniques can be further used in order to compensate for different kinds of variability that may be present in the recognizer input. The purpose of this paper is to re-visit some of the assumptions about the different sources of this variability and to discuss both on special transducer systems and on compensation/adaptation techniques that can be adopted. In particular, the paper will refer to the use of multi-microphone systems to overcome some undesired effects caused by room acoustics (e.g. reverberation) and by coherent/incoherent noise (e.g. competitive talkers, computer fans). The paper concludes with the description of some experiments that were conducted both on real and simulated speech data.
This paper examines the intonational characteristics of a number of types of non-word, e.g. numbers, dates, times and other abbreviations, which occur in text and are readily identifiable. Examples of such phenomena are presented, and heuristics for their treatment by an automatic system are proposed. A formal evaluation of these heuristics is presented, showing a success rate of over 94%. A final discussion outlines the advantages and disadvantages of such a treatment, and suggests lines of future research.
This article provides the editio princeps of a previously unknown maqāma attributed to Badīʿ al-Zamān al-Hamad̠ānī (d. 398/1008). It begins with a review of the scholarship on the manuscripts of Hamad̠ānī's Maqāmāt and discusses how the text of this lost maqāma was uniquely preserved in one manuscript, Yale University, Beinecke Library, Salisbury collection no. 63. This manuscript, copied in 603/1206, was well-known to European scholarship, having been in the possession of Everard Scheidius (1742-1794), Silvestre de Sacy (1775-1838), and Edward Eldridge Salisbury (1814-1901). The maqāma, preserved therein, describes a fraudulent doctor's sale of medicinal compounds allegedly composed of rare materia medica. The text of this maqāma, which the editors have entitled al-Maqāma l-Ṭibbiyya, is then provided in facsimile, a critical edition, and a fully-annotated English translation. A detailed analysis of the maqāma follows, in which the form, subject matter, language, and style of this maqāma are discussed in relation to the known corpus of Hamad̠ānī's other maqāmāt. The article concludes with several hypotheses about the possible authenticity of this lost work.
Formal Learning Theory may be conceived as a means of relating theories of comparative grammar to studies of linguistic development. After a brief review of relevant concepts, the present paper surveys formal results within Learning Theory that suggest correponding constraints on linguistic theory. Particular attention is devoted to the question: How many possible natural languages are there?
Synthetic aperture radar (SAR) is a microwave imagery system capable of producing high resolution images by processing properly data collected by a relatively small antenna. In this paper, the bi-dimensionnal received signal, using spatial coordinates, is formulated. The image quality is determined by that of the ambiguity function. This latter is analyzed and optimized for two performance criteria. First, for a matched filter receiver (maximal signal-to-noise ratio receiver), the optimal waveform is shown to be a non linear FM pulse which autocorrelation function is a Taylor. The optimal azmiuth weighting function is related to that of Taylor by a Fourier transform. Second, for a Wiener filter (least mean-squares receiver), we show that the optimal waveform is the first prolate spheroidal function. The single-hit measurement of the scattering matrix by mean of two optimal orthogonal waves is discussed.
Object extraction systems performances are not homogeneous over different corpora because objects can take many different aspects within such sets. An adaptation of these systems is thus required in order to maintain equal performances over every kind of object the system may be applied on. Focusing on the issue of parameters optimization, a method has been developed to restrict optimization to parameters of operators which compose the system, responsible for the different categories of errors produced by the system. Two stages are involved in our method. The first one is dedicated to the analysis of the system performances and leads to the extraction of the different error categories already mentionned. The second one relates to the analysis of the behavior of the different operators, leading to extract a single operator responsible for each error category. Experiments have been carried out over a video text detection system.
What is mirrored from such a methodological exercice is that the philological work surrounding an edition is only the beginning of any book's history… a mere yet indispensable pretext.
There is a strong consensus that the sounds and sound patterns of babbling and early speech are basically the same. The common state is one of “Frame Dominance” — a syllabic frame produced by an open-close mandibular oscillation dominates both stages, with limited ability of other articulators, including the tongue to produce active intrasyllabic and intersyllabic changes. The question of whether the first words are similar to babbling in all respects was evaluated in 4 subjects, using a database consisting of 152 hours of audio recording. Progress in words took the form of an increase in variegation of utterances, mainly due to vowel variegation, much of which derived from an increase in the use of high vowels and mid back yowels, especially in wordfinal position. The presence of regression and the limited nature of the progress were taken as evidence of the strength of the Frame Dominance pattern and the consequent difficulty of escaping from it.
A set of phonetic studies based on analysis of the TIMIT speech database is presented which addresses topics relevant to the linguistic and speech recognition communities. First, the advantages and shortcomings of using TIMIT for linguistic research are considered, and a database methodological approach is outlined. Next, several small studies are presented which detail new results on the effect of speakers' sex and dialect region on pronunciation. The goal of this paper is to use the database to explore sex and dialect related variation thereby ascertaining differences which may merit further experimental study. This report concerns speaker-dependent effects on certain phonetic characteristics often involved in reduction such as speech rate, stop releases, flapping, central vowels, laryngeal state, syllabic consonants, and palatalization processes. Specifically, it is suggested that the phonetic characteristics found more commonly with male speakers are also those typical of reduction in speech.
With the development and the availability of large textual corpora, appeared a need for structuring and organizing these corpora in a way that reflects some semantic relations between documents. For now, in Information Retrieval, these relations are indicated mainly via hyperlinks or by organizing documents into concept hierarchies, both being manually developed. We propose here an algorithm for automatically inferring concept and document hierarchies from a corpus. We also present numerical criteria for measuring the relevance of these automatically generated hierarchies and discuss some experiments performed on data from the Looksmart web site.
Our present work concerns Swedish prosody in a speech synthesis framework. Two main problem areas are examined: prominence and phrasing. In a model for Swedish prosody, prominence levels (stress, accent, focus) are represented as layered and multidimensional for different domains (syllable, foot, word). Phrasing involves both coherence in the form of specific combinations of existing accentual gestures and separate boundary gestures. The main features of the intonation model are given in outline. Experiments on prominence include modelling of durations in a combined speech data base and rule synthesis framework, where the stressed-unstressed alternation appears to be the most important duration factor. Other experimentation concerns typical differences in the timing characteristics of the tonal gesture for focal accent between compound words and simplex accent II words. Experiments on phrasing include both production data from a varied speech material as well as synthesis and perception. Our experiments demonstrate that both coherence and boundary cues are effective as phrasing signals and that a combination of F 0 and duration is typically used to signal phrasing. Our future plans include working with prosodic modelling of Swedish in a dialogue context and in a concept-to-speech framework.
In this paper we compare two different methods for automatically phonetically labeling a continuous speech database, as usually required for designing a speech recognition or speech synthesis system. Both systems have been evaluated on read utterances not part of the training set of the HMM systems, and compared to manual segmentation. This study outlines the advantages and drawbacks of both methods. The speech synthetic system has the great advantage that no training stage (hence no large labeled database) is needed, while HMM systems easily handle multiple phonetic transcriptions (phonetic lattice). The importance of such segmentation tools is a key point for the development of improved multilingual speech synthesis and recognition systems.
Vérard published his editio princeps of the prose Merlin in 1498, in three handsome folio volumes ; the third and final volume contains the Prophéties. This difficult, 'unreadable', text – a torrent of predictions relating largely to 12th- and 13th-century Italy – is presented in all the surviving manuscripts with a mise en page offering no assistance to a 'proper' reading: few paragraphs, no rubrics. The 1498 edition of the Prophéties, by contrast, is endowed with an elaborate and abundant programme of rubrics, which, it seems plausible, were the contribution of Vérard and his workshop. This, allied to the Table at the beginning of the volume, seems to constitute something like a primitive 'information retrieval system'.
We review in a common framework several algorithms that have been proposed recently, in order to improve the voice quality of a text-to-speech synthesis based on acoustical units concatenation (Charpentier and Moulines, 1988; Moulines and Charpentier, 1988; Hamon et al., 1989). These algorithms rely on a pitch-synchronous overlap-add (PSOLA) approach for modifying the speech prosody and concatenating speech waveforms. The modifications of the speech signal are performed either in the frequency domain (FD-PSOLA), using the Fast Fourier Transform, or directly in the time domain (TD-PSOLA), depending on the length of the window used in the synthesis process. We also discuss the different kinds of distortions involved in these different algorithms.
Research on proper names in French allows us to use the results of rhetoricians, grammarians and linguists for bilingual lexicographical processing of lexical units and phraseologisms containing proper names used figuratively in French and Macedonian. In this article, we try to distinguish relevant criteria allowing the lexicographer to select and list those units, put forward solutions for their processing and identify problems and characteristics.
We present a swarm intelligence algorithm that solves a discrete foraging problem. We describe simulations and provide a complete convergence analysis: we show that the population computes the solution of some optimal control problem and that its dynamics converges. We discuss the rate of convergence with respect to the number of agents: we give experimental and theoretical arguments that suggest that this convergence rate is superlinear with respect to the number of agents. Furthermore, we explain how this model can be extended to the case where the state space is continuous, and in order to solve optimal control problems in general. We argue that such an approach can be applied to any problem that involves the computation of the fixed point of a contraction mapping. This allows to design a large class of formally well understood swarm intelligence algorithms.
The time-domain harmonic-scaling (TDHS) algorithm provides a computationally efficient method (suitable for real-time implementation) for speech bandwith compression and expansion. Pitch estimation is an important operation in the TDHS process. In the present paper, we study a TDHS/sub-band coding system for speech operating at 16 kbits/s and investigate the relative effectiveness of five different pitch estimation methods (the autocorrelation method, the cepstrum method, the simplified inverse filtering technique, the average magnitude difference function method and the maximum likelihood method). A formal listening test using 17 human listeners is conducted for their comparative performance evaluation. The average magnitude difference function method was found to be the best pitch estimation method for TDHS/sub-band coding.
Word shadowing was used in order to obtain evidence concerning the relevance of the uniqueness point (UP, i.e., the moment at which the acoustic-phonetic information already presented remains compatible with a single lexical entry) as a determinant of the point where spoken word recognition occurs. Taken as an average, response latencies showed a minimal but significant effect of the UP. However, there were important differences between items with early and late UPs. A multiple regression analysis, taking UP position, word length and word frequency into account, showed that UP position is the best predictor of the shadowing latencies of early-UP items, but that it does not contribute at all to the shadowing of words with late UP. We conclude that the UP strongly mediates the recognition of spoken words with early UP. In addition, the shadowing of late-UP items is best predicted by word length in slower, and by word frequency in faster subjects; this suggests the intervention of different mechanisms.
Adaptation in games and serious games is an important feature that allows to individualize the game experience. It can also manage the players-learners ' frustration while increasing their motivation. This article presents the state of the art of works dealing with adaptation in games and serious games. These works are then described according to an evaluation framework that determines the scope, parameters, model of adaptation and whenever the game is single or multiplayer. The analysis of the state of the art shows that building adaptive multiplayer serious games raises many challenges such as managing multiple views without breaking the game design logics. This leads us to consider development of adaptive multiplayer serious games as an important challenge to be addressed.
This paper questions the phonetic bases of the theories dealing with segments' internal structure. It takes a look at the rise and the development of the idea that the phoneme is not the ultimate constituent of the speech stream which cannot be broken down into smaller units, but the association product of a finite inventory of universal parameters called differently by different frameworks: features, elements, gestures. The break with the “atomic” conception of the phoneme and the elaboration of the phonological primitives theories go, however, hand in hand with a complicated formalism and with the hermetization of phonology by its estrangement from phonetics.
The lexicon of a speech recognizer is supposed to contain pronunciation models describing how words can be realized as sequences of subword units (usually phonemes). In this contribution we present a method for upgrading initially simple pronunciation models to new models that can explain several pronunciation variants of each word. Since the presented strategy is capable of producing pronunciation variants and cross-word dependencies completely automatically, it is an attractive alternative to the manual encoding of multiple pronunciations in the lexicon. By learning pronunciation rules rather than pronunciation variants from the data, one can combine the advantages of data-driven and rule-based approaches. Important properties of the proposed methodology are that it incorporates dependencies between the rules from the very beginning (during the training), that it supports exception rules not producing pronunciation variants but affecting the production of such variants by other rules (called production rules), and that it has a sound probabilistic basis for the attachment of likelihoods to the word pronunciation variants. Experiments showed that the introduction of such variants in a segment-based recognizer significantly improves the recognition accuracy: on timit a relative word error rate reduction of as high as 17% was obtained.
This paper presents the results of a statistical and deterministic analysis of two phonemic lexicons, with respect to the storage and generation of spelling rules using graphemes. The aim of this paper is to demonstrate the feasibility of generating correctly spelled words for the English language using phoneme-to-grapheme rules. An algorithm for generating the rules is presented. A set of spelling rules were identified by the analysis of two differently sized lexicons, 96, 939 words and 11, 638 words, the smaller lexicon being a subset of the larger. These rules were then tested for their general usability. 62.3% of all words in the 96, 939 word lexicon could be spelled correctly utilising rules alone. A smaller lexicon which consisted of many of the more frequently occurring words plus a selection of less common words showed that 84.5% of this lexicon could be spelled correctly using rules generated by the analysis of its own lexicon. However, only 62.3% of this dictionary could be spelled correctly using rules generated from the lexicon of 96, 939 words. It was also shown that phoneme-to-grapheme mappings are between 63% and 69% alphabetic, depending on the size of dictionary used. 59 general default rules were identified, unfortunately only 22.6% of the smaller dictionary could be spelled correctly by using these rules.
Including speech knowledge in automatic speech recognition (ASR) systems is a good way to improve the performance of recognizers. In this paper, we propose the orion system which deals with speaker-independent ASR for isolated-words. orion is a two-pass hybrid system which uses several types of knowledge. This knowledge applies to psychoacoustics, physiology and phonetics. During the first pass, an auditory model, the perceptually-based linear prediction analysis (PLP), combines static and dynamic features to provide a set of parameters to the dynamic programming algorithm. After this stage 98% recognition accuracy was obtained for a digit vocabulary and 12 templates per word. In the case of a confusable vocabulary (E-SET), the introduction of phonetic knowledge in the second pass decreases the error rate by more than 60% (compared to the results of the first pass).
This paper reports on activites at LIMSI over the last few years directed at the transcription of broadcast news data. We describe our development work in moving from laboratory read speech data to real-world or `found' speech data in preparation for the DARPA evaluations on this task from 1996 to 1999. Two main problems needed to be addressed to deal with the continuous flow of inhomogenous data. These concern the varied acoustic nature of the signal (signal quality, environmental and transmission noise, music) and different linguistic styles (prepared and spontaneous speech on a wide range of topics, spoken by a large variety of speakers). The problem of partitioning the continuous stream of data is addressed using an iterative segmentation and clustering algorithm with Gaussian mixtures. The speech recognizer makes use of continuous density HMMs with Gaussian mixture for acoustic modeling and 4-gram statistics estimated on large text corpora. Word recognition is performed in multiple passes, where current hypotheses are used for cluster-based acoustic model adaptation prior to the next decoding pass. The overall word transcription error of the LIMSI evaluation systems were 27.1% (Nov96, partitioned test data), 18.3% (Nov97, unpartitioned data), 13.6% (Nov98, unpartitioned data) and 17.1% (Fall99, unpartitioned data with computation time under 10× real-time).
The design ofspatial coordination mechanisms for dynamical and continuous multiagent setting is a difficult challenge. While the top-down decomposition approach is inefficient on such problems, the bottom-up approach is more promising, but requires a tedious manual parameter tuning which raises scaling-up issues. Our own approach consists in replacing the manual tuning by a specially designed multicriteria evolutionary algorithm devoted to the tuning of our spatial coordination formalism. In this paper, through a quantitative comparison on a complex spatial coordination problem treated previously by Balch and Hybinette, we show that our system, GACS, finds a population of solutions as efficient as this predecessor though our approach requires less involvement from the designers and can find simpler solutions.
We describe a procedure for acquiring intonational phrasing rules for text-to-speech synthesis automatically, from annotated text, and some evaluation of this procedure for English and Spanish. Rules generated by this method have been implemented in the English version of the Bell Laboratories Text-to-Speech System and have been developed for the Mexican Spanish version of that system. These rules currently achieve better than 95% accuracy for English and better than 94% for Spanish.
This paper describes the design principles of a large vocabulary, free text, dictation system, employing advanced speech recognition technology. It can be used for a wide variety of tasks, and is rapidly customisable to new domains. A case study of the application of the technology in the creation of a Pathology reporting workstation is described. The objectives of using large vocabulary speech recognition technology to implement a system for the dictation of free text are described. The process of creating text, from draft to final version is discussed in terms of the pre-requisites on both the technology and its implementation. The constraints on the performance and user acceptability of a system using this technology are noted. The design of a flexible system component supporting user adaptation is shown. The use of a general purpose, standalone dictation system for a wide variety of potential applications is noted. The customisation of the system to a new task by the rapid rebuilding of its parameter tables is described. The application of a customised version of the system to an actual end user environment is exemplified by a case study of a Pathology reporting workstation.
In this paper, we describe sphinx, the world's first accurate large-vocabulary speaker-independent continuous speech recognition system. We will present current results of sphinx, compare its performance against similar systems, and account for its high accuracy.
A microprocessor system has been designed to convert ordinary French text into audible speech in real-time. The highly-modular Pascal software translates an input text into phonemes, and assigns duration and pitch via a simple syntactic analysis. The speech is generated at 10 000 samples/sec with a programmable DSP integrated circuit. Of two printed circuit cards in the system, one contains an Intel 8086 microprocessor and memory, and the other has the DSP chip and an associated interface, D/A converter and amplifier. An earlier version of the system, for a VAX-11/780 entirely in software with floating-point arithmetic, required eight times real-time calculation. The current real-time system shows the practicability of generating high quality French synthetic speech on a printed circuit board.
Interspeaker variability is a major source of errors in automatic speech recognition. This paper describes a series of experiments, conducted at TELECOM Paris by the «Pattern Recognition and Speech Processing» Group, for controlling some aspects of this variability, thus allowing for the adaptation of speech recognition systems to new users. The first experiments are based on a linear data analysis technique: multiple linear regression (MLR). The second set uses multilayer perceptrons, and yields slightly better results, because non linear phenomena are taken into account, versus 15 % with the first one. Those techniques can also be used for the adaptation of recognizers to new acoustical environments and recording conditions.
In this paper we report on a series of user trials carried out to assess the performance and usability of the Multimodal Multimedia Service Kiosk (Mask) prototype. The aim of the Esprit Mask project was to pave the way for advanced public service applications with user interfaces employing multimodal, multimedia input and output. The prototype kiosk was developed after analyzing the technological requirements in the context of users performing travel enquiry tasks, in close collaboration with the French Railways (SNCF) and the Ergonomics group at the University College of London (UCL). In addition to meeting or exceeding the performance goals set at the project onset in terms of success rate, transaction time, and user satisfaction, the Mask kiosk was judged to be user-friendly and simple to use.
Two new methods are presented here for the detection of the glottal closure instant from the speech waveform. Both detect abrupt changes in the short-term spectral characteristics of the speech signal within a pitch period caused by glottal events. The same statistical approach to the sequential detection of events by hypothesis testing is used for this purpose. The first method is based on the maximization of the likelihood ratio, while the second uses a divergence convexity test. Experiments on real speech data demonstrate the robust of these methods.
In this paper, we compare two alternative approaches for speaker verification based on hidden Markov model (HMM) technology: single Gaussian HMMs and different types of tied multi-Gaussian HMMs. In order to assess the performance under real-world constraints, we tested each system using a database of connected digit strings recorded over local and long-distance telephone lines. According to our experiments, tied-mixture models were able to perform better than the single Gaussian approach provided that sufficient training data were available. However, our experiments indicate that the single Gaussian HMM approach is to be preferred for real-world speaker verification when only limited amounts of training data are available. Results will be discussed for both text-dependent and text-independent speaker verification.
In pursuance of better performance, current speech recognition systems tend to use more and more complicated models for both the acoustic and the language component. Cross-word context dependent (CD) phone models and long-span statistical language models (LMs) are now widely used. In this paper, we present a memory-efficient search topology that enables the use of such detailed acoustic and language models in a one pass time-synchronous recognition system. Characteristic of our approach is (1) the decoupling of the two basic knowledge sources, namely pronunciation information and LM information, and (2) the representation of pronunciation information – the lexicon in terms of CD units – by means of a compact static network. The LM information is incorporated into the search at run-time by means of a slightly modified token-passing algorithm. The decoupling of the LM and lexicon allows great flexibility in the choice of LMs, while the static lexicon representation avoids the cost of dynamic tree expansion and facilitates the integration of additional pronunciation information such as assimilation rules. Moreover, the network representation results in a compact structure when words have various pronunciations, and due to its construction, it offers partial LM forwarding at no extra cost.
The voice source is an important factor in the production of different voice qualities. These different voice qualities are used in speech to convey, among other things, different suprasegmental aspects, e.g., emphasis, phrase boundaries and also different speaking styles such as an authorititave or a submissive voice. Voice source variations are also an important means of conveying extralinguistic information of various kinds in ordinary speech. In the present study, voice source variations in normal speech by female speakers have been investigated using inverse filtering. The results of the inverse filtering are given in voice source parameters appropriate for controlling speech synthesis. Accordingly, the resulting descriptions have been utilized to produce voice variations in our new synthesis system.
This paper outlines four novel methods for the task of speaker verification. The first model, a Hybrid Multi-Layer Perception (MLP)-Radial Basis Function (RBF) model, is an MLP predictor whose weights are then used as inputs to an RBF classifier for the verification process. The second model uses an array of linear predictors to model the true speaker where each predictor is associated with a particular sub-unit of the test utterance. The third, a Neural Prediction Model, consists of an array of MLP predictors and the fourth, a Hidden Control Neural Network, is a single MLP predictor with added control inputs. These control inputs modulate the MLP mapping and allow a single MLP to model a complete utterance. Each method was trained and tested on a modest database and each performs well with verification rates of 100% for the first three models and of 90% for the Hidden Control Neural Network.
A cross-cultural study of Japanese and American children has examined the development of awareness about syllables and phonemes. Using counting tests and deletion tests, Experiments I and III reveal that in contrast to first graders in America, most of whom tend to be aware of both syllables and phonemes, almost all first graders in Japan are aware of mora (phonological units roughly equivalent to syllables) but relatively few are aware of phonemes. This difference in phonological awareness may be attributed to the fact that Japanese first graders learn to read a syllabary whereas American first graders learn to read an alphabet. For most children at this age, awareness of phonemes may require experience with alphabetic transcription, whereas awareness of syllables may be facilitated by experience with a syllabary, but less dependent upon it. To further clarify the role of knowledge of an alphabet in children's awareness of phonemes, Experiments II and IV administered the same counting and deletion tests to Japanese children in the later elementary grades. Here the data reveal that many Japanese children become aware of phonemes by age whether or not they have received instruction in alphabetic transcription. Discussion of these results focuses on some of the other factors that may promote phonological awareness.
A systematic study on a speaker-independent vowel recognition model has been performed. Karhunen-Loève Transformation (KLT), or Principal Component Analysis, technique was applied subsequent to a spectral analysis of the speech signal by 18 non-overlapping critical-band filters. Four experiments have been conducted using selected segments of 8 isolated Putonghua (Mandarin) vowels, spoken twice in 5 tones by 38 females and 13 males. The first experiment uses the same speech sample in training and testing to evaluate the effects of KLT, speaker normalization, distance metric and number of vowel classes. A modified Mahalanobis distance coupled with a 7-class condition was found to give the best performance. In the next experiment, one sample was used to train the model, and another trial of the same speech, spoken by the same group of speakers, was used to test it. It was found that, in general, a sex-specific and tone-specific procedure could be avoided without significant loss in performance. The third experiment repeatedly trained the model with 50 speakers and tested it wiht the remaining one until all 51 speakers had been tested. Under this stringent condition, an average recognition rate of 88.2% was achieved using only 4 classificatory dimensions. In the last experiment, all segments of a vowel were labelled using the most stringent conditions. The model was confirmed to perform well for one male and one female speaker selected at random. Also, the vowel that had caused the greatest confusion was found to be well recognized when treated as an allophone of another vowel. Finally, the possibility of extending the present technique to diphthong recognition is discussed together with some preliminary results.
English and Italian provide some interesting contrasts that are relevant to a controversial problem in psycholinguistics: the boundary between grammatical and extra-grammatical knowledge in sentence processing. Although both are SVO word order languages without case inflections to indicate basic grammatical relations, Italian permits far more variation in word order for pragmatic purposes. Hence Italians must rely more than English listeners on factors other than word order. In this experiment, Italian and English adults were asked to interpret 81 simple sentences varying word order, animacy contrasts between the two nouns, topicalization and contrastive stress. Italians relied primarily on semantic strategies while the English listeners relied on word order—including a tendency to interpret the second noun as subject in non-canonical word orders (corresponding to word order variations in informal English production). Italians also made greater use of topic and stress information. Finally, Italians were much slower and less consistent in the application of word order strategies even for reversible NVN sentences where there was no conflict between order and semantics. This suggests that Italian is 'less' of an SVO language than English. Semantic strategies apparently stand at the 'core' of Italian to the same extent that word order stands at the 'core' of English. It is suggested that these results pose problems for claims about a 'universal' separation between semantics and syntax, and for theories that postulate a 'universal' priority of one type of information over another. Results are discussed in the light of the competition model, a functionalist approach to grammar that accounts in a principled way for probabilistic outcomes and differential 'weights' among competing and converging sources of information in sentence processing.
A real-time speech synthesis system for unrestricted German text is described. This system is based on the concatenation of stored single-sound and diphone transition elements. Input occurs (presently) in phonetic text with pitch information. The output signal is generated by a computer-controlled log-area-ratio (LAR) vocoder. The stationary sounds are coded by single frames of vocoder parameters and the transitions by frame pairs, taken from real speech with slight modifications. Intermediate frames are interpolated during synthesis. The preliminary experiments concerning interpolation and the choice of frames to be stored are described together with the synthesis procedure (two variants) including table structure and treatment of pitch and sound duration. Intelligibility measurements and quality comparisons of both variants have been carried out.
Research was conducted to determine if alterations in the acoustical characteristics of voice occur over periods of sustained operations. Twelve male United States Air Force B-1B bomber aircrewmen participated in the study. The participants served in crews of four and performed three 36-hour experimental periods (missions) in a high-fidelity simulator. The missions were interspersed with 36-hour rest breaks. Data were lost from two members of the third team due to a communication malfunction. Speech, cognitive and subjective fatigue data were collected approximately every three hours for 11 trials per mission. Fundamental frequency and word duration were both found to vary significantly over trials (fundamental frequency F(10,90) = 2.63, p = 0.0076, word duration F(10,90) = 2.5, p = 0.0106). Speech duration results also showed a significant main effect of mission (F(2,18) = 6.91, p = 0.0082). The speech data follow the same trend as the data from the cognitive tests and subjective measures. A strong diurnal pattern is reflected in nearly all of the dependent measures. Overall, the results support the proposition that voice may be a valid indicator of a speaker's fatigue state.
The syntactic and phonotactic structure of the sentences are systematically varied in order to understand how two functions can be carried out in parallel in the prosodic continuum: (1) enunciative: demarcation of constituents; (2) illocutory: speaker's attitude. The statistical analysis of the corpus demonstrates that global prototypical prosodic contours characterise each attitude. Such a global encoding is consistent with gating experiments showing that attitudes can be discriminated very early in utterances. These results are discussed in relation to a morphological and superpositional model of intonation. This model proposes that the information specific to each linguistic level (structure, hierarchy of constituents, semantic and pragmatic attributes) is encoded via superposed multiparametric contours. An implementation of this model is described that automatically captures and generates these prototypical prosodic contours. This implementation consists of parallel Recurrent Neural Networks each responsible for the encoding of one linguistic level. The identification rates of attitudes for both training and test synthetic utterances are similar to those for natural stimuli. We conclude that the study of discourse-level linguistic attributes such as prosodic attitudes is a valuable paradigm for comparing intonation models.
We attempted multi-talker, connected recognition of the spoken American English letter names b, d, e and v, using a recurrent neural network as the speech recognizer. Network training was based on forward-propagation of unit potentials, instead of back-propagation of unit errors in time. The target function was based on an input speech parameter which turns on and off at each onset of a spoken letter name. The network was trained to copy that input speech parameter to the output unit assigned to the correct letter name. Letter name discrimination was as high as 85% on test utterances.
Definitional accounts of language structure are explored in this paper. Several classes of arguments for definitions are reviewed; those which connect to: classical theories of reference, theories of informal validity, theories of sentence comprehension, and theories of concept learning. We suggest that, for each of these areas, accounts which rely upon definition are, in fact, not to be preferred on evidential grounds to plausible non-definitional alternatives. We also present a series of experimental observations bearing on one of these areas — that of sentence comprehension. Such subject judgements are independently demonstrated to be sensitive to structural relations of comparable type for other linguistically non-problematic types.
The presence ofa reduced visibility distance on a road network (thick fog, heavy rain, etc.) affects its safety. We designed a roadside system on which aims to detect critical situations such as dense fog or heavy rain with a simple CCTV camera. Different image processing are presented, particularly the estimation of visibility distance, the detection of fog, and the detection of rain. Based on the principles underlying these algorithms, a camera is specified to meet the needs expressed by the standard NF P 99-320 on highway meteorology. Experimental results are presented as well as prospective validation at a bigger scale.
A new glottal wave analysis method, Pitch Synchronous Iterative Adaptive Inverse Filtering (PSIAIF) is presented. The algorithm is based on a previously developed method, Iterative Adaptive Inverse Filtering (IAIF). In the IAIF-method the glottal contribution to the speech spectrum is first estimated with an iterative structure. In the new PSIAIF-method the glottal pulseform is computed by applying the IAIF-algorithm twice to the same signal. The first IAIF-analysis gives as a result a glottal excitation that spans over several pitch periods. This pulseform is used in order to determine positions and lengths of frames for the pitch synchronous analysis. The final result is obtained by analysing the original speech signal with the IAIF-algorithm one fundamental period at a time. The PSIAIF-algorithm was applied in glottal wave analysis using both synthetic and natural vowels. The results show that the method is able to give a fairly accurate estimate for the glottal flow excluding the analysis of vowels with a low first formant that are produced with a pressed phonation type.
This paper presents an analysis of a corpus of grammars written for learning French in England from 1660 to 1820, a period sometimes referred to euphemistically as the “long century” which saw language teaching evolve in response to broader social and epistemological developments, namely the increased codification of vernacular grammar against a backdrop of scientific rationalism and, in England, the greater institutionalisation of school-based pedagogies. The aim of the analysis is twofold: firstly, to identify some key shifts in the formulation of content, specifically changes in overall structure and distribution of sections, including differences in grammatical nomenclature, and, secondly, to contextualise these developments by considering the changing role of the grammarian-teachers as demonstrated in the way they position themselves as authors to different publics.
This paper describes the application of a one-stage Dynamic Programming (DP) algorithm to the acoustic-phonetic decoding stage in a speech recognition system. Essentially two methods are compared: (1) the recognition of demisyllable. and (2) the recognition of consonant cluster and syllabic nuclei (vowels or diphthongs). Demisyllables, consonant clusters and vowels or diphthongs have proved their usefulness in the recognition of continuous speech. Furthermore, the method of Dynamic Programming is a well established principle for the recognition of connected words. In this paper a stage of acoutic-phonetic decoding is presented which applies Dynamic Programming to these phonetic units. The algorithm used is essentially the same as the one commonly known for connected word recognition, which does not need explicit segmentation. However, for the recognition of demisyllables, consonant clusters and vowels, some specific modifications were necessary, including the introduction of a special internal syntax between the units. The derived algorithm was tested using fluently spoken sentences from 75 word lexicon in which the most frequent consonant clusters and vowels of the German language were represented. Different tests were made in order to get a comparison of the recognition accuracy using a context-dependent application (concerning neighbouring units) and a context-independent application. These results and a comparison to a similar acoustic-phonetic stage using an explicit segmentation are presented.
Participants in a discourse sometimes fail to understand one another, but, when aware of the problem, collaborate upon or negotiate the meaning of a problematic utterance. To address non-understanding, we have developed two plan-based models of collaboration in identifying the correct referent of a description: one covers situations where both conversants know of the referent, and the other covers situations, such as direction-giving, where the recipient does not. In the models, conversants use the mechanisms of refashioning, suggestion and elaboration, to collaboratively refine a referring expression until it is successful. To address misunderstanding, we have developed a model that combines intentional and social accounts of discourse to support the negotiation of meaning. The approach extends intentional accounts by using expectations deriving from social conventions in order to guide interpretation. Reflecting the inherent symmetry of the negotiation of meaning, all our models can act as both speaker and hearer, and can play both the role of the conversant who is not understood or misunderstood and the role of the conversant who fails to understand.
Including speech knowledge in automatic speech recognition (ASR) systems is a good way to improve the performance of recognizers. In this paper, we propose the ORION system which deals with speaker-independent ASR for isolated-words. During the first pass an auditory model, PLP (perceptually-based linear prediction analysis) combines static and dynamic features to provide a set of parameters to the dynamic programming algorithm. After this stage 98 % recognition accuracy was obtained for a digit vocabulary and 12 templates per word. The introduction of phonetic knowledge in the second pass decreases the error rate by more than 60 % (compared to the results of the first pass) for a confusable vocabulary (E-SET).
Experience with the use of the X-ray microbeam system has confirmed that a substantial reduction in X-ray dosage can be achieved. The movements of 6 pellets on the tongue and teeth are tracked at a rate of more than 100 frames per second, with an effective exposure area of about 1 cm2 per frame, and with an exposure rate of 120 mR per minute in this area. Approximation of the movements of the pellets on the tongue, jaw and velum by means of the step response of a linear second order system has revealed that there are considerable differences in the values of the time constant among these speech organs. The duration of the step command also varies according to the type of vowel. These differences are reflected on the undershoot pattern and on the pattern of the coarticulation between consonant and vowel. The investigation of the movements of the velum has showed that a simultaneous observation of EMG is important for interpreting the pattern of the underlying motor control.
The current article discusses the problem of appropriate intonation selection in Person-Machine dialogues, such as those expected in intelligent information systems when, for example, information retrieval is required. An approach is proposed which integrates the previously mostly separate paradigms of automatic natural language generation and speech synthesis in a Person-Machine dialogue scenario. The paper argues that such an approach removes some of the well-known gaps in both text-to-speech and concept-to-speech systems.
In the current context of a growing internationalization of scientific exchanges, the issue of the language of scientific publications – settled in the natural sciences since the 1980s – has now become a central issue for the Social Sciences. Our paper discusses this topical issue through a detailed analysis of the linguistic strategies adopted by two major French social science journals, Population and Revue française de sociologie (RFS), which have chosen to translate into English a selection (RFS) or all (Population) of their articles. In view of the measured effects in terms of visibility in the international scientific field – increasing visibility for the journal Population at the expense of its French edition and a marginal effect for the RFS – we raise questions about the role of national social sciences journals and recall that the specific intellectual mission of these journals lies beyond the pursuit of internationalization.
In this article we review strategies used in the design of two-folded rejection-based classifiers. Beside the so-called classical “accept-first” strategy we have recently proposed very general families built on two different approaches, namely the “reject-first” [Fré98a, MF01b] and “mixture-first” [SFM02] reject schemes. These three approaches differ by the kind, as well as the order, of the tests leading to the classifier final output. While the first one starts by testing for distance rejection and, if necessary, finishes by testing for exclusive classification or ambiguity rejection respectively, the two others start respectively by testing for exclusive classification and ambiguity rejection, and then finish by the remaining alternatives. We unify the three schemes by defining fuzzy operators built on De Morgan operators (t-norms, t-conorms, complement). Behaviours of such different classifiers are illustrated on artificially generated examples.
When the reference speakers are represented by Gaussian mixture model (GMM), the conventional approach is to accumulate the frame likelihoods over the whole test utterance and compare the results as in speaker identification or apply a threshold as in speaker verification. In this paper we describe a method, where frame likelihoods are transformed into new scores according to some non-linear function prior to their accumulation. We have studied two families of such functions. First one, actually, performs likelihood normalization – a technique widely used in speaker verification, but applied here at frame level. The second kind of functions transforms the likelihoods into weights according to some criterion. We call this transformation weighting models rank (WMR). Both kinds of transformations require frame likelihoods from all (or subset of all) reference models to be available. For this, every frame of the test utterance is input to the required reference models in parallel and then the likelihood transformation is applied. The new scores are further accumulated over the whole test utterance in order to obtain an utterance level score for a given speaker model. We have found out that the normalization of these utterance scores also has the effect for speaker verification.
The study of synthesizing Chinese is faced with the pressing task of improving sound quality. This article presents the structure and features of our synthesis system for Standard Chinese. This system was built on the basis of an acoustic-phonetic analysis of Chinese syllables. Several original models and rules are employed in the system. All the 1268 syllables in Standard Chinese have been synthesized by this system, which produces a sound quality close to that of natural speech with respect to both intelligibility and naturalness.
This paper describes the results of a joint project funded by two French research laboratories, LIMSI-CNRS and INSERM-CREARE, and one end-user organisation, INJA (National Institute for Young Blind People). This project applies multimodal interfaces including speech recognition and synthesis to provide improved computer access for the blind. A multimodal text editor designed to provide enriched texts, direct manipulation and immediate feedback for text editing tasks is described. Promising results are presented, but combining speech with other modalities in the same interface also reveals some new technological problems that are hidden when speech is used in an isolated way. These problems are discussed, and user needs and expectations are presented.
It has been developpedfor different applications which are also described.
An innovative method of coding Line Spectrum Pair (LSP) parameters for transmission over noisy channels is presented. Typically, low bit-rate speech coders use these parameters to convey perceptually important spectral information. Thus it is necessary that these parameters are not only efficiently quantized but preserved during transmission. The scheme uses a joint source and channel coding technique applied to a concatenated trellis structure. Operating as a source coder, the system encodes below the 1 dB spectral distortion limit. It is shown that by modifying the encoder cost function to include the expected channel distortion, the code exhibits improved robustness to channel errors. This is accomplished with minimal increase in complexity and without increase in bit-rate. It is noted that the scheme performs well over a wide range of channel bit error rates and compares favourably with a standard scalar LSP quantizer tandemed with a channel coder both in terms of bit-rate and channel noise immunity.
In traditional accounts on speech prosody, fundamental frequency, duration and intensity have been described as the most important attributes. Among these, intensity has attracted the least attention. In perceptual studies both F 0 and duration have had an undisputable role in signalling prosodic categories, but the role of intensity has been less clear. This has resulted in an emphasis on the former attributes in current speech synthesis schemes. We are in this study exploring the use of speech intensity and also other segmental correlates of prosody. Intensity has a dynamic aspect, discriminating emphasized and reduced stretches of speech. A more global aspect of intensity must be controlled when we try to model different speaking styles. Specifically, we have been trying to model the continuum from soft to loud speech.
Many groups have investigated the relationship of word error rate and perplexity of language models. This issue is of central interest because perplexity optimization can be done independent of a recognizer and in most cases it is possible to find simple perplexity optimization procedures. Moreover, many tasks in language model training such as the optimization of word classes may use perplexity as target function resulting in explicit optimization formulas which are not available if error rates are used as target. This paper first presents some theoretical arguments for a close relationship between perplexity and word error rate. Thereafter the notion of uncertainty of a measurement is introduced and is then used to test the hypothesis that word error rate and perplexity are correlated by a power law. There is no evidence to reject this hypothesis.
Derivatives in -(cu)-lus, -(cu)-la, -(cu)-lum in technical Latin texts are difficult to translate. In some cases, the suffix is used as a diminutive: the object is smaller than the object without a suffix. In other cases, it is used to denote differences in time or in the thematic environment: both are used to denote the same object in the same context but at different times or they denote two different (or similar) objects in different contexts without regard to size. Rare examples of doublets used by technical authors in the same chapter, same paragraph and even in the same sentence to describe an object by its name or by its derivative are analysed in this article.
In this article, we describe an automatic system for train timetable information over the telephone that provides accurate connections between 1200 German cities. The caller can talk to it in unrestricted, natural, and fluent speech, very much like he or she would communicate with a human operator, and is not given any instructions in advance. The system's four main components, speech recognition, speech understanding, dialogue control, and speech output, are separated into independent modules that are executed sequentially. In an ongoing field trial, this system has been made available to the general public, both to gather speech data and to evaluate its performance. This field test was organized as a bootstrapping process: initially, the system was trained with just the developers' voices, then the telephone number was passed around within the department, the company and, finally, the outside world. After each step, the newly collected material was used for retraining, as well as for general improvements.
Several computational methods based on mathematical tools already exist, but most of the time their implementation is complex and takes longer execution time. In this article we propose another learning and anticipation method intended to user assistance in dynamic situations. To do so, manipulated knowledge is especially structured to limit our solution's complexity and to facilitate learning and anticipation.
A rich variety of factors have been proposed as possible determinants of differences in the ease of processing of relative clauses. These determinants include the grammatical role of the head, the shape of surface order configurations the occurence of interruptions of the main clause, and the presence or absence of morphological cues. The strict SVO word order of English makes it so that subject-modifying relatives necessarily interrupt the main clause, thus confounding the effects of role and interruption determinants. Hungarian, with its variable word order, allows us to achieve a somewhat better understanding of the separate effects of roles, configurations, interruptions, and morphological cues. A study using 144 different restrictive relative clause patterns in Hungarian provided evidence for the importance of three determinants of relative clause processing. First, the importance of perspective maintenance was indicated by the fact that SS sentences were the easiest to process and that SO were the most difficult. Second, the extreme difficulty subjects had in processing NNV sentences with a relative clause modifying the second noun indicated the importance of limits on fragment construction of chunks in a bottom-up parsing process. The use of antecedent tagging to mark extraposed relatives in SOV languages with variable order such as Hungarian and Georgian also indicated the importance of limits on fragment construction. Third, the conflict between focusing in the relative clause and focusing in the main clause indicated the importance of focus maintenance. A variety of other proposed determinants were found to be of little importance in accounting for the processing of relative clauses in Hungarian.
High quality low-delay speech coding at 8–16 kbit/s can be obtained with backward adaptive analysis-by-synthesisalgorithms, such as Low-Delay CELP (the new CCITT 16 kbit/s standard), Low-Delay Vector Excitation Coding (LD-VXC), and backward adaptive tree/trellis codecs. The paper examines and reviews some of the basic techniques underlying low delay coding algorithms and presents design and performance trade-offs for low-delay analysis-by-synthesis codecs at rates of 8–16 kbit/s. A number of approaches for improving the speech quality at 8 kbit/s are discussed. Backward pitch prediction is compared to a closed-loop forward configuration similar to that used in conventional CELP codecs for the adaptive codebook. Finally, robustness to transmission errors is discussed and a number of trade-offs for reducing transmission error sensitivity are presented.
A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes. According to the revised theory, phonetic information is perceived in a biologically distinct system, a 'module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories. In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions. Thus, it is comparable to such other modules as the one that enables an animal to localize sound. Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations.
The main purpose of this paper is to present recent results of urban studies about the first urban révolution: two case studies are particularly developed, Uruk and Mari. Building analysis and landscape analysis are the two principles tools used in tis study, based on old and latest field results.
Stress induced by various types of situation leads to vocal signal modifications. Previous studies have indicated that stressed speech is associated with a higher fundamental frequency and noticeable changes in vowel spectrum. This paper presents pitch- and spectral-based analyses of stressed speech corpora drawn from both artificial and real situations. The laboratory corpus is obtained by means of the Stroop test, the real-case corpus is extracted from the Cockpit Voice Recording of a crashed aeroplane. Analyses relative to pitch are presented and an index of microprosodic variation, μ, is introduced. Spectrum-related indicators of stress are issued from a cumulative histogram of sound level and from statistical analyses of formant frequencies. Distances to the F1-F2-F3 centre are also investigated. All these variations, throughout the two different situations, show the direct link between some new vocal parameters and stress appearances. The results confirm the validity of laboratory experiments on stress, but emphazise quantitative as well as qualitative differences between the situations and the speakers involved.
Both isolated phonemes and continuous speech experiments are presented.
Rennes CITH has a double objective innovative and evaluative functions of health technologies. More specifically, multivariate monitoring system and active prosthesis allowing the exploration, the evaluation and the treatment of the cardiovascular, nervous and respiratory functions. This paper describes briefly its origin, the partnerships and some technological activities developed since 2001.
The aim of this paper is to derive the spectral density of signals with periodically missing data, and to establish their model when they are generated from ARMA processes.
Recognition performance decreases when recognition systems are used over the telephone network, especially wireless network and noisy environments. It appears that non-efficient speech/non-speech detection (SND) is an important source of this degradation. Therefore, speech detection robustness to noise is a challenging problem to be examined, in order to improve recognition performance for the very noisy communications. Several studies were conducted aiming to improve the robustness of SND used for speech recognition in adverse conditions. The present paper proposes some solutions aiming to improve SND in wireless environment. Speech enhancement prior detection is considered. Then, two versions of SND algorithm, based on statistical criteria, are proposed and compared. Finally, a post-detection technique is introduced in order to reject the wrongly detected noise segments.
The Two Stage Fuzzy Decision Classifier (TSFDC) consists of an Artificial Neural Network (ANN) providing a first stage of discrimination and a second post-processing stage. The latter stage uses reference fuzzy set information, for each class of data considered. The ANN isolates the two most likely classes for each test vector. Post-processing selects the final class from amongst the two. The TSFDC applies post-processing only to those classes which its ANN has difficulty recognising. Three text-independent Automatic Speaker Identification (ASI) experiments are conducted with emphasis on forensic needs. In these experiments, the signal is degraded by a range of factors affecting communication channels. The TSFDC increases the percentage of correctly identified speech frames, for those speakers poorly identified by its ANN, by a mean of 3.27% over the three experiments. Concurrently, the difference in number of identified frames between true and corresponding runner-up speakers improves by a mean of 5.27%. Post-processing better than halves the number of speakers misclassified by the ANN.
Many symbols in music scores are linear segments. In this context, we designed an extractor of segments, which modelizes the characteristics of binary images (scale factor, curvature, bias and noises). We were able to recognize music symbols (staves, stems, slurs, beams, bar lines, black note heads, quavers and note groups) by applying both this extractor and simple rules of classification for the detected segments, to the defined layers.
In this paper we suggest an improved algorithm for filtering multiplicative noise. We opted for a simple method to evaluate local statistics and we supposed that the local distribution of the original image was uniform in order to express our Jack of knowledge about the original probability function.
Predictive web usage modeling has undergone an intense period of investigation until the late 90's. However, two features of web browsing have rarely been taken into account: the presence of noise and parallel browsing. In this paper, we propose a new model, the SBR model (Skipping-Based Recommender) which uses a technique called skipping, and is able to take into account these features. In a series of experimental studies, we put forward the various contributions that this model possesses and show that its quality surpasses that of the state-of-the-art.
In spite of superficial similarities, the nominal sentence (jumla ismiyya) according to the Arabic grammatical tradition has little if anything to do with what is usually called by this name in general linguistics since Meillet (1906). It is not characterized by the lack of any verb or copula, but by a topic + predicate (mubtadaˀ + ḫabar) structure, and covers a substantial set of facts, while explaining them in a self-consistent way. This paper discusses how these facts are presented and analyzed in a set of Arabic grammars written in Europe from the 17th century to the present day, raising the issue of the inter-translatability of linguistic categories from one tradition to another.
This article presents an approach to Web Engineering which aims to account for context-awareness in a comprehensive and integrated fashion, thus enabling an enhanced adaptation of the application to the end-user. A conceptual model, permitting the combination of a domain ontology with context-relevant parameters and a degree of relevance, is presented. Subsequently, the use of such a model in a Web Engineering process is discussed, including appropriate modeling software, and requirements for a runtime system.
This paper is an overview of the NYNEX VoiceDialingSM service — the first introduction of speech recognition based technology to a mass market of residential and business customers. This is a network based service which allows telephone users to make calls by simply saying the name of the person or place they wish to reach. VoiceDialingSM is compatible with both touch-tone and rotary dial service and is designed to work on all existing telephone sets and, therefore, on any extensions in the customer's home. We describe the network architecture, user interface, and speech recognition technology, with special focus on the analysis of success in service deployment and customer acceptance. The paper opens with an overview of speech research and development at NYNEX Science and Technology.
We explore the use of features derived from multiresolution analysis of speech and the Teager energy operator for classification of drivers' speech under stressed conditions. We apply this set of features to a database of short speech utterances to create user-dependent discriminants of four stress categories. In addition we address the problem of choosing a suitable temporal scale for representing categorical differences in the data. This leads to two modeling approaches. In the first approach, the dynamics of the feature set within the utterance are assumed to be important for the classification task. These features are then classified using dynamic Bayesian network models as well as a model consisting of a mixture of hidden Markov models (M-HMM). In the second approach, we define an utterance-level feature set by taking the mean value of the features across the utterance. This feature set is then modeled with a support vector machine and a multilayer perceptron classifier. We compare the performance on the sparser and full dynamic representations against a chance-level performance of 25% and obtain the best performance with the speaker-dependent mixture model (96.4% on the training set, and 61.2% on a separate testing set). We also investigate how these models perform on the speaker-independent task. Although the performance of the speaker-independent models degrades with respect to the models trained on individual speakers, the mixture model still outperforms the competing models and achieves significantly better than random recognition (80.4% on the training set, and 51.2% on a separate testing set).
This paper challenges the major theoretical motivation underlying a stage-model for language development (Gleitman, 1981), namely, that early grammars are exclusively of a semantic nature. Data concerning the development of gender systems in a variety of languages is presented. Particularly, the development of the use of referential pronouns and inflected verb forms and the role of the animate/inanimate distinction in the development of linguistic gender are seen to involve strictly formal nonsemantic generalizations from their first appearances in children's language, ages 2 years and on. Early two-word stage grammar cannot then be exclusively 'semantic'. Since it already involves semantic as well as non-semantic generalizations, the more highly developed grammars of later phases need not trigger any qualitative changes that will warrant a stage model for language development.
Some experiments with voice modelling using recent developments of the KTH speech synthesis system will be presented. It contains an improved glottal source built on the LF voice source model, some extra control parameters for the voiced and noise sources and an extra pole/zero-pair in the nasal branch. Furthermore, the present research versions of the KTH text-to-speech system include possibilities for interactive manipulations at the parameter level with on-screen reference to natural speech. The synthesis system constitutes a flexible environment for voice modelling experiments. The new synthesis tools and models were used for synthesis-by-analysis experiments. A sentence uttered by a female speaker was analysed and a stylized copy was made using both the old and the new synthesis system. With the new system the synthetic copy sounded very similar to the natural utterance.
From this point of view morphological criteria do not appear to provide a decisive answer and in the end it is those of frequency and the linguistic sense of the speakers which seem to give the most reliable indications. The study of these processes should take into account not only the structure of the loanwords themselves (the 'signifiants'), but also their precise meanings (the 'signifiés')—the words being examined with reference to the phonological and morphological characteristics of the languages involved, and the meanings with reference to the meanings of the Arabic originals and of possible corresponding loans in other Fula dialects and in the intermediary languages.
Articulatory parameters are determined from the first five formant frequencies and the first three formant amplitudes by the minimization of the root mean square error in the acoustic space. Beforehand, the nonlinear properties of the articulatory-to-acoustic transformation are investigated by use of a parametric representation of the area function and an electric analogue of the lossy vocal tract. A global analysis of the transformation is performed to locate the excessive nonlinearities that induce errors during the minimization procedure. A table of reference articulatory-acoustic pairs is generated from that analysis. The articulatory parameter identification procedure consists of an initial guess obtained from the reference table and a least-squares algorithm. Tests on formant values produced by the model itself indicate the efficiency of the method in solving the inverse problem. Another test on formant values corresponding to Fant's area function data provides qualitatively acceptable results, but the lack in precision indicates the limitations of the actual articulatory model.
The aim of this paper is to consider the classical pitch determination method based on the short-time autorcorrelation analysis of the speech signal. Two commonly used estimators and the effect of signal windowing are taken into account. It is shown that in both periodic and purely random cases a similar decomposition holds for the estimated autocorrelation. Such a decomposition makes it possible to foresee the relative merits of the estimators considered, at least as far as gross pitch determination errors and voicing-errors are concerned. The expected behaviour is found to be in good agreement with results obtained through applying the SIFT algorithm to real speech.
This paper describes some of the implications of Ludmilla Chistovich's “spectral center of gravity” (SCG) effect for a model of the auditory representation of American English vowels. Chistovich's work on defining a critical distance for the SCG effect is closely related to two of the most fundamental problems in speech communication research: the relation between acoustic attributes and phonemic features and the problem of invariance despite large acoustic differences between speakers. Firstly, experimental findings relating to the SCG effect are reviewed. Secondly, a model incorporating these perceptual effects is described, and thirdly, three aspects of the model are discussed: (1) the resulting feature analysis, (2) normalization, and (3) acoustic variability as represented in the model and its relation to Steven's quantal theory of speech production.
Such a simplification allows to turn the learning problem into an optimization problem which relies on an hill-climbing strategy requiring only one pass over the database. Our strategy penalizes tree depth by using adjusted R2. The experiments show that generalization error rate of our trees is as good as the one of C4.5, while maintaining the intelligibility of the results.
He also pronounces a small vocabulary called the adaptation vocabulary. Each new speaker then merely pronounces the adaptation vocabulary. We have compared two adaptation methods, establishing a correspondence between the codebooks of the reference and the new speakers, on a 20-speaker database with a 104-word application vocabulary. Method I uses a transposed codebook to represent the new speaker during the recognition process, whereas Method II uses a codebook which is obtained by clustering analysis on the NS's pronunciation of the adaptation vocabulary. The adaptation vocabulary contains 136 words. Comparison of the performance of the two methods shows that a new speaker's codebook is not necessary to represent the new speaker. Consequently we have used the first method to perform tests with a 5000-word application vocabulary, and a 4-speaker database. The adaptation is still efficient (the mean improvement is about 14%), even if the relative improvement is 30% compared to 56% obtained in the 104-word application experiment. Further experiments show that the recognition accuracy can be improved by increasing the adaptation vocabulary size and the codebook size.
Pitch detection remains one of the most difficult problems in speech analysis. Therefore, to detect pitch contours of speech, we have developed a new method which is quite different from conventional ones. The method utilizes a bank of bandpass filter-pairs; it is a fully continuous method in the time domain. This paper describes the parameter optimization of the filter-pair for a database of enlarged vocabulary and the integration of the filter-pair method by adding a voicing detector. Compared with conventional pitch detection methods, the proposed method produced a low Gross Pitch Error rate.
We describes here a general gaming model that can represent the rules of almost any computable game. We first develop the model with a game's classification and then we expose the computable one. Then we briefly describes an application that allow the confrontation of multiple kind of decision engines on games concerned by this model. To conclude, we expose a short protocol that allow to manage both the application and the decision engine's competitions around the general gaming model.
In this paper, we will consider the problem of speech recognition under noisy conditions. Local statistical information of the speech and the noise are estimated online and used as inputs to the estimators. The results are comparable to those obtained in clean condition at a moderate signal-to-noise ratio (SNR) of 20 dB. Substantial improvement is also obtained at lower SNRs. By carefully studying the results, it is noted that the MLP based estimators appear to perform poorly when there is virtually no detectable significant speech activity. As a result, a modified gain function is introduced.
This paper presents a study on the optimization of the set of rules that can be extracted from a set of data using the requent itemset search methodology. The association rules have been extracted using standard frequent itemsets level-wise search. A discussion holds on the pruning of the set of rules and on the possible optimization of the pruning of this rule set. As usual, support and confidence of rules are taken into account. In parallel, other rule quality criteria are introduced and discussed (referring mainly to statistics criteria).
We propose a graphical environment using the new radial tree layout, zoom/pan techniques and some existing methods, including explorer-like, hierarchical visualization, interactive techniques to represent large decision trees in a graphical mode more intuitive than the results in output of usual decision tree algorithms. The user can easily extract inductive rules and prune the tree in the post-processing stage. The numerical test results with real datasets show that the proposed methods have given an insight into decision tree results. It can guide the user towards for evaluating the models and also making more accurate decisions.
This paper proposes a normalization scheme for HMM-based text-dependent speaker verification in which the claimed-speaker model score and the background model score are computed for a common alignment made on the speaker-independent model of the password. It is shown that such a normalization preserves some speaker-specific information contained in the alignment and makes the normalization score more consistent in emphasizing remarkable parts of the claimed-speaker model. A special training procedure is proposed. Experiments on a large-scale and realistic telephone database are reported. Finally, first experiments about the integration of an information based on alignment in the decision making are presented. All these results show the interest of the method and encourage further investigation on the speaker modeling in such an approach.
In this paper we present a new direction finding algorithm for non circular sources based on the polynomial rooting technique. Using polynomial rooting instead of a searching technique limites the method to linear uniformly spaced arrays. Polynomial rooting however reduces computation cost and enhances resolution power significantly. Computer simulations are used to show the performance of the algorithm compared with that given by some known algorithms.
The paper describes an experimental-phonetic approach to the study of speech melody, developed at the Institute for Perception Research (IPO). The advocated method leads to intonation models which are helpful for the interpretation of acoustic and physiological data on pitch in natural speech. It is also a powerful framework for the design of rules for intonation synthesis in a variety of languages.
This paper reviews what is currently known about the sensory and perceptual input that is made available to the word recognition system by processes typically assumed to be related to speech sound perception. In the first section, we discuss several of the major problems that speech researchers have tried to deal with over the last thirty years. In the second section, we consider one attempt to conceptualize the speech perception process within a theoretical framework that equates processing stages with levels of linguistic analysis. This framework assumes that speech is processed through a series of analytic stages ranging from peripheral auditory processing, acoustic-phonetic and phonological analysis, to word recognition and lexical access. Finally, in the last section, we consider several recent approaches to spoken word recognition and lexical access. We examine a number of claims surrounding the nature of the bottom-up input assumed by these models, postulated perceptual units, and the interaction of different knowledge sources in auditory word recognition. An additional goal of this paper was to establish the need to employ segmental representations in spoken word recognition.
This study presents a new frequency domain parameter, Parabolic Spectral Parameter (PSP), for the quantification of the glottal volume velocity waveform. PSP is based on fitting a parabolic function to the low-frequency part of a pitch-synchronously computed spectrum of the estimated glottal flow. PSP gives a single numerical value that describes how the spectral decay of an obtained glottal flow behaves with respect to a theoretical limit corresponding to maximal spectral decay. By analyzing speech signals of different phonation types the performance of the new parameter is compared to three commonly used time-based parameters and to one previously developed frequency domain method.
Various types of knowledge can be extracted from the data stemming from a questionnaire. They depend on the questioning of the analyst but also the methods of data processing which are used. And so one can obtain the refusal of a null hypothesis but also a typology of the items of the questionnaire, the subjects which answered it, but still a graphic structure of inferential filiation, a hierarchy of behavioral rules, etc. In this paper, we present several possible approaches of treatment of a questionnaire aiming at structuring features of personality derived from behavior of answer to the questionnaire.
Instead, a large speech corpus is created per emotion to synthesize speech with the appropriate emotion by simple switching between the emotional corpora. The acoustic characteristics of each corpus are different and the emotions identifiable. The acoustic characteristics of each emotional utterance synthesized by our method show clear correlations to those of each corpus. Perceptual experiments using synthesized speech confirmed that our method can synthesize recognizably emotional speech. We further evaluated the method's intelligibility and the overall impression it gives to the listeners. The results show that the proposed method can synthesize speech with a high intelligibility and gives a favorable impression. With these encouraging results, we have developed a workable text-to-speech system with emotion to support the immediate needs of nonspeaking individuals. This paper describes the proposed method, the design and acoustic characteristics of the corpora, and the results of the perceptual evaluations.
In this paper, a joint process estimation algorithm is presented that simultaneously estimates the smooth spectral structure of the speech signal and the pulse-like driving function used in Multi-Pulse Linear Predictive Coding (MPLPC). Experimental results indicate that while the resulting excitation function differs from the normal multi-pulse excitation, the difference between the optimized and non-optimized linear predictive coding parameters is minimal in both a subjective and a numerical sense.
This paper presents an original method to assess the quality of a grey level image. An example of application is presented on images compressed according to the JPEG standard. Contrary to most of the existing methods, this quality assessment is univariant, i.e. it doesn't require any reference image. The quality is represented by a mark whose variation depends on what use the image is for: visual, mathematical, data processing. A neural network is used to learn the marking way with a pool of known examples. The method is then compared to classical bivariant methods to make sure that it is reliable. The anticipated results arrived at have a less than 7 % precision.
The aim of this study is to propose a new approach to Automatic Language Identification: it is based on rhythmic modelling and fundamental frequency modelling and does not require any hand labelled data. First we need to investigate how prosodic or rhythmic information can be taken into account for Automatic Language Identification. A new automatically extracted unit, the pseudo syllable, is introduced. Rhythmic and intonative features are then automatically extracted from this unit. Elementary decision modules are defined with gaussian mixture models. These prosodic modellings are combined with a more classical approach, a vocalic system acoustic modelling. Experiments are conducted on the five European languages of the Multext corpus: English, French, German, Italian and Spanish. The relevance of the rhythmic parameters and the efficiency of each system (rhythmic model, fundamental frequency model and vowel system model) are evaluated. The influence of these approaches on the performances of automatic language identification system is addressed. We obtain 91 % of correct identification with 21 s. utterances using all the information sources
Recent work ([CHI 98], [DEN 98]) has shown the interest of using Metropolis procedures in a Bayesian framework to search for good classification trees. For a particular class ofprior distributions on the trees, we introduce a new algorithm for MCMC sampling oftrees, similar to a Gibbs sampler, based on Willems et al. 's “tree weighting” algorithm [WIL 95], which results in a dramatic increase of the number of models actually taken into account. The sampled tree models are then averaged to produce an aggregate estimator or classifier. We present results ofsimulations on three benchmark datasets, that show the practical interest of this procedure.
In this study we present a cross-linguistic analysis of the strategies used by Korean, Japanese and English-speaking children in processing sentences with relative clauses. The results of an experiment on the comprehension of relative clauses in Korean are reported and compared with prior research on the acquisition of relative clauses in Japanese and English. In our experiment on Korean, 6-year-old children acted out sentences with left-branching and center-embedded relative clauses in two matrix word orders, SOV and OSV, and two intonation conditions, “clear” (syntactically motivated) and “list” intonation. The findings provide strong evidence for a basic left-to-right processing strategy and significant roles for a canonical sentence strategy and for a parallel function strategy in the comprehension of relative clauses in Korean. We propose that an adequate cross-linguistic account of relative clause comprehension must be based upon an integrated view of multiple universal processing strategies, whose application will depend upon language-specific structural properties of relative clauses and upon the developmental stage of the child.
Volterra models are widely used in various application areas. Their usefulness is mainly due to their ability to approximate with an arbitrary precision any fading memory nonlinear system, and their property of linearity with respect to parameters, the kernels coefficients. The main drawback of these models is their parametric complexity needing to estimate a huge number of parameters. Considering Volterra kernels as symmetric tensors, we use their PARAFAC decomposition to derive the Volterra-Parafac models inducing a substantial parametric complexity reduction. We show that these models can be viewed as a set of Wiener models in parallel. Then, we apply the extended Kalman filter for recursively identifying such Volterra-Parafac models. Some simulation results illustrate the effectiveness of the proposed identification method, in the case of cubic Volterra systems.
KEAL is a continuous speech recognition system developed at the CNET laboratory in Lannion (France). Part of the laboratory's current work aims at extending it in the direction of a speech-understanding and man-machine dialog system. A question-answer-type dialog is set in motion in order to provide the user with information (the current application consists in simulating a directory inquiries service). This paper describes how syntactic, semantic and pragmatic knowledge is used for implementing such a dialog, and the main advantages and drawbacks of the methods chosen are discussed. Sentence recognition is performed by a left-to-right bottom-up parser by means of a semantic context-free grammar. Using a method analogous to that of semantic attributes, the parse-tree is then interpreted in order to obtain a semantic structure which represents the information relevant to the subsequent dialog. The dialog manager uses the semantic structure for instantiating a model graph, which represents the state of the dialog at any instant; it indicates the next message to be sent to the user, and how to analyse his answer. An example derived from the directory inquiries service is described.
This article proposes a principle of knowledge's integration for the improvement of a system of defects recognition by vision on wooden boards. We locate the problem of vision which is at the base of this study, then we clarify the expert knowledge, as well in the field of the wood profession as in the field of vision. We use for that a symbolic model based on the NIAM/ORM method, formalizing the expert knowledge expressed in natural language. Then we present the way we exploit these expert knowledge to generate nodes of an arborescent structure for the defects identification of wooden boards. Each node represents an engine of inference based on fuzzy linguistic rules. The results obtained prove the interest of this principle.
This paper reports the results of the development, deployment and testing of a large spoken-language dialogue application for use by the general public. We built an automated spoken questionnaire for the US Bureau of the Census. In the project's first phase, the basic recognizers and dialogue system were developed using 4000 calls. In the second phase, the system was adapted to meet Census Bureau requirements and deployed in the Bureau's 1995 national test of new technologies. In the third phase, we refined the system and showed empirically that an automated spoken questionnaire could successfully collect and recognize census data, and that subjects preferred the spoken system to written questionnaires. Our large data collection effort and two subsequent field tests showed that, when questions are asked correctly, the answers contain information within the desired response categories about 99% of the time.
Jean Marot's rebus poem La vraye disant Advocate des Dames (1506), was composed in the form of a “neuvain picard.” Even though “Grands Rhétoriqueurs” are known to be “jongleurs de syllables” (“jugglers of syllables”), the literary device used in this poem proves to be problematic. By analyzing the different reading levels of the poem (literary game, historical poem, controversial poem, etc.), the author attempts to demonstrate how La vraye disant Advocate des Dames influenced his future career. It has most certainly had more influence than his earlier “rebus-rondeau”, which had been analyzed by Adrian Armstrong and where specific spatial relationships proved to be significant in the layout.
Language involves both structure and process. Giving each its due, we present a cognitive process model and show how its empirical success is related to claims about syntactic structure. However, what is directly observed in the experiments is not a syntactic structure but the execution of a plan. We present a language of process for representing such plans and thereby provide a unified explanation of several developmental phenomena, including the results of the above experiments and of new experiments suggested by our approach. The explanation is in terms of the cognitive resources required to formulate and execute a plan. Since the explanation is based on nonsyntactic processing, the children's syntax need no longer be held faulty. This conclusion invigorates the claim that the range of phrase structures available to children is biologically constrained.
Understanding the neural bases of cognition has become a scientifically tractable problem, and neurally plausible models are proposed to establish a causal link between biological structure and cognitive function. In the course of development and in the adult this internal evolution is epigenetic and does not require alteration of the structure of the genome. A selective stabilization (and elimination) of synaptic connections by spontaneous and/or evoked activity in developing neuronal networks is postulated to contribute to the shaping of the adult connectivity within an envelope of genetically encoded forms. At a higher level, models of mental representations, as states of activity of defined populations of neurons, are discussed in terms of statistical physics, and their storage is viewed as a process of selection among variable and transient pre-representations. Theoretical models illustrate that cognitive functions such as short-term memory and handling of temporal sequences may be constrained by “microscopic” physical parameters. Finally, speculations are offered about plausible neuronal models and selectionist implementations of intentions.
The aim of this paper is to support the idea that the study of complex cooperative systems needs a new conceptual and methodological framework in order to be understood and ultimately reorganised for better efficiency. We will then stress the role of this new approach as a deep renewal in the field of social sciences.
This paper reports on experiments of porting the ITC-irst Italian broadcast news recognition system to two spontaneous dialogue domains. Porting was investigated by applying state-of-the-art adaptation methods on acoustic and language models, and by evaluating the trade-off between performance and required amount of task specific annotated data. The use of different levels of supervision for acoustic model adaptation was also studied. By employing 2 h of manually annotated speech, word error rates of 26.0% and 28.4% were achieved by the adapted systems. These results are to be compared with the performance of two domain specific baseline systems, 22.6% and 21.2%, respectively, which were developed on much more training data. Finally, a robust method is presented that allows to tune the insertion of spontaneous speech phenomena by the speech decoder.
A significant stake of shape query in databases of images is to define unsupervised thresholds making it possible to avoid a flood of false detections, or, on the contrary, rejections of shapes which should have been recognized. By taking as example a method of shape recognition introduced by Lisani [15, 16], we show that one can answer the following question: given a query shape and a database of images, below which distance between the query and a detected shape is one sure that the shape is recognized? This insurance is quantified by the number of false alarms associated with the pair query shape – candidate shape. Although this method only considers for the moment pieces of shape, it already leads to sure detections based on a single piece of shape.
Speech sounds, as heard by listeners, contain phonetic, personal, and transmission information. The differences between the formant frequencies of vowels spoken by men, women, and children show a fairly uniform tendency in several studies and languages, and they are regarded as personal quality differences. The differences between the sexes are mainly due to the descent of the larynx in males during puberty. The observed tendency in female/male formant frequency ratios is reproduced in a calculation taking into account the physiological consequences of larynx descent and assuming that the vowel specific neural commands to the articulators remain unchanged. The perception of phonetic quality is seen as a process of tonotopic gestalt recognition. The tonality (= critical-band rate) distances between the formants in phonetically identical vowels are claimed and shown to be invariant as long as they are smaller than 6 Bark. The absolute position of the formants allows personal variation. The tonality distance between the first formant and the fundamental is smaller in most vowels spoken by women than in those by men and children. As for the role of the fundamental in this connection, some alternative hypotheses are discussed.
Image's experts use different kind of attributes to represent texture information. We propose a methodology to automatically choose the best texture models using a feature selection algorithm. Therefore we compare the efficiency of several recent algorithms. The algorithms evaluation is performed using classification error rates and heuristics. We demonstrate the interest ofsuch a methodology on Brodatz and satellite images.
The familiar Arabic pronominal object marker iyyā- performs other functions within the language. One of these, the demonstrative, has been recognized in spoken Egyptian Arabic but passes virtually unremarked in written Arabic. Nevertheless, it is so used by writers from the eastern Arabophone world more often than by those from the west. As such, it usually performs four roles in structuring information: expressing contrast, emphatic reflexivity, and two degrees of distal deixis. While modern Arab writers appear to use it demonstratively more often than did those of medieval and classical Arabic, that earlier writers were using it suggests that its demonstrative property is an inherent feature. This is confirmed by comparing object markers in other Semitic languages, which may function as demonstratives in Hebrew and Aramaic, reflexives in Syriac, and in remote deixis in Amharic.
Working on a feasibility study of wheatears counting, a colour component texture's analysis method was developed. The agronomic goal is yield prediction before harvest evaluating mean number of wheatears per squared meter according to the field variation knowledge. To this counting system, we evaluate six textural parameters (two statistical parameters and four Haralick features from co-occurrence matrix) on the main colour systems and vegetation indices used in agronomic applications. A new hybrid system provides a representation of wheatears' pictures taken under natural conditions with a better extraction of wheat. A method based on distances measurements (Euclidian, Mahalanobis) allows to extract wheatears with few errors corrected by mathematical morphology. Although we encounter difficulties from light intensity's variation and high entropy in the scene (ears' covering and shadows), results allow to extract disturbed wheatears and last recent images give an higher accuracy in segmentation.
The use of automatic speech recognition (ASR) technology to automate telephone-service transactions provides opportunities for significant operational cost savings, as well as for improvements in the quality of customer service. At GTE, our focus has been on developing speech-interactive systems, named OASIS, for service-order transactions. In this paper, we discuss our methodology for developing dialogs, describe the dialog developed for our service-disconnect application, and present data from a field trial of the disconnect system. Our design methodology addresses development of a transaction model, specification of dialog flow, design of language structures, and construction of system speech. In addition, following our approach, dialog flow and recognition vocabularies are developed in the context of the system recognition and interpretation capabilities. Primary attributes of our methodology include the following: structured representation of conversational transactions as a progressive flow of information elements; query language that follows the style of discourse to elicit predictable responses; use of recognition outcomes and corresponding system actions to define dialog flow; and generation of scalable solutions. To evaluate the effectiveness of our approach, we present results from the service-disconnect field trial. In general, users responded cooperatively, adhered to the structured interaction, rarely anticipated future information, and provided on-target answers to system queries.
We present in this paper a new approach to Pattern Recognition based on the Transferable Belief Model, a non probabilistic interpretation of Dempster and Shafer's theory of belief functions. This method uses the formalism of belief functions to represent the information provided by the training set concerning the class of a new pattern. Various decision strategies generalizing Bayes decision theory are presented and demonstrated using a real example.
The Speech Under Simulated and Actual Stress (SUSAS) database is a collection of utterances recorded under conditions of simulated or actual stress, the purpose of which is to allow researchers to study the effects of stress and speaking style on the speech waveform. The aim of the present investigation was to assess the perceptual validity of the simulated portion of the database by determining the extent to which listeners classify its utterances according to their assigned labels. Seven listeners performed an eight-alternative, forced-choice response, judging whether monosyllabic or disyllabic words spoken by talkers from three different regional accent classes (Boston, General American, New York) were best classified as, clear, fast, loud, neutral, question, slow, or soft. Mean percentages of “correct” judgments were analysed using a 3 (regional accent class)×2 (number of syllables)×8 (speaking style) repeated measures analysis of variance. Results indicate that, overall, listeners correctly classify the utterances only 58% of the time, and that the percentage of correct classifications varies as a function of all three independent variables.
In the present paper the most commonly occurring phoneme sequences in German are investigated with regard to their structural rules. Knowledge of all possible phoneme sequences and their structural rules is important e.g. for the context-dependent evaluation of acoustic-phonetic cues in a syllable-based automatic speech recognition system. Since the main coarticulation effects are limited to syllable initial and final region, the presented investigations regard the phonemes with respect to their position within the syllable. The results take the form of two graphs for the temporal ordering of the manner of articulation, one applying to all 45 IC's and the other to al 160 FC's.
A multi-agent system is composed of numerous entities, called agents, interacting in various ways between them and their common environment. This technology is applied in many domains like computer vision, robotics, system simulation or electronic commerce. We consider that problems occuring in signal processing could also be tackled by this technology. We present first the basic tools available for multi-agent systems designers: models, platforms and methodologies. Two projects illustrate our purpose: SCALA in the management of aerospace fighter patrol, and goods routing. We focus then on the adaptation ability of these systems considered as an emergent problem solving question. We detailed in this field the AMAS (Adaptive Multi-Agent System) theory allowing a MAS design where the global fonction is derived from the cooperative self-organisation of its components. An example on flood forecast gives implementation information of this theory.
A model for the identification of one- and two-formant steady-state vowels is proposed. The model consists of a formant-picking algorithm and a classification algorithm. A signal is represented in the model as a combination of several patterns of three types. Each pattern is characterized by its position on the tonality axis, and by a “type coefficient”. The pattern positions are determined by the positions of the spectral peaks detected. The type coefficients depend on the distances between the detected peaks and the center of gravity of the auditory spectrum. the output of the model is a “response distribution”; a normalized vector of the similarities of a signal to given classes (phonemes). An experiment on the identification of two-formant signals with varying amplitude relations between the formants is reported on, and the results are compared to the “response distributions” of the model. The results of the second experiment on the identification of one-formant signals are used to verify the parameters of the formant-picking algorithm.
A Brain-Computer Interface (BCI) is a new type of human-machine interface that allows direct communication between user and machine by decoding brain activity. The ERPs as the P300 can be obtained through the odd ball paradigm, where targets are selected by the user. A new method for reducing the number of sensors that record electroencephalography (EEG) signals is proposed. Reducing the number of sensors allows reducing the time required for the installation of sensors and therefore increases user's comfort. The proposed approach is based on a recursive elimination where the cost function is based on the signal to signal plus noise ratio (SSNR), after spatial filtering. We show that this cost function is more robust and less costly in computing time than other functions based on evaluation of the detection of P300 or targets, thus avoiding a step of classification. We also propose a decision function to better categorize the importance of a sensor based on the number of desired sensors. The proposed approach is tested and validated on 20 subjects over several sessions.
Genetic algorithms, genetic programming, evolution strategies, and what is now called evolutionary algorithms, are stochastic optimisation techniques inspired by Darwin's theory. We present here an overview of these techniques, while stressing on the extreme versatility of the artificial evolution concept. Their applicative framework is very large and is not limited to pure optimisation. Artifical evolution implementations are however computationally expensive: an efficient tuning of the components and parameter of these algorithms should be based on a clear comprehension of the evolutionary mechanisms. Moreover, it is noticeable that the killer-applications of the domain are for the most part based on hybridisation with other optimisation techniques. As a consequence, evolutionary algorithms are not to be considered in competition but rather in complement to the “classical ” optimisation techniques.
We report a new experimental approach to studying Spoonerisms, whereby subjects are required to deliberately exchange phonemes at specified positions in word pairs presented in list form, with speed and accuracy measurements. With CVC words, initial phoneme transpositions were performed best, and final phoneme transpositions worst. The ease with which medial phonemes were transposed depended upon whether or not the medial vowel was a dipthong, and also upon whether or not a Spoonerism exchange would result in a major orthographic change were the purely articulatory response to be actually transcribed. Thus orthographic variables appear to influence processing even in a task presumably performed at a purely acoustic/articulatory level. While performance was not affected by the word/nonword status of the response, the articulatory environment of surrounding phonemes did influence performance in exchanging a target medial phoneme. Finally sinistrals proved better able to produce such Spoonerisms on demand, possibly reflecting their superior ability at mirro reading and writing. The findings generated new hypotheses for the study of spontaneous Spoonerisms.
This paper describes the attenuation and group-delay distorsion approximation for IIR digital filters. The developped method is characterized by the use of two successive steps: the attenuation approximation by an iterative algorithm asking the solution of a set of linear equations (very fast), followed by the simultaneous approximation based on the solution of a linear programming problem at each iteration. The convergence of the two algorithms is guaranteed and their efficiency is proved by solving a classical example.
Various kinds of non-text symbols appear in texts. The oral expressions of these symbols may vary with their senses. This paper proposes a three-layer classifier (TLC) which can disambiguate the senses of these symbols effectively. The layers within TLC are employed in sequence. The 1st layer is composed of two components: pattern table and decision tree. If this layer can disambiguate the sense of the target symbol, the disambiguation task stops. Otherwise the next two layers will be triggered. According to the algorithm confidence of sense disambiguation, the 3rd layer may exploit an alternative model to enhance the performance. Experiments show that our approaches can learn well even with only a small amount of data. The overall accuracies of training and testing sets are 99.8% and 97.5%, respectively.
This paper describes the approach taken at Chalmers University of Technology in building up an integrated multilevel speech database for the purpose of speech research and the development of speech coding techniques. The material comprises today isolated speech sounds (phones and diphones) as well as short, semantically unrelated sentences and coherent texts. Data collection is, to start with, restricted to Swedish material and read speech. Registration of the speech samples was carried out under optimal conditions (sound-insulated, anechoic studio) using digital recording equipment (SONY PCM-F1). Segmentation, classification and labeling is performed at eight interlacing levels of linguistic (including acoustic, phonetic and prosodic) analysis.
The Discriminative Feature Extraction (DFE) method provides an appropriate formalism for the design of the front-end feature extraction module in pattern classification systems. In the recent years, this formalism has been successfully applied to different speech recognition problems, like classification of vowels, classification of phonemes or isolated word recognition. The DFE formalism can be applied to weight the contribution of the components in the feature vector. This variant of DFE, that we call Discriminative Feature Weighting (DFW), improves the pattern classification systems by enhancing those components more relevant for the discrimination among the different classes. This paper is dedicated to the application of the DFW formalism to Continuous Speech Recognizers (CSR) based on Hidden Markov Models (HMMs). Two different types of HMM-based speech recognizers are considered: recognizers based on Discrete-HMMs (DHMMs) (for which the acoustic evaluation is based on an Euclidean distance measure) and Semi-Continuous-HMMs (SCHMMs) (for which the acoustic evaluation is performed making use of a mixture of multivariated Gaussians). We report how the components can be weighted and how the weights can be discriminatively trained and applied to the speech recognizers. We present recognition results for several continuous speech recognition tasks. The experimental results show the utility of DFW for HMM-based continuous speech recognizers.
Traditionnally case-based reasoning is conducted on experiences that are represented in a well structured format such as objects or database records. However different models have been proposed to overcome some of the limitations imposed by this structural approach and to undertake new applications domains. In this paper, we review some of the extensions proposed to apply CBR principles to experiences contained in textual documents, commonly refered to as textual CBR. Following a short presentation of some case-based reasoning principles, we present the main research works in Textual CBR and provide a synthesis of current state of the art in the field. We finally discuss some shortcomings of the current approaches and propose some directions for future research.
Two experiments investigated the role of syntactic presupposition in sentence comprehension. In Experiment I subjects verified cleft, pseudocleft and factive complement sentences with respect to preceding context paragraphs, which contradicted either the assertion or the presupposition of the target sentence. Subjects took significantly longer to verify sentences with false presuppositions than sentences with false assertions. In Experiment II subjects verified cleft and pseudocleft sentences with respect to subsequently presented pictures. Once again, verification times for sentences with false presuppositions were significantly longer than verification times for sentences with false assertions. It was argued that these findings are more adequately explained by a “structural” hypothesis, than in terms of strategies designed to locate given and new information.
Variation in pronunciation observed in speakers today parallels in many details the documented variation in pronunciation over the centuries (sound change). It is reasonable to conclude that there is some necessary link between the two. I argue that diachronic variation emerges for the most part from synchronic variation thus: universal and timeless physical constraints on speech production and perception leads listeners to misapprehend the speech signal. Any such misapprehension that leads the listener to pronounce things in a different way is potentially the beginning of a sound change. If we study sound change we can gain insights into how speech is produced and perceived. I exemplify this point by considering a variety of sound changes that involved voiceless fricatives: so-called spontaneous nasalization, s-aspiration, and nasal effacement. They suggest that one cue to this class of sounds is a special voice quality on that portion of vowels immediately abutting the fricative.
Three lexical decision experiments were concerned with the separability of syntactic and semantic processing in spoken word perception. An additional experiment examined the problem of measuring reaction times to a spoken stimulus. Words in the Serbo-Croatian language were used; each stimulus consisted of a noun stem (which was either a meaningful root or a pseudoword stem) plus an inflectional suffix which conveyed information about the noun's grammatical case. Speed of identifying the inflectionally related forms of a noun was a function of differences in their syntactic meanings rather than differences in their physical forms or their actual frequencies of occurrence. In addition, identification of a noun was facilitated when it was preceded by a stimulus carrying predictive inflectional information whether that stimulus was a real adjective or pseudoadjective. The results echo previous findings for word perception in print and provide evidence of essential structural uniformity in the processing of inflection for both spoken and printed words. For both, there is evidence that inflectional processing is modular, at least to the extent that it is independent from semantic processing for the initial portion of its time course.
Several activities have been undertaken in Italy in the field of Digital Mobile Radio (DMR). In particular, concerning speech coding two codecs were developed to be compared on experirental Single Carrier Per Channel (SCPC) systems. These two codecs are described in the paper and their performances are compared. The main feature of both schemes is the use of Vector Quantization (VQ) to compact the side information. In addition the SB-APC scheme uses the technique of post-filtering to shape the quantizing noise of each sub-band. The main result reported in the paper is that the performances of the two schemes are almost equivalent although their structure is very different.
We propose a new glottis signal model. Most glottal pulse shape models have been obtained so far by concatenating a small number of curved segments. We propose here an alternative approach which consists of expanding the signal into a combination of a finite set of basic time functions. These functions are chosen taking into account the point-like and non-linear character of the acoustic voice source. We establish a mathematical relationship between the weights of the individual time functions and the Fourier coefficients of the signal being modelled. The control parameters are the frequency and amplitude of a cosinusoidal driving function. The spectral envelope of the output signal (i.e. the glottal pulses) changes with the fundamental frequency, and its spectral content evolves with the amplitude of the driving function.
The first one performs low level processing and extracts characteristic points. The second one processes a state space transformation of the input picture, tries to recognize the learning objects and proposes a reconstruction to confirm the recognition During the training, the robot extracts characteristic points of one object and makes a transformation from each of these points. During the interpretation, the robot focuses its eye on a characteristic point, processes a complex logarithmic transformation and performs a mental rotation to match the present transformation with the learned representation. To complete its interpretation or to remove ambiguity, the robot focuses on other characteristic points used during learning. Objects can be recognized in real scene even if they are partially occluded or if the picture is noisy.
The performance of the existing speech recognition systems degrades rapidly in the presence of background noise. A novel representation of the speech signal, which is based on Linear Prediction of the One-Sided Autocorrelation sequence (OSALPC), has shown to be attractive for noisy speech recognition because of both its high recognition performance with respect to the conventional LPC in severe conditions of additive white noise and its computational simplicity. The aim of this work is twofold: (1) to show that OSALPC also achieves a good performance in a case of real noisy speech (in a car environment), and (2) to explore its combination with several robust similarity measuring techniques, showing that its performance improves by using cepstral liftering, dynamic features and multilabeling.
The paper addresses the problem of the temporal integration of information in vowel perception and the nature of this information. One-formant stimuli with, two-formant F1, F2 stimuli and stimuli with a jumping formant produced as trains of alternating F1 - and F2 - pulses occurring on different proportions were used in vowel identification experiments. The response distributions corresponding to two-formant stimuli and to stimuli with a jumping formant were approximated as weighted sums of two 'basic' distribution. 1 and 2, corresponding to one-formant stimuli. An increase in the proportion of F2-pulses in stimuli with a jumping formant was accompanied by a systematic increase in the 2 component in the response distribution. Stimuli with a jumping formant did not elicit the phonemic responses typical for two-formant stimuli with the same F1 and F2 frequencies. The data suggest that the running identification of the stimulus is integrated temporally.
This paper describes an attempt at capturing segmental transition information for speech recognition tasks. The slowly varying dynamics of spectral trajectories carries much discriminant information that is very crudely modelled by traditional approaches such as HMMs. In approaches such as recurrent neural networks there is the hope, but not the convincing demonstration, that such transitional information could be captured. The method presented here starts from the very different position of explicitly capturing the trajectory of short time spectral parameter vectors on a subspace in which the temporal sequence information is preserved. This was approached by introducing a temporal constraint into the well known technique of Principal Component Analysis (PCA). On this subspace, an attempt of parametric modelling the trajectory was made, and a distance metric was computed to perform classification of diphones. Using the Principal Curves method of Hastie and Stuetzle and the Generative Topographic map (GTM) technique of Bishop, Svensen and Williams as description of the temporal evolution in terms of latent variables was performed. On the difficult problem of /bee/, /dee/, /gee/ it was possible to retain discriminatory information with a small number of parameters. Experimental illustrations present results on ISOLET and TIMIT database.
A common problem in the field of speech synthesis is the lack of a quantitative description of speech sounds applicable to both vowels and consonants. Usually only vowels are specified in terms of formant frequencies. This paper proposes a parametric description of the steady-state zones of all non-plosive phones based on centroids in the log area ratio parameter space. For each phone, a centroid is computed as the center of gravity over an ensemble of realizations produced by a single speaker in different contextual situations. The centroids are quantitatively compared to one another by means of an objective distance measure. In addition some information relating to the dynamic behaviour of the speech signal is obtained by signal destruction and construction experiments. All these data then serve to discuss several phonoacoustic problems encountered in the delimitation of the German phones, with respect to vowel quality and quantity, for example.
This paper proposes Strada, a novel learning approach to the automatic design of adaptive strategies for modern strategy games. Strada combines new ideas with state-of-the-art techniques from several areas of machine learning. Solutions to these issues are combined into an efficient learning system, whose performance is demonstrated on a commercial war-game.
Speech analysis and synthesis methods developed at ECL in NTT are described. Starting from the LPC method the PARCOR and LSP schemes based on all pole filter are explained. The principles and physical interpretations of these methods are presented in comparison with each other. The characteristics of the feature parameters in these methods are clarified by means of several experiments. Synthesized speech quality is also shown by objective and subjective experiments. High quality synthesized speech can be achieved by these methods at a low bit rate transmission under 9600 bps.
In this article, we propose a method of characterization of images of old documents based on a texture approach. This characterization is carried out with the help of a multi-resolution study of the textures contained in the images of the document. Thus, by extracting five features linked to the frequencies and to the orientations in the different areas of a page, it is possible to extract and compare elements of high semantic level without expressing any hypothesis about the physical or logical structure of the analysed documents. Experimentations demonstrate the performance of our propositions and the advances that they represent in terms of characterization of content of a deeply heterogeneous corpus.
An important procedure in many prosodic analysis systems is locating syllables. The location of syllables is used in the identification of stress and of pitch accents, which in turn form the basis for the analysis of rhythm and intonation. This paper presents a novel syllabification system utilising recurrent neural networks which operates on speaker-independent continuous speech. It is trained and tested on dialect region 1 of the TIMIT database, and finds 94% of syllables and places most syllable boundary points within 20 msec of the desired location. Methods for optimising the performance and training of the recurrent neural networks are investigated.
This paper describes the real time implementation of the Klatt speech formant synthesizer (cascade and parallel) on the SGS-Thomson DSP Processor. The determination of optimal functional conditions of the synthesis algorithm is performed by means of the floating point simulation. Fixed point simulation is realized to estimate the world length and to define the adequate scale of the used variables. The realization of these procedures allows the programming, in assembly language, of the algorithm on the DSP. This is a first step towards a realization of a PC-compatible speech synthesis board based on the DSP processor.
In this paper we deal with the statistical grey-level segmentation, without any reference to texture. Adding a previous model parameter estimation step, which is a mixture estimation, all these methods can be rendered automated, or unsupervised. On the one hand, results obtained with unsupervised methods differ little from results obtained with true parameter based methods. Although global methods can give excellent results when data are well suited to the underlying model, in other situations local methods can ensure clearly better performance. We deduce the choice from two factors: class image homogeneity and spatial correlation of the noise. The good behaviour of our algorithm is validated with simulations and real-world image segmentation results.
It is argued that theories of semantic memory have diverged in a manner that parallels current linguistic controversy concerning the representation of meaning. The feature-comparison model (Smith, Shoben & Rips, 1974) applies the linguistic theory of Lakoff (1972) to predict people's reaction times to verify sentences, while the marker-search model, described here, uses the type of semantic representation outlined by Katz (1972) to explain a similar range of data. The two models are described and the evidence for each is reviewed. Available evidence supports the marker-search model, but disconfirms a major prediction of the feature-comparison model. It is argued that the feature-comparison model is in principle inadequate as a model of semantic representation, unless its conception of semantic components is substantially alatered.
In recent years, the sustainable management of agricultural and ecological systems has become a major challenge. Sustainable management has to solve crucial environmental problems linked, in part, to rapid changes in context: climatic changes, agricultural policy objectives changes, etc. Solving this challenge involves the joint development of researches in modelling, simulation and virtual experimentation. In this article, we present some recent work devoted to the modelling and simulation of complex systems involved in agro-ecosystem management. Then, we present new formalisms for management strategies design, based on the Weighted Constraint Satisfaction Problems or the Markov Decision Processes frameworks. We also how how simulation and conception of strategies can be integrated. Finally, we illustrate the use of the presented approaches on several case studies in Agroecosystems management, jointly tackled with research teams in Agronomy.
Piaget's theory of space perception in infancy is presented in the format of a hypothetico-deductive system. Eleven hypotheses are defined, regarding the perception of the agent of visual change; shape and size constancy; depth; and the perception of higher-order relationships among spatial elements. The proof Piaget offers for each hypothesis is presented in the following steps: behavioral evidence, interpretation in terms of inner states; inferences and generalizations. Some general conclusions are briefly discussed.
The paper investigates the acoustic measures of pitch perturbations and vocal noise in pathological voice signals in terms of their ability to discriminate between normal and pathological voice status. A brief review of the related work done by other researchers in Japan is also given.
This chapter proposes to the discussion a set of principles for a design-oriented analysis of technico-organisational systems as dynamic, living, social and cultural systems, through eleven questions. After developing an ontological notion of complexity adequate to technico-organisational systems, these questions deal with the different theoretical, epistemological and methodological (data collecting, analysis and modelling) aspects of the knowledge of this complexity and their relation to design. The discussion appeals to many different disciplinary contributions, in empirical science, ergonomics and philosophy, and relates to ideas developed by other authors in other chapters of this book.
Grammatically incorrect sentences (paragrammatisms) are characteristic of the spontaneous speech of fluent aphasics. The paragrammatisms produced by five neologistic jargon aphasics are analysed and compared to the paragrammatisms of four normal control subjects. We show that the paragrammatisms of the aphasics are qualitatively identical to the grammatical errors of normal subjects, although they are much more frequent. The reason for this is discussed in terms of models of speech production; we argue that paragrammatisms are a consequence of a breakdown in the control processes.
This paper is an analytical study of ten new Thamudic inscriptions written in the so-called Thamudic E script, collected by the authors during a survey in the region of al-Jafr (southeast Jordan). The study aims at analyzing the inscriptions, meanings and structure of the words and the proper nouns contained therein. This group of inscriptions emphasizes some new personal names mentioned in Thamudic inscriptions.
A multilayer perceptron has been trained to perform an analogue mapping from the power spectra of vowels and nasal consonants, spoken by a single speaker, to the control parameters of a speech synthesiser based on an acoustic tube model. The outputs of the neural network control these eleven areas, while its inputs are samples of the power spectrum which the synthesised speech spectrum is intended to copy. After training, natural speech, with this restricted phoneme set and by the same speaker, can be synthesised with good intelligibility.
A reduced complexity multipulse coder is proposed in this paper. A 33% complexity reduction affects the overall speech output quality only slightly as is demonstrated in both informal listening and SNR computation. On the other hand, it makes possible the implementation of the coder with cheap commercial signal processing chips. The TMS32020 has been used for real time implementation of the proposed algorithm at 16 kbit/s with only fixed point computations. The method studied here can be considered as an alternative to the regular pulse excitation technique.
We consider the design of a bot for the Tetris game. After a review of the literature, we emphasize the fact that comparing the performances must be done with great care, as the game scores have a huge standard deviation, and as subtle implementation details have a significant effect on the resulting performance. We then consider the cross-entropy method to tune a ratingbased one-piece bot as suggested by Szita et al. (2006). In this context, we discuss the influence of the noise, and we make experiments with several sets offeatures such as those introduced by Bertsekas et al. (1996), by Dellacherie (Fahey, 2003) and some original ones. This approach leads to a bot that outperforms the previous known results. On a simplified version of Tetris considered by most research works, it achieves 35,000,000 ±20% lines per game on average.
In this paper, we propose an original approach for sequential non-parametric density estimation defined in high-dimensional state spaces using the particle filtering framework. By exploiting conditional independences in the state space, we propose to swap independent sub-particle sets to generate new sets that better sample this space. We integrate this approach into two versions of particle filter, i.e., partition sampling and annealed particle filter, to prove its efficiency. We compare it to classical approaches on synthetic articulated object density estimation problems, and show that our approach reduces both estimation errors and computation times.
It is shown that the maximum of entropy rate corresponds to the maximum of prediction error. Using the reflection coefficients, the equivalence with the autoregressive method is directly established. An interpretation in terms of whitening is given and the minimum of entropy rate is also discussed.
In spoken document retrieval (SDR), speech recognition is applied to a collection to obtain either words or subword units, such as phonemes, that can be matched against queries. We have explored retrieval based on phoneme n-grams. The use of phonemes addresses the out-of-vocabulary (OOV) problem, while use of n-grams allows approximate matching on inaccurate phoneme transcriptions. Our experiments explored the utility of word boundary information, stopword elimination, query expansion, varying the length of phoneme sequences to be matched and various combinations of n-grams of different lengths. Our experiments show that there is some deterioration in effectiveness, but the particular form of matching is less vital if the sequence of phonemes was correct. When phone sequences are recognised directly, with higher error rates than for words, it was more important to select a good matching approach. Varying gram length trades precision against recall; combination of n-grams of different lengths, in particular 3-grams and 4-grams, can improve retrieval.
The manuscript BnF, fr. 6449 constitutes the unique known copy of Jean Miélot's Vie de sainte Katherine (1457), a complete biography of this Saint, native of the Egyptian town of Alexandria, which the Burgundian canon set into the context of the Roman history of the fourth century. This article highlights the interest of this work through a presentation of the manuscript, an investigation surrounding the Latin sources of the text, as well as an examination of the language and translation techniques by Miélot ; it provides so a preliminary study to the critical edition of the Vie.
This article presents an overview of the POLYCOST database dedicated to speaker recognition applications over the telephone network. The main characteristics of this database are: medium mixed speech corpus size (>100 speakers), English spoken by foreigners, mainly digits with some free speech, collected through international telephone lines, and minimum of nine sessions for 85% of the speakers.
This paper illustrates the importance of various cognitive factors involved in perceiving and comprehending synthetic speech. However, and more importantly, this difficulty can and does decrease with the subjects' exposure to said synthetic voices. Furthermore, greater workload demands are associated with synthetic speech and subjects listening to synthetic passages are required to pay more attention than those listening to natural passages.
Face recognition is the process of the automatic recognition of the person's identity based on individual informations that are included in face image. This document demonstrates how a face recognition system can be designed by a conventional artificial neural network and by another more recent neural network, which is called Spike neural network. The latter is developed to capture the important characteristics of the face, to simulate the human visual system and to optimise the computational time, this last characteristic has been one of the driving forces behind the development of spike neural networks. Note that the training process of the two networks, on different sets of noisy images, forced the two networks to learn how to deal.
This paper describes the use of a multiple codebook semi-continuous hidden Markov model (SCHMM) automatic speaker verification (ASV) system, which uses a novel technique for discriminative hidden Markov modelling known as discriminative observation probabilities (DOP). DOP is not a discriminative training technique, it is a method of constructing what is effectively a discriminating model by contrasting two standard HMMs so as to improve discrimination between the classes that those models represent. This paper experimentally evaluates the use of DOP HMMs for ASV. The experimental evaluation is based on a text-dependent task using isolated digits. The database contains 24 true (client) speakers and 100 casual impostors, recorded over the public telephone network in the United Kingdom.
In this study we present approaches to multilingual speech recognition. We first define different approaches, namely portation, cross-lingual and simultaneous multilingual speech recognition. We will show some experiments performed in the fields of multilingual speech recognition. In recent years we have ported our recognizer to other languages than German (Italian, Slovak, Slovenian, Czech, English, Japanese). We found that some languages achieve a higher recognition performance with comparable tasks, and are thus easier for automatic speech recognition than others. Furthermore, we present experiments which show the performance of cross-lingual speech recognition of an untrained language with a recognizer trained with other languages. The substitution of phones is important for cross-lingual and simultaneous multilingual recognition. We compared results in cross-lingual recognition for different baseline systems and found that the number of shared acoustic units is very important for the performance. With simultaneous multilingual recognition, performance usually decreases compared to monolingual recognition. In few cases, like in the case of non-native speech, however, the recognition can be improved.
In our work we attempted to answer the following question: How do listeners identify two sentences as spoken by one or by two persons when they can take into consideration only acoustic speaker characteristics and intonation? We used successive utterance portions forming a single turn or two subsequent turns in a dialogue, the lexical structure of which was held constant. The experiments are based on sentence combinations varying with respect to speakers, sentence types, and types of communicative coherence. Pairs of sentences with and without constant intervening pauses were presented to subjects with the task of ascribing them to one of three categories: “dialogue”, “monologue” and “metadialogue” (i.e., the imitation of dialogue by a single voice). The results showed that the following factors influence the decision of the listeners: voice properties, presence or absence of pause, and the communicative types of the sentences. Two principles involved in processing the input signals are supposed. If the sentences are separated by a pause, a comparison of the auditory voice properties of the two portions may take place.
We present in this paper a new bio-inspired algorithm that dynamically creates and visualizes groups of data. This algorithm uses the concepts of flying insects that move together in complex manner with simple local rules. Each insect represents one data. The insect moves aim at creating homogeneous groups of data that evolve together in a 2D environment. These created groups are visualized in real time and help the domain expert to understand the underlying class structure of the data set, like for instance a realistic number of classes, clusters of similar data, isolated data. We present several extensions of this algorithm like reducing its computational time and the use of a 3D display. This algorithm has been tested on artificial and real-world data. A heuristic algorithm can be used to evaluate the relevance of the obtained classification.
An improved, phoneme-based IWSR system is described, which employs a robust reference data extraction procedure and achieves increased recognition accuracy. Furthermore, a novel method for the adaptation of the IWSR-system to continuous speech is presented. The IWSR system employs a multisection codebook design technique and the LVQ algorithm, which provide well-defined and accurate codebooks, minimize the influence of the within-word coarticulation and allow the use of time-sequence information at the recognition stage. The adaptation method is based on modifications of the system's reference data codebook using a small amount of representative continuous speech data on linear transformations of the main prosodic parameters (i.e. energy and duration). Extensive testing under different conditions (speaker dependent versus speaker independent reference data, single versus multisection codebooks, adapted versus unadapted codebooks, phoneme versus word recognition accuracy, etc.) has shown the efficiency of the proposed methods.
This article is about the two earliest sources of the Cent Nouvelles Nouvelles: the manuscript of Glasgow (University Library, Hunter 252), and the edition by Vérard (Paris, BnF, Rés. Y2-174). A summary of philological problems concerning the transmission of the text is followed by a contrastive analysis of the iconographic programs in the two volumes (illuminations and woodcuts), in particular for nouvelles 9, 12, 27, 33, 46.
Chomsky and Halle (1968) claim that the stress and intonation of an utterance are not determined solely by physical properties of the acoustic signal but are also influenced by the syntactic organization of the utterance. Strong support for their contention has been obtained by presenting listeners with a continuously repeated string of monosyllabic words.
spicos II enables an on-going dialogue to take place with an office data-bank. This represents a further development of spicos I that, linguistically, was confined to a simple question-answer scheme. spicos II is speaker-adaptable and incorporates a vocabulary of some 1,200 words. The present article gives an overview of the elements connected with linguistic analysis. These elements are governed by a dialogue module that controls the dialogue and, by means of appropriate follow-up questions, avoids possible communications difficulties. The syntactic analysis is based on an augmented phrase-structure grammar. At the same time, semantic constraints are checked by means of a semantic network. Syntactic structure, together with semantic features and the results of the resolution of anaphoric bindings and a separate formal representation of discourse referents, is used to build formal-logic semantic representations of utterances. User presuppositions are also represented. From these formal representations date-bank queries are generated.
In this study coarticulatory effects on the formant frequencies and on the duration of the Dutch schwa were investigated, both in open syllables and in closed syllables, by using nonsense words of the form C1əC2V and VC1əC2. In these nonsense words C1 and C2 could be any of the consonants /p, t, k, f, s, χ, m, n, η, r, l, j, ν/ and V was taken from the vowel set /i, a:, u/. Consonants and vowels were systematically varied in all possible combinations which gave a total of 897 test words that were read aloud by three male speakers. It appeared that the coarticulatory effects on the schwa could be successfully described with a simple linear model. Especially for F 2-tracks of schwas, the model fit turned out to be very good. The model for F 2-tracks could also be successfully applied to schwas in meaningful words. We believe that the schwa should be interpreted as a vowel without articulatory target that is completely assimilated with its phonemic context. The widespread view of a schwa position in the centre of the vowel triangle, that the formant patterns of reduced vowels are shifting to, is not very accurate. In our interpretation vowel reduction results in a shift of formant frequencies to a schwa position that can be almost anywhere in the vowel plane, dependent on the phonemic context.
This paper deals with real time face detection and tracking by a video camera. The method is based on a simple and fast initializing stage for learning. The transferable belief model is used to deal with the prior model incompleteness due to the lack of exhaustiveness of the learning stage. The algorithm works in two steps. To deal with the colour information dependence in the fusion process, we propose a compromise operator close to the Denœux cautious rule. As regards the tracking phase, the pignistic probabilities from the face model guarantee the compatibility between the believes and the probability formalism. They are the inputs of a particle filter which ensures face tracking at video rate. The optimal parameter tuning of the evidential model is discussed.
A recently proposed pitch detection algorithm (Dologlou and Carayannis, Speech Communication, Vol. 8, 1989) used iterative low-pass, zero-phase filtering followed by pulse-to-pulse measurements on the filtered signal. The iterative low-pass filtering was terminated when the signal had a sufficiently sinusoidal appearance. The halting criterion used was that frequencies derived from an autocorrelation analysis and a second order LPC analysis should be sufficiently close to each other. A claim is made by the authors that unless the input was a pure sinusoid, the two frequencies would never be equal. We discuss the proposed halting criterion and give an example with a speech-like signal consisting of two sinusoids of comparable amplitudes; this example violates the claim concerning the uniqueness of the two frequencies and where the proposed algorithm would halt prematurely.
Errors were analyzed with reference to: (i) general structural quantitative parameters (numeral and number lengths…); (ii) general behavioral disturbances (perseveration, serial ordering disorders…); (iii) specific cognitive processes implied in this particular transcoding task. The third approach proved the most powerful; it indicated that systematic errors were produced resulting from the partial and/or inappropriate use of transcoding strategies such as the transcription of each numeral element into its lexical value or the systematic coding of 'MILLE' (thousand) and 'CENT' (hundred) into one digit, irrespective of their particular lexical/ multiplicand role in the syntactical structure underlying the word order in numerals. The results are relevant to the study of cognitive processes in normal subjects.
An algorithm which estimates both the input excitation and the parameters of a time-varying autoregressive moving-average (ARMA) speech production model is proposed. Here, the AR and MA coefficients are estimated by two independent recursive least squares (RLS) lattice joint process estimators, respectively, and the input excitation is estimated by a bootstrap from one of the two estimators. From the experimental spectral envelope estimations for the natural speech signals, it is shown that the proposed algorithm yields accurate parameter estimation of a slowly time-varying ARMA speech model.
This paper provides a new interpretation of the so-called “delta-cepstrum” and extends the formulation of the conventional delta-cepstrum towards an optimal design of the filter, which extracts important spectral dynamics from a cepstrum sequence. The algorithm to obtain new feature parameters is unified to a formulation using a matrix coefficient filter and is tested through Japanese speech recognition experiments. The average recognition error rate in a Japanese 24 phoneme recognition experiment for four speakers was reduced from 12.2% to 10.3%.
Control of unregulated animal diseases depends on farmers but is often encouraged by professional organizations to enhance the health status or the economy in the area. The challenge is to be able to offer tools for making decisions and assessing a priori the impact ofproposed decisions on the spread of a pathogen in epidemiological (prevalence) and in economic terms. In this paper, we evaluate the contribution of Markov Decision Processes (MDP): We propose a model of between-herds spread when a control action is prompted by a collective decision maker to optimize the cost of the disease and of its control at the group level. We suppose the efficiency of the advice is known. Then, an epidemiological MDP model can suggest at each time whether advice should be given or not. Resulting strategy is non-systematic. Although the goal was to decrease costs, we observe the prevalence decreases also. Here, an adaptive strategy is an advantage of our approach because these strategies are often not studied.
Speech is normally heard against a background of other sounds. This paper reviews recent work on listeners' ability to separate speech from other sounds. Evidence is presented that both low-level grouping mechanisms and knowledge specific to speech are deployed in solving this diffucult problem.
With the rise of significant speech recognition and text-to-speech applications, the activity of our lab encompasses now a broader set of activities, from new algorithmic approaches to speech product engineering and application development. In particular, the paper gives an overview of the products originated from our speech technology research.
This paper presents an approach based on the properties of group delay functions for extracting formants from speech signals. The algorithm is similar to the cepstral smoothing approach for formant extraction using homomorphic deconvolution. The significant differences are (i) the logarithmic operation is replaced by () r operation and (ii) the additive and high resolution properties of group delay function are expolited to emphasize formant peaks. The group delay function (or the negative derivative of the Fourier transform phase) is derived for a signal which in turn is derived from the Fourier transform magnitude of the speech signal. If a suitable value of r is used, this method gives highly consistent estimates of formants compared to both the cepstral approach and the model-based linear prediction (LP) approach for smoothing the magnitude spectrum. The effects of the parameters, exponent r and window width p, on the proposed technique for formant extraction are studied.
This paper deals with the estimation of a speech signal disturbed by acoustical additive noises when two noisy observations are available. The useful signals received on two microphones are issued from the same speech signal and noises are assumed to be decorrelated. Our concern is the improvement due to the second channel in comparison with the estimation obtained using only one channel. The vectorial Wiener filtering and two structures, called PIS (Preprocessing + Signal Identification), are presented. A theoretical study based on optimal filters is first presented. The different systems as well as the mono-channel Wiener filtering are compared in terms of distortion, residual noise and global error. The interest of the proposed structures is proved for low input signal to noise ratios on the first channel. The four algorithms — mono-channel and two-channel Wiener filterings and the two structures PIS – are tested on real noisy signals recorded in a moving car. They are compared using objective and subjective measures. Results confirm the improvement brought by our methods on real signals.
A novel speech recognizer is described which capitalizes on multi-dimensional articulatory structures and incorporates key ideas from autosegmental phonology and articulatory phonology. The novelty has been in the design of the atomic units of speech so as to arrive at a unified and parsimonious way to account for the context-dependent behavior of speech acoustics. At the heart of the recognizer is a procedure developed to automatically convert a probabilistic overlap pattern over five articulatory feature dimensions into a finite-state automaton which serves as the phonological construct of the recognizer. The phonetic-interface component of the recognizer, based on the nonstationary-state hidden Markov model or the trended HMM, is also described. Some phonetic recognition results using the TIMIT database are reported.
Utterance verification (UV) is a process by which the output of a speech recognizer is verified to determine if the input speech actually includes the recognized keyword(s). The output of the speech verifier is a binary decision to accept or reject the recognized utterance based on a UV confidence score. In this paper, we extend the notion of utterance verification by presenting an utterance verification method that will be utilized to perform three tasks: (1) detect non-keyword strings (false alarms), (2) detect keyword substitution errors, and (3) selectively correct substitution errors when N-best string hypotheses are available. The utterance verification method presented here employs a set of verification-specific models that are independent of the models used in the recognition process. The verification models are trained using a discriminative training procedure that seeks to minimize the verification error by simultaneously maximizing the rejection of non-keywords and misrecognized keywords while minimizing the rejection of correctly recognized keywords. The error correction is performed by reordering the hypotheses produced by an N-best recognizer based on a UV confidence score.
In order to detect laryngeal pathologies by acoustic analysis of the speech wave, several acoustic features have been proposed in literature. The evaluation of their individual performance in discriminating between normal and pathological speakers has been done either qualitatively by inspection or by classical statistical analysis. It seems that the evaluation of a whole set of acoustic features has never been done without explicit reference to a decision model. On the other hand there are strong indications that the distributions underlying the features space are multimodal. In order to evaluate quantitatively their discrimination power without reference to a statistical model, we performed a clustering analysis on a set of six well-known acoustic features. The features were computed from the acoustic signal of the steady vowel /a/ uttered by 37 normal speakers and by 24 dysphonic speakers. The results confirm the good performance of the pitch and amplitude perturbation measures reported elseewhere. On the contrary, the acoustic features solely defined on the residue signal show poor discrimination ability.
In this paper, we propose an MLP/HMM hybrid model in which the input feature vectors are transformed by nonlinear predictors using multilayer perceptrons (MLPs) assigned to each state of a Hidden Markov Model (HMM). The prediction error vectors in the states are modeled by Gaussian mixture densities. The use of a hybrid model is motivated from the need to model the prediction errors in the conventional neural prediction model (NPM) where the prediction errors are variable due to the effect of varying contexts and speaker identity. The MLP/HMM hybrid model is advantageous because frame-correlation in the input speech signal is exploited by employing the MLP predictors, and the variabilities in the prediction error signals are explicitly modeled. We present the training algorithms based on the maximum likelihood (ML) criterion and discriminative criterion for minimum error classification. Experiments were done on speaker-independent continuous speech recognition. By ML training of the hybrid model, we obtained a much better performance than a conventional NPM which does not explicitly model the prediction error signals. By training with the discriminative criterion, confusion among different models was significantly reduced and word error rate was reduced by 56% compared with the ML training.
In this paper, methods are proposed to detect objects in complex scenes using statistical global appearance based models. In our approach, the standard eigenspace representation of a training image database and a priori non- Gaussian hypotheses are brought together in a Bayesian framework. This work unifies standard (appearance- based) detection methods already proposed in the literature and leads naturally to the definition of a new family of probabilistic detectors. It allows the use of more general a priori assumptions about the distribution on the eigenspace and its orthogonal. Experimental results are illustrated with ROC (Receiver Operating Characteristic) curves and show the major improvement of our Bayesian approach in comparison to the standard methods that have been the reference up to now [2, 14].
A paradigm for automatic speech recognition using networks of actions performing variable depth analysis is presented. The paradigm produces descriptions of speech properties that are related to speech units through Markov models representing system performance. Results in the speaker-independent recognition of isolated letters and digits are presented.
This paper presents a work on a comparison between a user model and user's behavior based on three premises. First, any system includes a representation of its users. Second, the external representation of users in a system is related to how the system is used by users. Third, knowing how to use the system depends on the task context. For making context explicit in order to use it, we use contextual graphs to capture the effective behaviors of users in an activity of information retrieval on a scientific website. This approach allows dealing with a system that is able to incrementally acquire new knowledge from the user and learn new practices when the system is in a situation of failure.
As is widely known, most people who suffer from a defect in their auditory system perceive a valuable amount of speech information by lip-reading. This entire paper presents some results of a structural analysis of lip contours for different speakers during the articulation of different isolated vowels. The method used is based on the Fourier analysis of contourfunctions. Such an analysis is of great interest with respect to the artificial generation and real-time recognition of visual speech patterns.
Psychological theories of natural language processing have usually assumed that the sentence processor resolves local syntactic ambiguities by selecting a single analysis on the basis of structural criteria such as Frazier's (1978) “minimal attachment.” According to such theories, alternative analyses will only be attempted if the initial analysis subsequently proves inconsistent with the context. (See also Ferreira & Clifton, 1986; Ford, Bresnan, & Kaplan, 1982; Rayner, Carlson, & Frazier, 1983). An alternative hypothesis exists, however: If sentences are understood incrementally, more or less word-by-word (Marlsen-Wilson, 1973, 1975), then syntactic processing can in principle exploit the fact that interpretations are available, using them “interactively” to select among alternative syntactic analyses on the basis of their plausibility with respect to the context. The present paper considers possible architectures for such incremental and interactive sentence processors, and argues for an architecture in which alternative analyses are initially offered in parallel, and are then discriminated among by immediate appeal to the comprehension process under a selective or "weak" interaction, as opposed to directive or "strong" interaction. We note that such an architecture does not compromise the modularity hypothesis of Fodor (1983) in any way. We review experimental evidence which has been claimed to show that human sentence processing is non-interactive and mediated by purely structural criteria. New results are presented which appear incompatible with the structuralist proposal, and which support the interactive hypothesis. We suggest reasons why the earlier contrary results may be discounted, and conclude that the human sentence processing mechanism resolves modifier-attachment ambiguities by recourse to higher-level contextual and referential information under the weak interaction.
This paper first describes recent trends of ASR and TTS telecommunications applications in Japan. ASR applications focus on public services such as operator automation, operator assistance, voice-activated information retrieval, and voice dialing. Major TTS applications include information service by voice and e-mail reading. The usage of ASR and TTS functions is expected to dramatically increase in the near future with the penetration of handy and mobile telephone terminals; hot topics are text broadcasting and digital communication. Secondly this paper describes NTT's experimental interactive system featuring (1) highly accurate speaker independent and large vocabulary speech recognition based on context-dependent accurate acoustic phoneme HMM models trained with speech data from more than 10,000 speakers collected over telephone network, (2) high quality text-to-speech synthesis that generates speech by concatenating triphone-context-dependent waveform segments, (3) software-based configuration that requires no special hardware except a PC equipped with a sound board and a voice modem, and (4) easy and rapid prototyping which enables the developer to build a system by writing some types of service scenarios.
In this paper, a general class of “single-hidden layered, feedforward” Artificial Neural Network (ANN) based adaptive non-linear filters is proposed for processing band-limited signals in a multi-microphone sub-band adaptive speech-enhancement scheme. Initial comparative results achieved in simulation experiments using both simulated and real automobile reverberant data demonstrate that the proposed speech-enhancement system employing ANN-based sub-band processing is capable of outperforming conventional noise cancellation schemes.
This paper is devoted to the problem of the selection of those speech signal features which are relevant for automatic speech recognition. A comparison of the features was performed using a non parametric (template matching) model of an isolated word recognition system. The experiments were based on Serbo-Croat digits spoken by 109 speakers. The recognition accuracy of each of the speech signal features was established. The structure of recognition errors and their causes was also studied in relation to the test vocabulary.
Planning under uncertainty techniques are hard to apply to autonomous robotics problems: they require a tedious modeling effort when dealing with large state spaces. Factored representations, using state variables, are more compact, but they cannot cope very efficiently with variables that can take a large number of values, e.g. the robot localization variables. Search and rescue problems combine these with mission variables. We propose a generic hierarchical abstract model in order to simplify the modeling stage for robotics planning problems with action uncertainties. An algorithm automatically instanciates our abstract model, which is shown and assessed on several instances of search and rescue autonomous rotorcraft problems.
The primary goal of this paper is to propose a new learner for boosting algorithms, namely least general generalization. First experiments conducted on benchmarks show that ADABOOST boosting least general generalization obtains smaller error than reference systems.
This paper examines the psychological reality of feature representations. Given the non-cognitive linguistic and the low-level phonetic approaches, it is a priori an open question whether the psycholinguistic representation borrows from both, from either or from neither of these. As a test case, a point of divergence between phoneticians and linguistics has selected for scrutiny. While the former consider [f] and [v] to be labiodental fricatives, the latter regard them as underlying bilabials or simply labials. To determine if [f] and [v] behave psycholinguistically like bilabial or labiodental segments, the interactions between the fricatives at issue and the other phonemes are analyzed in two large corpora of slips of the tongue in English and German. The results indicate that [f] and [v] are indistinguishable from bilabials, that is, they are as likely to substitute for other bilabials and as unlikely to replace non-bilabials in the way that [p], [m] or [w] do. Three hypotheses are put forward in an attempt to come to grips with this finding. According to the bilabialness-only and the labialness-only hypotheses there is no node for labiodentalness in the processing network. The rival account starts from the assumption that features are represented as vectors in a space coordinate system. In this concept, there is a vector for bilabialness as well as one for labiodentalness. It is claimed that these vectors are closer to each other than their actual phonetic distance would imply. There is less empirical support for the vector hypothesis than for the bilabialness-only one. It is concluded that phonetic and psycholinguistic feature representations need not match.
Dynamic Time Warping (DTW) and Vector Quantisation (VQ) techniques have been applied with considerable success to speaker verification. It is standard practice to use these techniques to calculate a single distance score, and threshold this value to produce a verification decision. In this paper we examine applying a statistical weighting to a number of parameters extracted using the DTW warp path and VQ decision mechanisms. Results are presented which show that the additional parameters extracted encode further speaker specific information, and can be used to improve upon the speaker verification performance of the baseline systems. The application of a distance normalisation technique, which involves comparing DTW or VQ scores for the claimed identity against other speakers, is also investigated. Speaker verification results for baseline and enhanced DTW and VQ systems are reported for a population of 42 speakers.
The problem of speech modeling for generating stressed speech using a source generator framework is addressed in this paper. In general, stress in this context refers to emotional or task induced speaking conditions. Throughout this particular study, the focus will be limited to speech under angry, loud and Lombard effect (i.e., speech produced in noise) speaking conditions. Source generator theory was originally developed for equalization of speech under stress for robust recognition (Hansen, 1993, 1994). It was later used for simulated stressed training token generation for improved recognition (Bou-Ghazale, 1993; Bou-Ghazale and Hansen, 1994). The objective here is to generate stressed perturbed speech from neutral speech using a source generator framework previously employed for stressed speech recognition. The approach is based on (i) developing a mathematical model that provides a means for representing the change in speech production under stressed conditions for perturbation, and (ii) employing this framework in an isolated word speech processing system to produce emotional/stressed perturbed speech from neutral speech. A stress perturbation algorithm is formulated based on a CELP (code-excited linear prediction) speech synthesis structure. The algorithm is evaluated using four different speech feature perturbation sets. The stressed speech parameter evaluations from this study revealed that pitch is capable of reflecting the emotional state of the speaker, while formant information alone is not as good a correlate of stress. However, the combination of formant location, pitch and gain information proved to be the most reliable indicator of emotional stress under a CELP speech model. Results from formal listener evaluations of the generated stressed speech show successful classification rates of 87% for angry speech, 75% for Lombard effect speech and 92% for loud speech.
The problem of quantizer design for detection or classification has a long history, with classical contributions by Kassam, Poor, Picinbono, Bucklew and others. The goal was to design a quantizer such that a detection rule based on the quantized information was optimized. During recent years an alternative approach has been developed which seeks to jointly optimize quantization and classification by incorporating the Bayes risk resulting from the quantizer into the quantizer optimization. In this paper the general classical approach of Picinbono and Duvaut is compared contrasted with the joint approach and illustrated by a simple example.
This paper introduces MAS4AT, a cooperative and self-adaptive multi-agent system for abnormal behaviour detection and alert triggering in maritime surveillance. MAS4AT is designed and implemented in the context of the I2C project, a FP7 European project in which we are involved, which aims at implementing a new generation of maritime surveillance system able to permanently track and monitor all type of ship tracks in vulnerable trading lanes in order (i) to detect abnormal ships behaviours, (ii) to analyse it and, (iii) to trigger alerts if these behaviours correspond to threatening situations. This paper presents I2C and then focuses on MAS4AT and its learning abilities by reinforcements.
A series of experiments was performed to determine how the duration of a word spoken in a sentence is influenced by: (1) the grammatical category to which it belongs, and (2) the position of the word in a constituent. Experiment I contained Noun-Verb homophones (e.g. “I saw the coach…” — noun; “I saw him coach…” — verb), in sentences matched for phonetic environment and stress pattern. Results support the notion that Nouns are longer than Verbs in typical sentences. The results of Experiments II and III provided support for a lengthening account based on constituent-final position, while ruling out an account based on a debatable deletion site. Results of Experiment IV generalized the account of constituent-final lengthening for Nouns and Verbs to additional major categories, by comparing the duration of the phrase-initial Adjective two with the phrase-final Adverb too. Finally, Experiment V tested the distinction between minor and major categories. Results demonstrated that the Preposition to is shorter than the Adjective two by approximately 50 percent. Taken in sum, these findings indicate that it is sufficient to make a binary distinction between major and minor categories for purposes of a theory of speech timing and speech synthesis. Durational effects traditionally ascribed to differences within the class of major categories can be accounted for solely in terms of constituent boundaries, already required in the theory to account for three other classes of phenomena.
The problem of singing voice extraction from mono audio recordings, i.e., one microphone separation of voice and music, is studied. The approach is based on a priori probabilistic models for two sources, more precisely on Gaussian Mixture Models (GMM). A method for model adaptation to the characteristics of the mixed sources is developed and a comparative study of different models and estimators is performed. We show that the adaptation of the model of music from the non-vocal parts of songs yields good results in realistic conditions.
The selection of the “best” transcriptions is accomplished according to different criteria. The Frequency criterion chooses the k most frequent transcriptions in the set of all phonetic decoding of that word, while the Maximum Likelihood (ML) criterion chooses the k most likely ones. With the two criteria k is the same whatever the word is, and each of the k variants “describes” all the training utterances of the word. A partition procedure, which determines the “optimal” number of transcriptions for each word, is then investigated. This procedure assumes that, in the set of the selected transcriptions, each transcription must “describe” a subset of the utterances of the word. So, the goal is to find the “suitable” transcriptions and to associate each of them to a subset of the pronunciations (utterances). Two iterative algorithms are developed and evaluated, and a compromise between the likelihood and the number of elements of the “optimal” set of transcriptions is studied. Speaker-independent speech recognition experiments showed that the ML criterion outperforms the Frequency criterion and that the performance obtained with the former criterion is comparable to that obtained with reference transcriptions.
This study presents a new predictive method, Separated Linear Prediction (SLP), for spectral estimation of speech. The prediction of the signal value x(n) is computed from its p + 1 preceding samples by emphasising among the previous samples the one which is located next to sample x(n). All the p samples from x(n − 2) to x(n − (p + 1)) are linearly extrapolated with x(n − 1) to obtain p new values, which are used for prediction. Optimisation of the filter coefficients is computed using the autocorrelation criterion as in conventional LP. The SLP-analysis yields an all-pole filter of the order p + 1 with p unknowns in the normal equations. The performance of SLP was compared to conventional LP by analysing vowels produced by two female and four male speakers. Results showed that the proposed method yielded in general more accurate modelling of higher formants, residuals with smaller energy and increased flatness.
In a semi-spontaneous conversational setting, subjects were made to repeat the same correction of one digit in a three-digit sequence consisting of “five” or “nine” followed by “Pine Street”. Articulatory and acoustic signals were recorded by the University of Wisconsin Microbeam Facility for four speakers of American English. By analyzing jaw movements, syllable magnitude and time values were evaluated, to represent the rhythmic organization of the utterance by a linear string of syllable pulses. Preliminary results suggest that not only does the magnitude of the corrected syllable increase by the correction of a digit, but also, in most cases, there is some systematic increase of syllable magnitude both in the corrected digit and other digits in the same utterance, as the same correction is repeated. Considerable difference among different speakers is observed and discussed in terms of syllable magnitude and timing patterns.
The Information Visualization scientific community has solutions to facilitate the navigation within the knowledge bases. The usual object of these actions of visualization and navigation is the graphs. Our goal is to compare techniques of displaying and of handling graphs, on the cognitive contribution which get to the users. We developed a graph (3000 nodes, 10000 edges), bearing on common nouns. This structure is declined according to three handling and views: two graphic modes (coloured and monochrome), and a textual mode (with hyperlinks). This material is proposed to users for an experimental study, observing their preferences and performances on tasks of directed navigation (search for ways in a graph). If the users prefer to browse on a graphic and coloured display, their performances (measured time and number of actions) are not significantly better in this view. This reveals that a combination of two views would be an interesting solution: a global and graphic representation of the handled structure, coupled with a local, textual and more detailed representation of the zone of interest of this same structure.
The use of colour in computer vision has received growing attention. This paper gives the state-of-art in this subfield, and tries to answer the questions: What is color? Which are the adequate representations? How to compute it? What can be done, using it? Towards that goal, we make a deep and up-to-date review of the existing littérature on this subject, we outline the important research directions and issues, and we attempt to evaluate them.
We present in this article a self-calibration method for uncalibrated array of sensors. The proposed algorithm estimates the unknown sensors gain and phase. The originality of our approach is in the consideration of non circularity of the impinging signals. We show that this method works even in the case where the number of sources is larger than the number of sensors. Simulation examples are processed in order to show the behaviour of our algorithm, in terms of convergence speed and estimation accuracy.
We describe the broad phonetic classification and segmentation of continuous speech.
This paper presents a generic approach for designing on-line handwritten shapes recognizers. We present in detail our system and make the link between our models and more standard statistical models such as Hierarchical Hidden Markov Models and Dynamic Bayesian Networks. We then evaluate fundamental properties of our approach: learning from scratch any symbol, learning from very few training sample. We show experimentally that, using our approach, one can learn both a state-of-the-art writer- independent recognizer for alphanumeric characters, and a writer-dependent recognizer working with any twodimensional shapes that learns a new symbol with a few training samples and requires very few machines resources.
If natural rules in phonology, such as the rule which deletes a word final consonant before a consonant, are frequently found in unrelated languages, it must be because they tap universal features of production and/or perception. The present experiment employed a learning task to see whether naive subjects have a predisposition for the natural rule as opposed to its converse (consonant deletion before a vowel). Then three novel adjectives were combined with each of the four nouns, following the natural rule for one group of Ss, the unnatural rule for the other. The Ss learning the unnatural corpus had a strong tendency to give natural responses, whereas the converse was not true. Consequently they made many more errors en route to mastery than their natural counterparts, even when the operative rule was displayed on the first trial by presenting in turn each adjective with its four following nouns. It appears that our Ss had implicit knowledge of the natural rule, even though it does not operate to any significant extent in English.
This paper deals with neural network modeling for time series analysis or regression. Based on recent results about the least-square estimation for non-linear time series, we propose a complete andfeasible methodology for both parameter estimation (learning process) and model selection (architecture selection). In particular, we solve the pruning problem for multilayer perceptron models with a stepwise search method by using a BIC criterion which is proved to be consistent.
In this paper, we present the concept of speaker-specific mapping for the task of speaker recognition. The speaker-specific mapping is realized using a multilayer feedforward neural network. In the mapping approach, the aim is to capture the speaker-specific information by mapping a set of parameter vectors specific to linguistic information in the speech, to a set of parameter vectors having linguistic and speaker information. In this study, parameter vectors suitable for speaker-specific mapping are explored. Background normalization for score comparison and network error criterion for frame selection are proposed to improve the performance of the basic system. It is shown that removing the high frequency components of speech results in loss of performance of the speaker verification system. For all the 630 speakers of the TIMIT database, an equal error rate (EER) of 0.5% and 100% identification is achieved by the mapping approach. On a set of 38 speakers of the dialect region “dr1” of NTIMIT database, an EER of 6.6% is obtained.
The system uses diphones and a formant synthesizer chip for speech synthesis. Input to the system is in pseudo-phonetic notation. Intonation contours using a declination line and various rises and falls are generated starting from an input consisting of punctuation and accent marks. The hardware design has resulted in a small, portable and battery-powered device. A short evaluation with users has been carried out, which has shown possibilities for such a device but has also indicated some problems with the current pseudo-phonetic input.
As an introduction to the Special Issue, a tutorial examination of recent developments important to understand current research on reading acquisition is offered. The accent is put on the interrelations between studies of skilled adult performance, of effects of neurological damage and of early reading. The central puzzle of reading research is to identify the causes of the specific difficulties which acquiring literacy appears to entail. The problem has generally been attacked through correlational methods, based on the comparison of better and poorer achievers. The merits and shortcomings of that approach are examined and the need for linking differential studies to a general theoretical conception of the reading process and of its development is emphasized. The line of studies stemming from the hypothesis that a major difficulty in acquiring alphabetic literacy is to manipulate language at the level of phonemic segments is examined, and also the way the results of these studies can be related to current theories of lexical access. The limitations of the approach consisting of deriving hypotheses about development from theories of the adult stage are discussed and illustrated by data from studies of the reading performance of both children with normal reading achievement and developmental dyslexics. Finally, the possibility that sources of acquisition difficulties might be found at levels beyond that of word recognition is discussed.
This paper presents a new approach to automatic recognition of spoken words. After discussing the demands upon appropriate subword units and reporting some experiments in using phone superclasses for word recognition, we will develop technique of robust classification, segmentation, and lexical access, utilizing binary phonetic features as processing units.
Speakers in conversations between humans continually adapt the prosodic and structural aspects of their speech to the perceived needs of their listeners, in terms of judgments about the potentially masking effects of transient and ambient noise levels, and in response to explicit requests by listeners for repetition. Adaptive strategies for repetition include changing such prosodic aspects of utterances as pitch range and mean, intensity mean and overall tempo of speaking, together with intonational re-structuring. Such repetition also deploys re-start strategies based on structural linguistic knowledge.
Bayesian networks are well suited tools for diagnosis tasks. In this paper, we focus on classical algorithms used to build diagnosis systems based on bayesian networks, and more particularly, medical diagnosis systems. We review some methodological questions concerning the representation of probability densities (discretization? use of gaussian models?) and the choice of the adequate structure (naive Bayes structure? learning the structure with the help of an expert or from data?). A case study, thyroid cancer diagnosis, will illustrate those considerations and some implemented algorithms.
The recent research trend towards the use of harmonies/sinusoid based methods, in order to exploit the fine spectral structure of voiced speech, cannot be questioned. This paper discusses the state of the art in this area, both in terms of analysis-synthesis methods and of their application to coding. The key points are: • - Harmonic modelling is a very efficient tool for voiced regions, producing synthetic speech of very high quality, but being simultaneously prone to pitch and voicing errors. The main disadvantage of harmonic coding is the need for an alternative method for unvoiced regions. ATC is a natural choice. In this paper, an 8 kbit/s simulation is presented, using hard switching between harmonic coding and ATC. • - Sinusoid based modelling extends the basic analysis-synthesis framework to unvoiced and transition regions, by removing the constraint that the sinusoids be harmonically related. When it comes to coding, however, it still has many unsolved problems. As a conclusion, some guidelines for future research are discussed.
The work described in the paper was carried out in the SPEAK! project (Speech Generation in Multimodal Information Systems). The aim of the project was to improve the quality of synthesised speech output to be used in dialogue systems as an additional element of multimodal man-machine interfaces. German text and dialogue interaction analysis (theoretical research) has been carried out to predict the tone groups (TGs), the phrase boundaries in sentences and the place of the focus in the phrase. Tone groups represent the general intonation structure of the phrase not taking into account word level intonation. The results of this research are the intonation markers described in (Teich et al., 1997). The CTS synthesiser constructs the main intonation patterns from texts containing these additional markers. This paper describes the research results on German intonation, including the construction of intonation rules, combined with the study on timing adjustments, pause generation for rhythm (both for segmental and suprasegmental levels) for the MULTIVOX-SPEAK! system. Detailed rules and a new tone-group based prosody generation module are also introduced: these have been integrated into the MULTIVOX TTS system. Preliminary evaluation results are also given.
The letter-to-sound transcription rule system is built up taking into account the implicit phonotactic constraints of the Italian language. The system follows the mathematical model called Finite States Automata (FSA), which is generalized and augmented to obtain a simple syntax-directed translation schema. These rules are entirely based on the pure orthographic form of the words, and no level of grammatical knowledge of their classes is considered. Finally, the advantages and disadvantages of the graph-oriented approach will be shown and discussed, together with future trends and improvements.
An algorithm is described which abstracts acoustic parameters of a speech waveform to automatically transcribe sentential stress and pitch movements. The waveform acoustics used are duration, energy and fundamental frequency. The abstractions described aim to isolate the prosodically imposed variations in these parameters. A method of syllabification from acoustic parameters is presented. The prominence of each syllable is determined using the automatic process described and the resultant transcription is compared with a hand-labelled prosodic transcription. The agreement level of 61.6% suggests that acoustic parameters other than those already used by the algorithm may be available to the human labeller.
The performance of speech recognition systems is significantly degraded in the presence of noise. To solve the noise problem, there is a need to reconsider standard approaches by taking into account this new constraint. We first envisage two well-known cepstral representations (parametric and non-parametric) of speech signals and propose a unifying view of both schemes. We introduce a pseudo-autocorrelation domain, which can be interpreted as a “Root-cepstral domain”, and we show how non-parametric cepstral and linear predictive analyses converge to the same optimal solution. Experiments are carried out using an HMM-based isolated word recogniser for speaker-dependent and speaker-independent tasks in car noise environments.
Although Tamil has an indigenous tradition of «grammatical» description — in an extended sense of «grammatical» which encompasses poetics and metrics, along with phonetics, morphology and syntax — which goes back to the first half of the first millenium AD, it was described once again from the 16th century onwards from a new angle (which included an interest for ordinary language) by Christian missionaries who brought with them a Latin model of grammatical description, which they tried to apply, by creative extension. I shall therefore refer to the corpus of their productions as the Grammatici Tamulici, although the earliest among them make use of Portuguese as a metalanguage for the description of Tamil. Not being in the situation of some field linguists, who have to start from scratch when confronted with a language which has never been written, those missionaries progressively discovered that the Latin alphabet, although enriched by several extensions developed for the representation of European vernaculars, was a less efficient tool than the local (Tamil) syllabary for noting down Tamil sounds and words. They also discovered that their capacity to be convincing, in front of their converts, depended in a great part on their adopting the language hierarchies which governed (and still govern) the diglossic Tamil society, as we shall see here in this preliminary exploration covering five authors who were active during a period of ca. 200 years, up to the year 1739.
Edited and Condensed Nearest Neighbor Rules are used in various applications in Pattern Recognition problems. In this study, modified versions of these algorithms are applied to speaker-independent isolated word recognition to select the word templates, as opposed to the clustering techniques. It is shown that the approach improves the recognition rate when compared with clustering, with the disadvantage of being more costly.
A new system for the automatic segmentation and labelling of speech is presented. The system is capable of labelling speech originating from different languages without requiring extensive linguistic knowledge or large (manually segmented and labeled) training databases of that language. Due to the limited size of the neural networks, the segmentation and labelling strategy requires but a limited amount of computations, and the adaptation to a new task can be accomplished very quickly. The system was first evaluated on five isolated word corpora designed for the development of Dutch, French, American English, Spanish and Korean text-to-speech systems. The results show that the accuracy of the obtained automatic segmentation and labelling is comparable to that of human experts. In order to provide segmentation and labelling results which can be compared to data reported in the literature, additional tests were run on TIMIT and on the English, Danish and Italian portions of the EUROM0 continuous speech utterances. The performance of our system appears to compare favourably to that of other systems.
The behavior of real ants while resolving complex problems, that they encounter in their daily life, have inspired attracting algorithms for the resolution of the clustering problem. Most ant clustering algorithms extend the basic model developed by Lumer and Faieta (Lumer et al., 1994) which was inspired by cemetery organization and larval sorting phenomena detected on some ant species. Other algorithms were developed by taken other inspirations from real ants like self-assembling behavior and chemical recognition. In this paper we take inspiration from further biological properties of real ants mainly sensorial communication.
After some preliminary information concerning the success of Dante's Commedia in France during the XVthand XVIthcenturies, this article focuses on the anonymous translation of the Inferno, in alexandrines and in terza rima, now held at the Biblioteca Nazionale Universitaria in Turin (ms. L. III. 17). This translation is currently considered as the most ancient French version of the Commedia. Unfortunately, its transcription, which dates back to the end of the XIXthcentury, is extremely inaccurate, and needs to be replaced by a more reliable critical edition. After a brief summary of previously conducted studies, some preliminary questions are tackled in the next few pages. Firstly, this article provides both textual and iconographical elements that should help date the manuscript whilst identifying the source of the Italian text featured in the Turin codex, which is most likely the starting point for the translation. Secondly, this article deals with the possibility of determining the translator's origins on the basis of some lexical items. Subsequently, its attention focuses on the analysis of a number of translation strategies deployed, which may hopefully pave the way for further contributions on the literary aspects of this work.
A new low delay speech coder is proposed. In order to achieve low delay, the coder is based on the memory dependent vector quantization of the synthesis filter. The transformed samples are considered to be a sequence of Laplacian random variables that are vector quantized in an efficient manner using a geometrical lattice VQ. The coder was tested at a bit-rate of 8625 bit/s and a delay of 8 ms; a good reproduced voice quality was obtained.
F0 (fundamental frequency) control of human speech production is studied by using both a stochastic time series model and a system analysis with a vector autoregressive (VAR) model. We use two-dimensional time series data of F0 (ff0 and sf0) obtained through the transformed auditory feedback (TAF) experimentation system developed by one of the authors. sf0 is extracted from speech data that includes prolonged phonation of the vowel /a/, and the signal ff0 is extracted from frequency-modulated feedback speech by white Gaussian noise. Most of the data had mean-nonstationary characteristics. The stochastic procedure decomposes the mean-nonstationary and other components in one stage without pre-manufacturing the data. The cyclical components around the mean-nonstationary components are assumed to be generated by the VAR model. We can execute a stochastic system analysis using the estimates of the VAR model to analyze the physical characteristics of the data. We also performed a simulation study using the estimates obtained by the model to discover the role of F0 control under the situation in which we completely lost our hearing ability. The results clearly indicate the dynamic properties of F0 control for each segment which occur with each breath taken during a sustained tone.
It has been shown previously that spoonerisms (such as barn door → darn bore) can be elicited by having subjects attempt to articulate a target barn door) preceded by bias items which contain at least the initial phoneme (/d/) of the desired error outcome. Since certain linguistic characteristics of the error outcomes differ from those of their targets, variables which affect only these 'outcome' properties in a systematic way can be shown to be the result of prearticulatory output processes, independent of perceptual 'target' properties. The present study shows that the base-rate of errors produced by the phonetic bias technique can be increased dramatically by adding, to the word-pairs preceding the target, some items which are semantically synonymous to the error outcomes of the target. In this way, it is demonstrated rigorously that semantic bias increases the likelihood of slips of the tongue; which is one of the defining properties of so-called 'Freudian slips'. Implications are discussed.
The study presented in this paper deals with the modelling of extreme emotions occurring in abnormal situations. The aimed application is civil safety and surveillance in the public places in particular. A corpus of fiction (SAFE Corpus) is selected illustrating rich and varied contexts with the presence of extreme emotions, mainly fear. An annotation strategy adapted to the application is then developed, with both generic and specific descriptors. Finally, a detection system of fear emotions based on acoustic cues is implemented to carry out an evaluation. On the one hand the system is robust to context changes. On the other hand, the influence of multimodal annotation is minor. Results obtained with the various protocols are similar: fear is recognized with 67% of success.
The effect of sentence accent, word stress, and word class (function words versus content words) on the acoustic properties of 9 Dutch vowels in fluent speech was investigated. A list of sentences was read aloud by 15 male speakers. Each sentence contained one syllable of interest. This could be a monosyllabic function word, an unstressed syllable of a content word, or a stressed syllable of a content word. A total number of 3465 vowels were segmented from the syllables and analysed. It was found that all three factors mentioned above had a significant effect both on the steady-state formant frequencies (F 1 and F 2) and on the duration of the vowels. Word stress and word class had a stronger effect on the vowels than sentence accent. A listening experiment showed the perceptual significance of the acoustic measurements. It appeared that spectral vowel reduction could be better interpreted as the result of an increased contextual assimilation than as the tendency to centralize. We also studied changes in the dynamics of the formant tracks due to the experimental conditions. It was found that formant tracks of reduced vowels became flatter which supports the view of an increased contextual assimilation. Three simple models of vowel reduction are discussed.
We present in this article a Partial Connection Multilayered Network (PCMN), based on a technique of partial connection between layers. This network can be trained automatically by the gradient back-propagation (GBP) algorithm. A general network based on GBP has been implemented on a ring configuration of the F.P.S. T20 Hypercube. We have made a special effort to ensure an efficient parallelization of this algorithm. This implementation provides us with a large number of possible configurations for our network. In this experiment the network is used to effect isolated word recognition. Results show the advantages of partial connection as against full connection. This technique permits the efficient treatment of temporal information, which is very important in speech processing, unlike image processing. This experiment also permits us to gain a better understanding of network characteristics important for speech processing. Partial connection can introduce both temporal context constraints and some implicit knowledge into the network and may also lead to efficient learning on a small data base. Good recognition results have been obtained.
This corresponds to situations that occur frequently in areas like geophysics, ultrasonic imaging or nondestructive inspection. ARMA representations yield a non-standard state driving noise detection-estimation problem whose resolution is complex and requires great computational efforts. AR representations and the use of multi-pulse coding techniques cannot account for nonminimal phase systems and exhibit the disadvantages of output-error type methods. None of these approaches provide any on-line processing ability. Furthermore, fast modified Chandrasekhar equations can be used for the implementation of this procedure and produce significant savings in computational requirements. Simulation results are satisfactory, and are obtained with less computations than other existing methods.
This paper describes a real time vision system, allowing to localize faces in video sequences and to recognize their identity. These processes are based on combining techniques of image processing and neural network approach. The robustness of this system has been evaluated quantitatively on 8 video sequences. We have tested our model using the ORL database in order to compare performances with other systems. The system has also been implanted on electronic architectures based on dedicated chip ZISC and FPGA. We analyse the algorithm complexity and we present results of hardware implementations in terms of used resources and processing speed.
One of the most highly developed human abilities is communication by speech. Throughout the years, research on speech perception has demonstrated that humans are well adapted to extract highly encoded linguistic information from the speech signal. The sophisticated nature of these capacities and their early appearance during development suggest the existence of a rich biological substrate for speech perception. In the present paper, we describe some of these important capacities and examine research from different domains that may help illuminate the nature of their biological foundations.
Recently, a lot of algorithms minimizing a non-convex energy function have been proposed to solve low level vision problems. Different kinds of relaxation methods are available. The stochastic techniques, such as simulated annealing, asymptotically converge to the global minimum but require a high computational cost. Deterministic relaxation methods which are sub-optimal, give good results and are faster than the stochastic ones. In this paper, we focus on the parallel implementation of two deterministic algorithms for edge detection and image smoothing: the graduated non convexity (GNC) originally proposed by Blake & Zisserman and the mean field annealing (MFA) introduced by Geiger & Girosi and extended to anisotropic compound Gauss-Markov random fields by Zerubia & Chellappa. Both methods are based on a weak-membrane model and both algorithms are inherently serial: each step produces a pixel map which is taken as an input for the next step. For the GNC, we implement a checkerboard version of the successive over-relaxation (SOR) method to minimize the energy. For the MFA, we use an optimal step conjugate gradient descent.
A compensating ability of the articulatory control system for laryngectomized patients was studied. X-rays of the vocal tract and acoustic measurements were carried out on three patients before and after the operation, using the trachea – esophagus bypass. Within two weeks of the operation, the patients produced the Russian vowels /a, u, i/ with formant frequencies closer to the phonetic norm than before the operation. After two years, two patients produced the vowels with normal formant parameters. The acoustical characteristics of speech after the operation were measured on 14 patients. 1 to 2 years after the operation, four patients were able to make voicing–unvoicing distinction. One patient has recovered complete control of the vocal source. The results obtained imply that the adaptation of the articulatory control system to the distorted conditions of articulation and voice generation can be governed, not only by acoustical parameters like formant frequencies, but also by such a complex phonetic element as the voicing cue. The control system has demonstrated its ability to reorganize the activity of articulatory muscles and to transfer the functions of the excised laryngeal muscles to the muscles that had never been used for voice control. The implication of the observed phenomena for the concept of internal model is being discussed.
Learning Classifier Systems (LCSs) are rule-based systems that automatically build their ruleset. Initially, LCSs were dedicated to the modelling of the emergence of cognitive abilities thanks to adaptive mechanisms, particularly evolutionary processes. After a renewal of the field more focused on learning, LCSs have been reconsidered as sequential decision problem solving systems endowed with a generalization property. Finally, much more recently, LCSs have proved very efficient at solving classification tasks, which boosted the field. In this context, the aim of this contribution is to present the state-of-the-art of LCSs, insisting on recent developments, and focusing more on the sequential decision domain than on automatic classification.
Most approaches developed or used in the present work for seabed characterization are based on the use of texture analysis methods. Indeed, sonar images have different homogeneous areas of sediment that can be viewed as texture entities. Generally, texture features are numerous and not all are relevant; an extraction-reduction of these features seems necessary before the classification phase. We present in this work a complete chain for sonar images classification while optimizing the chain steps. We use the Knowledge Discovery in Databases (KDD) process for the chain development. The underwater environment is uncertain, which is reflected on the images obtained from the sensors used for their acquisition. Therefore, it is important to develop robust methods to these imperfections. We solve this problem in two different ways: a first solution is to make robust traditional classification methods, such as support vector machines or ^-nearest neighbors, to these imperfections. A second solution is to model these imperfections to be taken into account by belief or fuzzy classification methods. We present the results obtained using different texture analysis approaches and classification approaches. We use other approaches based on the uncertain theories to overcome sonar images imperfections problem.
In North America, people call the directory assistance operator to find the phone number of a business or residential listing. The directory assistance service is generally maintained by telcos, and it represents a significant cost to them. Partial or complete automation of directory assistance would result in significant cost savings for telcos. Nortel Networks has a product called Automated Directory Assistance System (ADAS) Plus which partially automates this directory assistance function through the use of speech recognition. The system has been deployed all across Quebec, through most of US West and BellSouth. ADAS Plus primarily automates the response to the question “for what city?” through speech recognition. We give details of this speech recognition system and outline its performance in the deployed regions.
Cooperation in work settings goes through communicative interactions where colleagues need to reach a certain level of mutual understanding for coordinating their actions or finding negotiated decisions. Mutual understanding is complex because of the heterogeneity of the participants which makes the interpretations uncertain and unpredictable. We argue here that chronic cooperation is still more complex because the memory of the previous cooperative interactions is an additional source of difference between the cognitive contexts of the participants. We studied the memory of cooperative interactions in four different collaborative work settings. The analysis indicates a massive forgetting of the verbal content and a greater remembering of the relational positionings, interactional structures and emotions.
An overview of a statistical paradigm for speech recognition is given where phonetic and phonological knowledge sources, drawn from the current understanding of the global characteristics of human speech communication, are seamlessly integrated into the structure of a stochastic model of speech. A consistent statistical formalism is presented in which the submodels for the discrete, feature-based phonological process and the continuous, dynamic phonetic process in human speech production are computationally interfaced. This interface enables global optimization of a parsimonious set of model parameters that accurately characterize the symbolic, dynamic, and static components in speech production and explicitly separates distinct sources of the speech variability observable at the acoustic level. The formalism is founded on a rigorous mathematical basis, encompassing computational phonology, Bayesian analysis and statistical estimation theory, nonstationary time series and dynamic system theory, and nonlinear function approximation (neural network) theory. Two principal ways of implementing the speech model and recognizer are presented, one based on the trended hidden Markov model (HMM) or explicitly defined trajectory model, and the other on the state-space or recursively defined trajectory model. Both implementations build into their respective recognition and model-training algorithms a continuity constraint on the internal, production-affiliated trajectories across feature-defined phonological units. The continuity and the parameterized structure in the dynamic speech model permit a joint characterization of the contextual and speaking-style variations manifested in speech acoustics, thereby holding promises to overcome some key limitations of the current speech recognition technology
Much current work in the field of context-aware systems focuses on application- specific solutions and ad-hoc approaches, and the lack of conceptual models for the design of context-aware systems hinders the development of more general and complex systems. Furthermore, it is often unclear what the consequences of early design decisions are for the quality of the final implementation, making the design difficult and leading to mistakes only to be discovered when the system is already implemented. In this article we present a classification of the architectural aspects of developing context-aware systems. We furthermore give a quality framework that describes the consequences of the architectural aspects on the quality of the context-aware system. We demonstrate the usefulness of the framework by modeling the architecture of a context-aware system.
This paper investigates the problem of automatic segmentation of speech recorded in noisy channel corrupted environments. Using an HMM-based speech segmentation algorithm, speech enhancement and parameter compensation techniques previously proposed for robust speech recognition are evaluated and compared for improved segmentation in colored noise. Speech enhancement algorithms considered include: Generalized Spectral Subtraction, Nonlinear Spectral Subtraction, Ephraim–Malah MMSE enhancement, and Auto-LSP Constrained Iterative Wiener filtering. In addition, the Parallel Model Combination (PMC) technique is also compared for additive noise compensation. In telephone environments, we compare channel normalization techniques including Cepstral Mean Normalization (CMN) and Signal Bias Removal (SBR) and consider the coupling of channel compensation with front-end speech enhancement for improved automatic segmentation. Compensation performance is assessed for each method by automatically segmenting TIMIT degraded by additive colored noise (i.e., aircraft cockpit, automobile highway, etc.), telephone transmitted NTIMIT, and cellular telephone transmitted CTIMIT databases.
Experiments and results of the application of the Approximating and Eliminating Search Algorithm (aesa) to multi-speaker data are reported. Previous (single-speaker) results had already shown that the performance (speed) of the aesa remains greatly insensitive to increasing the size of the dictionary, while a very strong (exponential)_tendency to higher performance is exhibited as the test utterances are close to their corresponding prototypes. Following these results we show in this paper that, by increasing the number of tokens included in dictionaries with multiply represented words, a simultaneous reduction can be achieved in both the error-rate and the number of distance computations required.
Training algorithms for natural speech recognition require very large amounts of transcribed speech data. Commercially distributed books on tape constitute an abundant source of such data, but it is difficult to take advantage of it using current training algorithms because of the requirement that the data be hand-segmented into chunks that can be comfortably processed in memory. In order to address this problem we have developed a training algorithm which is capable of handling unsegmented data files of arbitrary length; the computational requirements of the algorithm are linear in the amount of data to be processed and the memory requirements are constant.
This paper reviews the fundamental concepts of Linear Prediction (LP) and Maximum Entropy (ME) spectral analysis, and elucidates the reasons for their practical importance in the world of real signals. Subsequently, the powerful principle of Minimum Cross-Entropy (MCE) spectral analysis is introduced. MCE permits the incorporation of prior information into signal analysis. In a new approach to speech signal analysis, application of the MCE principle reduces the average number of predictor coefficients (poles) that have to be specified per time frame for a given spectral resolution by relying on prior spectral information. Such prior spectral information may be given by glottal source and lip radiation characteristics, microphone and transmission frequency responses, and spectral information from preceding time frames — particulary during steady-state or slowly-varying portions of a speech utterance. This paper stresses general principles rather than computational details.
This article presents a comparative study of four approaches of school enrolment of allophone pupils into French colleges. Our problematic consists in examining the elements that facilitate the success of allophone pupils, including how the way they are integrated affects their ability to earn. We shall analyze their management in the target devices from two different standpoints: the intrinsic interest of each device and how they articulate with the 'ordinary class'. This qualitative research, which consists in semi-directive interviews carried on with allophone students and teachers, points to the conclusion that the only really personalized and adapted approach for allophones students lies in the specific reception devices
In order to analyse and interpret speech signals, different time-frequency representations are used (e.g. spectrogram, Wigner-Ville distribution, wavelets). In this paper we construct within Cohen's class of time-frequency distributions the distribution that is optimally suited for the representation of speech signals. Thereby we take advantage of the special time-frequency structure of speech expressed in the Elementary Waveform Speech Model (EWSM, d'Alessandro, 1990). As an application we present an algorithm that extracts a point pattern in the time-frequency plane out of the speech signal using the optimized distribution. Thus we get a very simple representation of the speech signal that is well interpretable both for non-stationary and for stationary speech segments. Furthermore this representation could serve as a base for further analysis (e.g classification).
In many speech research institutes, work is in progress which aims at the collection and annotation of a large number of speech utterances. As the complexity of these databases increases, it becomes important to pay attention to certain aspects of database management and specifically to database access. In this paper, a user-friendly formalism is presented which may be used to phrase queries for speech databases. This formalism is domain-specific in the sense that the speech researcher may phrase a query in acoustic-phonetic terms, without first having to become a computer programmer.
This paper explores the dynamics of stock market from a behavioral perspective using a multi-agent simulation. The aim of this paper is to study the behavior of investors in the stock market to find a model as close as possible to reality. The main problem is to understand, through a novel model which includes behavioral and cognitive attitudes of the investor, the running of the market and determine the sources of his complexity. Simulation experiments are being performed to observe stylized facts of the financial time series. These experiments show that representing a behavioral model allows to observe emergent socioeconomic phenomena.
The use of hypothesis verification is recurrent in the model-based recognition literature. When data involved in the computation of the pose are noisy, the pose is inaccurate and difficult to verify, especially when the objects are partially occluded. It consists in a recursive multi resolution exploration of the pose space, discarding outliers in the match data while the search is progressing. Numerous experimental results are described.
Herbert Simon was the inventor of the “Scientific Discovery”. This domain of research whose main figures are Pat Langley, Jan Zytkow and Douglas Lenat, aims at a rational reconstruction of old scientific discoveries in a way which could be reproduced with a computer. The advantages are twofold. On the one hand, in the epistemological field, it models the scientific activity up to its more enigmatic appearances. On the other hand, from a practical point of view, it opens an exciting area in which the computer helps men in their quest of knowledge. Here is, in memory of Herbert Simon, a glimpse of the scientific discovery.
The mirror for princes known as the Naṣīḥat al-mulūk of al-Māwardī, probably a tenth-century text, is replete with references to sources identified by the author as “Indian”. A large number of these texts also appear in the so-called Waṣiyyat Arisṭāṭālīs li-l-Iskandar; some examples find parallels in Kalīla wa-Dimna and Bilawhar wa-Būḏāsaf. These coincidences raise several possibilities: first, that the author's “Indian” source represents a work of Indic background, translated from Sanskrit or another Indian language into Arabic, probably at the time when the Barmakids were sponsoring such translations in significant numbers; secondly, that it was rendered from an Indian language into Middle Persian in the Sasanian period and from that language into Arabic in the early centuries of the Islamic era; thirdly, that the text was composed in a non-Indian language, probably Middle Persian, and acquired a “forged” Indian genealogy in a parallel to the numerous spurious Greek attributions (a category that would subsequently include the pseudo-Aristotelian testament). The article addresses these three possibilities, and, on the basis of textual and contextual considerations, suggests that at the present stage of research, it is the second that seems most likely.
This paper deals with the problem of detecting multiple narrow-band sources and estimating their angles of arrival using the signals received on a large moving array in underwater acoustic passive listening. This result provides the antenna with its maximum gain and permits to reconstruct the line-array antenna underlying most array processing methods. Afterwards we can estimate the number of secondary sources and their location. After recalling the AIC and MDL detection criteria, we briefly describe the MUSIC algorithm that we use to identify the sources supposed narrow-band and far from the reception array. Finally, we apply this processing to experiment data. The results are compared with those, provided by the two-dimensional- Fourier Transform. This spatio-temporal representation points out that knowledge of the geometry of the receving antenna is very important.
In Middle French, the referential continuity and the choice of anaphoric expressions in the anaphoric chain - nominal, pronominal forms or zero anaphora supported by verbal morphology - are determined, in narrative and discourse, by five syntacticalsemantic rules: one, of “referential competition”, three, “valentiello-référentielles” and one, “synctactico-valentielle”. This study examines their frequency of application on a Middle French prose translation, namely that of Boccaccio's Decameron translated by L. de Premierfait (1411–1414), which may allow us to observe the influence the source language has on its medieval translation.
In this paper, we present a brief survey on the use of different types of Markov models in writing recognition. Recognition is done by a posteriori pattern class probability calculus. This computation implies several terms which, according to the dependency hypotheses akin to the considered application, can be decomposed in elementary conditional probabilities. Under the assumption that the pattern may be modeled as a uni- or two-dimensional stochastic process (random field) presenting Markovian properties, local maximisations of these probabilities result in maximum pattern likelihood. We have studied throughout the article several cases of subpattern probability conditioning. Each case is accompanied by practical illustrations related to the field of writing recognition.
This paper provides an overview of an approach for both predicting the performance of a previously characterised speech recogniser for particular applications and also for analysing the effect of speech variability on performance. The method also potentially provides a principles way of developing a database for assessment purposes. The method, Recogniser Sensitivity Analysis (RSA) was developed by Logica within the UK Alvey Speech Technology Assessment (STA) project MMI/132. The method relies principally on the characterisation of the many sources of variability in speech recogniser performance by a small number of measurable parameters. The experiments carried out to determine the validity of this approach and the results obtained are described. The applicability of the method for predicting likely field performance of a recogniser in a particular application as well as the influence of particular within- and between-speaker variations on recogniser performance are discussed. Preliminary results indicate a significant correlation between certain speech parameter values and recogniser performance. The paper concludes with a discussion on future directions for extending RSA.
Four speech coders in the CELP (Code Excited Linear Predictive) family are described. By replacing the long term prediction with a self excitation sequence (adaptive codebook) as well as substituting a stochastic multipulse, or sparse codebook for the commonly used codebook of Gaussian noise, the speech quality is improved. The coders are fully quantised at 7.0 and 5.0 kbit/s, interesting bit-rates for such possible applications as the half rate GSM system and the INMARSAT-M service. The performance of the coding schemes are evaluated by a formal listening test and presented by their Mean Opinion Scores (MOS). For the coder with maximum performance with respect to quality and complexity, a set of tolerable bit error rates (BER) are given. It is shown that the presence of acoustic background noise does not influence the coder quality, and that bit errors in this case will be partially masked by the background noise giving the coder a high degree of robustness. Considering the performance in the presence of bit errors and background noise, the coder seems to be suitable for use in a mobile communication system using satellite links.
When Coptic grammars appeared for the first time (middle of the thirteenth century), only the dominant Arabic linguistic tradition could serve as a point of reference. However, Coptic, which was losing its status of vernacular, does not belong to the same family as Arabic. Through some typical samples related to script, phonology, and morphology, this paper tempts an insight into the way the Arabic conceptual and terminological apparatus was applied and adapted to Ancient Egyptian in its final stage. We show that in general medieval Coptic grammarians did in fact succeed in their undertaking. Whatever deficiency one may find, this should not necessarily be imputable to the linguistic tradition that acts as a model, but rather to the external conditions affecting the very intellectual activity of the protagonists.
Since 1976 work has been carried out towards the development of a microcomputer-controlled set of equipment for displaying pitch contours of sentences. It was found that, for different groups of learners (Dutch learning English, Turks learning Dutch), subjects receiving audio-visual feedback performed better than those receiving only auditory feedback. No differential learning effects were found for L2-proficiency level or age.
The paper describes a simulation methodology for radar targets in marine environment. Our approach is based on a new model, called "Scattering Center Set Unified Representation", which is able to approximate the backscattered radar target echo for any aspect angle. This model is fast to calculate and has the advantage to take into account the partial or total target concealing due to the sea waves. It associates to any target aspect a scattering center set. An amplitude map accounts for each scattering center anisotropy and geometrical visibility. A virtual model is then used for describing the target motion and its concealing by the sea waves. The influence of the sea clutter is also taken into account. The radar signatures used in our simulations have been measured in the anechoic chamber of ENSIETA for four scale-reduced naval targets. The paper also presents some imagery and classification results, which are aimed to illustrate the other side of the naval target characterization problem.
This paper describes the essential speech processing techniques for interactive voice applications in the telecommunications field. These techniques include speech recognition and speech synthesis, both of which aim to make interactive speech communications between man and machine more natural. Keyword spotting, background noise effects reduction, and speakers and/or telephone adaptation techniques are considered essential in speech recognition in order to allow a more natural voice input as well as an adequate robustness against environmental variabilities. In the area of text-to-speech synthesis, we propose a rule-based synthesis method applicable to the Japanese language, aiming to produce high quality speech. The commercial system ANSER of a former project is also described as an example of an interactive speech processing system. Finally, a recently developed speech recognition server which includes a vocabulary-flexible recognition function is described.
This paper describes two algorithms for separating two overlapping speech signals. Both rely heavily on accurate measurements of the pitch of the target voice. The first uses a single microphone, and an important feature is the exploitation of the onset of a voiced sound as an aid to the extraction of its pitch in the presence of interference. The second uses two microphones, and an important feature is that it also makes use of the direction of the target voice.
Making a self-repair in speech typically proceeds in three phases. The first phase involves the monitoring of one's own speech and the interruption of the flow of speech when trouble is detected. From an analysis of 959 spontaneous self-repairs it appears that interrupting follows detection promptly, with the exception that correct words tend to be completed. Another finding is that detection of trouble improves towards the end of constituents. The second phase is characterized by hesitation, pausing, but especially the use of so-called editing terms. Speech errors induce other editing terms than words that are merely inappropriate, and trouble which is detected quickly by the speaker is preferably signalled by the use of 'uh'. The third phase consists of making the repair proper. The linguistic well-formedness of a repair is not dependent on the speaker's respecting the integrity of constituents, but on the structural relation between original utterance and repair. A bi-conditional well-formedness rule links this relation to a corresponding relation between the conjuncts of a coordination. It is suggested that a similar relation holds also between question and answer. In all three cases the speaker respects certain structural commitments derived from an original utterance. It was finally shown that the editing term plus the first word of the repair proper almost always contain sufficient information for the listener to decide how the repair should be related to the original utterance. Speakers almost never produce misleading information in this respect. It is argued that speakers have little or no access to their speech production process; self-monitoring is probably based on parsing one's own inner or overt speech.
Les entrées et les sorties ne sont pas des phénomènes indépendants dans les systèmes interactifs en général et plus particulièrement lorsque l'on considère le cas de l'interaction multimodale. Nous présentons les résultats issus d'une expérimentation de type «Magicien d'Oz» qui suggère que les modalités de sortie, utilisées par un système multimodal, ont une influence sur les modalités d'entrée effectivement employées par une large catégorie d'utilisateurs du grand public. Certains sujets ont cependant une modalité favorite et ne sont donc pas influençables. En effet ce type d'environnement ne requiert pas de connaissances en informatique et peut donc être utilisé par des personnes du grand public. Les expériences ont aussi fait ressortir que la modalité d'expression orale est la favorite lorsqu'on se situe dans une pièce intelligente, ceci pour une large proportion des sujets, sauf si la modalité graphique est utilisée par le système lui-même.
The creation and evolution of the 16 kbit/s Low-Delay CELP (LD-CELP) speech coding algorithm (CCITT Recommendation G.728) represents an intensive research effort from 1988 to 1992. In this paper, we give a historical overview of this four-year effort, with emphasis on discussions of technical merits of many alternative algorithmic techniques we investigated. As a part of such discussions, we explain why we put sume of these techniques in the final G.728 coder and left the others out. It is hoped that this paper illustrates how the G.728 algorithm was created out of very simple initial concepts, and then modified and improved little by little under real-time implementation constraints, until the final algorithm fully met the performance requirements that were seemingly impossible at the outset.
The Philips automatic telephone switchboard and directory information system PADIS provides a natural-language user interface to a telephone directory database. Using speech recognition and language understanding technologies, the system offers phone numbers, fax numbers, e-mail addresses, and room numbers as well as direct call completion to a desired party. In this paper, we present the underlying probabilistic framework, the system architecture, and the individual modules for speech recognition, language understanding, dialogue control, and speech output. In addition, we report results on performance and user behaviour obtained from a field test in our research lab with a 600-entry database. We derive a new maximum-a-posteriori decision rule which incorporates database knowledge and dialogue history as constraints in speech recognition and language understanding. It has improved speech understanding accuracy by 19% (in terms of concept error rate), and reduced attribute substitution errors (e.g. recognition of a wrong name) by 38%. The decision rule is implemented in a multi-stage approach as a combination of state-of-the-art speech recognition, partial parsing with an attributed stochastic context-free grammar, and an N-best algorithm which is also described in this paper.
The project of a of «mises en prose» database meant to update Georges Doutrepont's famous work (1939) was launched at 3rd International Congress of the AIEMF (Gargnano, May 2008); this paper outlines the initative in question, presents the form which will serve as a template to our collaborators, as well as two analytic subject entries: Jean Wauquelin's Manequine and anonymous Belle Hélène de Constantinople.
Many real-world networks can be represented as large graphs. Reducing the complexity of a graph so that it can be easily interpreted by the human eye is a valuable help to understand and analyze this type of data. We compare two approaches to grouping of nodes in communities and propose a multi-scale interactive visualization of large graphs based on these hierarchical classifications of nodes that allow us to represent these graphs in a legible and interpretable way. We then apply our methodology to a network of French-speaking weblogs to quickly illustrate the advantages and disadvantages of this approach.
On the background of mobile, ubiquitous, and pervasive applications, context determination and assignment is a necessary factor to provide IT solutions suited to a user and the user's current situation. In this paper, context is seen as n-ary relationship. Context gets embedded into ontologies, which are used to structure application specific knowledge. We present an integrative, case-based modelling approach for context and context management. We discuss the incorporation of context-based reasoning and explanation. And finally, we show how to apply our approach for trust management.
This is an introductory study and critical edition of a short treatise on Latin morphosyntax, as contained in MS London, B.L., Add. 10352. As a handbook meant for young students supposed to have studied Donatus' Ars minor, this anonymous Latin 'catechism' addresses three main subject areas: constructio, regimen and congruitas. The treatise is especially interesting for its linguistic terminology (first attestations and/or technical meanings).
Modeling and simulation have long been dominated by equation-based approaches, until the recent advent of agent-based approaches. To curb the increasing complexity of models resulting from this new approach, the trend has been to oversimplify the models. Some more descriptive models have still been developed for various phenomenons, but the cognition of agents is too often neglected, despite its great importance in some fields, such as Social and Human Sciences. The solution that we put forward in this paper is to use BDI agents. We will show that this is an expressive, realistic yet simple paradigm that thus offers numerous benefits to agent-based simulation.
We propose a robust recursive procedure, based on a weighted recursive least squares (WRLS) algorithm with variable forgetting factor (VFF) and a quadratic classifier with sliding training data set, for identification of non-stationary autoregressive (AR) model of speech production system. Experimental evaluation is done using the results obtained by analyzing speech signal with voiced and mixed excitation frames. Experimental results have shown that the proposed robust recursive procedure achieves more accurate AR speech parameter estimates and provides improved tracking performance.
This paper proposes a new approach for the color display of multispectral/hyperspectral images. The color representation of such data becomes problematic when the number of bands is higher than three, i.e. the basic RGB (Red, Green, Blue) representation is not straightforward. Here we employ a technique that uses a segmentation map, like an a priori information, and then compute a Factorial Discriminant Analysis (Fischer analysis) in order to allow, at best, a distribution of the information in the color space HSV (Hue, Saturation, Value). The information collected from the segmentation map (where each pixel is associated with class) has been shown to be advantages in the representation of the images through the results obtained on increasing size image collections in the framework of astronomical images. This method can easily be applied to other domains such as polarimetric or remote sensing imagery.
Information extraction is driven by an Ontological and Terminological Resource and uses patterns and identification of domain terms and their variations. It was integrated into an assistant, which helps experts to populate the database.
The initial conception of a model-based analysis synthesis image coding (MBASIC) system is described and a construction method for a three-dimensional (3-D) facial model that includes synthesis methods for facial expressions is presented. The proposed MBASIC system is an image coding method that utilizes a 3-D model of the object which is to be reproduced. An input image is first analyzed and an output image using the 3-D model is then synthesized. A very low bit rate image transmission can be realized because the encoder sends only the required analysis parameters. Output images can be reconstructed without the noise corruption that reduces naturalness because the decoder synthesizes images from a similar 3-D model. In order to construct a 3-D model of a person's face, a method is developed which uses a 3-D wire frame face model. A full-face image is then projected onto this wire frame model. For the synthesis of facial expressions two different methods are proposed; a clip-and-paste method and a facial structure deformation method.
In the present study the temporal aspects of syllables and words in spoken Italian are investigated. As for the syllables we tested the effects of syllable composition on the acoustic duration of vowels and consonants. The effects of word structure on segmental durations were tested by varying the word size and the position of lexical stress. The results indicate that both syllable structure and word structure have systematic effects on the duration of vowels and consonants. The effects of syllable structure and word size are primarily anticipatory, and are in the direction required for preserving the total duration of these units. Two points are made in the present paper: 1. the syllable data suggest that the unit tending to be constant in duration is the temporal interval from vowel onset. 2. The data relative to syllable and word would indicate that the durational variations due to these variables are realized by two different articulatory strategies. A few observations are made on syllable- and stress-timing.
A major deficiency in state-of-the-art automatic speech recognition (ASR) systems is the lack of robustness in additive and convolutional noise. The model of auditory perception (PEMO), developed by Dau et al. (T. Dau, D. Püschel, A. Kohlrausch, J. Acoust. Soc. Am. 99 (6) (1996) 3615–3622) for psychoacoustical purposes, partly overcomes these difficulties when used as a front end for automatic speech recognition. To further improve the performance of this auditory-based recognition system in background noise, different speech enhancement methods were examined, which have been evaluated in earlier studies as components of digital hearing aids. Monaural noise reduction, as proposed by Ephraim and Malah (Y. Ephraim, D. Malah, IEEE Trans. Acoust. Speech Signal Process. ASSP-32 (6) (1984) 1109–1121) was compared to a binaural filter and dereverberation algorithm after Wittkop et al. (T. Wittkop, S. Albani, V. Hohmann, J. Peissig, W. Woods, B. Kollmeier, Acustica United with Acta Acustica 83 (4) (1997) 684–699). Both noise reduction algorithms yield improvements in recognition performance equivalent to up to 10 dB SNR in non-reverberant conditions for all types of noise, while the performance in clean speech is not significantly affected. Even in real-world reverberant conditions the speech enhancement schemes lead to improvements in recognition performance comparable to an SNR gain of up to 5 dB. This effect exceeds the expectations as earlier studies found no increase in speech intelligibility for hearing-impaired human subjects.
The systematic errors children make in the course of phonological development, like adult production errors and adult phonological processes, can provide evidence of language production mechanisms. A detailed investigation of the environments in which velar stops are fronted by a phonologically delayed child reveals that fronting is dependent on both word stress and word boundaries; that it shows lexical exceptions; and that it occurs in output only. This distribution suggests that the child has output lexical representations which are independent of input lexical representations, and that the fronting error occurs in these output representations. It also suggests that prosodic features are crucial to the identification of articulatory features within these representations. Such an analysis has implications for theories of lexical access, and for the development of lexical access in children.
The inverse problem for vocal tract shape, area function and articulatory parameters was solved for steady-state vowels by means of an optimization procedure requiring the conditional minimum of work on the part of the articulatory organs. One to four formant frequencies were used as references. The shape of the tongue was measured with an X-ray microbeam system for male and female speakers. The shapes of the vocal tract calculated for the experiments are very similar to the measured shapes.
This paper aims at evaluating the contribution of semantic networks to the computational terminology. It focuses on the conceptual graph formalism and, at a lower degree, on the description logics. We nevertheless argue that such formalisms can play a role in the modeling process, for building an intermediate model between the natural language semantics and the operational model to be used in an automatic process.
The purpose of time series analysis is to take into account the fact that glottal cycles are produced sequentially and that relations between neighbouring perturbations exist. The jitter time series model statistically represents the present perturbation as a weighted sum of past perturbations and random noise. The model is fitted to observed jitter time series by means of conventional linear methods. A discriminant analysis of jitter time series extracted from 279 sustained vocoids [a] [i] [u] shows that the jitter features which separately describe the predictable and random components better characterise healthy and dysphonic speakers than a traditional jitter feature. The conclusion is that the relations between neighbouring cycle length perturbations are an aspect of jitter independent of the scatter of the cycle lengths which is described by conventional jitter features.
In this work we present a new shape from texture algorithm applied to natural scenes analysis. The originality of this approach is based on the modeling of the structure of the primary visual cortex (V1). The algorithm is able to deal with a large variety of textures presenting different types of irregularities. First to sample the amplitude spectra, we present new filters, called log-normal filters, inspired from the complex cells of V1, in replacement of the classical Gabor filters. These filters appear to be suitable for pattern analysis techniques due to their different theoretical properties, notably their radial frequency profile (adapted to the 1/f frequency profile of natural scenes) and their separability in orientation and frequency. We then use an estimation method of the local mean frequency applied to natural signals. This one does not imply the search for the adapted scale for the analysis and takes advantage of the frequencies of the used bank of filters. Finally, from a local estimation, the orientation and shape are extracted using the geometrical properties of the perspective projection. The precision of the method is evaluated on different types of textures, both regular and irregular, and on natural scenes. The presented method allows to obtain favorably comparable results to existing best known methods with a low computational cost. Finally the model can be adapted to other applications like texture analysis, characteristic points extraction or content-based image indexation.
One of the most striking characteristics typical of the speech of Broca's aphasics is its agrammatism — the omission of 'function' words and inflectional morphemes. Agrammatism is generally viewed as being symptomatic of a syntactic deficit. We argue here that such an account lacks grammatical systematicity, and that the only uniform and systematic interpretation of this deficit is in terms of phonological structure. A natural class consisting of 'function' words and some bound morphemes can be defined with reference to the junctural properties of sentences which characterize phonological words. It is this class of elements which tend to be omitted in agrammatic speech. The goal of this paper is not only to provide an hypothesis for the interpretation of one aphasic syndrome, but also to test and illustrate the efficacy of paying close attention to substantive universals of grammatical structure in proposing accounts of linguistic deficits.
A modified SEARMA method is proposed for estimating the speech spectrum in the presence of colored background noise. The following assumptions are used in developing the analysis. The speech production process is represented by an autoregressive moving-average (ARMA) model. Following these assumptions, the process during speech activity can be represented by an extended ARMA model. In this formulation, unique estimation of AR parameters of the vocal tract transfer function is always possible if the MA parameters of the noise process can be estimated separately, but the estimation of MA parameters of the speech production process requires further assumption of a high SNR. The validity of the proposed method is demonstrated by spectral estimation of both synthetic and natural speech sounds in the presence of additive colored noise, and by comparing the results with those obtained by the LPC method.
A method is presented to provide a useful searchable index for spoken audio documents. The task differs from the traditional (text) document indexing, because large audio databases are decoded by automatic speech recognition and decoding errors occur frequently. The idea in this paper is to take advantage of the large size of the database and select the best index terms for each document with the help of the other documents close to it using a semantic vector space. First, the audio stream is converted into a text stream by a speech recognizer. Then the text of each story is represented in a vector space as a document vector which is the normalized sum of the word vectors in the story. A large collection of such document vectors is used to train a self-organizing map (SOM) to find latent semantic structures in the collection. As the stories in spoken news are short and will include speech recognition errors, smoothing of the document vectors using the semantic clusters determined by the SOM is introduced to enhance the indexing. The application in this paper is the indexing and retrieval of broadcast news on radio and television. Test results are given using the evaluation data from the text retrieval conference (TREC) spoken document retrieval (SDR) task.
This paper proposes three noise adaptation algorithms which allow improvements in the performance of speech recognition systems under noisy conditions. They are VQ-based feature mapping techniques which hierarchically transform noisy feature vectors into clean feature vectors. The first algorithm was originally used for unsupervised speaker adaptation. It is based on hard clustering and iteratively adapts the noisy input data to a small set of codebooks created from clean data. The second algorithm is a modified version of the first one. It redefines the mapping function using the notion of cluster scope. The last algorithm proposes a fuzzy clustering technique as a substitute to the original hard clustering technique. In the NATO digit task, these algorithms significantly improve the performance of CRIM's speech recognition system.
EXPULSE method lifts the main drawback of the classical high resolution spectral analysis methods (MUSIC, MINORM, …) which have a poor robustness with respect to an unreliable knowledge of the number of sources. Its novelty stems from the modeling of the periodogram of complex sinusoids embedded in an additive noise, as the convolution of a perfectly known kernel (depending upon the window) and a compound Bemoulli—Gaussian process, plus a noise. The discrete frequencies where the Bemoulli process takes 1 values locate the sinusoids; the gaussian process describes the amplitudes.
Since climate change impact studies have shown that the marine environment could be greatly weakened by the disappearance of some flora and fauna species and the rapid aging of underwater infrastructure, new, efficient and robust observation tools are required. In this paper we propose acoustic cameras as innovative tools for acquiring underwater data along with a conceptual framework that allows an accurate and complete three-dimensional reconstruction implementation of the underwater environment using image sequences acquired by those acoustic cameras. Examples of information extracted from the acquired acoustic data that participate in such a 3D reconstruction are also provided as well as some preliminary results.
A data base of around 6500 syllables and their component segments were analysed to describe and develop a model of segment and syllable duration for Australian English. Segment duration was analysed according to prosodic context. Syllables were labelled and analysed according to their prosodic context, length (number of segments), and nature of syllabic peak. Syllable duration was modelled using a three-layer neural network that was trained and tested on different portions of the database. Segment durations were stretched or compressed to fit the network-assigned syllable duration frame. This relatively simple syllable model was able to account for nearly 80% of syllable-level durational variance observed in the database.
The performance of present-day automatic speech recognition (ASR) systems is seriously compromised by levels of acoustic interference (such as additive noise and room reverberation) representative of real-world speaking conditions. Studies on the perception of speech by human listeners suggest that recognizer robustness might be improved by focusing on temporal structure in the speech signal that appears as low-frequency (below 16 Hz) amplitude modulations in subband channels following critical-band frequency analysis. A speech representation that emphasizes this temporal structure, the “modulation spectrogram”, has been developed. Visual displays of speech produced with the modulation spectrogram are relatively stable in the presence of high levels of background noise and reverberation. Using the modulation spectrogram as a front end for ASR provides a significant improvement in performance on highly reverberant speech. When the modulation spectrogram is used in combination with log-RASTA-PLP (log RelAtive SpecTrAl Perceptual Linear Predictive analysis) performance over a range of noisy and reverberant conditions is significantly improved, suggesting that the use of multiple representations is another promising method for improving the robustness of ASR systems.
This paper investigates a weighted LPC analysis of voiced speech. In view of the speech production model, the weighting function is either chosen to be the short-time energy function of the preemphasized speech sample sequence with certain delays or is obtained by thresholding the short-time energy function. In this method, speech samples are selectively weighted based on how well they match the speech production model. Therefore, the estimates of the LPC coefficients obtained by this novel LPC analysis are more accurate than those obtained from the conventional LPC analysis. They are also less sensitive to the values of the fundamental frequency than conventional LPC.
The accuracy of speech recognition systems is known to be affected by fast speech. If fast speech can be detected by means of a measure of speaking rate, the acoustic as well as language models of a speech recognition system can be adapted to compensate for fast speech effects. We have studied several measures of speaking rate which have the advantage that they can be computed prior to speech recognition. The proposed measures have been compared with conventional measures, viz., word and phone rate on the TIMIT database. Some of the proposed measures have significant correlations with phone rate and vowel duration. We have shown that the mismatch between actual and expected durations of test vowels reduces if the vowel duration models are adapted to speaking rate, as estimated by the proposed measures. These measures can be computed from features commonly employed in speech recognition, do not entail significant additional computational load and do not need labeling or segmentation of unknown utterance in terms of linguistic units.
A statistical image model is used for segmenting polarimetric synthetic aperture radar (SAR) data into regions of homogeneous polarimetric backscatter characteristics. Optimal region labels of the data are those which maximize the a posteriori distribution of the region labels given the polarimetric complex data. Implementation of the MAP (Maximun A Posteriori) technique is accomplished either by the modified deterministic ICM (Iterative Conditional Modes) algorithm or by the stochastic SA (Simulated Annealing) algorithm. Unsupervised parameter estimation procedures are obtained either by the CMFMAP-NSO (unsupervised fuzzy partition - optimal number of classes) and CMF-NS or by SEM (Stochastic Estimation Maximization) algorithm. Results, using fully polarimetric SAR forest data, obtained by the CMFMAP-NSO following by the ICM algorithm with a K_ distribution model are quite satisfactory.
Six types of speech synthesis were evaluated for comprehensibility: standard linear predictive coding analysis/ resynthesis; pitch synchronous analysis/resynthesis; pitch synchronous multi-pulse analysis/resynthesis; and three PSOLA (pitch synchronous overlap-and-add) techniques. The relative comprehensibility of the synthesis types was tested by using the synthesised speech to convey information that subjects needed in order to perform a diagram-based multiple-choice task.
The application of learning theory to bayesian networks is still uncomplete and we propose a contribution, especially through the use of covering numbers. We deduce multiple corollaries, among which a nonfrequentist approach for parameters learning and a score taking into account a measure of structural entropy that has never been taken into account before. We then investigate the algorithmic aspects of our theoretical solution, based on BFGS and adaptive refining of gradient calculus.
In order to facilitate the access and the usage to the general public of the rapidly expanding applications and services, particularly on the internet, new assisting tools are needed with two main requirements: naturalness and acceptability. Conversational agents are a promising approach but assisting agents cannot rely only on rational reasoning over the structure and the functioning of the assisted applications. They must also express behavioral reactions that involve social relationships, character traits and affects. In the first part of this paper, we propose a flexible framework for modeling the relationships between the rational and behavioral reactions of an assisting agent. Then this framework is used to support a first case-study, based on cognitive biases.
To date, speech recognition systems have been applied in real world applications in which they must be able to provide a satisfactory recognition performance under various noise conditions. However, a mismatch between the training and testing conditions often causes a drastic decrease in the performance of the systems. The viability of the suggested technique was verified in various experiments using different background noises and microphones. In a multi-environment speaker-independent connected digit recognition task, the proposed method reduced the error rates by over 16%.
This paper reviews past work comparing modern speech recognition systems and humans to determine how far recent dramatic advances in technology have progressed towards the goal of human-like performance. Comparisons use six modern speech corpora with vocabularies ranging from 10 to more than 65,000 words and content ranging from read isolated words to spontaneous conversations. Error rates of machines are often more than an order of magnitude greater than those of humans for quiet, wideband, read speech. Machine performance degrades further below that of humans in noise, with channel variability, and for spontaneous speech. Humans can also recognize quiet, clearly spoken nonsense syllables and nonsense sentences with little high-level grammatical information. These comparisons suggest that the human-machine performance gap can be reduced by basic research on improving low-level acoustic-phonetic modeling, on improving robustness with noise and channel variability, and on more accurately modeling spontaneous speech.
This paper is a contribution to automatic speaker recognition. It considers speech analysis by linear prediction and investigates the recognition contribution of its two main resulting components, namely the synthesis filter on one hand and the residue on the other hand. This investigation is motivated by the orthogonality property and the physiological significance of these two components, which suggest the possibility of an improvement over current speaker recognition approaches based on nothing but the usual synthesis filter features. Specifically, we propose a new representation of the residue and we analyse its corresponding recognition performance by issuing experiments in the context of text-independent speaker verification. Experiments involving both known and new methods allow us to compare the recognition performance of the two components. First we consider separate methods, then we combine them. Each method is tested on the same database and according to the same methodology, with strictly disjoint training and test data sets. The results show the usefulness of the residue when used alone, even if it proves to be less efficient than the synthesis filter. However, when both are combined, the residue shows its true relevance. It achieves a reduction of the error rate which, in our case, went down from 5.7% to 4.0%.
The possibility to vary speaker type and speaking style will be a feature of the next generation text-to-speech systems. Already today, the need for these possibilities is apparent in dialogue systems and when speech synthesis is used as prostheses for persons with a communication handicap. Much of the information needed is not yet available. In this contribution we argue that speech synthesis itself is an efficient tool to study and understand the variability in speech. Some different methods are reviewed, representing both analysis/synthesis techniques and signal manipulations as well as text-to-speech. The main emphasis is, on the work at KTH, to study the variation of speaker and speaking styles in the context of our text-to-speech system.
This paper proposes an original method for robotic programming based on bayesian inference and learning. This method formally deals with problems of uncertainty and incomplete information that are inherent to the field. Indeed, the principal difficulties of robot programming comes from the unavoidable incompleteness of the models used. We present the formalism for describing a robotic task as well as the resolution methods. We illustrate it by programming a surveillance task with a mobile robot: the Khepera. In order to do this, we use generic programming resources called “descriptions”. We show how to define and use these resources in an incremental way (reactive behaviors, sensor fusion, situation recognition and sequences of behaviors) within a systematic and unified framework.
This paper presents a new frequency-domain approach to implement an adaptive postfilter for enhancement of noisy speech. The postfilter is described by a set of DFT coefficients which suppress noise in the spectral valleys and allow for more noise in formant regions which is masked by the speech signal. First, we perform an LPC analysis of the noisy speech and calculate the log magnitude spectrum of the input speech. After identifying the formants and valleys (by a new method), the log magnitude spectrum is modified to obtain the postfilter coefficients. The filtering operation is also done in the frequency domain through an FFT and an overlap-add strategy to get the postfiltered speech. Experimental results on 8-kHz-sampled speech show that this new frequency-domain approach results in enhanced speech of better perceptual quality than obtained by a time-domain method. This new method is especially efficient in eliminating high frequency noise and in preserving the weaker, high frequency formants in sonorant sounds.
Previous research suggested that rhythmic expectations could play a role in languages contrasting stressed syllables with unstressed ones, whereas languages without such a contrast and with a clear syllabic structure, such as French, would be processed according to a syllable-based procedure. Lexical parsing of bisyllabic words composed of two monosyllabic words are studied. Two experiments examine the effects of usual and reverse metrical patterns on segmentation. The usual iambic pattern produces, more often than not, recognition of bisyllables whereas lexical parsing is not influenced by monosyllable frequency and syllabic structure. The trochaïc pattern strongly increases the amount of segmentation. In Experiment 2, focusing subjects' attention on the timing structure strengthens these effects. Consequently, French subjects use a metrical segmentation strategy. By contrast, the processing of spondees (Experiment 3) shows an effect of structural parameters on parsing and suggests the use of a syllable-based segmentation procedure when rhythmic information is absent. Implications for speech recognition models are discussed.
These last years, there were many studies on the problem of conflict coming from information fusion, especially in evidence theory. We can summarize the solutions for managing the conflict in three different approaches: first, you can try to suppress or reduce the conflict before the combination step; secondly, you can manage the conflict in order to give no influenence to the conflict in the combination step, and then take into account the conflict in the decision step; thirdly, you can take into account the conflict in the combination step. The first approach is certainly the better, but not always feasible. It is difficult to say which approach is the best between second and third. However, the most important is the produced results in applications. We propose here a new combination rule that distributes the conflict proportionally on the elements giving this conflict. We compare these different combination rules on real data in Sonar imagery and Radar target classification.
This paper reports the design of a command-based speech interface for an answering machine or a voice mail system. Automatic speech recognition was integrated in order to facilitate the remote control and the retrieval of voice messages from any telephone in a speech-only dialogue. The design goal was that consumers would perceive the speech interface as a benefit compared with the common touch-tone interface. In this paper we will first describe the speech technology underlying the system. Then it will be shown how, based on this technology, the user interface was designed in a top-down approach. We started with the development of a concept and tested it by means of a Wizard-of-Oz simulation. After refining the concept in parallel design, it was implemented in a high-fidelity prototype. By means of qualitative user testing the design was improved in three iteration steps. The achievement of the design goal was finally verified with user tests in two countries.
It is well known that the introduction of acoustic background distortion and the variability resulting from environmentally induced stress causes speech recognition algorithms to fail. In this paper, several causes for recognition performance degradation are explored. It is suggested that recent studies based on a Source Generator Framework can provide a viable foundation in which to establish robust speech recognition techniques. This research encompasses three inter-related issues: (i) analysis and modeling of speech characteristics brought on by workload task stress, speaker emotion/stress or speech produced in noise (Lombard effect), (ii) adaptive signal processing methods tailored to speech enhancement and stress equalization, and (iii) formulation of new recognition algorithms which are robust in adverse environments. An overview of a statistical analysis of a Speech Under Simulated and Actual Stress (SUSAS) database is presented. This study was conducted on over 200 parameters in the domains of pitch, duration, intensity, glottal source and vocal tract spectral variations. These studies motivate the development of a speech modeling approach entitled Source Generator Framework in which to represent the dynamics of speech under stress. This framework provides an attractive means for performing feature equalization of speech under stress. In the second half of this paper, three novel approaches for signal enhancement and stress equalization are considered to address the issue of recognition under noisy stressful conditions. The first method employs (Auto:I,LSP:T) constrained iterative speech enhancement to address background noise and maximum likelihood stress equalization across formant location and bandwidth. The second method uses a feature enhancing artificial neural network which transforms the input stressed speech feature set during parameterization for keyword recognition. The final method employs morphological constrained feature enhancement to address noise and an adaptive Mel-cepstral compensation algorithm to equalize the impact of stress. Recognition performance is demonstrated for speech under a range of stress conditions, signal-to-noise ratios and background noise types.
In this paper, we present a reinforcement learning approach for multi-agent communication in order to learn what to communicate,when and to whom. This method is based on introspective agents, that can reason about their own actions and data so as to construct appropriate communicative acts. We propose an extention of classical reinforcement learning algorithms with multi-agent communication. We show how communicative acts and memory can solve non-markovity and asynchronism issues MAS.
The motion-compensated hybrid DCT/DPCM algorithm has been successfully adopted in various video coding standards, such as H.261, H.263, MPEG-1 and MPEG-2. However, its robustness is challenged in the face of an inadequate bit allocation, either globally for the whole video sequence, or locally as a result of an inappropriate distribution of the available bits. In either of these situations, the trade-off between quality and the availability of bits results in a deterioration in the quality of the decoded video sequence, both in terms of the loss of information and the introduction of coding artifacts. These distortions are an important factor in the fields of filtering, codec design, and the search for objective psychovisual-based quality metrics; therefore, this paper presents a comprehensive analysis and classification of the numerous coding artifacts which are introduced into the reconstructed video sequence through the use of the hybrid MC/DPCM/DCT video coding algorithm. Artifacts which have already been briefly described in the literature, such as the blocking effect, ringing, the mosquito effect, MC mismatch, blurring, and color bleeding, will be comprehensively analyzed. Additionally, we will present artifacts with unique properties which have not been previously identified in the literature.
The next generation of text-to-speech systems will have to be more sensitive to sociolinguistic 'style' variables. In order to assist in the adaptation of synthesis to a wider range of contexts, this article examines several sociolinguistic parameters which have been shown to influence the realization of negatives in actual discourse, analyzing their effects on the realization of negatives in English prose readings. Consistent with the results found in an earlier study, the analysis shows that pitch prominence on negatives is not common in read prose passages, and is even less common in read dialogue. The results show a surprising absence of conformity with 'theoretical' linguistic expectations, highlighting the necessity for consideration of register as an important variable for speech synthesis.
When we communicate through (natural) languages, we do not explicitly say everything. Both the speaker and the hearer utilize information available from the utterance situation, which includes the mental states of the speaker and the hearer. Interesting cases are frequently observed in the use of Japanese (in dialogue situations). Syntactic (or configurational) constraints of Japanese are weaker than those of English, in the sense that the speaker may omit almost any element in a sentence. In this paper we present a mechanism of the hearer in the light of situated reasoning and show how the missing information can be supplied from the situation. Although we believe that the model captures the essential nature of human communication, it may be too naive as a model of human cognition. Rather, the model is intended to be used in the design of software agents that communicate with each other in a mechanical but flexible and efficient way.
Evidence of syntactic control on the prosodic structure of an utterance in French has been exploited in various generative models of prosody. Nevertheless models without syntactic knowledge based only on rhythmic constraints can generate acceptable prosodic structures. The model presented here shows that syntax-driven and rhythm-driven strategies could be extreme cases of a more complex model which integrates both syntactic and rhythmic constraints. This generative model has been integrated into various text-to-speech systems. Preliminary perceptual comparative tests published elsewhere have shown an improvement in quality over a well-known syntax-driven model.
This paper describes the implementation of a new parametric model of the glottal geometry aimed at improving male and female speech synthesis in the framework of articulatory analysis synthesis. It is embedded in an articulatory analysis synthesis system (articulatory speech mimic). To introduce naturally occurring details in our synthetic glottal flow waveforms, we modelled two different kinds of leakage: a “linked leak” and a “parallel chink”. While the first is basically an incomplete glottal closure, the latter models a second glottal duct that is independent of the membranous (vibrating) part of the glottis. Characteristic for both types of leaks is that they increase de-flow and source/tract interaction. A linked leak, however, gives rise to a steeper roll-off of the entire glottal flow spectrum, whereas a parallel chink decreases the energy of the lower frequencies more than the higher frequencies. In fact, for a parallel chink, the slope at the higher freqencies is more or less the same as in the no-leakage case.
Environmental robustness for automatic speech recognition systems based on parameter modification can be accomplished in two complementary ways. One approach is to modify the incoming features of environmentally-degraded speech to more closely resemble the features of the (normally undegraded) speech used to train the classifier. The other approach is to modifying the internal statistical representations of speech features used by the classifier to more closely resemble the features representing degraded speech in a particular target environment. This paper attempts to unify these two approaches to robust speech recognition by presenting several techniques that share the same basic assumptions and internal structure while differing in whether they modify the features of incoming speech or whether they modify the statistics of the classifier itself. We present the multivaRiate gAussian-based cepsTral normaliZation (RATZ) family of algorithms which modify incoming cepstral features, along with the STAR (STAtistical Reestimation) family of algorithms, which modify the internal statistics of the classifier. Both types of algorithms are data driven, in that they make use of a certain amount of adaptation data for learning compensation parameters. The algorithms were evaluated using the SPHINX-II speech recognition system on subsets of the Wall Street Journal database. While all algorithms demonstrated improved recognition accuracy compared to previous algorithms, the STAR family of algorithms tended to provide lower error rates than the RATZ family of algorithms as the SNR was decreased.
The hardware described and test results presented correspond to the equipment tested in the European contest at Turin, Italy. The codec, whilst giving good speech quality, also offers several important features including low delay, low computational complexity and a good tolerance to transmission errors. The codec employs an 8-band subband coding algorithm incorporating backward-adaptive quantisation and backward-adaptive prediction. The hardware implementation described uses a pair of digital signal processing devices, thereby giving a very compact realisation. The results of both subjective and objective tests are also presented.
Prosodic relations in prose, poetry and music are discussed with an emphasis on durational properties. In order to gain a deeper understanding of speech prosody, we are presently engaged in a comparison of the timing relations in such activities as the reading of poetry and music performance, where there usually is a strong and obvious rhythmic patterning of the produced sound sequences. Also there are interesting parallels to be drawn by comparing the formal notations of prose, poetry and music. Generally, there are no simple relations between abstract notations and performance, and moreover, notations have varied with tradition and particular needs. However, it is a challenge to tie descriptive systems closer to common human constraints in production and perception.
In this paper the harmonic features based on the harmonic decomposition of the Hildebrand–Prony line spectrum are introduced. A Hildebrand–Prony method of spectral analysis was applied because of its high resolution and accuracy. Comparative tests with the LP and LP-cepstral features were made with 50 speakers from the Slovene database SNABI (isolated words corpus) and 50 speakers of the German database BAS Siemens 100 (utterances of sentences). With both databases the advantages of the harmonic features were noticed especially for the speaker identification while for the speaker verification the harmonic features have performed better on the SNABI database and as good as the LP cepstral features on the BAS Siemens 100 database.
This article introduces a methodology for quantifying the distortion introduced by a low or medium bit-rate speech coder. Since the perceptual acuity of a human being determines the precision with which speech data must be processed, the speech signal is transformed onto a perceptual-domain (PD). This is done using Lyon's cochlear (auditory) model whose output provides the probability-of-firing information in the neural channels at different clock times. In our present approach, we use a hidden Markov model to describe the basic firing/non-firing process operative in the auditory pathway. We consider a two-state fully-connected model of order one for each neural channel; the two states of the model correspond to the firing and non-firing events. Assuming that the models are stationary over a fixed duration, the model parameters are determined from the PD observations corresponding to the original signal. Then, the PD representations of the coded speech are passed through the respective models and the corresponding likelihood probabilities are calculated. These probability scores are used to define a cochlear hidden Markovian (CHM) distortion measure. This methodology considers the temporal ordering in the neural firing patterns. The CHM measure which utilizes the contextual information present in the firing pattern shows robustness against coder delays.
We present here a whole operational prototype for the compression and recognition of dance gestures from contemporary ballet. Our input data are motion trajectories followed by the joints of a dancing body provided by a motion-capture system. We propose a suitable tool for nonuniform sub-sampling of spatio-temporal signals. The key of our approach is the use of polygonal approximation to provide a compact and efficient representation of motion trajectories. Our dance gesture recognition method involves a set of Hidden Markov Models (HMMs), each of them being related to a motion trajectory followed by the joints. We have validated our recognition system on 12 fundamental movements from contemporary ballet performed by 4 dancers.
Information Retrieval techniques make use of terms that are automatically extracted from documents; these terms are used to give information access. In this paper we propose an approach to enrich semantically this extraction by adding knowledge from thesauri. More specifically, the methodology we promote in this paper aims at transforming a thesaurus into a domain ontology which will then be used to semantically index documents (indexes are concepts rather than terms). We also propose techniques that implement this transformation as well as an evaluation in the field of astronomy.
The articulatory pattern observed by electropalatography cannot be interpreted simply as the concatenation of assimilated segments.
Between 1399 and 1400 Christine de Pizan wrote her Epistre Othea. Sixty years later Jean Miélot rewrote and revised this successful work for the court of the Dukes of Burgundy. As well as pointing up the particularity of Jean Miélot's revision over his contemporaries like Jean Wauquelin or David Aubert the present article attempts to contextualize both the textual source and the textual target in order to answer the question of the emergence of Miélot's revision but also seeks possible explanation for its absence of dissemination. Finally we note that the traditional textual and philological approach does not entirely explain the reception of a text within a context and that this traditional approach should also be coupled with a factual and contextualized approach.
Transitional information, represented by first-order orthogonal coefficients which result from the orthogonal polynomials expansion of the cepstral contours, describes the speed of variation of the speech spectra. Cepstral and first-order orthogonal coefficients vectors were employed separately as well as joined into a single feature vector, in classical DTW-based verification algorithms. Investigations on a population of 22 speakers (high-quality speech) showed that the elimination of the time-invariant spectral components from the speech features, taking place when performing cepstral normalization or computing first-order orthogonal coefficients, brings a substantial reliability improvement. Furthermore, transitional information is practically as effective as instantaneous information, whereas combining both kinds of information does not lead to further improvement.
This paper describes a system for speech analysis based on known aspects of human auditory processing. A goal of this work is to provide signal transformations which preserve and enhance perceptually important speech features. A computer model of the peripheral auditory system incorporating phase-locking, two-tone suppression and additive adaptation effects is presented. Recent research in auditory physiology has highlighted the important contribution which these phenomena make to normal speech perception. The proposed model is able to emphasise dynamic spectral regions of the kind found throughout normal conversational speech.
In this paper quality evaluation procedures for hands-free telephones are described with reference to some examples. Instrumental measurements, showing procedures for instrumental evaluation, are shown using the examples of the different hands-free telephones.
We present in this paper a genetic search strategy for a search engine. We begin by showing that important relations exist between Web statistical studies, search engines based on agent approach, and standard techniques in optimization: the web is a graph which can be searched for relevant information with an evaluation function and with operators based on création or local exploration. It is then straightforward to define an évaluation function that is a mathematical formulation of the user request and to define a steady State genetic algorithm that evolves a population of pages with spécifie operators. The creation of individuals is performed bv querving standaid search engines. The mutation operator consists in exploring the neighborhood of a page thanks to the hyperlinks. We present a comparative evaluation which is performed with the same protocol as used in optimization.
The paper presents a model for the construction of an artificial agent that can express performatives through facial expression. The performative of a speech act or communicative act is the particular communicative intention a Sender has to one's Addressee, the way one wants to socially relate oneself to the interlocutor. Performatives are decomposed both on the meaning and on the signal side: on the meaning side, a performative is represented as a cluster of cognitive units, that in turn include subclusters mentioning the Sender's general goal (informing, asking, requesting), the power relationship between Sender and Addressee, the Sender's affective state, and further information peculiar of specific performatives; on the signal side, facial expressions are decomposed into Action Units. The proposed system computes the appropriate performative of one's communicative acts through consideration of the context of communication, particularly of the Addressee's cognitive capacity, social relationship and personality traits, and then expresses the computed performatives through 3D facial displays.
The paper is focussed on the vocalic differences between spontaneous and laboratory speech in Spanish. The first and second formants of 954 vowel utterances (477 in laboratory and 477 in spontaneous speech) have been measured. They constitute clusters in the F 1/F 2 space. The paper describes inter- and intra-cluster variabilities caused by communication situations changes. In spontaneous speech, the formants values show (1) a marked schwa-tendency; (2) increasing intra-cluster variability. Both phenomena result in lowered differenciation of the sounds in spontaneous speech.
An automatic text-independent speaker recognition system suitable for identification and verification purposes is presented. This database consists of L prototypes for every speaker, representing the vowels of the language, which are estimated from L vowel clusters. These are formed by applying a modified k-means algorithm on the patterns extracted from the vowels of training utterances. The patterns of the training utterances are stored in a training database to be used for updating the reference data of the system. The system was tested over a period of four months with a population of 15 male and female speakers with non-correlated training and test data. Its accuracy proved to be satisfactory (91.39% for verification, 90.19% for closed-set identification, 95.28% for open-set identification), considering that the training utterances per speaker do not exceed 50 sec and the test utterances have a duration of 1.3 sec on the average. The accuracy is substantially increased when increasing the length of the test utterance (e.g. 93.75% verification accuracy for test utterances having an average duration of 4 sec). Additional advantages of the system are the small memory requirements and the fast response.
The goal is to determine the effective membership of points of contour obtained by active contour method to the cortical. Several parameters associated to the points are taken into account (gray level, mean of gray leveland standard deviation on a neighbourhood, outdistances between points belonging to close slices). The architecture is based on the formalism of the evidence. We discuss the results obtained, the validity of them and we propose further objectives of this work.
The time-frequency analysis of magnetic signals, generated by ferromagnetic objects, is used to extract a robust discriminant parameter set for their classification. After the feature selection phase, an extensive study is performed in order to validate the most appropriate classifier structure, in terms of the correct classification rate and the generalization ability.
If SVM (Support Vector Machines) are now considered as one of the best learning methods, they are still considered as slow. We chose to implement this algorithm with Matlab environment since it is user-friendly and efficient - it uses the ATLAS library. The comparison to the state of the art in this field, SMO shows that in some cases, our solution is faster and less complex.
In this paper we describe a new method of medical image registration. We formulate the registration problem as a minimization problem involving robust estimators. We propose an efficient hierarchical optimization framework which is both multiresolution and multigrid in order to accelerate the algorithm and to improve the quality of the estimation. The benefits of this method are demonstrated on real data and its performances are objectively evaluated on simulated data.
From a corpus of CVCVCV nonsense words where the consonant was [b] or [m] and the vowel [a i u], the behavior of the orbicularis oris superior and levator veli palatini muscles was studied. The importance of preplanning of the utterance (the role of the initial position functioning as a reference) and of intrasegmental timing reorganization have been assessed within the framework of a motor encoding theory based on the concepts of “sequencing” and “phasing”. Furthermore, our data confirm the idea that muscular synchronization constitutes a fairly basic rule of speech production.
In many domains (biology, medicine, psychology, etc.), efficient text mining tools could help the expert. In order to obtain a usable tool, an expert of the domain must control the varions text mining steps. The approach proposed in this paper consists in extracting the association rides specifie to the field starting from a set of specialized and homogeneous texts. Our approach is made up of varions stages in which the expert's rôle is essential. The first stage consists in extracting the ternis in the texts and associating them to a concept, i.e. a set of terms having the saine semantic. Using this new spécifie knowledge, the initial corpus is transformed into a matrix. At the last stage of our approach, this matrix is discretised in order to extract association rides.
This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal. We introduce a new algorithm, SCIRL, whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multiclasse classifier. This approach produces a reward function for which the expert policy is provably near-optimal. Contrary to most of existing IRL algorithms, SCIRL does not require solving the direct RL problem. Moreover, with an appropriate heuristic, it can succeed with only trajectories sampled according to the expert behavior. This is illustrated on a car driving simulator.
This paper introduces a new approach of watermarking for copyright protection. The goal of the method is to hide signatures composed of w segments of r bits in digital images. The framework itself is founded upon a wavelet transformed domain, and an additive embedding rule using products of orthogonal basis functions. We will show how the choice of different kinds of orthogonal functions allows to improve the robustness of the watermarking scheme to signal processing or geometric attacks.
As a basic coder structure, the long-term predictor is implemented as an adaptive codebook, while a sparse Gaussian codebook with non-overlapping vectors is used for the stochastic excitation. In order to meet the complexity requirements, several methods for efficient codebook search are adopted. With these methods, it is shown that the computational effort for the basic coder structure can be reduced to 12.4 MIPS with a 7 bit stochastic codebook. A two-stage hierarchical search through the adaptive codebook is investigated. This search method reduces the computational effort further although at the cost of a small degradation in coder performance. The speech quality with the basic CELP structure is judged to be comparable to the G.722 coder at 48 kbit/s.
Sacadeau-Software is a decision support tool for agronomists working on water pollution in catchments, and for people in charge of the management of these catchments. This tool focuses on water contamination by pesticides applied on corn. It builds on a biophysical model representing the transfer processes ofpesticides through the catchment and on a decisional model representing the farming techniques for maïs culture. Sacadeau-Software allows to run simulations throughout a watershed and obtain the transfer rate of pollutants through the catchment. Classiﬁcation rules characterizing the sub-parts of the watershed with pollution, and the sub-parts without pollution, are automatically learned from the simulations. A visualization tool enables to relate the learned rules to the examples characterized by these rules. Finally, a action recommendation tool analyzes the learned rules and proposes actions that improve a situation of pollution.
Speech production variations due to perceptually induced stress contribute significantly to reduced speech processing performance. One approach for assessment of production variations due to stress is to formulate an objective classification of speaker stress based upon the acoustic speech signal. This study proposes an algorithm for estimation of the probability of perceptually induced stress. It is suggested that the resulting stress score could be integrated into robust speech processing algorithms to improve robustness in adverse conditions. First, results from a previous stress classification study are employed to motivate selection of a targeted set of speech features on a per phoneme and stress group level. Analysis of articulatory, excitation and cepstral based features is conducted using a previously established stressed speech database (Speech Under Simulated and Actual Stress (SUSAS)). Stress sensitive targeted feature sets are then selected across ten stress conditions (including Apache helicopter cockpit, Angry, Clear, Lombard effect. Loud, etc.) and incorporated into a new targeted neural network stress classifier. Second, the targeted feature stress classification system is then evaluated and shown to achieve closed speaker, open token classification rates of 91.0%. Finally, the proposed stress classification algorithm is incorporated into a stress directed speech recognition system, where separate hidden Markov model recognizers are trained for each stress condition. An improvement of +10.1% and +15.4% over conventionally trained neutral and multi-style trained recognizers is demonstrated using the new stress directed recognition approach.
Listeners are able to tell apart read-aloud and spontaneously produced speech. Prosody appears to be important for this perceptual distinction. In this paper, the importance of the distribution and realization of prosodic boundaries is investigated. Recordings were made of five male speakers, spontaneously producing so-called instruction monologues. Transcripts of these monologues were read aloud by the same speakers. A perception experiment was carried out to obtain classification scores for isolated utterances selected from the spontaneous and read material. Auditory prosodic transcriptions were made of the entire spontaneous and read monologues, assessing the distribution and realization of underlying prosodic boundaries in both speech types. The underlying prosodic structure was assessed by means of an automatic text-to-speech system. Observed differences in the production of prosodic boundaries in the spontaneous and read material are related to the perceptual classification scores by means of a multiple regression analysis.
This paper describes an accurate and efficient algorithm for very-large-vocabulary continuous speech recognition. It is based on a two-stage LR parser with hidden Markov models (HMMs) as phoneme models. To improve recognition accuracy, it uses the forward and backward trellis likehood. To improve search efficiency, it uses adjusting windows and merges candidates that have the same allophonic phoneme sequences and grammatical state, and then merges candidates at the meaning level. This algorithm was applied to a telephone directory assistance system that contains more than 70,000 subscribers (about 80,000 words) to evaluate its speaker-independent speech recognition capabilities. For eight speakers, the algorithm achieved a speech understanding rate of 65% for spontaneous speech. The results show that the system performs well in spite of the large word perplexity. This paper also describes a multi-modal dialog system that uses our large-vocabulary speech recognition algorithm.
The use of signal transformations is a necessary step for feature extraction in pattern recognition systems. These transformations should take into account the main goal of pattern recognition: the error-rate minimization. In this paper we propose a new method to obtain feature space transformations based on the Minimum Classification Error criterion. The goal of these transformations is to obtain a new representation space where the Euclidean distance is optimal for classification. The proposed method is tested on a speech recognition system using different types of Hidden Markov Models. The comparison with standard pre-processing techniques shows that our method provides an error-rate reduction in all the performed experiments.
We review different methods for the second task, emphasizing the advantages and disadvantages of the linear predictive (LPC) diphone approach. Diphones require more memory represent all possible spectral transitions between pairs of phonemes, but they directly capture many of the coarticulation effects that must otherwise be modeled in phonemic synthesis. Relatively simple interpolation is allowed due to the similarity of spectra at diphone boundaries.
This paper addresses the problem of predicting the quality of telephone speech. Starting from a definition of quality, which takes communicative as well as service-related factors into account, a new classification scheme for prediction models is proposed. It considers input and output parameters, the network components and application area the model is used for, as well as the psychoacoustic and judgment-related bases. According to this scheme, quality prediction models can be classified into signal-based comparative measures, network planning models and monitoring models. Whereas signal-based approaches have been described extensively in literature, this paper discusses the latter two approaches in detail. The underlying psychoacoustic properties of two network planning models, the E-model and the SUBMOD model, are analyzed, and combined approaches for monitoring models are developed. Possible future extensions to the models are pointed out, including wide-band scenarios and speech sound quality, non-stationary impairments as well as speech technology devices.
We propose a nonparametric classification method designed to support the inter- pretability of the prediction. On the one hand, the use of generalized additive models makes it possible to represent the effect of each input variable on the output variable graphically. On the other hand, parameters of this model are estimated via penalized likelihood, where the term of regularization generalizes the h-penalization to the splines functions. This penalization favors parsimonious solutions selecting one part ofthe set of input variables, while allowing a flexible modeling of the dependence on the selected variables. We study the adaptation of various analytical model selection criteria to these models, and we evaluate them on two real data sets
This paper presents an overview of Japanese research on individuality information in speech waves, which have been performed from various points of view. Whereas physical correlates having perceptual voice individuality have been investigated from the psychological viewpoint, research from the engineering viewpoint is related to automatic speaker recognition, speaker-independent speech recognition, and training algorithms in speech recognition. Speaker recognition research can be classified into two classes, depending on whether or not the text is predetermined. However, it has been made clear that even if the text is not predetermined, text-dependent individual information can be used that is based on explicit or implicit phoneme recognition. Various examples of speaker recognition methods are classified into these variations, and their performances are presented in this paper. In particular, this paper focuses on the long-term intra-speaker variability of feature parameters as on the most crucial problems in speaker recognition. Additionally, it presents an investigation into methods for reducing the effects of long-term spectral variability on recognition accuracy.
This paper presents a learning algorithm using hidden Markov models (HMMs) and genetic algorithms (GAs). Two standard problems to be solved with HMMs are how to determine the probabilities and the number of hidden states of the learned models. Generally, this number of states is determined either by the trial-error method that needs experimentation, or by the background knowledge available. The presented algorithm uses a GA in order to determine at the same time both the number of states and the probabilities of learned HMMs. This hybrid algorithm uses the Baum-Welch algorithm to optimise precisely the probabilities of HMMs. Several algorithms, either hybrid or not, are compared in a face recognition task. The obtained results highlight the strength of our approach for the concerned problem.
A hybrid coding algorithm, consisting of motion compensated interframe prediction and adaptive gain/shape vector quantization, is proposed for low bit rate video coding. A video coding system using the proposed coding algorithm and its coding characteristics are also described. Motion compensated interframe prediction is an efficient technique for reducing temporal redundancy contained in moving pictures. In the proposed scheme, constant amplitude blocks (“gray level blocks”) are also included in search vectors in order to increase coding efficiency for quick object movement or scene change. Adaptive gain/ shape vector quantization with a tree search codebook is employed to encode motion compensated interframe difference signals. It can respond to changes in source statistics because its codebook is independent of the gain components of the input vectors. The proposed coding algorithm has been evaluated by computer simulations and shown to be effective for low bit rate video transmission. A 64 kbit/s video coding system based on this coding algorithm has been implemented. The developed coding system can transmit multiplexed video, audio and digital data at the basic ISDN rate, and can be applied to videophone as well as videoconferencing.
This introduction sets the stage for the papers making up this special issue. Its focus is on two major problems in the study of lexical processing—determining the phases involved in recognising a spoken word and identifying the nature of different types of contextual influences on these phases. An attempt is made to decompose the process of recognising a word into phases which have both theoretical and empirical consequences. A similar analytic approach is taken in the discussion of the problem of context effects by distinguishing qualitatively different types of context (lexical, intra-lexical, syntactic, semantic, and interpretative). We argue that such an approach is necessary to make explicit the relationship between a particular type of contextual information and the phase(s) of processing at which it has its impact.
The general framework of this paper is speech analysis and synthesis. The speech signal may be separated into two components: (1) a periodic component (which includes the quasi-periodic or voiced sounds produced by regular vocal cord vibrations); (2) an aperiodic component (which includes the non-periodic part of voiced sounds (e.g. fricative noise in /v/) or sound emitted without any vocal cord vibration (e.g. unvoiced fricatives, or plosives)). This work is intended to contribute to a precise modelling of this second component and particularly of modulated noises. Firstly, a synthesis method, inspired by the “shot noise effect”, is introduced. This technique uses random point processes which define the times of arrival of spectral events (represented by Formant Wave Form (FWF)). Based on the theoretical framework provided by the Rice representation and the random modulation theory, an analysis/synthesis scheme is proposed. Perception tests show that this method allows to synthesize very natural speech signals. The representation proposed also brings new types of voice quality modifications (time scaling, vocal effort, breathiness of a voice, etc.).
Our study links a linguistic and psycholinguistic approach to the “metaphor” phenomenon based on the spontaneous utterances of 2-4-year-old children as well as on adult productions in scientific texts for the general public. We would like to show in what way these utterances, generally considered to be “deviant” or “ordinary”, can bring a new manner to look at both the structuring of the verb lexicon in young children and the organisation of the verb lexicon in adults. Concerning the utterances produced by young children we develop arguments which go against the notions, which they are usually given, of “error” or “metaphor”.
We also propose an extension of this formulation in order to achieve optimal modelling of pronunciation variations. Since different words will not in general exhibit the same amount of pronunciation variation, the procedure allows words to be represented by a different number of baseforms. The methods improve the subword description of the vocabulary words and have been shown to improve recognition performance on the DARPA Resource Management task.
The automatic recognition of acquired texts by a digitalizing tablet, opens the door to a new generation of computers, keyboardless and mouseless, that communicate with the user through an instrument that he is used to control since his youth: the pen. In this work, we studied various methods ofcooperation between a context analyzer and two classifier-experts that process the data in two different ways. The experimental results on a database of7000 letters and 12 writers are promising because they allowed a global improvement of efficiencies of 20%, leading to the general recognition rates of 84.2 % for letters and 64 % for words.
This article presents the various linguistic procedures used to create new forms of address (language creativity) and gives an account of some functions of the new address patterns in the management of social relations.
Among speech enhancement methods, the Ephraı̈m and Malah suppression rule (EMSR) has proven to be efficient in reducing the background noise while preventing from a common artefact: the musical noise. From psychoacoustic motivation, an implementation of the EMSR with a perceptually relevant frequency partition is proposed. This implementation is based on non-uniform oversampled filter-banks. The frequency resolution is nevertheless uniform on the equivalent rectangular bandwidth (ERB)-scale. Objective and subjective comparison with classical EMSR and uniform critically decimated filter-banks implementations has been achieved.
New techniques for automatic speaker recognition from telephone speech are described. The recognition is based on spectral analysis of fixed sentence-long utterances. From the whole utterance the form the time-dependent spectrogram. Normalization procedures are applied to the spectrogram to take account of spectral distortions introduced by the telephone transmission system and of amplitude variations in the utterance. The decision on the speaker's identity is arrived at by comparing the spectrogram of a sample utterance with stored reference spectrograms and calculating a measure of dissimilarity between the spectrograms. To perform this spectrogram comparison, it is necessary to take into account differences in speaking rate and to bring corresponding speech events of the utterances into exact time synchronisation. Time alignment is based on a measure of dissimilarity between speech events and is carried out by a dynamic programming algorithm which minimizes timing differences between corresponding speech events. A set of utterances spoken by cooperative speakers and transmitted via conventional dialed-up telephone lines was used for the evaluation of the system. Different versions of the recognition procedure were evaluated and compared. For identification as well as for verification, error rates of 2% or less have been obtained.
The talk gives an overview of prosodic research at the Department of Linguistics and Phonetics at Lund University since 1950. It shows how the word accents have been a prime motor in these activities. The interest first centered on local Fo configurations caused by the distinctive accents, their contextual and dialectal variation and perceptual cues for their identification. Further analyses of global aspects led to a compositional model of Swedish prosody in which the local accent shapes are seen as superposed on a global intonation. Some neglected areas of research are pointed out, e.g. perception and possible effects of general rules of economy. The talk ends with a plea for a more unified framework in prosodic analysis.
Some 550 vowel segments have been excised from a text read by a Dutch speaker, both at normal rate and at fast rate. The duration of each segment is measured, as well as static and dynamic formant characteristics, such as midpoint formant frequencies, and descriptions of the formant tracks in terms of 16 equidistant points per segment, or Legendre polynomial functions. We examined these formant characteristics as a function of vowel duration, but found no indication for duration-dependent undershoot. Instead, this speaker showed very consistent consonant-specific coarticulatory behavior and adapted his speaking style to the speaking rate in order to reach the same midpoint formant frequencies. Various (parabolically stylized) formant tracks, at various durations, in isolation or in CVC contexts, were synthesized and presented to listeners for identification. Net shifts in vowel responses, compared to stationary stimuli, showed no indication of perceptual overshoot. A weighted averaging method with the greatest weight to formant frequencies in the final part of the vowel tokens, explained the results best.
The processing scheme applies the Least Mean Squares (LMS) adaptive algorithm in frequency delimited sub-bands to process speech signals from simulated and real room acoustic environments with various realistic signal to noise ratios (SNR). The processing scheme aims to take advantage of binaural input channels to perform noise cancellation. The two wide-band signals are split into linear or cochlear distributed sub-bands, then processed according to their sub-band signal characteristics. The results of a series of intelligibility tests are presented in which speech and noise data, generated in simulated and real room conditions, was presented to human volunteer subjects at various SNRs, sub-band distributions and sub-band spacings. The results from both simulated and real room acoustical environments show that the MMSBA processing scheme significantly improves both SNR and intelligibility.
The paper proposes a technique to acquire morphological constructional (derivational) relations from dictionaries of synonyms in order to create morphological databases semi-automatically. It exploites the paradigmatic structure of the lexicons in several ways. The technique is based on the discovery of analogical (morpho-synonymic) quadruplets where semantic contraints defined as synonymic relations are combined with morphographical constraints. It is robust and language independent: it has been successfully applied to French and English dictionnaries ofsynonyms. The technique can be enhanced by typing the morpho-synonymic quadruplets in order to make explicit the fact that some pairs of lexemes are submitted to more constraints than others.
Smoothly bent paper-like surfaces are developable. They are however difficult to minimally parameterize since the number of meaningful parameters is intrinsically dependent on the actual deformation. Previous generative models are either incomplete, i.e. limited to subsets of developable surfaces, or depend on huge parameter sets. Our first contribution is a generative model governed by a quasi-minimal set of intuitive parameters, namely rules and angles. More precisely, a flat mesh is bent along guiding rules, while a number of extra rules controls the level of smoothness. The second contribution is an automatic multi-camera 3D reconstruction algorithm. First of all, the cameras and a sparse structure are reconstructed from the images using Structure-from-Motion method. A 2D parametrization of the reconstructed points is computed by dimensionality reduction. This parameterization is used to initialize the proposed model since it easily allows us to estimate the surface curvature. The initial model parameters are eventually tuned through model-based bundle-adjustment.
Some experiments have been carried out to study and compensate for within-speaker variations in speaker verification. To induce speaker variation, a speaking behaviour elicitation software package has been developed. A 50-speaker database with voluntary and involuntary speech variation has been recorded using this software. The database has been used for acoustic analysis as well as for automatic speaker verification (ASV) tests. The voluntary speech variations are used to form an enrolment set for the ASV system. This set is called structured training and is compared to neutral training where only normal speech is used. Both sets contain the same number of utterances. It is found that the ASV system improves its performance when testing on a mixed speaking style test without decreasing the performance of the tests with normal speech.
This paper describes the experimental set-up used by the SAM (ESPRIT-BRA Project no. 2589: Multilingual Speech Input/Output: Assessment, Methodology and Standardisation) group for evaluating the intelligibility of text-to-speech systems at sentence level. The SUS test measures overall intelligibility of Semantically Unpredictable Sentences which can be automatically generated using five basic syntactic structures and a number of lexicons containing the most frequently occurring mini-syllabic words in each language. The sentence material has the advantage of not being fixed, as words can be extracted from the lexicons randomly to form a new set of sentences each time the test is run. Various text-to-speech systems in a number of languages have been evaluated using this test. Results have demonstrated that the SUS test is effective and that it allows for reliable comparison across synthesisers provided guidelines are followed carefully regarding the definition of the test material and actual running of the test. These recommendations are the result of experience gained during the SAM project and beyond.
This paper addresses the problem of optimal feature extraction from a wavelet representation. For solving such a problem, we introduce a novel multiple kernel learning algorithm based on active constraints methods. When applied to wavelet kernel learning, our experimental results show that the approaches we propose are competitive with respect to the state of the art on Brodatz texture datasets.
This paper will challenge the simple notion of analogy, which relies entirely on phonological associations, and the assumption that frequency effects are incompatible with symbolic rules as they are applied in research on morphological processing. It will discuss the results of three experiments investigating the processing of Russian complex verbal morphology by adult native speakers of Russian, and American learners of Russian— two experiments on novel verb generation, and a lexical decision task.
This paper examines the question of whether and how the grammars proposed by linguists may be said to be 'realized' in adequate models of human sentence processing. We first review the assumptions guiding the so-called Derivational Theory of Complexity (DTC) experiments. Recall that the DTC experiments were taken to show that the theory of transformational grammar (TG) known as the Standard Theory was only a partially adequate model for human parsing. In particular, it was assumed (see Fodor et al., 1974) that the DTC experiments demonstrated that while the parser actually used the structural descriptions implicit in a transformational derivation, the computations it used bore little resemblance to the transformations proposed by a TG. The crucial assumptions behind the DTC were that (1) the processing model (or 'parser') performs operations in a linear, serial fashion; and (2) the parser incorporates a grammar written in more or less the same format as the competence grammar. If we assume strict seriality, then it also seems easier to embed an Extended Lexical Grammar, such as the model proposed in Bresnan (1978) (as opposed to a TG), into a parsing model. Therefore, this assumption plays an important role in Bresnan's critique of TG as an adequate part of a theory of language use. Both Fodor, Bever and Garrett (1974) and Bresnan (1978) attempt to make the grammatical rules compatible with the psycholinguistic data and with assumption (1) by proposing models that limit the amount of active computation performed on-line. They do this by eliminating the transformational component. However, we show that on-line computation need not be associated with added reaction time complexity. That is, we show that a parser that relates deep structure to surface structure by transformational rules (or, more accurately, by parsing rules tailored very closely after those of a transformational model) can be made to comport with the relevant psycholinguistic data, simply by varying assumption (1). In particular, we show that by embedding TG in a parallel computational architecture—an architecture that can be justified as a reasonable one for language use—one can capture the sentence processing complexity differences noted by DTC experimenters. Assumption (2) is also relevant to the evaluation of competing grammars as theories of language use. First we show that Bresnan (1978) must relax this assumption in order to make Extended Lexical Grammar compatible with the psycholinguistic results. Secondly, we analyze Tyler and Marslen-Wilson's (1977) and Tyler's (1980) claim that their experiments show that one cannot instantiate a TG in a model of parsing without varying assumption (2). This is because they insist that their experiments support an 'interactive model' of parsing that, they believe, is incompatible with the 'Autonomy of Syntax' thesis. We show that the Autonomy Thesis bears no relation to their 'interactive model'. Therefore, adopting this model is no barrier to the direct incorporation of a TG in a parser. Moreover, we show why meeting assumption (2), a condition that we dub the 'Type Transparency Hypothesis', is not an absolute criterion for judging the utility of a grammatical theory for the construction of a theory of parsing. We claim that the grammar need not be viewed as providing a parsing algorithm directly or transparently (assumption 2 above). Nevertheless, we insist that the theory of grammar figures centrally in the development of a model of language use even if Type Transparency is weakened in the ways that we suggest. Taken together, these considerations will be shown to bear on the comparative evaluation of candidate parsing models that incorporate transformational grammar, extended-lexical grammar, or the Tyler and Marslen-Wilson proposals.
Variations in rate-of-speech (ROS) produce variations in both spectral features and word pronunciations that affect automatic speech recognition systems. To deal with these ROS effects, we propose to use a set of parallel rate-specific acoustic and pronunciation models. Rate switching is permitted at word boundaries, to allow within-sentence speech rate variation, which is common in conversational speech. Because of the parallel structure of rate-specific models and the maximum likelihood decoding method, our approach does not require ROS estimation before recognition, which is hard to achieve. We evaluate our models on a large vocabulary conversational speech recognition task over the telephone. Experiments on the NIST 2000 Hub-5 development set show that word-level ROS-dependent modeling results in a 2.2% absolute reduction in word error rate over a rate-independent baseline system. Relative to an enhanced baseline system that models cross-word phonetic elision and reduction in a multiword dictionary, rate-dependent models achieve an absolute improvement of 1.5%. Furthermore, we introduce a novel method to modeling reduced pronunciations that are common in fast speech based on the approach of skipping short phones in the pronunciation models while preserving the phonetic context for the adjacent phones. This method is shown to also produce a small additional improvement on top of ROS-dependent acoustic modeling.
The debate between Emmanuel Macron and Marine Le Pen during the 2017 French presidential election campaign reveals the ambiguities of fact-checking in its claims to denounce lies propagated by public figures. While putting forward the principle of veracity, fact-checking seeks to identify falsehood rather than to expose truth. Fact-checkers delegate responsibility for determining truth to reliable sources. Analysis reveals that these sources are primarily institutional and are collectively considered to be legitimate. Fact-checking thus deploys a conservative approach to journalistic information, applied to all that is said in the public sphere.‪
This paper proposes a method of extracting the desired signal from a noisy signal, addressing the problem of segregating two acoustic sources as a model of acoustic source segregation based on Auditory Scene Analysis. Since the problem of segregating two acoustic sources is an ill-posed inverse problem, constraints are needed to determine a unique solution. The proposed method uses the four heuristic regularities proposed by Bregman as constraints and uses the instantaneous amplitude and phase of noisy signal components that have passed through a wavelet filterbank as features of acoustic sources. Then the model can extract the instantaneous amplitude and phase of the desired signal. Simulations were performed to segregate the harmonic complex tone from a noise-added harmonic complex tone and to compare the results of using all or only some constraints. The results show that the method can segregate the harmonic complex tone precisely using all the constraints related to the four regularities and that the absence of some constraints reduces the accuracy.
In this paper we consider a complex of language-related problems that research has identified in children with reading disorder and we attempt to understand this complex in relation to proposals about the language processing mechanism. The perspective gained by considering reading problems from the standpoint of language structure and language acquisition allows us to pose specific hypotheses about the causes of reading disorder. The hypotheses are then examined from the standpoint of an analysis of the demands of the reading task and a consideration of the state of the unsuccessful reader in meeting these demands. The remainder of the paper pursues one proposal about the source of reading problems, in which the working memory system plays a central part. This proposal is evaluated in the light of empirical research which has attempted to tease apart structural knowledge and memory capacity both in normal children and in children with notable reading deficiencies.
This paper describes a new model of intonation for English. The paper proposes that intonation can be described using a sequence of rise, fall and connection elements. Pitch accents and boundary rises are described using rise and fall elements, and connection elements are used to describe everything else. Equations can be used to synthesize fundamental frequency (F 0) contours from these elements. An automatic labelling system is described which can derive a rise/fall/connection description from any utterance without using prior knowledge or top-down processing. Synthesis and analysis experiments are described using utterances from six speakers of various English accents. An analysis/resynthesis experiment is described which shows that the contours produced by the model are similar to within 3.6 to 7.3 Hz of the originals. An assessment of the automatic labeller shows 72% to 92% agreement between automatic and hand labels. The paper concludes with a comparison between this model and others, and a discussion of the practical applications of the model.
This paper describes the first step of our research, a system which recognizes two speakers in each of Spanish and English and is limited to some four hundred words. The key new idea is that the speech recognition and the language analysis are tightly coupled by using the same language model, an augmented phrase-structure grammar, for both.
In this paper, we describe several objective and subjective methods to measure the performance of spoken language dialogue systems and their components. The focus is on the evaluation of spontaneous Human-Machine interaction, notably supported by information retrieval systems.
In this paper, we describe our contribution to the CEPT effort for the definition of a European standard for the cellular mobile radio.
We propose a descriptive model of forms of dyadic cooperative problem-solving activity, with associated learning mechanisms. Cooperative problem-solving is analysed in terms of three fundamental and gradual dimensions: symmetry, alignment and agreement. The first dimension relates to distribution of transactional roles, the second to action coordination, and the third to expression and resolution of disagreements. Combination of the three dimensions produces a space of eight main forms of cooperation, within which we situate "collaboration ". Illustrative analyses of face-to-face and computer-mediated interactions are presented, revealing relations between the dimensions of cooperation. Finally, we discuss related cooperative learning mechanisms, and possible extensions to the model required for analysing larger groups.
Articulatory parameters, vocal tract shape and cross-sectional area function were determined from fricative spectra. A model of fricative generation was used for providing acoustical constraints for an optimization procedure with muscles work as the criterion of optimality. A distance between spectra was measured with the use of the Cauchy-Bounjakovsky non-equality. A proper initial approximation of articulatory parameters is required to obtain an accurate and stable solution of the inverse problem.
In the field of Automatic Speech Recognition (ASR) research, it is conventional to pursue those approaches that reduce the word error rate. However, it is the authors' belief that this seemingly sensible strategy often leads to the suppression of innovation. The leading approaches to ASR have been tuned for years, effectively optimizing on test data for a local minimum in the space of available techniques. In this case, almost any sufficiently new approach will necessarily hurt the accuracy of existing systems and thus increase the error rate. However, if progress is to be made against the remaining difficult problems, new approaches will most likely be necessary. In this paper, we discuss some research directions for ASR that may not always yield an immediate and guaranteed decrease in error rate but which hold some promise for ultimately improving performance in the end applications. Issues that will be addressed in this paper include: discrimination between rival utterance models, the role of prior information in speech recognition, merging the language and acoustic models, feature extraction and temporal information, and decoding procedures reflecting human perceptual properties.
Inferring gene regulatory networks tends to use several biological information. Here we use data from genetic markers and expression data in the framework of discrete static bayesian networks. We compare several scores and also the impact of a network connectivity a priori. We propose and compare two models with existing approaches of gene regulatory network inference. On simulated data one of our models reached better results in the case ofsmall sample size. We use this model on real data in Arabidopsis thaliana.
It is well known that speaker variability caused by accent is one factor that degrades performance of speech recognition algorithms. If knowledge of speaker accent can be estimated accurately, then a modified set of recognition models which addresses speaker accent could be employed to increase recognition accuracy. In this study, the problem of language accent classification in American English is considered. A database of foreign language accent is established that consists of words and phrases that are known to be sensitive to accent. Next, isolated word and phoneme based accent classification algorithms are developed. The feature set under consideration includes Mel-cepstrum coefficients and energy, and their first order differences. It is shown that as test utterance length increases, higher classification accuracy is achieved. Isolated word strings of 7–8 words uttered by the speaker results in an accent classification rate of 93% among four different language accents. A subjective listening test is also conducted in order to compare human performance with computer algorithm performance in accent discrimination. The results show that computer based accent classification consistently achieves superior performance over human listener responses for classification. It is shown, however, that some listeners are able to match algorithm performance for accent detection. Finally, an experimental study is performed to investigate the influence of foreign accent on speech recognition algorithms. It is shown that training separate models for each accent rather than using a single model for each word can improve recognition accuracy dramatically.
The history and sociology of a special group of civil servants are presented, namely: stenographers in parliament, an occupation closely tied to the history of parliamentary government in most democracies. Based on original material drawn from sociology and history, this inquiry seeks to see how institutions are shaped, produced and reproduced through the knowledge and deeds that incarnate values and shape institutions. Besides the publication of parliamentary proceedings, the technical, material and social conditions are discussed that led the National Assembly to become a political institution.
This paper is dedicated to a floating block FFT error analysis. A statistical model of the error propagation in a pass is developped which considers separately the three following parts: the input, arithmetic roundoff and coefficient quantization errors.
This is largely due to the complexity of identifying and categorising the emotion factors in natural human speech, and implementing these factors within synthetic speech. Such models could also be used as practical tools in the investigation and validation of models of emotion and other speech-altering Stressors.
Achieving reliable performance for a speech recogniser is an important challenge, especially in the context of mobile telephony applications where the user can access telephone functions through voice. The breakthrough of such a technology is appealing, since the driver can concentrate completely and safely on his task while composing and conversing in a “full” hands-free mode. This paper addresses the problem of speaker-dependent discrete utterance recognition in noise. Special reference is made to the mismatch effects due to the fact that training and testing are made in different environments. A novel technique for noise compensation is proposed: nonlinear spectral subtraction (NSS). Robust variance estimates and robust pdf evaluations (projection) are also introduced and combined with NSS into the HMM framework. We show that the lower limit of applicability of the projection (low SNR values) can be loosened after combination with NSS. The performance of an HMM-based recogniser rises from 56% (no compensation) to 98% after speech enhancement. More than 3300 utterances have been used to evaluate the systems (three databases, two European languages). This result is achieved by the use of robust training/recognition schemes and by preprocessing the noisy speech by NSS.
We provide experimental results that compare the propagation algorithm developed for the possibilistic product-based networks and the inference algorithm developed in this paper.
The history of social control applications of psychology and the likelihood of a future increase in their importance are assessed. The effects of military funding of psychological research and the social consequences of very widespread unemployment are specifically considered. It is argued that psychology and related areas of cognitive science and neuroscience may well become increasingly relevant in the development of the technical components of such techniques rather than in providing ideological justifications for their use.
Assuming that the pixel values in the images are proportional to some conserved quantity, a new penalty function is defined for motion estimation of a deforming body. We use the theory of linear elasticity and the conservation laws of continuum medium to propose new constraining terms. The introduction of a deformation model gives a new interpretation of the Song and Leahy's solution [Song 91]. Examples of experiments using simulated and real images of deforming body are presented. The method is able to take into account compressible or incompressible motion according to the parameter values.
This article describes fully Bayesian algorithms to unmix hyperspectral images. Each pixel of the hyperspectral image is decomposed as a combination of pure endmember spectra according to the linear mixing model. In a supervised context, the endmembers are assumed to be known. The unmixing problem consists of estimating the mixing coefficients under positivity and additivity constraints. An appropriate distribution is chosen as prior distribution for these coefficients, that are estimated from their posterior distribution. A Markov chain Monte Carlo (MCMC) algorithm is developed to approximate the estimators. In a semi-supervised framework, the spectra involved in the mixtures are assumed to be unknown. They are supposed to belong to a known spectral library. A reversible-jump MCMC algorithm allows one to solve the resulting model selection problem. Finally, in a final step, the previous algorithms are extended to handle the unsupervised unmixing problem, i.e., to estimate the endmembers and the mixing coefficients jointly. This blind source separation problem is solved in a lower-dimensional space, which effectively reduces the number of degrees of freedom of the unknown parameters.
In this paper, we are interested in how to structure the terms of a domain, i.e acquiring relations between terms. The terminologies can no more just make a list of the terms used in a domain and succinctly organize them in a hierarchie. The terminologies have to be an adapted answer to the needs of the applications and propose a variety of relations which better reflects the knowledge of the domain. We constrast the place that traditionally the theory gives to the relations in the terminologies with the real needs of the constitution of terminologies and their use and recycling in applications. Then, we present existing approaches to acquire relations between terms from specialized corpora.
The process of spoken word-recognition breaks down into three basic functions, of access, selection and integration. Access concerns the mapping of the speech input onto the representations of lexical form, selection concerns the discrimination of the best-fitting match to this input, and integration covers the mapping of syntactic and semantic information at the lexical level onto higher levels of processing. This paper describes two versions of a “cohort”-based model of these processes, showing how it evolves from a partially interactive model, where access is strictly autonomous but selection is subject to top-down control, to a fully bottom-up model, where context plays no role in the processes of form-based access and selection. Context operates instead at the interface between higher-level representations and information generated on-line about the syntactic and semantic properties of members of the cohort. The new model retains intact the fundamental characteristics of a cohort-based word-recognition process. It embodies the concepts of multiple access and multiple assessment, allowing a maximally efficient recognition process, based on the principle of the contingency of perceptual choice.
This paper presents a segmentation system that automatically splits into letters a set of handwritten words. This is made in order to built a learning base for an on-line recognition system dedicated to handwritten words. The drawn words to be segmented are entered on a digitizing tablet before being converted into a sequence of vectors. At last, the corresponding alphabetic word is associated to each of these drawn words. The description of our segmentation system is followed by an experimental test of letter segmentation applied on a database of 10000 words coming from 10 different scriptors.
A computational simulation was used to generate impulse responses between points in a rectangular room and two points on opposite sides of a spherical “head”. Sounds were convolved with the impulse responses to generate stimuli with which to study the effects of reverberation on the ability of listeners to use differences in fundamental frequency (Δ Fos) to separate concurrent vowels. Experiment 1 verified the suitability of the simulation by showing that it produced (i) appropriate percepts of lateralization, (ii) a larger contribution to lateralization from interaural differences in timing than level, and (iii) no effects of reverberation on lateralization. Experiments 2–5 measured masked identification thresholds for synthetic harmonic “target” vowels in the presence of masking sounds. In Experiment 2, listeners identified targets against pink-noise maskers. The experiment established a spatial geometry and a degree of reverberation for which listeners did not benefit from binaural cues arising from the spatial geometry of the sources. Experiment 3 demonstrated that the same arrangement did not undermine the ability to use Δ Fos to separate targets from vowel-like maskers when both had static Fo contours, but did prevent listeners from using Δ Fos carried on coherently changing Fo contours. Experiment 4 showed that a modulation width of ±1.45% was sufficient to reduce the benefits of Δ Fos, but that the benefits were not eliminated until the width of modulation exceeded the Δ Fo. It is argued that these results are compatible with existing models of the ability to use Δ Fos to separate concurrent vowels and that reverberation undermines the ability when the Fos are changing by diffusing the periodicities of the competing sources. Finally, Experiment 5 demonstrated that reverberation had no effect on the ability to separate a modulated vowel from pink noise. Thus, reverberation may have its detrimental effects in these experiments by diffusing the periodicity of the masking sounds rather than the targets. Overall, the experiments demonstrate that Δ Fos can be more robust cues for separating concurrent sounds than binaural cues. The relevance of these results to the perception of natural continuous speech is discussed.
The first contains neural networks for transformation of ordinary signal processing cepstral parameters into a set of continuously valued acoustic-phonetic features for each frame of the speech signal and for its division into acoustic phonetic segments. The output from the first stage, which is a combination of the results of segmentation and acoustic-phonetic segments, gives a first estimate of the sequence of phonemes. The second stage contains an expert system consisting of allophonic rules, a lexicon of transcriptions of the words belonging to the selected application vocabulary, syntax rules and the overall control structure. In this stage the incoming string of phonemes is processed using the lexicon and an island-driven parsing system. The current vocabulary consists of 35 words given by a CAD-like application.
This paper presents a comparative study investigating the relation between the timing of a rising or falling pitch movement and the temporal structure of the syllable it accentuates for three languages: Dutch, French and Swedish. In a perception experiment, the five-syllable utterances /mamamamama/ and / a a a a a/ were provided with a relatively fast rising or falling pitch movement. The timing of the movement was systematically varied so that it accented the third or the fourth syllable. Subjects were asked to indicate which syllable they perceived as accented. The accentuation boundary (AB) between the third and the fourth syllable was then defined as the moment before which more than half of the subjects indicated the third syllable as accented and after which more than half of the subjects indicated the fourth syllable. The results show that there are significant differences between the three languages as to the location of the AB. In general, for the rises, well-defined ABs were found. They were located in the middle of the vowel of the third syllable for French subjects, and later in that vowel for Dutch and Swedish subjects. For the falls, a clear AB was obtained only for the Dutch and the Swedish listeners. This was located at the end of the third syllable. For the French listeners, the fall did not yield a clear AB. This corroborates the absence of accentuation by means of falls in French. By varying the duration of the pitch movement it could be shown that, in all cases in which a clear AB was found, the cue for accentuation was located at the beginning of the pitch movement.
In this paper a Walsh linear coding algorithm (WLC) is developed which yields a compressed approximation to the Walsh power spectrum of a time frame. This algorithm is interpolative, rather than predictive. It minimizes the mean square interpolative error, and it yields WLC model spectra. The effectiveness of utilizing WLC model spectra in speech recognition systems is explored. Each of two unity gain distortion measures is embedded in a dynamic time warp based speaker-dependent isolated word recognition system and is tested by means of a 10-digit vocabulary.
Trần Đức Thảo and Jean Piaget both aimed to revolutionize epistemology by combining scientific discoveries about the evolution of human intelligence and the development of our cognitive abilities. This point calls to mind the recapitulation theory in biology, which is false and obsolete, and therefore seems to condemn their efforts. We shall demonstrate, however, that this notion of recapitulation was reelaborated by the two scholars.
A time-domain simulation method of the vocal-tract system is described. The system, composed of a constant air pressure source, a time-varying narrow section representing the glottis and a tube corresponding to the vocal tract coupled with the nasal cavity was assumed as a vocal-tract model. The acoustic equations that govern the generation and the propagation of acoustic waves inside the model were transformed into the discrete variable representation by applying certain rules; the rectangular rule in space and the trapezoid rule in time. This particular manner of discretization causes spectral distortion due to the frequency warping. A theoretical analysis indicated that this warping can be interpreted as the manifestation of the frequency dependent phase velocity of waves in the discrete system. The magnitude of the warping depends on both the sampling frequency (f s) and the sampling interval in space (X). Eleven French vowels synthesized with f s = 20 kHz and X = 1 cm sounded very natural and highly intelligible, even though a trace of the frequency warping was noticeable at the third formant frequency region on their spectra. When f s = 40 kHz was used, the effect of the spectral distortion became practically negligible for frequencies below 4 kHz.
This paper reports on the speech synthesis module used to present spoken traffic messages through the car radio, as part of the “Traffic Message Control” system RDS-TMC. One of the basic ideas of this intended Pan-European service is its ability to provide traffic information in the driver's native language, independent of the language used in the geographical area or used for broadcasting. To accomplish this for an unlimited set of location names, speech synthesis is a must. A prototype has been developed for German. Other countries and languages will follow in 1998.
This paper promotes an individual-based approach for solving a well-known problem, the stable marriage problem. In this approach, a solution is the output of an emergent phenomena due to the negotiation between the agents. The agentification of the seminal Gale-Shapley algorithm is based upon two distinct behaviours (the proposer and the responder) negotiating to reach a stable matching which is not fair. The CASANOVA agent behaviour we propose in this paper relies on a society of agents that play at the same time both of these roles in multi bi-lateral negotiations. The agents applies the minimal concession strategy for negotiation to ensure their individual welfare. The emergent solutions are fair and they cannot be reached by classical multi-agent methods, whereas CASANOVA is decentralized and privacy-preserving.
A three-dimensional (3D) tongue model has been developed using MR images of a reference subject producing 44 artificially sustained Swedish articulations. Based on the difference in tongue shape between the articulations and a reference, the six linear parameters jaw height, tongue body, tongue dorsum, tongue tip, tongue advance and tongue width were determined using an ordered linear factor analysis controlled by articulatory measures. The first five factors explained 88% of the tongue data variance in the midsagittal plane and 78% in the 3D analysis. The six-parameter model is able to reconstruct the modelled articulations with an overall mean reconstruction error of 0.13 cm, and it specifically handles lateral differences and asymmetries in tongue shape. In order to correct articulations that were hyperarticulated due to the artificial sustaining in the magnetic resonance imaging (MRI) acquisition, the parameter values in the tongue model were readjusted based on a comparison of virtual and natural linguopalatal contact patterns, collected with electropalatography (EPG). Electromagnetic articulography (EMA) data was collected to control the kinematics of the tongue model for vowel-fricative sequences and an algorithm to handle surface contacts has been implemented, preventing the tongue from protruding through the palate and teeth.
SAS (Synthetic Aperture Sonar) has been used in sea bed imagery. Indeed, high resolution images provided by SAS are of great interest, especially for the detection, localization or eventually classification of objects lying on sea bed. But, SAS images are highly corrupted by a granular multiplicative noise, called speckle noise, which reduces spatial and radiometric resolutions. For this reason, an automatic analysis of these images is not so evident. A solution can consist on the use of a filtering before process, without a spatial resolution degradation. The purpose of this article is to present a new process consisting on the jointly use of the stochastic matched filter and an autoadaptive mean filter. Furthermore, in order to well preserve the spatial resolution, we propose to use as a criteria for the stochastic matched filter the minimization between the speckle noise local statistics with the removal signal ones, allowing a subimage size adaptation. Results obtained on real SAS data are proposed and compared with those obtained using other stochastic matched filtering based denoising methods.
The emergence of the lexicon for events is rather late in L1 acquisition, compared to the lexicon for entities, but the same holds for subsequent L2 acquisition. The semantic processing of verbs by children and adults shows a greater flexibility than the processing of nouns: explanations to that are the relational nature of verbs, and the fact that the lexical categorization of events is less determined by perception than structured by language-specific lexicalisation patterns. When a child's variety – or a learner variety – contains few verbs, their semantic flexibility will be maximally used, notably by analogical processes.
In this paper, the present situation in Japan regarding development and application of the electro-palatography is reviewed. Then, data on lingual articulation with respect to personal differences and child characteristics are reported. These data were obtained by combining the three-dimensional measurements of the plaster casts of the hard palates of fifteen adults, thirty children, and two children at different dental stages, with the electro-palatographic observation of the lingual contact patterns with the hard palates of the adult subjects and four children during utterances of the Japanese consonants.
In this paper, we propose a technique for detection and segmentation of skin color areas. Our method is using data mining techniques in order to produce classification rules, followed by segmentation phase in coherent regions of skin using that rules for decision making. Experimental results on the representative database of images show effectiveness and high reliabilty of our approach.
A method is described for designing speaker recognition features that are robust to telephone handset distortion. The approach transforms features such as mel-cepstral features, log spectrum, and prosody-based features with a non-linear artificial neural network. The neural network is discriminatively trained to maximize speaker recognition performance specifically in the setting of telephone handset mismatch between training and testing. The algorithm requires neither stereo recordings of speech during training nor manual labeling of handset types either in training or testing. Results on the 1998 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation corpus show relative improvements as high as 28% for the new multilayered perceptron (MLP)-based features as compared to a standard mel-cepstral feature set with cepstral mean subtraction (CMS) and handset-dependent normalizing impostor models.
In this paper is presented the DESAM project which was divided in two parts. Most aspects that have been considered in this project have led to the proposal of new methods which have been grouped together into the so-called DESAM Toolbox, a set of Matlab® functions dedicated to the estimation of widely used spectral models for music signals. Although those models can be used in Music Information Retrieval (MIR) tasks, the core functions of the toolbox do not focus on any specific application. It is rather aimed at providing a range of state-of-the-art signal processing tools that decompose music recordings according to different signal models, giving rise to different “mid-level” representations.
This paper proposes an algorithm based on F0 patterns to hypothesize word boundaries and function words in continuous speech in Hindi. It makes use of the properties of F0 contour such as declination tendency, resetting and fall-rise patterns in Hindi. The syllabic units are identified by using the energy contour, pitch and the first order LP coefficient. Each syllabic unit is assigned an accent value L (Low), H or h (High) by (i) comparing the F0 value at the mid point of each syllabic nucleus with that of the previous syllabic unit and (ii) comparing the F0 values at two different points within each syllabic unit in a sequence having an accent value L. Word boundaries are placed between the adjacent syllabic units (i) H and L, (ii) h and L, (iii) L and L, (iv) L and h and (v) H and h. An evaluation conducted on a corpus of 50 sentences in Hindi read aloud by five native speakers in an ordinary office environment showed that about 74 percent of the word boundaries and about 28 percent of the function words were correctly identified. The results of the word boundary hypothesization can be used to improve the performance of the acoustic-phonetic, lexical and syntactic modules in a speech-to-text conversion system. Robustness of the algorithm in handling noisy speech input conditions and telephone speech are also discussed.
This paper describes PEGASUS, a spoken dialogue interface for on-line air travel planning that we have recently developed. PEGASUS leverages off our spoken language technology development in the ATIS domain, and enables users to book flights using the American Airlines EAASY SABRE system. The input query is transformed by the speech understanding system to a frame representation that captures its meaning. The tasks of the System Manager include transforming the semantic representation into an EAASY SABRE command, transmitting it to the application backend, formatting and interpreting the resulting information, and managing the dialogue. Preliminary evaluation results suggest that users can learn to make productive use of PEGASUS for travel planning, although much work remains to be done.
The last several years have been an exciting time at AT&T in the field of advanced speech applications for telecommunications: technical progress and platform/processor advances have enabled the identification, development and testing of a range of new services. During this period, prior to the divestiture of AT&T of Lucent Technologies and NCR, AT&T brought together, under a single corporate `roof', a research laboratory committed to advancing speech technology, business organizations building platforms to leverage this technology for telecommunications applications, and yet other business organizations with responsibility for deploying speech-enabled services to facilitate the use and reduce the cost of telecommunications services for both consumers and businesses. While this period of our corporate history has drawn to a close, we can look back to provide an overview of how technical progress, platform advances and network services needs and opportunities interacted to make speech technology an everyday experience for millions of people – and some of the lessons we learned along the way.
Previous work has shown that it is possible to train a multi-layer perceptron to estimate the voice fundamental period (Tx) for multiple speakers in the presence of high levels of background noise. The algorithm has been implemented in real-time on a TMS320C25 based development system. A prototype pocket-sized portable device has been constructed and the real-time software transferred to it. This will provide the basis for a new generation of signal processing hearing aids for the profoundly and totally deaf. Power supply current is sufficiently low for battery operation for periods of 12 hours between charges. The basic algorithm has been adapted to provide the higher time resolution which will make it applicable to a wide range of other applications.
This paper addresses the implementation of a dynamic bayesian network, with exogenous continuous inputs, for the classification of discrete irregularly spaced events sequences. Mixture models, estimated using the EM algorithm, are used to model the conditional probability tables, which depends on the vector of distances between events. The application framework of this study is the classification of railway trucks singular points for the Paris metro. The proposed approach reveals also good parsimony properties as soon as the model order is greater or equal two. This implementation may improve decisions provided by a specific rail defect sensor.
Recently, techniques motivated by human auditory perception are being applied in main-stream speech technology and there seems to be renewed interest in implementing more knowledge of human speech communication into a design of a speech recognizer. The paper discusses the author's experience with applying auditory knowledge to automatic recognition of speech. It advances the notion that the reason for applying of such a knowledge in speech engineering should be the ability of perception to suppress some parts of the irrelevant information in the speech message and argues against the blind implementation of scattered accidental knowledge which may be irrelevant to a speech recognition task. The following three properties of human speech perception are discussed in some detail: • limited spectral resolution, • use of information from about syllable-length segments, • ability to ignore corrupted or irrelevant components of speech. It shows by referring to published works that selective use of auditory knowledge, optimized on and in some cases derived from real speech data, can be consistent with current stochastic approaches to ASR and could yield advantages in practical engineering applications.
This study examined the production of English consonants by native speakers of Italian. The 240 adult native Italian speakers of English who participated had begun learning English when they emigrated to Canada between the ages of 2 and 23 years. Word-initial, word-medial and word-final tokens of English stops and fricatives were assessed through forced-choice judgments made by native English-speaking listeners, and acoustically. The native Italian subjects' ages of learning (AOL) English exerted a systematic effect on their production of English consonants even though they had lived in Canada for an average of 32 years, and reported speaking English more than Italian. In all but two instances, one or more native Italian subgroup defined on the basis of AOL differed significantly from subjects in a native English (NE) control group. The AOL of the first native Italian subgroup to differ from the NE subjects varied across consonant and syllable position. The results are discussed in terms of hypotheses proposed in the literature concerning the basis of segmental errors in L2 speech production.
Normalization of formant frequencies have frequently been used to eliminate inter-speaker differences in vowel recognition. However, estimation of formant frequencies becomes difficult under certain circumstances, such as for telephone speech. This paper presents an approach to vowel normalization based on frequency warped spectral matching. A frequency normalized distance between test and reference spectra is defined on the basis of the minimum mean square difference over all possible choices of frequency warping functions under certain nonlinearity constraints and boundary conditions. After adaptively eliminating spectral slope differences due to the individual glottal characteristics, the spectral distance is computed by means of dynamic programming. The vowel identification experiments were conducted on the nine American English vowels in /hvd/ utterances spoken by 12 male and 12 female speakers. The results indicated that the frequency warping method substantially increased the identification scores for female vowels when the male vowels were used as reference. They also indicated that although the improvement in identification was attributed mainly to the linear frequency scaling, an additional improvement for vowel /ae/ was obtained by a slight nonlinear frequency warping. In addition, an application to speaker normalization for word detection in connected speech is discussed.
Nevertheless, this model has two drawbacks: first, the number of parameters grows exponentially with the order of the chain, secondly, the parameters are difficult to estimate. The proposed solutions for improving the model are probability smoothing or variable memory length Markov models, which can be represented by a tree known as Prediction Suffix Tree. In this paper, we propose an other improvement, based on a statistical test, which aim is to decide if the model of the sequence to classify includes subsequences (domains, patterns...) which are not in the model of its class and conversely. As illustration, we compare the results given by different models on the E. coli DNA sequences from the UCI repository of machine learning database and we show how the choice of parameters of these models influences the performances.
This paper proposes a new methodology to automatically build semantic hierarchies suitable for image annotation and classification. The aim is to provide a measure that best represents image semantics. We then propose rules based on this measure, for the building of the final hierarchy, and which explicitly encode hierarchical relationships between different concepts. Therefore, the built hierarchy is used in a semantic hierarchical classification framework for image annotation. Our experiments and results show that the built hierarchy improves significantly the classification accuracy.
This paper discusses some important topics in current speech synthesis research. Modeling of speaker characteristics and emotions are used as an example of new trends in the speech synthesis field. The relation to speech recognition research is emphasized. New methods such as automatic learning and the use of new analysis techniques are also discussed.
The stop /p,t,k/ recognition part of a speaker-independent speech recognition system is described in this paper. This work is based on the conclusions of several perceptual experiments and on the results of an acoustic investigation with stop consonants. These experiments allowed us to evaluate the discrimination power of the burst regarding the stop place of articulation, and how the vocalic information may help stop identification, which could not be done efficiently without taking into account the nature of the following vowel. Thus, a novel system architecture is proposed which is made up of two stages: first, an automatic detector of reliable cues regarding stop and vowel features, and, then, a context sensitive multilayered perceptron (ODWE) fed by the previous acoustic cues. The results show a recognition rate of 90% over the stop consonants.
Most difficulties arising in the development of speech understanding systems come from the great uncertainty inherent in the acoustic signal. The inaccuracy of acoustic-phonetic decoders makes it necessary to use higher level information: lexical, syntactic and semantic. We present here a possible solution for integrating some of this information in the particular case of an administrative database questioning system. First, we try to sketch how contextual knowledge can be extracted from local or global representations of an utterance, such as partial recognition or the dialogue history in the case of man-machine interaction.
Discussions at the ESCA-NATO Workshop on Speech Under Stress (Lisbon, Portugal, September 1995) centred on definitions and models of stress and its effects. Based on the Workshop discussions, in this paper we attempt to produce a definition of stress, and propose a number of stress models which clarify issues in this field, and which might be adopted by the speech community. The concept of stress is very broad and used differently in a number of domains — a definition of stress has thus remained elusive. Greater separation of Stressors (the causes of stress) and strain (the effects of stress) is proposed, and methods for relating Stressors to strains are presented. Suggestions for future research directions in this field are also made.
In this paper, methods are proposed for facial feature detection (eyes, brows, nose, mouth, chin) and for facial expression recognition. The methods are based on modified versions of the standard Active Appearance Model proposed by Cootes et al. [11] to control both the shape and the texture of a given face. The detection algorithm makes use of an active appearance model computed on hierarchical Gabor descriptions a set of training faces. In a second part, two expression models are proposed, based on the standard AAM, and used to recognize and then to cancel or modify the facial expression of a given unknown face.
The hippocampus and the amygdala are two brain structures which play a central role in several fundamental cognitive processes. Their segmentation from Magnetic Resonance Imaging (MRI) scans is a unique way to measure their atrophy in some neurological diseases, but it is made difficult by their complex geometry. Their simultaneous segmentation is considered here through a competitive homotopic region growing method. It is driven by relational anatomical knowledge, which enables to consider the segmentation of atrophic structures in a straightforward way. For both structures, this fast algorithm gives results which are comparable to manual segmentation with a better reproducibility. Its performances regarding segmentation quality, automation and computation time, are amongst the best published data.
In this paper, we develop data driven registration algorithms, relying on pixel similarity metrics, that enable an accurate rigid registration of dissimilar single or multimodal 2D/3D medical images. Gross dissimilarities are handled by considering similarity measures related to robust M-estimators. Fast stochastic multigrid optimization algorithms are used to minimize these similarity metrics. The proposed robust similarity metrics are compared to the most popular standard similarity metrics on real MRI/MRI and MRI/SPECT image pairs showing gross dissimilarities. Our robust similarity measures compare favourably with all standard (non robust) techniques.
This study aims at analysing, modelling and simulating human capabilities of planning and interaction. The experimental protocols have been analyzed from the planning point of view and from the interaction point of view. The planning model and the interaction model are integrated homogeneously into an architecture called BDIggy. The human interaction model is twofold: 1) it is based on the speech act theory to model the utterances, with a set of performatives applied to beliefs and desires 2) it uses a discourse model, represented by timed automata, to describe the dynamics of human conversations. In BDIggy, interaction and planning are linked thanks to the BDI concepts. The BDIggy architecture is validated with the use of a "Turing-like" test.
In the field of telemedicine, eHealth aims at remotely monitoring the health status of the patient living independently at home. In this paper we describe some principles of such systems with some running examples.
This paper is dedicated to the study of the main properties of the so called 'Relational Principal Components Analysis' (RPCA), that achieves the analysis of a random vector, with respect to the prior knowledge of one binary relationship upon the underlying probabilistic space. We detail the relational covariance and expectation properties that are the grounds of this technique, which whilst not being novel, remains scarcely studied. The paper presents with didactic examples for the properties we previously addressed and throw some light on interpretations in RPCA.
Often, decision-making processes are embedded in “official” procedures to address focuses in any case. However, procedures lead often to sub-optimal solutions for any specific decision making, and actors are obliged to develop practices to address the specificity of the context in which a decision is made. This opposition between procedure and practices is well known in different domains (prescribed and effective tasks, logic of functioning versus logic of use, etc.). We have shown what the differences were in a context-based formalism called Contextual Graphs. In this paper, we discuss the possibility to use of “good” and “bad” practices for the training of human actors, thanks to an incremental acquisition of knowledge and the learning of new practices by a system. We discuss these aspects in the framework of a real-world application in road safety (modeling of drivers' behaviors), but ideas can be easily reused in other domains.
In this paper, the AM–FM modulation model is applied to speech analysis, synthesis and coding. The AM–FM model represents the speech signal as the sum of formant resonance signals each of which contains amplitude and frequency modulation. Multiband filtering and demodulation using the energy separation algorithm are the basic tools used for speech analysis. First, multiband demodulation analysis (MDA) is applied to the problem of fundamental frequency estimation using the average instantaneous frequency as estimates of pitch harmonics. The MDA pitch tracking algorithm is shown to produce smooth and accurate fundamental frequency contours. Next, the AM–FM modulation vocoder is introduced, which represents speech as the sum of resonance signals. A time-varying filterbank is used to extract the formant bands and then the energy separation algorithm is used to demodulate the resonance signals into the amplitude envelope and instantaneous frequency signals. Efficient modeling and coding (at 4.8–9.6 kbits/sec) algorithms are proposed for the amplitude envelope and instantaneous frequency of speech resonances. Finally, the perceptual importance of modulations in speech resonances is investigated and it is shown that amplitude modulation patterns are both speaker and phone dependent.
The results of a recognition experiment, conducted on a speaker independent continuous Italian speech database, are reported in this paper. The recognition system is based on mixture density hidden Markov models of phonetic units. Different sets of units were tested, beginning with the most general context independent phones and ending with the most specific triphones; function word dependent units were also investigated. The recognition, based on a vocabulary of 979 words, was performed with no linguistic constraints, i.e., with a word branching factor of 979. The results confirm the effectiveness of context dependent units, for which a word accuracy of nearly 80% was obtained on a set of 300 sentences.
The problem of association rides mining is of particular interest in data mining research. Many knowledge is represented in the form of association rules and the exhaustive search for these rules is often very expensive in time when the data are dense and highly correlated. We propose a new approach based on a non-exhaustive search of association rules hidden in the data. The proposed algorithm (i) extracts the best association rules given a measure of quality, (ii) extracts "nuggets" of knowledge from the data (i.e., the association rules showing a weak support and a strong confidence).
In this paper a selective overview is given of methods used for the evaluation of text-to-speech (TTS) systems, with some comments on their advantages and disadvantages. The overview is confined to subjective methods of evaluation, i.e. methods which make use of human listeners. Objective methods, which try to assess quality by means of signal processing techniques, are not considered. In Section 1, four factors affecting the form of the evaluation, i.e. TTS-system component, text level, aspect of speech, and function, are discussed. In Sections 2 and 3 the methods themselves are presented, related to linguistic and phonetic/acoustic TTS-modules, respectively. Finally, in Section 4, some general conclusions are drawn.
Two experiments are reported concerning the detection of lexical stress in isolated English words. A non-statistical method using rules based on pitch variations is proposed. Better results are obtained with this method than when using static values of pitch as stress correlates. To avoid the problem of syllable segmentation, two methods based on pitch and energy function contours are presented to find stressed regions in words. These regions can be considered as regions of phonetic reliability. The second experiment analyzes lexical stress in words pronounced with a rising pitch. Maximum energy is reported to be the best correlate of stress, followed by duration. Finally, it is proposed that the study of lexical stress in continuous speech be accompanied by the study of prosodics and their general use in sentences.
Interfaces are direct communication devices between a person and a machine that do not rely on the activation of peripheral nerves or muscles. Brain-Machine Interfaces are based on the acquisition and analysis of the brain activity which is then converted to control signals for the device. Then, we present a state of the art of the present BMI devices specifically designedfor helping severely handicapped people to communicate and control systems.
Mutual dependence of articulatory parameters allows the reducing of codebook volume and helps to improve conditions for global optimum search. The initial approximations of articulatory vectors for the inverse problem solving are sampled along the trajectories of articulatory parameters in synthesized syllables. Piece-wise linear mapping of the space of articulatory parameters onto the space of acoustic parameters, the minimal value of cross-sectional area of the vocal tract and the Reynolds number accelerate the process of optimization over 100 times.
In this paper a new criterion, named HVS (heuristic for Variables Selection), to evaluate a set ofcandidate features and select an informative subset to be used as input data for a connectionist model, is presented. HVS criterion allows analysis of influence input variables have on the output of a connectionist model. The primary aim of our method is the selection of an appropriate subset ofinput variables to estimate the best and more parsimonious model.
This paper deals with the use of image processing techniques for tiling the time-frequency plane. This technique is applied on seismic wave separation. We consider data recorded by a linear array of sensors. For each recorded signal, the application of a time-frequency transform allows a two dimensional representation where the different seismic events are well localized and isolated. The segmentation by the watershed algorithm applied on each representation enables the definition of the time-frequency filters leading to the separation of the different waves. Then, in order to apply the separation algorithm to all the different recorded signals, we use the continuity from one signal to the other to perform the tracking of the different waves from one image to the next. After an initialisation step, this leads to an automatic algorithm. This algorithm is validated on a real data set and compared with a classical method. In comparison, the proposed method has the advantage to separate all the different waves simultaneously and without introducing artefact in the spatial domain. The limit of the algorithm is reached when the patterns associated to the different waves are not correctly separated in the time-frequency representation.
We present a new algorithm that extends the Reinforcement Learning framework to Partially Observed Markov Decision Processes (POMDP). The main idea of our method is to build a state extension, called exhaustive observable, which allow us to define a next processus that is Markovian. We bring the proof that solving this new process, to which classical RL methods can be applied, brings an optimal solution to the original POMDP. We apply the algorithm built on that proof to several examples to test its validity and robustness.
The goal of the Langues et Civilisations à Tradition Orale (LACITO) Linguistic Archive project is to conserve and disseminate recorded and transcribed oral literature and other linguistic materials, mainly in unwritten languages, giving simultaneous access to sound recordings and text annotation. The project uses XML markup for the kinds of annotation traditionally used in field linguistics. Transcriptions are segmented into sentences (roughly) and words. Annotations are associated with different levels: metadata at the text level, free translation at the sentence level, interlinear glosses at the word level, etc. The project makes maximum use of standard, generic software tools. Over 100 texts in 20 languages have been processed at the time of writing; some of these are available on the Internet for browsing and simple querying.
We present an environment and methodology for the representation and processing of acoustic, phonetic and lexical knowledge for speech recognition. The tools suggested enable the encoding and processing of numerical data (signals, parameters, shapes, etc.) and symbolic informations (words, phonemes, syllables, features, cues, etc.) to be carried out in a uniform, uninterrupted and dynamic manner. The application of this methodology is described with reference to a task involving the multi-speaker recognition of the names of the 26 letters of the alphabet given in French. Despite the widely acknowledged difficulty of this vocabulary, the results attained provide a clear validation of the approach, particularly in the case of acoustically very similar words.
This paper first gives an overview of Simon's research in psychology, and then focuses on his work, on the psychology of expertise.
In this paper, a new microphone array processing technique is proposed for blind dereverberation of speech signals affected by room acoustics. It is based on the separate processing of the minimum-phase and all-pass components of delay-steered multi-microphone signals. The minimum-phase components are processed in the cepstrum-domain, where spatial averaging followed by low-time filtering is applied. The all-pass components, which contain the source location information, are processed in the frequency-domain by performing spatial averaging and by retaining only the all-pass component of the resulting output. The underlying motivation for the new processor is to use spatio-temporal processing over a single set of synchronous speech segments from several microphones to reconstruct the source speech, such that it is applicable to practical time-variant acoustic environments. Simulated room impulse responses are used to evaluate the new processor and to compare it to a conventional beamformer. Significant improvements in array gain and important reductions of reverberation in listening tests are observed.
We report the results of a cognitive investigation of the language deficits of a single Wernicke's aphasic patient. The patient, R.D., showed poor speech comprehension but good reading comprehension. His spontaneous speech and his attempts at reading aloud contained many neologisms and some verbal paraphasias. Following Butterworth (1979) we interpret the neologisms as due to problems with retrieval of the phonological specifications of words from a speech output lexicon, and we present evidence showing that success in lexical retrieval was affected by word frequency and not by any syntactic distinction between content (open-class) and function (closed-class) words. R.D.'s spelling was better preserved than his spoken naming and he could spell many words he was unable to say correctly. His spelling errors appeared to be attempts at a target word based on retrieval of partial orthographic information (just as his neologisms seem to be based on partial retrieval of phonological information). We argue that normal subjects may make similar speech and writing errors under certain circumstances. Garrett's (1982) model of speech production is presented and we discuss how neologistic jargonaphasia and other forms of Wernicke's aphasia may be explained in terms of it. We also argue that when R.D.'s deficits are analyzed and compared with those of other patients in the literature a number of implications emerge for theories of normal language processing. These include 1) that the comprehension and production of familiar written words do not involve obligatoruy phonological mediation, 2) that there are distinct phonological and orthographic lexicons, 3) that morphemes are seperately represented in the phonological lexicon, 4) that ease and speed of retrieval of items from the phonological lexicon is affected by frequency of usage, and 5) that retrieval from both lexicons is not all-or-nothing.
Vector quantizers have traditionally been used in speech recognition systems as a pre-processor for sophisticated algorithmes such as Hidden Markov Modelling (HMM) or Dynamic Time Warping (DTW). Recently simpler systems based more directly on Vector Quantization (VQ) have been proposed for recognizing isolated words with small vocabularies. The major problem with such VQ-based systems is the lack of temporal information in the recognition algorithm. Recent and new variations of the VQ-based systems incorporating temporal information are described. The principal new variation introduced here is a conditional histogram technique which incorporates relative likelihoods of successive codewords into the distortion measure used in the VQ recognition algorithm. Several VQ-based recognition algorithms are applied to the recognition of spoken letters of the English alphabet, a subset of the IBM Spellmode vocabulary. Simulation results highlight the relative merits of the algorithms.
The understanding module of a spoken dialogue system must extract, from the speech recognizer output, the kind of request expressed by the caller (the call type) and its parameters (numerical expressions, time expressions or proper-names). Such expressions are called Named Entities and their definitions can be either generic or linked to the dialogue application domain. Detecting and extracting such Named Entities within a mixed-initiative dialogue context like How May I Help You? sm,tm (HMIHY) is the subject of this study. After reviewing standard methods based on hand-written grammars and statistical tagging, we propose a new approach, combining the advantages of both in a 2-step process. We also propose a novel architecture which exploits understanding to improve recognition accuracy: the output of the Automatic Speech Recognition module is now a word lattice and the understanding module is responsible for transcribing the word strings which are useful to the Dialogue Manager. All the methods proposed are trained and evaluated on a corpus comprising utterances from live customer traffic.
This paper describes a bi-directional letter/sound generation system based on a strategy combining data-driven techniques with a rule-based formalism. Our approach provides a hierarchical analysis of a word, including stress pattern, morphology and syllabification. Generation is achieved by a probabilistic parsing technique, where probabilities are trained from a parsed lexicon. Our training and testing corpora consisted of spellings and pronunciations for the high frequency portion of the Brown Corpus (10,000 words). The phonetic labels are augmented with markers indicating morphology and stress. We will report on two distinct grammars representing a historical perspective. Our early work with the first grammar inspired us to modify the grammar formalism, leading to greater constraint with fewer rules. We evaluated our performance on letter-to-sound generation in terms of whole word accuracy as well as phoneme accuracy. For the unseen test set, we achieved a word accuracy of 69.3% and a phoneme accuracy of 91.7% using a set of 52 distinct phonemes. While this paper focuses on letter-to-sound generation, our system is also capable of generation in the reverse direction, as reported elsewhere (Meng et al., 1994a). We believe that our formalism will be especially applicable for entering unknown words orally into a recognition system.
The proposed method is based on the cooperative use of a conventional n-gram constraint and additional grammatical constraints which take deviations from the grammar into account with a multi-pass search strategy. The partial utterance segments are obtained with high confidence as the segments that satisfy both n-gram and grammatical constraints. For improved efficiency, the context-free grammar expressing the grammatical constraints is approximated by a finite-state automaton. We consider all kinds of deviations from the grammar such as insertions, deletions and substitutions when applying the grammatical constraints. As a result, we can achieve a more robust application of grammatical constraints compared to a conventional word-skipping robust parser that can only handle one type of deviation, that is, insertions. Our experiments confirm that the proposed method can recognize partial segments of utterances more reliably than conventional continuous speech recognition methods using only n-grams. In addition, our results indicate that allowing more deviations from the grammatical constraints leads to better performance than the conventional word-skipping robust parser approach.
The conic fitting from image points is a very old topic in estimation and pattern recognition. Systematically, these works have been based on the algebraic representation of the conic to establish the optimization criteria. Less studied, the polar representation of the ellipse is costlier because it needs the optimization of the parametrization. Yet, we propose in this paper some new ideas about this question. First, we show that the estimation of the parameters and the parametrization separated permit to make the problem easier leading to a direct inversion and the search of the roots of a four degree polynomial respectively. We also show that the parametrization carries the dimensional characteristics of the ellipse and when it is correctly disrupted in the minimization process, we constraint the ellipse search space. This new result gives an estimate without dimensional bias in a noised and incomplete context. A confidence envelope is then estimated to direct the search for continuations of the ellipse. At last, we propose a hierarchical grouping and fitting stage following with a fuzzy decision step to detect automatically the elliptic shapes in the images.
Autism is characterized by profound difficulties in emotional interactions and should therefore be an appropriate field of research for emotion based human-computer interfaces. This article focuses on emotionally charged facial expressions when they occur during a dialogue. We designed a human-computer interface that matches an emotional facial expression to each reply in the dialogue. Facial expressions are displayed using two different graphical designs, one being more cartoon-like than the other. We describe an experimental protocol used to evaluate the impact of this interface on 10 teenagers with autism and 10 children without autism. Results show that subjects without autism are more successful than subjects with autism in using facial expressions jointly with speech to disambiguate a dialogue.
The present study investigates on the basis of which units and strategies children initially organize their speech. Transcribed longitudinal data from children acquiring German as their L1 are presented. The data were obtained in weekly recording sessions, which began when the subjects were between seven and thirteen months old. All the material was collected within the framework of the Kiel Project on Early Phonological Development. The evidence presented in this paper suggests that a limited inventory of articulatory patterns functioning as underlying organizational units may determine the phonetic structure of the large majority of a child's first words. The patterns are most probably constructed on the basis of a child's preferred articulations as well as on the basis of the acoustically and auditorily most salient features of adult model words. There are basically five ways in which the early or original patterns change over time.
The linguistics of enunciation formulated by Antoine Culioli is a theory of predicative and enunciative operations. Thus, the concepts of operation and representation are central to the epistemological model and the analysis method which aims to disentangle the links in play in the construction of utterances to relate them to the symbolic activity of representation and to the mental process that it presupposes. A critical examination of the model of the three levels of representation leads us to distinguish the function of representing from the mode of representing. The return to Saussure enables us to re-examine the model by integrating, at the level of notional representations, what Saussure names figures. In conclusion, a brief case study exemplifies the theoretical development.
This paper presents an experimental study of “stress shift” in category-ambiguous and non-ambiguous material. Ambiguous sequences such as Chinese fan exhibit phonological evidence for two structural analyses. If the sequences is a syntactic phrase, with Chinese an adjective modifying the noun fan, then fan has greater relative prominence. If Chinese is a noun and the sequence is a compound, then fan is deaccented and Chinese has greater relative prominence. Additionally, since Chinese is a “stress shift” item, stress shift may apply in the phrasal interpretation. Thus, category-ambiguous words with a potential for stress shift might contain earlier cues to syntactic category, in the form of a modified stress pattern, than non-stress shift items. Production data show that stress shift patterns do indeed map onto syntactic categories, but only if the second element in the sequence is not right-branching. A comprehension experiment with category-ambiguous material suggests that compound or phrasal prominence patterns and stress shift facilitate syntactic processing. A second comprehension experiment replicates this effect and extends the investigation to non-ambiguous material such as Torquay College. In non-ambiguous material, again, phrasal and compound stress appear to affect processing, but stress shift does not.
We address the problem of detecting slow moving target within ground clutter with a non side looking monostatic airborne radar using either a uniformly spaced and linear antenna (ULA) or a uniformly curved or circular antenna (UCuA and UCA respectively). With a monostatic radar using an ULA in side looking configuration, the clutter has specific properties: the space-time repartition of the clutter spectral power which is called “Clutter Ridge'' is range independent. But in case of a monostatic radar using an ULA in a non side looking configuration or using a UcuA or a UCA, the clutter properties are changed. The clutter ridges are range dependent and specific methods must be applied to compensate the range dependency [4], [5], [6].
A novel speech coding algorithm, named pitch synchronous multi-band (PSMB), is proposed. The new coding algorithm uses the multi-band excitation (MBE) model to generate a representative pitch-cycle waveform (PCW) for each frame. The representative PCW of a frame is encoded by two out of three codebooks depending upon whether the frame is related or unrelated to the previous frame. When a frame is unrelated to its previous frame, it is encoded by a bandlimited single pulse excitation (BSPE) codebook and the stochastic codebook. The new speech coder introduces a pitch-period-based coding feature. It overcomes some weaknesses existing in the improved MBE (IMBE) speech coder. The PSMB coder operating at 4 kbps outperforms the Inmarsat 4.15 kbps IMBE coder by a clear margin. Our listening tests also indicate that it is slightly better than the FS1016 4.8 kbps code excited linear predictive (CELP) coder in terms of perceptual quality. Fast search algorithms for the three codebooks used in PSMB are also developed. The fast algorithms render the new speech coder comparable to the FS1016 CELP coder, in terms of computational complexity.
This paper describes our recent work in developing multilingual spoken language systems that support human-computer interactions. Our approach is based on the premise that a common semantic representation can be extracted from the input for all languages, at least within the context of restricted domains. In our design of such systems, language dependent information is separated from the system kernel as much as possible, and encoded in external data structures. The internal system manager, discourse and dialogue component, and database are all maintained in a language transparent form. Our description will focus on the development of the multilingual MIT Voyager spoken language system, which can engage in verbal dialogues with users about a geographical region within Cambridge, MA in the USA. The system can provide information about distances, travel times or directions between objects located within this area (e.g., restaurants, hotels, banks, libraries), as well as information such as the addresses, telephone numbers or location of the objects themselves. Evaluations for the English, Japanese and Italian systems are reported. Other related multilingual research activities are also briefly mentioned.