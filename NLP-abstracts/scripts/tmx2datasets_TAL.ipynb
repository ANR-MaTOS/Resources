{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcad2174-ec02-44d9-b628-57b8de6c365c",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "\n",
    "\n",
    "The data in [NLP-abstracts] correspond to approximately 2,000 abstracts of PhD Theses and journal abstracts in the NLP domain in French associated with their English translation (or vice-versa). These texts have been downloaded from public sources, manually curated, and aligned at the sentence level. It is redistributed under the terms of the [CC-BY Licence](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "This data has been used in the following paper, published at the 2024 edition of the TALN conference: \n",
    "\n",
    ">Ziqian Peng, Rachel Bawden, and François Yvon. 2024. À propos des difficultés de traduire automatiquement de longs documents. In Actes de la 31ème Conférence sur le Traitement Automatique des Langues Naturelles, volume 1 : articles longs et prises de position, pages 2–21, Toulouse, France. ATALA and AFPC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef08437-1ff5-42b7-bb09-13822abce879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# from xml.etree import ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "print(ET.VERSION)\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970e135a-b3dc-4e97-ba23-115d3ea74645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to the corresponding folder in your local\n",
    "tmx_path_dict= {\n",
    "    'THE': 'TAL/corpora/tmx/theses.fr',\n",
    "    'rTAL': 'TAL/corpora/tmx/rTAL_abstract',\n",
    "    'ISTEX': 'TAL/corpora/tmx/abstract_m_trankit',\n",
    "}\n",
    "\n",
    "store_path_dict = {\n",
    "    'THE': 'TAL/corpora/NLP_abstracts/THE_abstracts',\n",
    "    'rTAL': 'TAL/corpora/NLP_abstracts/rTAL_abstracts',\n",
    "    'ISTEX': 'TAL/corpora/NLP_abstracts/ISTEX_abstracts',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937def6-48a3-4d59-b9b2-1e8e29fc9f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a4e11b-329e-45ec-a4a4-3b42a25aabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmx2txt_file(fpath):\n",
    "    try:\n",
    "        tree = ET.parse(fpath, ET.XMLParser(encoding='utf-8'))\n",
    "    except:\n",
    "        print(f\"Cannot parse, potentially no text in {fpath}\")\n",
    "        warnings.warn(f\"Cannot parse file {fpath} as tree\")\n",
    "        \n",
    "    root = tree.getroot() \n",
    "\n",
    "    sents_dict = {  'EN': [], 'FR': []} \n",
    "    \n",
    "    doc_id = 'docid'\n",
    "    for d in root.iter('header'):\n",
    "        for el in d.iter():\n",
    "            if el.tag == 'docid':\n",
    "                doc_id = el.text.strip()\n",
    "    # todo check empty alignment\n",
    "    for d in root.iter('body'):\n",
    "        doc_info = []\n",
    "        for el in d.iter():\n",
    "            if el.tag == 'tu':\n",
    "                # print(el.attrib) # segId\n",
    "                # reset lang for each pair of sentences\n",
    "                lang = ''\n",
    "            if el.tag == 'tuv':\n",
    "                lang = el.attrib['{http://www.w3.org/XML/1998/namespace}lang']\n",
    "\n",
    "            if el.tag == 'seg':\n",
    "                # !!! unicode normalization\n",
    "                txt = el.text if el.text is not None else '' \n",
    "                    \n",
    "                sents_dict[lang] = sents_dict[lang] + [ txt ]\n",
    "    assert(len(sents_dict['EN']) ==len(sents_dict['FR']))\n",
    "    return doc_id, sents_dict\n",
    "\n",
    "\n",
    "\n",
    "# process functions  adapted from the initial version made by Maxime Bouthor\n",
    "# to process the documents in THE and rTAL\n",
    "def process(sentence):\n",
    "    if isinstance(sentence, list):\n",
    "        # if sentence is a list of token form\n",
    "        sentence = ' '.join(sentence)\n",
    "    sentence = re.sub(\"’\", \"'\", sentence)\n",
    "    sentence = re.sub(\"‘\", \"'\", sentence)\n",
    "    sentence = re.sub('', '*', sentence)\n",
    "    sentence = re.sub('', '*', sentence)\n",
    "    sentence = re.sub('', ',', sentence)\n",
    "    \n",
    "    sentence = re.sub(' +\\.', '.', sentence)\n",
    "    # sentence = re.sub(' *-', '-', sentence)\n",
    "    sentence = re.sub(' *,', ',', sentence)\n",
    "    sentence = re.sub(' *\\?', '?', sentence)\n",
    "    sentence = re.sub(' *!', '!', sentence)\n",
    "    sentence = re.sub(' *:', ':', sentence)\n",
    "    sentence = re.sub(' *\\)', ')', sentence)\n",
    "    sentence = re.sub('\\( *', '(', sentence)\n",
    "    sentence = re.sub(' *\\]', ']', sentence)\n",
    "    sentence = re.sub('\\[ *', '[', sentence)\n",
    "    sentence = re.sub(' @@', '', sentence)\n",
    "    sentence = re.sub(' *»', '»', sentence)\n",
    "    sentence = re.sub('« *', '«', sentence)\n",
    "    sentence = re.sub(' +', ' ', sentence)\n",
    "\n",
    "    return sentence.strip()\n",
    "\n",
    "def _process_to_french(sentence):\n",
    "    sentence = re.sub('\\?', ' ?', sentence)\n",
    "    sentence = re.sub('!', ' !', sentence)\n",
    "    sentence = re.sub(' *\\? +!', '?!', sentence)\n",
    "    sentence = re.sub(' *: *', ' : ', sentence)\n",
    "    sentence = re.sub(' *; +', ' ; ', sentence)\n",
    "    sentence = re.sub(' *» *', ' » ', sentence)\n",
    "    sentence = re.sub(' *« *', ' « ', sentence)\n",
    "    sentence = re.sub('&lt ;', '&lt;', sentence)\n",
    "    sentence = re.sub('&gt ;', '&gt;', sentence)\n",
    "    sentence = re.sub('&amp ;', '&amp;', sentence)\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "\n",
    "def process_to_french(sentences):\n",
    "    return list(map(_process_to_french, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354f928d-132b-4c3c-85b5-eeb6e391115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def docid2dataset_sent(data_path, lst_path, store_path_prefix, task = 'train', tmx_fname = 'MaTOS_THE_aligned.tmx'):\n",
    "    # store_path_prefix, the store path without extension, such as .../.../train-these_sent\n",
    "    id_list = [ l.strip() for l in open(lst_path, 'r').read().strip().split('\\n') ]\n",
    "       \n",
    "    data_en = []\n",
    "    data_fr = []\n",
    "    data_idx = []\n",
    "    for order, docid in enumerate( id_list):\n",
    "        # read parallel sentences\n",
    "        doc_id, sents_dict = tmx2txt_file(os.path.join(data_path, docid, tmx_fname ))\n",
    "        \n",
    "        for i in range(len(sents_dict['EN'])):\n",
    "            data_en.append(sents_dict['EN'][i] )\n",
    "            data_fr.append(sents_dict['FR'][i] )\n",
    "            data_idx.append(f'{order}.{i+1}')\n",
    "    \n",
    "    assert(len(data_fr) == len(data_en))\n",
    "    \n",
    "    data_en = list(map(process, data_en))   \n",
    "    data_fr = list(map(process, data_fr))   \n",
    "    data_fr = process_to_french(data_fr)\n",
    "\n",
    "    with open(f'{store_path_prefix}.en', 'w') as f:\n",
    "        f.write('\\n'.join( data_en).strip() )\n",
    "    \n",
    "    with open(f'{store_path_prefix}.fr', 'w') as f:\n",
    "        f.write('\\n'.join(data_fr).strip() )\n",
    "    \n",
    "    with open(f'{store_path_prefix}.idx', 'w') as f:\n",
    "        f.write('\\n'.join(data_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a91f61-9d6b-46b2-930c-1294a9f5bcb7",
   "metadata": {},
   "source": [
    "## sent-level corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2c3270-f905-472e-a974-f39aaa5f0fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_train/sents\n",
      "dev\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_dev/sents\n",
      "test\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_test/sents\n"
     ]
    }
   ],
   "source": [
    "# nb_dev = 101\n",
    "# nb_test=100\n",
    "# store_path = f'TAL/corpora/NLP_abstracts/these_data-1500-{nb_dev}-{nb_test}'\n",
    "\n",
    "key = 'THE'\n",
    "data_path = tmx_path_dict[key]\n",
    "\n",
    "store_path = store_path_dict[key]\n",
    "\n",
    "\n",
    "fpath_dict = {\n",
    "    'train' : 'txt_train/sents/THE_sent_train',\n",
    "    'dev' : 'txt_dev/sents/dev',\n",
    "    'test': 'txt_test/sents/THE_sent'\n",
    "}\n",
    "\n",
    "\n",
    "for task in ['train', 'dev', 'test']:\n",
    "    print(task)\n",
    "    store_dir = os.path.join(store_path, os.path.dirname(fpath_dict[task]))\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    store_path_prefix =  os.path.join(store_path, fpath_dict[task])\n",
    "    docid2dataset_sent(data_path, os.path.join(data_path,  f'train.lst'), store_path_prefix, task = task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20d2110-b6ee-4904-adf7-569ae7c812c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "TAL/corpora/NLP_abstracts/rTAL_abstracts/txt_test/sents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "key = 'rTAL'\n",
    "\n",
    "data_path = tmx_path_dict[key]\n",
    "store_path = f\"{store_path_dict[key]}/txt_test\"\n",
    "\n",
    "for task in ['test']:\n",
    "    print(task)\n",
    "    store_dir = os.path.join(store_path, 'sents')\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    store_path_prefix =  os.path.join(store_dir, f'{key}_sent')\n",
    "    docid2dataset_sent(data_path, os.path.join(data_path,  f'rTAL_doc.lst'), store_path_prefix, task = task, tmx_fname = f'MaTOS_{key}_aligned.tmx')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "813aead6-b486-45d3-80fe-36a299169b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAL/corpora/NLP_abstracts/ISTEX_abstracts/txt_train/sents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "key = 'ISTEX'\n",
    "data_path = tmx_path_dict[key]\n",
    "store_path = f\"{store_path_dict[key]}/txt_train\"\n",
    "\n",
    "\n",
    "Path( store_path ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for task in ['train']:\n",
    "    store_dir = os.path.join(store_path, 'sents')\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    store_path_prefix =  os.path.join(store_dir, f'{key}_sent_train')\n",
    "    docid2dataset_sent(data_path, os.path.join(data_path,  f'train.lst'), store_path_prefix, task = task, tmx_fname = f'MaTOS_{key}_aligned.tmx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8619a5-be97-4ee4-b1c1-39ce38b66c4a",
   "metadata": {},
   "source": [
    "## doc-level corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87853f57-0bf7-4c61-8ba2-dacaa1d5603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docid2dataset_doc(data_path, lst_path, src_store_path, tgt_store_path, tmx_fname = 'MaTOS_THE_aligned.tmx', sep_tag = '</eos>'):\n",
    "    id_list = open(lst_path, 'r').read().strip().split('\\n')\n",
    "       \n",
    "    data_en = []\n",
    "    data_fr = []\n",
    "    for docid in id_list:\n",
    "        # read parallel sentences for each document pair\n",
    "        _, sents_dict = tmx2txt_file(os.path.join(data_path, docid, tmx_fname ))\n",
    "\n",
    "        data_en.append(f'{sep_tag} '.join(sents_dict['EN']))\n",
    "        data_fr.append(f'{sep_tag} '.join(sents_dict['FR'])) \n",
    "    assert(len(data_fr) == len(data_en))\n",
    "    \n",
    "    data_en = list(map(process, data_en))   \n",
    "    data_fr = list(map(process, data_fr))   \n",
    "    data_fr = process_to_french(data_fr)    \n",
    "    \n",
    "    with open(os.path.join(src_store_path), 'w') as f:\n",
    "        f.write('\\n'.join(data_en))\n",
    "    \n",
    "    with open(os.path.join(tgt_store_path), 'w') as f:\n",
    "        f.write('\\n'.join(data_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6fbf8f-f4bf-4d24-b619-b560e2ee6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_train/doc_with_sep\n",
      "dev\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_dev/doc_with_sep\n",
      "test\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_test/doc_with_sep\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "key = 'THE'\n",
    "data_path = tmx_path_dict[key]\n",
    "store_path = store_path_dict[key]\n",
    "\n",
    "fpath_dict = {\n",
    "    'train' : 'txt_train/doc_with_sep/THE_doc_sep_train',\n",
    "    'dev' : 'txt_dev/doc_with_sep/dev',\n",
    "    'test': 'txt_test/doc_with_sep/THE_doc_sep'\n",
    "}\n",
    "\n",
    "\n",
    "for task in ['train', 'dev', 'test']:\n",
    "    print(task)\n",
    "    store_dir = os.path.join(store_path, os.path.dirname(fpath_dict[task]))\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    src_store_path = f'{store_path}/{fpath_dict[task]}.en'\n",
    "    tgt_store_path = f'{store_path}/{fpath_dict[task]}.fr'\n",
    "    docid2dataset_doc(\n",
    "        data_path, \n",
    "        os.path.join(data_path,  f'{task}.lst'), \n",
    "        src_store_path,\n",
    "        tgt_store_path, \n",
    "        tmx_fname = f'MaTOS_{key}_aligned.tmx', \n",
    "        sep_tag = '<sep>'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a6608a3-d478-4290-8a14-eb785a12ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_train\n",
      "dev\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_dev\n",
      "test\n",
      "TAL/corpora/NLP_abstracts/THE_abstracts/txt_test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "key = 'THE'\n",
    "data_path = tmx_path_dict[key]\n",
    "store_path = store_path_dict[key]\n",
    "\n",
    "\n",
    "fpath_dict = {\n",
    "    'train' : 'txt_train/THE_doc_train',\n",
    "    'dev' : 'txt_dev/dev',\n",
    "    'test': 'txt_test/THE_doc'\n",
    "}\n",
    "\n",
    "\n",
    "for task in ['train', 'dev', 'test']:\n",
    "    print(task)\n",
    "    store_dir = os.path.join(store_path, os.path.dirname(fpath_dict[task]))\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    src_store_path = f'{store_path}/{fpath_dict[task]}.en'\n",
    "    tgt_store_path = f'{store_path}/{fpath_dict[task]}.fr'\n",
    "    \n",
    "    docid2dataset_doc(\n",
    "        data_path, \n",
    "        os.path.join(data_path,  f'{task}.lst'), \n",
    "        src_store_path,\n",
    "        tgt_store_path, \n",
    "        tmx_fname = f'MaTOS_{key}_aligned.tmx', \n",
    "        sep_tag = ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921858d-0fac-49cc-bc21-a50bd6da3c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9bc32c5-24e2-43cf-b3ce-6ea7a117ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAL/corpora/NLP_abstracts/rTAL_abstracts/txt_test/doc_with_sep\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "key = 'rTAL'\n",
    "\n",
    "data_path = tmx_path_dict[key]\n",
    "store_path = f\"{store_path_dict[key]}/txt_test\"\n",
    "\n",
    "\n",
    "for task in [ 'test']:\n",
    "    store_dir = os.path.join(store_path, 'doc_with_sep' )\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    src_store_path = f'{store_dir}/{key}_doc_sep.en'\n",
    "    tgt_store_path = f'{store_dir}/{key}_doc_sep.fr'\n",
    "    docid2dataset_doc(data_path, os.path.join(data_path,  f'rTAL_doc.lst'), src_store_path, tgt_store_path, tmx_fname = f'MaTOS_{key}_aligned.tmx', sep_tag = '<sep>')\n",
    "\n",
    "    \n",
    "    src_store_path = f'{store_path}/{key}_doc.en'\n",
    "    tgt_store_path = f'{store_path}/{key}_doc.fr'\n",
    "    docid2dataset_doc(data_path, os.path.join(data_path,  f'rTAL_doc.lst'), src_store_path, tgt_store_path, tmx_fname = f'MaTOS_{key}_aligned.tmx', sep_tag = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672fd6e-53ff-4d99-a639-67862129b4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a5d0e4-f087-46f4-8415-9744f36065e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAL/corpora/NLP_abstracts/ISTEX_abstracts/txt_train/doc_with_sep\n"
     ]
    }
   ],
   "source": [
    "# ISTEX collected by Mathilde\n",
    "\n",
    "\n",
    "key = 'ISTEX'\n",
    "\n",
    "data_path = tmx_path_dict[key]\n",
    "store_path = f\"{store_path_dict[key]}/txt_train\"\n",
    "\n",
    "for task in [ 'train']:\n",
    "    store_dir = os.path.join(store_path, 'doc_with_sep' )\n",
    "    print(store_dir)\n",
    "    Path(store_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    src_store_path = f'{store_dir}/{key}_doc_sep_{task}.en'\n",
    "    tgt_store_path = f'{store_dir}/{key}_doc_sep_{task}.fr'\n",
    "    docid2dataset_doc(data_path, os.path.join(data_path,  'train.lst'), src_store_path, tgt_store_path, tmx_fname = f'MaTOS_{key}_aligned.tmx', sep_tag = '<sep>')\n",
    "\n",
    "    \n",
    "    src_store_path = f'{store_path}/{key}_doc_{task}.en'\n",
    "    tgt_store_path = f'{store_path}/{key}_doc_{task}.fr'\n",
    "    docid2dataset_doc(data_path, os.path.join(data_path,  'train.lst'), src_store_path, tgt_store_path, tmx_fname = f'MaTOS_{key}_aligned.tmx', sep_tag = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cce6f-1e68-4ba4-bcb5-3035d13851c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4db8225-95bc-418c-a787-f487b6af7fe7",
   "metadata": {},
   "source": [
    "## TAL-D and TAL-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeeff7c4-007e-4f03-aa18-bf3ea7085512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "\n",
    "DATA_DIR = 'TAL/corpora/NLP_abstracts'\n",
    "THE_path = f'{DATA_DIR}/THE_abstracts/txt_train'\n",
    "ISTEX_path = f'{DATA_DIR}/ISTEX_abstracts/txt_train'\n",
    "\n",
    "train_paths_doc = [\n",
    "    f'{THE_path}/THE_doc_train',\n",
    "    f'{ISTEX_path}/ISTEX_doc_train',\n",
    "]\n",
    "\n",
    "train_paths_sent = [\n",
    "    f'{THE_path}/sents/THE_sent_train',\n",
    "    f'{ISTEX_path}/sents/ISTEX_sent_train',\n",
    "]\n",
    "\n",
    "FTdata_path = f'{DATA_DIR}/NLP_abstracts_txt_all/txt_train'\n",
    "Path( f\"{FTdata_path}/TAL-D\" ).mkdir(parents=True, exist_ok=True)\n",
    "Path( f\"{FTdata_path}/TAL-S\" ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "for lang in ['en', 'fr']:\n",
    "    to_write = [open(f\"{fpath}.{lang}\").read().strip() for fpath in train_paths_doc]\n",
    "    \n",
    "    with open(os.path.join(FTdata_path, 'TAL-D', f'train.{lang}'), 'w') as f:\n",
    "        f.write(re.sub( '<sep>', '', '\\n'.join(to_write)).strip() )\n",
    "        \n",
    "    to_write = [open(f\"{fpath}.{lang}\").read().strip() for fpath in train_paths_sent]\n",
    "    with open(os.path.join(FTdata_path , 'TAL-S' ,f'train.{lang}'), 'w') as f:\n",
    "        f.write('\\n'.join(to_write).strip() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12a705-a29a-4b97-a1d3-c1c4c307a10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf010b2-d01c-4035-a76d-ca95f28bd825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
