Numerous fields of applied sciences and industries have been recently witnessing a process of digitisation. This trend has come with an increase in the amount digital data whose processing becomes a challenging task. In this context, parsimony, also known as sparsity, has emerged as a key concept in machine learning and signal processing. It is indeed appealing to exploit data only via a reduced number of parameters. This thesis focuses on a particular and more recent form of sparsity, referred to as structured sparsity. As its name indicates, we shall consider situations where we are not only interested in sparsity, but where some structural prior knowledge is also available. The goal of this thesis is to analyze the concept of structured sparsity, based on statistical, algorithmic and applied considerations. To begin with, we introduce a family of structured sparsity-inducing norms whose statistical aspects are closely studied. We then turn to sparse structured dictionary learning, where we use the previous norms within the framework of matrix factorization. Different efficient algorithmic tools, such as proximal methods, will then be proposed. With these methods in place, we illustrate on numerous real-world applications from various fields, when and why structured sparsity is useful. This includes, for instance, restoration tasks in image processing, the modelling of text documents as hierarchy of topics, the inter-subject prediction of sizes of objects from fMRI signals.
In the agri-food sector, the characterization of culinary techniques - through their observation and analysis - is increasingly seen as a lever for innovation. By analyzing how their products are used in the kitchen by the users (consumers or professionals), the manufacturers can detect improvement tracks or ideas for new products. In this research, we propose new methods dedicated to the characterization of culinary techniques. Their innovative nature is based on the fact that they are based on a quantitative approach, and not on a qualitative approach as is usually the case. They partially draw their inspiration in two methods of characterization used in sensory analysis: the free sorting and the binary Q-methodology. We see how methodological developments brought to the latter make it possible to characterize large sets of stimuli. These developments are both related to the procedure of data collection and to the procedure of statistical analysis of the data. In particular, we propose several statistical procedures to address various issues: the statistical analysis of a set of partitions containing missing data, the unsupervised agreement-based clustering of a set of profiles of binary evaluations, etc. Then, we see that these two 'improved' quantitative methods of characterization can be successfully applicable to culinary techniques.
This thesis suggests the use of tools from the disciplines of Computational Linguistics and Knowledge Representation with the idea that such tools would enable the partial automation of two processes of Conceptual Design: the analysis of Requirements and the synthesis of concepts of solution. The viewpoint on Conceptual Design developed in this research is based on the systematic methodologies developed in the literature. The evolution of these methodologies provided precise description of the tasks to be achieved by the designing team in order to achieve successful design. Therefore, the argument of this thesis is that it is possible to create computer models of some of these tasks in order to partially automate the refinement of the design problem and the exploration of the design space. In Requirements Engineering, the definition of requirements consists in identifying the needs of various stakeholders and formalizing it into design specifications. During this task, designers face the problem of having to deal with individuals from different expertise, expressing their needs with different levels of clarity. This research tackles this issue with requirements expressed in natural language (in this case in English). The analysis of needs is realised from different linguistic levels: lexical, syntactic and semantic. The lexical level deals with the meaning of words of a language. The semantic level aims at finding about the specific meaning of words in the context of a sentence. This research makes extensive use of a semantic atlas based on the concept of clique from graph theory. Such concept enables the computation of distances between a word and its synonyms. Additionally, a methodology and a metric of similarity was defined for clarifying requirements at syntactic, lexical and semantic levels. This methodology integrates tools from research collaborators. In the synthesis process, a Knowledge Representation of the necessary concepts for enabling computers to create concepts of solution was developed.
This PhD proposes to investigate some of the aspects of self-supervised learning, a form of unsupervised learning that allows models and agents to learn without explicit supervision. One of its major benefits is to increase data efficiency: achieving comparable or better performance with less labeled data or fewer environment steps. Many works involving self-supervised learning have been developed in the last few years, mostly in Computer Vision and Natural Language Processing. However, the current methods are often limited to simple datasets and static environments. The project will focus on self-supervised learning methods in dynamic environments (in which features evolve over time) such as video games. The first research focus of the PhD will be to study curiosity-driven methods in dynamic environments. In recent works, researchers have developed agents that do not use any external reward to improve, but instead, use an intrinsic motivation that drives them to explore, and thus, to advance. This curiosity-based strategy has allowed solving many tasks with sparse reward by training agents in a self-supervised manner. However, such methods still fail to generalize to dynamic environments and are rarely generalizable to realistic setup. In a second step, the PhD will investigate self-supervised learning techniques applied to video datasets. One research direction will be to study the parallel between video datasets and visual 3D environments: exploring a virtual environment can be seen as designing a sampling policy for a set of videos. Therefore, there is a strong correlation between methods developed for both types of data although this connection has not been studied yet. Indeed, problems that appear in video datasets are often seen from the Computer Vision perspective, whereas tasks in virtual environments are tackled from the Reinforcement Learning point of view. Hence, one of the goals for the PhD will be to examine and transpose methods from one field to the other.
Multi-label learning is a specific supervised learning problem where each instance can be associated with multiple target labels simultaneously. Multi-label learning is ubiquitous in machine learning and arises naturally in many real-world applications such as document classification, automatic music tagging and image annotation. We first discuss why the state-of-the art single multi-label algorithms using an effective committee of multi-label models suffer from certain practical drawbacks. We then propose a novel strategy to build and aggregate k-labelsets based committee in the context of ensemble multi-label classification. We then analyze the effect of the aggregation step within ensemble multi-label approaches in depth and investigate how this aggregation impacts the prediction performances with respect to the objective multi-label loss metric. We then address the specific problem of identifying relevant subsets of features-among potentially irrelevant and redundant features-in the multi-label context based on the ensemble paradigm. Three wrapper multi-label feature selection methods based on the Random Forest paradigm are proposed. These methods differ in the way they consider label dependence within the feature selection process. Finally, we extend the multi-label classification and feature selection problems to the semi-supervised setting and consider the situation where only few labelled instances are available. We propose a new semi-supervised multi-label feature selection approach based on the ensemble paradigm. The proposed model combines ideas from co-training and multi-label k-labelsets committee construction in tandem with an inner out-of-bag label feature importance evaluation. Satisfactorily tested on several benchmark data, the approaches developed in this thesis show promise for a variety of applications in supervised and semi-supervised multi-label learning
In this thesis, we propose a method to build a decision support tool for the diagnosis of rare diseases. We aim to minimize the number of medical tests necessary to achieve a state where the uncertainty regarding the patient's disease is less than a predetermined threshold. In doing so, we take into account the need in many medical applications, to avoid as much as possible, any misdiagnosis. To solve this optimization task, we investigate several reinforcement learning algorithm and make them operable in our high-dimensional. To do this, we break down the initial problem into several sub-problems and show that it is possible to take advantage of the intersections between these sub-tasks to accelerate the learning phase. The strategies learned are much more effective than classic greedy strategies. We also present a way to combine expert knowledge, expressed as conditional probabilities, with clinical data. This is crucial because the scarcity of data in the field of rare diseases prevents any approach based solely on clinical data. We show, both empirically and theoretically, that our proposed estimator is always more efficient than the best of the two models (expert or data) within a constant. Finally, we show that it is possible to effectively integrate reasoning taking into account the level of granularity of the symptoms reported while remaining within the probabilistic framework developed throughout this work.
Adverse drug reaction (ADR) is a serious concern that has important health and economical repercussions. Between 1.9%-2.3% of the hospitalized patients suffer from ADR, and the annual cost of ADR have been estimated to be of 400 million euros in Germany alone. Furthermore, ADRs can cause the withdrawal of a drug from the market, which can cause up to millions of dollars of losses to the pharmaceutical industry. Multiple studies suggest that genetic factors may play a role in the response of the patients to their treatment. This covers not only the response in terms of the intended main effect, but also % according toin terms of potential side effects. We frame them through multitask machine learning frameworks, which combine all data available for related problems in order to solve them at the same time. We propose a novel model for multitask linear prediction that uses task descriptors to select relevant features and make predictions with better performance as state-of-the-art algorithms. Finally, we study strategies for increasing the stability of the selected features, in order to improve interpretability for biological applications.
This thesis in the field of Natural Language Processing aims at optimizing documents classification in search engines. This work focuses on the development of a tool that automatically detects documents topics (ATDS-fr). Using poor knowledge, the hybrid method combines statistical techniques for topic segmentation and linguistic methods that identify cohesive markers.
Text-independent automatic speaker recognition is a recent method in biometric area. Its increasing interest is reflected both in the increasing participation in international competitions and in the performance progresses. Moreover, the accuracy of the methods is still limited by the quantity of speaker discriminant information contained in the representations of speech utterances. This thesis presents a study on speech representation for speaker recognition systems. It shows firstly two main weaknesses. First, it fails to take into account the temporal behavior of the voice, which is known to contain speaker discriminant information. Secondly, speech events rare in a large population of speakers although very present for a given speaker are hardly taken into account by these approaches, which is contradictory when the goal is to discriminate among speakers. In order to overpass these limitations, we propose in this thesis a new speech representation for speaker recognition. This method represents each acoustic vector in a a binary space which is intrinsically speaker discriminant. A similarity measure associated with a global representation (cumulative vectors) is also proposed. This new speech utterance representation is able to represent infrequent but discriminant events and to work on temporal information. It allows also to take advantage of existing « session » variability compensation approaches (« session » variability represents all the negative variability factors). In this area, we proposed also several improvements to the usual session compensation algorithms. An original solution to deal with the temporal information inside the binary speech representation was also proposed. Thanks to a linear fusion approach between the two sources of information, we demonstrated the complementary nature of the temporal information versus the classical time independent representations.
This prospective lexicological investigation belongs to the field of L3 French didactics. The purpose is to elaborate a French-Italian-Portuguese-Spanish interlexicon out of the frequent adjectives, nouns and verbs of the healthcare scientific writings, and their analogue translation equivalents in French, Italian, Portuguese and Spanish. Two words are analogue if they have the same meaning and a similar form. Related concepts of analogy, similarity and identity are discussed, types of intralinguistic and cross-linguistic analogies reviewed, and the main analogies and differences between English, French and Romance languages detailed. Their many analogies are justified by Indo-European origins and mostly by intense language contacts. Once the importance of analogy in learning procedures has been highlighted, we show how this research and two types of didactic approaches connect together: intercomprehension, which develops comprehension skills in neighbor languages, and corpus approaches which enable to get a closer insight into scientific phraseology. The 2000 most frequent English lemmas were extracted from the ScienText English scientific corpus, their 2208 frequent acceptions explored from their combinatory profile and sorted out in two semantic categories: healthcare subject-specific vocabulary and science specific trans-disciplinary vocabulary. The English lemmas were translated into the four Romance languages, and similarity measurements were carried out with the longest common substring method. The interlexicon contains 47% of the frequent acceptions. Analogy is even higher by language pairs: English – French, 66%, English – Italian, 65%, English-Spanish, 63%, English – Portuguese, 58%. Consequently, this analogue vocabulary could form a transfer basis in learning activities of L3 French for health care providers, and L2 English seems to be a possible bridge language toward Romance languages. Plurilingual activities are built on concordances extracted from multilingual aligned corpora (EMEA, Europarl). Metalinguistic questions in English point out (morpho)syntactic features of French; the analogies between both languages are systematically enhanced, and in case of lexical opacity, those between English and the other Romance languages.
Even though modern retrieval systems typically use a multitude of features to rank documents, the backbone for search ranking is usually the standard retrieval models. This thesis addresses a limitation of the standard retrieval models, the term mismatch problem. The term mismatch problem is a long standing problem in information retrieval. However, it was not well understood how often term mismatch happens in retrieval, how important it is for retrieval, or how it affects retrieval performance. This thesis answers the above questions. This research is enabled by the formal definition of term mismatch. In this thesis, term mismatch is defined as the probability that a term does not appear in a document given that this document is relevant. We propose several approaches for reducing term mismatch probability through modifying documents or queries. Our proposals are then followed by a quantitative analysis of term mismatch probability that shows how much the proposed approaches reduce term mismatch probability with maintaining the system performance. First, we propose a document modification approach according to a user query. The main idea of our document modification approach is to deal with mismatched query terms. The modified document is then used in a standard retrieval model in order to obtain a mismatch aware retrieval model. Second, we propose a semantic query expansion approach based on a collaborative knowledge resource. We focus on the collaborative resource structure to obtain interesting expansion terms that contribute to reduce term mismatch probability, and as a result, improve the effectiveness of search. Third, we propose a query expansion approach based on neural language models. Neural language models are proposed to learn term vector representations, called distributed neural embeddings. We propose to use distributed neural embeddings as a knowledge resource in a query expansion scenario. Fourth, we apply the term mismatch probability definition for each contribution of the above contributions. We show how we use standard retrieval corpora with queries and relevance judgments to estimate the term mismatch probability. We estimate the term mismatch probability using original documents and queries, and we figure out how mismatch problem is clearly found in search systems for different types of indexing terms. Then, we point out how much our contributions reduce the estimated mismatch probability, and improve the system recall. Promising research directions are identified where the term mismatch research may make a significance impact on improving the search scenarios.
As an epistemological critique of dominant linguistic theories, this research leans on the fallout of the theory of value (K. Marx), the consequences of modernity and those of contemporary technological advances upon language forms. The first volume is a presentation and illustration of our theoretical model: semio-activity. Distinguishing between domains, enunciative statuses and discursive statuses, the semio-active approach critically integrates G. Guillaume's theory, enunciation theory (Benveniste, Culioli, Adamczewski) and praxematic theory (R. Lafont). The second volume, standing on the rouen school of sociolinguistics theoretical openings (J. B. Marcellesi), is mainly dedicated to Maghrebin glottopolitics, on the one hand, and to the consequences of technology upon language forms, on the other hand. A critical survey of computational linguistics opens on to new formulations of a formalism supporting algorithms for natural language processing.
This research focuses on the linguistic analysis of the French verb and its correspondents in Arabic, using a contrastive and comparative approach, to integrate them into the NooJ computer system for an application of machine translation. In the first part, we have presented the theoretical foundations on which we have based our study of the semantic-syntactic characteristics of the verb classes of motion and of communication and which are essentially the Theory of the Classes of Objects of Gaston Gross and the classification of the French verbs by JD and FD-C. According to these two theories already presented in this section, we will make a comparison between the predicates of the two classes with their correspondents in Arabic to which we have already given their linguistic characteristics in the third chapter of this part. In the second part, we have followed an analytical but at the same time comparative approach between the two verb systems: French-Arabic. In the first chapter, we have compared the modes and tenses in both languages. In the last two chapters, we have studied the semantic-syntactic input / output specificities of the two classes of communication and of motion whose subject is human [N0hum] according to the classification of the French verbs (LVF) of JD and FD­-C. In this semantic-syntactic analysis, we have tried to disassemble the different syntactic patterns of the two classes, by indicating, for each syntactic type, the semantic nature and the type of its necessary arguments without forgetting to give to each predicate its equivalent in Arabic and to make a comparison between the argumentative structure of the French verbs and that of the Arabic verbs, in order finally to corne out with the similarities and the divergences between the two verb systems. And finally, in the third part, we have tried to show all the necessary steps for the development of our application of machine translation using the NooJ platform. In this step, we have created: 1/ Two bilingual dictionaries of communication and motion verbs. 2/ Formal grammars for the realization of the analysis phase and the recognition of the syntactic patterns of the predicates of class C and class E. 3/ Grammars of translation of verbs in the affirmative or negative form at the appropriate tenses, while specifying the rules of inflection and agreement in gender and number according to the standards of Arabic. At the output of our translation tool, we have found translated and inflected Arabic verbs that respect the rules of the target language.
The revolution in new sequencing technologies, by strongly improving the production of omics data, is greatly leading to new understandings of the relations between genotype and phenotype. To interpret and analyze data grouped according to a phenotype of interest, methods based on statistical enrichment became a standard in biology. However, these methods synthesize the biological information by a priori selecting the over-represented terms and focus on the most studied genes that may represent a limited coverage of annotated genes within a gene set. During this thesis, we explored different methods for annotating gene sets. In this frame, we developed three studies allowing the annotation of gene sets and thus improving the understanding of their biological context. First, visualization approaches were applied to represent annotation results provided by enrichment analysis for a gene set or a repertoire of gene sets. In this work, a visualization prototype called MOTVIS (MOdular Term VISualization) has been developed to provide an interactive representation of a repertoire of gene sets combining two visual metaphors: a treemap view that provides an overview and also displays detailed information about gene sets, and an indented tree view that can be used to focus on the annotation terms of interest. This illustrates the interest of using different visual metaphors to facilitate the comprehension of biological results by representing complex data. Secondly, to address the issues of enrichment analysis, a new method for analyzing the impact of using different semantic similarity measures on gene set annotation was proposed. To evaluate the impact of each measure, two relevant criteria were considered for characterizing a "good" synthetic gene set annotation: (i) the number of annotation terms has to be drastically reduced while maintaining a sufficient level of details, and (ii) the number of genes described by the selected terms should be as large as possible. Thus, nine semantic similarity measures were analyzed to identify the best possible compromise between both criteria while maintaining a sufficient level of details. Then, we developed GSAn (Gene Set Annotation), a novel gene set annotation web server that uses semantic similarity measures to synthesize a priori GO annotation terms. GSAn contains the interactive visualization MOTVIS, dedicated to visualize the representative terms of gene set annotations. Compared to enrichment analysis tools, GSAn has shown excellent results in terms of maximizing the gene coverage while minimizing the number of terms. At last, the third work consisted in enriching the annotation results provided by GSAn. Since the knowledge described in GO may not be sufficient for interpreting gene sets, other biological information, such as pathways and diseases, may be useful to provide a wider biological context. Thus, two additional knowledge resources, being Reactome and Disease Ontology (DO), were integrated within GSAn. The integration of these resources improved the results in terms of gene coverage without affecting significantly the number of involved terms. In practice, GO terms were mapped to terms of Reactome and DO, before and after applying the GSAn method. We have shown that a mapping process before computing the GSAn method allowed to obtain a larger number of inter-relations between the two knowledge resources.
This work is conceived in the present panorama of fast development of large databases gathering experimental results about the organization of the human brain at different scales. This abundance of information calls for an intra and inter-disciplinary effort aimed to synthesize this information in a coherent way. The aim of this thesis was to contribute to this effort for knowledge synthesis to better understand the multiscale organization of the cerebral cortex. The work followed two paths: an intra-disciplinary effort to bring together results produced by the brain imaging community with particular focus on Resting State and Task Based MRI experiments; an inter-disciplinary attempt to draw a link between the anatomo-functional organization of the cortex as emerging from brain imaging studies and the cortical patterns of gene expression as revealed by recently published atlases of the adult human brain transcriptome. The thesis is organized into three parts: In Part I studied the anatomo-functional organization of the human cortex starting from brain imaging studies. In Part II we studied the link between cortical gene expression and the anatomo-functional organization of the cortex both in term of their topography and in term of their function, focusing in particular on information processing and memory formation. In Part III we present a platform that we developed to favor knowledge integration between cognitive networks and gene expression databases. In perspective we show how our approach may provide new insights to the debate about neurodegenerative and psychiatric diseases on one hand, modeling of dynamical processes in different areas of the cortex on the other.
Infiltrative lung diseases (ILDs) enclose a large group of irreversible lung disorders which require regular follow-up with computed tomography (CT) imaging. A quantitative assessment is mandatory to establish the (regional) disease progression and/or the therapeutic impact. This implies the development of automated computer-aided diagnosis (CAD) tools for pathological lung tissue segmentation, problem addressed as pixel-based texture classification. Traditionally, such classification relies on a two-dimensional analysis of axial CT images by means of handcrafted features. Recently, the use of deep learning techniques, especially Convolutional Neural Networks (CNNs) for visual tasks, has shown great improvements with respect to handcrafted heuristics-based methods. However, it has been demonstrated the limitations of "classic" CNN architectures when applied to texture-based datasets, due to their inherently higher dimension compared to handwritten digits or other object recognition datasets, implying the need of redesigning the network or enriching the system to learn meaningful textural features from input data. The classification targeting the whole lung parenchyma achieves an average of 84% accuracy (75.8% for normal, 90% for emphysema and fibrosis, 81.5% for ground glass)
Natural Language Generation (NLG) is the natural language processing task of generating natural language from a machine representation system. In this thesis report, we present an architecture of NLG system relying on statistical methods. The originality of our proposition is its ability to use a corpus as a learning resource for sentences production. This method offers several advantages: it simplifies the implementation and design of a multilingual NLG system, capable of sentence production of the same meaning in several languages. Our method also improves the adaptability of a NLG system to a particular semantic field. We suggest in our conclusions that this particular method of generation could offer promising avenues of investigation on the nature of the sentence generation process.
This thesis discusses the automatic construction of semantic representations as simply typed lambda termes. We introduce non-formal and formal semantics and present one attempt to build semantic representations with the help of reasonning tools. We explain why we find this approach limited and propose our own, at the heart of which is a tool called Nessie. This tool builds semantic representations using a lexicon specifying representations for words and a tree to guide semantic consruction. We present the implementation of this tool in detail, and compare its results with an earlier systme. We then apply Nessie to two different tasks. First we show how it can be used (in conjunction with inference tools) to study the semantics of tense and apsect of Polish verbs. Second, we present two different type-theoretic approaches to the compositional computation of discourse semantics. Finally, we prove that our approach to semantic construction is compatible with a wide range of grammatical formalisms, namely those which can be expressed in terms of an abstract categorial grammar of order 2. One consequence of this characterisation is that Nessie can handle a large range of invertible grammars and hence is compatible with generation as well as analysis. We conclude by discussing the adequacy of simply typed lambda calculus for natural language semantics.
This thesis addresses the problem of recognition, modelling and description of human activities. We describe results on three problems: (1) the use of transfer learning for simultaneous visual recognition of objects and object states, (2) the recognition of manipulation actions from state transitions, and (3) the interpretation of a series of actions and states as events in a predefined story to construct a narrative description. These results have been developed using food preparation activities as an experimental domain. We start by recognising food classes such as tomatoes and lettuce and food states, such as sliced and diced, during meal preparation. We adapt the VGG network architecture to jointly learn the representations of food items and food states using transfer learning. We model actions as the transformation of object states. We use recognised object properties (state and type) to detect corresponding manipulation actions by tracking object transformations in the video. Experimental performance evaluation for this approach is provided using the 50 salads and EPIC-Kitchen datasets.
This thesis analyses some preverbs and prepositions in Bulgarian (v/v-, iz/iz-) and in French (dans, hors de, en-/in-, é-/ex-) within the framework of the cognitive and formal semantics, the Cognitive and Applicative Grammar (CAG) in particular. The formalisation using quasi-topological scheme and abstract loci allows the construction of appropriate representations and the formulation of an abstract invariant meaning for each item. The analysis leads to the construction of a semantic map of the interior and exterior meanings of the studied items which underlines the similarities, the differences and the oppositions between the meanings of different preverbs and prepositions within a single language as well as between the two languages, Bulgarian and French. The results of the analysis show that each language builds its own representations using invariant primitives and that, even if some of the studied meanings are very similar, it is impossible to establish absolute equivalencies between them. This study contributes to argue for the anti-anti-relativist hypothesis of the CAG.
Many problems in machine learning pertain to tackling the minimization of a possibly non-convex and non-smooth function defined on a Euclidean space. Optimization methods, used to solve those problems, have been widely studied in the literature for convex objective functions and are extensively used in practice. However, recent breakthroughs in statistical modeling, such as deep learning, coupled with an explosion of data samples, require improvements of non-convex optimization procedure for large datasets. This thesis is an attempt to address those two challenges by developing algorithms with cheaper updates, ideally independent of the number of samples, and improving the theoretical understanding of non-convex optimization that remains rather limited. In this manuscript, we are interested in the minimization of such objective functions for latent data models, ie, when the data is partially observed which includes the conventional sense of missing data but is much broader than that. In the first part, we consider the minimization of a (possibly) non-convex and non-smooth objective function using incremental and online updates. To that end, we propose several algorithms through some applications. In the second part, we focus on the maximization of non-convex likelihood using the EM algorithm and its stochastic variants. We analyze several faster and cheaper algorithms and propose two new variants aiming at speeding the convergence of the estimated parameters.
Extracting public opinion by analyzing Big Social data has grown substantially due to its interactive nature, in real time. In fact, our actions on social media generate digital traces that are closely related to our personal lives and can be used to accompany major events by analysing peoples'behavior. It is in this context that we are particularly interested in Big Data analysis methods. The volume of these daily-generated traces increases exponentially creating massive loads of information, known as big data. The main contribution of this PHD thesis is to propose a generic analysis approach to automatically detect trends on given topics from big social data. Indeed, given a very small set of manually annotated hashtags, the proposed approach transfers information from hashtags known sentiments (positive or negative) to individual words. The resulting lexical resource is a large-scale lexicon of polarity whose efficiency is measured against different tasks of sentiment analysis. The comparison of our method with different paradigms in literature confirms the impact of our method to design accurate sentiment analysis systems. Indeed, our model reaches an overall accuracy of 90.21%, significantly exceeding the current models on social sentiment analysis.
In this thesis, we examine some practical difficulties of deep learning models. Indeed, despite the promising results in computer vision, implementing them in some situations raises some questions. For example, in classification tasks where thousands of categories have to be recognised, it is sometimes difficult to gather enough training data for each category. We propose two new approaches for this learning scenario, called &lt;&lt;zero-shot learning&gt;&gt;. We use semantic information to model classes which allows us to define models by description, as opposed to modelling from a set of examples. In the first chapter we propose to optimize a metric in order to transform the distribution of the original data and to obtain an optimal attribute distribution. In the following chapter, unlike the standard approaches of the literature that rely on the learning of a common integration space, we propose to generate visual features from a conditional generator. The artificial examples can be used in addition to real data for learning a discriminant classifier. In the second part of this thesis, we address the question of computational intelligibility for computer vision tasks. Due to the many and complex transformations of deep learning algorithms, it is difficult for a user to interpret the returned prediction. This semantic bottleneck allows to detect failure cases in the prediction process so as to accept or reject the decision.
This thesis presents work in the area of Large Vocabulary Continuous Speech Recognition (LVCSR) system combination. The thesis focuses on methods for harnessing heterogeneous systems in order to increase the efficiency of speech recognizer with reduced latency. Automatic Speech Recognition (ASR) is affected by many variabilities present in the speech signal, therefore single ASR systems are usually unable to deal with all these variabilities. System combination techniques are usually used within multi-passes ASR architecture. We then propose an efficiently generalizable improvement based on Bag Of N-Gram for the driven decoding, called BONG. We study various theoretical harnessing models and we present an example of harnessing implementation based on client/server distributed architecture. Then we propose, an adaptation of the ROVER combination method to be performed during the decoding process using a local vote procedure followed by voting based on word frequencies.
Nowadays, Social Networks are ubiquitous in all aspects of life. A fundamental feature of these networks is the connection between users. These are gradually engaged to contribute by adding their own content. This field has now led to a great deal of research in recent years. One of the main problems is the detection of communities. The research presented in this thesis is positioned in the themes of the semantic analysis of Social Networks and the generation of personalized interactive applications. This thesis proposes an approach for the detection of communities of interest in Social Networks. This approach models social data in the form of a social user profile represented by an ontology. It implements a method for the Sentiment Analysis based on the phenomena of social influence and homophily. This generation is based on an approach of type MDA, independent of the application domain. In addition, this manuscript reports an evaluation of our proposals on data from Real Social Networks.
Question Answering (QA) is a field in computer science, which is concerned about building a system, which can automatically answer a given question posed by user in natural language. There are mainly three sources that Question Answering systems use to find an answer: free text (like office documents, web-pages and books), web-services (like services for weather, stock prices and time) and knowledge bases (KB). KBs are a structured collection of information that can be accessed and interpreted easily by machines. The main topic of this thesis is to study QA systems over such KB. QA systems over KBs are important because there is a large amount of information available in KBs and at the same time the information in KBs is very difficult to access by end users. QA systems over KBs are used in diverse industrial applications like Google Search, Siri, Alexa and Bing. One thing is common between these applications that they query proprietary KBs. This thesis provides several contributions in the domain of QA systems over KBs. First, it presents a comprehensive study of existing QA systems. It presents a detailed analysis and comparison of all QA systems that were evaluated over a popular benchmark called QALD, and points to the QA systems evaluated over other two very popular benchmarks, namely WebQuestions and SimpleQuestions. The analysis also contains a list of important challenges in this domain. Second, it presents a novel algorithm to construct QA systems over KBs. We prove these claims by applying this novel algorithm to 5 different languages (namely English, German, French, Spanish and Italian), to 6 different knowledge bases (namely Wikidata, DBpedia, Freebase, Scigraph, Dblp and MusicBrainz) and by comparing its performance over popular benchmarks like QALD and SimpleQuestions. Third, it shows how the answer of a QA system can be presented to a user. KBs contain main possible contextual information that can be presented together with the answer itself. These include textual descriptions, external links, images and videos. This thesis show how this information can be presented to the user together with the answer. The last part describes a possible architecture for QA systems that allows to construct QA systems in a modular and collaborative way. It allows to build QA systems which can be distributed over the web and is a tentative approach to standardize typical QA work flows. All the contributions of this thesis are integrated in an online demo that is available under www.wdaqua.eu/qa.
With the evolution of Web technology, healthcare documentation is becoming increasingly abundant and accessible to all, especially to patients, who have access to a large amount of health information. Unfortunately, the ease of access to medical information does not guarantee its correct understanding by the intended audience, in this case non-experts. Our PhD work aimsat creating a resource for the simplification of medical texts, based on a syntactico-semantic analysis of verbs in four French medical corpora, that are distinguished according to the level of expertise of their authors and that of the target audiences. The resource created in the present thesis contains 230 syntactico-semantic patterns of verbs (called pss), aligned with their non-specialized equivalents. The semi-automatic method applied, for the analysis of verbs,in order to achieve our goal is based on four fundamental tasks: the syntactic annotation of the corpora, carried out thanks to the Cordial parser (Laurent et al., 2009) ; the semantic annotation of verb arguments, based on semantic categories of the French version of a medical terminology known as Snomed International (Côté, 1996) ; the acquisition of syntactico-semantic patterns of verbs and the contrastive analysis of the verbs behaviors in the different corpora. The pss, acquired at the end of this process, undergo an evaluation (by three teams of medical experts) which leads to the selection of candidates constituting the nomenclature of our texts implification resource. These pss are then aligned with their non-specialized equivalents, this alignment leads to the creation of the simplification resource, which is the main result of our PhD study. The content of the resource was evaluated by two groups of people: linguists and non-linguists. The results show that the simplification of pss makes it easier for non-experts to understand the meaning of verbs used in a specialized way, especially when a certain set of parameters is collected.
The paramount stake of this research is to achieve an integrative functional model for the analysis of affective verbs in French and Arabic. I have chosen four affective verbs: two verbs of emotion (to astonish and to rage in French and their equivalent in Arabic) and two verbs of sentiment (to admire and to envy in French and their equivalent [ʔadhaʃa], [ʔaɣḍaba] in Arabic) they belong to semantic dictions of Surprise, Anger, Admiration, and Jealousy. More concretely, the analysis is shaped: - On the semantic and syntactic level: the semantic dimensions carried by verbal collocations such as to extremely astonish, to rage prodigiously in French, and [ʔaʕʒaba ʔiʕʒāban kabīran] (admire admiration big) *, [ɣaḍaba ɣaḍabaan ʃadīdan] (to rage rage extreme), and in Arabic are systematically linked to syntax (the recurrent grammatical constructions) (Hoey 2005). - On the syntactic and discursive level: the usage of passive, active and reflexive forms of affective verbs are dealt with from the perspective of informational dynamics in the sentence (Van Valin et LaPolla 1997). From a methodological point of view, the study is based on the quantitative and qualitative approach of the verbal combination and favours the contrastive one. It is founded on the French journalistic corpus of Emobase Database (Emolex project 100 M of words) and the journalistic corpus Arabicorpus) (137 M of words). Furthermore, the thesis participates in the studies of semantic values, the syntactic and the discursive behavior of affective verbs'combinations, in Arabic and in French, which will enable to better structure the diction of emotions in relation to what is proposed by current studies in lexicography. The main results of the study can be applied in language teaching, translation, and automated processing of emotions'lexicon in the two compared languages.
Sequencing technologies has revolutionized the field of personalized genomics through their resolution and low cost. However, these new technologies are associated with a relatively high error rate, which varies between 0.1% and 1% for second-generation sequencers. This value is problematic when searching for low allelic ratio variants, as observed in the case of heterogeneous tumors. Indeed, such error rate can lead to thousands of false positives. Each region of the studied DNA must therefore be sequenced several times, and the variants are then filtered according to criteria based on their depth. Despite these filters, the number of errors remains significant, showing the limit of conventional approaches and indicating that some sequencing errors are not random. In the context of this thesis, we have developed an exact algorithm for over-represented degenerate DNA motifs discovery on the upstream of non-random sequencing errors and thus potentially linked to their appearance. This algorithm was implemented in a software called DiNAMO, which was tested on sequencing data from IonTorrent and Illumina technologies. The experimental results revealed several motifs, specific to each of these two technologies. We then showed that taking these motifs into account in the analysis reduced significantly the false-positive rate. DiNAMO can therefore be used downstream of each analysis, as an additional filter to improve the identification of variants, especially, variants with low allelic ratio.
These past ten years, TV series became increasingly popular. In contrast to classical TV series consisting of narratively self-sufficient episodes, modern TV series develop continuous plots over dozens of successive episodes. However, the narrative continuity of modern TV series directly conflicts with the usual viewing conditions:due to modern viewing technologies, the new seasons of TV series are being watched over short periods of time. As a result, viewers are largely disengaged from the plot, both cognitively and emotionally, when about to watch new seasons. Such a situation provides video summarization with remarkably realistic use-case scenarios,that we detail in Chapter 1. Furthermore, automatic movie summarization, long restricted to trailer generation based on low-level features, finds with TV series a unprecedented opportunity to address in well-defined conditions the so-called semantic gap: summarization of narrative media requires content-oriented approaches capable to bridge the gap between low-level features and human understanding. We review in Chapter 2 the two main approaches adopted so far to address automatic movie summarization. In Chapter 4, we make use of social network analysis as a possible way to model the plot of modern TV series: the narrative dynamics can be properly captured by the evolution over time of the social network of interacting characters. Nonetheless, we have to address here the sequential nature of the narrative when taking instantaneous views of the state of the relationships between the characters. We show that standard time-windowing approaches can not properly handle this case, and we detail our own method for extracting dynamic social networks from narrative media. Chapter 5 is dedicated to the final generation and evaluation of character-oriented summaries, both able to reflect the plot dynamics and to emotionally re-engage viewers into the narrative. We evaluate our framework by performing a large-scale user study in realistic conditions.
Next-generation communication services should be able to cooperate in order to meet specific needs, while keeping their autonomy. This implies to master their architectures and to share these architectures within the enterprise. Architectural frameworks are therefore essential. After a review of the state of the art in the telecom, web and IT fields, and after having discussed the stakes of the telecom service convergence, we introduce here four viewpoints (business, functional, technical and applicative), as well as a methodology to build reference views that will be shared across the enterprise, and architectural views that are dedicated to each service. We illustrate this approach through case studies and we point out its applications to build service offers, to rationalize existing services and to realize a convergence between various services.
The work presented in this manuscript investigates the use of virtual huamans, and, more broadly, of video game technologies to improve the care of older adults with cognitive impairment. Our work revolved around two use cases of virtual humans: as non-self and as self. More specifically, we have designed, implemented and evaluated an embodied conversational agent, called LOUISE (LOvely User Interface for Servicing Elders), meant to serve as an accessible user interface in cognitive compensation systems for older adults with cognitive impairment, and a system for virtual reality-based psychological therapy addressing the consequences of falls, called Virtual Promenade. These two projects have been conducted according to the principles of living lab participatory design, involving several stakeholders in the design process (patients, caregivers and healthcare professionals).
Molecular docking is a very complex task that can not be deal by only one user. Based on this observation, we propose to study the cognitive workload distribution on group of users in collaboration. However, it highlights limits with coordination conflicts through communication problem. Based on these results, we proposed a new working configuration coupled with new haptic communication metaphors to improve the communication and the coordination between the members of the group.
This thesis joins in the current searches(researches) on the feelings and the emotional reactions, on the modelling and the transformation of the speech, as well as on the musical performance. It seems that the capacity to express, to feign and to identify emotions, humors, intentions or attitudes, is fundamental in the human communication. The ease with which we understand the state of a character, from the only observation of the behavior of the actors and the sounds which they utter, shows that this source of information is essential and, sometimes, sufficient in our social relationships. This thesis joins in these last two domains and proposes several contributions. From a theoretical point of view, this thesis proposes a definition of the expressivity, a definition of the neutral expressivity, a new representation mode of the expressivity, as well as a set of expressive categories common to the speech and to the music. It places the expressivity among the census of the available levels of information in the performance which can be seen as a model of the artistic performance. It proposes an original model of the speech and its constituents, as well as a new hierarchical prosodic model. From an experimental point of view, this thesis supplies a protocol for the acquisition of performed expressive data. Collaterally, it makes available three corpora for the observation of the expressivity. It supplies a new statistical measure of the degree of articulation as well as several analysis results concerning the influence of the expressivity on the speech. From a technical point of view, it proposes a speech processing algorithm allowing the modification of the degree of articulation. It presents an innovative database management system which is used, already, used by some other automatic speech processing applications, requiring the manipulation of corpus. It shows the establishment of a bayesian network as generative model of context dependent transformation parameters. From a technological point of view, an experimental system of high quality transformation of the expressivity of a French neutral utterance, either synthetic or recorded, has been produced, as well as an on-line interface for perceptive tests. Finally and especially, from a forward-looking point of view, this thesis proposes various research tracks for the future, both on the theoretical, experimental, technical, and technological aspects. Among these, the confrontation of the demonstrations of the expressivity in the speech and in the musical performance seems to be a promising way.
Automated Information Extraction from texts is necessary for all applications that require the processing of large amounts of textual data, but the development of such systems, adapted to a specific domain, is still very costly because it requires to specify very precisely the information to extract or to produce lots of annotated data in order to train machine learning models. The goal of this thesis is to work on new ways to reduce the cost of domain adapation for information extraction systems. The proposed approach is based on two axes: (1) the development of a generic information extraction model, based on a model of semantic frames representing general events that can be made more generic by the use of word embeddings, such as BERT or ELMO and (2) the study of methods to automatically adapt this generic model to a new domain, exploiting only few annotated examples: this adaptation will rely on approaches such as distant supervision, active learning or transfer learning.
In this thesis we explore the problem of signature analysis in avionics maintenance, to identify failures in faulty equipment and suggest corrective actions to resolve the failure. The thesis takes place in the context of a CIFRE convention between Thales R &amp; T and the Université Paris-Sud, thus it has both a theoretical and an industrial motivation. The signature of a failure provides all the information necessary to understand, identify and ultimately repair a failure. Thus when identifying the signature of a failure it is important to make it explainable. We propose an ontology based approach to model the domain, that provides a level of automatic interpretation of the highly technical tests performed in the equipment. Once the tests can be interpreted, corrective actions are associated to them. The approach is rooted on concept learning, used to approximate description logic concepts that represent the failure signatures. Since these signatures are not known in advance, we require an unsupervised learning algorithm to compute the approximations. In our approach the learned signatures are provided as description logics (DL) definitions. These serve as explanations for the discovered signatures. We use a different perspective, and rely on a bottom-up construction of the ontology. Similarly to other approaches, the learning process is achieved through a refinement operator that traverses the space of concept expressions, but an important difference is that in our algorithms this search is guided by the information of the individuals in the ontology. To this end the notions of justifications in ontologies, most specific concepts and concept refinements, are revised and adapted to our needs. The approach is then adapted to the specific avionics maintenance case in Thales Avionics, where a prototype has been implemented to test and evaluate the approach as a proof of concept.
For political or environmental matters, land cover mapping has become more and more important since the beginning of the XXIst century. Adressing very various applications, from a local scale (city-wise) to a more global scale (world-wide), several projects have been initiated so as to build land cover maps that would fit one of these applications. Yet, remote sensors have multiplied for the past two decades and the panel of such sensors is very wide and diverse in a spectral way and in terms of resolution. SPOT 6 and 7 were launched in 2012 and 2014 respectively, and offer very high resolution images at 1.5m, in four bands. The French Mapping Agency computes each year a full France coverage from the SPOT data available through the THEIA data portal. Although this task has been widely studied in the past by the remote sensing community, by the means of supervised classifiers such as SVMs or random forests, none of the results have yet led to truly automated and satisfying maps that match existing specifications due to mislabelling errors that occur too often. These days, deep learning has become a part of everyday life, not only for academic research purposes, but also it society where it runs in the background of our smartphones for various tasks. In terms of methodology, the deep learning have proved to be far more efficient than other machine learning algorithms in various fields of research, from natural language processing to object recognition in images. The heavy need for data is what allows deep learning to to be so successful in various fields. The works reported in this dissertation are focused on this problematic of land cover mapping, with the additional constraint of tackling this task in a more general operational environment. Transfer learning is a tool that can help a lot in lots of scenarii so as to reduce both time computation and needs for training samples. Finally, in a context of the automatic update or re-computation of existing topographic database, the use of aerial images in deep neural networks is assessed in several tasks, with an emphasis on the training dataset conception from topographic database that can include some drawbacks
Properties of compounds beginning with a preposition are then considered. The items are consequently automatically classified. Relational data analysis (Condorcet criterion) allows us to establish a statistical classification showing they are adverbs, which can be characterized with a positive correlation between syntax and semantic on the one hand, and between the compound's frozenness and their integration into phrases on the other hand.
Time Series Classification (TSC) has received an important amount of interest over the past years due to many real-life applications. Extensive experiments exhibit that D-BoTSW significantly outperforms nearly all compared standalone baseline classifiers. Then, we propose an enhancement of the Learning Time Series Shapelets (LTS) algorithm called Adversarially-Built Shapelets (ABS) based on the introduction of adversarial time series during the learning process. Due to the lack of available RS time series datasets,we also present and experiment on two remote sensing time series datasets called TiSeLaCand Brazilian-Amazon
The last years have seen the development of online health forums, virtual places for information exchange on the theme of health or disease. Considered as an informal learning medium, some have the particularity of being independent of the healthcare system because they are moderated by patients. The exchanges are less subject to social desirability and therefore, more genuine. Currently, regarding their formative properties, self-directed learning seems relevant to the analysis regarding the learning process on forums. To better understand the operation and interest of these forums, this research aims to identify the circumstances and conditions of use in relation to these forums by Internet users; and to characterize learning through these forums. In a retrospective and qualitative approach, the author chose mixed methods of collection and analysis. Indeed, the qualitative analysis of 30 interviews conducted with Internet users completes the quantitative analysis of 1319 messages posted by 40 users from the same forum. The exploration focuses on the users profiles, the reasons why, the time and the ways they use the forum, the contents discussed, the use of information exchanged, and also the forms and learning process. This media is a complete tool for users that respects their pace and their learning needs. The results show traces of intentional learning as well as several levels of simultaneous learning, despite the absence of carers and the fact that the forum is not specifically designed for learning. This observation supports the hypothesis that part of the learning is related to its use and that it is a place favourable to self-training. It is complementary to therapeutic patient education programs and could relate to follow-up and reinforcement therapeutic education. The results highlight the importance of developing critical thinking about information as an adaptive skill in patients. They invite to think about the roles and means of the moderators of these forums in this process of cognitive evolution.
This thesis presents contributions to the resolution (on GPUs) of real optimization problems of large sizes. The vehicle routing problems (VRP) and the hub location problems (HLP) are treated. A parallel genetic algorithm (GA) on GPU is proposed to solve different variants of the HLP. The proposed GA adapts its encoding, initial solution, genetic operators and its implementation to each of the variants treated. Finally, we used the GA to solve the HLP with uncertainties on the data. The numerical tests show that the proposed approaches effectively exploit the computing power of the GPU and have made it possible to resolve large instances up to 6000 nodes.
Major nuclear accidents generate large scale crisis that may contaminate wide areas for decades. The inhabitants of these areas must then gain new knowledge and adapt their lifestyle to limit the health and social consequences of radioactivity. France developed a procedures to manage such a situation, but the communication stragies presented do not fully support neither dialogue between authorities and citizen nor the use of social Web tools. It aims to assess the benefits of information-communication technologies to develop more suitable crisis communication strategies for post-nuclear-accident situations. After the Fukushima Daiichi nuclear disaster, collaborative practices have emerged to collect and aggregate radiation measurements thanks to social media. The analysis of Twitter uses revealed that the diffusion of these measurements is not well adapted to the post-nuclear-accident situation. We developed a set of guidelines and a software prototype to support the re-usability of the radiation measurements that are shared on social media. We presented several methods to support the identification and the formal representation of this knowledge using Semantic Web technologies. From these models, we built and tested Ginkgo, a mobile web application designed to support knowledge sharing and appropriation.
In Natural Language Processing area, several researches were carried out in order to gather semantically close terms. The linguistic underlying problem is the categorization. We demonstrate what the distributive method and the justification inspired by the semiotic can bring to this topic. We suppose, according to Harris, that the study of words and their operations can be used as foundation for researches on the semantic of these words.
With the increase in size of supercomputers, also increases the number of failures or abnormal events.This increase of the number of failures reduces the availability of these systems. To manage these failures and be able to reduce their impact on HPC systems, it is important to implement solutions to understand the failures and to predict them. HPC systems produce a large amount of monitoring data that contains useful information about the status of these systems. However, the analysis of these data is difficult and can be very tedious because these data reflect the complexity and the size of HPC systems. The work presented in this thesis proposes to use machine-learning-based solutions to analyse these data in an automated way. More precisely, this thesis presents two main contributions: the first one focuses on the prediction of processors overheating events in HPC systems, the second one focuses on the analysis and the highlighting of the relationships between the events present in the system logs. Both contributions are evaluated on real data from a large HPC system used in production. To predict CPU overheating events, we propose a solution that uses only the temperature of the CPUs. It is based on the analysis of the general shape of the temperature prior to an overheating event and on the automated learning of the correlations between this shape and overheating events using a supervised learning model. The use of the general curve shape and a supervised learning model allows learning using temperature data with low accuracy and using a limited number of overheating events. The evaluation of the solution shows that it is able to predict overheating events several minutes in advance with high accuracy and recall. Furthermore, the evaluation of these results shows that it is possible to use preventive actions based on the predictions made by the solution to reduce the impact of overheating events on the system. To analyze and to extract in an automated way the causal relations between the events described in the HPC system logs, we propose an unconventional use of a deep machine learning model. Indeed, this type of model is classically used for prediction tasks. Thanks to the addition of a new layer proposed by state-of-the-art contributions of the machine learning community, it is possible to determine the weight of the algorithm inputs associated to its prediction. Using this information, we are able to detect the causal relations between the different events. The evaluation of the solution shows that it is able to extract the causal relations of the vast majority of events occurring in an HPC system. Moreover, its evaluation by administrators validates the highlighted correlations. Both contributions and their evaluations show the benefit of using machine learning solutions for understanding and predicting failures in HPC systems by automating the analysis of supervision data.
During the last few years, the technological progress in collecting, storing and processing a large quantity of data for a reasonable cost has raised serious privacy issues. Privacy concerns many areas, but is especially important in frequently used services like search engines (e.g., Google, Bing, Yahoo!). These services allow users to retrieve relevant content on the Internet by exploiting their personal data. In this context, developing solutions to enable users to use these services in a privacy-preserving way is becoming increasingly important. In this thesis, we introduce SimAttack an attack against existing protection mechanism to query search engines in a privacy-preserving way. This attack aims at retrieving the original user query. We show with this attack that three representative state-of-the-art solutions do not protect the user privacy in a satisfactory manner. We therefore develop PEAS a new protection mechanism that better protects the user privacy. This solution leverages two types of protection: hiding the user identity (with a succession of two nodes) and masking users'queries (by combining them with several fake queries). To generate realistic fake queries, PEAS exploits previous queries sent by the users in the system. Finally, we present mechanisms to identify sensitive queries. Our goal is to adapt existing protection mechanisms to protect sensitive queries only, and thus save user resources (e.g., CPU, RAM). We design two modules to identify sensitive queries. By deploying these modules on real protection mechanisms, we establish empirically that they dramatically improve the performance of the protection mechanisms.
The purpose of our thesis is to study linguistic realizations of discourse relations, considering especially the relation of Elaboration. Until now, there have been few studies based on Elaboration. This lack of interest as regards this relation can be explained by the fact that it does not have any prototypical marker. Nevertheless, this relation can often be found in texts and has a central role in discourse structure. Our objective is of two kinds. We propose linguistic descriptions of markers, cues, and combinations of cues in order to enrich a discourse model and to allow an automatic or semi-automatic identification of the relation. The first stakes are essentially empirical. We aim at improving the descriptive analysis for this relation. The second stakes are more applicative. Indeed, an automatic detection of discourse relations is a very challenging task for computational linguistics.
This dissertation deals with the application of experimental [acoustic and perceptual] phonetics to the teaching of pronunciation. The issue is illustrated by Japanese speakers' difficulties in learning French vowels in general. Experiments were specifically conducted on the vowels 'u y ø'. The objective is to elucidate the case of individual phones depending on whether or not their phonemic status and their phonetic realisation differ in the two languages under study. French 'u' differs phonetically from its phonemic counterpart, Japanese 'u'. The present study confirms that French 'u', which is phonemically “similar” [to Japanese 'u'], turns out to be more difficult than the “new” vowel 'y', which has no phonemic or phonetic counterpart in Japanese. The production of 'ø', which is phonemically “new” but acoustically close to Japanese 'u', seems to present still less difficulty. The dissertation also brings a reflection on the teaching of pronunciation. The analysis of general French textbooks published in Japan suggests that learners and teachers are seldom aware of the difference in the difficulties caused by the three vowels 'u y ø'. Also, some methods of pronunciation teaching [some traditional, others new] are presented in terms of how they foster learners' awareness of these difficulties. The goal of this dissertation is to help to shed light on the learning processes of the pronunciation of foreign languages, and to improve its learning and teaching.
The neOCampus operation, started in 2013 by Paul Sabatier University in Toulouse, aims to create a connected, innovative, intelligent and sustainable campus, by exploiting the skills of 11 laboratories and several industrial partners. These multidisciplinary skills are combined in order to improve users (students, teachers, administrative staff) daily comfort and to reduce the ecological footprint of the campus. The intelligence we want to bring to the campus of the future requires to provide to its buildings a perception of its intern activity. Indeed, optimizing the energy resources needs a characterization of the user's activities so that the building can automatically adapt itself to it. Human activity being open to multiple levels of interpretation, our work is focused on extracting people trajectories, its more elementary component. Characterizing users activities, in terms of movement, uses data extracted from cameras and microphones distributed in a room, forming a sparse network of heterogeneous sensors. From these data, we then seek to extract audiovisual signatures and rough localizations of the people transiting through this network of sensors. While protecting person privacy, signatures must be discriminative, to distinguish a person from another one, and compact, to optimize computational costs and enables the building to adapt itself. Having regard to these constraints, the characteristics we model are the speaker's timbre, and his appearance, in terms of colorimetric distribution. The scientific contributions of this thesis are thus at the intersection of the fields of speech processing and computer vision, by introducing new methods of fusing audio and visual signatures of individuals. To achieve this fusion, new sound source location indices as well as an audiovisual adaptation of a multi-target tracking method were introduced, representing the main contributions of this work. Acoustic and visual modalities are not correlated, so two signatures are separately computed, one for video and one for audio, using existing methods in the literature. After a first chapter dedicated to the state of the art in re-identification and speaker recognition methods, the details of the computation of the signatures is explored in chapter 2. Spatio-temporal coherence of the bimodal observations is then discussed in chapter 4, in a context of multi-target tracking.
As suggested in the works of Assia Djebar about the body of the woman excluding the cleavage of the flesh and soul and of the body and personality means a vision of a united body encompasses a broad duration and space. The thesis puts the emphasis on its continuity, resistance and even the survival of its identity, despite the factors or contexts that have harmed it. Before giving back to the female body its lost unity, we identified the key terms of body and personality through the humanities, while taking into account the specifics of these terms related to the Arab-Muslim identity, to the Berber characteristics and to the French influence. With this starting point, we do not fall into the conventional and strictly social categorizations of the Algerian woman. But it goes beyond the appearances since, supported by the language and the imagination, it drives a reflexive dimension. The Djebarien female character transitions from the stage "have a body" to the stage of "being a body" with several dimensions: physical, psychological, intellectual, linguistic and imaginary (I). But that image of unity and harmony is faced with less favorable pictures that appeared because Islam moved away from its original doctrine as presented in the book “Far from Medina”, and the valuation of certain concepts such as honor, modesty and shame. Faced with the male authority that is exercised on the Algerian female body in every moment of life, and which results in confinement, humiliation, arrest to some very well-defined roles (such as mother and wife), orders, beatings, insults, etc.., the female body develops a “micro psychology” (M. Maffesoli) that is transmitted from generation to generation and provides built-in answers to various situations. All actions are impregnated with these, but that doesn't stop preventing the emergence of hidden traces of the female personality in very specific contexts. These traces highlight the cunning, the challenge and even the hatred of women to men, designated in the Algerian female imaginary by the term "e'dou" (enemy). These feelings reveal the strength of the female body made of a silent revolt expressed or debased by shouts, murmurs, attentive listening, a need to share and support each other.(II) So we have in front of our eyes a fragmented body, which has forgotten its qualities due to the internalization of these symbolic prisons. But thanks to the female solidarity, the appreciation of the house as a place to cocoon, the relationships between women, and the return to the first language, the traces of the distant past are renewed by the actions and words of some free women. These pave the way for the release of the Algerian female body that will learn again to watch, to walk outside, to reminisce, to talk about itself and to appreciate the presence of the beloved man (III). The analysis of body parts visible in our corpus, the feminine posture, the gestures in which the tradition is recorded, the reactions that reveal both the physical and psychic dimension, the terms used by Djebar to talk about his feminine characters, allowed us to reveal a female body with a heart, memories, feelings, personalities and roles that are outside the framework imposed by the society. The female body able to make gestures, which falls within the time and space reclaimed, acquires a performative speech which, in turn, recreates and provides it with the opportunity to perform, while maintaining contact with the origins and the "living word".
This thesis examines the automatic processing of elliptical phenomena in English. Situated at the crossroads of various disciplines, such as theoretical linguistics, corpus linguistics, computational linguistics and translation studies, it implements an experimental approach in order to achieve two main goals. The first one consists in establishing that ellipsis can be detected automatically, and the second one in exploring procedures to facilitate its machine translation from English into French. For these purposes, this thesis resorts to morpho-syntactic analyses, which seem sufficient for the automatic detection of some types of ellipsis, since through breaking down the phenomenon into its specific features it makes it possible to identify it among others. The investigation draws on a parallel, multi-genre corpus compiled to test the research hypotheses. In order to develop detection patterns and exploit the corpus, the CoreNLP package developed at the University of Stanford (US) is used, and its limitations in the processing of ellipsis are highlighted. Findings and results emphasize the close link between the detection of ellipsis and its machine translation as a key factor in understanding translation errors caused by its automatic processing.
The wide spread of Semantic Web technologies such as the Resource Description Framework (RDF) enables individuals to build their databases on the Web, to write vocabularies, and define rules to arrange and explain the relationships between data according to the Linked Data principles. As a consequence, a large amount of structured and interlinked data is being generated daily. A close examination of the quality of this data could be very critical, especially, if important research and professional decisions depend on it. The quality of Linked Data is an important aspect to indicate their fitness for use in applications. Several dimensions to assess the quality of Linked Data are identified such as accuracy, completeness, provenance, and conciseness. This thesis focuses on assessing completeness and enhancing conciseness of Linked Data. In particular, we first proposed a completeness calculation approach based on a generated schema. Indeed, as a reference schema is required to assess completeness, we proposed a mining-based approach to derive a suitable schema (i.e., a set of properties) from data. We further proposed an approach to discover equivalent predicates to improve the conciseness of Linked Data. This approach is based, in addition to a statistical analysis, on a deep semantic analysis of data and on learning algorithms. We argue that studying the meaning of predicates can help to improve the accuracy of results. Finally, a set of experiments was conducted on real-world datasets to evaluate our proposed approaches.
Cognitive solutions, including conversational agents, are based on unstructured textual data. The design of these solutions is slowed down by the lack of annotation and semantic interpretation tools. We plan to use clustering to automatically group the data and thus speed up data labelling. However, clustering faces many problems that make this method not optimal depending on the conditions of use. The aim of the thesis is to study the applicative scope of the feature maximization technique associated with clustering to overcome these problems and meet the needs of support tools for the design of these solutions.
In the manufacturing domain, the detection of anomalies such as mechanical faults and failures enables the launching of predictive maintenance tasks, which aim to predict future faults, errors, and failures and also enable maintenance actions. With the trend of Industry 4.0, predictive maintenance tasks are benefiting from advanced technologies such as Cyber-Physical Systems (CPS), the Internet of Things (IoT), and Cloud Computing. These advanced technologies enable the collection and processing of sensor data that contain measurements of physical signals of machinery, such as temperature, voltage, and vibration. However, due to the heterogeneous nature of industrial data, sometimes the knowledge extracted from industrial data is presented in a complex structure. Therefore formal knowledge representation methods are required to facilitate the understanding and exploitation of the knowledge. Furthermore, as the CPSs are becoming more and more knowledge-intensive, uniform knowledge representation of physical resources and reasoning capabilities for analytic tasks are needed to automate the decision-making processes in CPSs. These issues bring obstacles to machine operators to perform appropriate maintenance actions. To address the aforementioned challenges, in this thesis, we propose a novel semantic approach to facilitate predictive maintenance tasks in manufacturing processes. These approaches have been validated on both real-world and synthetic data sets.
In order to make these systems widely usable by teachers and learners, they have to provide means to assist teachers in their task of exercise generation. Among these exercises, multiple-choice tests are very common. However, writing Multiple-Choice Questions (MCQ) that correctly assess a learner's level is a complex task. Guidelines were developed to manually write MCQs, but an automatic evaluation of MCQ quality would be a useful tool for teachers. We are interested in automatic evaluation of distractor (wrong answer choice) quality. To do this, we studied characteristics of relevant distractors from multiple-choice test writing guidelines. This study led us to assume that homogeneity between distractors and answer is an important criterion to validate distractors. Homogeneity is both syntactic and semantic. We validated the definition of homogeneity by a MCQ corpus analysis, and we proposed methods for automatic recognition of syntactic and semantic homogeneity based on this analysis. Then, we focused our work on distractor semantic homogeneity. To automatically estimate it, we proposed a ranking model by machine learning, combining different semantic homogeneity measures. The evaluation of the model showed that our method is more efficient than existing work to estimate distractor semantic homogeneity
In this dissertation we study the French cross-disciplinary scientific lexicon (CSL), a lexicon which fall within the genre of scientific articles in humanities and social sciences. As the CSL is commonly used in scientific texts, it is a gateway of interest to explore this genre. This lexicon has also practical applications in the fields of automatic terms identification and foreign language teaching in the academic background. To this end, we apply a corpus-driven approach in order to extract and structure the CSL lexical units which are complex to circumscribe. As we analyze the combinatorial properties extracted from a parsed corpus of scientific articles, we performed a CSL study based on its genre specific use. We follow the same approach to identify cross-disciplinary meanings for the CSL nouns and to design a nominal semantic classification. This two-level typology allow us to explore rhetorical and phraseological CSL properties by identifying frequent syntactico-semantic patterns.
Ontologies are used in different fields like the semantic Web or the knowledge engineering. The ontologies allow to share, to diffuse and to update knowledge domain. This thesis propose a methodology to build ontologies using methods of Natural Language Processing (NLP) and Information Extraction (IE) for extracting prepared data from each kind of available resources in the domain (text corpora, databases, thesaurus). Then, these prepared data are mining with the mining methods: Formal Concepts Analysis (FCA) and Relational Concepts Analysis (RCA). The FCA regroups a set of objects sharing the same set of attributes in the same concept. The RCA, an extension of the FCA regroups a set of objects sharing the same attributes and the same relations (relational attributes) in the same concept. The apposition of contexts, a property of the FCA, affects a set of attributes and relational attributes to classes pre-defined and hierarchised by the domain experts. These affectations allow us to present classes and their definitions to the experts of domain as well as new nonexistent classes in the initial hierarchy. These new classes can be considered appropriate and added by experts as new «knowledge units». The Lattices resulting from the data mining methods are considered as «ontology schema». This ontology schema is represented in the FLE description logics language to obtain ontology. This ontology is implemented in the OWL language (Web Ontology Language) for allowing us to request it. This methodology was tested in different domains: Microbiology and Astronomy.
In the thesis we focus on encoding inpatient episode into standard codes, a highly sensitive medical task in French hospitals, requiring minute detail and accuracy, since the hospital's income directly depends on it. Encoding inpatient episode includes encoding the primary diagnosis that motivates the hospitalisation stay and other secondary diagnoses that occur during the stay. We propose a retrospective analysis on the encoding task of some selected secondary diagnoses. Hence, the PMSI database is analysed in order to extract, from previously encoded inpatient episodes, the decisive features to encode a difficult secondary diagnosis occurred with frequent primary diagnosis. Consequently, at the end of an encoding session, once all the features are available, we propose to help the coders by proposing a list of relevant encodings as well as the features used to predict these encodings. With respect to the medical domain knowledge challenge, we collaborate with expert coders in a local hospital in order to provide expert insight on some difficult secondary diagnoses to encode and in order to evaluate the results of the proposed methodology. With respect to the medical databases exploitation challenge, we use ML methods such as Feature Selection (FS), focusing on resolving several issues such as the incompatible format of the medical databases, the excessive number features of the medical databases in addition to the unstable features extracted from the medical databases. Regarding to issue of the incompatible format of the medical databases caused by relational databases, we propose a series of transformation in order to make the database and its features more exploitable by any FS methods. To limit the effect of the excessive number of features in the medical database, usually motivated by the amount of the diagnoses and the medical procedures, we propose to group the excessive number features into a proper representation level and to study the best representation level. Lastly, as the dataset linked with diagnoses are highly imbalanced due to classification categories that are unequally represented.
This thesis presents an open and flexible methodological framework for the automatic acquisition of multiword expressions (MWEs) from monolingual textual corpora. This research is motivated by the importance of MWEs for NLP applications. After briefly presenting the modules of the framework, the work reports extrinsic evaluation results considering two applications: computer-aided lexicography and statistical machine translation. Both applications can benefit from automatic MWE acquisition and the expressions acquired automatically from corpora can both speed up and improve their quality. The promising results of our experiments encourage further investigation about the optimal way to integrate MWE treatment into these and many other applications.
These systems receive a question in natural language from the user and search for the answer in a collection of documents. This work relies on the notion of justification, which is formalised as a mapping between the pieces of linguistic information of the question and the corresponding elements in the answer passage. That model takes into account three categories of linguistic phenomena: paradigmatic (local) variations of terms (semantical, morphological, inference), syntagmatic links between sentence constituents, and a component of enunciative semantics linking together the remote elements (by anaphora, coreference, thematisation), in a multi-sentence context, as well mono- or multi-documents. In this work, I first describe the semi-automatic extraction of a corpus of question-answer couples. On the corpus, we measure the justifications'conformation in terms of semantic variation and spatial extension. Then, I describe an evaluate a program for extracting and weighting the justifications located in the newspaper articles'passages brought by a question-answering processing chain. My program aims at preserving the system's ability to produce a structured justification, while making possible to integrate a large variety of heterogeneous linguistic processes of various nature, granularity level and reliability.
Over the last decade, the growing popularity of Online Social Networks has attracted a pervasive presence of social spammers. While this presence has started with spam advertising and common scams, the recent years have seen this escalate to the far more concerning mass manipulation attempts. This targeted and largely automated abuse of social platforms is risking the credibility and usefulness of the information disseminated on these platforms. The social spam detection problem has been traditionally modeled as a supervised problem where the goal is to classify individual social accounts. This common choice is problematic for two reasons. First, the dynamic and adversarial nature of social spam makes the performance achieved by features-based supervised systems hard to maintain. Second, features-based modeling of individual social accounts discards the collusive context in which social attacks are increasingly undertaken. Acting synchronously allows spammers to gain greater exposure and efficiently disseminate their content. Thus, even when spammers change their characteristics, they continue to act collusively, inevitably creating links between collusive spamming accounts. This constitutes an unsupervised signal that is relatively easy to maintain and hard to evade. It is therefore beneficial to find a suitable similarity measure that captures this collusive behavior. Accordingly, we propose in this work to cast the social spam detection problem in probabilistic terms using the undirected graphical models framework. Instead of the individual detection paradigm that is commonly used in the literature, we aim to model the classification task as one of joint inference. In this context, accounts are represented as random variables and the dependency between these variables is encoded in a graphical structure. We propose two graphical models: the Markov Random Field with inference performed via Loopy Belief Propagation, and the Conditional Random Field with a setting that is more adapted to the classification problem, namely by adopting the Tree Reweighted message passing algorithm for inference and a loss that minimizes the empirical risk. Both models, evaluated on Twitter, demonstrate an increase in classification performance compared to state-of-the-art supervised classifiers.
Image classification has a prominent interest in numerous visual recognition tasks, particularly for vehicle recognition in airborne systems, where the images have a low resolution because of the large distance between the system and the observed scene. During the training phase, complementary data such as knowledge on the position of the system or high-resolution images may be available. In our work, we focus on the task of low-resolution image classification while taking into account supplementary information during the training phase. We first show the interest of deep convolutional networks for the low-resolution image recognition, especially by proposing an architecture learned on the targeted data. On the other hand, we rely on the framework of learning using privileged information to benefit from the complementary training data, here the high-resolution versions of the images. We propose two novel methods for integrating privileged information in the learning phase of neural networks. Our first model relies on these complementary data to compute an absolute difficulty level, assigning a large weight to the most easily recognized images. Our second model introduces a similarity constraint between the networks learned on each type of data. We experimentally validate our models on several application cases, especially in a fine-grained oriented context and on a dataset containing annotation noise.
Multilingual Web Document (MWD) processing has become one of the major interests of research and development in the area of information retrieval. Therefore, we observed that the structure of the multilingual resources has not been enough explored in most of the research works in this area. Discarding the multilingual information structures could affect the processing performance and generate various problems: i) Redundancy: if the site proposes simultaneously translations in several languages, ii) Noisy information: by using labels to shift from language to another, iii) Loosing information: if the process does not consider the structure specificity of each language. The framework of this experimental research thesis is structures analysis for information extraction from a great number of heterogeneous structured or semi-structured electronic documents (essentially the Web document). To experiment and validate our aim we have developed Hyperling which is a formal, language independent, system dealing with Web Documents.
One way to bypass this limitation is the approach of cross-lingual transfer, whereby resources available in another (source) language are leveraged to help building accurate systems in the desired (target) language. However, despite promising results in research settings, the standard transfer techniques lack the flexibility regarding cross-lingual resources needed to be fully usable in real-world scenarios: exploiting very sparse resources, or assorted arrays of resources. This strategy is put into practice in the frame of transition-based dependency parsing. To this end, a new transfer framework is designed, with a cascading architecture: it enables the desired combination, while ensuring better targeted exploitation of each resource, down to the level of the word. Empirical evaluation dampens indeed the enthusiasm for the purely cross-lingual approach -- it remains in general preferable to annotate just a few target sentences -- but also highlights its complementarity with other approaches. Several metrics are developed to characterize precisely cross-lingual similarities, syntactic idiosyncrasies, and the added value of cross-lingual information compared to monolingual training. The substantial benefits of typological knowledge are also explored. The whole study relies on a series of technical improvements regarding the parsing framework: this work includes the release of a new open source software, PanParser, which revisits the so-called dynamic oracles to extend their use cases. Several purely monolingual contributions complete this work, including an exploration of monolingual cascading, which offers promising perspectives with easy-then-hard strategies.
Linear programming relaxations are central to maximum a posteriori (MAP) inference in discrete Markov Random Fields (MRFs). In this thesis, we study the benefit of using Newton methods to efficiently optimize the Lagrangian dual of a smooth version of the problem. We investigate their ability to achieve superior convergence behavior and to better handle the ill-conditioned nature of the formulation, as compared to first order methods. We show a general framework for unsupervised learning based on optimal transport and sparse regularization. We especially demonstrate a promising approach to address the pre-image problem in kernel PCA. From an optimization point of view, we show how to compute the gradient of a smooth version of the Schatten p-norm and how it can be used within a majorization-minimization scheme.
The perpetual hardware and software evolution provide to the users new solutions in order to come up to their real needs. However, the setting up of these new tools should not call into question the user's work. Computer science does not require additional work from the users, but on the contrary provide them convivial and natural solutions permitting them to realize their daily tasks in better conditions. The first chapter describes the evolution of computer systems based on cooperative solutions using different components: personal computers, network and servers. The second chapter emphases the new "needs" of big organization by giving back computer science its real role: making user's work easier. So, the teleadministration concept has been defined to fit this goal. The last chapter presents two sides of human-computer interaction: on the one hand, proposing an object-based representation model and design method, on the other hand, integrating input and output vocal modality.
Syntactic parsing consists in assigning syntactic trees to sentences in natural language. Syntactic parsing of non-configurational languages, or languages with a rich inflectional morphology, raises specific problems. This dissertation investigates transition-based methods for robust discontinuous constituency parsing. First of all, we propose a multitask learning neural architecture that performs joint parsing and morphological analysis. Then, we introduce a new transition system that is able to predict discontinuous constituency trees, i.e.\ syntactic structures that can be seen as derivations of mildly context-sensitive grammars, such as LCFRS. Finally, we investigate the question of lexicalization in syntactic parsing. Some syntactic parsers are based on the hypothesis that constituent are organized around a lexical head and that modelling bilexical dependencies is essential to solve ambiguities.
The emergence of Information and Comunication Technologies (ICT) in the early 1990s, especially the Internet, made it easy to produce data and disseminate it to the rest of the world. The strength of new Database Management System (DBMS) and the reduction of storage costs have led to an exponential increase of volume data within entreprise information system. The large number of correlations (visible or hidden) between data makes them more intertwined and complex. The data are also heterogeneous, as they can come from many sources and exist in many formats (text, image, audio, video, etc.) or at different levels of structuring (structured, semi-structured, unstructured). All companies now have to face with data sources that are more and more massive, complex and heterogeneous technical information. The data may either have different denominations or may not have verifiable provenances. Consequently, these data are difficult to interpret and accessible by other actors. They remain unexploited or not maximally exploited for the purpose of sharing and reuse. Data access (or data querying), by definition, is the process of extracting information from a database using queries to answer a specific question. Extracting information is an indispensable function for any information system. However, the latter is never easy but it always represents a major bottleneck for all organizations (Soylu et al. 2013). In the environment of multiuse of complex and heterogeneous, providing all users with easy and simple access to data becomes more difficult for two reasons: - Lack of technical skills: In order to correctly formulate a query a user must know the structure of data, ie how the data is organized and stored in the database. When data is large and complex, it is not easy to have a thorough understanding of all the dependencies and interrelationships between data, even for information system technicians. Moreover, this understanding is not necessarily linked to the domain competences and it is therefore very rare that end users have sufficient theses such skills. - Different user perspectives: In the multi-use environment, each user introduces their own point of view when adding new data and technical information.
Semantic annotations are often used in a wide range of applications ranging from information retrieval to decision support. Annotations are produced through the association of concept labels from Knowledge Organization System (KOS), i.e. ontology, thesaurus, dictionaries. New concepts can be added, obsolete ones removed and the definition of existing concepts may be refined through the modification of their labels/properties. Annotations enable machines to interpret, link, and use a vast amount of data. In this thesis we propose a framework called MAISA to tackle the problem of adapting outdated annotations when the KOS utilized to create them change. We distinguish two different cases. In the first one we consider that annotations are directly modifiable. In this case, we proposed a rule-based approach implementing information derived from the evolution of KOS as well as external knowledge from the Web. In the second case, we consider that the annotations are not modifiable. The goal is then to keep the annotated documents searchable even if the annotations are produced with a given KOS version but the user used another version to query them. In this case, we designed a knowledge graph that represent a KOS and its successive evolution and propose a method to extract the history of a concept and add the gained label to the initial query allowing to deal with annotation evolution. We experimentally evaluated MAISA on realistic cases-studies built from four well-known biomedical KOS: ICD-9-CM, MeSH, NCIt and SNOMED CT. We show that the proposed maintenance method allow to maintain semantic annotations using standard metrics.
Our first aim is to build linguistic resources for a French - Romanian factored phrase - based statistical machine translation system. Our second aim is to study the impact of exploited linguistic information in the lexical alignment and translation process. On the one hand, this study is motivated by the lack of such systems for the studied languages. On the other hand, it is motivated by the high number of errors provided by the current machine translation systems. The linguistic resources required by the system are tokenized, lemmatized, tagged, word, and sentence - aligned parallel corpora.
Word embeddings are a standard component of modern natural language processing architectures. Every time there is a breakthrough in word embedding learning, the vast majority of natural language processing tasks, such as POS-tagging, named entity recognition (NER), question answering, natural language inference, can benefit from it. In this purpose I pre-compute word co-occurrence statistics with corpus2graph, an open-source NLP-application-oriented Python package that I developed: it efficiently generates a word co-occurrence network from a large corpus, and applies to it network algorithms such as random walks. For cross-lingual contextual word embedding mapping, I link contextual word embeddings to word sense embeddings. The improved anchor generation algorithm that I propose also expands the scope of word embedding mapping algorithms from context independent to contextual word embeddings.
This thesis focuses on the use of French general monolingual dictionary in elementary education. How far does this now commonplace practice go back in history? Since when does the dictionary have a place in education? What dictionaries do we refer to in the twenty-first century? Are they commonly consulted by teachers and students? What roles do they play in learning the mother tongue? What is the place of the digital tools? In order to try to provide an answer to these questions, the study is organized into three parts: the institutionalization of elementary education; the adaptation of the first dictionaries to this education; the current use of the dictionary in schools. The historical context has always been taken into account, as have contemporary comparisons, especially geographical. The conclusion gives an evaluation of efforts undertaken so far and offers future perspectives. Several appendices are attached: legislation, lists of dictionaries classified by date or theme, excerpts from official reports, comparative tables..
This doctoral dissertation is about the improvement of social capacities of a conversationnal system to interact with humans. When the system is not dedicated to one particular task, it must take into account the inherent difficulties of social interaction it-self. Humor is a natural mechanism present in social interactions. We consider humour in a robotic system as a simulation of simplified behaviors from human humor: derision, jokes, puns. This work is based on theories issued from various research domains as sociology, psychology, neurosciences and linguistics to enable integration of humor in a robotic system. Implemented in some dialog systems, humorous capacities are however rarely used when programming the robot's behavior. In our study, the humourous behavior is implemented in the system by using the ritual theory of face-work. The face analysis of the interlocutor can be used to direct the robot's reactions during a casual humorous talk. In order to evaluate the faces of participants in interaction, we study, using data collections created for this purpose, the participant's behavior, emotionnal and expressive responses to different types of humor (humorous act targeting the robot, the participant or a neutral subject). Participant's reaction to humor are made upon a multi-level processing of emotionnal, linguistic and behavioral cues. Machine learning is used to extract rules defining appreciation or not and update the participant's face evaluation in regards of the humorous act produced by the robot. An implementation of these rules in an automatic dialog system allows us to evaluate their accuracy. Numerous experiments were carried out on various populations: elderly persons, adults, teenagers. Finally, the use of the participant's preferences in the conversation raises ethical questions, in particular against the persuasive and manipulative power of humor.
Integrative patterns of biodiversity, such as the distribution of taxa abundances and the spatial turnover of taxonomic composition, have been under scrutiny from ecologists for a long time, as they offer insight into the general rules governing the assembly of organisms into ecological communities. They can also be measured for the whole tree of life, including the vast and previously unexplored diversity of microorganisms. Taking full advantage of this new type of data is challenging however: DNA-based surveys are indirect, and suffer as such from many potential biases; they also produce large and complex datasets compared to classical censuses. The first goal of this thesis is to investigate how statistical tools and models classically used in ecology or coming from other fields can be adapted to DNA-based data so as to better understand the assembly of ecological communities. The second goal is to apply these approaches to soil DNA data from the Amazonian forest, the Earth's most diverse land ecosystem. Two broad types of mechanisms are classically invoked to explain the assembly of ecological communities: 'neutral' processes, i.e. the random birth, death and dispersal of organisms, and 'niche' processes, i.e. the interaction of the organisms with their environment and with each other according to their phenotype. Disentangling the relative importance of these two types of mechanisms in shaping taxonomic composition is a key ecological question, with many implications from estimating global diversity to conservation issues. In the first chapter, this question is addressed across the tree of life by applying the classical analytic tools of community ecology to soil DNA samples collected from various forest plots in French Guiana. The second chapter focuses on the neutral aspect of community assembly.
By knowing what clinical research is undertaken worldwide, where it is conducted, which diseases are studied, and who is supporting it, we could have a better understanding on how is created the knowledge concerning health interventions. A global landscape of health research may inform policy makers on knowledge gaps and on how to reallocate resources to address health needs, in particular in low-resource settings. In this thesis we mapped clinical research, i.e. we analyzed at a macro-level the complex system of health research to bring information on the global landscape of health research effort. We based our analyses on clinical trial registries included in the International Clinical Trials Registry Platform from the World Health Organization. In a first project, we evaluated within 7 regions the local alignment between the effort of research and the burden for 27 groups of diseases. This work needed the development of a knowledge-based classifier of clinical trial registries according to diseases studied based on natural language processing methods. In all other regions we identified research gaps. In particular, for Sub-Saharan Africa, where major causes of burden such as HIV and malaria received a high research attention, research was lacking for major causes of burden, especially for common infectious diseases and neonatal disorders. In a second project, we compared the mappings of multi-country trials for industry- and non-industry–sponsored clinical trials, and analyzed the networks of collaboration of countries participating together to the same multi-country trials. We showed that among industry- and non-industry–sponsored trials, 30% and 3% were multi-country, respectively. The collaboration within Eastern European countries was particularly over-represented for industry-sponsored research. Industry sponsors may thus have a greater capacity to conduct globalized research, using well-defined networks of countries. These projects are in-line with the development of a global observatory for health research.
In the Information Retrieval field, documents are usually considered as a "bag-of-words". This model does not take into account the temporal structure of the document and is sensitive to noises which can alter its lexical form. These noises can be produced by different sources: uncontrolled form of documents in microblogging platforms, automatic transcription of speech documents which are error prone, lexical and grammatical variabilities in Web forums... The work presented in this thesis addresses issues related to document representations from noisy sources. The thesis consists of three parts in which different representations of content are available. The first one compares a classical representation based on a term-frequency representation to a higher level representation based on a topic space. The abstraction of the document content allows us to limit the alteration of the noisy document by representing its content with a set of high-level features. Our experiments confirm that mapping a noisy document into a topic space allows us to improve the results obtained during different information retrieval tasks compared to a classical approach based on term frequency. The major problem with such a high-level representation is that it is based on a space theme whose parameters are chosen empirically. The second part presents a novel representation based on multiple topic spaces that allow us to solve three main problems: the closeness of the subjects discussed in the document, the tricky choice of the "right" values of the topic space parameters and the robustness of the topic-based representation. Based on the idea that a single representation of the contents cannot capture all the relevant information, we propose to increase the number of views on a single document. This multiplication of views generates "artificial"observations that contain fragments of useful information. The first experiment validated the multi-view approach to represent noisy texts. However, it has the disadvantage of being very large and redundant and of containing additional variability associated with the diversity of views. In the second step, we propose a method based on factor analysis to compact the different views and to obtain a new robust representation of low dimension which contains only the informative part of the document while the noisy variabilities are compensated. During a dialogue classification task, the compression process confirmed that this compact representation allows us to improve the robustness of noisy document representation. Nonetheless, during the learning process of topic spaces, the document is considered as a "bag-of-words" while many studies have showed that the word position in a document is useful. A representation which takes into account the temporal structure of the document based on hyper-complex numbers is proposed in the third part. This representation is based on the hyper-complex numbers of dimension four named quaternions. Our experiments on a classification task have showed the effectiveness of the proposed approach compared to a conventional "bag-of-words" representation.
This research belongs to the Natural Language Processing (NLP) field and more specifically focuses on topic Sub-sentential Alignment which is closely related to Machine Translation. The originality of this work consists in an example-based approach bootstrapped by the participation of non-expert annotators through an appropriate interface. Seeking for a greater expressivity, such as observed in manual alignments, mainly motivates the whole approach. An important effort has been made to define a formal environment for this original architecture based on aligned examples. A couple of new alignment methods were compared with state-of-the-art measures and three transformational metrics were introduced.
The purpose of this thesis is to propose a deep learning model in natural language processing (TALN) which would highlight the discriminating information of a court decision using a neural network of antagonistic networks type.
The i3 lab and the LGIPH, applies these approaches in various experimental setups. However, limitations have been observed when using conventional approaches for annotating gene expression signatures. The main objective of this thesis was to develop an alternative annotation approach to overcome this problem. The approach we have developed is based on the contextualization of genes and their products, and then biological pathways modeling to produce a knowledge base for the study of gene expression. We define a gene expression context as follows: cell population+ anatomical compartment+ pathological condition. We show here that it ensures better performance for text annotation the reference tool. To model the biological pathways we have developed, in collaboration with the LIPAH lab a modeling method based on a genetic algorithm that allows combining the results semantics proximity using the Biological Process ontology and the interactions data from db-string.
This thesis deals with automatic recommendation systems. Automatic recommendation systems are systems that allow, through data mining techniques, to recommend automatically to users, based on their past consumption, items that may interest them. These systems allow for example to increase sales on e-commerce websites: the Amazon site has a marketing strategy based mainly on the recommendation. The central contribution of this thesis is to analyze the automatic recommendation systems in the industrial context, including marketing needs, and to cross this analysis with academic works.
This paper is part of the thesis by Bruno Mery advised by Christian Bassac and Christian Retore in the years 2006-2011, on the topic "Modelling lexical semantics in a type-theoretic framework''. It is a doctoral thesis in computer science, in the area of natural language processing, aiming to bring forth a formal framework that takes into account, in the parsing of the semantics of a sentence, of lexical data. After a discussion of the topic, this thesis reviews the many works perceding it and adopts the tradition of the generative lexicon. It presents samples of data to account for, and gives a proposal for a calculus system based upon a second-order logic. It afterwards reviews the validity of this proposal, coming back to the data samples and the other formal approaches, and gives an implementation of that system. At last, it engages in a short discussion of the remaining questions.
The main purpose of this thesis is to show that the lexical information issuing from a language dictionary, as the Trésor de la langue française informatisé (TLFi), can improve the image indexing and retrieval process. The problem of using of such resource is that it is not sufficiently formalized to be exploited immediately in such application domain. To solve this problem, we propose a first approach of automatic construction of semantic hierarchies from TLFi. After defining a quantitative (measurable) and comparable characteristic of names appearing in dictionary definitions, through a weighting formula that allows us to select the name of the maximum weight as a good hypernym candidate for a given TLFi lexeme, we suggest an algorithm of automatic construction of semantic hierarchies for the lexemes of TLFi vocables. Once our approach is validated through manual evaluations, we demonstrate in the second time that the semantic hierarchies obtained from TLFi can be used to enrich a thesaurus manually built as well as for automatic image indexing using their associated text descriptions. We also prove that the use of such resource in the domain of image retrieval improves the accuracy of search by structuring the results according the domains to which the concepts of the search query are related to. The implementation of a prototype allowed us to evaluate and validate the proposed approaches.
Phishing is a kind of modern swindles that targets electronic communications users and aims to persuade them to perform actions for a another's benefit. Phishing attacks rely mostly on social engineering and that most phishing vectors leverage directing links represented by domain names and URLs, we introduce new solutions to cope with phishing. These solutions rely on the lexical and semantic analysis of the composition of domain names and URLs. Both of these resource pointers are created and obfuscated by phishers to trap their victims. Hence, we demonstrate in this document that phishing domain names and URLs present similarities in their lexical and semantic composition that are different form legitimate domain names and URLs composition. We use this characteristic to build models representing the composition of phishing URLs and domain names using machine learning techniques and natural language processing models. The built models are used for several applications such as the identification of phishing domain names and phishing URLs, the rating of phishing URLs and the prediction of domain names used in phishing attacks. All the introduced techniques are assessed on ground truth data and show their efficiency by meeting speed, coverage and reliability requirements.
Software networks have the potential to take the network infrastructure to a more advanced level, a level that can make the configuration autonomic. This ability can overcome the rapidly growing complexity of current networks, and allow management entities to enable an effective behavior in the network for overall performance improvement without any human intervention. Configuration parameters can be automatically selected for network resources to cope with various situations that networks encounter like errors and performance degradation. Unfortunately, some challenges need to be tackled to reach that advanced level of networks. Currently, the configuration is still often generated manually by domain experts in huge semi-structured files written in XML, JSON, and YAML. This is a complex, error-prone, and tedious task to do by humans. Also, there is no formal strategy except experience and best practices of domain experts to design the configuration files. Different experts may choose different configurations for the same performance goal. This situation makes it harder to extract features from the configuration files and learn models that could generate or recommend automatic configuration. In this thesis, we present our contributions that tackle the aforementioned challenges related to automating the configuration in software networks. To tack the problem of heterogeneity between the configuration files we propose a semantic framework based ontologies that can federate common elements from different configuration files.
In avionics engineering, integration tests are crucials: they allow to ensure the right behavior of an airplane before his first flight, they are needed to the certification process and they allow non-regression testing for each new version of a system, of a software or of a hardware. The design of an integration test is expensive because it involves the implementation of the procedure, the configuration of tools of the bench and the setup of the interfaces of the system under test. With procedure written in natural language, the interpretation of statements of a test during the manual execution can lead to mistakes that are expensive to fix due to accurate actions needed to perform a statement. The formalization and the automation of those procedures allow testers team to focus on the implementation of new test cases. First of all, we introduce Domain Specific Language (DSL) and show how we use it to formalize tests procedures dedicated to a kind of system. Those languages should be able to be use by avionic testers which do not necessarily have programming skills. They allow test automation, while maintaining test intention in the test description. Then, we proposed a BDD (Behavior Driven Development) approach to validate the integration of systems thanks to behavioral scenarios describing the expected behavior of the airplane. We named Domain Specific Test Languages (DSTL), languages used by expert testers. A first DSTL about the validation of airflow control systems has been developed as a proof of concept from existing procedures pseudo-formalized. The experimentation has been continued with IMA (Integrated Modular Avionic) calculators for which test procedures are written in natural language and thus are not automatable. From a corpus of procedures, we propose a first empirical process to identify sentence patterns composing the DSTL. The corpus provided is composed by ten procedures totaling 108 test chapters and 252 tests or subtests involving 3708 statements for a total of 250 Word pages. Make Agile integration tests in this context consist to propose a collaborative approach to formalize a DSTL and to integrate it in the orchestration framework to generate automatically the glu code.
This dissertation deals with the field of usable security, particularly in the contexts of online authentication and verifiable voting systems. The ever-expanding role of online accounts in our lives, from social networks to banking or online voting, has led to some initially counterproductive solutions. The problem is not just technical but has a very real psychosocial component, which is linked to password-based authentication, the subject of most of this thesis. Everyday, users face trade-offs between protecting their security and spending valuable mental resources, with a choice made harder by conflicting recommendations, a lack of standards, and the ad-hoc constraints still frequently encountered. We try to address those problems with solutions that are not only simplified for the user's sake but also for the developer's. To this end, we use tools from cryptography and psychology, and report on seven usability experiments. The first part of the contributions uses a service provider's point of view, with two tools to improve the end-user's experience without requiring their cooperation. We start by analysing how easily codes of different structures can be transcribed, with a proposal that reduces error rates while increasing speed. We then look at how servers can accept typos in passwords without changing the general hashing protocol, and how this could improve security. The second part focuses on end-users, starting by a proposed mental password manager that only depends on remembering only a single passphrase and PIN, with guarantees on the mutual security of generated passwords if some get stolen. In the third part, we focus on voting protocols, and investigate why changing the ones used in practice is an uphill battle. We try to answer a demand for simple paper-based systems by providing a set of low-tech primitives combined in a protocol that allows usable verifiable voting with no electronic means in small elections.
European legislation requires the assessment of reproductive toxicity of cosmetic ingredients without using tests on the organisms defined by the European directive on animals used for scientific purposes. To screen by excluding chemicals, cosmetics industry needs to develop an alternative method to animal testing, which needs to be predictive and specific in identifying teratogen agents (during embryonic development substances causing definitive physical and / or functional defects). For this, zebrafish, medaka fish and Xenopus amphibian at embryonic stages were assessed on a list of 43 reference substances. Medaka has been selected to the reliability of its supply, the robustness of its embryonic stages during handling, as well as for performance of its test. In addition, this test can detect the most potent teratogen agents, nearly half of the 26 teratogens of our list. Its specificity rate is set at 100% for correctly identifying 17 negative substances for which the absence of teratogenic effects is proved in humans or mammalian model organism. However, the performance of this test may be improved by its automation and integration of new quantification parameters for the identification of functional defects. Finally, to predict the safety of a chemical in humans, the predictive teratogenicity test using medaka embryos must be integrated into a comprehensive strategy for assessing the teratogenicity.
In manufacturing enterprises the Product Lifecycle Management (PLM) approach has been considered as an essential solution for improving the product competitive ability. It aims at providing a shared platform that brings together different enterprise systems at each stage of a product life cycle in or across enterprises. Although the main software companies are making efforts to create tools for offering a complete and integrated set of systems, most of them have not implemented all of the systems. Finally, they do not provide a coherent integration of the entire information system. This results in a kind of "tower of Babel", where each application is considered as an island in the middle of the ocean of information, managed by many stakeholders in an enterprise, or even in a network of enterprises. The different peculiarities of those stakeholders are then over increasing the issue of interoperability. The objective of this thesis is to deal with the issue of semantic interoperability, by proposing a formal semantic annotation method to support the mutual understanding of the semantics inside the shared and exchanged information in a PLM environment
This thesis deals with natural language processing and text mining, at the intersection of machine learning and statistics. We are particularly interested in Term Weighting Schemes (TWS) in the context of supervised learning and specifically the Text Classification (TC) task. In TC, the multi-label classification task has gained a lot of interest in recent years. Multi-label classification from textual data may be found in many modern applications such as news classification where the task is to find the categories that a newswire story belongs to (e.g., politics, middle east, oil), based on its textual content, music genre classification (e.g., jazz, pop, oldies, traditional pop) based on customer reviews, film classification (e.g. action, crime, drama), product classification (e.g. Electronics, Computers, Accessories). Traditional classification algorithms are generally binary classifiers, and they are not suited for the multi-label classification. The multi-label classification task is, therefore, transformed into multiple single-label binary tasks. However, this transformation introduces several issues. First, terms distributions are only considered in relevance to the positive and the negative categories (i.e., information on the correlations between terms and categories is lost). Second, it fails to consider any label dependency (i.e., information on existing correlations between classes is lost). Finally, since all categories but one are grouped into one category (the negative category), the newly created tasks are imbalanced. This information is commonly used by supervised TWS to improve the effectiveness of the classification system. Hence, after presenting the process of multi-label text classification, and more particularly the TWS, we make an empirical comparison of these methods applied to the multi-label text classification task. We find that the superiority of the supervised methods over the unsupervised methods is still not clear. We show then that these methods are not fully adapted to the multi-label classification problem and they ignore much statistical information that coul be used to improve the classification results. Thus, we propose a new TWS based on information gain. This new method takes into consideration the term distribution, not only regarding the positive and the negative categories but also in relevance to all classes. Finally, aiming at finding specialized TWS that also solve the issue of imbalanced tasks, we studied the benefits of using genetic programming for generating TWS for the text classification task. Unlike previous studies, we generate formulas by combining statistical information at a microscopic level (e.g., the number of documents that contain a specific term) instead of using complete TWS. Furthermore, we make use of categorical information such as (e.g., the number of categories where a term occurs). Experiments are made to measure the impact of these methods on the performance of the model. We show through these experiments that the results are positive.
At a time where data, often interpreted as "ground truth", are produced in gigantic quantities, a need for understanding and interpretability emerges in parallel. Dataset are nowadays mainly relational, therefore developping methods that allows relevant information extraction describing both objects and relation among them is a necessity. Association rules, along with their support and confidence metrics, describe co-occurrences of object features, hence explicitly express and evaluate any information contained in a dataset. In this thesis, we present and develop the relational concept analysis approach to extract the association rules that translate objects proper features along with the links with sets of objects. A first part present the mathematical part of the method, while a second part highlights three case studies to assess the pertinence of such a development. Case studies cover various domains to demonstrate the method polyvalence: the first case deals with error analysis in industrial production, the second covers psycholinguistics for dictionary analysis and the last one shows the method application in knowledge engineering.
The aim of this thesis is to minimize the performance loss of a one-class detection system when it encounters a data distribution change. The idea is to use transfer learning approach to transfer learned information from related old task to the new one. A multi-task learning model is proposed to solve the above problem; it uses one parameter to balance the amount of information brought by the old task versus the new task. This model is formalized so that it can be solved by classical one-class SVM except with a different kernel matrix. Experiments show that this model can give a smooth transition from the old detection system to the new one whenever it encounters a data distribution change. Moreover, as the proposed model can be solved by classical one-class SVM, online learning algorithms for one-class SVM are studied later in the purpose of getting a constant false alarm rate. It can be applied to the online learning of the proposed model directly
The expansion of the web and the development of different information technologies have contributed to the proliferation of digital documents online. The first problem is related to the extraction of the useful available information. A second problem concerns the use of this knowledge which sometimes results in plagiarism. The aim of this thesis is the development of a model that better characterizes documents to facilitate their access and also to detect those with a risk of plagiarism. This model is based on domain ontologies for the classification of documents and for calculating the similarity of documents belonging to the same domain as well. We are particularly interested in scientific papers, specifically their abstracts, short texts that are relatively well structured. The problem is, therefore, to determine how to assess the semantic proximity/similarity of two papers by examining their respective abstracts. Forasmuch as the domain ontology provides a useful way to represent knowledge relative to a given domain, our process is based on two actions:(i) An automatic classification of documents in a domain selected from several candidate domains. This classification determines the meaning of a document from the global context in which its content is used. The semantic comparison of the abstracts is based on a segmentation of their respective content into zones, documentary units, reflecting their logical structure.
Floating-point numbers represent only a subset of real numbers. As such, floating-point arithmetic introduces approximations that can compound and have a significant impact on numerical simulations. We introduce a new way to estimate and localize the sources of numerical error in an application and provide a reference implementation, the Shaman library. Our method uses a dedicated arithmetic over a type that encapsulates both the result the user would have had with the original computation and an approximation of its numerical error. We thus can measure the number of significant digits of any result or intermediate result in a simulation. We show that this approach, while simple, gives results competitive with state-of-the-art methods. It has a smaller overhead and is compatible with parallelism which makes it suitable for the study of large scale applications.
Robust semantic scene understanding is challenging due to complex object types, as well as environmental changes caused by varying illumination and weather conditions. This thesis studies the problem of deep semantic segmentation with multimodal image inputs. Multimodal images captured from various sensory modalities provide complementary information for complete scene understanding. We provided effective solutions for fully-supervised multimodal image segmentation and few-shot semantic segmentation of the outdoor road scene. Regarding the former case, we proposed a multi-level fusion network to integrate RGB and polarimetric images. A central fusion framework was also introduced to adaptively learn the joint representations of modality-specific features and reduce model uncertainty via statistical post-processing. In the case of semi-supervised semantic scene understanding, we first proposed a novel few-shot segmentation method based on the prototypical network, which employs multiscale feature enhancement and the attention mechanism. Comprehensive empirical evaluations on different benchmark datasets demonstrate that all the proposed algorithms achieve superior performance in terms of accuracy as well as demonstrating the effectiveness of complementary modalities for outdoor scene understanding for autonomous navigation.
This study aims to give a detailed description of evidentiality in English in contrast with Tibetan. It is based on a specialised corpus collected in Tibet and in England (TSC and CSC/LAC, 2010-2012, 10 h.). Tibetan has a complex and grammaticalised evidential system, and its description can provide a preliminary analytical grid for a semantic assessment of English evidentiality. Athentic examples and quantitative data from the corpus illustrate and supplement the analyses of the Tibetan verb phrase from previous research (Tournadre and Sangda Dorje 1998), in order to lay the foundation of the semantics of evidentiality. The evidential markers that emerge in the Tibetan and English sections of the corpus are examined so as to determine the parameters that motivate their usage. Tibetan evidentials are mainly grammatical and paradigmatised: copulas, verb suffixes and enclitics. English evidentials are either lexical or semi-grammatical: perception verbs, cognition verbs, speech verbs, modals, adverbs, conjuncts, parentheticals and discourse markers (Nuyts 2001a, Cappelli 2007, Sanders and Sweetser 2009, Mortensen 2010, Whitt 2010, Gisborne 2010, Miller 2008, Boulonnais 2010, Gurajek 2010, Kaltenböck et al. 2011, Heine 2013). This survey of Tibetan and English evidentiality provides precise data for the analysis of the consequences of a grammatical or a lexical rendering of this notion (Talmy 2000, Bybee et al. 1994, Nuyts 2001a, Boye and Harder 2009). Qualitative and quantitative evidence illustrates the differences in complexity, optionality, frequency, semantic restriction, speaker commitment, informative status and discourse strategy in the two systems. This study does not question that the Tibetan evidential system is more grammaticalised than the English one, but it shows that the latter presents all the signs of partial grammaticalisation. This reassessment leads to the conclusion that evidentiality is a relevant and necessary notion for a thorough linguistic description of English.
In this study, we will consider that the sentence is composed of a “nucleus plus affixes” and that it is above all a means of analysis as well as a production unit which are both temporary within the process of the productive and interpretative activity. We took special interest in the various functions performed by verbless segments in a spoken French corpus. A first subgroup includes verbless segments which work like sentences in so far as they express various levels of predicability, such as verbless predications, interjectional predications, or verbless realisations of an implicit verbal predication. Yet, this analysis does not seem to work when applied to many verbless segments called floating segments since they do not correspond to the canonical definitions of the sentence. We thus have to reconsider what a sentence is, especially when we tackle postponed complements, or verbless constituents which partake of the three following categories: integrated elements, detached elements and autonomous elements.
WMNs comprise nodes that are able to receive and forward the data to other destinations in the networks. Consequently, WMNs are able to dynamically self-organize and self-configure [5]. Each node itself creates and maintains the connectivity with its neighbors. The availability of ad-hoc mode on popular IEEE 802.11 allows low-cost implementation of WMNs. Nevertheless, WMNs have two major drawbacks: interference and scalability as discussed in [6]. (D1) Interference: (D2) Scalability: Meanwhile, the scalability of WMNs could be tackled by routing solutions [11]. Routing algorithms are responsible for computing routes so as to convey data through multiple hops until reaching the destinations. As shown in [11], the shortest-path routes, which are the default solutions of conventional routing algorithms, usually have more interference. The solution, subsequently, is finding other routes that have less interference. These routes could be optimal or sub-optimal with given objectives and arguments. The arguments of routing problems comprise of network-oriented metrics and User-oriented metrics. Network-oriented metrics, also called as Quality of Service (QoS) metrics, are derived from the network directly such as bandwidth, delay, jitter, etc. Meanwhile, User-oriented metrics, also called as Quality of Experience (QoE) metrics, are based on users' experience such as mean opinion score (MOS). The good perception of users is the major objective of video streaming services. Most of existing routing algorithms give routing decisions based on single or combination of network-oriented metrics. For example, the routing algorithms in [12, 13, 14] determine routes based on the bandwidth and congestion. Nevertheless, network-oriented metrics may not be well-correlated to users' experience [15, 16, 17, 18]. In other words, users may not be satisfied even with optimal network-oriented metric routes. As a result, it is necessary to develop routing algorithms that take user-oriented metrics into account. This thesis addresses the routing of video streaming over WMNs and proposes novel routing algorithms. These routing algorithms give routing decisions based on the perception of users. To do that, the proposed solution has to address two challenges as follows: (M1) estimate users' perception in real-time and (M2) find optimal or sub-optimal routes efficiently.
Abstract: Technological advancements made it possible for Electric vehicles (EVs) to have onboard computation, communication, storage, and sensing capabilities. Nevertheless, most of the time these EVs spend their time in parking lots, which makes onboard devices cruelly underutilized. Thus, a better management and pooling these underutilized resources together would be strongly recommended. The new aggregated resources would be useful for traffic safety applications, comfort related applications or can be used as a distributed data center. Moreover, parked vehicles might also be used as a service delivery platform to serve users. Therefore, the use of aggregated abundant resources for the deployment of different local mobile applications leads to the development of a new architecture called Vehicular Fog Computing (VFC). Through VFC, abundant resources of vehicles in the parking area, on the mall or in the airport, can act as fog nodes. In another context, mobile applications have become more popular, complex and resource intensive. Some sophisticated embedded applications require intensive computation capabilities and high-energy consumption that transcend the limited capabilities of mobile devices. Throughout this work, we tackle the problem of achieving an effective deployment of a VFC system by aggregating unused resources of parked EVs, which would be eventually used as fog nodes to serve nearby mobile users' computation demands. At first, we present a state of the art on EVs and resource allocation in VFC. In addition, we assess the potential of aggregated resources in EVs for serving local mobile users' applications demands by considering the battery State of Health (SOH) and State of Charge (SOC). Then, we address the problem of resource allocation scheme with a new solution based on Markov Decision Process (MDP) that aims to optimize the use of EVs energy for both computing users' demands and mobility. Finally, we propose a stochastic theoretical game approach to show the dynamics of both mobile users' computation demands and the availability of EVs resources.
Our research mainly focuses on tasks with an application purpose: depression and anorexia detection on the one hand and aggression detection on the other; this from messages posted by users on a social media platform. We have also proposed an unsupervised method of keyphrases extraction. More precisely, we improve an unsupervised graph-based method to solve the weaknesses of graph-based methods by combining existing solutions. We evaluated our approach on eleven data collections including five containing long documents, four containing short documents and finally two containing news articles. We have shown that our proposal improves the results in certain contexts. The second contribution of this thesis is to provide a solution for early depression and anorexia detection. We proposed models that use classical classifiers, namely logistic regression and random forest, based on: (a) features and (b) sentence embedding. We evaluated our models on the eRisk data collections. We have observed that feature-based models perform very well on precision-oriented measures both for depression or anorexia detection. The model based on sentence embedding is more efficient on ERDE_50 and recall-oriented measures. We also obtained better results compared to the state-of-the-art on precision and ERDE_50 for depression detection, and on precision and recall for anorexia detection. Our last contribution is to provide an approach for aggression detection in messages posted by users on social networks. We reused the same models used for depression or anorexia detection to create models. We added other models based on deep learning approach. We evaluated our models on the data collections of TRAC shared task. We observed that our models using deep learning provide better results than our models using classical classifiers. Our results in this part of the thesis are in the middle (fifth or ninth results) compared to the competitors. We still got the best result on one of the data collections.
In many statistical learning problems, it is necessary to be able to compare data that can be represented as probability measures or histograms. There are various examples that include word cloud analysis for natural language processing, computer vision, image classification or the study of bio-markers in bioinformatics. The use of the notion of Wasserstein distance associated with the optimal transport problem between probability measures has recently been a favored tool for the comparison of this type of data which makes it possible to reach state-of-the-art performances in many applications. A central problem in using Wasserstein distances in statistical learning is the cost of the numerical computation (or approximation) of the optimal transport between two probability measures. To circumvent this difficulty in using Wasserstein distances for data analysis, estimators based on stochastic gradient algorithms to evaluate a transport distance (possibly regularized) have recently been introduced. The topic of this thesis deals with the study of the properties of such stochastic optimization algorithms for optimal transport and their applications in statistical learning. We will focus in particular on the statistical and computational properties of this type of approach for the computation of barycenters and geodetic principal component analysis (PCA) in the Wasserstein space in a context of large data. The thesis subject covers both theoretical and numerical aspects. The main concepts involved in the thesis will use tools from statistics, probability and optimization with possible applications in data processing in bioinformatics.
This thesis lies in the fields of Public Economics and Political Economy and is articulated around two axes. The first and second chapters focus on redistributive policies. More precisely, they present contributions to the theory of income taxation, and adopt both a normative and a political economy perspective. The third and fourth chapters are grounded in political economy and contribute to providing a better understanding of political forces guiding reforms in multi-State unions such as the European Union. They focus on the preferences of individuals who are of great importance in decision-making processes but for whom little direct evidence is available: politicians. Both my first and second chapters seek to enrich a standard framework of optimal income taxation with new considerations, which allow to account better for both institutional and social contexts. Chapter 1 accounts for links between individuals when designing a redistribution system. Indeed, up until now income taxation problems have always dealt with redistribution between individuals, or couples, as separate entities. But in a lot of contexts, individuals are actually part of networks that may include their family, friends, village or community members, and make transfers to them on a regular basis. And this should matter to the government, because these transfers flow from richer to poorer individuals, and thus represent another form of redistribution, an informal one. My research question is then: how should the existence of such informal but redistributive transfers affect the design of income taxes? Chapter 2 starts from the observation that in many countries that have well-developed tax systems, the proportion of the population who does not pay the income tax is sizeable -- a salient feature of many tax systems in developed countries. Using tools from the normative taxation literature which we apply to a political economy of tax reforms framework, we manage to study the political economy of reforms of non-linear tax systems, which helps us understand why it might be the case that such important shares of the population are exempt from the income tax. Even though both chapters are theoretical, I use administrative and survey data to illustrate and draw concrete conclusions from my models. The aim of this project is to carry out these surveys on a regular basis so as to better understand European Union dynamics and help feed current EU debates with academic insights. The third and fourth chapters thus study the opinion of Members of Parliament regarding policies which hold particular importance in light of today's and the last ten years' debates: labour market policies, and the European Monetary Union. They provide a different take on political economy questions, and inform the preferences of an important set of actors in decision-making processes: politicians, for whom apart from their public votes and statements, little evidence is available on their opinions. In these chapters we seek to disentangle which of two factors is most important in the differences observed: cultural or ideological differences? Surprisingly, we find that for a majority of questions the answers reflect more of an ideological (left/right) divide than a French/German one. For instance, the creation of a common unemployment insurance and more labor market flexibility highlight a divide that pertains much more to ideology than to nationality. Such results may help shed light on potential directions that may be fruitful for more European integration.