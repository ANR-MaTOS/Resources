De nombreux domaines issus de l'industrie et des sciences appliquées ont été les témoins d'une révolution numérique.<sep> Cette dernière s'est accompagnée d'une croissance du volume des données, dont le traitement est devenu un défi technique.<sep> Dans ce contexte, la parcimonie est apparue comme un concept central en apprentissage statistique.<sep> Il est en effet naturel de vouloir exploiter les données disponibles via un nombre réduit de paramètres.<sep> Cette thèse se concentre sur une forme particulière et plus récente de parcimonie, nommée parcimonie structurée.<sep> Comme son nom l'indique, nous considérerons des situations où, au delà de la seule parcimonie, nous aurons également à disposition des connaissances a priori relatives à des propriétés structurelles du problème.<sep> L'objectif de cette thèse est d'analyser le concept de parcimonie structurée, en se basant sur des considérations statistiques, algorithmiques et appliquées.<sep> Nous commencerons par introduire une famille de normes structurées parcimonieuses dont les aspects statistiques sont étudiées en détail.<sep> Nous considérerons ensuite l'apprentissage de dictionnaires, où nous exploiterons les normes introduites précédemment dans un cadre de factorisation de matrices.<sep> Différents outils algorithmiques efficaces, tels que des méthodes proximales, seront alors proposés.<sep> Grâce à ces outils, nous illustrerons sur de nombreuses applications pourquoi la parcimonie structurée peut être bénéfique.<sep> Ces exemples contiennent des tâches de restauration en traitement de l'image, la modélisation hiérarchique de documents textuels, ou encore la prédiction de la taille d'objets à partir de signaux d'imagerie par résonance magnétique fonctionnelle.
Dans le secteur agroalimentaire, la caractérisation des gestes culinaires est considérée de plus en plus comme un levier d'innovation.<sep> En observant et en analysant la manière avec laquelle leurs produits sont appréhendés en cuisine par les usagers (consommateurs ou professionnels), les industriels peuvent en effet déceler des pistes d'amélioration ou des idées de nouveaux produits.<sep> Dans ce travail de recherche, nous proposons de nouvelles méthodes dédiées à la caractérisation des gestes culinaires.<sep> Leur caractère innovant repose sur le fait qu'elles relèvent d'une approche quantitative, et non d'une approche qualitative comme cela est usuellement le cas.<sep> Elles s'inspirent en partie de deux méthodes de caractérisation utilisées en analyse sensorielle : le tri libre et la Q-méthodologie binaire.<sep> Nous voyons comment des développements méthodologiques apportés à ces dernières permettent de caractériser des grands ensembles de stimuli.<sep> Ces développements sont liés à la fois à la procédure de recueil des données et à la procédure d'analyse statistique des données.<sep> En particulier, nous proposons plusieurs procédures statistiques permettant d'aborder des problématiques variées : l'analyse d'un ensemble de partitions contenant des données manquantes, la classification non supervisée de profils d'évaluations binaires basée sur la notion d'accord inter-évaluateurs, etc.<sep> Nous voyons ensuite que les deux méthodes de caractérisation quantitatives 'améliorées' sont applicables à des gestes culinaires.
L'idée ainsi défendue est que de tels outils permettraient l'automatisation partielle des processus d'analyse des besoins et de synthèse des concepts de solution.<sep> Le point de vue sur le développement conceptuel développé dans cette recherche est basé sur les méthodologies de développement systématique développées dans la littérature.<sep> L'évolution de ces méthodologies apporte une description précise des tâches à réaliser par l'équipe de conception afin de parvenir à un produit efficace et performant.<sep> Par conséquent, l'argument de cette thèse est qu'il est possible de créer des modèles informatiques de certaines de ces tâches afin d'assister l'équipe de conception dans la reformulation du problème de conception ainsi que dans l'exploration de l'espace de conception.<sep> En ingénierie des exigences, la définition des besoins consiste à identifier les besoins des divers intervenants puis à les formaliser en spécifications du produit.<sep> La difficulté est alors que ces personnes expriment leurs besoins avec des niveaux de clarté et des champs lexicaux différents.<sep> Ces travaux de recherche aborde la question du traitement des exigences exprimées en langage naturel (dans ce cas en anglais).<sep> L'analyse de ces besoins est réalisée à différents niveaux linguistiques : lexicaux, syntaxiques et sémantiques.<sep> Le niveau lexical traite de la signification des mots du langage.<sep> Le niveau sémantique est l'étude du sens précis d'utilsation des mots dans leur contexte.<sep> Cette recherche fait un usage intensif d'un atlas sémantique basé sur le concept mathématique de clique provenant de la théorie des graphes.<sep> Ce concept permet le calcul de distances entre un mot et ses synonymes.<sep> En outre, une méthodologie et une métrique de similitude ont été définies pour clarifier les exigences aux niveaux syntaxique, lexical et sémantique.<sep> Cette méthodologie intègre des outils de collaborateurs de recherche.<sep> Pour le processus de synthèse, une représentation numérique des concepts de connaissance nécessaires à la création de solutions de préconception a été développée.
Cette thèse propose d'investiguer certains des aspects de l'apprentissage auto-supervisé, une forme d'apprentissage non-supervisé qui permet à des modèles et agents d'apprendre sans supervision explicite.<sep> L'un de ses principaux avantages est le fait d'utiliser plus efficacement les données, en obtenant une performance supérieure ou égale à partir de moins de données étiquetées ou moins d'itérations dans l'environnement.<sep> De nombreux travaux de recherche contenant des méthodes d'apprentissage auto-supervisé ont vu le jour depuis quelques années, principalement dans le domaine de la vision par ordinateur, ainsi qu'en traitement du langage naturel.<sep> Cependant, les approches actuelles sont souvent limitées à des base de données simples ou à des environnements statiques.<sep> Ce projet visera donc à étudier et à développer des méthodes d'apprentissage auto-supervisé dans des environnements dynamiques (dont les caractéristiques évoluent au cours du temps), comme par exemple les jeux vidéos.<sep> Le premier axe de recherche de cette thèse sera d'étudier les méthodes d'exploration guidées par la curiosité dans des environnements dynamiques.<sep> Dans des travaux récents, des agents ont été développés de manière à n'utiliser aucun signal extérieur pour s'améliorer, mais au contraire, en se servant d'une motivation intrinsèque qui les encourage à explorer et donc, à se perfectionner.<sep> Ces stratégies guidées par la curiosité ont permis de résoudre de nombreuses tâches à récompenses éparses en entraînant des agents de manière auto-supervisée.<sep> Cependant, ces méthodes peinent encore à être généralisées à des environnements dynamiques, et sont rarement adaptables à des configurations réalistes.<sep> Dans un second temps, cette thèse se penchera sur l'exploration des techniques d'apprentissage auto-supervisé appliquées aux bases de données vidéos.<sep> Une des directions de recherche sera d'étudier le parallèle entre les bases de données vidéos et les environnements tri-dimensionnels (3D) : l'exploration d'un environnement 3D peut se voir comme la mise en place d'une stratégie d'échantillonnage pour un ensemble de vidéos.<sep> Ainsi, il existe une forte corrélation entre les méthodes développées pour ces deux types de problèmes, bien que cette connexion n'ait pas encore été exploitée.<sep> En effet, les problèmes concernant les bases de données vidéos sont souvent étudiées par le spectre de la vision par ordinateur, alors que les problèmes d'exploration dans des environnements virtuels sont traités du point de vue de l'apprentissage par renforcement.<sep> L'un des objectifs de cette thèse sera donc d'examiner et de transposer les méthodes propres à l'un et à l'autre de ces domaines.
L'apprentissage multi-label est un problème d'apprentissage supervisé où chaque instance peut être associée à plusieurs labels cibles simultanément.<sep> Il est omniprésent dans l'apprentissage automatique et apparaît naturellement dans de nombreuses applications du monde réel telles que la classification de documents, l'étiquetage automatique de musique et l'annotation d'images.<sep> Nous discutons d'abord pourquoi les algorithmes multi-label de l'etat-de-l'art utilisant un comité de modèle souffrent de certains inconvénients pratiques.<sep> Nous proposons ensuite une nouvelle stratégie pour construire et agréger les modèles ensemblistes multi-label basés sur k-labels.<sep> Nous analysons ensuite en profondeur l'effet de l'étape d'agrégation au sein des approches ensemblistes multi-label et étudions comment cette agrégation influece les performances de prédictive du modèle en fonction de la nature de fonction cout à optimiser.<sep> Nous abordons ensuite le problème spécifique de la selection de variables dans le contexte multi-label en se basant sur le paradigme ensembliste.<sep> Trois méthodes de sélection de caractéristiques multi-label basées sur le paradigme des forêts aléatoires sont proposées.<sep> Ces méthodes diffèrent dans la façon dont elles considèrent la dépendance entre les labels dans le processus de sélection des varibales.<sep> Enfin, nous étendons les problèmes de classification et de sélection de variables au cadre d'apprentissage semi-supervisé.<sep> Nous proposons une nouvelle approche de sélection de variables multi-label semi-supervisée basée sur le paradigme de l'ensemble.<sep> Le modèle proposé associe des principes issues de la co-training en conjonction avec une métrique interne d'évaluation d'importnance des varaibles basée sur les out-of-bag.<sep> Testés de manière satisfaisante sur plusieurs données de référence, les approches développées dans cette thèse sont prometteuses pour une variété d'applications dans l'apprentissage multi-label supervisé et semi-supervisé.
Dans cette thèse, nous proposons une méthode pour construire un outil d'aide à la décision pour le diagnostic de maladie rare.<sep> Nous cherchons à minimiser le nombre de tests médicaux nécessaires pour atteindre un état où l'incertitude concernant la maladie du patient est inférieure à un seuil prédéterminé.<sep> Ce faisant, nous tenons compte de la nécessité dans de nombreuses applications médicales, d'éviter autant que possible, tout diagnostic erroné.<sep> Pour résoudre cette tâche d'optimisation, nous étudions plusieurs algorithmes d'apprentissage par renforcement et les rendons opérationnels pour notre problème de très grande dimension.<sep> Pour cela nous décomposons le problème initial sous la forme de plusieurs sous-problèmes et montrons qu'il est possible de tirer partie des intersections entre ces sous-tâches pour accélérer l'apprentissage.<sep> Les stratégies apprises se révèlent bien plus performantes que des stratégies gloutonnes classiques.<sep> Nous présentons également une façon de combiner les connaissances d'experts, exprimées sous forme de probabilités conditionnelles, avec des données cliniques.<sep> Il s'agit d'un aspect crucial car la rareté des données pour les maladies rares empêche toute approche basée uniquement sur des données cliniques.<sep> Nous montrons, tant théoriquement qu'empiriquement, que l'estimateur que nous proposons est toujours plus performant que le meilleur des deux modèles (expert ou données) à une constante près.<sep> Enfin nous montrons qu'il est possible d'intégrer efficacement des raisonnements tenant compte du niveau de granularité des symptômes renseignés tout en restant dans le cadre probabiliste développé tout au long de ce travail.
Les effets indésirables médicamenteux (EIM) ont des répercussions considérables tant sur la santé que sur l'économie.<sep> De 1,9% à 2,3% des patients hospitalisés en sont victimes, et leur coût a récemment été estimé aux alentours de 400 millions d'euros pour la seule Allemagne.<sep> De plus, les EIM sont fréquemment la cause du retrait d'un médicament du marché, conduisant à des pertes pour l'industrie pharmaceutique se chiffrant parfois en millions d'euros.<sep> De multiples études suggèrent que des facteurs génétiques jouent un rôle non négligeable dans la réponse des patients à leur traitement.<sep> Cette réponse comprend non seulement les effets thérapeutiques attendus, mais aussi les effets secondaires potentiels.<sep> Pour ce faire, nous nous plaçons dans le cadre de l'apprentissage statistique multitâche, qui consiste à combiner les données disponibles pour plusieurs problèmes liés afin de les résoudre simultanément.<sep> Nous proposons un nouveau modèle linéaire de prédiction multitâche qui s'appuie sur des descripteurs des tâches pour sélectionner les variables pertinentes et améliorer les prédictions obtenues par les algorithmes de l'état de l'art.<sep> Enfin, nous étudions comment améliorer la stabilité des variables sélectionnées, afin d'obtenir des modèles interprétables.
Cette thèse se situe dans le domaine du Traitement Automatique des Langues et vise à optimiser la classification des documents dans les moteurs de recherche.<sep> Les travaux se concentrent sur le développement d'un outil de détection automatique des thèmes des documents (ATDS-fr).<sep> Utilisant peu de connaissances, la méthode hybride adoptée allie des techniques statistiques de segmentation thématique à des méthodes linguistiques identifiant des marqueurs de cohésion.
La reconnaissance automatique du locuteur indépendante du texte est une méthode récente dans le domaine des systèmes biométriques.<sep> Le développement de la reconnaissance du locuteur se reflète tout autant dans la participation croissante aux compétitions internationales et dans les progrès en termes de performance relevés dans ces campagnes.<sep> Cependant la précision des méthodes reste limitée par la quantité d'information discriminante du locuteur présente dans les représentations informatiques des énoncés vocaux.<sep> Cette thèse présente une étude sur ces représentations.<sep> Elle identifie deux faiblesses principales.<sep> Tout d'abord, les représentations usuelles ignorent les paramètres temporels de la voix pourtant connus pour leur pouvoir discriminant.<sep> Par ailleurs, ces représentations reposent sur le paradigme de l'apprentissage statistique et diminuent l'importance d'événements rares dans une population de locuteurs, mais fréquents dans un locuteur donné.<sep> Pour répondre à ces verrous, cette thèse propose une nouvelle représentation des énoncés.<sep> Celle-ci projette chaque vecteur acoustique dans un large espace binaire intrinsèquement discriminant du locuteur.<sep> Une mesure de similitude associée à une représentation globale (vecteurs cumulatifs) est également proposée.<sep> L'approche proposée permet ainsi à la fois de représenter des événements rares mais pertinents et de travailler sur des informations temporelles.<sep> Cette approche permet de tirer parti des solutions de compensation de la variabilité «  session  » , qui provient de l'ensemble des facteurs indésirables, exploitées dans les approches de type «  iVector  » .<sep> Dans ce domaine, des améliorations aux algorithmes de l'état de l'art ont été proposées.<sep> Une solution originale permettant d'exploiter l'information temporelle à l'intérieur de cette représentation binaire a été proposée.<sep> La complémentarité des sources d'information a été attestée par un gain en performance relevé grâce à une fusion linéaire des deux types d'information, indépendant et dépendant de la séquence temporelle.
Cette étude lexicologique prospective s'inscrit dans la didactique des L3.<sep> L'objectif est d'élaborer un interlexique anglais-espagnol-français-italien-portugais composé des adjectifs, noms et verbes anglais fréquents dans les écrits scientifiques de la santé, et de leurs équivalents de traduction analogues en espagnol, français, italien et portugais.<sep> Deux mots sont analogues s'ils ont le même sens et une forme similaire.<sep> Les rapports entre les concepts d'analogie, de similarité et d'identité sont examinés, les types d'analogies intralinguistiques et interlinguistiques illustrés et les principales analogies et dissemblances entre l'anglais, le français et les langues romanes exposées.<sep> L'existence de celles-ci est justifiée par les origines indoeuropéennes et surtout d'intenses contacts de langues.<sep> Après avoir rappelé l'importance de l'analogie dans l'apprentissage, nous montrons le lien entre notre recherche et deux types d'approches didactiques des langues : l'intercompréhension, qui développe la compréhension de langues voisines, et les approches sur corpus qui permettent de mieux connaitre et faire connaitre la phraséologie scientifique.<sep> Les 2000 lemmes anglais les plus fréquents ont été extraits du corpus scientifique anglais de ScienText, leurs 2208 acceptions fréquentes délimitées sur la base du profil combinatoire et triées en deux catégories sémantiques : lexique de spécialité et lexique scientifique transdisciplinaire.<sep> Les lemmes anglais ont été traduits dans les quatre langues romanes, et la similarité mesurée en fonction de la sous-chaine maximale commune (SMC).<sep> L'interlexique contient 47 % des acceptions fréquentes.<sep> Par couples de langues, l'analogie est encore plus élevée : anglais – français, 66 %, anglais-italien, 65 %, anglais-espagnol, 63 %, anglais-portugais, 58 %.<sep> Ce lexique analogue pourrait donc servir comme base de transfert dans des activités de FLE L3 pour des professionnels de la santé, et l'anglais L2 semble être une passerelle possible vers les langues romanes.<sep> Des activités plurilingues sont construites sur des concordances extraites des corpus multilingues alignés EMEA et Europarl.<sep> Un questionnement métalinguistique en anglais sensibilise à des traits (morpho)syntaxiques du français ; les analogies des deux langues sont systématiquement mises en relief, et dans les cas d'opacité, celles des autres langues romanes avec l'anglais.
Les systèmes de recherche d'information utilisent généralement une multitude de fonctionnalités pour classer les documents.<sep> Cette thèse aborde une limitation fondamentale des modèles de recherche d'information, à savoir le problème de la disparité des termes &lt ; Term Mismatch Problem&gt ; .<sep> Le problème de la disparité des termes est un problème de longue date dans la recherche d'informations.<sep> Cependant, le problème de la récurrence de la disparité des termes n'a pas bien été défini dans la recherche d'information, son importance, et à quel point cela affecterai les résultats de la recherche.<sep> Cette thèse tente de répondre aux problèmes présentés ci-dessus.<sep> Nos travaux de recherche sont rendus possibles par la définition formelle de la probabilité de la disparité des termes.<sep> Dans cette thèse, la disparité des termes est définie comme étant la probabilité d'un terme ne figurant pas dans un document pertinent pour la requête.<sep> De ce fait, cette thèse propose des approches pour réduire la probabilité de la disparité des termes.<sep> De plus, nous confortons nos proposions par une analyse quantitative de la probabilité de la disparité des termes qui décrit de quelle manière les approches proposées permettent de réduire la probabilité de la disparité des termes tout en conservant les performances du système.<sep> Au première niveau, nous proposons une approche de modification des documents en fonction de la requête de l'utilisateur.<sep> Il s'agit de traiter les termes de la requête qui n'apparaissent pas dans le document.<sep> Le modèle de document modifié est ensuite utilisé dans un modèle standard de recherche afin d'obtenir un modèle permettant de traiter explicitement la disparité des termes.<sep> Au second niveau, nous aproposons une approche d'expansion de requête sémantique basée sur une ressource collaborative.<sep> Nous concentrons plutôt sur la structure de ressources collaboratives afin d'obtenir des termes d'expansion intéressants qui contribuent à réduire la probabilité de la disparité des termes, et par conséquent, d'améliorer la qualité de la recherche.<sep> Deuxièmement, nous proposons un modèle d'expansion de requête basé sur les modèles de langue neuronaux.<sep> Les modèles de langue neuronaux sont proposés pour apprendre les représentations vectorielles des termes dans un espace latent, appelées &lt ; Distributed Neural Embeddings&gt ; .<sep> Cependant, nous proposons d'utiliser ces représentations vectorielles comme une ressource qui définit les relations entre les termes.<sep> Nous adaptons la définition de la probabilité de la disparité des termes pour chaque contribution ci-dessus.<sep> Nous décrivons comment nous utilisons des corpus standard avec des requêtes et des jugements de pertinence pour estimer la probabilité de la disparité des termes.<sep> Premièrement, nous estimons la probabilité de la disparité des termes à l'aide les documents et les requêtes originaux. Ainsi, nous présentons les différents cas de la disparité des termes clairement identifiée dans les systèmes de recherche pour les différents types de termes d'indexation.<sep> Ensuite, nous indiquons comment nos contributions réduisent la probabilité de la disparité des termes estimée et améliorent le rappel du système.<sep> Des directions de recherche prometteuses sont identifiées dans le domaine de la disparité des termes qui pourrait présenter éventuellement un impact significatif sur l'amélioration des scénarios de la recherche.
Critique épistémologique de la théorie linguistique dominante, ce travail s'appuie à la fois sur les retombées de la théorie de la valeur (K. Marx), sur les effets de la modernité et sur les conséquences des mutations technologiques contemporaines sur les formes du langage.<sep> Le premier volume présente notre modèle théorique, la sémioactivité.<sep> Opérant une distinction entre domaines, statuts énonciatifs et statuts discursifs, l'approche sémio-active réintégré, de manière critique, la théorie d'un G. Guillaume, la théorie de l'énonciation (E. Benveniste, A. Culioli, H. Adamczewski) et la praxematique (R. Lafont).<sep> Le deuxième volume qui intègre les ouvertures théoriques de l'école rouennaise de sociolinguistique (J. B. Marcellesi), étudie plus particulièrement la glottopolitique maghrébine, d'une part, et les retombées de la technologie contemporaine sur le langage, de l'autre.<sep> Un survol critique des expériences en linguistique informatique débouche sur des propositions de formalisation relayées par des algorithmes.
Notre recherche porte sur l'analyse linguistique du verbe français et ses correspondants dans la langue arabe, dans le cadre d'une approche contrastive et comparative, pour leur intégration dans le système informatique NooJ pour une application de traduction automatique.<sep> Dans la première partie, nous avons présenté les assises théoriques sur lesquelles nous avons fondé notre étude des caractéristiques sémantico-syntaxiques des classes verbales de mouvement et de communication et qui sont essentiellement la théorie des classes d'objets de Gaston Gross et la classification des verbes français de JD and FD-C.<sep> Selon ces deux théories déjà présentées dans cette partie, nous allons faire une comparaison entre les prédicats des deux classes avec leurs correspondants en langue arabe dont nous avons donné déjà ses caractéristiques linguistiques, dans le troisième chapitre de cette partie.<sep> Dans la deuxième partie, nous avons suivi une démarche analytique mais en même temps comparative entre les deux systèmes verbaux : français-arabe.<sep> Dans le premier chapitre, nous avons effectué une comparaison entre les modes et les temps dans les deux langues.<sep> Dans les deux derniers chapitres, nous avons étudié les spécificités sémantico-syntaxiques des deux classes de communication et de mouvement entrée/sortie dont le sujet est humain [N0hum] selon la classification des « verbes français (LVF) » de JD and FD-C.<sep> Par cette analyse sémantico-syntaxique, nous avons essayé de démonter les différents patrons syntaxiques des deux classes avec la précision pour chaque type syntaxique la nature sémantique et le type de ses arguments nécessaires sans oublier de donner pour chaque prédicat son équivalent en langue arabe et faire une comparaison entre la structure argumentale des verbes français et celle des verbes arabes, pour en ressortir enfin les ressemblances et les divergences entre les deux systèmes verbaux.<sep> Et enfin, dans la troisième partie, nous avons essayé de montrer toutes les étapes nécessaires pour l'élaboration de notre application de traduction automatique en utilisant la plate-forme NooJ.<sep> Dans cette étape, nous avons créé : <sep> 1/ Deux dictionnaires bilingues des verbes de communication et de mouvement.<sep> 2/ Des grammaires formelles pour la réalisation de la phase d'analyse et de reconnaissance des patrons syntaxiques des prédicats de la classe C et la classe E.<sep> 3/ Des grammaires de traduction des verbes à la forme affirmative ou négative aux temps convenables tout en précisant les règles de flexion et de l'accord en genre et en nombre selon les normes de la langue arabe.<sep> A la sortie de notre outil de traduction, nous trouvons des verbes traduits et fléchis en arabe et qui respectent les règles de la langue cible.
Les avancées dans l'analyse de l'expression différentielle de gènes ont suscité un vif intérêt pour l'étude d'ensembles de gènes présentant une similarité d'expression au cours d'une même condition expérimentale.<sep> Les approches classiques pour interpréter l'information biologique reposent sur l'utilisation de méthodes statistiques.<sep> Cependant, ces méthodes se focalisent sur les gènes les plus connus tout en générant des informations redondantes qui peuvent être éliminées en prenant en compte la structure des ressources de connaissances qui fournissent l'annotation.<sep> Au cours de cette thèse, nous avons exploré différentes méthodes permettant l'annotation d'ensembles de gènes.<sep> Premièrement, nous présentons les solutions visuelles développées pour faciliter l'interprétation des résultats d'annotation d'un ou plusieurs ensembles de gènes.<sep> Dans ce travail, nous avons développé un prototype de visualisation, appelé MOTVIS, qui explore l'annotation d'une collection d'ensembles des gènes.<sep> MOTVIS utilise ainsi une combinaison de deux vues inter-connectées : une arborescence qui fournit un aperçu global des données mais aussi des informations détaillées sur les ensembles de gènes, et une visualisation qui permet de se concentrer sur les termes d'annotation d'intérêt.<sep> La combinaison de ces deux visualisations a l'avantage de faciliter la compréhension des résultats biologiques lorsque des données complexes sont représentées.<sep> Deuxièmement, nous abordons les limitations des approches d'enrichissement statistique en proposant une méthode originale qui analyse l'impact d'utiliser différentes mesures de similarité sémantique pour annoter les ensembles de gènes.<sep> Pour évaluer l'impact de chaque mesure, nous avons considéré deux critères comme étant pertinents pour évaluer une annotation synthétique de qualité d'un ensemble de gènes : (i) le nombre de termes d'annotation doit être réduit considérablement tout en gardant un niveau suffisant de détail, et (ii) le nombre de gènes décrits par les termes sélectionnés doit être maximisé.<sep> Ainsi, neuf mesures de similarité sémantique ont été analysées pour trouver le meilleur compromis possible entre réduire le nombre de termes et maintenir un niveau suffisant de détails fournis par les termes choisis.<sep> Enfin, nous avons développé GSAn, un serveur web basé sur les développements précédents et dédié à l'annotation d'un ensemble de gènes a priori.<sep> GSAn intègre MOTVIS comme outil de visualisation pour présenter conjointement les termes représentatifs et les gènes de l'ensemble étudié.<sep> Nous avons comparé GSAn avec des outils d'enrichissement et avons montré que les résultats de GSAn constituent un bon compromis pour maximiser la couverture de gènes tout en minimisant le nombre de termes.<sep> Le dernier point exploré est une étape visant à étudier la faisabilité d'intégrer d'autres ressources dans GSAn.<sep> Nous avons ainsi intégré deux ressources, l'une décrivant les maladies humaines avec Disease Ontology (DO) et l'autre les voies métaboliques avec Reactome.<sep> Nous avons ainsi intégré deux ressources, l'une décrivant les maladies humaines avec Disease Ontology (DO) et l'autre les voies métaboliques avec Reactome.<sep> L'intégration a amélioré les résultats en couvrant d'avantage de gènes sans pour autant affecter de manière significative le nombre de termes impliqués.<sep> Ensuite, les termes GO ont été mis en correspondance avec les termes DO et Reactome, a priori et a posteriori des calculs effectués par GSAn.<sep> Nous avons montré qu'un processus de mise en correspondance appliqué a priori permettait d'obtenir un plus grand nombre d'inter-relations entre les deux ressources.
Ce travail est conçu dans le panorama de développement rapide de grandes bases de données qui rassemblent des ensembles de résultats expérimentaux sur l'organisation anatomo-fonctionnelle du cerveau humain à différentes échelles ; <sep> l'abondance d'informations demande un effort intra et interdisciplinaire pour les synthétiser de façon cohérente.<sep> Le but de cette thèse est de contribuer à cet effort de synthèse.<sep> Le travail suit deux chemins : intra disciplinaire pour relier et synthétiser les résultats produits par la communauté de l'imagerie cérébrale, avec une focalisation particulière sur les Réseaux de Repos et les Réseaux Cognitifs ; inter-disciplinaire pour relier l'organisation anatomo-fonctionnelle du cortex cérébral (résultats en imagerie cérébrale), et les expressions des gènes révélées par les bases de données publiées très récemment sur le transcriptome humain.<sep> Cette thèse est organisée en trois parties : <sep> dans Partie I nous étudions l'organisation anatomo-fonctionnelle du cortex à partir des études d'imagerie cérébrale.<sep> Dans la Partie II, nous étudions les liens entre l'expression corticale des gènes et l'organisation anatomo-fonctionnelle du cortex, à la fois en termes de similitude topographique et de congruence de fonction, en se focalisant en particulier sur le traitement de l'information et la mémorisation.<sep> Dans la Partie III, nous présentons une plate-forme pour intégrer dans une même représentation les données d'imagerie cérébrale et d'expression génétique.<sep> En perspective, nous montrons comment notre approche pourrait donner des nouveaux points de vu au débat sur les maladies neurodégénératives et psychiatriques, et sur les modelés des dynamiques corticales.
Les pathologies infiltrantes diffuses recensent un large groupe de désordres pulmonaires et nécessitent un suivi régulier en imagerie tomodensitométrique (TDM).<sep> Une évaluation quantitative est nécessaire pour établir la progression (régionale) de la maladie et/ou l'impact thérapeutique.<sep> Cela implique le développement d'outils automatiques de diagnostic assisté par ordinateur (DAO) pour la segmentation du tissu pathologique dans les images TDM, problème adressé comme classification de texture.<sep> Traditionnellement, une telle classification repose sur une analyse des caractéristiques texturales 2D dans les images TDM axiales selon des critères définis par l'utilisateur.<sep> Récemment, des techniques d'intelligence artificielle fondées sur l'apprentissage profond, notamment les réseaux neuronaux convolutionnels (CNN), ont démontré des performances meilleures pour résoudre des tâches visuelles.<sep> Toutefois, pour les architectures CNN « classiques » il a été prouvé que les performances étaient moins bonnes en classification de texture par rapport à la reconnaissance d'objets, en raison de la dimensionnalité intrinsèque élevée des données texturales.<sep> La classification, s'appliquant à l'ensemble du volume pulmonaire, atteint une précision moyenne de 84% (75.8% pour le tissu normal, 90% pour l'emphysème et la fibrose, 81.5% pour le verre dépoli)
La Génération Automatique de Texte (GAT) est le champ de recherche de la linguistique informatique qui étudie la possibilité d'attribuer à une machine la faculté de produire du texte intelligible.<sep> Dans ce mémoire, nous présentons une proposition de système de GAT reposant exclusivement sur des méthodes statistiques.<sep> Son originalité est d'exploiter un corpus en tant que ressource de formation de phrases.<sep> Cette méthode offre plusieurs avantages : elle simplifie l'implémentation d'un système de GAT en plusieurs langues et améliore les capacités d'adaptations d'un système de génération à un domaine sémantique particulier.<sep> Nous envisageons dans nos conclusions que cette méthode particulière de génération puisse ouvrir des voies d'investigations prometteuses sur la nature du processus de formation de phrases.
Cette thèse étudie la construction automatique de représentations sémantiques comme lambda-termes simplement typés.<sep> Nous introduisons la sémantique formelle et computationnelle et présentons une tentative de construction sémantique à l'aide d'outils d'inférence.<sep> Nous expliquons les limites de cette approche et en proposons une autre, basée sur un outil appelé Nessie.<sep> Cet outil construit des représentations sémantiques grâce à un lexique spécifiant le sens des mots et à un arbre guidant la construction.<sep> L'implantation de l'outil est présentée et ses résultats sont comparés à ceux du système évoqué plus haut.<sep> Nous appliquons ensuite Nessie à deux tâches différentes.<sep> D'abord, nous montrons comment il peut être utilisé (avec des outils d'inférence) pour étudier la sémantique du temps et de l'aspect des verbes polonais.<sep> Ensuite, nous présentons deux approches de la construction compositionnelle du sens de discours basées sur la théorie des types.<sep> Enfin, nous prouvons que notre approche de la construction sémantique est compatible avec une large gamme de formalismes syntaxiques : ceux reconnus par une grammaire catégorielle abstraite d'ordre 2.<sep> Une conséquence de cette caractérisation est que Nessie peut gérer une large gamme de grammaires inversibles et peut donc, théoriquement, être utilisé aussi bien en génération qu'en analyse.<sep> Nous concluons en discutant de la pertinence du lambda calcul simplement typé pour la sémantique des langues naturelles.
Cette thèse aborde le problème de la reconnaissance, de la modélisation et de la description des activités humaines.<sep> Nous décrivons nos résultats sur trois problèmes : (1) l'utilisation de l'apprentissage par transfert pour la reconnaissance visuelle simultanée d'objets et de leur état, (2) la reconnaissance d'actions de manipulation à partir de transitions d'états, et (3) l'interprétation d'une série d'actions et d'états comme les événements d'une histoire prédéfinie afin d'en construire une description narrative.<sep> Ces résultats ont été développés en utilisant les activités culinaires comme domaine expérimental.<sep> Nous commençons par reconnaître les ingrédients comme les tomates et la laitue et les ingrédients tranchés et coupés en dés pendant la préparation d'un repas.<sep> Nous adaptons l'architecture VGG afin d'apprendre conjointement les représentations des ingrédients et de leurs états selon une approche par transfert d'apprentissage.<sep> Nous modélisons les actions en tant que transformations d'état d'objets.<sep> Nous détectons ainsi les actions de manipulation en suivant les transformations des propriétés correspondantes des objets (état et type) dans la vidéo.<sep> L'évaluation expérimentale de cette approche est réalisée en se servant des jeux de données 50 salads et EPIC
Cette thèse étudie quelques préverbes et prépositions en bulgare (v/v-, iz/iz-) et en français (dans, hors de, en-/in-, ex-/é-) dans le cadre de la sémantique cognitive et formelle et plus particulièrement dans celui de la Grammaire Applicative et Cognitive (GAC).<sep> La formalisation à l'aide d'un schème quasi-topologique utilisant des lieux abstraits permet de construire des représentations adéquates et de dégager un invariant abstrait pour chaque unité étudiée.<sep> L'analyse aboutit à la construction d'une carte sémantique des significations d'intériorité et d'extériorité étudiées qui met en évidence les similarités, les différences et les oppositions entre les significations des différents préverbes et prépositions au sein d'une même langue ainsi qu'entre les deux langues, bulgare et français.<sep> Les résultats obtenus montrent que chaque langue construit ses propres représentations à partir de primitives invariantes et que, bien que les emplois des unités étudiées soient souvent très proches, il n'est pas possible d'établir des équivalences absolues entre significations.<sep> Cette étude contribue à argumenter en faveur de l'hypothèse anti-anti-relativiste de la GAC.
De nombreux problèmes en Apprentissage Statistique consistent à minimiser une fonction non convexe et non lisse définie sur un espace euclidien.<sep> Les algorithmes d'optimisation utilisés pour résoudre ce genre de problèmes ont été largement étudié pour des fonctions convexes et grandement utilisés en pratique.<sep> Cependant, l'accrudescence du nombre d'observation dans l'évaluation de ce risque empirique ajoutée à l'utilisation de fonctions de perte de plus en plus sophistiquées représentent des obstacles.<sep> Ces obstacles requièrent d'améliorer les algorithmes existants avec des mis à jour moins coûteuses, idéalement indépendantes du nombre d'observations, et d'en garantir le comportement théorique sous des hypothèses moins restrictives, telles que la non convexité de la fonction à optimiser.<sep> Dans ce manuscrit de thèse, nous nous intéressons à la minimisation de fonctions objectives pour des modèles à données latentes, ie, lorsque les données sont partiellement observées ce qui inclut le sens conventionnel des données manquantes mais est un terme plus général que cela.<sep> Dans une première partie, nous considérons la minimisation d'une fonction (possiblement) non convexe et non lisse en utilisant des mises à jour incrémentales et en ligne.<sep> Nous proposons et analysons plusieurs algorithmes à travers quelques applications.<sep> Dans une seconde partie, nous nous concentrons sur le problème de maximisation de vraisemblance non convexe en ayant recourt à l'algorithme EM et ses variantes stochastiques.<sep> Nous en analysons plusieurs versions rapides et moins coûteuses et nous proposons deux nouveaux algorithmes du type EM dans le but d'accélérer la convergence des paramètres estimés.
Extraire l'opinion publique en analysant les Big Social data a connu un essor considérable en raison de leur nature interactive, en temps réel.<sep> En effet, les données issues des réseaux sociaux sont étroitement liées à la vie personnelle que l'on peut utiliser pour accompagner les grands événements en suivant le comportement des personnes.<sep> C'est donc dans ce contexte que nous nous intéressons particulièrement aux méthodes d'analyse du Big data.<sep> La problématique qui se pose est que ces données sont tellement volumineuses et hétérogènes qu'elles en deviennent difficiles à gérer avec les outils classiques.<sep> La contribution principale de la thèse de doctorat est de proposer une approche d'analyse générique pour détecter de façon automatique des tendances d'opinion sur des sujets donnés à partir des réseaux sociaux.<sep> En effet, étant donné un très petit ensemble de hashtags annotés manuellement, l'approche proposée transfère l'information du sentiment connue des hashtags à des mots individuels.<sep> La ressource lexicale qui en résulte est un lexique de polarité à grande échelle dont l'efficacité est mesurée par rapport à différentes tâches de l'analyse de sentiment.<sep> La comparaison de notre méthode avec différents paradigmes dans la littérature confirme l'impact bénéfique de notre méthode dans la conception des systèmes d'analyse de sentiments très précis.<sep> En effet, notre modèle est capable d'atteindre une précision globale de 90,21%, dépassant largement les modèles de référence actuels sur l'analyse du sentiment des réseaux sociaux.
Dans cette thèse nous étudions différentes questions relatives à la mise en pratique de modèles d'apprentissage profond.<sep> En effet malgré les avancées prometteuses de ces algorithmes en vision par ordinateur, leur emploi dans certains cas d'usage réels reste difficile.<sep> Une première difficulté est, pour des tâches de classification d'images, de rassembler pour des milliers de catégories suffisamment de données d'entraînement pour chacune des classes.<sep> C'est pourquoi nous proposons deux nouvelles approches adaptées à ce scénario d'apprentissage, appelé &lt ; &lt ; classification zero-shot&gt ; &gt ; .<sep> L'utilisation d'information sémantique pour modéliser les classes permet de définir les modèles par description, par opposition à une modélisation à partir d'un ensemble d'exemples, et rend possible la modélisation sans donnée de référence.<sep> L'idée fondamentale du premier chapitre est d'obtenir une distribution d'attributs optimale grâce à l'apprentissage d'une métrique, capable à la fois de sélectionner et de transformer la distribution des données originales.<sep> Dans le chapitre suivant, contrairement aux approches standards de la littérature qui reposent sur l'apprentissage d'un espace d'intégration commun, nous proposons de générer des caractéristiques visuelles à partir d'un générateur conditionnel.<sep> Une fois générés ces exemples artificiels peuvent être utilisés conjointement avec des données réelles pour l'apprentissage d'un classifieur discriminant.<sep> Dans une seconde partie de ce manuscrit, nous abordons la question de l'intelligibilité des calculs pour les tâches de vision par ordinateur.<sep> En raison des nombreuses et complexes transformations des algorithmes profonds, il est difficile pour un utilisateur d'interpréter le résultat retourné.<sep> L'intelligibilité de la représentation permet à un utilisateur d'examiner sur quelle base l'inférence a été réalisée et ainsi d'accepter ou de rejeter la décision suivant sa connaissance et son expérience humaine.
Nous abordons, dans cette thèse, les méthodes de combinaison de systèmes de transcription de la parole à Large Vocabulaire.<sep> Notre étude se concentre sur l'attelage de systèmes de transcription hétérogènes dans l'objectif d'améliorer la qualité de la transcription à latence contrainte.<sep> Les systèmes statistiques sont affectés par les nombreuses variabilités qui caractérisent le signal de la parole. Un seul système n'est généralement pas capable de modéliser l'ensemble de ces variabilités.<sep> Les méthodes de combinaison proposées dans la littérature sont majoritairement appliquées a posteriori, dans une architecture de transcription multi-passes.<sep> Nous proposons ensuite, une amélioration efficacement généralisable basée sur le décodage guidé par sac de n-grammes, appelé BONG.<sep> Nous présentons différents modèles théoriques de l'architecture d'attelage et nous exposons un exemple d'implémentation en utilisant une architecture client/serveur distribuée.<sep> Nous présentons également, une adaptation de la combinaison ROVER applicable durant le processus de décodage via un processus d'alignement local suivi par un processus de vote basé sur la fréquence d'apparition des mots.
De nos jours, les Réseaux Sociaux sont omniprésents dans tous les aspects de la vie.<sep> Une fonctionnalité fondamentale de ces réseaux est la connexion entre les utilisateurs.<sep> Ces derniers sont engagés progressivement à contribuer en ajoutant leurs propres contenus.<sep> Ce domaine a conduit désormais à de nombreux travaux de recherche ces dernières années.<sep> L'un des problèmes principaux est la détection des communautés.<sep> Les travaux de recherche présentés dans ce mémoire se positionnent dans les thématiques de l'analyse sémantique des Réseaux Sociaux et de la génération des applications interactives personnalisées.<sep> Cette thèse propose une approche pour la détection des communautés d'intérêt dans les Réseaux Sociaux.<sep> Cette approche modélise les données sociales sous forme d'un profil utilisateur social représenté par un ontologie.<sep> Elle met en oeuvre une méthode pour l'Analyse des Sentiments basées sur les phénomènes de l'influence sociale et d'Homophilie.<sep> Cette génération est basée sur une approche de type MDA, indépendante du domaine d'application.<sep> De surcroît, cet ouvrage fait état d'une évaluation de nos propositions sur des données issues de Réseaux Sociaux réels.
Question Answering (QA) est un domaine de l'informatique qui se préoccupe de la construction d'un système qui peut répondre automatiquement à une question posée par un utilisateur en langage naturel.<sep> Les systèmes Question Answering utilisent principalement trois sources pour trouver une réponse : du texte libre (documents de bureau, pages web et livres), des services Web (services météorologiques, prix des actions, horaire, etc.) et bases de connaissances.<sep> Une base de connaissances est une collection structurée d'informations qui peuvent être consultées et interprétées facilement par des machines.<sep> Le sujet principal de cette thèse est d'étudier les systèmes de QA sur de telles bases de connaissances.<sep> Les systèmes de QA sur les bases de connaissances sont importants car ces bases contiennent de très grandes quantités d'informations et en même temps ces informations sont a priori très difficiles d'accès pour les utilisateurs.<sep> Les systèmes QA sur bases de connaissances sont utilisés dans différentes applications industrielles telles que Google Search, Siri, Alexa et Bing.<sep> Ces applications ont en commun d'interroger des bases de connaissances propriétaires.<sep> Cette thèse apporte plusieurs contributions dans le domaine des systèmes QA sur bases de connaissances.<sep> D'abord, nous présentons une étude complète des systèmes QA existants.<sep> Nous présentons une analyse détaillée et une comparaison de tous les systèmes QA qui ont été évalués sur un benchmark populaire appelé QALD, et nous ciblons les systèmes de QA évalués par rapport à deux autres benchmarks très populaires, à savoir WebQuestions et SimpleQuestions.<sep> L'analyse contient également une liste des défis importants dans ce domaine.<sep> Dans un deuxième temps, nous présentons un nouvel algorithme pour construire des systèmes de QA sur des bases de connaissances.<sep> Nous prouvons ces revendications en appliquant ce nouvel algorithme à 5 langues différentes (à savoir anglais, allemand, français, espagnol et italien), à 6 bases de connaissances différentes (à savoir Wikidata, DBpedia, Freebase, Scigraph, Dblp et MusicBrainz) et en comparant les performances sur les benchmarks populaires QALD et SimpleQuestions.<sep> Dans une troisième partie, nous montrons comment la réponse d'un système QA peut être présentée à un utilisateur.<sep> Les bases de connaissances contiennent beaucoup d'informations contextuelles qui peuvent être présentées avec la réponse elle-même.<sep> Ces informations sont par exemple des descriptions textuelles, des liens externes, des images et des vidéos.<sep> Nous montrons comment ces informations peuvent être présentées à l'utilisateur pour enrichir la réponse.<sep> La dernière partie décrit une architecture pour les systèmes QA qui permet de construire des systèmes QA de manière modulaire et collaborative.<sep> Cette approche permet de créer des systèmes QA distribués sur le Web et constitue une proposition pour standardiser le processus d'un système QA.<sep> Toutes les contributions de cette thèse sont intégrées dans une démonstration en ligne disponible sous www.wdaqua.eu/qa.
Grâce à l'évolution de la technologie à travers le Web, la documentation relative à la santé est de plus en plus abondante et accessible à tous, plus particulièrement aux patients, qui ont ainsi accès à une panoplie d'informations sanitaires.<sep> Malheureusement, la grande disponibilité de l'information médicale ne garantit pas sa bonne compréhension par le public visé, en l'occurrence les non-experts.<sep> Notre projet de thèse a pour objectif la création d'une ressource de simplification de textes médicaux, à partir d'une analyse syntaxico-sémantique des verbes dans quatre corpus médicaux en français qui se distinguent de par le degré d'expertise de leurs auteurs et celui des publics cibles.<sep> La ressource conçue contient 230 patrons syntaxicosémantiques des verbes (appelés pss), alignés avec leurs équivalents non spécialisés.<sep> La méthode semi-automatique d'analyse des verbes appliquée pour atteindre notre objectif est basée sur quatre tâches fondamentales : l'annotation syntaxique des corpus, réalisée grâce à l'analyseur syntaxique Cordial (Laurent, Dominique et al, 2009) ; l'annotation sémantique des arguments des verbes, à partir des catégories sémantiques de la version française de la terminologie médicale Snomed Internationale (Côté, 1996) ; l'acquisition des patrons syntactico-sémantiqueset l'analyse contrastive du fonctionnement des verbes dans les différents corpus.<sep> Les patrons syntaxico-sémantiques des verbes acquis au terme de ce processus subissent une évaluation (par trois équipes d'experts en médecine) qui débouche sur la sélection des candidats constituant la nomenclature de la ressource de simplification.<sep> Les pss sont ensuite alignés avec leurs correspondants non spécialisés, cet alignement débouche sur le création de la ressource de simplification, qui représente le résultat principal de notre travail de thèse.<sep> Une évaluation du rendement du contenu de la ressource a été effectuée avec deux groupes d'évaluateurs : des linguistes et des non-linguistes.<sep> Les résultats montrent que la simplification des pss permet de faciliter la compréhension du sens du verbe en emploi spécialisé, surtout lorsque un certains paramètres sont réunis.
Le principal enjeu de notre recherche est d'aboutir à un modèle intégratif fonctionnel pour l'analyse des verbes d'affect en français et en arabe.<sep> Nous avons choisi d'étudier quatre V_affect : deux verbes d'émotion (étonner et énerver en français et leurs équivalents [ʔadhaʃa], [ʕaɣḍaba] en arabe]) et deux verbes de sentiment (admirer et envier et leurs équivalents [ʔaʕʒaba] et [ħasada]) appartenant aux champs sémantiques de la surprise, la colère, l'admiration et la jalousie.<sep> Plus concrètement, l'analyse se situe : <sep> – au niveau sémantique et syntaxique : les dimensions sémantiques véhiculées par les collocatifs verbaux comme étonner tellement, énerver prodigieusement, en français, et [ʔaʕʒaba ʔiʕʒāban kabīran] (admirer admiration grand), [ɣaḍaba ɣaḍabaan ʃadīdan] (énerver colère sévère), en arabe, sont systématiquement reliées à la syntaxe (les constructions grammaticales récurrentes) (Hoey, 2005).<sep> – au niveau syntaxique et discursif : les emplois actifs, passifs, et pronominaux des V_affect sont étudiés dans la perspective des dynamiques informationnelles au sein de la phrase (Van Valin et LaPolla, 1997).<sep> D'un point de vue méthodologique, l'étude s'appuie sur une démarche quantitative et qualitative de la combinatoire verbale et privilégie la démarche contrastive.<sep> Elle est fondée sur le corpus journalistique français de la base de données EmoBase (projet Emolex 100 M de mots) et du corpus journalistique ArabiCorpus (137 M de mots).<sep> La thèse contribue ainsi à l'étude des valeurs sémantiques, du comportement syntaxique et discursif de la combinatoire des V_affect, en arabe et en français, ce qui permet de mieux structurer le champ lexical des affects par rapport à ce que proposent les études existantes en lexicologie.<sep> Les principaux résultats de l'étude peuvent être appliqués en didactique des langues, en traductologie et en traitement automatisé du lexique des affects dans les deux langues comparées.
L'arrivée des technologies de séquençage d'ADN à haut-débit a représenté une révolution dans le domaine de la génomique personnalisée, en raison de leur résolution et leur faible coût.<sep> Toutefois, ces nouvelles technologies présentent un taux d'erreur élevé, qui varie entre 0,1% et 1% pour les séquenceurs de seconde génération.<sep> Cette valeur est problématique dans le cadre de la recherche de variants de faible ratio allélique, comme ce qui est observé dans le cas des tumeurs hétérogènes.<sep> En effet, un tel taux d'erreur peut mener à des milliers de faux positifs.<sep> Chaque région de l'ADN étudié doit donc être séquencée plusieurs fois, et les variants sont alors filtrés en fonction de critères basés sur leur profondeur.<sep> Malgré ces filtres, le nombre d'artefacts reste important, montrant la limite des approches conventionnelles et indiquant que certains artefacts de séquençage ne sont pas aléatoires.<sep> Dans le cadre de cette thèse, nous avons développé un algorithme exact de recherche des motifs d'ADN dégénérés sur-représentés en amont des erreurs de séquençage non aléatoires et donc potentiellement liés à leur apparition.<sep> Cet algorithme a été mis en oeuvre dans un logiciel appelé DiNAMO, qui a été testé sur des données de séquençage issues des technologies IonTorrent et Illumina.<sep> Les résultats expérimentaux ont mis en évidence plusieurs motifs, spécifiques à chacune de ces deux technologies.<sep> Nous avons ensuite montré que la prise en compte de ces motifs dans l'analyse, réduisait considérablement le taux de faux positifs.<sep> DiNAMO peut donc être utilisé en aval de chaque analyse, comme un filtre supplémentaire permettant d'améliorer l'identification des variants, en particulier des variants à faible ratio allélique.
Ces dix dernières années, les séries télévisées sont devenues de plus en plus populaires.<sep> Par opposition aux séries TV classiques composées d'épisodes autosuffisants d'un point de vue narratif, les séries TV modernes développent des intrigues continues sur des dizaines d'épisodes successifs.<sep> Cependant, la continuité narrative des séries TV modernes entre directement en conflit avec les conditions usuelles de visionnage : en raison des technologies modernes de visionnage, les nouvelles saisons des séries TV sont regardées sur de courtes périodes de temps.<sep> Par conséquent, les spectateurs sur le point de visionner de nouvelles saisons sont largement désengagés de l'intrigue, à la fois d'un point de vue cognitif et affectif.<sep> Une telle situation fournit au résumé de vidéos des scénarios d'utilisation remarquablement réalistes, que nous détaillons dans le Chapitre 1.<sep> De plus, le résumé automatique de films, longtemps limité à la génération de bande-annonces à partir de descripteurs de bas niveau, trouve dans les séries TV une occasion inédite d'aborder dans des conditions bien définies ce qu'on appelle le fossé sémantique : le résumé de médias narratifs exige des approches orientées contenu, capables de jeter un pont entre des descripteurs de bas niveau et le niveau humain de compréhension.<sep> Nous passons en revue dans le Chapitre 2 les deux principales approches adoptées jusqu'ici pour aborder le problème du résumé automatique de films de fiction.<sep> Dans le Chapitre 4, nous utilisons l'analyse des réseaux sociaux comme une manière possible de modéliser l'intrigue des séries TV modernes : la dynamique narrative peut être adéquatement capturée par l'évolution dans le temps du réseau des personnages en interaction.<sep> Cependant, nous devons faire face ici au caractère séquentiel de la narration lorsque nous prenons des vues instantanées de l'état des relations entre personnages.<sep> Nous montrons que les approches classiques par fenêtrage temporel ne peuvent pas traiter convenablement ce cas, et nous détaillons notre propre méthode pour extraire des réseaux sociaux dynamiques dans les médias narratifs.<sep> Le Chapitre 5 est consacré à la génération finale de résumés orientés personnages,capables à la fois de refléter la dynamique de l'intrigue et de ré-engager émotionnellement les spectateurs dans la narration.<sep> Nous évaluons notre système en menant à une large échelle et dans des conditions réalistes une enquête auprès d'utilisateurs.
Les services de communication de nouvelle génération doivent pouvoir coopérer pour répondre à des besoins spécifiques tout en gardant leur autonomie.<sep> Ceci nécessite de maîtriser leurs architectures et de partager ces architectures au sein de l'entreprise.<sep> Des cadres architecturaux communs sont alors indispensables.<sep> Après avoir fait le point des travaux sur ce sujet dans les domaines télécoms, web et IT, et après avoir discuté des enjeux de la convergence des services télécoms, nous introduisons ici quatre angles de vue (métier, fonctionnel, technique et applicatif), ainsi qu'une méthodologie pour construire des vues de référence partagées au sein d'une entreprise et des vues d'architecture propres à chaque service.<sep> Nous illustrons cette démarche à l'aide d'exemples et montrons ses applications pour construire des offres de service, pour rationnaliser les services existants et pour réaliser une convergence entre différents services.
Les travaux présentés dans ce manuscrit portent sur l'utilisation des humains virtuels et, plus globalement, des technologies du jeu vidéo pour améliorer le soin aux personnes âgées atteintes de troubles cognitifs.<sep> Nos travaux s'articulent autour de deux cas d'utilisation des humains virtuels : comme non-soi et comme soi.<sep> Plus spécifiquement, nous avons conçu, implémenté et évalué un agent conversationnel animé, appelé LOUISE (LOvely User Interface for Servicing Elders), ayant pour but de servir d'interface utilisateur accessible dans les dipositifs de compensation cognitives à destination des personnes âgées attenteintes de troubles cognitifs, et un dispositif de thérapie psychologique par la réalité virtuelle pour traiter les conséquences des chutes, appelé Promenade virtuelle.<sep> Ces deux projets ont été menés suivant des principes de conception participative en living lab, implicant plusieurs parties prenantes dans le processus de conception (patients, aidants et professionnels de santé).
Le docking moléculaire est une tâche complexe, difficile à appréhender pour une personne seule.<sep> C'est pourquoi, nous nous proposons d'étudier la distribution cognitive des charges de travail à travers la collaboration.<sep> Cependant, elle a mis en évidence des conflits de coordination ainsi que des problématiques<sep> Suite à cette première étude, nous avons proposés une nouvelle configuration de travail associée à des métaphores de communication haptiques afin d'améliorer la communication et les interactions entre les différents collaborateurs.
Cette thèse s'inscrit dans les recherches actuelles sur les émotions et les réactions émotionnelles, sur la modélisation et la transformation de la parole, ainsi que sur l'interprétation musicale.<sep> Il semble que la capacité d'exprimer, de simuler et d'identifier des émotions, des humeurs, des intentions ou des attitudes, est fondamentale dans la communication humaine.<sep> La facilité avec laquelle nous comprenons l'état d'un personnage, à partir de la seule observation du comportement des acteurs et des sons qu'ils émettent, montre que cette source d'information est essentielle et, parfois même, suffisante dans nos relations sociales.<sep> Cette thèse s'inscrit dans ces deux derniers domaines et propose plusieurs contributions.<sep> D'un point de vue théorique, cette thèse propose une définition de l'expressivité, une définition de l'expressivité neutre, un nouveau mode de représentation de l'expressivité, ainsi qu'un ensemble de catégories expressives communes à la parole et à la musique.<sep> Elle situe l'expressivité parmi le recensement des niveaux d'information disponibles dans l'interprétation qui peut être vu comme un modèle de la performance artistique.<sep> Elle propose un modèle original de la parole et de ses constituants, ainsi qu'un nouveau modèle prosodique hiérarchique.<sep> D'un point de vue expérimental, cette thèse fournit un protocole pour l'acquisition de données expressives interprétées.<sep> Collatéralement, elle rend disponible trois corpus pour l'observation de l'expressivité.<sep> Elle fournit une nouvelle mesure statistique du degré d'articulation ainsi que plusieurs résultats d'analyses concernant l'influence de l'expressivité sur la parole.<sep> D'un point de vue technique, elle propose un algorithme de traitement du signal permettant la modification du degré d'articulation.<sep> Elle présente un système de gestion de corpus novateur qui est, d'ores et déjà, utilisé par d'autres applications du traitement automatique de la parole, nécessitant la manipulation de corpus.<sep> Elle montre l'établissement d'un réseau bayésien en tant que modèle génératif de paramètres de transformation dépendants du contexte.<sep> D'un point de vue technologique, un système expérimental de transformation, de haute qualité, de l'expressivité d'une phrase neutre, en français, synthétique ou enregistrée, a été produit.<sep> Enfin et surtout, d'un point de vue prospectif, cette thèse propose différentes pistes de recherche pour l'avenir, tant sur les plans théorique, expérimental, technique, que technologique.<sep> Parmi celles-ci, la confrontation des manifestations de l'expressivité dans les interprétations verbale et musicale semble être une voie prometteuse.
Le développement de systèmes d'extraction d'information spécifiques à un domaine reste néanmoins très coûteux, que ce soit en termes de spécifications détaillées des informations à extraire ou d'annotation d'une quantité suffisante de données pour entraîner des modèles d'apprentissage.<sep> Cette thèse s'inscrit dans un objectif de réduction du coût d'adaptation d'un système d'extraction d'information à un domaine spécifique.<sep> L'approche envisagée s'appuie sur deux axes : (1) la mise au point d'un modèle d'extraction d'information générique, indépendant du domaine, en s'appuyant sur l'amélioration de la généralité de cadres événementiels génériques par l'utilisation de plongements de mots (word embeddings) de type BERT ou ELMO et (2) l'étude des méthodes d'adaptation de ce modèle générique à un domaine particulier en exploitant peu d'exemples annotés, par exemple par supervision distante, par apprentissage actif ou en exploitant des techniques d'apprentissage par transfert.
Dans cette thèse, nous étudions le problème de l'analyse de signatures de pannes dans le domaine de la maintenance avionique, afin d'identifier les défaillances au sein d'équipements en panne et suggérer des actions correctives permettant de les réparer.<sep> La thèse a été réalisée dans le cadre d'une convention CIFRE entre Thales Research &amp ; Technology et l'Université Paris-Sud. Les motivations sont donc à la fois théoriques et industrielles.<sep> Une signature de panne devrait fournir toutes les informations nécessaires pour identifier, comprendre et réparer la panne.<sep> Pour comprendre le mécanisme la panne son identification doit donc être explicable.<sep> Nous proposons une approche à base d'ontologies pour modéliser le domaine d'étude, permettant une interprétation automatisée des tests techniques réalisés afin d'identifier les pannes et obtenir les actions correctives associées.<sep> Il s'agit d'une approche d'apprentissage de concepts permettant de découvrir des concepts représentant les signatures de pannes tout en fournissant des explications sur les choix de propositions de réparations.<sep> Comme les signatures ne sont pas connues a priori, un algorithme d'apprentissage automatique non supervisé approxime les définitions des concepts.<sep> Les signatures apprises sont fournies sous forme de définitions de la logique de description (DL) et ces définitions servent d'explications.<sep> Nous avons utilisé une perspective différente. Elle repose sur une construction bottom-up de l'ontologie.<sep> Le processus d'apprentissage est réalisé via un opérateur de raffinement appliqué sur l'espace des expressions de concepts et le processus est guidé par les données, c'est-à-dire les individus de l'ontologie.<sep> Ainsi, les notions de justifications, de concepts plus spécifiques et de raffinement de concepts ont été révisées et adaptées pour correspondre à nos besoins.<sep> L'approche a ensuite été appliquée au problème de la maintenance avionique. Un prototype a été implémenté et mis en œuvre au sein de Thales Avionics à titre de preuve de concept.
La connaissance de la couverture des territoires en terme d'occupation des sols est devenue un enjeu majeur du XXIème siècle.<sep> Que ce soit à l'échelle nationale ou à une échelle plus globale, les initiatives se multiplient pour proposer des cartographies d'occupation des sols qui répondent à des besoins propres à chacune.<sep> La télédétection spatiale a connu un essor conséquent avec la multiplication des capteurs optiques d'observation de la Terre disponibles et de leur diversité en terme de résolutions spectrale, spatiale et temporelle.<sep> Les satellites SPOT 6 et SPOT 7 sont dotés de capteurs optiques à très haute résolution spatiale, et acquièrent des images dans quatre bandes spectrales à très haute résolution après pan-sharpening (1,5m).<sep> L'IGN, à partir de ces acquisitions SPOT disponibles sur le pôle de données surfaces continentales THEIA, produit chaque année une couverture d'orthophotos sur l'ensemble du territoire français.<sep> La problématique de cartographie de l'occupation des sols automatique à partir d'images aériennes ou satellites occupe la communauté de télédétection depuis longtemps, par le biais de processus de classification supervisés, tels que les SVMs, ou les forêts aléatoires pour, entre autres, la vitesse d'exécution de ces derniers. Mais les résultats obtenus par ces méthodes n'ont pas encore permis une réelle automatisation, notamment en adéquation avec des spécifications existantes (erreurs encore trop importantes).<sep> Représentants principaux de cette famille d'apprentissage, les réseaux de neurones profonds ont un impact au quotidien, que ce soit au niveau académique en terme de recherche, au niveau sociétal, au travers des smartphones par exemple.<sep> L'apprentissage profond a fait ses preuves dans de nombreux domaines depuis le traitement naturel du langage, à la reconnaissance d'objets dans des images.<sep> En effet, pour afficher de telles performances dans tant de domaines, ces algorithmes nécessitent de gros volumes de données.<sep> C'est cette approche que nous proposons dans ces travaux de thèse, avec une volonté d'étudier cette problématique tout en se plaçant dans un cadre plus large à visée opérationnelle.<sep> Nous étudions les possibilités de transfert d'apprentissage qui offre beaucoup d'avantages en matière de charges de calcul et de jeu d'apprentissage.<sep> Enfin, l'exploitation jointe d'images aériennes et de réseaux de neurones profonds est étudiée, avec un accent mis sur la préparation des données d'apprentissage issues des bases de données géographiques de l'IGN qui présentent certains inconvénients
Les propriétés des composes commençant par une préposition sont ensuite décrites.<sep> Les unités sont alors classées automatiquement par rapport à leurs propriétés.<sep> L'analyse relationnelle des données (critère de Condorcet) permet ici de dégager une classification qui montre que ce sont des adverbes caractères par une corrélation positive entre syntaxe et sémantique d'une part, entre le degré de figement des composes et leur intégration à l'énoncé d'autre part.
La classification de séries temporelles a suscité beaucoup d'intérêt au cours des dernières années en raison de ces nombreuses applications.<sep> Des expériences approfondies montrent que notre méthode D-BoTSW surpassent de façon significative presque tous les classificateurs de référence comparés.<sep> Ensuite, nous proposons un nouvel algorithmebasé sur l'algorithme Learning Time Series Shapelets (LTS) que nous appelons Adversarially-Built Shapelets (ABS).<sep> En raison du manque de jeux de données labelisés, formatés et disponibles enligne, nous utilisons deux jeux de données appelés TiSeLaC et Brazilian-Amazon.
Les dernières années ont vu se développer les forums de santé en ligne, lieux virtuels d'échange d'informations consacrés au thème de la santé ou de la maladie.<sep> Considérés comme un média d'apprentissage informel, certains ont la particularité d'être indépendants des lieux de soins car modérés par des patients.<sep> Les échanges y sont plus authentiques, car moins soumis à la désirabilité sociale.<sep> Actuellement, au regard de leurs propriétés formatrices, l'autoformation paraît pertinente pour analyser le processus d'apprentissage sur les forums.<sep> Pour mieux comprendre le fonctionnement et l'intérêt de ces forums, ce travail vise à en identifier les circonstances et les conditions d'utilisation par les internautes, et à caractériser l'apprentissage au moyen de ces forums.<sep> Dans une approche rétrospective et qualitative, l'auteur a choisi des méthodes de recueil et d'analyse mixtes.<sep> En effet, l'analyse qualitative de 30 entretiens réalisés avec des internautes complète l'analyse quantitative de 1319 messages postés par 40 utilisateurs du même forum.<sep> Leur exploration porte sur le profil des utilisateurs, les raisons, le moment et la manière d'utilisation du forum, les contenus abordés, l'utilisation des informations échangées, et les formes et modes d'apprentissage.<sep> Ce média constitue pour les utilisateurs un outil complet, respectueux de leur rythme et de leurs besoins d'apprentissage.<sep> Les résultats relèvent des traces d'apprentissage intentionnel ainsi que de plusieurs niveaux d'apprentissage en simultané et cela en dépit de l'absence des soignants et du fait que le forum n'est pas spécifiquement conçu pour l'apprentissage.<sep> Ce constat soutient l'hypothèse qu'une partie des apprentissages est liée à son utilisation et qu'il est un moyen propice à l'autoformation.<sep> Il est complémentaire des programmes d'éducation thérapeutique du patient et pourraient se rapporter à l'éducation thérapeutique de suivi et de renforcement.<sep> Les résultats soulignent l'importance du développement de l'esprit critique vis-à-vis des informations en tant que compétence d'adaptation chez les patients.<sep> Ils invitent à réfléchir quant aux rôles et moyens des intervenants officiels des forums que sont les modérateurs dans ce processus d'évolution cognitive.
Cette thèse, présente des contributions à la résolution (sur les GPUs) de problèmes d'optimisations réels de grandes tailles.<sep> Les problèmes de tournées de véhicules (VRP) et ceux de localisation des hubs (HLP) sont traités.<sep> Un algorithme génétique (GA) parallèle sur GPU est proposé pour résoudre différentes variantes du HLP.<sep> Le GA adapte son codage, sa solution initiale, ses opérateurs génétiques et son implémentation à chacune des variantes traitées.<sep> Enfin, nous avons utilisé le GA pour résoudre le HLP avec des incertitudes sur les données.<sep> Les tests numériques montrent que les approches proposées exploitent efficacement la puissance de calcul du GPU et ont permis de résoudre de larges instances jusqu'à 6000 nœuds.
Les accidents nucléaires majeurs constituent des crises à grande échelle, susceptibles de contaminer de large territoires pour des décennies.<sep> Les habitants de ces territoires doivent alors acquérir de nouvelles connaissances et adapter leur mode de vie pour limiter l'impact sanitaire et sociétal des radiations.<sep> Si la France a développé un plan de gestion d'une telle situation, les stratégies de communication proposées laissent peu de place au dialogue entre les autorités et les citoyens ainsi qu'aux usages du Web social.<sep> Elle vise à examiner le potentiel des technologies de l'information et de la communication pour la mise en œuvre d'une communication de crise plus adaptée aux situations post-accidentelles nucléaires.<sep> Après l'accident nucléaire de Fukushima Daiichi, les mesures de la radioactivité ont fait l'objet de pratiques collaboratives de collecte et d'agrégation s'appuyant sur les médias sociaux.<sep> L'étude des usages de Twitter a révélé que les modalités de diffusion de ces mesures ne sont pas adaptées aux spécificités de la situation post-accidentelle nucléaire.<sep> Sur la base de ces résultats, nous avons proposé des recommandations et un prototype d'outil logiciel pour favoriser la réutilisabilité de mesures de la radioactivité ainsi partagées.<sep> Plusieurs méthodes sont présentées pour faciliter l'identification de ces connaissances et leur représentation formelle à l'aide des technologies du Web Sémantique.<sep> À partir de ces modèles, nous décrivons la conception et l'évaluation d'une application web, Ginkgo, visant à faciliter l'appropriation et le partage des connaissances en matière de radioprotection.
Dans le domaine du Traitement Automatique du Langage, plusieurs études ont été menées afin de regrouper des termes sémantiquement proches.<sep> Le problème linguistique sous-jacent est celui de la catégorisation.<sep> Nous montrons ce que la méthode distributionnelle et la justification inspirée par la sémiotique peuvent apporter à ce sujet.<sep> Nous reprenons l'idée harrissienne qui suppose que l'examen de mots et de leurs fonctionnements peut servir de base pour des travaux sur la sémantique de ces mots.
Les systèmes informatiques dédiés à la haute performance (HPC) se livrent à une course à la puissance de calcul avec une augmentation de leur taille, Cependant, cette augmentation entraîne des défaillances fréquentes qui peuvent réduire la disponibilité des systèmes HPC.<sep> Pour gérer ces défaillances et être capable de réduire leur influence sur les systèmes HPC, il est important de mettre en place des solutions permettant de comprendre les défaillances, voire de les prédire.<sep> En effet, les systèmes HPC produisent une grande quantité de données de supervision qui contiennent de nombreuses informations utiles à propos de leur état de fonctionnement.<sep> Cependant, l'analyse de ces données n'est pas facile à réaliser et peut être très fastidieuse car elles reflètent la complexité et la taille des systèmes HPC.<sep> Les travaux présentés dans cette thèse proposent d'utiliser des solutions d'apprentissage machine pour réaliser de manière automatisée cette analyse.<sep> De manière plus précise, cette thèse présente deux contributions principales : la première s'intéresse à la prédiction des surchauffes des processeurs dans les systèmes HPC, la deuxième se concentre sur l'analyse et la mise en évidence des relations entre les événements présents dans les journaux systèmes.<sep> Ces deux contributions sont évaluées sur des données réelles provenant d'un système HPC de grande taille utilisé en production.<sep> Pour prédire les surchauffes de processeur, nous proposons une solution qui utilise uniquement la température des processeurs.<sep> Elle repose sur l'analyse de la forme générale de la température avant un événement de surchauffe et sur l'apprentissage automatisé des corrélations entre cette forme et les événements de surchauffe grâce à un modèle d'apprentissage supervisé.<sep> L'utilisation de la forme générale des courbes et d'un modèle d'apprentissage supervisé permet l'apprentissage en utilisant des données de température avec une faible précision et en utilisant un nombre de cas de surchauffe restreint.<sep> L'évaluation de la solution montre qu'elle est capable de prédire plusieurs minutes en avance les surchauffes avec une précision et un rappel élevés.<sep> De plus, l'évaluation de ces résultats montre qu'il est possible d'utiliser des actions préventives reposant sur les prédictions faites par la solution pour réduire l'influence des surchauffes sur le système.<sep> Pour analyser et mettre en évidence de manière automatisée les relations causales entre dans les événements décrits dans les journaux des systèmes HPC, nous proposons une utilisation détournée d'un modèle d'apprentissage machine profond.<sep> En effet, ce type de modèle est classiquement utilisé pour des tâches de prédiction.<sep> Grâce à l'ajout d'une nouvelle couche proposée par des travaux de l'état de l'art étudiant les méthodes d'apprentissage machine, il est possible de déterminer l'importance des entrées de l'algorithme dans sa prédiction.<sep> En utilisant les informations sur l'importance des entrées, nous sommes capables de reconstruire les relations entre les différents événements.<sep> L'évaluation de la solution montre qu'elle est capable de mettre en évidence les relations de la grande majorité des événements survenant sur un système HPC.<sep> De plus, son évaluation par des administrateurs montre la validité des corrélations mises en évidence.<sep> Les deux contributions et leurs évaluations montrent le bénéfice de l'utilisation de solutions d'apprentissage machine pour la compréhension et la prédiction des défaillances dans les systèmes HPC en automatisant l'analyse des données de supervision.
Au cours des dernières années les progrès technologiques permettant de collecter, stocker et traiter d'importantes quantités de données pour un faible coût, ont soulevés de sérieux problèmes concernant la vie privée.<sep> La protection de la vie privée concerne de nombreux domaines, en particulier les sites internet fréquemment utilisés comme les moteurs de recherche (ex. : Google, Bing, Yahoo !).<sep> Ces services permettent aux utilisateurs de retrouver efficacement du contenu sur Internet en exploitant leurs données personnelles.<sep> Dans ce contexte, développer des solutions pour permettre aux utilisateurs d'utiliser ces moteurs de recherche tout en protégeant leurs vies privées est devenu primordial.<sep> Dans cette thèse, nous introduirons SimAttack, une attaque contre les solutions protégeant la vie privée de l'utilisateur dans ses interactions avec les moteurs de recherche.<sep> Cette attaque vise à retrouver les requêtes initialement envoyées par l'utilisateur.<sep> Nous avons montré avec cette attaque que trois mécanismes représentatifs de l'état de l'art ne sont pas satisfaisants pour protéger la vie privée des utilisateurs.<sep> Par conséquent, nous avons développé PEAS, un nouveau mécanisme de protection qui améliore la protection de la vie privée de l'utilisateur.<sep> Cette solution repose sur deux types de protection : cacher l'identité de l'utilisateur (par une succession de deux serveurs) et masquer sa requête (en la combinant avec des fausses requêtes).<sep> Afin de générer des fausses requêtes réalistes, PEAS se base sur les précédentes requêtes envoyées par les utilisateurs du système.<sep> Pour finir, nous présenterons des mécanismes permettant d'identifier la sensibilité des requêtes.<sep> Notre objectif est d'adapter les mécanismes de protection existants pour protéger uniquement les requêtes sensibles, et ainsi économiser des ressources (ex. : CPU, mémoire vive).<sep> Nous avons développé deux modules pour identifier les requêtes sensibles.<sep> En déployant ces modules sur des mécanismes de protection existants, nous avons établi qu'ils permettent d'améliorer considérablement leurs performances.
Nous nous interrogeons dans cette thèse sur les réalisations linguistiques des relations de discours en considérant la relation d'<sep> La relation d'Élaboration a été jusqu'alors peu étudiée.<sep> On peut expliquer ce désintérêt pour cette relation par le fait que celle-ci n'a pas de marqueur prototypique.<sep> Pourtant cette relation est très présente dans les textes et occupe une place centrale dans les structures du discours.<sep> Notre objectif est double.<sep> Nous proposons des descriptions linguistiques de marqueurs, d'indices et de configurations d'indices pouvant enrichir un modèle du discours et permettre une identification automatique ou semi-automatique de la relation.<sep> Les premiers enjeux sont essentiellement empiriques.<sep> Nous souhaitons faire progresser des analyses descriptives sur cette relation.<sep> Les seconds enjeux sont plus applicatifs.<sep> Repérer automatiquement les relations de discours constitue un enjeu considérable pour des applications de traitement automatique des langues.
Ce travail s'intéresse à l'application de la phonétique expérimentale [acoustique et perceptive] à la didactique de la prononciation des langues étrangères.<sep> Le propos est illustré par les difficultés d'apprentissage par des japonophones des voyelles du français ; <sep> les expériences portent spécifiquement sur les voyelles 'u y ø'.<sep> Le but est d'élucider les difficultés que présentent ces phones selon que leur statut phonémique et leur réalisation phonétique diffèrent ou non entre la langue maternelle et la langue apprise.<sep> Le 'u' français diffère phonétiquement de son équivalent phonémique, le 'u' japonais.<sep> L'étude confirme que le 'u' français, phonémiquement « similaire » au 'u' japonais, est plus difficile que la voyelle « nouvelle » 'y', qui n'a pas d'équivalent ni phonémique ni phonétique en japonais.<sep> La production du 'ø', qui est « nouveau » phonémiquement mais proche du 'u' japonais au plan acoustique, semble présenter encore moins de difficulté.<sep> La thèse apporte également une réflexion sur la didactique de la prononciation.<sep> L'analyse de manuels généralistes de français publiés au Japon suggère que les apprenants et les enseignants sont rarement conscients de la différence de difficultés des 'u y ø'.<sep> Quelques méthodes d'enseignement de la prononciation – certaines traditionnelles, d'autres innovantes – sont proposées, dans l'idée de favoriser la conscientisation de ces difficultés.<sep> Le but de cette thèse est une contribution à l'éclaircissement des processus d'apprentissage de la prononciation des langues étrangères, et à l'amélioration de son apprentissage et de son enseignement.
L'opération neOCampus, initiée en 2013 par l'Université Paul Sabatier, a pour objectif de créer un campus connecté, innovant, intelligent et durable en exploitant les compétences de 11 laboratoires et de plusieurs partenaires industriels.<sep> Pluridisciplinaires, ces compétences sont croisées dans le but d'améliorer le confort au quotidien des usagers du campus (étudiants, corps enseignant, personnel administratif) et de diminuer son empreinte écologique.<sep> L'intelligence que nous souhaitons apporter au Campus du futur exige de fournir à ses bâtiments une perception de son activité interne.<sep> En effet, l'optimisation des ressources énergétiques nécessite une caractérisation des activités des usagers afin que le bâtiment puisse s'y adapter automatiquement.<sep> L'activité humaine étant sujet à plusieurs niveaux d'interprétation nos travaux se focalisent sur l'extraction des déplacements des personnes présentes, sa composante la plus élémentaire.<sep> La caractérisation de l'activité des usagers, en termes de déplacements, exploite des données extraites de caméras et de microphones disséminés dans une pièce, ces derniers formant ainsi un réseau épars de capteurs hétérogènes.<sep> Nous cherchons alors à extraire de ces données une signature audiovisuelle et une localisation grossière des personnes transitant dans ce réseau de capteurs.<sep> Tout en préservant la vie privée de l'individu, la signature doit être discriminante, afin de distinguer les personnes entre elles, et compacte, afin d'optimiser les temps de traitement et permettre au bâtiment de s'auto-adapter.<sep> Eu égard à ces contraintes, les caractéristiques que nous modélisons sont le timbre de la voix du locuteur, et son apparence vestimentaire en termes de distribution colorimétrique.<sep> Les contributions scientifiques de ces travaux s'inscrivent ainsi au croisement des communautés parole et vision, en introduisant des méthodes de fusion de signatures sonores et visuelles d'individus.<sep> Pour réaliser cette fusion, des nouveaux indices de localisation de source sonore ainsi qu'une adaptation audiovisuelle d'une méthode de suivi multi-cibles ont été introduits, représentant les contributions principales de ces travaux.<sep> Les modalités sonores et visuelles ne présentant aucune corrélation, deux signatures, une vidéo et une audio sont générées séparément, à l'aide de méthodes préexistantes de la littérature.<sep> Le détail de la génération de ces signatures est l'objet du chapitre 2.<sep> La cohérence spatio-temporelle des observations sonores et visuelles est ensuite traitée dans le chapitre 4, dans un contexte de suivi multi-cibles.
Parler du corps féminin dans l'œuvre de Assia Djebar, tout en dépassant le clivage chair/âme ou corps/personnalité, signifie l'inscrire dans une vision unitaire, dans une durée et un espace élargis et totalisants.<sep> Nous avons voulu insister sur sa continuité, sa résistance et même la survie de l'identité, malgré les facteurs ou les contextes qui l'ont mis à mal.<sep> Avant de conférer l'unité perdue au corps féminin, nous avons essayé de définir les termes clé de corps et de personnalité grâce aux sciences humaines, tout en tenant compte de leur spécificité liée à l'identité arabo-musulmane, aux particularités berbères et à l'influence française.<sep> Ce point de départ multiple nous a permis de ne pas tomber dans les catégorisations classiques, strictement sociales, de la femme algérienne.<sep> Mais celui-ci dépasse les apparences car, prise en charge par le langage et l'imaginaire, il conduit à la manifestation de la dimension réflexive.<sep> Le personnage féminin djebarien passe du stade « avoir un corps » à celui d' « être un corps » doté de plusieurs dimensions, physique, psychique, intellectuelle, langagière et imaginaire (I).<sep> Mais cette image corporelle unie et heureuse est confrontée à des époques moins favorables qui sont apparues à cause de l'éloignement de la doctrine islamique initiale, telle qu'elle est présentée dans Loin de Médine, de la valorisation de certains concepts comme l'honneur, la pudeur, la honte.<sep> Confronté à l'autorité masculine qui s'exerce sur la femme algérienne dans tous les moments de la vie, et qui se traduit par l'enfermement, l'humiliation, l'assignation à certains rôles très bien définis (comme celui de mère et d'épouse), les ordres, les coups, les insultes, etc., ce corps féminin développe une « micropsychologie » (M. Maffesoli) qui se transmet de génération en génération et qui offre des réponses toutes faites à des situations diverses.<sep> Tous les gestes en sont imprégnés, mais cela n'empêche pas le réveil et le surgissement des traces cachées de la personnalité féminine dans des contextes très particuliers.<sep> Ces traces mettront en lumière la ruse, le défi et même la haine de la femme lancés à l'homme, désigné déjà dans l'imaginaire féminin algérien par le terme de « e'dou » (ennemi).<sep> Ces sentiments révèlent donc la résistance du corps féminin, faite à la fois d'une révolte muette, de cris ravalés, de murmures, d'une écoute attentive, d'un besoin de partager et de se soutenir (II).<sep> Nous avons donc devant nos yeux un corps morcelé, qui a oublié ses qualités à cause de l'intériorisation des prisons symboliques.<sep> Mais grâce à la solidarité féminine, à la valorisation de la maison vue comme espace cocon et des relations entre femmes, au retour à la langue première, les traces de liberté et de plénitude du passé éloigné sont réactualisées par les gestes et les paroles de certaines femmes libres.<sep> Celles-ci ouvrent la voie de la libération du corps féminin algérien qui réapprendra à regarder, à marcher dehors, à raconter ses souvenirs, à parler de lui et à apprécier la présence de l'homme aimé (III).<sep> L'analyse des parties corporelles visibles, de la posture féminine, des gestes dans lesquels la tradition s'est inscrite, des réactions qui dévoilent à la fois la dimension corporelle et psychique, des termes utilisés par Djebar pour parler de ses personnages féminins, nous a permis de dévoiler un corps féminin doté d'un cœur, de souvenirs, de sentiments, une personnalité et des rôles qui sortent du cadre imposé par la société.<sep> Ce corps féminin, capable de faire des gestes qui l'inscrivent dans la durée et dans l'espace reconquis, acquiert une parole performative qui le recrée et lui donne la possibilité de s'accomplir.
Cette thèse a pour objet le traitement automatique du phénomène elliptique.<sep> À la croisée de plusieurs disciplines – linguistique théorique, linguistique de corpus, linguistique outillée et traductologie –, elle s'inscrit dans une démarche expérimentale en poursuivant deux objectifs essentiels.<sep> Il s'agit tout d'abord de vérifier la possibilité de détecter automatiquement le phénomène elliptique en anglais pour explorer ensuite les procédures facilitant sa traduction automatique de l'anglais vers le français.<sep> La détection automatique repose sur des analyses morphosyntaxiques qui paraissent suffisantes à la détection automatique de certaines catégories d'ellipse, puisqu'en décomposant le phénomène, elles permettent de l'identifier parmi d'autres.<sep> Un corpus parallèle et multi-genres, collecté et conçu pour répondre aux hypothèses de recherche, est utilisé.<sep> Afin d'élaborer des patrons de détection et exploiter le corpus, cette recherche utilise les outils CoreNLP développés à l'université de Stanford (USA) et met en lumière leurs limites lorsqu'ils sont confrontés à l'ellipse.<sep> Les résultats obtenus s'articulent autour du lien établi entre la détection et la traduction automatiques du phénomène elliptique, facteur déterminant dans la compréhension des erreurs de traduction générées lors de son traitement automatique.
La large diffusion des technologies du Web Sémantique telles que le Resource Description Framework (RDF) permet aux individus de construire leurs bases de données sur le Web, d'écrire des vocabulaires et de définir des règles pour organiser et expliquer les relations entre les données selon les principes des données liées.<sep> En conséquence, une grande quantité de données structurées et interconnectées est générée quotidiennement.<sep> Un examen attentif de la qualité de ces données pourrait s'avérer très critique, surtout si d'importantes recherches et décisions professionnelles en dépendent.<sep> La qualité des données liées est un aspect important pour indiquer leur aptitude à être utilisées dans des applications.<sep> Plusieurs dimensions permettant d'évaluer la qualité des données liées sont identifiées, telles que la précision, la complétude, la provenance et la concision.<sep> Cette thèse se concentre sur l'évaluation de la complétude et l'amélioration de la concision des données liées.<sep> En particulier, nous avons d'abord proposé une approche de calcul de complétude fondée sur un schéma généré.<sep> En effet, comme un schéma de référence est nécessaire pour évaluer la complétude, nous avons proposé une approche fondée sur la fouille de données pour obtenir un schéma approprié (c.-à<sep> Nous avons également proposé une approche pour découvrir des prédicats équivalents afin d'améliorer la concision des données liées.<sep> Cette approche s'appuie, en plus d'une analyse statistique, sur une analyse sémantique approfondie des données et sur des algorithmes d'apprentissage.<sep> Nous soutenons que l'étude de la signification des prédicats peut aider à améliorer l'exactitude des résultats.<sep> Enfin, un ensemble d'expériences a été mené sur des ensembles de données réelles afin d'évaluer les approches que nous proposons.
Les solutions cognitives, notamment les agents conversationnels, se basent sur des données textuelles non-structurées.<sep> La conception de ces solutions est ralentie par l'absence d'outils d'annotation et d'interprétation sémantique.<sep> Nous envisageons d'employer le clustering pour regrouper automatiquement les données et ainsi permettre d'accélérer la labellisation des données.<sep> Toutefois, le clustering fait face à de nombreux problèmes qui rendent cette méthode non optimale suivant les conditions d'utilisation.<sep> Le but de la thèse est d'étudier la portée applicative de la technique de maximisation des traits associée au clustering pour pallier ces problèmes et répondre aux besoins d'outils de support destinés à la conception de ces solutions.
Dans le domaine de la fabrication, la détection d'anomalies telles que les défauts et les défaillances mécaniques permet de lancer des tâches de maintenance prédictive, qui visent à prévoir les défauts, les erreurs et les défaillances futurs et à permettre des actions de maintenance.<sep> Avec la tendance de l'industrie 4.0, les tâches de maintenance prédictive bénéficient de technologies avancées telles que les systèmes cyberphysiques (CPS), l'Internet des objets (IoT) et l'informatique dématérialisée (cloud computing).<sep> Ces technologies avancées permettent la collecte et le traitement de données de capteurs qui contiennent des mesures de signaux physiques de machines, tels que la température, la tension et les vibrations.<sep> Cependant, en raison de la nature hétérogène des données industrielles, les connaissances extraites des données industrielles sont parfois présentées dans une structure complexe.<sep> Des méthodes formelles de représentation des connaissances sont donc nécessaires pour faciliter la compréhension et l'exploitation des connaissances.<sep> En outre, comme les CPSs sont de plus en plus axées sur la connaissance, une représentation uniforme de la connaissance des ressources physiques et des capacités de raisonnement pour les tâches analytiques est nécessaire pour automatiser les processus de prise de décision dans les CPSs.<sep> Ces problèmes constituent des obstacles pour les opérateurs de machines qui doivent effectuer des opérations de maintenance appropriées.<sep> Pour relever les défis susmentionnés, nous proposons dans cette thèse une nouvelle approche sémantique pour faciliter les tâches de maintenance prédictive dans les processus de fabrication.<sep> Ces approches ont été validées sur des ensembles de données réelles et synthétiques.
Afin que ces environnements soient largement utilisés par les enseignants et les apprenants, ils doivent fournir des moyens pour assister les enseignants dans leur tâche de génération d'exercices.<sep> Parmi ces exercices, les Questionnaires à Choix Multiples (QCM) sont très présents.<sep> Cependant, la rédaction d'items à choix multiples évaluant correctement le niveau d'apprentissage des apprenants est une tâche complexe.<sep> Des consignes ont été développées pour rédiger manuellement des items, mais une évaluation automatique de la qualité des items constituerait un outil pratique pour les enseignants.<sep> Nous nous sommes intéressés à l'évaluation automatique de la qualité des distracteurs (mauvais choix de réponse).<sep> Pour cela, nous avons étudié les caractéristiques des distracteurs pertinents à partir de consignes de rédaction de QCM.<sep> Cette étude nous a conduits à considérer que l'homogénéité des distracteurs et de la réponse est un critère important pour valider les distracteurs.<sep> L'homogénéité est d'ordre syntaxique et sémantique.<sep> Nous avons validé la définition de l'homogénéité par une analyse de corpus de QCM, et nous avons proposé des méthodes de reconnaissance automatique de l'homogénéité syntaxique et sémantique à partir de cette analyse.<sep> Nous nous sommes ensuite focalisé sur l'homogénéité sémantique des distracteurs.<sep> Pour l'estimer automatiquement, nous avons proposé un modèle d'ordonnancement par apprentissage, combinant différentes mesures d'homogénéité sémantique.<sep> L'évaluation du modèle a montré que notre méthode est plus efficace que les travaux existants pour estimer l'homogénéité sémantique des distracteurs.
Cette thèse s'intéresse au lexique scientifique transdisciplinaire (LST), lexique inscrit dans le genre de l'article de recherche en sciences humaines et sociales.<sep> Le LST est fréquemment mobilisé dans les écrits scientifiques et constitue ainsi un objet d'importance pour l'étude de ce genre.<sep> Ce lexique trouve également des applications concrètes tant en indexation terminologique que pour l'aide à la rédaction/compréhension de textes scientifiques.<sep> Ces différents objectifs nous amènent à adopter une approche outillée pour identifier et caractériser les unités lexicales du LST, lexique complexe à circonscrire, situé entre lexique de la langue générale et terminologie.<sep> L'analyse de la combinatoire à l'aide d'un corpus arboré autorise ainsi une caractérisation du LST ancrée sur l'usage dans le genre de l'article de recherche.<sep> Selon cette même approche, nous identifions les acceptions nominales transdisciplinaires et proposons une classification sémantique fondée sur la combinatoire en corpus pour intégrer à notre ressource lexicale une typologie nominale sur deux niveaux.<sep> Nous montrons enfin que cette structuration du LST nous permet d'aborder la dimension phraséologique et rhétorique du LST en faisant émerger du corpus des constructions récurrentes définies par leurs propriétés syntactico-sémantiques.
Les ontologies sont diversement employées notamment dans les domaines du Web sémantique, de l'ingénierie des connaissances, …<sep> En effet, elles permettent de partager, de diffuser et d'actualiser les connaissances d'un domaine.<sep> Afin de construire ces ontologies, notre méthodologie utilise tout d'abord des méthodes de Traitement Automatique de la Langue Naturelle (TALN) et d'Extraction d'Information (IE) pour extraire des données préparées à partir de chaque ressource du domaine (corpus de textes, bases de données, thesaurus).<sep> Puis, ces données sont fouillées avec les méthodes de fouilles : l'Analyse Formelle de concepts (AFC) et l'Analyse Relationnelle de Concepts (ARC).<sep> L'AFC regroupe des objets partageant les mêmes attributs binaires dans des concepts d'un treillis.<sep> L'ARC est une extension de l'AFC qui permet de regrouper des objets partageant les mêmes attributs binaires, mais aussi les mêmes attributs relationnels.<sep> L'apposition de contextes (une propriété de l'AFC) permet d'associer ces attributs (binaires et relationnels) à un ensemble de classes prédéfinies et hiérarchisées par les experts du domaine.<sep> De cette façon, des définitions étendues sont proposées aux experts du domaine pour ces classes prédéfinies ainsi que de nouvelles classes inexistantes dans la hiérarchie initiale.<sep> Ces nouvelles classes peuvent être considérées pertinentes et ajoutées par les experts en tant que nouvelles « unités de connaissances » .<sep> Les treillis résultant des méthodes de fouille constituent ce que nous appelons schéma d'ontologie.<sep> Ce schéma d'ontologie est ensuite représenté par le langage FLE de la famille des logiques de descriptions afin d'avoir une ontologie.<sep> Cette ontologie, implémentée en OWL (Web Ontology Language), a permis à notre système de répondre automatiquement à différentes questions proposées par les experts du domaine (instanciation de concepts, comparaison de concepts,…).<sep> Des expériences pratiques ont été menées dans deux domaines d'application que sont l'astronomie et la microbiologie.
Dans cette thèse, nous nous concentrons sur le codage du séjour d'hospitalisation en codes standards. Ce codage est une tâche médicale hautement sensible dans les hôpitaux français, nécessitant des détails minutieux et une haute précision, car le revenu de l'hôpital en dépend directement.<sep> L'encodage du séjour d'hospitalisation comprend l'encodage du diagnostic principal qui motive le séjour d'hospitalisation et d'autres diagnostics secondaires qui surviennent pendant le séjour.<sep> Nous proposons une analyse rétrospective mettant en oeuvre des méthodes d'apprentissage, sur la tâche d'encodage de certains diagnostics secondaires sélectionnés.<sep> Par conséquent, la base de données PMSI est analysée afin d'extraire à partir de séjours de patients hospitalisés antérieurement, des variables décisives (Features). Identifier ces variables permet de pronostiquer le codage d'un diagnostic secondaire difficile qui a eu lieu avec un diagnostic principal fréquent.<sep> Ainsi, à la fin d'une session de codage, nous proposons une aide pour les codeurs en proposant une liste des encodages pertinents ainsi que des variables utilisées pour prédire ces encodages.<sep> En ce qui concerne le défi lié à la connaissance du domaine médical, nous collaborons avec des codeurs experts dans un hôpital local afin de fournir un aperçu expert sur certains diagnostics secondaires difficiles à coder et afin d'évaluer les résultats de la méthodologie proposée.<sep> En ce qui concerne le défi lié à l'exploitation des bases de données médicales par des méthodes d'apprentissage automatique, plus spécifiquement par des méthodes de "Feature Selection" (FS), nous nous concentrons sur la résolution de certains points : le format des bases de données médicales, le nombre de variables dans les bases de données médicales et les variables instables extraites des bases de données médicales.<sep> Nous proposons une série de transformations afin de rendre le format de la base de données médicales, en général sous forme de bases de données relationnelles, exploitable par toutes les méthodes de type FS.<sep> Pour limiter l'explosion du nombre de variables représentées dans la base de données médicales, généralement motivée par la quantité de diagnostics et d'actes médicaux, nous analysons l'impact d'un regroupement de ces variables dans un niveau de représentation approprié et nous choisissons le meilleur niveau de représentation.<sep> Enfin, les bases de données médicales sont souvent déséquilibrées à cause de la répartition inégale des exemples positifs et négatifs.
Cette thèse présente un environnement ouvert et souple pour l'acquisition automatique d'expressions multimots (MWE) à partir de corpus textuels monolingues.<sep> Cette recherche est motivée par l'importance des MWE pour les applications du TALN.<sep> Après avoir brièvement présenté les modules de l'environnement, le mémoire présente des résultats d'évaluation intrinsèque en utilisant deux applications : la lexicographie assistée par ordinateur et la traduction automatique statistique.<sep> Ces deux applications peuvent bénéficier de l'acquisition automatique de MWE, et les expressions acquises automatiquement à partir de corpus peuvent à la fois les accélérer et améliorer leur qualité.<sep> Les résultats prometteurs de nos expériences nous encouragent à mener des recherches ultérieures sur la façon optimale d'intégrer le traitement des MWE dans ces applications et dans bien d'autres
Ce mémoire se situe dans le domaine des systèmes de questions-réponses, ces systèmes qui, à partir d'une question en langage naturel posée par l'utilisateur recherche une réponse dans une collection de documents.<sep> Notre travail se fonde sur la notion de justification, que nous formalisons comme un graphe d'appariement entre les informations linguistiques extraites de la question et les éléments justificatifs correspondants de le passage réponse.<sep> Ce modèle fait intervenir trois types de phénomènes linguistiques : les variations paradigmatiques locales d'un terme (sémantiques, morphologiques, inférences), les liens syntagmatiques entre les constituants d'une phrase, et une composante de sémantique énonciative reliant des éléments distants (anaphores, coréférences, thématisation), dans un contexte multiphrase, aussi bien monoque multi-documents.<sep> Dans ce travail, nous décrivons premièrement l'extraction semi-automatique d'un corpus de questions<sep> Nous mesurons sur ce corpus la conformation des justifications en termes de variation sémantique et d'étendue spatiale.<sep> Ensuite, nous décrivons et évaluons un programme extrayant et pondérant des justifications à partir de passages d'articles de journaux rapportés par une chaîne de traitements questions-réponses.<sep> Notre programme vise à conserver au système la capacité à produire une justification structurée, tout en rendant possible l'intégration d'une grande hétérogénéité de traitements linguistiques, de nature, de niveau de granularité et de fiabilité variés.
Au cours de la dernière décennie, la popularité incomparable des réseaux sociaux numériques s'est traduite par l'omniprésence des spammeurs sur ces plateformes.<sep> Cette présence a commencé par se manifester sous la forme de messages de publicité et d'arnaques traditionnels simples à identifier.<sep> Cet abus ciblé et largement automatisé des réseaux sociaux numériques réduit la crédibilité et l'utilité des informations diffusées sur ces plateformes.<sep> Le problème de détection du spam social a été traditionnellement modélisé comme un problème de classification supervisée où l'objectif est de classer les comptes sociaux individuellement.<sep> Ce choix est problématique pour deux raisons.<sep> Tout d'abord, la nature dynamique du spam social rend les performances des systèmes supervisés difficiles à maintenir.<sep> En outre, la modélisation basée sur les caractéristiques (features) des comptes sociaux individuels ne prend pas en compte le contexte collusoire dans lequel les attaques sur les réseaux sociaux sont menées.<sep> Pour maximiser leur efficacité et la visibilité de leur contenu, les spammeurs actent d'une manière qu'on peut décrire comme “synchronisée”.<sep> Ainsi, même lorsque les spammeurs changent de caractéristiques, ils continuent à agir de manière collusoire, créant des liens entre les comptes complices.<sep> Ceci constitue un signal non supervisé qui est relativement facile à maintenir et difficile à contourner.<sep> Il est donc avantageux de trouver une mesure de similarité adaptée qui soit capable de capturer ce comportement collusoire.<sep> Dans ce travail, nous proposons d'exprimer le problème de détection de spam social en termes probabilistes en utilisant le cadre des modèles graphiques non dirigés.<sep> Au lieu du paradigme de détection individuelle qui est couramment utilisé dans la littérature, nous cherchons à modéliser la tâche de classification comme une tâche d'inférence sur la probabilité jointe d'un graphe de variables.<sep> Dans ce contexte, les comptes sont représentés comme des variables aléatoires et la dépendance entre ces variables est représentée par un graphe.<sep> Nous proposons deux modèles graphiques : le Champs Aléatoire de Markov où l'inférence est effectuée par l'algorithme de Propagation des Convictions à Boucle, et le Champs Aléatoire Conditionnel, où on choisit d'utiliser l'algorithme du Tree Reweighted Message Passing pour l'inférence et une fonction de perte qui minimise le risque empirique.<sep> Les deux modèles, évalués sur Twitter, montrent une augmentation des performances de classification par rapport aux classifieurs supervisés de la littérature.
La classification des images revêt un intérêt majeur dans de nombreuses tâches de reconnaissance visuelle, en particulier pour la reconnaissance de véhicules au sol via les systèmes aéroportés, où les images traitées sont de faible résolution du fait de la large distance entre le porteur et la scène observée.<sep> Durant l'apprentissage, des données complémentaires peuvent être disponibles, qu'il s'agisse de connaissances sur les conditions de prise de vue ou de la version haute-résolution des images.<sep> Dans nos travaux, on s'intéresse au problème de la reconnaissance d'images faiblement résolues en prenant en compte des informations complémentaires pendant l'apprentissage.<sep> On montre d'abord l'intérêt des réseaux convolutionnels profonds pour la reconnaissance d'images faiblement résolues, en proposant notamment une architecture apprise sur les données.<sep> D'autre part, on s'appuie sur le cadre de l'apprentissage avec information privilégiée pour bénéficier des données d'entraînement complémentaires, ici les versions haute-résolution des images.<sep> Nous proposons deux méthodes d'intégration de l'information privilégiée dans l'apprentissage des réseaux de neurones.<sep> Notre premier modèle s'appuie sur ces données complémentaires pour calculer un niveau de difficulté absolue, attribuant un poids important aux images les plus facilement reconnaissables.<sep> Notre deuxième modèle introduit une contrainte de similitude entre les modèles appris sur chaque type de données.<sep> On valide expérimentalement nos deux modèles dans plusieurs cas d'application, notamment dans un contexte orienté grain-fin et sur une base de données contenant du bruit d'annotation.
Les ressources d'information multilingues sur le Web sont devenues de plus en plus des objets d'études importantes pour différents domaines intervenant au traitement de l'information.<sep> Néanmoins, nous constatons que la structure des ressources multilingues est très peu explorée par rapport à l'abondance des méthodes de traitement automatique des langues naturelles.<sep> L'ignorance des structures multilingues pourrait être à l'origine de divers problèmes de performance tels que : i) la redondance, si le site propose simultanément des traductions en plusieurs langues, ii) les parcours bruités lors d'un passage d'une langue à une autre via les vignettes (génération de graphes, conceptuellement, non signifiant), iii) la perte de l'information par la négligence de la spécificité structurelle (même implicite) de chaque langue.<sep> Le cadre de cette thèse s'insère dans le cadre des travaux de recherche sur l'extraction semi-automatique (interactive) d'information à partir d'un grand nombre de documents électroniques (essentiellement des documents web) hétérogènes structurées ou semi-structurées.<sep> Pour expérimenter et valider notre propos nous avons développé une méthode d'analyses structurelles concrétisée par le réalisation du système Hyperling.
La stratégie dite du transfert cross-lingue permet de contourner cette limitation : une langue peu dotée en ressources (la cible) peut être traitée en exploitant les ressources disponibles dans une autre langue (la source).<sep> Les progrès accomplis sur ce plan se limitent néanmoins à des scénarios idéalisés, avec des ressources cross-lingues prédéfinies et de bonne qualité, de sorte que le transfert reste inapplicable aux cas réels de langues peu dotées, qui n'ont pas ces garanties.<sep> L'étude est menée en utilisant l'analyse en dépendance par transition comme cadre applicatif.<sep> Le cœur de ce travail est l'élaboration d'un nouveau méta-algorithme de transfert, dont l'architecture en cascade permet la combinaison fine des diverses ressources, en ciblant leur exploitation à l'échelle du mot.<sep> L'approche cross-lingue pure n'étant en l'état pas compétitive avec la simple annotation de quelques phrases cibles, c'est avant tout la complémentarité de ces méthodes que souligne l'analyse empirique.<sep> Une série de nouvelles métriques permet une caractérisation fine des similarités cross-lingues et des spécificités syntaxiques de chaque langue, de même que de la valeur ajoutée de l'information cross-lingue par rapport au cadre monolingue.<sep> L'exploitation d'informations typologiques s'avère également particulièrement fructueuse.<sep> Ces contributions reposent largement sur des innovations techniques en analyse syntaxique, concrétisées par la publication en open source du logiciel PanParser, qui exploite et généralise la méthode dite des oracles dynamiques.<sep> Cette thèse contribue sur le plan monolingue à plusieurs autres égards, comme le concept de cascades monolingues, pouvant traiter par exemple d'abord toutes les dépendances faciles, puis seulement les difficiles.
Les relaxations en problème d'optimisation linéaire jouent un rôle central en inférence du maximum a posteriori (map) dans les champs aléatoires de Markov discrets.<sep> Nous étudions ici les avantages offerts par les méthodes de Newton pour résoudre efficacement le problème dual (au sens de Lagrange) d'une reformulation lisse du problème.<sep> Nous comparons ces dernières aux méthodes de premier ordre, à la fois en terme de vitesse de convergence et de robustesse au mauvais conditionnement du problème.<sep> Nous exposons donc un cadre général pour l'apprentissage non-supervisé basé sur le transport optimal et les régularisations parcimonieuses.<sep> Nous exhibons notamment une approche prometteuse pour résoudre le problème de la préimage dans l'ACP à noyau.<sep> Du point de vue de l'optimisation, nous décrivons le calcul du gradient d'une version lisse de la norme p de Schatten et comment cette dernière peut être utilisée dans un schéma de majoration-minimisation.
L'évolution rapide du matériel et des logiciels permet de proposer de nouvelles solutions aux utilisateurs afin de répondre au mieux à leurs besoins.<sep> Toutefois, la mise en place de ces nouveaux outils ne doit pas mettre en cause le travail des utilisateurs.<sep> L'informatique ne doit pas leur demander un surcroit de travail mais au contraire leur fournir des solutions naturelles et conviviales leur permettant de réaliser leurs tâches quotidiennes dans les meilleures conditions.<sep> Le premier chapitre décrit l'évolution des systèmes informatiques vers des solutions coopératives utilisant au mieux les différentes composantes : micro-ordinateurs, réseau et serveurs.<sep> Le second chapitre met en évidence les nouveaux besoins des grandes organisations afin de rendre à l'informatique son véritable rôle : faciliter la tâche des utilisateurs.<sep> Pour atteindre cet objectif, le concept de télé-administration a été défini.<sep> Le dernier chapitre présente deux aspects des interfaces homme-machine : d'une part la proposition d'un modèle de représentation et d'une méthode de conception s'appuyant sur l'approche objet, d'autre part l'intégration de la modalité vocale en tant que modalité d'entrée et de sortie.
L'analyse syntaxique consiste à prédire la représentation syntaxique de phrases en langue naturelle sous la forme d'arbres syntaxiques.<sep> Cette tâche pose des problèmes particuliers pour les langues non-configurationnelles ou qui ont une morphologie flexionnelle plus riche que celle de l'anglais.<sep> Dans cette thèse, nous nous plaçons dans le cadre de l'analyse syntaxique robuste en constituants par transitions.<sep> Dans un premier temps, nous étudions comment intégrer l'analyse morphologique à l'analyse syntaxique, à l'aide d'une architecture de réseaux de neurones basée sur l'apprentissage multitâches.<sep> Dans un second temps, nous proposons un système de transitions qui permet de prédire des structures générées par des grammaires légèrement sensibles au contexte telles que les LCFRS.<sep> Enfin, nous étudions la question de la lexicalisation de l'analyse syntaxique.<sep> Les analyseurs syntaxiques en constituants lexicalisés font l'hypothèse que les constituants s'organisent autour d'une tête lexicale et que la modélisation des relations bilexicales est cruciale pour désambiguïser.
L'émergence des technologies de l'information et de la communication (TIC) au début des années 1990, notamment internet, a permis de produire facilement des données et de les diffuser au reste du monde.<sep> L'essor des bases de données, le développement des outils applicatifs et la réduction des coûts de stockage ont conduit à l'augmentation quasi exponentielle des quantités de données au sein de l'entreprise.<sep> Le grand nombre de corrélations (visibles ou cachées) entre données rend les données plus entrelacées et complexes.<sep> Les données sont aussi plus hétérogènes, car elles peuvent venir de plusieurs sources et exister dans de nombreux formats (texte, image, audio, vidéo, etc.) ou à différents degrés de structuration (structurées, semi-structurées, non-structurées).<sep> Les systèmes d'information des entreprises actuelles contiennent des données qui sont plus massives, complexes et hétérogènes.<sep> Les données peuvent soit avoir des dénominations différentes, soit ne pas avoir des provenances vérifiables.<sep> Par conséquent, ces données sont difficilement interprétées et accessibles aux autres acteurs.<sep> Elles restent inexploitées ou non exploitées au maximum afin de pouvoir les partager et/ou les réutiliser.<sep> L'accès aux données (ou la recherche de données), par définition est le processus d'extraction des informations à partir d'une base de données en utilisant des requêtes, pour répondre à une question spécifique.<sep> L'extraction des informations est une fonction indispensable pour tout système d'information.<sep> Cependant, cette dernière n'est jamais facile car elle représente toujours un goulot majeur d'étranglement pour toutes les organisations (Soylu et al. 2013).<sep> Dans l'environnement de données complexes, hétérogènes et de multi-utilisation de données, fournir à tous les utilisateurs un accès facile et simple aux données devient plus difficile pour deux raisons : <sep> - Le manque de compétences techniques.<sep> Pour formuler informatiquement une requête complexe (les requêtes conjonctives), l'utilisateur doit connaitre la structuration de données, c'est-à-dire la façon dont les données sont organisées et stockées dans la base de données.<sep> Quand les données sont volumineuses et complexes, ce n'est pas facile d'avoir une compréhension approfondie sur toutes les dépendances et interrelations entre données, même pour les techniciens du système d'information.<sep> De plus, cette compréhension n'est pas forcément liée au savoir et savoir-faire du domaine et il est donc, très rare que les utilisateurs finaux possèdent les compétences suffisantes.<sep> - Différents points de vue des utilisateurs.<sep> Dans l'environnement de multi-utilisation de données, chaque utilisateur introduit son propre point de vue quand il ajoute des nouvelles données et informations techniques.
Les annotations sémantiques sont utilisées dans de nombreux domaines comme celui de la santé et servent à différentes tâches notamment la recherche et le partage d'information ou encore l'aide à la décision.<sep> Les annotations sont produites en associant à des documents digitaux des labels de concepts provenant des systèmes d'organisation de la connaissance (Knowledge Organization Systems, ou KOS, en anglais) comme les ontologies.<sep> Cependant, la nature dynamique de la connaissance engendre régulièrement de profondes modifications au niveau du contenu des KOS provoquant ainsi un décalage entre la définition des concepts et les annotations.<sep> Elles permettent alors aux ordinateurs d'interpréter, connecter et d'utiliser de grandes quantités de données.<sep> Dans ce mémoire de thèse, nous proposons une approche originale appelée MAISA pour résoudre le problème de l'adaptation des annotations sémantiques engendrée par l'évolution des KOS et pour lequel nous distinguons deux cas.<sep> Dans le premier cas, nous considérons que les annotations sont directement modifiables.<sep> Pour traiter ce problème nous avons défini une approche à base de règles combinant des informations provenant de l'évolution des KOS et des connaissances extraites du Web.<sep> Dans le deuxième cas, nous considérons que les annotations ne sont pas modifiables comme c'est bien souvent le cas des annotations associées aux données des patients.<sep> L'objectif ici étant de pouvoir retrouver les documents annotées avec une version du KOS donnée lorsque l'utilisateur interroge le système stockant ces documents avec le vocabulaire du même KOS mais d'une version différente.<sep> Pour gérer ce décalage de versions, nous avons proposé un graphe de connaissance représentant un KOS et son historique et un mécanisme d'enrichissement de requêtes permettant d'extraire de ce graphe l'historique d'un concept pour l'ajouter à la requête initiale.<sep> Nous proposons une évaluation expérimentale de notre approche pour la maintenance des annotations à partir de cas réels construits sur quatre KOS du domaine de la santé : ICD-9-CM, MeSH, NCIt et SNOMED CT.<sep> Nous montrons à travers l'utilisation des métriques classiques que l'approche proposée permet, dans les deux cas considérés, d'améliorer la maintenance des annotations sémantiques.
Un premier objectif de cette thèse est la constitution de ressources linguistiques pour un système de traduction automatique statistique factorisée français - roumain.<sep> Un deuxième objectif est l'étude de l'impact des informations linguistiques exploitées dans le processus d'alignement lexical et de traduction.<sep> Cette étude est motivée, d'une part, par le manque de systèmes de traduction automatique pour la paire de langues étudiées et, d'autre part, par le nombre important d'erreurs générées par les systèmes de traduction automatique actuels.<sep> Les ressources linguistiques requises par ce système sont segmentés lexicalement, lemmatisés et étiquetés au niveau morphosyntaxique.
Les plongements lexicaux sont un composant standard des architectures modernes de traitement automatique des langues (TAL).<sep> Chaque fois qu'une avancée est obtenue dans l'apprentissage de plongements lexicaux, la grande majorité des tâches de traitement automatique des langues, telles que l'étiquetage morphosyntaxique, la reconnaissance d'entités nommées, la recherche de réponses à des questions, ou l'inférence textuelle, peuvent en bénéficier.<sep> Dans ce but, je précalcule des statistiques de cooccurrence entre mots avec corpus2graph, un paquet Python en source ouverte orienté vers les applications en TAL : il génère efficacement un graphe de cooccurrence à partir d'un grand corpus, et lui applique des algorithmes de graphes tels que les marches aléatoires.<sep> Pour la mise en correspondance translingue de plongements lexicaux, je relie les plongements lexicaux contextuels à des plongements de sens de mots.<sep> L'algorithme amélioré de création d'ancres que je propose étend également la portée des algorithmes de mise en correspondance de plongements lexicaux du cas non-contextuel au cas des plongements contextuels.
Cette thèse porte sur l'usage du dictionnaire général français monolingue dans l'enseignement primaire.<sep> Cet usage, aujourd'hui banal, est-il ancien ?<sep> Depuis quand le dictionnaire a-t-il sa place dans l'enseignement ?<sep> Au XXIe siècle, de quels dictionnaires s'agit-il ?<sep> Sont-ils couramment consultés par les enseignants et les élèves ?<sep> Quel rôle jouent-ils dans l'apprentissage de la langue maternelle ?<sep> Quelle place occupent les outils numériques ?<sep> Pour tenter de répondre à ces questions, cette étude s'organise en trois parties consacrées, respectivement, à l'institutionnalisation de l'enseignement primaire ; <sep> Il est toujours tenu compte du contexte historique, sans négliger les rapprochements contemporains, notamment géographiques.<sep> La conclusion dresse un bilan des efforts accomplis et ouvre des perspectives.<sep> À cette thèse sont jointes plusieurs annexes : textes de lois, listes de dictionnaires chronologiquement ou thématiquement classés, extraits de rapports officiels, tableaux comparatifs..
Les travaux de recherche de cette thèse portent sur l'amélioration des capacités sociales d'un système conversationnel en interaction avec un humain.<sep> Lorsque le système n'est pas dédié à une tâche particulière, il doit tenir compte de difficultés relevant de l'interaction sociale elle-même.<sep> L'humour est un mécanisme naturel dans les interactions sociales.<sep> Nous considérons un humour-machine comme une simulation de comportements simplifiés des capacités humoristiques humaines : dérision, blagues, jeux de mots.<sep> Les travaux de cette thèse s'appuient sur des théories issues de domaines variés en sociologie, psychologie, neurosciences et linguistique pour l'intégration de cet humour-machine dans un système robotique.<sep> Implémentées dans certains systèmes de dialogue, ces capacités humoristiques sont cependant rarement utilisées pour pouvoir choisir les comportements à générer du robot.<sep> Dans nos travaux, la mise en œuvre des comportements humoristiques du système en conversation est réalisée en utilisant la théorie des rites d'interaction.<sep> L'estimation de la face de l'interlocuteur permet de diriger le comportement du robot dans la conversation casuelle humoristique.<sep> Pour réaliser cette estimation de la face, nous étudions, à partir de corpus créés à cet effet, les réactions comportementales, affectives et expressives des participants à différents types d'humour réalisés par le robot et ayant des impacts variables sur celle-ci (l'humour prenant pour cible le robot, le participant ou un autre sujet).<sep> Les réactions des participants à l'humour sont établies sur une représentation multi-niveaux d'indices émotionnels, comportementaux et linguistiques, extraits à partir de l'audio.<sep> Des règles sont ainsi construites à partir de l'apprentissage automatique de ces indices issus des corpus, concernant l'appréciation de la réaction des participants à l'humour et la mise à jour de l'estimation de la face présentée du participant.<sep> Leur implémentation dans un système automatique nous permet de les évaluer.<sep> De nombreuses expérimentations ont été menées avec des publics variés : personnes âgées, adultes, adolescents.<sep> Enfin, l'utilisation des préférences du participant à l'humour dans la conversation fait émerger des questions éthiques, notamment face au pouvoir persuasif et manipulateur de l'humour.
La distribution de l'abondance des espèces en un site, et la similarité de la composition taxonomique d'un site à l'autre, sont deux mesures de la biodiversité ayant servi de longue date de base empirique aux écologues pour tenter d'établir les règles générales gouvernant l'assemblage des communautés d'organismes.<sep> Cette approche présente l'avantage d'être rapide et standardisée, et donne accès à un large éventail de taxons microbiens jusqu'alors indétectables.<sep> Toutefois, ces jeux de données de grande taille à la structure complexe sont difficiles à analyser, et le caractère indirect des observations complique leur interprétation.<sep> Le premier objectif de cette thèse est d'identifier les modèles statistiques permettant d'exploiter ce nouveau type de données afin de mieux comprendre l'assemblage des communautés.<sep> Le deuxième objectif est de tester les approches retenues sur des données de biodiversité du sol en forêt amazonienne, collectées en Guyane française.<sep> Deux grands types de processus sont invoqués pour expliquer l'assemblage des communautés d'organismes : les processus "neutres", indépendants de l'espèce considérée, que sont la naissance, la mort et la dispersion des organismes, et les processus liés à la niche écologique occupée par les organismes, c'est-à-dire les interactions avec l'environnement et entre organismes.<sep> Démêler l'importance relative de ces deux types de processus dans l'assemblage des communautés est une question fondamentale en écologie ayant de nombreuses implications, notamment pour l'estimation de la biodiversité et la conservation.<sep> Le premier chapitre aborde cette question à travers la comparaison d'échantillons d'ADN environnemental prélevés dans le sol de diverses parcelles forestières en Guyane française, via les outils classiques d'analyse statistique en écologie des communautés.<sep> Le deuxième chapitre se concentre sur les processus neutres d'assemblages des communautés.
Pour comprendre comment se construisent les connaissances sur l'effet des interventions en médecine, il est nécessaire de savoir où est faite la recherche clinique dans le monde, quelles maladies sont étudiées, et quels acteurs la mettent en place.<sep> Une vision globale du système de recherche peut aider à identifier des lacunes dans la production de connaissances et à orienter l'activité de recherche vers les priorités de santé, en particulier dans les régions où les ressources sont limitées.<sep> Dans ce travail nous avons construit des cartographies de la recherche clinique, c'est-à-dire des analyses agrégées de ce système complexe visant à extraire de l'information sur l'activité globale de recherche.<sep> Nous avons utilisé les registres d'essais cliniques inclus dans l'International Clinical Trials Registry Platform de l'Organisation Mondiale de la Santé pour cartographier l'activité de recherche.<sep> Dans un premier travail nous avons évalué pour 7 régions l'alignement entre l'effort local de recherche sur 10 ans et le fardeau de 27 groupes de maladies.<sep> Ce travail a nécessité le développement d'un algorithme de classification automatique des maladies étudiées dans les essais clinique basé sur des méthodes de traitement automatique du langage.<sep> Dans toutes les autres régions nous avons identifié des lacunes dans l'effort de recherche.<sep> En particulier, en Afrique Subsaharienne, même si des causes majeures de fardeau comme le VIH et le paludisme reçoivent un effort de recherche important, d'autres priorités locales, les maladies infectieuses communes et les pathologies du nouveau-né, ont été négligées par l'effort de recherche.<sep> Dans un deuxième travail nous avons évalué l'influence du type de promoteur (industriel ou non-industriel) dans l'utilisation de réseaux de pays pour recruter des patients dans des essais cliniques multi-pays.<sep> Nous avons montré que 30% contre 3% des essais à promoteur industriel et non-industriel sont multi-pays, respectivement.<sep> Les pays d'Europe de l'Est participent dans leur ensemble de façon surreprésentée dans la recherche multi-pays industrielle.<sep> Ceci suggère les grandes capacités des industriels à globaliser leur recherche en s'appuyant sur des réseaux de pays bien définis.<sep> Ces travaux forment une brique pour le développement d'un observatoire global de la recherche médicale.
En recherche d'information, les documents sont le plus souvent considérés comme des "sacs-de-mots".<sep> Ce modèle ne tient pas compte de la structure temporelle du document et est sensible aux bruits qui peuvent altérer la forme lexicale.<sep> Ces bruits peuvent être produits par différentes sources : forme peu contrôlée des messages des sites de micro-blogging, messages vocaux dont la transcription automatique contient des erreurs, variabilités lexicales et grammaticales dans les forums du Web...<sep> Le travail présenté dans cette thèse s'intéresse au problème de la représentation de documents issus de sources bruitées.<sep> La thèse comporte trois parties dans lesquelles différentes représentations des contenus sont proposées.<sep> La première partie compare une représentation classique utilisant la fréquence des mots à une représentation de haut-niveau s'appuyant sur un espace de thèmes.<sep> Cette abstraction du contenu permet de limiter l'altération de la forme de surface du document bruité en le représentant par un ensemble de caractéristiques de haut-niveau.<sep> Nos expériences confirment que cette projection dans un espace de thèmes permet d'améliorer les résultats obtenus sur diverses tâches de recherche d'information en comparaison d'une représentation plus classique utilisant la fréquence des mots.<sep> Le problème majeur d'une telle représentation est qu'elle est fondée sur un espace de thèmes dont les paramètres sont choisis empiriquement.<sep> La deuxième partie décrit une nouvelle représentation s'appuyant sur des espaces multiples et permettant de résoudre trois problèmes majeurs : la proximité des sujets traités dans le document, le choix difficile des paramètres du modèle de thèmes ainsi que la robustesse de la représentation.<sep> Partant de l'idée qu'une seule représentation des contenus ne peut pas capturer l'ensemble des informations utiles, nous proposons d'augmenter le nombre de vues sur un même document.<sep> Cette multiplication des vues permet de générer des observations "artificielles" qui contiennent des fragments de l'information utile.<sep> Une première expérience a validé cette approche multi-vues de la représentation de textes bruités.<sep> Elle a cependant l'inconvénient d'être très volumineuse,redondante, et de contenir une variabilité additionnelle liée à la diversité des vues.<sep> Dans un deuxième temps, nous proposons une méthode s'appuyant sur l'analyse factorielle pour fusionner les vues multiples et obtenir une nouvelle représentation robuste,de dimension réduite, ne contenant que la partie "utile" du document tout en réduisant les variabilités "parasites".<sep> Lors d'une tâche de catégorisation de conversations,ce processus de compression a confirmé qu'il permettait d'augmenter la robustesse de la représentation du document bruité.<sep> Cependant, lors de l'élaboration des espaces de thèmes, le document reste considéré comme un "sac-de-mots" alors que plusieurs études montrent que la position d'un terme au sein du document est importante.<sep> Une représentation tenant compte de cette structure temporelle du document est proposée dans la troisième partie.<sep> Cette représentation s'appuie sur les nombres hyper-complexes de dimension appelés quaternions.<sep> Nos expériences menées sur une tâche de catégorisation ont montré l'efficacité de cette méthode comparativement aux représentations classiques en "sacs-de-mots".
Cette thèse s'inscrit dans le cadre du traitement automatique du langage naturel, et traite plus précisément de l'alignement sous-phrastique bilingue classiquement lié à la traduction automatique statistique.<sep> Les travaux exposés s'en distinguent en proposant une mécanique évolutive à base d'exemples initiée par des annotateurs non-experts via une interface adaptée.<sep> L'approche est principalement motivée par la recherche d'une expressivité comparable à celle observée dans les alignements manuels.<sep> Une partie importante de ce travail consiste à définir un cadre formel sous-tendant une architecture originale à base d'exemples alignés.<sep> Deux nouvelles méthodes d'alignement sont comparées à des références connues via des mesures d'accord classiques et trois distances transformationnelles sont introduites.
L'objet de cette thèse est de proposer un modèle d'apprentissage profond en traitement du langage naturel (TALN) qui mettrait en évidence les informations discriminantes d'une décision de justice à l'aide d'un réseau de neurones de type réseaux antagonistes génératifs (GAN).
Le laboratoire i3 et le laboratoire LGIPH, utilisent des approches à haut débit pour l'étude du système immunitaire et ces disfonctionnements.<sep> Des limites ont été observées quant à l'utilisation des approches classiques pour l'annotation des signatures d'expression des gènes.<sep> L'objectif principal a été de développer une approche d'annotation pour répondre à ce besoin.<sep> L'approche que nous avons développée est une approche basée sur la contextualisation des gènes et de leurs produits puis sur la modélisation des voies biologiques pour la production de bases de connaissances pour l'étude de l'expression des gènes.<sep> Nous définissons ici un contexte d'expression des gènes comme suit : population cellulaire+compartiment anatomique+état pathologique.<sep> Nous montrons ici que notre package a des performances meilleures que un outil de référence.<sep> Pour la modélisation des voies biologiques nous avons développé en collaboration avec le LIPAH une méthode de modélisation basée sur un algorithme génétique qui permet de combiner les résultats de mesure de la proximité sémantique sur la base des annotations des gènes et les données d'interactions.
Cette thèse traite des systèmes de recommandation automatiques.<sep> Les moteurs de recommandation automatique sont des systèmes qui permettent, par des techniques de data mining, de recommander automatiquement à des clients, en fonction de leurs consommations passées, des produits susceptibles de les intéresser.<sep> Ces systèmes permettent par exemple d'augmenter les ventes sur des sites web marchands : le site Amazon a une stratégie marketing en grande partie basée sur la recommandation automatique.<sep> La contribution centrale de cette thèse est d'analyser les systèmes de recommandation automatiques dans le contexte industriel, et notamment des besoins marketing, et de croiser cette analyse avec les travaux académiques.
Le présent manuscrit constitue la partie écrite du travail de thèse réalisé par Bruno Mery sous la direction de Christian Bassac et Christian Retoré entre 2006 et 2011, portant sur le sujet "Modélisation de la sémantique lexicale dans la théorie des types".<sep> Il s'agit d'une thèse d'informatique s'inscrivant dans le domaine du traitement automatique des langues, et visant à apporter un cadre formel pour la prise en compte, lors de l'analyse sémantique de la phrase, d'informations apportées par chacun des mots.<sep> Après avoir situé le sujet, cette thèse examine les nombreux travaux l'ayant précédée et s'inscrit dans la tradition du lexique génératif.<sep> Elle présente des exemples de phénomènes à traiter, et donne une proposition de système de calcul fondée sur la logique du second ordre.<sep> Elle examine ensuite la validité de cette proposition par rapport aux exemples et aux autres approches déjà formalisées, et relate une implémentation de ce système.<sep> Enfin, elle propose une brève discussion des sujets restant en suspens.
L'objectif principal de cette thèse est de montrer que les informations lexicales issues d'un dictionnaire de langue, tel le Trésor de la langue française informatisé (TLFi), peuvent améliorer les processus d'indexation et de recherche d'images.<sep> Le problème d'utilisation d'une telle ressource est qu'elle n'est pas suffisamment formalisée pour être exploitée d'emblée dans un tel domaine d'application.<sep> Pour résoudre ce problème, nous proposons, dans un premier temps, une approche de construction automatique de hiérarchies sémantiques à partir du TLFi.<sep> Après avoir défini une caractéristique quantitative (mesurable) et comparable des noms apparaissant dans les définitions lexicographiques, à travers une formule de pondération permettant de sélectionner le nom de poids maximal comme un bon candidat hyperonyme pour un lexème donné du TLFi, nous proposons un algorithme de construction automatique de hiérarchies sémantiques pour les lexèmes des vocables du TLFi.<sep> Une fois notre approche validée à travers des évaluations manuelles, nous montrons, dans un second temps, que les hiérarchies sémantiques obtenues à partir du TLFi peuvent être utilisées pour l'enrichissement d'un thésaurus construit manuellement ainsi que pour l'indexation automatique d'images à partir de leurs descriptions textuelles associées.<sep> Nous prouvons aussi que l'exploitation d'une telle ressource dans le domaine de recherche d'images améliore la précision de la recherche en structurant les résultats selon les domaines auxquels les concepts de la requête de recherche peuvent faire référence.<sep> La mise en place d'un prototype nous a permis ainsi d'évaluer et de valider les approches proposées.
L'hameçonnage est une escroquerie moderne qui cible les utilisateurs de communications électroniques et vise à les convaincre de réaliser des actions pour le bénéfice d'un individu nommé hameçonneur.<sep> Les attaques d'hameçonnage s'appuient essentiellement sur de l'ingénierie sociale et la plupart de ces attaques utilisent des liens représentés par des noms de domaine et des URLs.<sep> Nous proposons donc dans cette thèse de nouvelles solutions, reposant sur une analyse lexicale et sémantique de la composition des noms de domaine et des URLs, pour combattre l'hameçonnage.<sep> Ces deux types de pointeurs sont créés et offusqués par les hameçonneurs pour piéger leurs victimes.<sep> Ainsi, nous démontrons que les noms de domaine et les URLs utilisés dans des attaques d'hameçonnage présentent des similitudes dans leur composition lexicale et sémantique, et que celles-ci sont différentes des caractéristiques présentées par les noms de domaine et les URL légitimes.<sep> Nous utilisons ces caractéristiques pour construire des modèles représentant la composition des URLs et des noms de domaine d'hameçonnage en utilisant des techniques d'apprentissage automatique et des méthodes de traitement du langage naturel.<sep> Les modèles construits sont utilisés pour des applications telles que l'identification de noms de domaine et des URLs d'hameçonnage, la notation des URLs et la prédiction des noms de domaine utilisés dans les attaques d'hameçonnage.<sep> Les techniques proposées sont évaluées sur des données réelles et elles montrent leur efficacité en répondant aux exigences de vitesse, d'universalité et de fiabilité
Les réseaux logiciels ont le potentiel de porter l'infrastructure réseau à un niveau plus avancé, un niveau qui peut rendre la configuration autonome.<sep> Cette capacité peut surmonter la complexité croissante des réseaux actuels et permettre aux entités de gestion d'activer un comportement efficace dans le réseau pour une amélioration globale des performances sans aucune intervention humaine.<sep> Les paramètres de configuration peuvent être sélectionnés automatiquement pour les ressources réseau afin de faire face à diverses situations que les réseaux rencontrent, comme les erreurs et la dégradation des performances.<sep> Malheureusement, certains défis doivent être relevés pour atteindre ce niveau avancé de réseaux.<sep> Actuellement, la configuration est encore souvent générée manuellement par des experts du domaine dans d'énormes fichiers semi-structurés écrits en XML, JSON et YAML.<sep> C'est une tâche complexe, sujette aux erreurs et fastidieuse à accomplir par les humains.<sep> De plus, il n'y a pas de stratégie formelle, à part l'expérience et les meilleures pratiques des experts du domaine pour concevoir les fichiers de configuration.<sep> Différents experts peuvent choisir une configuration différente pour le même objectif de performances.<sep> Cette situation rend plus difficile l'extraction des fonctionnalités des fichiers de configuration et l'apprentissage des modèles susceptibles de générer ou de recommander automatiquement la configuration.<sep> Dans cette thèse, nous présentons nos contributions qui abordent les défis susmentionnés liés à l'automatisation de la configuration dans les réseaux logiciels.<sep> Pour aborder le problème de l'hétérogénéité entre les fichiers de configuration, nous proposons un cadre sémantique basé sur des ontologies qui peuvent fédérer des éléments communs à partir de différents fichiers de configuration.
Dans l'ingénierie avionique, les tests d'intégration sont cruciaux : ils permettent de s'assurer du bon comportement d'un avion avant son premier vol, ils sont nécessaires au processus de certification et permettent des tests de non-régression à chaque nouvelle version d'un système, d'un logiciel ou d'un matériel.<sep> La conception d'un test d'intégration coûte cher car elle mêle la réalisation de la procédure, le paramétrage de nombreux outils couplés au banc de test ainsi que l'adressage des interfaces du système testé.<sep> Avec des procédures de test écrites en langage naturel, l'interprétation des instructions d'un test lors de son rejeu manuel peut provoquer des erreurs coûteuses à corriger, en raison notamment des actions précises à entreprendre lors de l'exécution d'une instruction de test.<sep> Notre contribution est alors un framework orchestrant les langages de test dédiés à l'intégration de systèmes avioniques dans une vision Agile.<sep> Nous introduisons tout d'abord le concept de langage spécifique à un domaine (Domain Specific Language ou DSL) et montrons comment nous l'utilisons pour la formalisation des procédures de test dédiées à un type de système particulier.<sep> Ces langages devront pouvoir être utilisés par des testeurs avioniques qui n'ont pas forcément de compétences en informatique.<sep> Ils permettent l'automatisation des tests d'intégration, tout en conservant l'intention du test dans la description des procédures.<sep> Puis, nous proposons l'approche BDD (Behavior Driven Development) pour valider l'intégration de systèmes par scénarios comportementaux décrivant le comportement attendu de l'avion.<sep> Test Languages (DSTL) les langages utilisés par les testeurs.<sep> Un premier DSTL concernant les systèmes de régulation de l'air a été développé entièrement en tant que preuve du concept à partir de procédures existantes pseudo-formalisées.<sep> L'expérimentation s'est poursuivie avec les calculateurs standardisés IMA (Integrated Modular Avionic) pour lesquels les procédures de test sont décrites en langage naturel et sont donc non automatisables.<sep> A partir d'un corpus de procédures, nous proposons un premier processus empirique d'identification des patrons de phrases peuplant un DSTL.<sep> Le corpus fourni est composé de dix procédures totalisant 108 chapitres de test et 252 tests ou sous-tests comportant au total 3708 instructions pour 250 pages Word.<sep> Rendre agile ces tests d'intégration consiste à proposer une approche collaborative pour formaliser un DSTL que ce soit pour les patrons de phrase de la grammaire concrète ou pour les patrons de transformations vers des langages exécutables.
Cette thèse est consacrée au domaine de l'utilisabilité de la sécurité, en particulier dans le contexte de l'authentification en ligne et du vote vérifiable.<sep> Le rôle toujours plus important de nos identifiants en ligne, qu'ils soient utilisés pour accéder aux réseaux sociaux, aux services bancaires ou aux systèmes de vote, a débouché sur des solutions faisant plus de mal que de bien.<sep> Le problème n'est pas juste technique mais a une forte composante psycho-sociale, qui se révèle dans l'usage des mots de passe --- objet central d'étude de cette thèse.<sep> Les utilisateurs font quotidiennement face à des compromis, souvent inconscients, entre sécuriser leurs données et dépenser les ressources mentales limitées et déjà trop sollicitées. Des travaux récents ont montré que l'absence de règles communes, les contraintes ad-hoc si fréquentes et les recommandations contradictoires compliquent ce choix.<sep> Cette thèse vise à résoudre ces problèmes avec des solutions inspirées par la cryptographie, la psychologie, ainsi que sept études utilisateurs, afin d'obtenir des outils simplifiés non seulement pour l'utilisateur final mais aussi pour le développeur.<sep> La première partie des contributions se concentre sur le fournisseur de service, avec deux outils permettant d'améliorer l'expérience utilisateur sans effort de sa part.<sep> Nous commençons par une étude sur la facilité de transcription de différents types de codes, afin d'obtenir un design réduisant les erreurs tout en augmentant la vitesse de frappe.<sep> Nous montrons aussi comment accepter les fautes de frappe dans les mots de passe peut améliorer la sécurité, en offrant un protocole compatible avec les méthodes de hachage standard.<sep> La deuxième partie offre des outils directement aux utilisateurs, avec un gestionnaire de mot de passe mental qui ne nécessite que la mémorisation d'une phrase et d'un code PIN, avec des garanties sur la sécurité des mots de passe si certains sont compromis.<sep> Enfin, nous nous consacrons aux nouveaux protocoles de vote, en commençant par les difficultés à les faire accepter en pratique.<sep> Nous répondons à une demande pour des systèmes non-électroniques en proposant plusieurs implémentations de vote vérifiable en papier, une panoplie de primitives et un protocole de vote pour les très petites élections.
La règlementation européenne requiert l'évaluation de la reprotoxicité des ingrédients cosmétiques sans avoir recours aux tests sur les organismes définis par la directive européenne sur les animaux utilisés à des fins scientifiques.<sep> Pour cribler par exclusion des substances chimiques, l'industrie cosmétique a besoin de développer une méthode alternative à l'expérimentation animale prédictive et spécifique à l'identification d'agents tératogènes (substances entraînant au cours du développement embryonnaire et de manière définitive des malformations physiques et/ou fonctionnelles).<sep> Pour cela, le poisson zèbre, le poisson médaka et l'amphibien xénope aux stades embryonnaires ont été évalués sur une liste de 43 substances de référence.<sep> Le médaka a été sélectionné pour la fiabilité de son approvisionnement, la robustesse de ses stades embryonnaires lors des manipulations, ainsi que pour les performances du test l'utilisant.<sep> De plus, ce test permet de détecter les agents tératogènes les plus puissants, près de la moitié des 26 substances tératogènes de notre liste.<sep> Son taux de spécificité est fixé à 100% pour identifier correctement les 17 substances négatives dont l'absence d'effets tératogènes chez l'homme ou un organisme modèle mammifère, est avérée.<sep> Néanmoins, les performances de ce test pourraient être améliorées par son automatisation et par l'intégration de la quantification de nouveaux paramètres pour l'identification de malformations fonctionnelles.<sep> Enfin, pour prédire l'innocuité d'une substance chimique chez l'Homme, le test prédictif de tératogénicité utilisant les embryons de médaka doit être intégré dans une stratégie globale d'évaluation de la tératogénicité.
Dans l'industrie l'approche de gestion du cycle de vie du produit (PLM) a été considérée comme une solution essentielle pour améliorer la compétitivité des produits.<sep> Elle vise à fournir une plate-forme commune qui rassemble les différents systèmes de l'entreprise à chaque étape du cycle de vie du produit dans ou à travers les entreprises.<sep> Bien que les principaux éditeurs de logiciels fassent des efforts pour créer des outils offrant un ensemble complet et intégré de systèmes, la plupart d'entre eux n'intègrent pas l'ensemble des systèmes.<sep> Enfin, ils ne fournissent pas une intégration cohérente de l'ensemble du système d'information.<sep> Il en résulte une sorte de « tour de Babel » , où chaque application est considérée comme une île au milieu de l'océan de l'information, gérée par de nombreuses parties prenantes dans une entreprise, ou même dans un réseau d'entreprises.<sep> L'hétérogénéité des parties prenantes augmente le problème d'interopérabilité.<sep> L'objectif de cette thèse est de traiter la question de l'interopérabilité sémantique, en proposant une méthode d'annotation sémantique formelle pour favoriser la compréhension mutuelle de la sémantique de l'information partagée et échangée dans un environnement PLM
Cette thèse porte sur le traitement du langage naturel et l'exploration de texte, à l'intersection de l'apprentissage automatique et de la statistique.<sep> Nous nous intéressons plus particulièrement aux schémas de pondération des termes (SPT) dans le contexte de l'apprentissage supervisé et en particulier à la classification de texte.<sep> Dans la classification de texte, la tâche de classification multi-étiquettes a suscité beaucoup d'intérêt ces dernières années.<sep> La classification multi-étiquettes à partir de données textuelles peut être trouvée dans de nombreuses applications modernes telles que la classification de nouvelles où la tâche est de trouver les catégories auxquelles appartient un article de presse en fonction de son contenu textuel (par exemple, politique, Moyen-Orient, pétrole), la classification du genre musical (par exemple, jazz, pop, oldies, pop traditionnelle) en se basant sur les commentaires des clients, la classification des films (par exemple, action, crime, drame), la classification des produits (par exemple, électronique, ordinateur, accessoires).<sep> La plupart des algorithmes d'apprentissage ne conviennent qu'aux problèmes de classification binaire.<sep> Par conséquent, les tâches de classification multi-étiquettes sont généralement transformées en plusieurs tâches binaires à label unique.<sep> Cependant, cette transformation introduit plusieurs problèmes.<sep> Premièrement, les distributions des termes ne sont considérés qu'en matière de la catégorie positive et de la catégorie négative (c'est-à-dire que les informations sur les corrélations entre les termes et les catégories sont perdues).<sep> Deuxièmement, il n'envisage aucune dépendance vis-à-vis des étiquettes (c'est-à-dire que les informations sur les corrélations existantes entre les classes sont perdues).<sep> Enfin, puisque toutes les catégories sauf une sont regroupées dans une seule catégories (la catégorie négative), les tâches nouvellement créées sont déséquilibrées.<sep> Ces informations sont couramment utilisées par les SPT supervisés pour améliorer l'efficacité du système de classification.<sep> Ainsi, après avoir présenté le processus de classification de texte multi-étiquettes, et plus particulièrement le SPT, nous effectuons une comparaison empirique de ces méthodes appliquées à la tâche de classification de texte multi-étiquette.<sep> Nous constatons que la supériorité des méthodes supervisées sur les méthodes non supervisées n'est toujours pas claire.<sep> Nous montrons ensuite que ces méthodes ne sont pas totalement adaptées au problème de la classification multi-étiquettes et qu'elles ignorent beaucoup d'informations statistiques qui pourraient être utilisées pour améliorer les résultats de la classification.<sep> Nous proposons donc un nouvel SPT basé sur le gain d'information.<sep> Cette nouvelle méthode prend en compte la distribution des termes, non seulement en ce qui concerne la catégorie positive et la catégorie négative, mais également en rapport avec toutes les autres catégories.<sep> Enfin, dans le but de trouver des SPT spécialisés qui résolvent également le problème des tâches déséquilibrées, nous avons étudié les avantages de l'utilisation de la programmation génétique pour générer des SPT pour la tâche de classification de texte.<sep> Contrairement aux études précédentes, nous générons des formules en combinant des informations statistiques à un niveau microscopique (par exemple, le nombre de documents contenant un terme spécifique) au lieu d'utiliser des SPT complets.<sep> De plus, nous utilisons des informations catégoriques telles que (par exemple, le nombre de catégories dans lesquelles un terme apparaît).<sep> Des expériences sont effectuées pour mesurer l'impact de ces méthodes sur les performances du modèle.<sep> Nous montrons à travers ces expériences que les résultats sont positifs.
À une époque où les données, souvent interprétées comme une « réalité terrain » ,sont produites dans des quantités gargantuesques, un besoin de compréhension et d'interprétation de ces données se développe en parallèle.<sep> Les jeux de données étant maintenant principalement relationnels, il convient de développer des méthodes qui permettent d'extraire de l'information pertinente décrivant à la fois les objets et les relations entre eux.<sep> Les règles d'association, adjointes des mesures de confiance et de support, décrivent les co-occurences entre les caractéristiques des objets et permettent d'exprimer et d'évaluer de manière explicite l'information contenue dans un jeu de données.<sep> Dans cette thèse, on présente et développe l'analyse relationnelle de concepts pour extraire des règles traduisant tant les caractéristiques propres d'un ensemble d'objets que les liens avec d'autres ensembles.<sep> Une première partie développe la théorie mathématique de la méthode, alors que la seconde partie propose trois cas d'application pour étayer l'intérêt d'un tel développement.<sep> Les études sont réalisées dans des domaines variés montrant ainsi la polyvalence de la méthode : un premier cas traite l'analyse d'erreur en production industrielle métallurgique, un second cas est réalisé en psycholinguistique pour l'analyse de dictionnaires et un dernier cas montre les possibilités de la méthode en ingénierie de connaissance.
Le but de cette thèse est de minimiser la perte de performance d'un système de détection lorsqu'il rencontre un changement de distribution de données à la suite d'un événement connu (maintenance, ajout de capteur etc.).<sep> L'idée est d'utiliser l'approche d'apprentissage par transfert pour exploiter l'information apprise avant l'événement pour adapter le détecteur au système modifié.<sep> Il utilise un paramètre pour équilibrer la quantité d'informations apportées par l'ancien système par rapport au nouveau.<sep> Ce modèle est formalisé de manière à pouvoir être résolu par un SVM mono-classe classique avec une matrice de noyau spécifique.<sep> Les expériences menées dans le cas de changement de distribution et d'ajout de capteurs montrent que ce modèle permet une transition en douceur de l'ancien système vers le nouveau.<sep> De plus, comme le modèle proposé peut être formulé comme un SVM mono-classe classique, des algorithmes d'apprentissage en ligne pour SVM mono-classe sont étudiés dans le but d'obtenir un taux de fausses alarmes stable au cours de la phase de transition.<sep> Ils peuvent être appliqués directement à l'apprentissage en ligne du modèle proposé
L'expansion du web et le développement des technologies de l'information ont contribué à la prolifération des documents numériques en ligne.<sep> Un premier problème est lié à l'extraction de l'information utile parmi celle qui est disponible.<sep> Un second problème concerne l'appropriation de ces connaissances qui parfois, se traduit par du plagiat.<sep> L'objectif de cette thèse est le développement d'un modèle permettant de mieux caractériser les documents afin d'en faciliter l'accès mais aussi de détecter ceux présentant un risque de plagiat.<sep> Ce modèle s'appuie sur des ontologies de domaine pour la classification des documents et pour le calcul de la similarité des documents appartenant à un même domaine.<sep> Nous nous intéressons plus spécifiquement aux articles scientifiques, et notamment à leurs résumés, textes courts et relativement structurés.<sep> Il s'agit dès lors de déterminer comment évaluer la proximité/similarité sémantique de deux articles à travers l'examen de leurs résumés respectifs.<sep> Considérant qu'une ontologie de domaine regroupe les connaissances relatives à un domaine scientifique donné, notre processus est basé sur deux actions : (i)<sep> Une classification automatique des documents dans un domaine choisi parmi plusieurs domaines candidats.<sep> Cette classification détermine le sens d'un document à partir du contexte global dans lequel s'inscrit son contenu.<sep> La comparaison sémantique des résumés s'appuie sur une segmentation de leur contenu respectif en zones, unités documentaires, reflétant leur structure logique.
Les nombres à virgule flottante ne représentent qu'un sous-ensemble des nombres réels.<sep> De ce fait, l'arithmétique à virgule flottante introduit des approximations qui sont susceptibles de se cumuler et d'avoir un impact significatif sur les simulations numériques.<sep> Nous introduisons une nouvelle façon d'estimer et de localiser les sources d'erreur numérique dans une application et fournissons une implémentation de référence, la bibliothèque Shaman.<sep> Notre méthode utilise une arithmétique dédiée sur un type qui encapsule à la fois le résultat des calculs (identique à la version non instrumentée du code) et une approximation de son erreur numérique.<sep> Nous pouvons ainsi mesurer le nombre de chiffres significatifs de tout résultat ou résultat intermédiaire dans une simulation.<sep> Nous montrons que notre approche, bien que simple, donne des résultats compétitifs avec l'état de l'art.<sep> Qui plus est, elle a un surcoût en temps de calcul moins important et est compatible avec le parallélisme, ce qui la rend appropriée pour l'étude de larges applications.
Une analyse sémantique robuste des scènes extérieures est difficile en raison des changements environnementaux causés par l'éclairage et les conditions météorologiques variables, ainsi que par la variation des types d'objets rencontrés.<sep> Cette thèse étudie le problème de la segmentation sémantique à l'aide de l'apprentissage profond et avec des d'images de différentes modalités.<sep> Les images capturées à partir de diverses modalités d'acquisition fournissent des informations complémentaires pour une compréhension complète de la scène.<sep> Nous proposons des solutions efficaces pour la segmentation supervisée d'images multimodales, de même que pour la segmentation semi-supervisée de scènes routières en extérieur.<sep> Concernant le premier cas, nous avons proposé un réseau de fusion multi-niveaux pour intégrer des images couleur et polarimétriques.<sep> Une méthode de fusion centrale a également été introduite pour apprendre de manière adaptative les représentations conjointes des caractéristiques spécifiques aux modalités et réduire l'incertitude du modèle via un post-traitement statistique.<sep> Dans le cas de la segmentation semi-supervisée, nous avons d'abord proposé une nouvelle méthode de segmentation basée sur un réseau prototypique, qui utilise l'amélioration des fonctionnalités multi-échelles et un mécanisme d'attention.<sep> Des évaluations empiriques complètes sur différentes bases de données de référence montrent que les algorithmes proposés atteignent des performances supérieures en termes de précision et démontrent le bénéfice de l'emploi de modalités complémentaires pour l'analyse de scènes extérieures dans le cadre de la navigation autonome.
Cette étude vise à fournir une description détaillée de l'évidentialité en anglais dans une approche contrastive avec le tibétain.<sep> Elle repose sur un corpus spécialement recueilli au Tibet et en Angleterre (TSC et CSC/LAC, 2010-2012, 10 h.).<sep> Le tibétain possède un système évidentiel complexe et grammaticalisé dont la description peut fournir une grille d'analyse préliminaire pour notre étude de l'évidentialité en anglais.<sep> Des exemples authentiques et des données quantitatives issus du corpus nous permettent d'illustrer et de compléter les descriptions du groupe verbal tibétain dans plusieurs travaux pionniers antérieurs à la présente recherche (Tournadre and Sangda Dorje 1998) afin de poser les bases de la sémantique évidentielle.<sep> Les marqueurs évidentiels qui émergent dans les parties tibétaine et anglaise du corpus sont examinés afin de déterminer les paramètres qui motivent leur utilisation.<sep> Les marqueurs tibétains sont principalement grammaticaux et intégrés à des paradigmes syntaxiques : copules, suffixes verbaux et enclitiques.<sep> Les marqueurs évidentiels anglais sont lexicaux ou semi-grammaticaux : verbes de perception, verbes de cognition, verbes de discours, modaux, adverbes, conjoints, parenthétiques et marqueurs de discours (Nuyts 2001a, Cappelli 2007, Sanders and Sweetser 2009, Mortensen 2010, Whitt 2010, Gisborne 2010, Miller 2008, Boulonnais 2010, Gurajek 2010, Kaltenböck et coll. 2011, Heine 2013).<sep> Cette description de l'évidentialité en tibétain et en anglais nous permet d'analyser les conséquences d'un rendu grammatical ou lexical de cette notion (Talmy 2000, Bybee et coll. 1994, Nuyts 2001a, Boye and Harder 2009).<sep> Des analyses quantitatives et qualitatives du corpus contrastif et sur large corpus illustrent les différences de complexité, d'optionalité, de fréquence, de restriction sémantique, de prise en charge, de statut informatif, et de stratégie de discours dans les deux systèmes.<sep> Cette étude ne remet pas en question que le système évidentiel tibétain est plus grammaticalisé que le système anglais, mais démontre que ce dernier présente tous les signes d'une grammaticalisation partielle.<sep> Nous concluons ainsi que l'évidentialité est une notion pertinente et nécessaire pour une description linguistique complète de l'anglais.
La phrase, désormais analysée en « noyau + affixes » , est en effet avant tout une unité de traitement et de production n'ayant qu'une existence temporaire dans le flux de l'activité productive et interprétative.<sep> Nous nous sommes donc intéressée plus particulièrement aux différents fonctionnements des segments sans verbe dans un corpus oral.<sep> Un premier ensemble rassemble les segments sans verbe fonctionnant en tant que phrases selon des degrés de prédicativité plus au moins élevés : prédications averbales, prédications interjectives et réalisations averbales d'une prédication verbale implicite.<sep> Cependant, pour de nombreux segments sans verbe, l'interprétation est moins sûre et ces segments dits « flottants » , en marge des emplois canoniques, nous renvoient à la problématique de la phrase.<sep> C'est par exemple le cas de compléments différés, constituants averbaux oscillant entre trois pôles : éléments intégrés, éléments détachés et éléments autonomes.
Les WMNs comportent des noeuds qui sont capables de recevoir et de transmettre des données vers de multiples destinations dans le réseau.<sep> De ce fait, les WMNs sont capables de s'auto-organiser et auto-configurer dynamiquement [5].<sep> Chaque noeud crée et maintient la connectivité avec ses voisins.<sep> La disponibilité du mode ad-hoc basée sur la norme IEEE 802.11 permet une mise en œuvre de WMNs à faible coût.<sep> Les WMNs présentent cependant deux inconvénients majeurs liés aux interférences d'une part et à la scalabilité d'autre part [6].<sep> (D1) Le problème des interférences<sep> (D2) Le problème de scalabilité.<sep> D'un autre côté, le problème de scalabilité dans les WMNs peut être résolu par les solutions de routage efficaces [11].<sep> En effet, les algorithmes de routage dans les WMNs sont chargés de calculer des routes pour transporter des données de multiples sauts jusqu'à atteindre les destinations.<sep> Comme illustré dans [11], les routes les plus courtes, qui sont les solutions par défaut des algorithmes de routage classiques, ont généralement plus d'interférences.<sep> En conséquences, il faut trouver des routes qui ont moins d'interférences.<sep> Pour un objectif de routage donné et des paramètres donnés, ces routes peuvent être optimales ou sub-optimales.<sep> Les paramètres dans les problèmes de routage comprennent des métriques orientées réseau et des métriques orientées utilisateur.<sep> Les métriques orientées réseau, également appelées les métriques de la qualité de service (QoS), sont dérivées à partir des paramètres réseau comme la bande passante, le délai, la gigue, etc.<sep> En revanche, les métriques orientées vers l'utilisateur, également appelées les métriques de qualité d'expérience (QoE), sont basées sur l'expérience de l'utilisateur, tels que les notes MOS (Mean Opinion Score).<sep> La perception de l'utilisateur est un objectif majeur des services de streaming vidéo.<sep> La plupart des algorithmes de routage existants prennent des décisions de routage en fonction d'une seule ou d'une combinaison des métriques orientées réseau.<sep> Ainsi, les algorithmes de routage dans [12, 13, 14] déterminent les routes basées sur la bande passante et la charge du réseau.<sep> Cependant, les métriques orientées réseau ne sont pas nécessairement corrélée à l'expérience de l'utilisateur [15, 16, 17, 18].<sep> En d'autres termes, les utilisateurs peuvent ne pas être satisfaits même avec les routes optimales qui sont basées sur les métriques orientés réseau.<sep> En conséquences, il est nécessaire de développer les algorithmes de routage qui tiennent compte de métriques orientées utilisateur.<sep> Cette thèse traite d'algorithmes de routage dans les WMNs avec comme objectif d'améliorer la qualité pour les applications de streaming vidéo.<sep> Les algorithmes de routage proposés prendront des décisions de routage basées sur la perception de l'utilisateur.<sep> Dans ce contexte, toutes les solutions doivent faire face aux deux challenges suivants : (M1) l'estimation en temps réel de la perception utilisateur et (M2) découverte des routes optimales ou sous-optimales.
Résumé : Les progrès technologiques ont permis aux véhicules électriques d'avoir des capacités à la fois de calcul, de communication, de stockage et de perception.<sep> Néanmoins, la plupart du temps, ces véhicules électriques sont en stationnement, ce qui engendre une sous utilisation de leurs capacités embarquées.<sep> Ainsi, une meilleure gestion et une mise en commun de ces ressources sous-utilisées deviennent fortement recommandées.<sep> Les ressources agrégées seraient utiles pour des applications de sécurité routière, des applications liées au confort ou pourraient même être utilisées en tant que centre de calcul distribué.<sep> En outre, les véhicules en stationnement pourraient également être utilisés comme plate-forme de fourniture de services.<sep> Par conséquent, l'utilisation de ressources abondantes agrégées pour le déploiement de différentes applications mobiles locales a conduit au développement du concept d'informatique en brouillard véhiculaire (an anglais, Vehicular Fog Computing - VFC).<sep> Grâce à ce dernier, les véhicules dans les aires de stationnement, les centres commerciaux ou les aéroports vont agir en tant que nœuds fog.<sep> Dans un autre contexte, les applications mobiles sont devenues de plus en plus populaires, complexes et gourmandes en ressources.<sep> Certaines applications mobiles nécessitent des capacités de calcul intensives et une consommation d'énergie élevée qui transcendent les capacités limitées des appareils mobiles.<sep> Tout au long de ce travail, nous abordons les verrous liés au déploiement efficace d'un système VFC agrégeant les ressources inutilisées des véhicules électriques en stationnement pour être utilisées comme nœuds fogs répondants aux demandes de calcul des utilisateurs mobiles à proximité.<sep> Notre travail commence par un état de l'art sur les véhicules électriques et l'allocation de ressources dans le système VFC.<sep> En outre, nous évaluons le potentiel des ressources agrégées dans les véhicules électriques pour répondre aux demandes d'applications d'utilisateurs mobiles locaux en prenant en compte l'état de santé de la batterie (en anglais, State of Health - SOH) et son état de charge (en anglais, State of Charge - SOC).<sep> Nous abordons, par la suite, le problème d'allocation de ressources avec une nouvelle solution basée sur le processus de décision Markovien (en anglais, Markov Decision Process- MDP) qui vise à optimiser l'utilisation de l'énergie des véhicules électriques pour répondre à la fois à aux demandes de calcul et de mobilité des utilisateurs.<sep> Enfin, nous proposons une approche basée sur un jeu stochastique pour montrer la dynamique de la demande de calcul des utilisateurs mobiles et la disponibilité des ressources des véhicules électriques.
Notre recherche porte essentiellement sur des tâches ayant une finalité applicative : détection de la dépression et de l'anorexie d'une part et détection de l'agressivité d'autre part ; cela à partir de messages postés par des utilisateurs de plates-formes de type réseaux sociaux.<sep> Nous avons également proposé une méthode non supervisée d'extraction de termes-clés.<sep> Plus précisément, nous améliorons une méthode non supervisée à base de graphes.<sep> Nous avons évalué notre approche sur onze collections de données dont cinq contenant des documents longs, quatre contenants des documents courts et enfin deux contenant des documents de type article de presse.<sep> Nous avons montré que notre proposition permet d'améliorer les résultats dans certains contextes.<sep> La deuxième contribution de cette thèse est une solution pour la détection au plus tôt de la dépression et de l'anorexie.<sep> Nous avons proposé des modèles utilisant des classifieurs, s'appuyant sur la régression logistique ou les forêts d'arbres de décision, basés sur (a) des caractéristiques et (b) le plongement de phrases.<sep> Nous avons évalué nos modèles sur les collections de données de la tâche eRisk.<sep> Nous avons observé que les modèles basés sur les caractéristiques sont très performants lorsque la mesure de précision est considérée, soit pour la détection de la dépression, soit pour la détection de l'anorexie.<sep> Le modèle utilisant le plongement de phrases, quant à lui, est plus performant lorsque l'on mesure la détection au plus tôt (ERDE_50) et le rappel.<sep> Nous avons aussi obtenu de bons résultats par rapport à l'état de l'art : meilleurs résultats sur la précision et ERDE_50 pour la détection de la dépression, et sur la précision et le rappel pour la détection de l'anorexie.<sep> Notre dernière contribution concerne la détection de l'agression dans les messages postés par des utilisateurs sur les réseaux sociaux.<sep> Nous avons réutilisé les mêmes modèles que ceux utilisés pour la détection de la dépression ou de l'anorexie.<sep> À cela, nous avons ajouté d'autres modèles basés sur l'apprentissage profond.<sep> Nous avons évalué nos modèles sur les collections de données de la tâche internationale TRAC.<sep> Nous avons observé que nos modèles, utilisant l'apprentissage profond, fournissent de meilleurs résultats que nos modèles utilisant des classifieurs classiques.<sep> Nos résultats dans cette partie de la thèse sont comparables à l'état de l'art du domaine.<sep> Nous avons toutefois obtenu le meilleur résultat sur une des collections de données.
Dans de nombreux problèmes d'apprentissage statistique, il est nécessaire de pouvoir comparer des données qui peuvent se représenter sous la forme de mesures de probabilités ou d'histogrammes.<sep> Il existe des exemples variés qui incluent l'analyse de nuages de mots pour l'étude du langage, la vision par ordinateur, la catégorisation d'images ou bien l'étude de bio-marqueurs en bio-informatique.<sep> L'utilisation de la notion de distance de Wasserstein associée au problème de transport optimal entre des mesures de probabilités est depuis récemment un outil privilégié pour la comparaison de ce type de données qui permet d'atteindre les performances à l'état de l'art pour de nombreuses applications.<sep> Une problématique centrale dans l'utilisation de distances de Wasserstein en apprentissage statistique est le coût du calcul numérique (ou de l'approximation) du transport optimal entre deux mesures de probabilités.<sep> Pour contourner cette difficulté dans l'utilisation des distances de Wasserstein pour l'analyse de données, il a été récemment introduit des estimateurs d'une distance de transport (possiblement régularisée) basés sur des algorithmes de gradient stochastique.<sep> Le sujet de cette thèse porte sur l'étude des propriétés de tels algorithmes d'optimisation stochastique pour le transport optimal et leurs applications en apprentissage statistique.<sep> On s'intéressera en particulier aux propriétés statistiques et computationnelles de ce type d'approche pour le calcul de barycentres et de l'analyse en composantes principales (ACP) géodésique dans l'espace de Wasserstein dans un contexte de données de grande dimension.<sep> Le sujet de thèse est de nature à la fois théorique et numérique.<sep> Les principales notions abordées feront appel à des outils de probabilité, statistique et d'optimisation avec des applications possibles en traitement de données en bio-informatique.
Cette thèse s'inscrit dans les domaines de l'économie publique et de l'économie politique et s'articule autour de deux axes.<sep> Les premier et deuxième chapitres se concentrent sur les politiques de redistribution.<sep> Plus précisément, ils présentent des contributions à la théorie de l'impôt sur le revenu et adoptent une perspective normative et une perspective politique.<sep> Les troisième et quatrième chapitres contribuent à fournir une meilleure compréhension des forces politiques qui guident les réformes dans les unions de plusieurs Etats telles que l'Union Européenne.<sep> Ils se concentrent sur les préférences d'individus qui sont d'une grande importance dans les processus décisionnels mais sur lesquels nous avons peu d'information directe : les hommes et femmes politiques.<sep> Mes premier et deuxième chapitres cherchent à enrichir un modèle standard de taxation optimale, afin de mieux tenir compte de contextes institutionnels et sociaux.<sep> Le Chapitre 1 se penche sur la question de la conception d'un système d'imposition lorsque les liens entre individus sont pris en compte.<sep> En effet, jusqu'à présent, la théorie de l'imposition des revenus s'est toujours penchée sur la redistribution entre individus séparés (ou couples).<sep> Mais dans de nombreux contextes, les individus sont liés à leur famille, leurs amis, les membres de leur village ou de leur communauté, et leur font régulièrement des transferts.<sep> Ces transferts vont des plus riches aux plus pauvres, et représentent donc une forme de redistribution informelle.<sep> Ma question de recherche est la suivante : comment l'existence de ces transferts informels mais redistributifs devrait-elle affecter la conception des systèmes d'imposition ?<sep> Le Chapitre 2 étudie caractéristique saillante de nombreux pays développés : la proportion importante de la population qui ne paie pas l'impôt sur le revenu.<sep> En utilisant les outils de la littérature fiscale normative que nous appliquons à un cadre d'économie politique, nous étudions l'économie politique des réformes fiscales non linéaires, ce qui nous aide à comprendre pourquoi une part si importante de la population est exonérée de l'impôt sur le revenu dans certains pays.<sep> Même si les deux chapitres sont théoriques, j'utilise des données administratives et d'enquête pour illustrer et tirer des conclusions concrètes de mes modèles.<sep> A l'avenir, nous comptons poursuivre ces enquêtes régulièrement afin de mieux comprendre la dynamique de l'Union européenne et de fournir une perspective académique aux débats actuels de l'UE avec des perspectives.<sep> Les troisième et quatrième Chapitres étudient ainsi l'opinion des députés sur des politiques qui, à la lumière des débats actuels et des dix dernières années, revêtent une importance particulière : les politiques du marché du travail et l'Union monétaire européenne.<sep> Ils offrent une nouvelle approche aux questions d'économie politique en s'attachant aux opinions d'un ensemble d'acteurs importants dans les processus de prise de décision : les politiques, pour qui, en dehors de leurs votes et déclarations publics, nous avons peu d'information directe.<sep> Dans ces chapitres, nous cherchons à démêler lequel des deux facteurs est le plus important dans les différences observées : culturel ou idéologique ?<sep> Étonnamment, nous constatons que pour une majorité de questions, les réponses reflètent davantage un fossé idéologique qu'un fossé franco-allemand.<sep> Par exemple, la création d'une assurance chômage commune et une plus grande flexibilité du marché du travail mettent en évidence un fossé beaucoup plus l'idéologique que national.<sep> Ces résultats peuvent contribuer à mettre en lumière de potentielles orientations d'intégration européenne qui pourraient s'avérer fructueuses.