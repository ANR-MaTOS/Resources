<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2018GREAM007. segId begin by 1, tuid = segId</note>
        <docid>2018GREAM007</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>L'accroissement rapide des données numériques vidéographiques fait de la compréhension automatique des vidéos un enjeu de plus en plus important.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>With the rapid growth of digital video content, automatic video understanding has become an increasingly important task.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Comprendre de manière automatique une vidéo recouvre de nombreuses applications, parmi lesquelles l'analyse du contenu vidéo sur le web, les véhicules autonomes,les interfaces homme-machine (eg, Kinect).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Video understanding spans several applications such as web-video content analysis, autonomous vehicles, human-machine interfaces (eg, Kinect).</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Cette thèse présente des contributions dans deux problèmes majeurs pour la compréhension automatique des vidéos : la détection d'actions supervisée par des données web, et la localisation d'actions humaines.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis makes contributions addressing two major problems in video understanding: webly-supervised action detection and human action localization.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>La détection d'actions supervisées par des données web a pour objectif d'apprendre à reconnaître des actions dans des contenus vidéos sur Internet, sans aucune autre supervision.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Webly-supervised action recognition aims to learn actions from video content on the internet, with no additional supervision.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Nous proposons une approche originale dans ce contexte, qui s'appuie sur la synergie entre les données visuelles (les vidéos) et leur description textuelle associée, et ce dans le but d'apprendre des classifieurs pour les événements sans aucune supervision.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose a novel approach in this context, which leverages the synergy between visual video data and the associated textual metadata, to learn event classifiers with no manual annotations.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Nous montrons l'importance des deux étapes principales, c'est-à-dire la créations des requêtes et l'étape de suppression des vidéos, par des résutats quantitatifs.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We show the importance of both the main steps of our method, ie,query generation and data pruning, with quantitative results.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Notre approche est évaluée dans des conditions difficiles, où aucune annotation manuelle n'est disponible, dénotées EK0 dans les challenges TrecVid. Nous obtenons l'état de l'art sur les bases de données MED 2011 et 2013.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We evaluate this approach in the challenging setting where no manually annotated training set is available, i.e., EK0 in the TrecVid challenge, and show state-of-the-art results on MED 2011 and 2013 datasets.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Dans la seconde partie de notre thèse, nous nous concentrons sur la localisation des actions humaines, ce qui implique de reconnaître à la fois les actions se déroulant dans la vidéo, comme par exemple "boire" ou "téléphoner", et leur étendues spatio-temporelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the second part of the thesis, we focus on human action localization, which involves recognizing actions that occur in a video, such as "drinking" or "phoning", as well as their spatial and temporal extent.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Nous proposons une nouvelle méthode centrée sur la personne, traquant celle-ci dans les vidéos pour en extraire des tubes encadrant le corps entier, même en cas d'occultations ou dissimulations partielles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose a new person-centric framework for action localization that tracks people in videos and extracts full-body human tubes, i.e., spatio-temporal regions localizing actions, even in the case of occlusions or truncations.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Deux raisons motivent notre approche.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The motivation is two-fold.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>La première est qu'elle permet de gérer les occultations et les changements de points de vue de la caméra durant l'étape de localisation des personnes, car celle-ci estime la position du corps entier à chaque frame.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>First, it allows us to handle occlusions and camera viewpoint changes when localizing people, as it infers full-body localization.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Notre algorithme de tracking connecte les détections temporellement pour extraire des tubes encadrant le corps entier.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our tracking algorithm connects the image detections temporally to extract full-body human tubes.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Nous évaluons notre nouvelle méthode d'extraction de tubes sur une base de données difficile, DALY, et atteignons l'état de l'art.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We evaluate our new tube extraction method on a recent challenging dataset, DALY, showing state-of-the-art results.</seg>
            </tuv>
        </tu>
    </body>
</tmx>