<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020LYSEI045. segId begin by 1, tuid = segId</note>
        <docid>2020LYSEI045</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Avec l’augmentation massive du contenu vidéo sur Internet et au-delà, la compréhension automatique du contenu visuel pourrait avoir un impact sur de nombreux domaines d’application différents tels que la robotique, la santé, la recherche de contenu ou le filtrage.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>With the massive increase of video content on Internet and beyond, the automatic understanding of visual content could impact many different application fields such as robotics, health care, content search or filtering.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Le but de cette thèse est de fournir des contributions méthodologiques en vision par ordinateur et apprentissage statistique pour la compréhension automatique du contenu des vidéos.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The goal of this thesis is to provide methodological contributions in Computer Vision and Machine Learning for automatic content understanding from videos.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Nous mettons l’accent sur les problèmes de la reconnaissance de l’action humaine à grain fin et du raisonnement visuel à partir des interactions entre objets.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We emphasis on problems, namely fine-grained human action recognition and visual reasoning from object-level interactions.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Dans la première partie de ce manuscrit, nous abordons le problème de la reconnaissance fine de l’action humaine.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the first part of this manuscript, we tackle the problem of fine-grained human action recognition.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Nous introduisons deux différents mécanismes d’attention, entrainés sur le contenu visuel à partir de la pose humaine articulée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We introduce two different trained attention mechanisms on the visual content from articulated human pose.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Une première méthode est capable de porter automatiquement l’attention sur des points pré-sélectionnés importants de la vidéo, conditionnés sur des caractéristiques apprises extraites de la pose humaine articulée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first method is able to automatically draw attention to important pre-selected points of the video conditioned on learned features extracted from the articulated human pose.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Nous montrons qu’un tel mécanisme améliore les performances sur la tâche finale et fournit un bon moyen de visualiser les parties les plus discriminantes du contenu visuel.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We show that such mechanism improves performance on the final task and provides a good way to visualize the most discriminative parts of the visual content.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Une deuxième méthode va au-delà de la reconnaissance de l’action humaine basée sur la pose.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second method goes beyond pose-based human action recognition.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Nous développons une méthode capable d’identifier automatiquement un nuage de points caractéristiques non structurés pour une vidéo à l’aide d’informations contextuelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We develop a method able to automatically identify unstructured feature clouds of interest in the video using contextual information.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>De plus, nous introduisons un système distribué entrainé pour agréger les caractéristiques de manière récurrente et prendre des décisions de manière distribuée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Furthermore, we introduce a learned distributed system for aggregating the features in a recurrent manner and taking decisions in a distributed way.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Nous démontrons que nous pouvons obtenir de meilleures performances que celles illustrées précédemment, sans utiliser d’informations de pose articulée au moment de l’inférence.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We demonstrate that we can achieve a better performance than obtained previously, without using articulated pose information at test time.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Dans la deuxième partie de cette thèse, nous étudions les représentations vidéo d’un point de vue objet.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the second part of this thesis, we investigate video representations from an object-level perspective.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Étant donné un ensemble de personnes et d’objets détectés dans la scène, nous développons une méthode qui a appris à déduire les interactions importantes des objets à travers l’espace et le temps en utilisant uniquement l’annotation au niveau vidéo.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Given a set of detected persons and objects in the scene, we develop a method which learns to infer the important object interactions through space and time using the video-level annotation only.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Cela permet d’identifier une interaction inter-objet importante pour une action donnée ainsi que le biais potentiel d’un ensemble de données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>That allows to identify important objects and object interactions for a given action, as well as potential dataset bias.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Enfin, dans une troisième partie, nous allons au-delà de la tâche de classification et d’apprentissage supervisé à partir de contenus visuels, en abordant la causalité à travers les interactions, et en particulier le problème de l’apprentissage contrefactuel.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, in a third part, we go beyond the task of classification and supervised learning from visual content by tackling causality in interactions, in particular the problem of counterfactual learning.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Nous introduisons une nouvelle base de données, à savoir CoPhy, où, après avoir regardé une vidéo, la tâche consiste à prédire le résultat après avoir modifié la phase initiale de la vidéo.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We introduce a new benchmark, namely CoPhy, where, after watching a video, the task is to predict the outcome after modifying the initial stage of the video.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Nous développons une méthode basée sur des interactions au niveau des objets capables d’inférer les propriétés des objets sans supervision ainsi que les emplacements futurs des objets après l’intervention.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We develop a method based on object-level interactions able to infer object properties without supervision as well as future object locations after the intervention.</seg>
            </tuv>
        </tu>
    </body>
</tmx>