<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2013PA112103. segId begin by 1, tuid = segId</note>
        <docid>2013PA112103</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Depuis 2006, les algorithmes d’apprentissage profond qui s’appuient sur des modèles comprenant plusieurs couches de représentations ont pu surpasser l’état de l’art dans plusieurs domaines.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Since 2006, deep learning algorithms which rely on deep architectures with several layers of increasingly complex representations have been able to outperform state-of-the-art methods in several settings.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Les modèles profonds peuvent être très efficaces en termes du nombre de paramètres nécessaires pour représenter des opérations complexes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Deep architectures can be very efficient in terms of the number of parameters required to represent complex operations which makes them very appealing to achieve good generalization with small amounts of data.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Bien que l’entraînement des modèles profonds ait été traditionnellement considéré comme un problème difficile, une approche réussie a été d’utiliser une étape de pré-entraînement couche par couche, non supervisée, pour initialiser des modèles profonds supervisés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Although training deep architectures has traditionally been considered a difficult problem, a successful approach has been to employ an unsupervised layer-wise pre-training step to initialize deep supervised models.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Tout d’abord, l’apprentissage non-supervisé présente de nombreux avantages par rapport à la généralisation car il repose uniquement sur des données non étiquetées qu’il est facile de trouver.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>First, unsupervised learning has many benefits w.r.t. generalization because it only relies on unlabeled data which is easily found.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Deuxièmement, la possibilité d’apprendre des représentations couche par couche, au lieu de toutes les couches à la fois, améliore encore la généralisation et réduit les temps de calcul.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Second, the possibility to learn representations layer by layer instead of all layers at once improves generalization further and reduces computational time.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Cependant, l’apprentissage profond pose encore beaucoup de questions relatives à la consistance de l’apprentissage couche par couche, avec de nombreuses couches, et à la difficulté d’évaluer la performance, de sélectionner les modèles et d’optimiser la performance des couches.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However, deep learning is a very recent approach and still poses a lot of theoretical and practical questions concerning the consistency of layer-wise learning with many layers and difficulties such as evaluating performance, performing model selection and optimizing layers.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Dans cette thèse, nous examinons d’abord les limites de la justification variationnelle actuelle pour l’apprentissage couche par couche qui ne se généralise pas bien à de nombreuses couches et demandons si une méthode couche par couche peut jamais être vraiment consistante.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this thesis we first discuss the limitations of the current variational justification for layer-wise learning which does not generalize well to many layers.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Nous constatons que l’apprentissage couche par couche peut en effet être consistant et peut conduire à des modèles génératifs profonds optimaux.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We ask if a layer-wise method can ever be truly consistent, i.e. capable of finding an optimal deep model by training one layer at a time without knowledge of the upper layers.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Pour ce faire, nous introduisons la borne supérieure de la meilleure probabilité marginale latente (BLM upper bound), un nouveau critère qui représente la log</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We find that layer-wise learning can in fact be consistent and can lead to optimal deep generative models.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Nous prouvons que la maximisation de ce critère pour chaque couche conduit à une architecture profonde optimale, à condition que le reste de l’entraînement se passe bien.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We prove that maximizing this criterion for each layer leads to an optimal deep architecture, provided the rest of the training goes well.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Bien que ce critère ne puisse pas être calculé de manière exacte, nous montrons qu’il peut être maximisé efficacement par des auto-encodeurs quand l’encodeur du modèle est autorisé à être aussi riche que possible.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Although this criterion cannot be computed exactly, we show that it can be maximized effectively by auto-encoders when the encoder part of the model is allowed to be as rich as possible.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Cela donne une nouvelle justification pour empiler les modèles entraînés pour reproduire leur entrée et donne de meilleurs résultats que l’approche variationnelle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This gives a new justification for stacking models trained to reproduce their input and yields better results than the state-of-the-art variational approach.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>En outre, nous donnons une approximation calculable de la BLM upper bound et montrons qu’elle peut être utilisée pour estimer avec précision la log-vraisemblance finale des modèles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Additionally, we give a tractable approximation of the BLM upper-bound and show that it can accurately estimate the final log-likelihood of models.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Nous proposons une nouvelle méthode pour la sélection de modèles couche par couche pour les modèles profonds, et un nouveau critère pour déterminer si l’ajout de couches est justifié.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Taking advantage of these theoretical advances, we propose a new method for performing layer-wise model selection in deep architectures, and a new criterion to assess whether adding more layers is warranted.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Quant à la difficulté d’entraîner chaque couche, nous étudions aussi l’impact des métriques et de la paramétrisation sur la procédure de descente de gradient couramment utilisée pour la maximisation de la vraisemblance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>As for the difficulty of training layers, we also study the impact of metrics and parametrization on the commonly used gradient descent procedure for log-likelihood maximization.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Nous montrons que la descente de gradient est implicitement liée à la métrique de l’espace sous-jacent et que la métrique Euclidienne peut souvent être un choix inadapté car elle introduit une dépendance sur la paramétrisation et peut entraîner une violation de la symétrie.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We show that gradient descent is implicitly linked with the metric of the underlying space and that the Euclidean metric may often be an unsuitable choice as it introduces a dependence on parametrization and can lead to a breach of symmetry.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Pour pallier ce problème, nous étudions les avantages du gradient naturel et montrons qu’il peut être utilisé pour restaurer la symétrie, mais avec un coût de calcul élevé.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To mitigate this problem, we study the benefits of the natural gradient and show that it can restore symmetry, regrettably at a high computational cost.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>Nous proposons donc qu’une paramétrisation centrée peut rétablir la symétrie avec une très faible surcharge computationnelle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We thus propose that a centered parametrization may alleviate the problem with almost no computational overhead.</seg>
            </tuv>
        </tu>
    </body>
</tmx>