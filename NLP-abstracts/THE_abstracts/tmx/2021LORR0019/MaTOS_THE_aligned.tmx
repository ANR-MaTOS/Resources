<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2021LORR0019. segId begin by 1, tuid = segId</note>
        <docid>2021LORR0019</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Nous traitons dans cette thèse la modélisation de la coarticulation par les réseaux de neurones, dans l’objectif de synchroniser l’animation d’un visage virtuel 3D à de la parole.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis deals with neural network based coarticulation modeling, and aims to synchronize facial animation of a 3D talking head with speech.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Nous proposons dans cette thèse un modèle de coarticulation, c’est-à-dire un modèle qui prédit les trajectoires spatiales des articulateurs à partir de la parole.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose in this work a coarticulation model, i.e. a model able to predict spatial trajectories of articulators from speech.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Nous exploiterons pour cela un modèle séquentiel, les réseaux de neurones récurrents (RNN), et plus particulièrement les Gated Recurrent Units, capables de considérer la dynamique de l’articulation au cœur de leur modélisation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We rely on a sequential model, the recurrent neural networks, and more specifically the Gated Recurrent Units, which are able to consider the articulation dynamic as a central component of its modeling.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Malheureusement, la quantité de données classiquement disponible dans les corpus articulatoires et audiovisuels semblent de prime-abord faibles pour une approche deep learning.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Unfortunately, the typical amount of data in articulatory and audiovisual databases seems to be quite low for a deep learning approach.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Pour pallier cette difficulté, nous proposons une stratégie permettant de fournir au modèle des connaissances sur les gestes articulatoires du locuteur dès son initialisation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To overcome this difficulty, we propose to integrate articulatory knowledge into the networks during its initialization.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>La robustesse des RNNs nous a permis d’implémenter notre modèle de coarticulation pour prédire les mouvements des lèvres pour le français et l’allemand, et de la langue pour l’anglais et l’allemand.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The RNNs robustness allow uw to apply our coarticulation model to predict both face and tongue movements, in french and german for the face, and in english and german for the tongue.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>L’évaluation du modèle fut réalisée par le biais de mesures objectives de la qualité des trajectoires et par des expériences permettant de valider la bonne réalisation des cibles articulatoires critiques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Evaluation has been conducted through objective measures of the trajectories, and through experiments to ensure a complete reach of critical articulatory targets.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Nous avons également réalisé une évaluation perceptive de la qualité de l’animation des lèvres du visage parlant.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We also conducted a subjective evaluation to attest the perceptual quality of the predicted articulation once applied to our facial animation system.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Enfin, nous avons conduit une analyse permettant d’explorer les connaissances phonétiques acquises par le modèle après apprentissage.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, we analyzed the model after training to explore phonetic knowledges learned.</seg>
            </tuv>
        </tu>
    </body>
</tmx>