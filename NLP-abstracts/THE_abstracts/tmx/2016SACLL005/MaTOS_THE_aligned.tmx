<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2016SACLL005. segId begin by 1, tuid = segId</note>
        <docid>2016SACLL005</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Dans le monde d’aujourd’hui de multiples acteurs de la technologie numérique produisent des quantités infinies de données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Nowadays, multiple actors of Internet technology are producing very large amounts of data.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Capteurs, réseaux sociaux ou e-commerce, ils génèrent tous de l’information qui s’incrémente en temps-réel selon les 3 V de Gartner : en Volume, en Vitesse et en Variabilité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Sensors, social media or e-commerce, all generate real-time extending information based on the 3 Vs of Gartner: Volume, Velocity and Variety.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>L’objectif premier de cette étude est de pouvoir établir au moyen de ces approches une vision intégratrice du cycle de vie des données qui s’établit selon 3 étapes, (1) la synthèse des données via la sélection des valeurs-clés des micro-données acquises par les différents opérateurs au niveau de la source, (2) la fusion en faisant le tri des valeurs-clés sélectionnées et les dupliquant suivant un aspect de dé-normalisation afin d’obtenir un traitement plus rapide des données et (3) la transformation en un format particulier de carte de cartes de cartes, via Hadoop dans le processus classique de MapReduce afin d’obtenir un graphe défini dans la couche applicative.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The primary goal of this study is to establish, based on these approaches, an integrative vision of data life cycle set on 3 steps, (1) data synthesis by selecting key-values of micro-data acquired by different data source operators, (2) data fusion by sorting and duplicating the selected key-values based on a de-normalization aspect in order to get a faster processing of data and (3) the data transformation into a specific format of map of maps of maps, via Hadoop in the standard MapReduce process, in order to define the related graph in applicative layer.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Cette réflexion est en outre soutenue par un prototype logiciel mettant en oeuvre les opérateurs de modélisation sus-décrits et aboutissant à une boîte à outils de modélisation comparable à un AGL et, permettant une mise en place assistée d'un ou plusieurs traitements sur BigData</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In addition, this study is supported by a software prototype using the already described modeling tools, as a toolbox compared to an automatic programming software and allowing to create a customized processing chain of BigData</seg>
            </tuv>
        </tu>
    </body>
</tmx>