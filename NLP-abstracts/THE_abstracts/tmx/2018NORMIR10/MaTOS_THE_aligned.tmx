<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2018NORMIR10. segId begin by 1, tuid = segId</note>
        <docid>2018NORMIR10</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Les réseaux de neurones profonds récents possèdent de nombreuses couches cachées ce qui augmente significativement le nombre total de paramètres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Most successful deep neural models are the ones with many layers which highly increases their number of parameters.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>L'apprentissage de ce genre de modèles nécessite donc un grand nombre d'exemples étiquetés, qui ne sont pas toujours disponibles en pratique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Training such models requires a large number of training samples which is not always available.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Le sur-apprentissage est un des problèmes fondamentaux des réseaux de neurones, qui se produit lorsque le modèle apprend par coeur les données d'apprentissage, menant à des difficultés à généraliser sur de nouvelles données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>One of the fundamental issues in neural networks is overfitting which is the issue tackled in this thesis.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Le problème du sur-apprentissage des réseaux de neurones est le thème principal abordé dans cette thèse.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Such problem often occurs when the training of large models is performed using few training samples.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Dans la littérature, plusieurs solutions ont été proposées pour remédier à ce problème, tels que l'augmentation de données, l'arrêt prématuré de l'apprentissage ("early stopping"), ou encore des techniques plus spécifiques aux réseaux de neurones comme le "dropout" ou la "batch normalization".</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Many approaches have been proposed to prevent the network from overfitting and improve its generalization performance such as data augmentation, early stopping, parameters sharing, unsupervised learning, dropout, batch normalization, etc.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Dans cette thèse, nous abordons le sur-apprentissage des réseaux de neurones profonds sous l'angle de l'apprentissage de représentations, en considérant l'apprentissage avec peu de données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this thesis, we tackle the neural network overfitting issue from a representation learning perspective by considering the situation where few training samples are available which is the case of many real world applications.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Pour aboutir à cet objectif, nous avons proposé trois différentes contributions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose three contributions.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>La première contribution, présentée dans le chapitre 2, concerne les problèmes à sorties structurées dans lesquels les variables de sortie sont à grande dimension et sont généralement liées par des relations structurelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first one presented in chapter 2 is dedicated to dealing with structured output problems to perform multivariate regression when the output variable y contains structural dependencies between its components.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Notre proposition vise à exploiter ces relations structurelles en les apprenant de manière non-supervisée avec des autoencodeurs.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our proposal aims mainly at exploiting these dependencies by learning them in an unsupervised way.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Notre approche a montré une accélération de l'apprentissage des réseaux et une amélioration de leur généralisation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Validated on a facial landmark detection problem, learning the structure of the output data has shown to improve the network generalization and speedup its training.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>La deuxième contribution, présentée dans le chapitre 3, exploite la connaissance a priori sur les représentations à l'intérieur des couches cachées dans le cadre d'une tâche de classification.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second contribution described in chapter 3 deals with the classification task where we propose to exploit prior knowledge about the internal representation of the hidden layers in neural networks.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Cet à priori est basé sur la simple idée que les exemples d'une même classe doivent avoir la même représentation interne.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This prior is based on the idea that samples within the same class should have the same internal representation.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Nous avons formalisé cet à priori sous la forme d'une pénalité que nous avons rajoutée à la fonction de perte.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We formulate this prior as a penalty that we add to the training cost to be minimized.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Des expérimentations empiriques sur la base MNIST et ses variantes ont montré des améliorations dans la généralisation des réseaux de neurones, particulièrement dans le cas où peu de données d'apprentissage sont utilisées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Empirical experiments over MNIST and its variants showed an improvement of the network generalization when using only few training samples.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Notre troisième et dernière contribution, présentée dans le chapitre 4, montre l'intérêt du transfert d'apprentissage ("transfer learning") dans des applications dans lesquelles peu de données d'apprentissage sont disponibles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our last contribution presented in chapter 4 showed the interest of transfer learning in applications where only few samples are available.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Dans cette application, la tâche consiste à localiser la troisième vertèbre lombaire dans un examen de type scanner.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this application, the task consists in localizing the third lumbar vertebra in a 3D CT scan.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>L'utilisation du transfert d'apprentissage ainsi que de prétraitements et de post traitements adaptés a permis d'obtenir des bons résultats, autorisant la mise en oeuvre du modèle en routine clinique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A pre-processing of the 3D CT scan to obtain a 2D representation and a post-processing to refine the decision are included in the proposed system.</seg>
            </tuv>
        </tu>
    </body>
</tmx>