<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2012GRENM113. segId begin by 1, tuid = segId</note>
        <docid>2012GRENM113</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Par exemple, plusieurs millions de photos sont partagées quotidiennement sur les réseaux sociaux.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We are currently experiencing an exceptional growth of visual data, for example, millions of photos are shared daily on social-networks.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Les méthodes d'interprétation d'images vise à faciliter l'accès à ces données visuelles, d'une manière sémantiquement compréhensible.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Image understanding methods aim to facilitate access to this visual data in a semantically meaningful manner.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Dans ce manuscrit, nous définissons certains buts détaillés qui sont intéressants pour les taches d'interprétation d'images, telles que la classification ou la recherche d'images, que nous considérons dans les trois chapitres principaux.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this dissertation, we define several detailed goals which are of interest for the image understanding tasks of image classification and retrieval, which we address in three main chapters.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Tout d'abord, nous visons l'exploitation de la nature multimodale de nombreuses bases de données, pour lesquelles les documents sont composés d'images et de descriptions textuelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>First, we aim to exploit the multi-modal nature of many databases, wherein documents consists of images with a form of textual description.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Dans ce but, nous définissons des similarités entre le contenu visuel d'un document, et la description textuelle d'un autre document.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In order to do so we define similarities between the visual content of one document and the textual description of another document.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Ces similarités sont calculées en deux étapes, tout d'abord nous trouvons les voisins visuellement similaires dans la base multimodale, puis nous utilisons les descriptions textuelles de ces voisins afin de définir une similarité avec la description textuelle de n'importe quel document.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These similarities are computed in two steps, first we find the visually similar neighbors in the multi-modal database, and then use the textual descriptions of these neighbors to define a similarity to the textual description of any document.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Ensuite, nous présentons une série de modèles structurés pour la classification d'images, qui encodent explicitement les interactions binaires entre les étiquettes (ou labels).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Second, we introduce a series of structured image classification models, which explicitly encode pairwise label interactions.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Un scenario interactif comme celui-ci offre un compromis intéressant entre la précision, et l'effort d'annotation manuelle requis.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Such an interactive scenario offers an interesting trade-off between accuracy and manual labeling effort.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Nous explorons les modèles structurés pour la classification multi-étiquette d'images, pour la classification d'image basée sur les attributs, et pour l'optimisation de certaines mesures de rang spécifiques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We explore structured models for multi-label image classification, for attribute-based image classification, and for optimizing for specific ranking measures.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Enfin, nous explorons les classifieurs par k plus proches voisins, et les classifieurs par plus proche moyenne, pour la classification d'images à grande échelle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, we explore k-nearest neighbors and nearest-class mean classifiers for large-scale image classification.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Nous proposons des méthodes d'apprentissage de métrique efficaces pour améliorer les performances de classification, et appliquons ces méthodes à une base de plus d'un million d'images d'apprentissage, et d'un millier de classes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose efficient metric learning methods to improve classification performance, and use these methods to learn on a data set of more than one million training images from one thousand classes.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Comme les deux méthodes de classification permettent d'incorporer des classes non vues pendant l'apprentissage à un coût presque nul, nous avons également étudié leur performance pour la généralisation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Since both classification methods allow for the incorporation of classes not seen during training at near-zero cost, we study their generalization performances.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Nous montrons que la classification par plus proche moyenne généralise à partir d'un millier de classes, sur dix mille classes à un coût négligeable, et les performances obtenus sont comparables à l'état de l'art.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We show that the nearest-class mean classification method can generalize from one thousand to ten thousand classes at negligible cost, and still perform competitively with the state-of-the-art.</seg>
            </tuv>
        </tu>
    </body>
</tmx>