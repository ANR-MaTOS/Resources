<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2018AZUR4115. segId begin by 1, tuid = segId</note>
        <docid>2018AZUR4115</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Notre travail est présenté en trois parties indépendantes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our work is presented in three separate parts which can be read independently.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Tout d'abord, nous proposons trois heuristiques d'apprentissage actif pour les réseaux de neurones profonds : Nous mettons à l'échelle le `query by committee', qui agrège la décision de sélectionner ou non une donnée par le vote d'un comité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Firstly we propose three active learning heuristics that scale to deep neural networks: We scale query by committee, an ensemble active learning methods.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Pour se faire nous formons le comité à l'aide de différents masques de dropout.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We speed up the computation time by sampling a committee of deep networks by applying dropout on the trained model.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Un autre travail se base sur la distance des exemples à la marge.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Another direction was margin-based active learning.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Nous proposons d'utiliser les exemples adversaires comme une approximation de la dite distance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose to use an adversarial perturbation to measure the distance to the margin.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Nous démontrons également des bornes de convergence de notre méthode dans le cas de réseaux linéaires.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We also establish theoretical bounds on the convergence of our Adversarial Active Learning strategy for linear classifiers.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Notre méthode sélectionne les données qui minimisent l'énergie libre variationnelle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Secondly, we focus our work on how to fasten the computation of Wasserstein distances.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Dans un second temps, nous nous sommes concentrés sur la distance de Wasserstein.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose to approximate Wasserstein distances using a Siamese architecture.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Nous projetons les distributions dans un espace où la distance euclidienne mimique la distance de Wasserstein.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>From another point of view, we demonstrate the submodular properties of Wasserstein medoids and how to apply it in active learning.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Également, nous démontrons les propriétés sous-modulaires des prototypes de Wasserstein et comment les appliquer à l'apprentissage actif.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>First, we hijack an active learning strategy to confront the relevance of the sentences selected with active learning to state-of-the-art phraseology techniques.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Enfin, nous proposons de nouveaux outils de visualisation pour expliquer les prédictions d'un CNN sur du langage naturel.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These works help to understand the hierarchy of the linguistic knowledge acquired during the training of CNNs on NLP tasks.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Deuxièmement, nous profitons des algorithmes de déconvolution des CNNs afin de présenter une nouvelle perspective sur l'analyse d'un texte.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Secondly, we take advantage of deconvolution networks for image analysis to present a new perspective on text analysis to the linguistic community that we call Text Deconvolution Saliency.</seg>
            </tuv>
        </tu>
    </body>
</tmx>