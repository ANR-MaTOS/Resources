<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2019GREAM037. segId begin by 1, tuid = segId</note>
        <docid>2019GREAM037</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Les représentations des mots sont à la base du plupart des systèmes modernes pour le traitement automatique du langage, fournissant des résultats compétitifs.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Continuous word representations (word type embeddings) are at the basis of most modern natural language processing systems, providing competitive results particularly when input to deep learning models.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Cependant, d'importantes questions se posent concernant les défis auxquels ils sont confrontés pour faire face aux phénomènes complexes du langage naturel et leur capacité à saisir la variabilité du langage naturel.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However, important questions are raised concerning the challenges they face in dealing with the complex natural language phenomena and regarding their ability to capture natural language variability.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Pour mieux gérer les phénomènes complexes du langage, de nombreux travaux ont été menées pour affiner les représentations génériques de mots ou pour créer des représentations spécialisées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To better handle complex language phenomena, much work investigated fine-tuning the generic word type embeddings or creating specialized embeddings that satisfy particular linguistic constraints.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Bien que cela puisse aider à distinguer la similarité sémantique des autres types de relations sémantiques, il peut ne pas suffire de modéliser certains types de relations, telles que les relations logiques d'implication ou de contradiction.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>While this can help distinguish semantic similarity from other types of semantic relatedness, it may not suffice to model certain types of relations between texts such as the logical relations of entailment or contradiction.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>La première partie de la thèse étudie l'encodage de la notion d'implication textuelle dans un espace vectoriel en imposant l'inclusion d'information.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first part of the thesis investigates encoding the notion of entailment within a vector space by enforcing information inclusion, using an approximation to logical entailment of binary vectors.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Des opérateurs d'implication sont ensuite développées et le cadre proposé peut être utilisé pour réinterpréter un modèle existant de la sémantique distributionnelle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We further develop entailment operators and show how the proposed framework can be used to reinterpret an existing distributional semantic model.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Des évaluations sont fournies sur la détection d'hyponymie en tant que une instance d'implication lexicale.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Evaluations are provided on hyponymy detection as an instance of lexical entailment.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Un autre défi concerne la variabilité du langage naturel et la nécessité de désambiguïser les unités lexicales en fonction du contexte dans lequel elles apparaissent.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Another challenge concerns the variability of natural language and the necessity to disambiguate the meaning of lexical units depending on the context they appear in.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Les représentations génériques de mots ne réussissent pas à elles seules, des architectures différentes étant généralement utilisées pour aider à la désambiguïsation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For this, generic word type embeddings fall short of being successful by themselves, with different architectures being typically employed on top to help the disambiguation.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Étant donné que les représentations de mots sont construites à partir de statistiques de cooccurrence sur de grands corpus et qu’elles reflètent ces statistiques, elles fournissent une seule représentation pour un mot donné, malgré ses multiples significations.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>As type embeddings are constructed from and reflect co-occurrence statistics over large corpora, they provide one single representation for a given word, regardless of its potentially numerous meanings.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Même dans le cas de mots monosémiques, cela ne fait pas la distinction entre les différentes utilisations d’un mot en fonction de son contexte.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Furthermore, even given monosemous words, type embeddings do not distinguish between the different usages of a word depending on its context.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Dans ce sens, on pourrait se demander s'il est possible d'exploiter directement les informations linguistiques fournies par le contexte d'un mot pour en ajuster la représentation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In that sense, one could question if it is possible to directly leverage available linguistic information provided by the context of a word to adjust its representation.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Ces informations seraient-elles utiles pour créer une représentation enrichie du mot dans son contexte ?</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Would such information be of use to create an enriched representation of the word in its context?</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Et si oui, des informations de nature syntaxique peuvent-elles aider au processus ou le contexte local suffit ?</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>And if so, can information of syntactic nature aid in the process or is local context sufficient?</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Dans la deuxième partie de la thèse, nous étudions une façon d’incorporer la connaissance contextuelle dans les représentations de mots eux-mêmes, en exploitant les informations provenant de l’analyse de dépendance de phrase ainsi que les informations de voisinage local.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the second part of the thesis, we investigate one possible way to incorporate contextual knowledge into the word representations themselves, leveraging information from the sentence dependency parse along with local vicinity information.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Nous proposons des représentations de mots contextualisées sensibles à la syntaxe (SATokE) qui capturent des informations linguistiques spécifiques et encodent la structure de la phrase dans leurs représentations.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose syntax-aware token embeddings (SATokE) that capture specific linguistic information, encoding the structure of the sentence from a dependency point of view in their representations.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Cela permet de passer des représentations de type générique (invariant du contexte) à des représentations spécifiques (tenant compte du contexte).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This enables moving from generic type embeddings (context-invariant) to specific token embeddings (context-aware).</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>Alors que la syntaxe était précédemment considérée pour les représentations de mots, ses avantages n'ont peut-être pas été entièrement évalués au-delà des modèles qui exploitent ces informations à partir de grands corpus.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>While syntax was previously considered for building type representations, its benefits may have not been fully assessed beyond models that harvest such syntactical information from large corpora.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>Les représentations obtenues sont évaluées sur des tâches de compréhension du langage naturel : classification des sentiments, détection de paraphrases, implication textuelle et analyse du discours.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The obtained token representations are evaluated on natural language understanding tasks typically considered in the literature: sentiment classification, paraphrase detection, textual entailment and discourse analysis.</seg>
            </tuv>
        </tu>
        <tu tuid="20">
            <tuv xml:lang="FR">
                <seg>Nous démontrons empiriquement la supériorité de ces représentations par rapport aux représentations génériques et contextualisées des mots existantes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We empirically demonstrate the superiority of the token representations compared to popular distributional representations of words and to other token embeddings proposed in the literature.</seg>
            </tuv>
        </tu>
        <tu tuid="21">
            <tuv xml:lang="FR">
                <seg>Le travail proposé dans la présente thèse contribue à la recherche dans le domaine de la modélisation de phénomènes complexes tels que l'implication textuelle, ainsi que de la variabilité du langage par le biais de la proposition de représentations contextualisés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The work proposed in the current thesis aims at contributing to research in the space of modelling complex phenomena such as entailment as well as tackling language variability through the proposal of contextualized token embeddings.</seg>
            </tuv>
        </tu>
    </body>
</tmx>