Avec la croissance sans précédent des publications sur les plateformes de micro-blogging, trouver du contenu intéressant pour un utilisateur est devenu un enjeu majeur. Cependant, en raison des propriétés intrinsèques des plateformes de micro–blogging, comme le flux gigantesque de messages arrivant tous les jours et leur faible durée de vie, il est difficile d'appliquer les méthodes traditionnelles de recommandation comme la factorisation matricielle. Après une étude approfondie d'un large jeu de donnée issu de Twitter, nous présentons un modèle de propagation qui repose sur l'homophilie présente dans le réseau pour recommander des messages aux utilisateurs. Notre approche s'appuie sur la construction d'un graphe de similarités lié aux interactions des utilisateurs. Nous présentons plusieurs expérimentations pour démontrer la qualité de prédiction de notre modèle et sa capacité à passer à l'échelle. Il semble que, de façon contre intuitive, dans la majorité des cas les systèmes de recommandation ouvrent les perspectives des utilisateurs. Cependant, une minorité de personnes est concerné par l'effet de bulle et nous proposons donc un modèle reposant sur les liens entre communautés pour adapter les recommandations afin d'être plus en accord avec leur profil communautaire.
La reconnaissance de motifs est une tâche cruciale pour les êtres vivants, exécutée avec efficacité par le cerveau. Les réseaux de neurones profonds artificiels reproduisent de mieux en mieux ces performances, avec des applications telles que la reconnaissance d'images ou le traitement du langage. Ils nécessitent cependant un apprentissage intensif sur de grands jeux de données et couteux en calculs. Les réseaux de neurones à impulsions, plus proches du fonctionnement du cerveau avec des neurones émettant des impulsions et des lois d' apprentissage dites STDP dépendant du temps entre deux impulsions, constituent une alternative intéressante. Ils permettent un apprentissage non supervisé et ont déjà été utilisés pour la reconnaissance visuelle ou auditive, mais les applications restent limitées par rapport à l'apprentissage profond classique. Il est d'autant plus intéressant de développer de nouvelles applications pour ces réseaux qu'ils peuvent être implémentés sur des circuits neuromorphiques connaissant aujourd'hui des développements importants, notamment avec les composants analogiques « memristifs » qui miment la plasticité synaptique. Ici, nous avons choisi de développer un réseau STDP pour un problème crucial en neuroscience : le spike-sorting. Les implants cérébraux composés de matrices de microélectrode permettent d'enregistrer l'activité individuelle de multiples neurones, prenant la forme de pics de potentiel dans le signal, appelés potentiels d'action. Une même électrode enregistre l'activité de plusieurs neurones. Le spike-sorting a pour but de détecter et trier cette activité, en utilisant le fait que la forme d'un potentiel d'action dépend du neurone qui l'a émis. Les méthodes classiques de spike-sorting consistent en trois étapes : la détection des potentiels d'action, l'extraction de traits caractéristiques de leurs formes, et le tri de ces caractéristiques en groupes correspondant alors aux différentes cellules neurales. Bien que les méthodes onlines existent, les méthodes les plus répandues nécessitent un traitement offline, qui n'est pas compatible avec les applications temps réelles telles que les interfaces cerveau-machine (BCI). Utiliser un réseau STDP apporte une nouvelle méthode pour répondre à ces besoins. Le réseau que nous avons conçu prend en entrée le signal de l'électrode et produit en sortie un train d'impulsions qui correspond à l'activité des cellules enregistrées. Il est organisé en différentes couches, connectées en série, chacune effectuant une étape du traitement. La première couche, constituée de neurones senseurs, convertit le signal d'entrée en train d'impulsions. Les couches suivantes apprennent les motifs générés par la couche précédente grâce aux lois STDP. Chaque couche est améliorée par l'implémentation de différents mécanismes, tels que le STDP avec ressources, l'adaptation de seuil, la plasticité déclenchée par l'inhibition, ou un modèle de neurone déchargeant par rebond. Un mécanisme d'attention permet au réseau de ne traiter que les parties du signal contenant des potentiels d'action. Ce réseau a été conçu dans un premier temps pour traiter des données mono-électrode, puis adapté pour traiter des signaux provenant d'électrodes multiples. Il a été testé d'abord sur des données simulées qui permettent de comparer la sortie du réseau à la vérité, puis sur des enregistrements réels de microélectrodes associés à des enregistrements intracellulaires donnant une vérité partielle. Les différentes versions du réseau ont été ainsi évaluées et comparées à d'autres algorithmes, donnant des résultats très satisfaisants. Suite à ces résultats simulés sur ordinateur, nous avons travaillé à une implémentation FPGA, constituant une première étape vers une implémentation embarquée neuromorphique.
La présente étude s'inscrit dans le cadre d'un essai de traitement automatique de la langue malgache visant à construire un dictionnaire électronique. Notre travail est articulé autour de deux axes principaux : une étude descriptive de la langue d'une part et une modélisation informatique d'autre part. Après un chapitre préliminaire consacré à une présentation générale, nous présentons dans la première partie une analyse des termes malgaches portant tant sur les formes de la structure morphologique ou morphématique que sur leur comportement syntaxique. Nous décrivons les différentes voix des énoncés malgaches et les modalités : le temps, l'aspect et le mode. Dans la deuxième partie, nous formalisons les données linguistiques dégagées dans la partie descriptive pour en faire une implantation informatique d'un système d'Analyseur-Générateur des Termes prédicatifs Malgaches (AGTM). Nous terminons cette partie pratique par le bilan de notre étude et les perspectives d'avenir. La thèse est complétée par des annexes qui rassemblent le module de programmation et les résultats obtenus.
Ce travail porte sur le rôle de la polysémie dans la structuration du lexique. La thèse propose de faire une évaluation qualitative de la polysémie, en la comparant aux autres relations qui structurent le lexique. Cette entreprise doit permettre de vérifier que les liens de polysémie ne doivent pas être modélisés indépendamment des liens de dérivation ou de conversion. Les résultats de l'évaluation montrent que la frontière entre polysémie et conversion est poreuse. Les comparaisons entre relations utilisent les propriétés de l'analogie, bien adaptée pour caractériser les rapports entre relations. Ce sont les liens qui connectent les lexies qui font l'objet d'une comparaison. Un lien de polysémie est ce qui connecte deux lexies en relation de polysémie. Ce lien peut être comparé à un lien qui connecte deux autres lexies en relation de conversion. La langue d'étude est le wolof, langue atlantique d'Afrique de l'ouest. Cette langue est un terrain propice à une telle recherche.
La modélisation de la langue naturelle est l¿un des défis fondamentaux de l¿intelligence artificielle et de la conception de systèmes interactifs, avec applications dans les systèmes de dialogue, la génération de texte et la traduction automatique. Nous proposons un modèle log-linéaire discriminatif donnant la distribution des mots qui suivent un contexte donné. Le résultat est un modèle efficace qui capte suffisamment les dépendances longues sans occasionner une forte augmentation des ressources en espace ou en temps. Dans un modèle log-linéaire, les phases d¿apprentissage et de tests deviennent de plus en plus chères avec un nombre croissant de classes. Le nombre de classes dans un modèle de langue est la taille du vocabulaire, qui est généralement très importante. Une astuce courante consiste à appliquer le modèle en deux étapes : la première étape identifie le cluster le plus probable et la seconde prend le mot le plus probable du cluster choisi. Cette idée peut être généralisée à une hiérarchie de plus grande profondeur avec plusieurs niveaux de regroupement. Cependant, la performance du système de classification hiérarchique qui en résulte dépend du domaine d¿application et de la construction d¿une bonne hiérarchie. Nous étudions différentes stratégies pour construire la hiérarchie des catégories de leurs observations.
Cette thèse développe un formalisme théorique pour la sémantique du discours. Il s'appuie sur l'extension des grammaires de Montague, sur la notion de continuation et sur les mécanismes de levée et de traitement des exceptions. Le formalisme permet de traiter des phénomènes dynamiques tels que les anaphores d'une phrase à l'autre, les présuppositions déclenchées par des référents et les projections présuppositions.
Partant du postulat que ce parti politique français est un parti désormais ancré dans le système mais qui, paradoxalement, se revendique comme un parti « antisystème » , nous étudions la mise en discours de cette opposition. Pour réaliser cette recherche, nous avons créé, sur le logiciel Hyperbase, une plateforme d'analyse de données textuelles, un vaste corpus de plus de 3 millions d'occurrences structurées en 5 bases de données. Plus de 300 discours lepéniens et trois campagnes présidentielles Pour répondre à notre interrogation fondamentale, trois champs de pertinence – correspondant chacun à une approche linguistique – se succèdent et progressent d'une analyse infra-textuelle, centrée sur l'étude des spécificités lexicales et syntaxiques du discours FN, vers une analyse textuelle consacrée à la cohésion inter- et supra-phrastique de la textualité lepénienne, pour aboutir à l'organisation discursive et aux relations que pose le discours du FN localement et globalement aux autres discours.
La présente thèse a pour objet l'analyse automatique des formes lexicales dans les textes écrits en lituanien, sur la base d'une heuristique forme - valeur qui s'inscrit dans une approche symbolique du traitement automatique des langues. Cette étude accorde une attention spécifique à l'éxploitation optimale des indices formels et s'appuie principalement sur deux domaines de la linguistique, la graphématique et la morphologie. Le point d'entrée formaliste couplé à l'objectif d'automatisation a réclamé une révision de la perspective grammaticale traditionnelle, qui nous a conduit à esquisser un renouvellement de la description relative à plusieurs aspects du système linguistique, notamment les parties du discours, la structure lexicale et la suffixation. Le modèle linguistique, qui reste à développer, a servi de fondement à la réalisation d'un analyseur de formes lexicales nommé ALeksas. Ce logiciel possède une structure hybride principalement basée sur des automates à nombre fini d'états. ALeksas, qui est encore à l'état expérimental, assure l'analyse grammaticale des mots formes selon une approche indépendante d'une base de données lexicale permettant de formuler des hypothèses d'interprétation sur des critères formels. Le prototype a fait l'objet d'une mise à l'épreuve par confrontation à un corpus de textes authentiques variés, afin d'évaluer ses capacités, notamment par rapport aux outils comparables, et de mieux cerner les améliorations nécessaires.
Historiquement, les Modèles Graphiques Probabilistes (PGMs) sont une solution d'apprentissage à partir des données incertaines et plates, appelées aussi données propositionnelles ou représentations attribut-valeur. Au début des années 2000, un grand intérêt a été adressé au traitement des données relationnelles présentant un grand nombre d'objets participant à des différentes relations. Les Modèles Probabilistes Relationnels (PRMs) présentent une extension des PGMs pour le contexte relationnel. Avec l'évolution rapide issue de l'internet, des innovations technologiques et des applications web, les données sont devenues de plus en plus variées et complexes. D'où l'essor du Big Data. Toutefois, tous les travaux d'apprentissage des PRMs sont consacrés à apprendre à partir des données bien structurées et stockées dans des bases de données relationnelles. Les bases de données graphe sont non structurées et n'obéissent pas à un schéma bien défini. Les arcs entre les noeuds peuvent avoir des différentes signatures. En effet, les relations qui ne correspondent pas à un modèle ER peuvent exister dans l'instance de base de données. Ces relations sont considérées comme des exceptions. Dans ce travail de thèse, nous nous intéressons à ce type de bases de données. Nous étudions aussi deux types de PRMs à savoir, Direct Acyclic Probabilistic Entity Relationship (DAPER) et chaines de markov logiques (MLNs). Nous proposons deux contributions majeures. Premièrement, Une approche d'apprentissage des DAPERs à partir des bases de données graphe partiellement structurées. Une deuxième approche consiste à exploiter la logique de premier ordre pour apprendre les DAPERs en utilisant les MLNs pour prendre en considération les exceptions qui peuvent parvenir lors de l'apprentissage. Nous menons une étude expérimentale permettant de comparer nos méthodes proposées avec les approches déjà existantes.
Cette thèse est composée de trois études distinctes qui examinent empiriquement le rôle des informations divulguées sur la dépréciation de la survaleur pour les principales parties prenantes de l'entreprise (c'est-à-dire les analystes financiers, les entreprises homologues et les auditeurs externes). Dans la première étude, j'examine l'effet de la transparence de la divulgation sur le désaccord entre les analystes, et le désaccord entre les analystes et les gestionnaires, dans le contexte de la dépréciation du goodwill. La deuxième étude examine si la déclaration d'une dépréciation significative du goodwill par une entreprise (entreprise dépréciée) affecte le comportement d'investissement d'autres entreprises du même secteur (entreprises homologues). La troisième étude examine l'impact de la divulgation élargie des rapports d'audit sur les décisions des entreprises en matière de divulgation financière. Plus précisément, j'examine si les entreprises adaptent les niveaux de divulgation sur la dépréciation des écarts d'acquisition lorsque les auditeurs signalent la dépréciation des écarts d'acquisition comme un risque d'inexactitudes importantes dans le rapport d'audit élargi. Cette étude contribue au débat sur l'utilité du rapport d'audit élargi en identifiant le mécanisme par lequel le rapport d'audit élargi a un impact sur l'information financière et les décisions des entreprises.
La dysarthrie est un trouble de la parole affectant la réalisation motrice de la parole causée par des lésions du système nerveux central ou périphérique. Elle peut être liée à différentes pathologies : la maladie de Parkinson, la Sclérose Latérale Amyotrophique (SLA), un Accident Vasculaire Cérébral (AVC), etc. Plusieurs travaux de recherche ont porté sur la caractérisation des altérations liées à chaque pathologie afin de les regrouper dans des classes de dysarthrie. La classification la plus répandue est celle établie par F. L. Darley comportant 6 classes en 1969, (complétée par deux classes supplémentaires en 2005). Actuellement, l'évaluation perceptive (à l'oreille) reste le standard utilisé dans la pratique clinique pour le diagnostique et le suivi thérapeutique des patients. Cette approche est néanmoins reconnue comme étant subjective, non reproductible et coûteuse en temps. Ces limites la rendent inadaptée à l'évaluation de larges corpus (dans le cadre d'études phonétiques par exemple) ou pour le suivi longitudinal de l'évolution des patients dysarthriques. Le travail présenté dans ce rapport s'inscrit dans ce cadre et étudie l'apport que peuvent avoir ces outils dans l'évaluation de la parole dysarthrique, et plus généralement pathologique. Dans ce travail, une approche pour la détection automatique des phonèmes anormaux dans la parole dysarthrique est proposée et son comportement est analysé sur différents corpus comportant différentes pathologies, classes dysarthriques, niveaux des évérité de la maladie et styles de parole. Contrairement à la majorité des approches proposées dans la littérature permettant des évaluations de la qualité globale de la parole(évaluation de la sévérité, intelligibilité, etc.), l'approche proposée se focalise sur le niveau phonème dans le but d'atteindre une meilleure caractérisation de la dysarthrieet de permettre un feed-back plus précis et utile pour l'utilisateur (clinicien, phonéticien,patient). L'approche s'articule autours de deux phases essentielles : (1) une première phase d'alignement automatique de la parole au niveau phonème (2) une classification de ces phonèmes en deux classes : phonèmes normaux et anormaux. L'évaluation de l'annotation réalisée par le système par rapport à une évaluation perceptive d'un expert humain considérée comme "référence" montre des résultats très encourageants et confirme la capacité de l'approche à detecter les anomalies au niveau phonème. L'approche s'est aussi révélée capable de capter l'évolution de la sévérité de la dysarthrie suggérant une potentielle application lors du suivi longitudinal des patients ou pour la prédiction automatique de la sévérité de leur dysarthrie. Aussi, l'analyse du comportement de l'outil d'alignement automatique de la parole face à la parole dysarthrique a révélé des comportements dépendants des pathologies et des classes dysarthriques ainsi que des différences entre les catégories phonétiques. De plus, un effet important du style de parole (parole lue et spontanée) a été constaté sur les comportements de l'outil d'alignement de la parole et de l'approche de détection automatique d'anomalies. Finalement, les résultats d'une campagne d'évaluation de l'approche de détection d'anomalies par un jury d'experts sont présentés et discutés permettant une mise en avant des points forts et des limites du système.
Les bases de données médico-administratives sont des bases de données de santé particulièrement exhaustives. De nouveaux modèles de processus et un algorithme de process mining adapté sont présentés, modélisant les transitions et leurs temporalités. Une solution de prétraitement des journaux d'événements est également proposée, permettant une représentation des évènements complexes caractérisés par de multiples codes appartenant à différents systèmes de codage, organisés en structures hiérarchiques. Cette méthode de clustering par auto-encodage permet de regrouper dans l'espace latent les événements similaires et produit automatiquement des labels pertinents pour le process mining, explicables médicalement. Un premier algorithme de prédiction adapté aux parcours est alors proposé, produisant via une procédure d'optimisation un modèle de processus utilisé pour classifier les parcours directement à partir des données de journaux d'événements. Ce modèle de processus sert également de support pour expliquer les patterns de parcours distinctifs entre deux populations. Une seconde méthode de prédiction est présentée, avec un focus particulier sur les événements médicaux récurrents.
Avec l'essor du Big Data, le traitement du Volume, de la Vélocité (croissance et évolution) et de la Variété de la donnée concentre les efforts des différentes communautés pour exploiter ces nouvelles ressources. Ces nouvelles ressources sont devenues si importantes, que celles-ci sont considérées comme le nouvel « or noir » . Au cours des dernières années, le volume et la vélocité sont des aspects de la donnée qui sont maitrisés contrairement à la variété qui elle reste un défi majeur. Cette thèse présente deux contributions dans le domaine de mise en correspondance de données hétérogènes, avec un focus sur la dimensions spatiale. La première contribution repose sur un processus de mise en correspondance de données textuelles hétérogènes divisé en deux étapes : la géoreprésentation et le géomatching. Dans la première phase, nous proposons de représenter la dimension spatiale de chaque document d'un corpus à travers une structure dédiée, la Spatial Textual Representation (STR). Cette représentation de type graphe est composée des entités spatiales identifiées dans le document, ainsi que les relations spatiales qu'elles entretiennent. Pour identifier les entités spatiales d'un document et leurs relations spatiales, nous proposons une ressource dédiée, nommée Geodict. La seconde phase, le géomatching, consiste à mesurer la similarité entre les représentations générées (STR). Pour évaluer la pertinence d'une correspondance, nous proposons un ensemble de 6 critères s'appuyant sur une définition de la similarité spatiale entre deux documents. La seconde contribution repose sur la dimension thématique des données textuelles et sa participation dans le processus de mise en correspondance spatiale. Nous proposons d'identifier les thèmes apparaissant dans la même fenêtre contextuelle que certaines entités spatiales. L'objectif est d'induire certaines des similarités spatiales implicites entre les documents. Pour cela, nous proposons d'étendre la structure de la STR à l'aide de deux concepts : l'entité thématique et de la relation thématique. L'entité thématique représente un concept propre à un domaine particulier (agronome, médical) et représenté selon différentes orthographes présentes dans une ressource terminologique, ici un vocabulaire. Une relation thématique lie une entité spatiale à une entité thématique si celles-ci apparaissent dans une même fenêtre contextuelle. Les vocabulaires choisis ainsi que la nouvelle forme de la STR intégrant la dimension thématique sont évalués selon leur couverture sur les corpus étudiés, ainsi que leurs contributions dans le processus de mise en correspondance spatiale.
Cette thèse explore l'utilisation de fonctions de perte structurées dans deux domaines distincts. Dans la première contribution, nous nous intéressons à l'apprentissage par renforcement multi-agent, dans le contexte d'environnements qui peuvent être séparés en plusieurs tâches faiblement dépendantes. On s'attache à trouver des politiques qui se généralisent à plus d'agents et de tâches que les scénarios d'entraînement, permettant ainsi d'augmenter la taille des problèmes qui peuvent être approchés. Notre solution affecte les agents aux tâches en résolvant un problème d'optimisation centralisé dont la fonction objectif est paramétrée par un réseau de neurones. On montre que l'expressivité du problème d'optimisation et celle du réseau de neurones influencent la capacité du modèle à généraliser, et qu'avec les bons choix, la politique peut généraliser à plus de 5 fois plus d'agents que pendant l'entraînement. Dans la seconde contribution, nous formulons la détection d'objets comme un problème de prédiction d'ensemble, et nous concevons un modèle dans cette optique. Notre solution utilise un réseau convolutionel profond, comme souvent en vision par ordinateur, et un encodeur-décodeur de Transformer, une architecture qui a récemment permis d'importants progrès en traitement du langage. Remarquablement, notre solution n'incorpore que peu de biais inductif, et ne nécessite donc pas de composants spécifiques à la détection d'objets, tels que les ancres de détection. Avec un nombre de paramètres comparable, notre modèle égale la performance de modèles de référence, tels que Retinanet et Faster R-CNN sur le dataset de détection COCO. Pour finir, nous montrons que la méthode peut naturellement être étendue à la segmentation panoptique, où elle surpasse les approches concurrentes, démontrant ainsi sa généralité.
La tâche de segmentation et de regroupement en locuteur (SRL) consiste à déterminer le nombre de locuteurs ainsi que leurs interventions dans un document audio. Cette tâche intéresse de nombreuses entreprises qui souhaitent indexer leurs contenus audiovisuels. En particulier, l'institut national de l'audiovisuel (INA) désire appliquer cette tâche sur ses archives afin d'en améliorer l'accessibilité mais également l'annotation. Cependant, les usages de l'institut requièrent une qualité minimum qui n'est, la plupart du temps, pas encore atteinte par les systèmes automatiques de SRL à l'état de l'art. Pour atteindre les performances voulues, un humain peut corriger la sortie d'un système de SRL. Néanmoins, une intervention humaine est généralement chronophage et coûteuse. Afin de réduire ces coûts, une solution possible est d'utiliser un système assisté par l'humain : un humain donne des informations à un système afin qu'il améliore ses prédictions pour faire décroître son coût de correction. Le présent manuscrit s'articule autour de la SRL assistée par l'humain. Il propose une mesure afin d'évaluer le coût d'intervention humain pour corriger une SRL, un protocole pour évaluer les interactions d'un humain pour la SRL, un automate simulant les corrections humaines à faire pour une SRL et des systèmes de SRL assistés réduisant le coût d'intervention humain total. Plus précisément, les systèmes de SRL assistés présentés réévaluent soit uniquement le regroupement en locuteurs, soit la segmentation et le regroupement en locuteurs.
Elles ont donné lieu à de nombreuses études en Traitement Automatique du Langage Naturel. En effet, leur étude et leur identification précise sont primordiales, sur les plans théorique et applicatif. Cependant, la majorité des travaux de recherche sur le sujet portent sur des usages de langage quotidien : dialogues " à bâtons rompus ", demandes d'horaire, discours, etc. Mais qu'en est-il des productions orales spontanées produites dans un cadre contraint ? Aucune étude n'a à notre connaissance été menée dans ce contexte. Or, on sait que l'utilisation d'une " langue de spécialité " dans le cadre d'une tâche donnée entraîne des comportements spécifiques. Notre travail de thèse est consacré à l'étude linguistique et informatique des disfluences dans un tel cadre. Il s'agit de dialogues de contrôle de trafic aérien, aux contraintes pragmatiques et linguistiques. Nous effectuons une étude exhaustive des phénomènes de disfluences dans ce contexte. Dans un premier temps nous procédons à l'analyse fine de ces phénomènes. Ensuite, nous les modélisons à un niveau de représentation abstrait, ce qui nous permet d'obtenir les patrons correspondant aux différentes configurations observées. Enfin nous proposons une méthodologie de traitement automatique. Celle-ci consiste en plusieurs algorithmes pour identifier les différents phénomènes, même en l'absence de marqueurs explicites. Elle est intégrée dans un système de traitement automatique de la parole. Enfin, la méthodologie est validée sur un corpus de 400 énoncés.
Conduite dans le but de rendre les robots comme socio-communicatifs, les chercheurs ont cherché à mettre au point des robots dotés de compétences sociales et de « bon sens » pour les rendre acceptables. Cette intelligence sociale ou « sens commun » du robot est ce qui finit par déterminer son acceptabilité sociale à long terme. Cependant, ce n'est pas commun. Les robots peuvent donc seulement apprendre à être acceptables avec l'expérience. Cependant, en enseignant à un humanoïde, les subtilités d'une interaction sociale ne sont pas évidentes. Même un échange de dialogue standard intègre le panel le plus large possible de signes qui interviennent dans la communication et sont difficiles à codifier (synchronisation entre l'expression du corps, le visage, le ton de la voix, etc.). Dans un tel scénario, l'apprentissage du modèle comportemental du robot est une approche prometteuse. Cet apprentissage peut être réalisé avec l'aide de techniques d'IA. Cette étude tente de résoudre le problème de l'apprentissage des modèles comportementaux du robot dans le paradigme automatisé de planification et d'ordonnancement (APS) de l'IA. Dans le domaine de la planification automatisée et de l'ordonnancement (APS), les agents intelligents nécessitent un modèle d'action (plans d'actions dont les exécutions entrelacées effectuent des transitions de l'état système) afin de planifier et résoudre des problèmes réels. Au cours de cette thèse, nous présentons deux nouveaux systèmes d'apprentissage qui facilitent l'apprentissage des modèles d'action et élargissent la portée de ces nouveaux systèmes pour apprendre les modèles de comportement du robot. Ces techniques peuvent être classées dans les catégories non optimale et optimale. Les techniques non optimales sont plus classiques dans le domaine, ont été traitées depuis des années et sont de nature symbolique. Cependant, ils ont leur part de quirks, ce qui entraîne un taux d'apprentissage moins élevé que souhaité. Les techniques optimales sont basées sur les progrès récents dans l'apprentissage en profondeur, en particulier la famille à long terme (LSTM) de réseaux récurrents récurrents. Ces techniques sont de plus en plus séduisantes par la vertu et produisent également des taux d'apprentissage plus élevés. Cette étude met en vedette ces deux techniques susmentionnées qui sont testées sur des repères d'IA pour évaluer leurs prouesses. Ils sont ensuite appliqués aux traces HRI pour estimer la qualité du modèle de comportement du robot savant. Ceci est dans l'intérêt d'un objectif à long terme d'introduire l'autonomie comportementale dans les robots, afin qu'ils puissent communiquer de manière autonome avec les humains sans avoir besoin d'une intervention de « magicien » .
Un corpus aussi vaste et riche que celui représenté par le conte de tradition orale de la province de Valladolid justifie la création d'un outil lexicostatistique. L'outil informatique permet la visualisation de données précises et exhaustives et la mise en évidence des différentes unités lexicales exploitées au sein des contes extraits d'une part de l'oeuvre de Joaquin DIAZ et d'autre part de l'oeuvre de Aurelio M. ESPINOSA (hijo). Dans un premier temps cette étude s'attache à présenter le contexte géographique et terminologique. Le travail de recherche consiste ensuite en la création d'une base de données lexicales -fruit du dépouillement de l'intégralité du corpus de référence- laquelle représente une vaste banque terminologique répertoriée en différents champs lexicosémantiques, en la comptabilisation et la représentation graphique des données obtenues et en l'interprétation des résultats enregistrés selon une approche sociolinguistique et socioculturelle. A terme, ce travail permet la réalisation d'une classification des récits favorisée par une nouvelle méthode de gestion textuelle.
Ces travaux s'intéressent à la modélisation formelle de la sémantique des langues naturelles. Pour cela, nous suivons le principe de compositionnalité qui veut que le sens d'une expression complexe soit une fonction du sens de ses parties. Ces fonctions sont généralement formalisées à l'aide du [lambda]-calcul. Cependant, ce principe est remis en cause par certains usages de la langue, comme les pronoms anaphoriques ou les présuppositions. Ceci oblige à soit abandonner la compositionalité, soit modifier les structures du sens. Dans le premier cas, le sens n'est alors plus obtenu par un calcul qui correspond à des fonctions mathématiques, mais par un calcul dépendant du contexte, ce qui le rapproche des langages de programmation qui manipulent leur contexte avec des effets de bord. Dans le deuxième cas, lorsque les structures de sens sont ajustées, les nouveaux sens ont tendance à avoir une structure de monade. Ces dernières sont elles-mêmes largement utilisées en programmation fonctionnelle pour coder des effets de bord, que nous retrouvons à nouveau. Par ailleurs, s'il est souvent possible de proposer le traitement d'un unique phénomène, composer plusieurs traitements s'avère être une tâche complexe. Nos travaux proposent d'utiliser les résultats récents autour des langages de programmation pour parvenir à combiner ces modélisations par les effets de bord. Dans la première partie de la thèse, nous démontrons les propriétés fondamentales de ce calcul (préservation de type, confluence et terminaison). Dans la seconde partie, nous montrons comment utiliser le calcul pour le traitement de plusieurs phénomènes linguistiques : deixis, quantification, implicature conventionnelle, anaphore et présupposition. Enfin, nous construisons une unique grammaire qui gère ces phénomènes et leurs interactions.
Les plateformes en ligne telles que les blogs et les réseaux sociaux permettent aux internautes de s'exprimer sur des sujets d'une grande variété (produits commerciaux, politique, services, etc.). Cet important volume de données d'opinions peut être exploré et exploité grâce à des techniques de fouille de texte connues sous le nom de fouille d'opinions ou analyse de sentiments. Contrairement à la majorité des travaux actuels en fouille d'opinions, qui se focalisent sur les opinions simplement positives ou négatives (ou un intermédiaire entre ces deux extrêmes), nous nous intéressons dans cette thèse aux points de vue. La fouille de point de vue généralise l'opinion au delà de son acception usuelle liée à la polarité (positive ou négative) et permet l'étude d'opinions exprimées plus subtilement, telles que les opinions politiques. Dans notre première contribution, nous avons exploré l'idée de différencier mots d'opinions (spécifiques à la fois à un point de vue et à un thème) et mots thématiques (dépendants du thème mais neutres vis-à-vis des différents points de vue) en nous basant sur les parties de discours, inspirée par des pratiques similaires dans la littérature de fouille d'opinions classique - restreinte aux opinions positives et négatives. Notre seconde contribution se focalise quant à elle sur les points de vue exprimés sur les réseaux sociaux. Notre objectif est ici d'analyser dans quelle mesure l'utilisation des interactions entre utilisateurs, en outre de leur contenu textuel généré, est bénéfique à l'identification de leurs points de vue. Nos différentes contributions ont été évaluées et comparées à l'état de l'art sur des collections de documents réels.
La reconnaissance des actions à partir de vidéos est l'un des principaux problèmes de vision par ordinateur. Malgré des recherches intensives, la différenciation et la reconnaissance d'actions similaires restent un défi. Cette thèse porte sur la classification des gestes sportifs à partir de vidéos, avec comme cadre applicatif le tennis de table. Nous proposons une méthode d'apprentissage profond pour segmenter et classifier automatiquement les différents coup de Tennis de Table. Les annotations consistent en une description des coups effectués (début, fin et type de coup). Au total, 20 différents coups de tennis de table sont considérés plus une classe de rejet. La reconnaissance des actions similaires présente des différences avec la reconnaissance d'actions classique. En effet, dans les bases de données classiques, le contexte de l'arrière plan fournit souvent des informations discriminantes que les méthodes peuvent utiliser pour classer l'action plutôt que de se concentrer sur l'action elle-même. Dans notre cas, la similarité entre classes est élevée, les caractéristiques visuelles discriminantes sont donc plus difficiles à extraire et le mouvement joue un rôle clef dans la caractérisation de l'action. Dans cette thèse, nous introduisons un réseau de neurones spatio-temporel convolutif avec une architecture Jumelle. Ce réseau d'apprentissage profond prend comme entrées une séquence d'images RVB et son flot optique estimé. Les données RVB permettent à notre modèle de capturer les caractéristiques d'apparence tandis que le flot optique capture les caractéristiques de mouvement. Ces deux flux sont traités en parallèle à l'aide de convolutions 3D, et sont fusionnés à la dernière étape du réseau. Notre méthode obtient une performance de classification de 93.2% sur l'ensemble des données tests. Appliquée à la tâche jointe de détection et de classification, notre méthode atteint une précision de 82.6%.Nous étudions les performances en fonction des types de données utilisés en entrée et la manière de les fusionner. Différents estimateurs de flot optique ainsi que leur normalisation sont testés afin d'améliorer la précision. Les caractéristiques de chaque branche de notre architecture sont également analysées afin de comprendre le chemin de décision de notre modèle. Enfin, nous introduisons un mécanisme d'attention pour aider le modèle à se concentrer sur des caractéristiques discriminantes et aussi pour accélérer le processus d'entraînement. Nous comparons notre modèle avec d'autres méthodes sur TTStroke-21 et le testons sur d'autres ensembles de données. Nous constatons que les modèles fonctionnant bien sur des bases de données d'actions classiques ne fonctionnent pas toujours aussi bien sur notre base de données d'actions similaires. Les travaux présentés dans cette thèse ont été validés par des publications dans une revue internationale, cinq papiers de conférences internationales, deux papiers d'un workshop international et une tâche reconductible dans le workshop MediaEval où les participants peuvent appliquer leurs méthodes de reconnaissance d'actions à notre base de données TTStroke-21. Deux autres papiers de workshop internationaux sont en cours de préparation, ainsi qu'un chapitre de livre.
Avec notre thèse, nous voulons poser les fondements linguistiques et didactiques nécessaires à la future élaboration d'un programme ou d'une méthode en intercompréhension slave, en prenant l'exemple des langues slaves de l'ouest et du sud-ouest et en fournissant une analyse linguistique de trois langues : le tchèque, le slovène et le croate. Dans notre travail, nous cherchons principalement à fournir deux éléments : - Une série d'hypothèses linguistiques ayant pour objectif de déterminer les points à enseigner dans une méthode d'intercompréhension concernant le tchèque, le slovène et le croate ; - Une présentation de programmes et de supports en didactique de l'intercompréhension réalisés et testés dans le cadre de notre cursus. Dans notre travail, nous constatons que la didactique de l'intercompréhension slave diffère en de nombreux points avec les apprentissages classiques. Dans le cas de l'intercompréhension, de nombreux points normalement lourds et complexes à maîtriser peuvent n'être que survolés. Grâce à nos analyses, tant sur le plan linguistique que didactique, nous avons pu fournir une réflexion sur l'une des formes que pourra prendre une formation en intercompréhension slave dans le futur. Nous préconisons particulièrement l'utilisation de ressources en ligne, via, par exemple, le site www.rozrazum.eu, développé dans le cadre de cette thèse afin de tester des activités respectant la méthodologie proposée par Eurom 5 (Bonvino et al. 2001). Ce site pourra servir, dans un premier temps, de plate-forme de test et de mise au point d'approches didactiques, tout en étant fonctionnel, et donc disponible à un public d'apprenants.
L'intelligence épidémiologique a pour but de détecter, d'analyser et de surveiller au cours du temps les potentielles menaces sanitaires. Ce processus de surveillance repose sur des sources dites formelles, tels que les organismes de santé officiels, et des sources dites informelles, comme les médias. Cette thèse se concentre sur l'extraction et la combinaison d'informations épidémiologiques extraites d'articles de presse en ligne, dans le cadre de la veille des maladies infectieuses animales. Dans ce manuscrit, nous proposons de nouvelles représentations textuelles fondées sur la sélection, l'expansion et la combinaison de descripteurs épidémiologiques. Nous montrons que l'adaptation et l'extension de méthodes de fouille de texte et de classification permet d'améliorer l'utilisation des articles en ligne tant que source de données sanitaires. Nous mettons en évidence le rôle de l'expertise quant à la pertinence et l'interprétabilité de certaines des approches proposées. Bien que nos travaux soient menés dans le contexte de la surveillance de maladies en santé animale, nous discutons des aspects génériques des méthodes proposées, vis-à-vis de de maladies inconnues et dans un contexte
La traduction automatique des documents est considérée comme l'une des tâches les plus difficiles en traitement automatique des langues et de la parole. Les particularités linguistiques de certaines langues, comme la langue arabe, rendent la tâche de traduction automatique plus difficile. Notre objectif dans cette thèse est d'améliorer les systèmes de traduction de l'arabe vers le français et vers l'anglais. Les principales recherches portent à la fois sur la construction de corpus parallèles, le prétraitement de l'arabe et sur l'adaptation des modèles de traduction et de langue. Tout d'abord, un corpus comparable journalistique a été exploré pour en extraire automatiquement un corpus parallèle. Ensuite, différentes approches d'adaptation du modèle de traduction sont exploitées, soit en utilisant le corpus parallèle extrait automatiquement soit en utilisant un corpus parallèle construit automatiquement. Nous démontrons que l'adaptation des données du système de traduction permet d'améliorer la traduction. Un texte en arabe doit être prétraité avant de le traduire et ceci à cause du caractère agglutinatif de la langue arabe. Nous présentons notre outil de segmentation de l'arabe, SAPA (Segmentor and Part-of-speech tagger for Arabic), indépendant de toute ressource externe et permettant de réduire les temps de calcul. Cet outil permet de prédire simultanément l'étiquette morpho-syntaxique ainsi que les proclitiques (conjonctions, prépositions, etc.) pour chaque mot, ensuite de séparer les proclitiques du lemme (ou mot de base). Nous décrivons également dans cette thèse notre outil de détection des entités nommées, NERAr (Named Entity Recognition for Arabic), et nous examions l'impact de l'intégration de la détection des entités nommées dans la tâche de prétraitement et la pré-traduction de ces entités nommées en utilisant des dictionnaires bilingues. Nous présentons par la suite plusieurs méthodes pour l'adaptation thématique des modèles de traduction et de langue expérimentées sur une application réelle contenant un corpus constitué d'un ensemble de phrases multicatégoriques. Ces expériences ouvrent des perspectives importantes de recherche comme par exemple la combinaison de plusieurs systèmes lors de la traduction pour l'adaptation thématique. Il serait également intéressant d'effectuer une adaptation temporelle des modèles de traduction et de langue. Finalement, les systèmes de traduction améliorés arabe-français et arabe-anglais sont intégrés dans une plateforme d'analyse multimédia et montrent une amélioration des performances par rapport aux systèmes de traduction de base.
Cette recherche se concentre sur un modèle causal de certains antécédents et les conséquences des dimensions des relations BRQ (chaud vs froid) : un test empirique dans un contexte Vietnamien. Les objectifs de cette recherche sont : premièrement, déterminer les effets de la personnalité de la marque sur les antécédents de deux composantes de la BRQ dans le contexte Vietnamien. Deuxièmement, étudier l'impact des antécédents et les conséquences des deux composantes de BRQ sur l'intention d'achat de la marque dans le contexte Vietnamien. Un modèle structurel a été développé illustrant la relation entre la personnalité de la marque (antidépresseur) et les conséquences d'une relation de qualité de marque (BRQ). Cela a entraîné le développement de vingt hypothèses. Pour répondre aux objectifs de cette recherche, des données ont été recueillies, axées sur six classes de produits et, au final, 634 questionnaires ont été recueillis. Tout d'abord, les résultats de cette étude révèlent que la personnalité de la marque a une influence positive sur les deux variables de l'auto-congruence et de la qualité des partenaires, mais on voit clairement qu'il existe une différence entre le niveau d'influence et d'importance. Deuxièmement, étant donné que l'auto-congruence a un effet plus important sur le chaud que froid BRQ, d'autre part, la qualité des partenaires a un effet plus important sur le froid que chaud BRQ. Cependant, en fonction du coefficient de chemin de l'auto-congruence et de la qualité des partenaires, les résultats révèlent que l'auto-congruence a un effet positif sur les chauds et froids BRQ par rapport à la qualité des partenaires. En ce qui concerne les résultats de la condamnation des deux composantes de BRQ, nous avons constaté que la taille de l'ensemble de considération et le bouche-à-oreille n'ont aucune relation avec l'intention de la marque, alors que WTP a un effet positif sur la marque. Les principales contributions de cette recherche permettent de mieux comprendre le comportement des consommateurs sur le marché Vietnamien. Cependant, le froid BRQ a considérablement influencé le bouche-à-oreille du consommateur. Par conséquent, le chaud BRQ, qui est la qualité de la relation émotionnelle, augmente principalement le comportement de fidélité des clients. En revanche, le froid BRQ aide à attirer de nouveaux clients' grâce à une communication positive de bouche-à-oreille des clients. Le maintien des clients actuels et l'attrait des clients potentiels sont des moteurs essentiels pour la survie d'une marque ou d'un produit. Les gestionnaires ont besoin, par conséquent, d'avoir une incidence positive sur le chaud et froid BRQ de leurs clients. En outre, en fonction des résultats de la recherche, ils devraient se concentrer sur une volonté de payer un prix haut de gamme afin d'augmenter leur intention d'achat de marque.
Le génome bactérien est classiquement pensé comme constitué de “chromosomes”, éléments génomiques essentiels pour l'organisme, stables et à évolution lente, et de “plasmides”, éléments génomiques accessoires, mobiles et à évolution rapide. La distinction entre plasmides et chromosomes a récemment été mise en défaut avec la découverte dans certaines lignées bactériennes d'éléments génomiques intermédiaires, possédant à la fois des caractéristiques de chromosomes et de plasmides. Cependant, leur véritable nature et les mécanismes permettant leur intégration dans le génome stable reste à caractériser. En utilisant les protéines liées aux Systèmes de Transmission de l'Information Génétique (STIG) comme variables descriptives des éléments génomiques bactériens (ou réplicons), une étude globale de génomique comparative a été conduite sur l'ensemble des génomes bactériens disponibles.
Les avancées récentes des Technologies de l'Information et de la Communication (TIC) ont entraîné des transformations radicales de plusieurs secteurs de l'industrie. L'adoption des technologies du Web Sémantique a démontré plusieurs avantages, surtout dans une application de Recherche d'Information (RI) : une meilleure représentation des données et des capacités de raisonnement sur celles-ci. Cependant, il existe encore peu d'applications industrielles car il reste encore des problèmes non résolus, tels que la représentation de documents hétérogènes interdépendants à travers des modèles de données sémantiques et la représentation des résultats de recherche accompagnés d'informations contextuelles. Dans cette thèse, nous abordons deux défis principaux. Le deuxième défi porte sur la construction des résultats de RI, à partir de ce corpus de documents hétérogènes, aidant les utilisateurs à mieux interpréter les informations pertinentes de leur recherche surtout quand il s'agit d'exploiter les relations inter/intra-documentaires. Pour faire face à ces défis, nous proposons tout d'abord une représentation sémantique du corpus de documents hétérogènes à travers un modèle de graphe sémantique couvrant à la fois les dimensions structurelle et métier du corpus. Ensuite, nous définissons une nouvelle structure de données pour les résultats de recherche, extraite à partir de ce graphe, qui incorpore les informations pertinentes directes ainsi qu'un contexte structurel et métier. Afin d'exploiter cette nouvelle structure dans un modèle de RI novateur, nous proposons une chaine de traitement automatique de la requête de l'utilisateur, allant du module d'interprétation de requête, aux modules de recherche, de classement et de présentation des résultats. Cependant, dans cette thèse, les expérimentations ont été appliquées au domaine du Bâtiment et Travaux Publics (BTP), en s'appuyant sur des projets de construction.
Cette thèse porte sur l'adaptation de la synthèse paramétrique de la parole à partir d'un texte écrit à la langue arabe. Pour ce faire, différentes méthodes ont été développées afin de mettre en place des systèmes de synthèse. Ces méthodes sont basées sur une description du signal de parole par un ensemble de paramètres acoustiques et prosodiques. De même, chaque son est représenté par un ensemble de descripteurs contextuels contenant toutes les informations affectant la prononciation de celui-ci. Une partie de ces descripteurs dépend de la langue et de ses particularités, ainsi, afin d'adapter l'approche de synthèse paramétrique à l'arabe, une étude des particularités phonologiques de l'arabe était nécessaire. L'accent a été mis sur deux phénomènes : la gémination et la longueur des voyelles (courte/longue). Deux descripteurs associés à ces deux phénomènes ont été ajoutés à l'ensemble des descripteurs contextuels. Quatre combinaisons de modélisation sont possibles en alternant la différentiation ou la fusion des consonnes simples et géminées d'une part et des voyelles courtes et longues d'autres part. Un ensemble des tests perceptifs et objectifs a été conduit afin d'évaluer l'effet des quatre approches de modélisation des unités sur la qualité de la parole synthétisée. Les évaluations ont été faites dans le cas de synthèse paramétrique par HMM (Hidden Markov Model) puis dans le cas de la synthèse paramétrique par DNN. Les résultats subjectifs sont montrés que dans le cas de l'approche par HMM, les quatre approches produisent des signaux de qualité similaire, une conclusion qui a été confirmée par les mesures objectives calculées pour évaluer la prédiction des durées des unités de parole. Cependant, les résultats des évaluations objectives dans le cas de l'approche par DNN ont montré que la différentiation des consonnes simples (respectivement des voyelles courtes) des consonnes géminées (respectivement des voyelles longues) permet d'avoir une prédiction des durées légèrement meilleure qu'avec les autres des approches de modélisation. Une dernière partie de la thèse a été consacrée à la comparaison de l'approche de synthèse par HMM à celle par DNN. L'ensemble des tests conduits ont montré que l'utilisation des DNN a amélioré la qualité perçue des signaux générés.
Cette "mémoire syntaxique" se construit à partir de l'expérience et notamment de l'observation de séquences, suites d'objets dont l'organisation séquentielle obéit à des règles syntaxiques. Elle doit pouvoir être utilisée ultérieurement pour générer des séquences valides, c'est-à-dire respectant ces règles. Cette production de séquences valides peut se faire de façon explicite, c'est-à-dire en évoquant les règles sous-jacentes, ou de façon implicite, quand l'apprentissage a permis de capturer le principe d'organisation des séquences sans recours explicite aux règles. Bien que plus rapide, plus robuste et moins couteux en termes de charge cognitive que le raisonnement explicite, le processus implicite a pour inconvénient de ne pas donner accès aux règles et de ce fait, de devenir moins flexible et moins explicable. Au début, l'expert réalise un choix pour suivre explicitement les règles du métier. Mais ensuite, à force de répétition, le choix se fait automatiquement, sans évocation explicite des règles sous-jacentes. Ce changement d'encodage des règles chez un individu en général et particulièrement chez un expert métier peut se révéler problématique lorsqu'il faut expliquer ou transmettre ses connaissances. Si les concepts métiers peuvent être formalisés, il en va en général de tout autre façon pour l'expertise. Dans nos travaux, nous avons souhaité nous pencher sur les séquences de composants électriques et notamment la problématique d'extraction des règles cachées dans ces séquences, aspect important de l'extraction de l'expertise métier à partir des schémas techniques. Nous nous plaçons dans le domaine connexionniste, et nous avons en particulier considéré des modèles neuronaux capables de traiter des séquences. Nous avons évalué ces deux modèles sur différentes grammaires artificielles (grammaire de Reber et ses variations) au niveau de l'apprentissage, de leurs capacités de généralisation de celui-ci et leur gestion de dépendances séquentielles. Finalement, nous avons aussi montré qu'il était possible d'extraire les règles encodées (issues des séquences) dans le réseau récurrent doté de LSTM, sous la forme d'automate.
Ces systèmes sont entraînés sur des gros corpus de données, que l'on nomme big data et qui sont devenus le nouveau pétrole du 21e siècle. Les systèmes réussissent à capturer des éléments que l'humain n'est pas capable de mémoriser, en extrayant des régularités de ces grandes quantités de données. Nous nous proposons donc, dans cette optique, d'étudier les liens entre apprentissage et performances des systèmes de traitement automatique de la parole en se concentrant sur la répartition homme/femme. Le choix de la parole est motivé par la tendance actuelle faisant de la voix la nouvelle interface homme/machine. Nous souhaitons donc proposer non pas des catégories, mais un continuum qui nous permettra de caractériser nos corpus non plus en terme de genre, mais en terme de diversité vocale. Cette représentation permettra de mettre en évidence la variabilité vocale et de remettre en question la pertinence du genre en tant que distinction catégorielle, tout en permettant une étude socio-phonétique des rôles des locuteurs et des interactions.
Si le concept de décentralisation est en quelque sorte inscrit dans le principe même de l'Internet, son urbanisme actuel n'intègre ce principe que de manière limitée. En cherchant les meilleures solutions, certains développeurs se retournent vers les qualités persistantes d'une ancienne technologie, le pair-à-pair (P2P), qui replonge dans la topologie de l'Internet pré-commercial, mettant à profit les ressources des « nains » du réseau – ses marges ou sa périphérie. Cette thèse explore l'approche distribuée et décentralisée de l'architecture technique des services Internet. Il s'agit de comprendre ce que dessine une architecture de réseau décentralisée du point de vue de l'articulation des acteurs et des contenus, de la répartition de responsabilités, de l'organisation du marché et de la capacité à exercer du contrôle, des formes d'existence et des rôles d'entités telles que les nœuds du réseau, ses usagers, ses unités centrales. Ce travail analyse sous quelles conditions un réseau qui répartit à ses marges la responsabilité de son propre fonctionnement, et qui suit un modèle non hiérarchisé ou hybride, peut se développer dans l'Internet d'aujourd'hui. La thèse suit les développeurs de trois services Internet – un moteur de recherche, un service de stockage et une application pour le streaming vidéo – construits sur un modèle de réseau décentralisé, les collectifs d'usagers pionniers qui se développent avec ces services, et ponctuellement, les arènes politiques où l'on discute de l'organisation et de la gouvernance de l'Internet à moyen et long terme.
Cette thèse contribue à l'étude de la fiabilité et de la sécurité-innocuité des systèmes informatisés, modélisés par des systèmes à événements discrets. Les principales contributions concernent la théorie des Systèmes de Contrôle (notés C Systems) et l'approche par Monitoring des modèles. Dans la première partie de la thèse, nous étudions la théorie des Systèmes de Contrôle qui combine et étend de façon significative, les systèmes de réécriture de la théorie des langages et le contrôle supervisé. Un système de contrôle est une structure générique qui contient deux composants : le composant contrôlé et le composant contrôlant qui restreint le comportement du composant contrôlé. Les deux composants sont exprimés en utilisant le même formalisme comme des automates ou des grammaires. Nous considérons différentes classes de systèmes de contrôle basés sur différents formalismes comme, par exemple, les automates, les grammaires, ainsi que leurs versions infinies et concurrentes. Ensuite, une application de cette théorie est présentée. Les systèmes de contrôle basés sur les automates de Büchi sont utilisés pour vérifier par model-checking, des propriétés définissant la correction sur des traces d'exécution spécifiées par une assertion de type nevertrace. Dans la seconde partie de la thèse, nous investiguons l'approche de monitoring des modèles dont la théorie des systèmes de contrôle constitue les fondations formelles. Le principe pivot de cette approche est la « spécification de propriétés comme contrôleur » . En d'autres termes, pour un système, les exigences fonctionnelles, d'une part, et des propriétés, d'autre part, sont modélisées et implantées séparément, les propriétés spécifiées contrôlant le comportement issu des exigences fonctionnelles. De cette approche découle ainsi deux techniques alternatives, respectivement nommées monitoring de modèle et génération de modèle. Cette approche peut être utilisée de diverses manières pour améliorer la fiabilité et la sécurité-innocuité de divers types de systèmes. Nous présentons quelques applications qui montrent l'intérêt pratique de cette contribution théorique. Tout d'abord, cette approche aide à prendre en compte les évolutions des spécifications des propriétés. En second lieu, elle fournit une base théorique à la sécurité fonctionnelle, popularisée par la norme IEC 61508. En troisième lieu, l'approche peut être utilisée pour formaliser et vérifier l'application de guides de bonnes pratiques ou des règles de modélisation appliquées par exemple pour des modèles UML.Ces résultats constituent les bases pour des études futures de dispositifs plus perfectionnés, et fournissent une nouvelle voie pour s'assurer de la fiabilité et de la sécurité-innocuité des systèmes
Les interactions requièrent une confiance mutuelle des parties impliquées. Les entités d'Internet endossent plusieurs rôles, chacun ayant ses propres intérêts et motivations ; conduisant à des conflits qui doivent être adressés afin de permettre confiance et sécurité. Dans cette thèse nous nous concentrons sur la dynamique de frappe au clavier afin de résoudre quelques de ces conflits. La manière de taper au clavier est une modalité biométrique sans coûts et transparente, elle ne requiert ni capteurs ni actions additionnels. Malheureusement, elle permet aussi le profilage des utilisateurs (s.a. identification, âge, sexe), contre leur consentement et connaissance. Afin de protéger la vie privée des utilisateurs, nous proposons d'anonymiser la dynamique de frappe au clavier. Cependant, cette information peut être légitimement requise afin de renforcer l'authentification des utilisateurs Nous proposons ainsi un Code Personnel d'Identité Respectueux de la Vie Privée, permettant l'authentification biométrique des utilisateurs, sans menacer leur vie privée. Nous proposons aussi une preuve sociale d'identité permettant de vérifier des déclarations d'identités ainsi que la génération synthétique de dynamique de frappe au clavier.
Notre travail de recherche est consacré à l'analyse, d'un point de vue aspectuel, du rôle de la morphologie dérivationnelle dans le processus de construction du sens aspectuel lexical. Le préfixe RE- est très intéressant parce que sa structure processuelle est complexe. Il implique une relation entre un procès présupposé et un procès posé et cette relation est établie par un troisième procès, intermédiaire, qui crée un lien de continuité, de reprise ou d'interruption. L'activation de ces liens est notamment dépendante du sens de la base lexicale du dérivé et du sens aspectuel de cette même base.
L'un des aspects d'une interface homme-machine réussie (p. ex. interaction homme-robot, chatbots, parole, écriture manuscrite, etc.) est la possibilité d'avoir une interaction personnalisée. Cela affecte l'expérience humaine globale et permet une interaction plus fluide. Actuellement, il y a beaucoup de travaux qui utilisent l'apprentissage machine afin de modéliser de telles interactions. Cependant, ces modèles n'abordent pas la question du comportement personnalisé : ils tentent de faire la moyenne des différents exemples provenant de différentes personnes. L'identification des styles humains (persona) ouvre la possibilité de biaiser la sortie des modèles pour prendre en compte la préférence humaine. Dans cette thèse, nous nous sommes concentrés sur le problème des styles dans le contexte de l'écriture manuscrite. L'objectif de cette thèse est d'étudier ces problèmes de styles, dans le domaine de l'écriture. Nous disposons d'un jeu de données IRONOFF, un jeu de données d'écriture manuscrite en ligne, avec 410 rédacteurs, avec ~25K exemples de dessins en majuscules, minuscules et chiffres. Pour le problème de l'apprentissage par transfert, nous avons utilisé un jeu de données supplémentaire, QuickDraw !, un jeu de données de dessin d'esquisses contenant environ 50 millions de dessins sur 345 catégories. Les principales contributions de ma thèse sont :  1) Proposer un pipeline de travail pour étudier le problème des styles d'écriture. Il s'agit de proposer une méthodologie, des repères et des paramètres d'évaluation (et de fonder ces paramètres d'évaluation). Nous choisissons le paradigme des modèles génératifs temporels dans l'apprentissage profond afin de générer des dessins et d'évaluer leur proximité/pertinence par rapport aux dessins de vérité voulus/de terrain. Nous avons proposé deux métriques, pour évaluer la courbure et la longueur des dessins générés. Afin d'enraciner ces métis, nous avons proposé de multiples repères - dont nous connaissons le pouvoir relatif à l'avance -, puis vérifié que les mesures respectent effectivement la relation de pouvoir relatif. 2) Proposer un cadre pour l'étude et l'extraction des styles, et vérifier son avantage par rapport aux repères proposés précédemment. Nous nous sommes mis d'accord sur l'idée d'utiliser un auto-encodeur conditionné en profondeur pour résumer et extraire les informations de style, sans avoir besoin de nous concentrer sur l'identité de la tâche (puisqu'elle est donnée comme une condition). Nous validons ce cadre par rapport au repère proposé précédemment à l'aide de nos paramètres d'évaluation. Nous visualisons également les styles extraits, ce qui nous permet d'obtenir des résultats passionnants ! 3) En utilisant le cadre proposé, proposer un moyen de transférer l'information sur les styles entre les différentes tâches, et un protocole afin d'évaluer la qualité du transfert. Nous avons exploité le codeur automatique conditionné profond utilisé précédemment, en extrayant la partie codeur - qui, selon nous, contenait les informations pertinentes sur les styles - et en l'utilisant dans de nouveaux modèles formés sur de nouvelles tâches. Nous testons intensivement ce paradigme sur une gamme différente de tâches, à la fois sur les ensembles de données IRONOFF et QuickDraw !. Nous montrons que nous pouvons transférer avec succès les informations de style entre différentes tâches.
L'étude 1 nous a permis de mettre en évidence des vitesses de saisie de texte chez les personnes tétraplégiques et d'étudier l'influence de leurs aides techniques d'accès à l'outil informatique sur cette vitesse. L'étude 2 nous a permis de mettre en avant l'hétérogénéité des résultats d'un logiciel de prédiction de mots sur la vitesse de saisie de texte sur une population hétérogène et sans paramétrage de ces logiciels. L'étude 3 nous a permis d'étudier les habitudes de préconisations et de paramétrages des logiciels de prédictions de mots par les professionnels. Les études 4 et 5 nous ont permis d'évaluer l'influence des paramétrages (nombre de mots affichés dans la liste de prédiction et l'adaptation du logiciel au vocabulaire de l'utilisateur) sur cette saisie de texte. Enfin, l'étude 6 nous a permis d'étudier l'influence d'un entraînement dirigé par des professionnels sur les logiciels de prédictions de mots chez des personnes tétraplégiques, sur la vitesse de saisie de texte. Les paramétrages (nombre de mots affichés dans la liste de prédiction et l'adaptation du logiciel au vocabulaire de l'utilisateur) ont une influence différente en fonction du niveau lésionnel des personnes tétraplégiques sur la vitesse de saisie de texte, le nombre d'erreurs ou le confort. De plus, une différence entre l'importance donnée aux paramétrages par les professionnels préconisateurs et les paramétrages effectivement réglés a été mise en évidence. Enfin, l'influence d'un entraînement dirigé sur la vitesse de saisie de texte a été mise en évidence sur la vitesse de saisie de texte. Au regard de l'ensemble de ces résultats, il apparait nécessaire de paramétrer les logiciels de prédictions de mots, mais aussi de connaitre l'influence des différents réglages et de diffuser cette information au sein des réseaux professionnels. Une systématisation des entraînements dirigés sur les logiciels de prédiction de mots nécessite une réflexion et une validation sur les modalités et la nature de ces accompagnements.
Dans ce cadre, nous proposons une mise en oeuvre statistique de la sémantique textuelle interprétative sur un corpus de textes de Pierre Mendès France (1922-1982) : sont construits des parcours interprétatifs qui vont du global – le corpus – vers le local – ses partitions contrastives – en considérant l'influence de la situation historico-sociale sur les textes. Ainsi, deux grandes variables qui traversent le corpus sont testées : la variable chronologique et la variable générique. Nous établissons que le corpus est structuré par ses hautes fréquences et étudions, de manière privilégiée, le vocabulaire banal. On y applique deux traitements cooccurrentiels : l'environnement cotextuel et la cooccurrence asymétrique. Le premier aboutit à la description de fonds textuels et intertextuels sur lesquels se détachent des formes sémantiques. La seconde décrit des contours rythmiques de variation lexicale qui semblent associés, dans notre corpus, à des visées argumentatives distinctes : informatives ou persuasives-explicatives.
Définissons une préférence spécifique comme une préférence qui ne serait partagée pour aucun groupe d'utilisateurs. Un utilisateur possédant plusieurs préférences spécifiques qu'il ne partage avec aucun autre utilisateur sera probablement mal servi par une approche de FC classique. Il s'agit du problème des Grey Sheep Users (GSU). Dans cette thèse, je réponds à trois questions distinctes. 1) Qu'est-ce qu'une préférence spécifique ? J'apporte une réponse en proposant des hypothèses associées que je valide expérimentalement. 2) Comment identifier les GSU dans les données ? Cette identification est importante afin d'anticiper les mauvaises recommandations qui seront fournies à ces utilisateurs. Je propose des mesures numériques permettant d'identifier les GSU dans un jeu de données de recommandation sociale. Ces mesures sont significativement plus performantes que celles de l'état de l'art. Enfin, comment modéliser ces GSU pour améliorer la qualité des recommandations qui leurs sont fournies ? Je propose des méthodes inspirées du domaine de l'apprentissage automatique et dédiées à la modélisation des GSU permettant d'améliorer la qualité des recommandations qui leurs sont fournies.
Ce travail de thèse se situe dans le difficile contexte de la linguistique et de l'informatique. Plus précisément, il s'agit de montrer l'intérêt de la prise en compte simultanée de la structure du document et des connaissances linguistiques pour la classification de documents suivant leur style. Pour cela, nous avons défini de nouveaux descripteurs, qui, combinés avec des descripteurs linguistiques exploitant la hiérarchie textuelle, sont pertinents pour caractériser des types de documents. Puis, nous avons proposée une méthode de classification fondée sur l'absence des motifs dans les documents. Une des originalités de notre travail est d'associer des méthodes linguistiques et d'apprentissage automatique à des techniques de recherche de motifs locaux. Cette hiérarchisation représente la structure logique du document fondée sur le principe que différentes fenêtres d'observation correspondent à des différents types d'information. Ces derniers sont reliés entre eux par le biais de la notion de l'héritage du contexte afin de préserver la cohérence globale du document. D'autre part, des hypothèses liées à la tâche de catégorisation sont émergées telle que l'exploitation de l'absence totale ou partielle de motifs sous certaines contraintes, qui peut servir à construire de nouvelles analogies pour la catégorisation des documents. Alors, en analysant par évidence les motifs à fréquences faibles ou nulles, une nouvelle approche de catégorisation par exclusion-inclusion a été proposée en introduisant une nouvelle notion telle que les motifs exclusifs
Depuis une trentaine d'années, les images scientifiques font l'objet d'un intérêt croissant dans le champ des sciences humaines et sociales, de l'histoire de l'art, de l'histoire des sciences et de l'épistémologie de manière générale. Ce qui a favorisé la naissance, puis l'essor, dans le monde anglo-saxon, des visual studies et plus particulièrement d'une branche qui intéresse spécifiquement l'épistémocritique : les visual studies of science. Les sciences entretiennent en effet un lien intime dans leur production et leur diffusion du savoir avec les représentations visuelles : diagrammes, schémas, photographies, figures géométriques, équations, etc. Cette articulation entre le « voir » et le « savoir » a ouvert de nouvelles voies à la recherche en épistémocritique qui noue désormais des relations fécondes avec les études visuelles. Ces dernières ont insisté à la fois sur le rôle crucial de l'image dans le processus d'acquisition des connaissances (modélisation, force de propositions paradigmatiques, support visuel à vocation didactique, document-témoin des pratiques scientifiques, etc.). Au cœur de notre projet, se trouve un type d'image particulier, les diagrammes qui sont des outils heuristiques permettant à un savoir encore tâtonnant de surgir et de prendre forme. Formes hybrides, faites d'image et d'écriture les diagrammes posent à leur manière la question des rapports entre la connaissance et les procédures de visualisation dans la démarche scientifique, question qui a pris une acuité particulière avec la crise du langage et de la représentation suscitée par la révolution scientifique au tournant du XIXème et du XXème siècle. Traditionnellement, on oppose le concept qui est du côté de l'argumentation discursive au diagramme qui est du côté de l'intuition et de l'imagination. Dans ce domaine, le début du XXème siècle est aussi marqué par des innovations formelles, notamment par le recours à des procédures de visualisation et de spatialisation qui visent à renouveler les modes de représentation. Si l'on considère que ces innovations sont le produit d'une imagination diagrammatique, alors on peut envisager cette dernière comme un pont entre la démarche scientifique et la démarche littéraire. Le diagramme est une représentation visuelle de concepts ou d'idées mis en relation. Il peut aussi montrer les relations constitutives d'un objet ou les relations entre des objets hétérogènes. Comme les théoriciens du diagramme l'ont bien montré, le diagramme n'est pas une illustration : il n'est pas une mise en forme spatialisée d'une idée qui lui préexisterait mais il fait advenir l'idée à travers une représentation visuelle. La relation analogique avec l'objet (relation iconique) sur laquelle se fonde le diagramme concerne les relations des parties entre elles et avec le tout. Par ailleurs, les diagrammes se caractérisent par leur caractère hybride qui mêle texte et image. Prenant pour objet la pensée diagrammatique, notre projet s'intéressera à tous les dispositifs où texte et image se combinent dans des figures graphiques qui entretiennent une relation iconique avec leur objet. Plus largement, on fera l'hypothèse que l'imagination diagrammatique met en jeu une série de tensions qui recouvrent en partie les tensions entre littérature et science : tension entre image et concept, entre imagination et raison, entre intelligible et sensible, entre textuel et visuel, entre visible et lisible, etc. Nos œuvres dessineront donc un corpus synchronique dans la période du fin XIXème-XXème qui correspond à celle où les sciences, et les mathématiques en particulier, sont confrontées à une crise de la représentation qui interroge à nouveau frais les relations entre le langage et le monde. Nous nous demanderons comment les textes littéraires enregistrent et répercutent cette crise, comment ils la mettent en scène avec leurs propres moyens, et comment la pensée mathématique peut devenir l'origine même du processus de création.
Dans le contexte de changement climatique qui voit se multiplier les épisodes de sécheresse et les territoires sujets à de forts stress hydriques, cette thèse s'intéresse à la manière de gérer le déséquilibre entre disponibilité de la ressource en eau et demande croissante à Phoenix et à Tucson, dans l'Ouest aride des États-Unis. Pour ce faire, la thèse mobilise les outils et concepts de l'urban political ecology afin d'observer les luttes de pouvoir entre les acteurs de la gestion de l'eau dans un contexte où le système de grandes infrastructures hydrauliques sous-tendant la croissance urbaine est de plus en plus remis en question. Ce travail de recherche met en œuvre des protocoles d'enquête quantitatifs et qualitatifs : analyse des discours à partir de corpus de textes extraits de la presse et de Twitter pour déconstruire le discours dominant sur l'eau ; entretiens semi-directifs avec les acteurs de l'eau (institutions et militants écologistes) et observation participante pour questionner les tensions entre discours et changements entrepris dans les pratiques urbaines à l'échelle locale pour économiser l'eau. La thèse montre d'une part que les stratégies d'adaptation sont mises en œuvre par les acteurs dominants dans le cadre de fixes socio-écologiques afin de maintenir la trajectoire de croissance de villes particulièrement attractives.
Dans ce thèse, nous présentons les implicatures conversationnelles de manière intuitive en nous appuyant sur le concept très large de granularité de Jerry Hobbs. Nous présentons ensuite les implicatures conversationnelles d'un point de vue interdisciplinaire en débutant par ses origines Gricéennes et en passant par la sociologie à travers la théorie de la politesse, l'inférence par l'abduction et les systèmes de dialogues par la théorie des actes de langage. Enfin, nous motivons les deux lignes d'approches de cette thèse pour l'étude des implicatures conversationnelles : l'analyse empirique d'un corpus de conversation située et finalisée, et l'analyse par synthèse dans le cadre d'un jeu d'aventure textuel.
La disponibilité de quantités massives de données, comme des images dans les réseaux sociaux, des signaux audio de téléphones mobiles, ou des données génomiques ou médicales, a accéléré le développement des techniques d'apprentissage automatique. Récemment, les systèmes d'apprentissage profond ont émergé comme des algorithmes d'apprentissage très efficaces. Ces modèles multi-couche effectuent leurs prédictions de façon hiérarchique, et peuvent être entraînés à très grande échelle avec des méthodes de gradient. Leur succès a été particulièrement marqué lorsque les données sont des signaux naturels comme des images ou des signaux audio, pour des tâches comme la reconnaissance visuelle, la détection d'objets, ou la reconnaissance de la parole. Pour de telles tâches, l'apprentissage profond donne souvent la meilleure performance empirique, mais leur compréhension théorique reste difficile à cause du grand nombre de paramètres, et de la grande dimension des données. Leur succès est souvent attribué à leur capacité d'exploiter des structures des signaux naturels, par exemple en apprenant des représentations invariantes et multi-échelle de signaux naturels à travers un bon choix d'architecture, par exemple avec des convolutions et des opérations de pooling. Néanmoins, ces propriétés sont encore mal comprises théoriquement, et l'écart entre la théorique et pratique en apprentissage continue à augmenter. Cette thèse vise à réduire cet écart grâce à l'étude d'espaces de fonctions qui surviennent à partir d'une certaine architecture, en particulier pour les architectures convolutives. Notre approche se base sur les méthodes à noyaux, et considère des espaces de Hilbert à noyaux reproduisant (RKHS) associés à certains noyaux construits de façon hiérarchique selon une architecture donnée. Cela nous permet d'étudier précisément des propriétés de régularité, d'invariance, de stabilité aux déformations du signal, et d'approximation des fonctions du RKHS. Ces propriétés sur la représentation sont aussi liées à des questions d'optimisation pour l'entraînement de réseaux profonds à très grand nombre de neurones par descente de gradient, qui donnent lieu à de tels noyaux. Cette théorie suggère également des nouvelles stratégies pratiques de régularisation qui permettent d'obtenir une meilleure performance en généralisation pour des petits jeux de données, et une performance état de l'art pour la robustesse à des perturbations adversariales en vision.
Une Ligne de Produits Logiciels (LPL) supporte la gestion d'une famille de logiciels. Cette approche se caractérise par une réutilisation systématique des artefacts communs qui réduit le coût et le temps de mise sur le marché et augmente la qualité des logiciels. Cependant, une LPL exige un investissement initial coûteux. Cependant, l'efficacité de cette pratique se dégrade proportionnellement à la croissance de la famille de produits, qui devient difficile à maintenir. Dans cette thèse, nous proposons une approche hybride qui utilise à la fois une LPL et l'approche C&amp ; O pour faire évoluer une famille de produits logiciels. Le développeur peut alors réduire ces possibilités en exprimant ses préférences (e.g. produits, artefacts) et en utilisant les estimations de coûts sur les opérations que nous proposons. Nous avons étayé cette thèse en développant le framework SUCCEED (SUpporting Clone-and-own with Cost-EstimatEd Derivation) et l'avons appliqué à une étude de cas sur des familles de portails web.
Depuis les travaux fondateurs de R. Bolt « Mets ça là » combinant la voix et le geste, les modalités d'interaction se sont multipliés, diversifiés et améliorés. Les récents paradigmes d'interaction comme les interfaces tangibles incarnées ou la réalité augmentée, couplés aux progrès des systèmes de localisation, à la miniaturisation des dispositifs, à la qualité des réseaux sans fils, à l'amélioration de la reconnaissance de la parole ou de gestes ouvrent un vaste champ de possibilités d'interaction pour les systèmes multimodaux. Ce travail de thèse aborde ce problème de conception et de développement pour la multimodalité en entrée (de l'utilisateur vers le système informatique). Nous décrivons un modèle conceptuel de la multimodalité qui organise dans un canevas unificateur les modalités et leurs formes de combinaison. Basé sur ce modèle, nous définissons une approche générique à composants logiciels, notée ICARE, facilitant et accélérant la conception, le développement et le maintien des interfaces multimodales. Nous démontrons l'apport de cette approche par l'outil ICARE qui est une opérationnalisation de notre approche à composants. Un éditeur graphique est fourni, simplifiant la phase d'assemblage des composants et générant automatiquement le code correspondant à l'interaction multimodale.
La planification de tournées de véhicules dans des environnements urbains denses est un problème difficile qui nécessite des solutions robustes et flexibles. Au lieu de cela, nous proposons dans cette thèse d'étudier l'application de méthodes d'apprentissage par renforcement multi-agent (MARL) aux DS-VRPs en s'appuyant sur des réseaux de neurones profonds (DNNs). Nous avons ensuite proposé un nouveau modèle de décision séquentiel en relâchant la contrainte d'observabilité partielle que nous avons baptisé MDP multi-agent séquentiel (sMMDP). Ce modèle permet de décrire plus naturellement les DS-VRPs, dans lesquels les véhicules prennent la décision de servir leurs prochains clients à l'issu de leurs précédents services, sans avoir à attendre les autres.
Cependant, l'implémentation de cette technologie dans des applications réelles est entravée par la grande dégradation des performances en présence de nuisances acoustiques en phase d'utilisation. Un grand effort a été investi par la communauté de recherche en RAL dans la conception de techniques de compensation des nuisances acoustiques. Ces techniques opèrent à différents niveaux : signal, paramètres acoustiques, modèles ou scores. Afin de mettre en œuvre cette méthodologie, des exemples de données propres / corrompues sont générés artificiellement et utilisés pour construire des algorithmes de compensation des nuisances acoustiques. Ce procédé permet d'éviter les dérivations qui peuvent être complexes, voire très approximatives. La deuxième classe de techniques n'utilise aucun modèle de distorsion dans le domaine des i Des expériences ont été réalisées sur les données bruitées ainsi que les données de courte durée ; donnés de NIST SRE 2008 bruitées/découpées artificiellement ainsi que les données du challenge SITW bruitées naturellement / de courte durée.
Même avec le récent passage à 280 caractères, les messages de Twitter considérés dans leur singularité, sans information additionnelle exogène, peuvent confronter leurs lecteurs à des difficultés d'interprétation. L'ajout d'une contextualisation à ces messages s'avère donc une voie de recherche prometteuse pour faciliter l'accès à leur contenu informationnel. Dans la dernière décennie, la majorité des travaux se sont concentrés sur la construction de résumés à partir de sources d'information complémentaires telles que Wikipédia. Nous avons choisi dans cette thèse une voie complémentaire différente qui s'appuie sur l'analyse des conversations sur Twitter afin d'extraire des informations utiles à la contextualisation d'un tweet. Ces informations ont été intégrées dans un prototype qui, pour un tweet donné, propose une visualisation d'un sous-graphe du graphe de conversation associé au tweet. Ce sous-graphe extrait automatiquement à partir de l'analyse des distributions des indicateurs structurels, permet de mettre en évidence notamment des individus qui jouent un rôle majeur dans la conversation et des tweets qui ont contribué à la dynamique des échanges. Ce prototype a été testé sur un panel d'utilisateurs, pour valider son apport et ouvrir des perspectives d'amélioration.
Dans un second temps, nous considérons le problème des bandits corrompus, dans lequel un processus de corruption stochastique perturbe le retour d'information. Pour ce problème aussi, nous concevons des algorithmes pour obtenir un regret cumulatif asymptotiquement optimal.
Cette thèse a pour objectif la détection du langage figuratif dans les réseaux sociaux. Nous nous focalisons en particulier sur l'ironie et le sarcasme dans Twitter et proposons une approche basée sur l'apprentissage supervisée afin de prédire si le message véhiculé dans un tweet est ironique ou non. Les résultats obtenus pour cette tâche extrêmement complexe sont très encourageants et permettrons d'améliorer sensiblement la détection de polarité lors de l'analyse de sentiments.
La publication scientifique dans les revues spécialisées et les actes de conférences permet de communiquer les progrès en sciences. Les comités de rédaction et de programme sous-jacents représentent la clé de voûte du processus d'évaluation. Avec le développement des revues et le nombre croissant de conférences scientifiques organisées chaque année, rechercher des experts pour participer à ces comités est une activité chronophage mais critique. Cette thèse se focalise sur la tâche de suggestion de membres de comité de programme (CP) pour des conférences scientifiques. Elle comporte trois volets. Premièrement, nous proposons une modélisation basée sur un graphe hétérogène pondéré de l'expertise scientifique multifacette des chercheurs. Deuxièmement, nous définissons des indicateurs scientométriques pour quantifier les critères impliqués dans la constitution de CP. Troisièmement, nous concevons une approche de suggestion de membres de CP pour une conférence donnée, en combinant les résultats des indicateurs scientométriques susmentionnés. Notre approche est expérimentée pour une des conférences de premier plan de notre communauté de recherche : SIGIR, en considérant ses éditions de 1971 à 2015, ainsi que les conférences proches thématiquement.
Dans une interaction humain-agent, l'engagement de l'utilisateur est un élément essentiel pour atteindre l'objectif de l'interaction. Dans cette thèse, nous étudions comment l'engagement de l'utilisateur pourrait être favorisé par le comportement de l'agent. Basé sur les résultats de la dernière étude, nous proposons un Gestionnaire de Sujets axé sur l'engagement (modèle computationnel) qui personnalise les sujets d'une interaction dans des conversations où l'agent donne des informations à un utilisateur humain. Le Modèle de Sélection des Sujets du Gestionnaire de Sujets décide sur quoi l'agent devrait parler et quand. Pour cela, il prend en compte la perception par l'agent de l'utilisateur, qui est dynamiquement mis à jour, ainsi que l'état mental et les préférences de l'agent. Le Modèle de Transition de Sujets du Gestionnaire de Sujet, basé sur une étude empirique, calcule comment l'agent doit présenter les sujets dans l'interaction en cours sans perdre la cohérence de l'interaction. Nous avons implémenté et évalué le Gestionnaire de Sujets dans un agent virtuel conversationnel qui joue le rôle d'un visiteur dans un musée.
Depuis le premier séquençage du génome humain au début des années 2000, de grandes initiatives se sont lancé le défi de construire la carte des variabilités génétiques inter-individuelles, ou bien encore celle des altérations de l'ADN tumoral. Ces projets ont posé les fondations nécessaires à l'émergence de la médecine de précision, dont le but est d'intégrer aux dossiers médicaux conventionnels les spécificités génétiques d'un individu, afin de mieux adapter les traitements et les stratégies de prévention. La traduction des variations et des altérations de l'ADN en prédictions phénotypiques constitue toutefois un problème difficile. Les séquenceurs ou puces à ADN mesurent plus de variables qu'il n'y a d'échantillons, posant ainsi des problèmes statistiques. Les données brutes sont aussi sujettes aux biais techniques et au bruit inhérent à ces technologies. Enfin, les vastes réseaux d'interactions à l'échelle des protéines obscurcissent l'impact des variations génétiques sur le comportement de la cellule, et incitent au développement de modèles prédictifs capables de capturer un certain degré de complexité. Cette thèse présente de nouvelles contributions méthodologiques pour répondre à ces défis. Tout d'abord, nous définissons une nouvelle représentation des profils de mutations tumorales, qui exploite leur position dans les réseaux d'interaction protéine-protéine. Pour certains cancers, cette représentation permet d'améliorer les prédictions de survie à partir des données de mutations, et de stratifier les cohortes de patients en sous-groupes informatifs. Nous présentons ensuite une nouvelle méthode d'apprentissage permettant de gérer conjointement la normalisation des données et l'estimation d'un modèle linéaire. Nos expériences montrent que cette méthode améliore les performances prédictives par rapport à une gestion séquentielle de la normalisation puis de l'estimation. Pour finir, nous accélérons l'estimation de modèles linéaires parcimonieux, prenant en compte des interactions deux à deux, grâce à un nouvel algorithme. L'accélération obtenue rend cette estimation possible et efficace sur des jeux de données comportant plusieurs centaines de milliers de variables originales, permettant ainsi d'étendre la portée de ces modèles aux données des études d'associations pangénomiques.
Notre étude porte sur les verbes pronominaux en français, en anglais et en allemand. La première partie est une évaluation des travaux précédents, depuis les travaux "classiques" en terminologie, grammaire et typologies jusqu'aux travaux faits dans le domaine du TALN et de la TAO. Ces énoncés sont soumis à différents tests et transformations en vue de dégager des constantes de comportement. La typologie résultante est exposée en partie 3 et sert de base à la formalisation précédant la phase d'implémentation effectuée sous Sygmart, un modèle de transfert de structures. Enfin, la double lecture (lecture réfléchie et lecture réciproque) fait problème pour certains énoncés d'ambiguïté extrême, à moins qu'une base de connaissance complète ne soit intégrée dans la chaîne
Cette thèse se propose d'analyser les effets d'une formation plurilingue en ligne sur l'acquisition de compétences langagières et, en particulier, en langue cible (français) pour des apprenants de lycée, de Barcelone. Pour pouvoir répondre à l'hypothèse selon laquelle la formation développerait les compétences des apprenants d'une part dans les langues de la formation mais également en français, la méthodologie adoptée a été la réalisation d'un protocole (quasi-) expérimental auprès de lycéens d'un établissement de Barcelone. Ce protocole a consisté à faire participer un groupe d'élèves à une session sur la plate-forme Galanet, de façon intégrée à leurs enseignements scolaires, ainsi qu'à la réalisation de tests pré- et post-formation. Un profil langagier a été complété par les sujets en amont de la formation. Chacun des tests présente le même type d'exercices, portant sur les mêmes compétences, et pour chacune des situations (initiale ou finale), il y a un test dont les supports sont en français et un autre dont les supports sont dans d'autres langues romanes (italien, portugais, roumain et occitan vivarois). Les compétences ciblées par les tests sont la cohérence/cohésion textuelle, la compréhension globale et fine, l'identification du degré de perception de l'interculturalité, la traduction/transposition, la segmentation et la reconnaissance/application d'un modèle discursif. Les données recueillies lors des tests ont fait l'objet d'analyses quantitatives et qualitatives, suivies de traitement statistiques quand il s'est agi d'établir, en particulier, des corrélations. En parallèle, le degré d'interactivité intercompréhensif (prenant en compte l'alternance des langues dans ce contexte bi-plurilingue) des messages de forum déposés lors de la formation plurilingue a été déterminé par le traitement semi-automatique du corpus de messages par la plate-forme Calico. C'est cette double analyse, basée sur des données concrètes expérientielles qui a permis de réaliser une étude de cas et de définir par l'étude longitudinale de leurs différents résultats si le fait de suivre de manière satisfaisante, en terme de participation, une formation en intercompréhension a une incidence positive sur l'acquisition des compétences évaluées par les tests, sur quelles compétences particulières et de quelle manière.
Les questions des élèves sont utiles pour leur apprentissage et l'adaptation pédagogique des enseignants. Nous abordons cette problématique principalement dans le cadre d'une formation hybride dans lequel chaque semaine les étudiants posent des questions en ligne, selon une approche de classe inversée, pour aider les enseignants à préparer leur séances de questions-réponses en présentiel. Notre objectif est d'outiller l'enseignant pour qu'il détermine les types de questions posées par les différents groupes d'apprenants. Pour mener ce travail, nous avons développé un schéma de codage de questions guidé par l'intention des élèves et la réaction pédagogique de l'enseignant. Plusieurs outils de classification automatique ont été conçus, évalués et combinés pour catégoriser les questions. Nous avons montré comment un modèle dérivé de clustering des données et entraîné sur des sessions antérieures peut être utilisé pour prédire le profil des élèves en ligne et établir des liens avec leurs questions. Ces résultats nous ont permis de proposer trois organisations de questions aux enseignants (basées sur les catégories de questions et profils des apprenants) qui ouvrent des perspectives de traitement différent lors des séances de questions-réponses. Nous avons testé et montré la possibilité d'adapter notre schéma de codage et les outils associés au contexte très différent d'un MOOC, ce qui suggère une certaine généricité de notre approche.
Les véhicules autonomes sont des systèmes assez complexes où plusieurs types de défaillances peuvent se produire, enclenchant une fausse action sur la route. Chaque composant devra donc être testé rigoureusement pour anticiper et éliminer les potentielles défaillances. Des techniques de simulation numérique sont utilisées pour complémenter les essais de conduite réelle dans la validation. Les contributions de cette thèse sont organisées selon trois objectifs : détection d'un nombre maximal de défaillances de la loi de commande, détection de scénarios près de la frontière entre zones défaillantes et sûres, et construction de modèles explicables de la frontière pour l'identifier aussi précisément que possible. Des techniques d'apprentissage (forêt aléatoire) et d'optimisation (CMA-ES) sont utilisées pour satisfaire la contrainte industrielle de réduire la puissance de calcul, et trois approches sont considérées pour construire des modèles de frontière en analysant leurs performances et explicabilités : réseaux de neurones, programmation mathématique linéaire, et programmation génétique appliquée à la régression symbolique.
Le travail développé dans cette thèse porte sur l'analyse de séquences vidéo. Cette dernière est basée sur 3 taches principales : la détection, la catégorisation et le suivi des objets. Le développement de solutions fiables pour l'analyse de séquences vidéo ouvre de nouveaux horizons pour plusieurs applications telles que les systèmes de transport intelligents, la vidéosurveillance et la robotique. Dans cette thèse, nous avons mis en avant plusieurs contributions pour traiter les problèmes de détection et de suivi d'objets multiples sur des séquences vidéo. Les techniques proposées sont basées sur l'apprentissage profonds et des approches de transfert d'apprentissage. Dans une première contribution, nous abordons le problème de la détection multi-objets en proposant une nouvelle technique de transfert d'apprentissage basé sur le formalisme et la théorie du filtre SMC (Sequential Monte Carlo) afin de spécialiser automatiquement un détecteur de réseau de neurones convolutionnel profond (DCNN) vers une scène cible. Dans une deuxième contribution, nous proposons une nouvelle approche de suivi multi-objets original basé sur des stratégies spatio-temporelles (entrelacement / entrelacement inverse) et un détecteur profond entrelacé, qui améliore les performances des algorithmes de suivi par détection et permet de suivre des objets dans des environnements complexes (occlusion, intersection, fort mouvement). Dans une troisième contribution, nous fournissons un système de surveillance du trafic, qui intègre une extension du technique SMC afin d'améliorer la précision de la détection de jour et de nuit et de spécialiser tout détecteur DCNN pour les caméras fixes et mobiles. Tout au long de ce rapport, nous fournissons des résultats quantitatifs et qualitatifs. Sur plusieurs aspects liés à l'analyse de séquences vidéo, ces travaux surpassent les cadres de détection et de suivi de pointe. En outre, nous avons implémenté avec succès nos infrastructures dans une plate-forme matérielle intégrée pour la surveillance et la sécurité du trafic routier.
Les larges ontologies biomédicales décrivent généralement le même domaine d'intérêt, mais en utilisant des modèles de modélisation et des vocabulaires différents. Les systèmes de matching d'ontologies combinent différents types de matcher pour résoudre ces problèmes. Le nouvel algorithme de partitionnement est basé sur le clustering hiérarchique par agglomération (CHA). L'application de techniques de sélection de caractéristiques à chaque classificateur local augmente la valeur de rappel pour chaque tâche d'alignement local.
Vu les ambiguïtés que créent les anaphores au niveau des langages naturelle, plusieurs chercheurs du domaine du traitement automatique des langues (TAL) ont mis en place divers approches pour résoudre le problème. Nous nous proposons d'adapter ces approches à la résolution automatique des anaphores pronominales dans RESUMAN. Dans la première partie sont présentés les modèles cognitifs, linguistiques et textuelles qui rendent compte du fonctionnement et de l'interprétation des pronoms. Dans la deuxième partie, les procédures d'interprétation des anaphores pronominales sont exposées : la procédure de distance minimale, la procédure des fonctions parallèles, la procédure du sujet ou la procédure thématique, ainsi que les procédures d'analyse morphologique, sémantique et pragmatique. Dans la troisième partie, une nouvelle version d'algorithme, se basant sur une approche statistique, est présentée. L'outil RESUMAN permet d'améliorer les performances d'un TAL en cas de textes denses en anaphores. Les performances de ce logiciel sont évaluées et ses limites sont commentées.
La génomique comparative est un sujet essentiel pour éclaircir la biologie évolutionnaire. La première étape pour dépasser une connaissance seulement descriptive est de développer une méthode pour représenter le contenu du génome. Nous avons choisi la représentation modulaire des génomes pour étudier les lois quantitatives qui réglementent leur composition en unités élémentaires de type fonctionnel ou évolutif. La première partie de la thèse se fonde sur l'observation que le nombre de domaines ayant la même fonction est lié à la taille du génome par une loi de puissance. Puisque les catégories fonctionnelles sont des agrégats de familles de domaines, on se demande comment le nombre de domaines dans la même catégorie fonctionnelle est lié à l'évolution des familles. Le résultat est que les familles suivent également une loi de puissance. Le deuxième partie présente un modèle positif qui construit une réalisation à partir des composants liés dans un réseau de dépendance. L'ensemble de toutes les réalisations reproduit la distribution des composants partagés et la relation entre le nombre de familles distinctes et la taille du génome. Le dernier chapitre étend l'approche modulaire aux écosystèmes microbiens. Sur la base des constatations que nous avons faites sur les lois de puissance pour les familles de domaines, nous avons analysé comment le nombre de familles dans un metagénome en est influencé. Par conséquence, nous avons défini une nouvelle observable dont la forme fonctionnelle comprend des informations quantitatives sur la composition originelle du metagénome.
Ces dernières années, le contenu disponible sur le Web a augmenté de manière considérable dans ce qu'on appelle communément le Web social. Pour l'utilisateur moyen, il devient de plus en plus difficile de recevoir du contenu de qualité sans se voir rapidement submergé par le flot incessant de publications. Pour les fournisseurs de service, le passage à l'échelle reste problématique. L'objectif de cette thèse est d'aboutir à une meilleure expérience utilisateur à travers la mise en place de systèmes de filtrage et de recommandation. Le filtrage consiste à offrir la possibilité à un utilisateur de ne recevoir qu'un sous ensemble des publications des comptes auxquels il est abonné. Tandis que la recommandation permet la découverte d'information à travers la suggestion de comptes à suivre sur des sujets donnés. Nous avons élaboré MicroFilter un système de filtrage passant à l'échelle capable de gérer des flux issus du Web ainsi que RecLand, un système de recommandation qui tire parti de la topologie du réseau ainsi que du contenu afin de générer des recommandations pertinentes.
Cette thèse porte sur des analyses acoustiques et prosodiques du français à partir de grandes masses de données orales illustrant différents styles de parole (préparée et spontanée). Nous nous sommes intéressées aux attributs acoustiques et prosodiques qui pourraient caractériser la prononciation. Une classification automatique (CA) a été effectuée pour discriminer deux paires homophones ('et/est'et 'à/a') par des propriétés acoustiques et prosodiques. Les résultats de la CA ont montré que le paire 'et/est'était plus dissociable. La CA par des attributs prosodiques et inter-segmentaux (15 attributs) s'est avérée aussi performante que celle utilisant la totalité des 62 attributs. Un test perceptif a été également effectué pour vérifier si les humains utilisaient eux aussi ces paramètres. Les résultats ont suggéré que des informations acoustiques et prosodiques pourraient être utiles pour effectuer un choix correct de mots dans des structures syntaxiquement ambigües. Ensuite, nous avons examiné des propriétés prosodiques globales aux niveaux du nom et du syntagme nominal. La comparaison entre mots lexicaux et grammaticaux a montré que la fréquence fondamentale (F0) montante et l'allongement vocalique de la dernière syllabe caractérisent les mots lexicaux, par opposition aux mots grammaticaux. Ainsi, le profil de F0 moyenne d'un syntagme nominal de longueur n pourrait être différent de celui du nom avec une valeur de F0 basse au début du syntagme. Les profils prosodiques peuvent être utiles pour localiser frontières de mots. Les résultats de ce travail pourront servir à localiser le focus et les entités-nommées par des classifieurs discriminants, et de manière plus générale à améliorer les techniques de localisation des frontières des mots pour la RAP.
Les malwares, autrement dit programmes malicieux ont grandement évolué ces derniers temps et sont devenus une menace majeure pour les utilisateurs grand public, les entreprises et même le gouvernement. Afin de limiter ces problèmes, les chercheurs spécialisés dans les malwares ont proposé différentes approches comme l'exploration des données (data mining) ou bien l'apprentissage automatique (machine learning) pour détecter et classifier les échantillons de malwares en fonction de leur propriétés statiques et dynamiques. De plus les méthodes proposées sont efficaces sur un petit ensemble de malwares, le passage à l'échelle de ses méthodes pour des grands ensembles est toujours en recherche et n'a pas été encore résolu. Il est évident aussi que la majorité des malwares sont une variante des précédentes versions. Par conséquent, le volume des nouvelles variantes créées dépasse grandement la capacité d'analyse actuelle. C'est pourquoi développer la classification des malwares est essentiel pour lutter contre cette augmentation pour la communauté informatique spécialisée en sécurité. Le challenge principal dans l'identification des familles de malware est de réussir à trouver un équilibre entre le nombre d'échantillons augmentant et la précision de la classification. Pour atteindre notre objectif, premièrement nous avons développé une version portable, évolutive et transparente d'analyse de malware appelée VirMon pour analyse dynamique de malware visant les OS windows. Deuxièmement, nous avons mis en place un cluster de 5 machines pour notre module d'apprentissage en ligne (Jubatus) ; qui permet de traiter une quantité importante de données. Cette configuration permet à chaque machine d'exécuter ses tâches et de délivrer les résultats obtenus au gestionnaire du cluster. Notre outil proposé consiste essentiellement en trois niveaux majeures. Le premier niveau permet l'extraction des comportements des échantillons surveillés et observe leurs interactions avec les ressources de l'OS. Durant cette étape, le fichier exemple est exécuté dans un environnement « sandbox » . Notre outil supporte deux « sandbox » : VirMon et Cuckoo. Durant le second niveau, nous appliquons des fonctionnalités d'extraction aux rapports d'analyses. Le label de chaque échantillon est déterminé Virustotal, un outil regroupant plusieurs anti-virus permettant de scanner en ligne constitués de 46 moteurs de recherches. Enfin au troisième niveau, la base de données de malware est partitionnée en ensemble de test et d'apprentissage.
L'objectif de cette thèse est de concevoir et de construire, un système Text-To-Speech (TTS) haute qualité à base de HMM (Hidden Markov Model) pour le vietnamien, une langue tonale. Le système est appelé VTED (Vietnamese TExt-to-speech Development system). Au vu de la grande importance de tons lexicaux, un tonophone” – un allophones dans un contexte tonal – a été proposé comme nouvelle unité de la parole dans notre système de TTS. Un nouveau corpus d'entraînement, VDTS (Vietnamese Di-Tonophone Speech corpus), a été conçu à partir d'un grand texte brut pour une couverture de 100% de di-phones tonalisés (di-tonophones) en utilisant l'algorithme glouton. Un total d'environ 4000 phrases ont été enregistrées et pré-traitées comme corpus d'apprentissage de VTED. Dans la synthèse de la parole sur la base de HMM, bien que la durée de pause puisse être modélisée comme un phonème, l'apparition de pauses ne peut pas être prédite par HMM. Les niveaux de phrasé ne peuvent pas être complètement modélisés avec des caractéristiques de base. Cette recherche vise à obtenir un découpage automatique en groupes intonatifs au moyen des seuls indices de durée. Des blocs syntaxiques constitués de phrases syntaxiques avec un nombre borné de syllabes (n), ont été proposés pour prévoir allongement final (n = 6) et pause apparente (n = 10). Des améliorations pour allongement final ont été effectuées par des stratégies de regroupement des blocs syntaxiques simples. La qualité du modèle prédictive J48-arbre-décision pour l'apparence de pause à l'aide de blocs syntaxiques, combinée avec lien syntaxique et POS (Part-Of-Speech) dispose atteint un F-score de 81,4 % (Précision = 87,6 %, Recall = 75,9 %), beaucoup mieux que le modèle avec seulement POS (F-score=43,6%) ou un lien syntaxique (F-score=52,6%). L'architecture du système a été proposée sur la base de l'architecture HTS avec une extension d'une partie traitement du langage naturel pour le Vietnamien. L'apparence de pause a été prédit par le modèle proposé. Les caractéristiques contextuelles incluent les caractéristiques d'identité de “tonophones”, les caractéristiques de localisation, les caractéristiques liées à la tonalité, et les caractéristiques prosodiques (POS, allongement final, niveaux de rupture). Mary TTS a été choisi comme plateforme pour la mise en oeuvre de VTED. Dans le test MOS (Mean Opinion Score), le premier VTED, appris avec les anciens corpus et des fonctions de base, était plutôt bonne, 0,81 (sur une échelle MOS 5 points) plus élevé que le précédent système – HoaSung (lequel utilise la sélection de l'unité non-uniforme avec le même corpus) ; mais toujours 1,2-1,5 point de moins que le discours naturel. La qualité finale de VTED, avec le nouveau corpus et le modèle de phrasé prosodique, progresse d'environ 1,04 par rapport au premier VTED, et son écart avec le langage naturel a été nettement réduit. Dans le test d'intelligibilité, le VTED final a reçu un bon taux élevé de 95,4%, seulement 2,6% de moins que le discours naturel, et 18% plus élevé que le premier. Le taux d'erreur du premier VTED dans le test d'intelligibilité générale avec le carré latin test d'environ 6-12% plus élevé que le langage naturel selon des niveaux de syllabe, de ton ou par phonème. Le résultat final ne s'écarte de la parole naturelle que de 0,4-1,4%.
Les images hyperspectrales suscitent un intérêt croissant depuis une quinzaine d'années. Elles fournissent une information plus détaillée d'une scène et permettent une discrimination plus précise des objets que les images couleur RVB ou multi-spectrales. Les travaux de cette thèse s'inscrivent dans le cadre de la réduction et de partitionnement des images hyperspectrales de grande dimension spatiale. L'approche proposée se compose de deux étapes : calcul d'attributs et classification des pixels. Une nouvelle approche d'extraction d'attributs à partir des matrices de tri-occurrences définies sur des voisinages cubiques est proposée en tenant compte de l'information spatiale et spectrale. Une étude comparative a été menée afin de tester le pouvoir discriminant de ces nouveaux attributs par rapport aux attributs classiques. Concernant la classification, nous nous intéressons ici au partitionnement des images par une approche de classification non supervisée et non paramétrique car elle présente plusieurs avantages : aucune connaissance a priori, partitionnement des images quel que soit le domaine applicatif, adaptabilité au contenu informationnel des images. Une étude comparative des principaux classifieurs semi-supervisés (connaissance du nombre de classes) et non supervisés (C-moyennes, FCM, ISODATA, AP) a montré la supériorité de la méthode de propagation d'affinité (AP). Nous avons proposé une approche qui apporte des solutions à ces deux problèmes. Pour estimer le nombre de classes, la méthode AP utilise de manière implicite un paramètre de préférence p dont la valeur initiale correspond à la médiane des valeurs de la matrice de similarité. Cette valeur conduisant souvent à une sur-segmentation des images, nous avons introduit une étape permettant d'optimiser ce paramètre en maximisant un critère lié à la variance interclasse. L'approche proposée a été testée avec succès sur des images synthétiques, mono et multi-composantes. Elle a été également appliquée et comparée sur des images hyperspectrales de grande taille spatiale (1000 × 1000 pixels × 62 bandes) avec succès dans le cadre d'une application réelle pour la détection des plantes invasives.
AF) primaire, cooccurrent à la frontière prosodique, s'effacerait perceptivement en frontière prosodique majeure. L'Accent Initial (AI), dit secondaire et optionnel, serait un accent rythmique apparaissant sur les longs constituants. L'existence d'un Syntagme Intermédiaire (ip) est en revanche controversée. L'objectif de cette étude est d'explorer l'organisation du phrasé prosodique en français. Dans ce cadre, nous proposons une étude perceptive, via un corpus de parole contrôlée manipulant des structures syntaxiques ambiguës, où 80 participants ont effectué 3 tâches de perception : proéminence, frontière et groupement. Les événements prosodiques perçus ont ensuite été mis en relation avec leurs réalités acoustiques. Les résultats montrent que les auditeurs sont capables de percevoir des niveaux de granularité de frontières plus fins que ce que les descriptions traditionnelles du français prédisent. Par ailleurs, les mots lexicaux sont systématiquement réalisés par un marquage bipolaire (AI+AF) de même force métrique. AI joue également un rôle plus structurel que rythmique, en marquant la structure prosodique de manière plus privilégiée qu'AF. Enfin, AF ne s'efface pas perceptivement en frontière prosodique majeure, et garde au contraire une trace métrique au niveau du mot lexical, qui ne varie pas strictement en fonction du niveau de constituance.
Afin d'optimiser la langue existante, nous avons entrepris d'évaluer les niveaux appropriés de simplification qui permettraient une compréhension plus précise et plus rapide, réduisant ainsi le temps de formation des pilotes. Nous avons tout d'abord exploré le domaine des langues contrôlées afin d'avoir un aperçu des langues contrôlées existantes, de leur contexte et de leurs règles. À partir de cette recherche, nous avons tenté de trouver des solutions d'optimisation, tout en nous efforçant d'apporter une contribution originale à ce domaine.
Les agents virtuels conversationnels ayant un comportement social reposent souvent sur au moins deux disciplines différentes : l'informatique et la psychologie. Dans la plupart des cas, les théories psychologiques sont converties en un modèle informatique afin de permettre aux agents d'adopter des comportements crédibles. Nous nous intéressons aux agents conversationnels orientés tâche, qui sont utilisés dans un contexte professionnel pour produire des réponses à partir d'une base de connaissances métier. Nous proposons un modèle affectif pour ces agents qui s'inspire des mécanismes affectifs chez l'humain. L'approche que nous avons choisie de mettre en œuvre dans notre modèle s'appuie sur la théorie des Tendances à l'Action en psychologie. Ce modèle a été implémenté dans une architecture d'agent conversationnel développée au sein de l'entreprise DAVI. Afin de confirmer la pertinence de notre approche, nous avons réalisé plusieurs études expérimentales. La deuxième porte sur l'impact des différentes stratégies de régulation possibles sur la perception de l'agent par l'utilisateur. Enfin, la troisième étude porte sur l'évaluation des agents affectifs en interaction avec des participants. Nous montrons que le processus de régulation que nous avons implémenté permet d'augmenter la crédibilité et le professionnalisme perçu des agents, et plus généralement qu'ils améliorent l'interaction. Nos résultats mettent ainsi en avant la nécessité de prendre en considération les deux mécanismes émotionnels complémentaires : la génération et la régulation des réponses émotionnelles. Ils ouvrent des perspectives sur les différentes manières de gérer les émotions et leur impact sur la perception de l'agent.
Dans cette thèse, j'étudie, dans une perspective à la fois théorique et historique, l'apparition de la crise macroéconomique. En m'appuyant sur l'économie narrative, les techniques text mining et les théories des systèmes complexes, j'explore les mécanismes qui alimentent la crise systémique et je montre que c'est l'architecture du système économique qui crée la vulnérabilité des économies et les ingrédients d'une crise à grande échelle plutôt que la nature des chocs. et une approche narrative pour créer des données structurées, reflétant les perspectives macroéconomiques des membres du FMI depuis le début des années 1950. En utilisant une combinaison de techniques d'apprentissage supervisé et non supervisé, nous créons un vaste échantillon de 30 000 rapports du FMI. Cet échantillon permet ensuite de caractériser chaque document dans environ 20 catégories de crise économiquement pertinentes, allant de la défaillance d'un État souverain, d'une crise monétaire ou d'une crise bancaire à des épidémies ou des conflits violents. La base de données et les outils quantitatifs seront disponibles dans un package R (bientôt disponible sur github). En m'appuyant les modèles écologiques de résilience, je montre que les systèmes macroéconomiques sont des systèmes en perpétuel déséquilibre avec des crises récurrentes (les déclencheurs) qui affectent de manière hétérogène le cœur du système (les propagateurs) et finalement les nœuds critiques (les points chauds) dans une dynamique similaire à celle des incendies de forêt. Dans un troisième chapitre, j'adopte une perspective plus étroite en examinant les épisodes où le FMI est intervenu dans le cadre d'un accord de prêt. Dans ce chapitre intitulé "Les instruments du prêteur en dernier ressort : guérir les fondamentaux ou calmer la panique", je donne un aperçu historique des 70 années d'intervention du FMI pour fournir une assistance aux États souverains en détresse. Dans un quatrième chapitre, j'apporte la preuve de l'application d'une règle de Taylor par le FMI lors de la conception des accords de prêt destinés à aider les pays en détresse. Dans l'ensemble, ce travail apporte de nouveauw éléments sur l'apparition de la crise systémique et sur la dynamique des perturbations mondiales que certains événements inattendus peuvent générer. Ces travaux tentent de mettre au centre de l'analyse la nature narrative de la crise économique et l'importance de l'architecture du système de crise et visent à attirer l'attention sur l'importance de trouver le juste équilibre entre le développement économique, la complexité des systèmes économiques et la gravité de la crise. Dans le contexte d'un système économique international très fragile et vulnérable, combiné à l'émergence croissante de crises non critiques sur le plan économique et écologique, une attention particulière devrait être accordée à l'élaboration de nouvelles normes de gestion de crise visant à réduire l'intensité de la crise tout en atténuant la vulnérabilité due à l'interconnexion des secteurs économiques, en augmentant la resilience décentralisée des différents nœuds économiques et en isolant les éléments les plus critiques du système afin d'éviter des effondrements économiques systémiques à grande échelle.
Le récit est un outil de communication qui permet aux individus de donner un sens au monde qui les entoure. Il représente une plate-forme pour comprendre et partager leur culture, connaissances et identité. Le récit porte une série d'événements réels ou imaginaires, en provoquant un ressenti, une réaction ou même, déclenche une action. Pour cette raison, il est devenu un sujet d'intérêt pour différents domaines au-delà de la Littérature (Éducation, Marketing, Psychologie, etc.) qui cherchent d'atteindre un but particulier au travers de lui (Persuader, Réfléchir, Apprendre, etc.). Cependant, le récit reste encore sous-développé dans le contexte informatique. Il existent des travaux qui visent son analyse et production automatique. Les algorithmes et implémentations, par contre, restent contraintes à imiter le processus créatif derrière des textes littéraires provenant de sources textuelles. Ainsi, il n'existent pas des approches qui produisent automatiquement des récits dont 1) la source est constitué de matériel non formatées et passé dans la réalité et 2) et le contenu projette une perspective qui cherche à transmettre un message en particulier. Travailler avec des données brutes devient relevante vu qu'elles augmentent exponentiellement chaque jour grâce à l'utilisation d'appareils connectés. Ainsi, vu le contexte du Big Data, nous présentons une approche de génération automatique de récits à partir de données ambiantes. L'objectif est de faire émerger l'expérience vécue d'une personne à partir des données produites pendant une activité humaine. Tous les domaines qui travaillent avec des données brutes pourraient bénéficier de ce travail, tels que l'Éducation ou la Santé. Il s'agit d'un effort interdisciplinaire qui inclut le Traitement Automatique de Langues, la Narratologie, les Sciences Cognitives et l'Interaction Homme-Machine. Cette approche est basée sur des corpus et modèles et comprend la formalisation de ce que nous appelons le récit d'activité ainsi qu'une démarche de génération adaptée. Elle a est composé de 4 étapes : la formalisation des récits d'activité, la constitution de corpus, la construction de modèles d'activité et du récit, et la génération de texte. Chacune a été conçue pour surmonter des contraintes liées aux questions scientifiques posées vue la nature de l'objectif : la manipulation de données incertaines et incomplètes, l'abstraction valide d'après l'activité, la construction de modèles avec lesquels il soit possible la transposition de la réalité gardée dans les données vers une perspective subjective et la rendue en langage naturel. Nous avons utilisé comme cas d'usage le récit d'activité, vu que les pratiquant se servent des appareils connectés, ainsi qu'ils ont besoin de partager son expérience. Les résultats obtenus sont encourageants et donnent des pistes qui ouvrent beaucoup de perspectives de recherche.
Le travail realise au cours de cette these a essentiellement pour objectif la valorisation de donnees existantes et leur acces par l'ensemble de la communaute parole. Les aspects importants de la constitution de cette base sont la sauvegarde des donnees. Sa gestion d'acces et l'interfacage avec les outils de traitement et de visualisation des donnees. Nous avons propose un modele de representation des donnees parole dans un systeme de gestion de base de donnees oriente-objet. Pour cela, nous avons introduit le concept de donnees primaires : donnees brutes (signaux et images cineradiographiques) ou bien donnees descriptives des locuteurs, des corpus et des conditions d'enregistrements, et des donnees derivees extraites des donnees brutes. Afin de montrer l'interet d'une base de donnees cineradiographiques, nous avons exploite un corpus enregistre par un locuteur de langue francaise. La methode d'analyse detaillee des traces du conduit vocal s'est averee interessante, particulierement pour recuperer l'ensemble des gestes articulatoires impliques dans la production caque sequence vcv. Cette methode nous a aide a suivre de pres les mouvements des articulateurs au niveau de chaque trace et a detecter les debuts des mouvements d'anticipation des levres et de la langue dans les sequences vcv analysees. L'analyse de ses sequences plaide en faveur des hypotheses du modele d'anticipation m. E. M propose par abry &amp ; 
Associée à la recherche d'information, la notion de contexte est de plus en plus mobilisée dans les domaines des sciences de l'information, de l'ingénierie des connaissances, des sciences cognitives et de l'informatique. En effet, le sens d'une expression linguistique, la lecture d'un document la stratégie mise en œuvre dans l'activité de recherche d'information, le raisonnement adopté dans l'opération de classement d'un document, le choix d'avoir recours à tel dispositif, varient fortement d'un contexte à l'autre. Notre contribution cherche ainsi à éclairer l'environnement informationnel de ce groupe d'acteurs et propose quelques axes de réflexion pour accompagner la construction d'une démarche instrumentée de gestion de l'information en entreprise. Ce travail associe différents acteurs (chercheurs et ingénieurs) et participe à un projet de recherche appliquée (ANR MIIPA-Doc).
Le peuplement de base de connaissance (KBP) est une tâche importante qui présente de nombreux défis pour le traitement automatique des langues. Nous nous sommes intéressés à la reconnaissance de relations entre entités. Généralement, ces relations sont extraites en fonction des informations lexicales et syntaxiques au niveau de la phrase. Cependant, l'exploitation d'informations globales sur les entités n'a pas encore été explorée. Nous proposons d'extraire un graphe d'entités du corpus global et de calculer des caractéristiques sur ce graphe afin de capturer des indices des relations entre paires d'entités. Pour évaluer la pertinence des fonctionnalités proposées, nous les avons testées sur une tâche de validation de relation dont le but est de décider l'exactitude de relations extraites par différents systèmes. Les résultats expérimentaux montrent que les caractéristiques proposées conduisent à améliorer les résultats de l'état de l'art.
Le travail de thèse consiste en la réalisation de modèles et outils d'analyse sémantique pour l'exploitation de corpus d'entretiens réalisés par une société d'aide à l'innovation. Ces entretiens sont analysés afin de déterminer si un projet d'innovation répond ou non à plusieurs critères d'acceptabilité. L'automatisation de cette analyse, au-delà des difficultés d'ambiguïtés généralement rencontrées en analyse sémantique des opinions, doit permettre de traiter des contenus relevant d'une multitude de langues de spécialité. Il s'agira donc de réaliser des modèles et outils intégrant des procédés généraux de traitement automatique des langues (analyse syntaxique et sémantique) et des ontologies permettant d'intégrer différentes terminologies.
Les plate-formes de Calcul Haute Performance (High Performance Computing, HPC) augmentent en taille et en complexité. De manière contradictoire, la demande en énergie de telles plates-formes a également rapidement augmenté. Les supercalculateurs actuels ont besoin d'une puissance équivalente à celle de toute une centrale d'énergie. Dans le but de faire un usage plus responsable de ce puissance de calcul, les chercheurs consacrent beaucoup d'efforts à la conception d'algorithmes et de techniques permettant d'améliorer différents aspects de performance, tels que l'ordonnancement et la gestion des ressources. Cependent, les responsables des plate-formes HPC hésitent encore à déployer des méthodes d'ordonnancement à la fine pointe de la technologie et la plupart d'entre eux recourent à des méthodes heuristiques simples, telles que l'EASY Backfilling, qui repose sur un tri naïf premier arrivé, premier servi. Les nouvelles méthodes sont souvent complexes et obscures, et la simplicité et la transparence de l'EASY Backfilling sont trop importantes pour être sacrifiées. Dans un premier temps, nous explorons les techniques d'Apprentissage Automatique (Machine Learning, ML) pour apprendre des méthodes heuristiques d'ordonnancement online de tâches parallèles. À l'aide de simulations et d'un modèle de génération de charge de travail, nous avons pu déterminer les caractéristiques des applications HPC (tâches) qui contribuent pour une réduction du ralentissement moyen des tâches dans une file d'attente d'exécution. La modélisation de ces caractéristiques par une fonction non linéaire et l'application de cette fonction pour sélectionner la prochaine tâche à exécuter dans une file d'attente ont amélioré le ralentissement moyen des tâches dans les charges de travail synthétiques. Appliquées à des traces de charges de travail réelles de plate-formes HPC très différents, ces fonctions ont néanmoins permis d'améliorer les performances, attestant de la capacité de généralisation des heuristiques obtenues. Dans un deuxième temps, à l'aide de simulations et de traces de charge de travail de plusieurs plates-formes HPC réelles, nous avons effectué une analyse approfondie des résultats cumulés de quatre heuristiques simples d'ordonnancement (y compris l'EASY Backfilling). Nous avons également évalué des outres effets tels que la relation entre la taille des tâches et leur ralentissement, la distribution des valeurs de ralentissement et le nombre de tâches mises en calcul par backfilling, par chaque plate-forme HPC et politique d'ordonnancement. Nous démontrons de manière expérimentale que l'on ne peut que gagner en remplaçant l'EASY Backfilling par la stratégie SAF (Smallest estimated Area First) aidée par backfilling, car elle offre une amélioration des performances allant jusqu'à 80% dans la métrique de ralentissement, tout en maintenant la simplicité et la transparence d'EASY Backfilling. La SAF réduit le nombre de tâches à hautes valeurs de ralentissement et, par l'inclusion d'un mécanisme de seuillage simple, nous garantonts l'absence d'inanition de tâches. Dans l'ensemble, nous avons obtenu les remarques suivantes :  (i) des heuristiques simples et efficaces sous la forme d'une fonction non linéaire des caractéristiques des tâches peuvent être apprises automatiquement, bien qu'il soit subjectif de conclure si le raisonnement qui sous-tend les décisions d'ordonnancement de ces heuristiques est clair ou non. (ii) La zone (l'estimation du temps d'exécution multipliée par le nombre de processeurs) des tâches semble être une propriété assez importante pour une bonne heuristique d'ordonnancement des tâches parallèles, car un bon nombre d'heuristiques (notamment la SAF) qui ont obtenu de bonnes performances ont la zone de la tâche comme entrée. (iii) Le mécanisme de backfilling semble toujours contribuer à améliorer les performances, bien que cela ne remédie pas à un meilleur tri de la file d'attente de tâches, tel que celui effectué par SAF.
Cette thèse a pour objet l'extraction d'informations relationnelles à partir de documents scientifiques en sciences de la vie, c'est-à-dire la transformation de texte non structuré en information structurée exploitable par une machine. L'extraction de relations sémantiques spécialisées entre entités détectées dans le texte rend explicite et formalise les structures sous-jacentes. Les méthodes actuelles à l'état de l'art s'appuient sur de l'apprentissage supervisé. L'apprentissage supervisé, et particulièrement les méthodes récentes d'apprentissage profond, ont besoin de beaucoup d'exemples d'apprentissages qui sont coûteux à produire, d'autant plus dans les domaines spécialisés comme les sciences de la vie. Nous faisons l'hypothèse que combiner l'information et la connaissance disponibles dans les domaines spécialisés avec les derniers modèles d'apprentissage profond de plongements lexicaux (word embeddings) peut pallier l'absence ou le nombre réduit de données d'entraînement annotées. Dans ce but, cette thèse concevra une représentation riche des textes qui s'appuie à la fois sur des informations linguistiques obtenues par analyse syntaxique et sur des connaissances du domaine issues de graphes de connaissance tels que des ontologies. L'utilisation d'ontologies dans le processus d'extraction d'information facilitera en outre l'intégration de l'information avec d'autres données, telles que des données expérimentales ou analytiques.
Les technologies de l'information et de la communication impactent largement de nombreuses branches du droit. Le droit des obligations n'y fait pas exception et de nombreux contrats sont désormais conclus en ligne, quel que soit le terminal utilisé. Le recours à ce moyen de communication n'est pas sans influence sur la perfection du contrat, en particulier sur les modes d'expression de la volonté dans l'univers numérique. En effet, ce dernier offre de vastes perspectives en termes d'instantanéité, d'immatérialité et d'automatisation de l'expression du consentement contractuel, conduisant à s'interroger sur la validité des contrats formés par voie électronique. L'observation des pratiques qui se sont installées sur l'internet permet de mesurer aujourd'hui le net impact du numérique sur l'expression du consentement contractuel, c'est-à-dire sur les volontés des internautes cocontractants, ainsi que sur le mécanisme de rencontre de celles-ci. Les volontés individuelles se sont ainsi vues soumises à un processus constitué d'une série d'étapes obligatoires, supposées limiter les cas dans lesquels la perfection de la convention interviendrait par erreur. Ce découpage ouvre toutefois la voie à l'automatisation de l'expression des volontés et de leur rencontre, annonçant alors l'ère de contrats conclus voire exécutés en un trait de temps grâce aux récentes avancées de l'intelligence artificielle appliquée au domaine juridique.
La localisation est le processus d'estimation de la position d'une entité dans un système de coordonnées local ou global. Les applications de localisation sont largement réparties dans des contextes différents. Dans les événements, le suivi des participants peut sauver des vies pendant des crises. Dans le domaine de la santé, les personnes âgées peuvent être suivies pour répondre à leurs besoins dans des situations critiques comme les chutes. Dans les entrepôts, les robots transférant des produits d'un endroit à un autre nécessitent une connaissance précise de ses positions, la position des produits ainsi que des autres robots. Dans un contexte industriel, la localisation est essentielle pour réaliser des processus automatisés qui sont assez flexibles pour être reconfiguré à diverses missions. La localisation est considérée comme un sujet de grand intérêt tant dans l'industrie que dans l'académie, en particulier avec l'avènement de la 5G avec son "Enhanced Mobile Broadband (eMBB)" qui devrait atteindre 10 Gbits/s, "Ultra-Reliable Low-Latency Communication (URLLC)" qui est moins d'une milliseconde et "massive Machine-Type Communication (mMTC)" permettant de connecter environ 1 million d'appareils par kilomètre. Dans ce travail, nous nous concentrons sur deux principaux types de localisation ; la localisation basée sur la distance entre des appareils et la localisation basée sur les empreintes digitales. Dans la localisation basée sur la distance, un réseau d'appareils avec une distance de communication maximale estime les valeurs de distances par rapport à leurs voisins. Ces distances ainsi que la connaissance des positions de quelques nœuds sont utilisées pour localiser d'autres nœuds du réseau à l'aide d'une solution basée sur la triangulation. La méthode proposée est capable de localiser environ 90% des nœuds d'un réseau avec un degré moyen de 10. Dans la localisation basée sur les empreintes digitales, les réponses des chaînes sans fil sont utilisées pour estimer la position d'un émetteur communiquant avec une antenne MIMO. Dans ce travail, nous appliquons des techniques d'apprentissage classiques (K-nearest neighbors) et des techniques d'apprentissage en profondeur (Multi-Layer Perceptron Neural Network et Convolutional Neural Networks) pour localiser un émetteur dans des contextes intérieurs et extérieurs. Notre travail a obtenu le premier prix au concours de positionnement préparé par IEEE Communication Theory Workshop parmi 8 équipes d'universités de grande réputation du monde entier en obtenant une erreur carrée moyenne de 2,3 cm.
Cette thèse se concentre sur le développement de modèles de représentation dense d'objets 3-D à partir d'images. L'objectif de ce travail est d'améliorer les modèles surfaciques 3-D fournispar les systèmes de vision par ordinateur, en utilisant de nouveaux éléments tirés des images, plutôt que les annotations habituellement utilisées, ou que les modèles basés sur unedivision de l'objet en différents parties. Des réseaux neuronaux convolutifs (CNNs) sont utilisés pour associer de manière dense les pixels d'une image avec les coordonnées 3-D d'un modèle de l'objet considéré. Cette méthode permet de résoudre très simplement une multitude de tâches de vision par ordinateur,telles que le transfert d'apparence, la localisation de repères ou la segmentation sémantique, en utilisant la correspondance entre une solution sur le modèle surfacique 3-D et l'image 2-D considérée. On démontre qu'une correspondance géométrique entre un modèle 3-D et une image peut être établie pour le visage et le corps humains.
Cette étude vise à développer un système d'aide au diagnostic (CAD) pour la détection de lésions épileptogènes, reposant sur l'analyse de données de neuroimagerie, notamment, l'IRM T1 et FLAIR. L'approche adoptée, introduite précédemment par Azami et al., 2016, consiste à placer la tâche de détection dans le cadre de la détection de changement à l'échelle du voxel, basée sur l'apprentissage d'un modèle one-class SVM pour chaque voxel dans le cerveau. Les caractéristiques manuelles ne sont pas forcément les plus pertinentes pour la tâche visée. Notre première contribution porte sur l'intégration de différents réseaux profonds non-supervisés, pour extraire des caractéristiques dans le cadre du problème de détection de changement. Nous introduisons une nouvelle configuration des réseaux siamois, mieux adaptée à ce contexte. Le système CAD proposé a été évalué sur l'ensemble d'images IRM T1 des patients atteints d'épilepsie. Afin d'améliorer la performance obtenue, nous avons proposé d'étendre le système pour intégrer des données multimodales qui possèdent des informations complémentaires sur la pathologie. Notre deuxième contribution consiste donc à proposer des stratégies de combinaison des différentes modalités d'imagerie dans un système pour la détection de changement. Ce système multimodal a montré une amélioration importante sur la tâche de détection de lésions épileptogènes sur les IRM T1 et FLAIR. Notre dernière contribution se focalise sur l'intégration des données TEP dans le système proposé. Etant donné le nombre limité des images TEP, nous envisageons de synthétiser les données manquantes à partir des images IRM disponibles. Nous démontrons que le système entraîné sur les données réelles et synthétiques présente une amélioration importante par rapport au système entraîné sur les images réelles uniquement.
Le raisonnement par analogie est reconnu comme une des principales caractéristiques de l'intelligence humaine. En tant que tel, il a pendant longtemps été étudié par les philosophes et les psychologues, mais de récents travaux s'intéressent aussi à sa modélisation d'un point de vue formel à l'aide de proportions analogiques, permettant l'implémentation de programmes informatiques. Nous nous intéressons ici à l'utilisation des proportions analogiques à des fins prédictives, dans un contexte d'apprentissage artificiel. Dans de récents travaux, les classifieurs analogiques ont montré qu'ils sont capables d'obtenir d'excellentes performances sur certains problèmes artificiels, là où d'autres techniques traditionnelles d'apprentissage se montrent beaucoup moins efficaces. Partant de cette observation empirique, cette thèse s'intéresse à deux axes principaux de recherche. Le premier sera de confronter le raisonnement par proportion analogique à des applications pratiques, afin d'étudier la viabilité de l'approche analogique sur des problèmes concrets. Le second axe de recherche sera d'étudier les classifieurs analogiques d'un point de vue théorique, car jusqu'à présent ceux-ci n'étaient connus que grâce à leurs définitions algorithmiques. Les propriétés théoriques qui découleront nous permettront de comprendre plus précisément leurs forces, ainsi que leurs faiblesses. Comme domaine d'application, nous avons choisi celui des systèmes de recommandation. On reproche souvent à ces derniers de manquer de nouveauté ou de surprise dans les recommandations qui sont adressées aux utilisateurs. Le raisonnement par analogie, capable de mettre en relation des objets en apparence différents, nous est apparu comme un outil potentiel pour répondre à ce problème. Du côté théorique, une contribution majeure de cette thèse est de proposer une définition fonctionnelle des classifieurs analogiques, qui a la particularité d'unifier les approches préexistantes. Cette définition fonctionnelle nous permettra de clairement identifier les liens sous-jacents entre l'approche analogique et l'approche par k plus De plus, nous avons pu identifier un critère qui rend l'application de notre principe d'inférence analogique parfaitement certaine (c'est-à-dire sans erreur), exhibant ainsi les propriétés linéaires du raisonnement par analogie.
Notre objectif a été le développement de biomarqueurs quantitatifs pour caractériser l'organisation géométrique des quatre variants de FN à partir d'images de microscopie confocale 2D, puis de comparer les tissus sains et cancéreux. Premièrement, nous avons montré à travers deux pipelines de classification fondés sur les curvelets et sur l'apprentissage profond, que les variants peuvent être distingués avec une performance similaire à celle d'un annotateur humain. Nous avons ensuite construit une représentation des fibres (détectées avec des filtres Gabor) fondée sur des graphes. Les variantes ont été classés en utilisant des attributs spécifiques aux graphes, prouvant que ceux-ci intègrent des informations pertinentes dans les images confocales. De plus, nous avons identifié différentes techniques capables de différencier les graphes, afin de comparer les variants de FN quantitativement et qualitativement. Une analyse des performances sur des exemples simples a montré la capacité des méthodes fondées sur l'appariement de graphes et le transport optimal, de comparer les graphes. Nous avons ensuite proposé différentes méthodologies pour définir le graphe représentatif d'une certaine classe. De plus, l'appariement de graphes nous a permis de calculer des cartes de déformation des paramètres entre tissus sains et cancéreux. Ces cartes ont ensuite été analysées dans un cadre statistique montrant si la variation du paramètre peut être expliquée ou non par la variance au sein d'une même classe.
Dans ce travail, nous étudions ces deux aspects de la mobilité humaine en proposant des algorithmes de machine learning adapté aux sources d'information disponibles dans chacun des contextes. Pour l'étude de la mobilité en environnement extérieur, nous utilisons les données de coordonnées GPS collectées pour découvrir les schémas de mobilité quotidiens des utilisateurs. Cette méthode de segmentation est basée sur l'estimation des densités de probabilité des trajectoires, ce qui atténue les problèmes causés par le bruit des données. Concernant l'étude de la mobilité humaine dans les bâtiments, nous utilisons les données d'empreintes digitales WiFi collectées par les smartphones. De plus, en ce qui concerne la localisation précise en intérieur, nous supposons qu'il existe une distribution latente régissant l'entrée et la sortie en même temps. Sur la base de cette hypothèse, nous avons développé un modèle d'apprentissage semi-supervisé basé sur le variational autoencoder (VAE). Dans la procédure d'apprentissage non supervisé, nous utilisons un modèle VAE pour apprendre une distribution latente de l'entrée qui est composée de données d'empreintes digitales WiFi. Dans la procédure d'apprentissage supervisé, nous utilisons un réseau de neurones pour calculer la cible, coordonnées par l'utilisateur. De plus, sur la base de la même hypothèse utilisée dans le modèle d'apprentissage semi-supervisé basé sur le VAE, nous exploitons la théorie des goulots d'étranglement de l'information pour concevoir un modèle basé sur le variational information bottleneck (VIB). Il s'agit d'un modèle d'apprentissage en profondeur de bout en bout plus facile à former et offrant de meilleures performances.
La TA contextuelle a pour objectif de dépasser cette limitation en explorant différentes méthodes d'intégration du contexte extra-phrastique dans le processus de traduction. Les phrases environnantes (contexte linguistique) et le contexte de production des énoncés (contexte extra-linguistique) peuvent fournir des informations cruciales pour la traduction, notamment pour la prise en compte des phénomènes discursifs et des mécanismes référentiels. La prise en compte du contexte est toutefois un défi pour la traduction automatique. Évaluer la capacité de telles stratégies à prendre réellement en compte le contexte et à améliorer ainsi la qualité de la traduction est également un problème délicat, les métriques d'évaluation usuelles étant pour cela inadaptées voire trompeuses. Dans cette thèse, nous proposons plusieurs stratégies pour intégrer le contexte, tant linguistique qu'extra-linguistique, dans le processus de traduction. Nos expériences s'appuient sur des méthodes d'évaluation et des jeux de données que nous avons développés spécifiquement à cette fin. Nous explorons différents types de stratégies : les stratégies par pré-traitement, où l'on utilise le contexte pour désambiguïser les données fournies en entrée aux modèles ; les stratégies par post-traitement, où l'on utilise le contexte pour modifier la sortie d'un modèle non-contextuel, et les stratégies où l'on exploite le contexte pendant la traduction proprement dite. Nous nous penchons sur de multiples phénomènes contextuels, et notamment sur la traduction des pronoms anaphoriques, la désambiguïsation lexicale, la cohésion lexicale et l'adaptation à des informations extra-linguistiques telles que l'âge ou le genre du locuteur. Nos expériences, qui relèvent pour certaines de la TA statistique et pour d'autres de la TA neuronale, concernent principalement la traduction de l'anglais vers le français, avec un intérêt particulier pour la traduction de dialogues spontanés.
Une deuxième difficulté qui handicape grandement les systèmes de dialogue est le fort taux d'erreur du système de reconnaissance vocale. Certaines propositions récentes permettent de réduire la complexité du modèle. Un premier suivi de croyance (belief update) est réalisé dans l'espace maitre (en intégrant des observations probabilistes sous forme de listes nbest). Les tests réels sur des utilisateurs humains montrent qu'un système optimisé par renforcement obtient cependant de meilleures performances sur le critère pour lequel il a été optimisé.
La présence relativement récente de migrants transnationaux invite à porter un regard renouvelé sur les dynamiques d'identification entre « autochtones » et « allochtones » et, plus particulièrement, à en considérer les implications au niveau éducatif. En donnant des appareils photos jetables à des collégien-nes de la ville de Bozen-Bolzano avec la mission de photographier « les langues du quartier » pour en faire une exposition, il s'agit d'interroger les façons dont, dans une ville qui semble incarner une certaine hybridité, des jeunes interprètent la pluralité qui les environne.
Fournir les infrastructures de calcul nécessaires à la résolution des problèmes complexes de la société moderne constitue un défi stratégique. Les organisations y répondent classiquement en mettant en place de larges infrastructures de calcul parallèle et distribué. Les vendeurs de systèmes de Calcul Hautes Performances sont incités par la compétition à produire toujours plus de puissance de calcul et de stockage, ce qui mène à des plateformes "Petascale" spécifiques et sophistiquées, et bientôt à des machines "Exascale". Ces systèmes sont gérés de manière centralisée à l'aide desolutions logicielles de gestion de jobs et de resources dédiées. Un problèmecrucial auquel répondent ces logiciels est le problème d'ordonnancement, pourlequel le gestionnaire de resources doit choisir quand, et sur quellesresources exécuter quelle tache calculatoire. Cette thèse fournit des solutions à ce problème. Toutes les plateformes sont différentes. En effet, leur infrastructure, le comportement de leurs utilisateurs et les objectifs del'organisation hôte varient. Nous soutenons donc que les politiques d'ordonnancement doivent s'adapter au comportement des systèmes. Dans ce manuscrit, nous présentons plusieurs manières d'obtenir cette adaptativité. A travers une approche expérimentale, nous étudions plusieurs compromis entre la complexité de l'approche, le gain potentiel, et les risques pris.
Notre problématique porte sur les liens entre logique et topoï, sémantique et topoï et enfin entre logique et sémantique. Le travail consiste donc à mettre en relief la pérennité de la notion de topos, son évolution et son intégration dans la logique, dans la sémantique et même dans l'informatique. Dès lors, topoï, logique et sémantique sont les trois facettes de cette problématique. Il consiste aussi à montrer que cette notion est présente dans toutes les approches linguistiques et que la sémantique est l'axe fédérateur de toutes les recherches linguistiques. Nous n'avons pas l'intention de donner une définition nouvelle, mais nous avons essayé de faire la synthèse des caractéristiques principales des topoï et de dresser un tableau des recherches passées et actuelles élaborées sur cette notion. L'approche que nous avons adoptée est constructive et diachronique. Elle a mis en relief l'évolution des topoï, de leurs formes, de leurs sens et de leurs usages. La notion de topoï continue à exister de l'antiquité jusqu'à nos jours, puisqu'elle a des fondements sémantiques, qui sont inhérents à toutes les langues naturelles. La première expose la conception terminologique de la notion de topoï et son aspect évolutif. Elle met en évidence les différents emplois, les différentes définitions et l'aspect évolutif des topoï. Dans cette première partie, nous avons essayé de dresser à grands traits l'historique de la notion de topoï, l'emploi qui en a été fait par les logiciens, les rhéteurs, les linguistes, les pragmaticiens et les informaticiens. La deuxième est consacrée à la présentation des fondements sémantiques de la logique et des topoï et à la relation de complémentarité de ces trois notions, à savoir logique, sémantique et topoï. Nous avons précisé que la théorie des topoï est une théorie du sens. Et la troisième partie traite les fondements lexicaux des topoï et les exploitations modernes de cette notion, à savoir le traitement automatique, la théorie des cadres et l'ontologie, tout en présentant les différentes théories sémantiques argumentatives. Nous avons présenté un petit exemple, un échantillon d'extraction de l'information à l'aide de la plateforme NooJ.
La compréhension du langage naturel repose souvent sur des raisonnements de sens commun, pour lesquels la connaissance de relations sémantiques, en particulier entre prédicats verbaux, peut être nécessaire. Cette thèse porte sur la problématique de l'utilisation d'une méthode distributionnelle pour extraire automatiquement les informations sémantiques nécessaires à ces inférences de sens commun. Des associations typiques entre des paires de prédicats et un ensemble de relations sémantiques (causales, temporelles, de similarité, d'opposition, partie/tout) sont extraites de grands corpus, par l'exploitation de la présence de connecteurs du discours signalant typiquement ces relations. Afin d'apprécier ces associations, nous proposons plusieurs mesures de signifiance inspirées de la littérature ainsi qu'une mesure novatrice conçue spécifiquement pour évaluer la force du lien entre les deux prédicats et la relation. La pertinence de ces mesures est évaluée par le calcul de leur corrélation avec des jugements humains, obtenus par l'annotation d'un échantillon de paires de verbes en contexte discursif. Nous évaluons le potentiel de ces représentations pour plusieurs applications. Concernant l'analyse du discours, les tâches de la prédiction d'attachement entre unités du discours, ainsi que la prédiction des relations discursives spécifiques les reliant, sont explorées. En utilisant uniquement les traits provenant de notre ressource, nous obtenons des améliorations significatives pour les deux tâches, par rapport à plusieurs bases de référence, notamment des modèles utilisant d'autres types de représentations lexico-sémantiques. Nous proposons également de définir des ensembles optimaux de connecteurs mieux adaptés à des applications sur de grands corpus, en opérant une réduction de dimension dans l'espace des connecteurs, au lieu d'utiliser des groupes de connecteurs composés manuellement et correspondant à des relations prédéfinies. Ces applications diverses démontrent les contributions prometteuses amenées par notre approche permettant l'extraction non supervisée de relations sémantiques.
Cette recherche aborde l'étude d'un sous-ensemble de formules expressives de la conversation (FEC) (tu parles, à quoi bon, c'est bon, ça m'étonne) appréhendées sous l'angle de leur analyse et leur description syntaxique, sémantique, pragmatique et discursive. Cette étude permet d'examiner des questions complexes telles que la terminologie choisie, les valeurs sémantiques, le comportement syntaxique, le rôle du contexte ou les fonctionnements pragmatiques. Nous nous proposons aussi de réfléchir sur le statut des FEC dans la lexicographie, ce qui permet de mieux approfondir leur description par rapport à ce que proposent les dictionnaires généraux et spécialisés. Nous avons mené cette étude en quatre temps. Tout d'abord, sur le plan théorique, nous avons présenté une synthèse des travaux existants en montrant les difficultés de nature terminologique et définitoire liées à ce phénomène linguistique pour lequel une délimitation s'impose. Pour notre part, nous avons choisi le terme formule expressive et nous avons justifié notre choix à travers la sélection des critères permettant de circonscrire cette notion. Par la suite, du point de vue méthodologique, l'étude se situe dans l'approche de la linguistique de corpus dans une démarche qualitative et quantitative. Elle s'appuie sur des corpus envisagés dans plusieurs cadres : l'écrit (littérature — tweets) et l'oral (Orféo). Après avoir présenté le cadre théorique et méthodologique, nous avons procédé à l'étude concrète à travers l'analyse discursive des FEC dans deux sous-corpus différents. L'objectif primordial de cette étude est d'aboutir à des critères bien déterminés limitant cette sous-classe de phraséologismes pragmatique par rapport aux autres sous-types proposés par les chercheurs dans ce domaine linguistique. Finalement, nous avons proposé le traitement lexicographique de certaines formules sélectionnées dans le cadre du projet Polonium. L'enjeu est ici d'aboutir à un modèle lexicographique fonctionnel pour le traitement lexicographique des FEC.Nous pensons être parvenue à réaliser les enjeux que nous nous étions fixés au départ. Par une analyse discursive des FEC dans le texte, nous avons voulu affiner les traits définitoires et d'opter pour une définition exhaustive de ce sous-ensemble. Par l'examen de leur statut dans le domaine de la lexicographie, nous avons retenu une méthode descriptive détaillée et prometteuse des FEC.
La population des villes devrait doubler d'ici le milieu du siècle, selon les estimations de l'OMS. Cette augmentation rapide de la population a un impact sur les transports et la croissance économique, et accroîtra les responsabilités des autorités de gestion locales. Nous vivons une transformation des villes en villes intelligentes offrant de nouveaux services à la population, en optimisant l'utilisation des ressources disponibles. Qu'il s'agisse de données provenant des citoyens, de données gouvernementales ouvertes ou d'autres sources en ligne, une pluralité de sources de données peut permettre la création d'outils intelligents pour gérer efficacement les activités quotidiennes. De plus, grâce au progrès d'Internet et des technologies mobiles, les plateformes de réseaux sociaux (Twitter) sont devenues des modes de communication populaires. Elles permettent aux utilisateurs de partager un large éventail d'informations, y compris des données spatio-temporelles. Ainsi, Il est aisé d'accéder, en temps réel, à des connaissances provenant de différents types de données disponibles, riches, géo-référencées et issues de sources multiples et de les intégrer sur une carte. Dans cette thèse, nous proposons d'abord un système de recommandation d'itinéraires, tenant compte des contraintes temps réel, en l'absence d'infrastructure physique ; en exploitant les données géolocalisées issues de réseaux sociaux (twitter) pour identifier les contraintes de trafic temps réel et, par conséquent, recommander un chemin optimisé. Nous avons mis en œuvre un système d'indexation à base de grille spatiale pour notre modèle de prédiction en quasi-temps réel. Ensuite, nous avons introduit le concept de "cartes intelligentes " intégrant la représentation visuelle de couches de « connaissances pertinentes » par le biais de la collecte, la gestion et l'intégration de sources de données hétérogènes. Contrairement aux cartes conventionnelles, les cartes intelligentes extraient des informations à partir des événements annoncés et découverts en temps réel (concerts, incidents,...), les offres en ligne et les analyses statistiques (zones dangereuses, …) en encapsulant les données entrantes semi-structurées et non structurées dans des paquets génériques structurés. Cette méthodologie ouvre la voie à la fourniture de services et applications intelligents. De plus, le développement de ''cartes intelligentes'' nécessite un traitement efficace et évolutif et la visualisation de couches basées sur les connaissances à plusieurs échelles cartographiques, permettant ainsi une navigation fluide et sans encombre. Enfin, nous présentons Hadath, un système évolutif et efficace qui extrait les événements sociaux d'une multitude de flux de données non structurés. Le système comprend un composant de gestion et de prétraitement des différents types de sources de données et génère des paquets de données structurés à partir de flux non structurés. Par conséquent, les événements détectés sont affichés à différentes résolutions spatio-temporelles, ce qui permet une navigation fluide. Enfin, pour valider notre approche, nous avons mené des expériences sur des flux de données réelles. Le résultat final du système proposé, nommé Hadath crée une expérience unique et dynamique de navigation cartographique.
Ce manuscrit porte sur la reconnaissance automatique du locuteur indépendante du texte en utilisant des paramètres de régression linéaire par maximum de vraisemblance (MLLR). Ces paramètres sont obtenus par l'adaptation d'un modèle acoustique indépendant du locuteur aux données de parole d'un locuteur et sont des indices pertinents qui caractérisent ce locuteur. Nous utilisons le paradigme MLLR-SVM qui classifie ces coefficients avec une Machine à Vecteurs Support (SVM). Nous proposons une approche purement acoustique qui n'utilise pas de transcriptions tout en évitant de dépendre de la langue en utilisant des transformations MLLR contraintes (CMLLR) et l'apprentissage d'un modèle du monde adapté au locuteur (SAT). Nous explorons les systèmes multi-classe (C)MLLR-SVM basés sur des modèles acoustiques phonémiques. Une étude expérimentale complète des schémas d'adaptation est réalisée sur de multiples axes tels que le type de paramètres cepstraux, le type et le nombre de transformations, le type de modèle et la méthode d'apprentissage.
Pourquoi et comment les grandes entreprises traitent-elles les réclamations de leur clientèle ? Quels effets ce traitement a-t-il sur les régulations internes des firmes ? Que peut espérer le réclamant ? Cette thèse se propose de traiter ce faisceau de questions par une enquête ethnographique menée dans deux grandes entreprises françaises. S'appuyant sur l'outillage analytique développé par Albert O. Hirschman, elle propose une description historique et sociologique des pratiques de traitement des réclamations. Ainsi, elle souhaite contribuer à la problématique de l'influence du destinataire final d'une marchandise sur les entreprises qui la produisent et la vendent.
Une jurisprudence est un corpus de décisions judiciaires représentant la manière dont sont interprétées les lois pour résoudre un contentieux. Elle est indispensable pour les juristes qui l'analysent pour comprendre et anticiper la prise de décision des juges. Son analyse exhaustive est difficile manuellement du fait de son immense volume et de la nature non-structurée des documents. L'estimation du risque judiciaire par des particuliers est ainsi impossible car ils sont en outre confrontés à la complexité du système et du langage judiciaire. L'automatisation de l'analyse des décisions permet de retrouver exhaustivement des connaissances pertinentes pour structurer la jurisprudence à des fins d'analyses descriptives et prédictives. Afin de rendre la compréhension d'une jurisprudence exhaustive et plus accessible, cette thèse aborde l'automatisation de tâches importantes pour l'analyse métier des décisions judiciaires. En premier, est étudiée l'application de modèles probabilistes d'étiquetage de séquences pour la détection des sections qui structurent les décisions de justice, d'entités juridiques, et de citations de lois. Ensuite, l'identification des demandes des parties est étudiée. L'approche proposée pour la reconnaissance des quanta demandés et accordés exploite la proximité entre les sommes d'argent et des termes-clés appris automatiquement. Nous montrons par ailleurs que le sens du résultat des juges est identifiable soit à partir de termes-clés prédéfinis soit par une classification des décisions. Enfin, pour une catégorie donnée de demandes, les situations ou circonstances factuelles où sont formulées ces demandes sont découvertes par regroupement non supervisé des décisions. A cet effet, une méthode d'apprentissage d'une distance de similarité est proposée et comparée à des distances établies. Cette thèse discute des résultats expérimentaux obtenus sur des données réelles annotées manuellement. Le mémoire propose pour finir une démonstration d'applications à l'analyse descriptive d'un grand corpus de décisions judiciaires françaises.
Les matériels mobiles actuels, et les téléphones mobiles en particulier, sont équipés de différentes technologies sans fil qui augmentent et diversifient leurs capacités de communication. L'utilisation combinée et efficace de ces technologies offre des possibilités variées et accrues en termes de services et d'applications. Néanmoins elle requiert la réalisation d'analyses fines en matières de sécurité et de choix du mode de communication à utiliser en fonction de critères dépendant du contexte : coût énergétique, coût financier, préférences des entités impliquées, préservation de la vie privée, etc. Notre contribution à ce projet est la création d'applications collaboratives qui utilisent de façon appropriée la gamme des technologies sans fil disponibles sur les matériels considérés. En d'autres termes, on cherche à utiliser les moyens de transmission les plus appropriés (au sens des critères indiqués plus haut) que deux ou plusieurs équipements mobiles peuvent utiliser pour réaliser leurs échanges, qui plus est, sans que cela ne nécessite de connaître leurs positions respectives. La transparence de la localisation des cibles devient ainsi une règle. On peut synthétiser la question centrale que nous avons choisie d'étudier de la manière suivante : comment faire communiquer un ensemble de terminaux mobiles (des téléphones portables en particulier) de façon sécurisée en utilisant la technologie la plus adaptée en fonction du contexte ? Notre objectif est de proposer une réponse à cette question en définissant une plate-forme multi-niveaux prenant en compte les différentes technologies disponibles sur les équipements considérés. Il s'agit en particulier d'identifier l'ensemble des éléments à prendre en compte dans la conception de la plate-forme, de les modéliser, de développer des applications de référence et de valider la pertinence des solutions proposées par des tests, ainsi que des évaluations qualitatives et quantitatives.
Cette thèse propose une nouvelle approche des écrits scientifiques en prenant comme point de départ les marqueurs discursifs (MD). Elle s'inscrit dans le cadre du Français sur Objectif Universitaire (FOU). Dans ce travail, nous nous intéressons tout particulièrement aux MD polylexicaux et les intégrons dans une conception large de la phraséologie. La particularité de cette recherche réside dans le fait de relier les descriptions linguistiques des MD et la transposition didactique de ces unités lexicales à l'aide de corpus, ce qui est encore peu abordé dans le champ de la didactique francophone. Nous cherchons à répondre à des objectifs à la fois linguistiques et didactiques. Pour les objectifs linguistiques, nous mettons en place un modèle d'analyse des MD polylexicaux associant les propriétés syntaxiques et sémantiques et qui est tout à fait réadaptable à d'autres MD. Les analyses linguistiques des MD serviront par la suite à l'enseignement/apprentissage de ces unités. Pour les objectifs didactiques, cette recherche vise à concevoir une méthodologie d'enseignement/apprentissage des MD à partir de l'observation de corpus. Les considérations méthodologiques proposées dans le cadre de cette thèse ouvrent des pistes intéressantes pour l'enseignement/apprentissage de ces éléments linguistiques ainsi que pour faciliter l'accès aux écrits scientifiques auprès des étudiants non-natifs.
Les humains sont au coeur de nombreux problèmes de vision par ordinateur, tels que les systèmes de surveillance ou les voitures sans pilote. Ils sont également au centre de la plupart des contenus visuels, pouvant amener à des jeux de données très larges pour l'entraînement de modèles et d'algorithmes. Par ailleurs, si les données stéréoscopiques font l'objet d'études depuis longtemps, ce n'est que récemment que les films 3D sont devenus un succès commercial. Dans cette thèse, nous étudions comment exploiter les données additionnelles issues des films 3D pour les tâches d'analyse des personnes. Nous explorons tout d'abord comment extraire une notion de profondeur à partir des films stéréoscopiques, sous la forme de cartes de disparité. En s'appuyant sur la relative facilité de la tâche de détection de personne dans les films 3D, nous développons une méthode pour collecter automatiquement des exemples de personnes dans les films 3D afin d'entraîner un détecteur de personne pour les films non 3D. Nous nous concentrons ensuite sur la segmentation de plusieurs personnes dans les vidéos. Nous formulons ce problème comme un problème d'étiquetage de graphe multi-étiquettes, et notre méthode intègre un modèle des occlusions pour produire une segmentation multi-instance par plan. Après avoir montré l'efficacité et les limitations de cette méthode, nous proposons un second modèle, qui ne repose lui que sur des détections de personne à travers la vidéo, et pas sur des estimations de posture. Nous formulons ce problème comme la minimisation d'un coût quadratique sous contraintes linéaires. Ces contraintes encodent les informations de localisation fournies par les détections de personne. Cette méthode ne nécessite pas d'information de posture ou des cartes de disparité, mais peut facilement intégrer ces signaux supplémentaires. Elle peut également être utilisée pour d'autres classes d'objets. Nous évaluons tous ces aspects et démontrons la performance de cette nouvelle méthode.
Les relations d'équivalences entre les différentes requêtes permettent de réunir les participants au sein de communautés d'intérêt. Celles-ci forment alors une abstraction permettant de séparer le problème d'organisation du système en plusieurs sous-problèmes plus simples et de taille réduite. Afin de garantir une généricité vis-à-vis du langage, l'organisation repose sur une API simple et modulable. Le choix entre ces différentes possibilités est ensuite effectué à l'aide d'un modèle de coût paramétrable. Les relations entre communautés sont concrétisées par un échange de ressources entre elles, un participant de l'une venant contribuer à l'autre. Cela permet de s'affranchir des limitations de capacités au niveau abstrait, tout en en tenant hautement compte pour la mise en relation effective des participants. Au sein des communautés, un arbre de diffusion permet à l'ensemble des participants de récupérer les résultats requis. L'approche, mise en œuvre de manière incrémentale, permet une réduction efficace des coûts de calcul et de diffusion (l'optimalité est atteinte, notamment, dans le cas de l'inclusion de requête) pour un coût d'organisation limité et une latence raisonnable. Les expérimentations réalisées ont montré une grande adaptabilité aux variations concernant les requêtes exprimées et les capacités des participants. Le démonstrateur mis en place peut être utilisé à la fois pour des simulations (automatiques ou interactives) et pour un déploiement réel, par une implémentation commune générique vis-à-vis du langage.
L'extraction d'information non supervisée en domaine ouvert est une évolution récente de l'extraction d'information adaptée à des contextes dans lesquels le besoin informationnel est faiblement spécifié. Dans ce cadre, la thèse se concentre plus particulièrement sur l'extraction et le regroupement de relations entre entités en se donnant la possibilité de traiter des volumes importants de données. L'extraction de relations se fixe plus précisément pour objectif de faire émerger des relations de type non prédéfini à partir de textes. Leur extraction est réalisée en deux temps : des relations candidates sont d'abord extraites sur la base de critères simples mais efficaces pour être ensuite filtrées selon des critères plus avancés. Ce filtrage associe lui-même deux étapes : une première étape utilise des heuristiques pour éliminer rapidement les fausses relations en conservant un bon rappel tandis qu'une seconde étape se fonde sur des modèles statistiques pour raffiner la sélection des relations candidates. Il est réalisé dans le cas présent selon une stratégie multiniveau permettant de prendre en compte à la fois un volume important de relations et des critères de regroupement élaborés. Un premier niveau de regroupement, dit de base, réunit des relations proches par leur expression linguistique grâce à une mesure de similarité vectorielle appliquée à une représentation de type « sac-de-mots » pour former des clusters fortement homogènes. Un second niveau de regroupement est ensuite appliqué pour traiter des phénomènes plus sémantiques tels que la synonymie et la paraphrase et fusionner des clusters de base recouvrant des relations équivalentes sur le plan sémantique. Ce second niveau s'appuie sur la définition de mesures de similarité au niveau des mots, des relations et des clusters de relations en exploitant soit des ressources de type WordNet, soit des thésaurus distributionnels. Enfin, le travail illustre l'intérêt de la mise en œuvre d'un clustering des relations opéré selon une dimension thématique, en complément de la dimension sémantique des regroupements évoqués précédemment. Ce clustering est réalisé de façon indirecte au travers du regroupement des contextes thématiques textuels des relations. Pour les mesures externes, une méthode interactive est proposée pour construire manuellement un large ensemble de clusters de référence. Son application sur un corpus journalistique de grande taille a donné lieu à la construction d'une référence vis-à-vis de laquelle les différentes méthodes de regroupement proposées dans la thèse ont été évaluées.
L'analyse linguistique est une étape fondamentale et essentielle pour le traitement automatique des langues. En effet, elle permet d'étiqueter les mots avec des catégories morphosyntaxiques et d'identifier des entités nommées pour pouvoir réaliser des applications du plus haut niveau, par exemple la recherche d'information, la traduction automatique, la question réponse, etc. Puisque le mot est l'unité essentielle d'une langue, une segmentation des phrases en mots est indispensable pour le traitement du chinois. Parmi des études existantes, la segmentation, l'étiquetage morphosyntaxique et l'identification des entités nommées sont souvent enchaînés comme les étapes différentes. C'est pourquoi des modèles statistiques qui réalisent la segmentation, l'étiquetage morphosyntaxique et l'identification des entités nommées ou la segmentation et l'un des deux autres traitements simultanément, ont été créés. Par conséquent, cette approche n'est pas approprie pour créer des systèmes d'analyse automatique multilingue. L'objectif de mon étude consiste à intégrer l'analyse automatique du chinois dans un système d'analyse D'abord, des traitements pour le chinois doivent être compatibles avec ceux d'autres langues. Ensuite, pour garder la cohérence et l'unité du système, il est favorable d'employer au maximum des modules de traitement en commun pour toutes les langues traitées par le système. En conséquence, le choix s'est porté sur l'utilisation des modules séparés pour la segmentation, l'étiquetage morphosyntaxique et l'identification des entités nommées. Néanmoins, ce type de méthodes enchaînant des trois traitements ne prend pas en compte des dépendances entre eux. Pour surmonter ce défaut, nous utilisons les informations fournies par l'analyse morphosyntaxique, par l'identification des entités nommées et par des connaissances linguistiques afin d'améliorer la segmentation. Etant donné ces interdépendances, trois traitements spécifiques sont rajoutés au système : un prétraitement avant la segmentation basée sur le modèle de cooccurrence, une tokenization de termes liés aux chiffres écrits en caractères chinois et un traitement complémentaire pour la segmentation en identifiant certaines entités nommées entre l'étape de la segmentation et celle de l'étiquetage morphosyntaxique.
Les systèmes déterminatifs du français et du polonais sont différents. Nos descriptions sont formalisées dans des bases de données dédiées au traitement automatique des langues. Nous rendons compte systématiquement du fonctionnement et des propriétés des déterminants polonais. Nous retenons tout particulièrement leurs propriétés syntactico-sémantiques selon qu'ils relèvent de la détermination prédicative et de la détermination argumentale. Le tout contribue à l'élaboration des dictionnaires électroniques du LDI.
Afin d'accompagner les professionnels de santé dans leur démarche clinique, plusieurs systèmes de suivi et deprise en charge médicale ont été construits et déployés dans le milieu hospitalier. Ces systèmes permettent principalement de collecter des données médicales sur les patients, de les analyser et de présenter les résultats de différentes manières. Ils représentent un appui et une aide aux professionnels de santé dans leur prise de décision par rapport à l'évolution de l'état de santé des patients suivis. L'utilisation de tels systèmes nécessite systématiquement une adaptation à la fois au domaine médical concerné et au mode d'intervention. Il est nécessaire, dans un milieu hospitalier, que ces systèmes puissent s'adapter et évoluer d'une manière simple, en limitant toute maintenance corrective ou évolutive. Ils doivent être en mesure de prendre en compte dynamiquement des connaissances théoriques et empiriques du domaine issues des experts médicaux. Cette approche permet notamment l'organisation de la collecte des données médicales, en tenant compte du contexte du patient, la représentation des connaissances du domaine à base d'ontologies ainsi que leur exploitation associée aux guides de bonnes pratiques et à l'expérience clinique. Dans la continuité des travaux précédemment réalisés au sein de notre équipe de recherche, nous avons choisi d'enrichir, avec notre approche, la plateforme E-care qui est dédiée au suivi et à la détection précoce de toute anomalie de l'état de patients atteints de maladies chroniques. Nous avons pu ainsi adapter facilement la plateforme E-care aux différentes expérimentations qui sont été menées notamment dans des EPHAD de la Mutualité Française en Anjou-Mayenne, au CHU de Hautepierre et au CHUV à Lausanne. Les résultats de ces expérimentations ont montré l'efficacité de l'approche proposée. L'adaptation de la plateforme par rapport au domaine et au mode d'intervention de chacune de ces expérimentations se limite à de la simple configuration. De plus, l'approche proposée a suscité l'intérêt du personnel médical par rapport à l'organisation de la collecte des données, qui tient compte du contexte du patient, et par rapport à l'exploitation des connaissances médicales qui apporte aux professionnels de santé une assistance pour une meilleure prise de décision.
Nous exposons d'abord une méthode de suivi basée sur un filtre particulaire, permettant de déterminer à tout moment la position de la tête, des coudes, du buste et des mains d'un signeur dans une vidéo monovue. Cette méthode a été adaptée à la LSF pour la rendre plus robuste aux occultations, aux sorties de cadre et aux inversions des mains du signeur. Ensuite, l'analyse de données issues de capture de mouvements nous permet d'aboutir à une catégorisation de différents mouvements fréquemment utilisés dans la production de signes. Nous en proposons un modèle paramétrique que nous utilisons dans le cadre de la recherche de signes dans une vidéo, à partir d'un exemple vidéo de signe.
L'analyse en dépendances est une composante essentielle de nombreuses applications de TAL (Traitement Automatique des Langues), Ce type d'analyse est dès lors limité à quelques langues seulement, qui disposent des ressources adéquates. Pour les langues peu dotées, la production de données annotées est une tâche impossible le plus souvent, faute de moyens et d'annotateurs disponibles. Afin de résoudre ce problème, la thèse examine trois méthodes d'amorçage, à savoir (1) l'apprentissage par transfert multilingue, (2) les plongements vectoriels contextualisés profonds et (3) le co-entrainement. La première idée, l'apprentissage par transfert multilingue, permet de transférer des connaissances d'une langue pour laquelle on dispose de nombreuses ressources, et donc de traitepments efficaces, vers une langue peu dotée. Les plongements vectoriels contextualisés profonds, quant à eux, permettent une représentation optimale du sens des mots en contexte, grâce à la notion de modèle de langage. Nos approches ne nécessitent qu'un petit dictionnaire bilingue ou des ressources non étiquetées faciles à obtenir (à partir de Wikipedia par exemple) pour améliorer la précision de l'analyse pour des langues où les ressources disponibles sont insuffisantes. Nous avons évalué notre analyseur syntaxique sur 57 langues à travers la participation aux campagnes d'évaluation proposées dans le cadre de la conférence CoNLL. Notre système a obtenu des résultats très compétitifs lors de campganes d'évaluation officielles, notamment lors des campagnes CoNLL 2017 et 2018. Cette thèse offre donc des perspectives intéressantes pour le traitement automatique des langues peu dotées, un enjeu majeur pour le TAL dans les années à venir.
Une entreprise, et particulièrement une PME ou une PMI, doit être apte à évoluer sur des secteurs d'activités souvent très concurrentiels qui évoluent rapidement, par exemple, en fonction d'une clientèle volatile et soucieuse de trouver des produits et des services moins chers et plus adaptés à ses besoins. La PME se trouve alors confrontée à des problèmes de réactivité et de flexibilité face à cette clientèle. Par effet direct, elle recherche à réduire les délais et les coûts de réalisation tout en privilégiant aussi la qualité et le degré d'innovation des biens et des services qu'elle propose. Le système d'information de cette PME est un enjeu essentiel pour mettre en œuvre cette stratégie et maximiser donc la réactivité et la flexibilité mais aussi la rentabilité et la qualité recherchées. Ce sont des qualités incontournables, garantes d'une autonomie et d'une reconnaissance dont la PME a grand besoin. Une partie de ce système d'information est de fait informatisée. Celui-ci supporte, mémorise et traite les informations nécessaires aux différents processus de décision, métier et support qui tapissent l'organisation pour servir la stratégie de l'entreprise. Les fonctionnalités, les interfaces et les données qui forment ce système informatisé sont donc cruciales à comprendre, à développer en accord avec les besoins de la PME, à améliorer au fur et à mesure de l'évolution de ces besoins. La PME est donc tentée de se lancer, seule ou accompagnée, dans des projets dits d'informatisation i.e. des projets visant le développement ou l'amélioration de son système informatisé. Nous nous intéressons ici à des projets visant à développer des applicatifs de gestion et de pilotage de la PME. La PME – prenant alors le rôle de la maîtrise d'ouvrage (MOA) – tout comme la société de services qui l'accompagne – prenant alors en charge le rôle de maîtrise d'œuvre (MOE) – doivent partager une vision commune des besoins d'informatisation. Elles sont alors appelées à mener en commun des activités d'ingénierie des besoins et des exigences (IBE). L'IBE guide et accompagne la PME pour arriver à décrire et formaliser ses besoins. Elle permet ensuite à la société de service de spécifier de manière plus formelle ces besoins sous forme d'exigences qui définissent alors les travaux de développement souhaités. L'IBE est souvent réalisée avec une assistance à maitrise d'ouvrage. Cette étape cruciale reste cependant difficile pour une PME. Elle est de plus souvent réalisée par la MOE elle-même pour faire face au manque de moyens, de temps et de compétences de la PME. Or, l'implication des collaborateurs de la PME est primordiale pour la réussite de tout projet d'informatisation, surtout si celui-ci impacte durablement le fonctionnement de la PME. Ces travaux, développés dans le cadre d'une collaboration Industrie/recherche avec la SSII RESULIS, ont consisté à développer une méthode d'IBE qui offre aux PME des concepts, des langages et des moyens de modélisation et de vérification simples mais suffisants tout en tant aisément manipulables de manière intuitive et donnant lieu à une formalisation pertinente pour la MOE. Cette méthode est basée sur le croisement et la complémentarité de principes issus de la Modélisation d'Entreprise et de l'Ingénierie Système pour l'élicitation de besoins. Des moyens de vérification et de validation semi-formels sont appliqués pour garantir certaines qualités attendues des exigences résultantes. La méthode s'intègre également au cycle de développement basé sur les modèles pour permettre a posteriori d'accélérer la production de prototypes et de rendre interopérables les langages et outils de la MOA et de la MOE.
Les sciences participatives, et en particulier la myriadisation (crowdsourcing) bénévole, représentent un moyen peu exploité de créer des ressources langagières pour certaines langues encore peu dotées, et ce malgré la présence de locuteurs sur le Web. Nous présentons dans ce travail les expériences que nous avons menées pour permettre la myriadisation de ressources langagières dans le cadre du développement d'un outil d'annotation automatique en parties du discours. Nous avons appliqué cette méthodologie à trois langues non standardisées, en l'occurrence l'alsacien, le créole guadeloupéen et le créole mauricien. Pour des raisons historiques différentes, de multiples pratiques (ortho)graphiques co-existent en effet pour ces trois langues. Les difficultés posées par l'existence de cette variation nous ont menée à proposer diverses tâches de myriadisation permettant la collecte de corpus bruts, d'annotations en parties du discours, et de variantes graphiques. L'analyse intrinsèque et extrinsèque de ces ressources, utilisées pour le développement d'outils d'annotation automatique, montrent l'intérêt d'utiliser la myriadisation dans un cadre linguistique non standardisé : les locuteurs ne sont pas ici considérés comme un ensemble uniforme de contributeurs dont les efforts cumulés permettent d'achever une tâche particulière, mais comme un ensemble de détenteurs de connaissances complémentaires. Les ressources qu'ils produisent collectivement permettent de développer des outils plus robustes à la variation rencontrée. Les plateformes développées, les ressources langagières, ainsi que les modèles de taggers entraînés sont librement disponibles.
Depuis son émergence au début des années 1990, la notion d'ontologie s'est rapidement diffusée dans un grand nombre de domaines de recherche. Compte tenu du caractère prometteur de cette notion, de nombreux travaux portent sur l'utilisation des ontologies dans des domaines aussi divers que la recherche d'information, le commerce électronique, le web sémantique, l'intégration de données, etc. L'efficacité de tous ces travaux présuppose l'existence d'une ontologie de domaine susceptible d'être utilisée. Or, la conception d'une telle ontologie s'avère particulièrement difficile si l'on souhaite qu'elle fasse l'objet de consensus. S'il existe des outils utilisés pour éditer une ontologie supposée déjà conçue, et s'il existe également plusieurs plate-formes de traitement automatique de la langue permettant d'analyser automatiquement les corpus et de les annoter tant du point de vue syntaxique que statistique, il est difficile de trouver une procédure globalement acceptée, ni a fortiori un ensemble d'outils supports permettant de concevoir une ontologie de domaine de façon progressive, explicite et traçable à partir d'un ensemble de ressources informationnelles relevant de ce domaine. L'objectif du projet ANR DaFOE4App (Differential and Formal Ontologies Editor for Application), dans lequel s'inscrit notre travail, était de favoriser l'émergence d'un tel ensemble d'outils. Contrairement à d'autres outils de construction d'ontologies, la plate-forme DaFOE, présentée dans cette thèse, ne propose pas un processus de construction figé ni en nombre d'étapes, ni sur la représentation des étapes. En effet, dans cette thèse nous généralisons le processus de construction d'ontologies pour un nombre quelconque d'étapes. L'intérêt d'une telle généralisation étant, par exemple, d'offrir la possibilité de raffiner le processus de construction en insérant ou modifiant des étapes. On peut également souhaiter supprimer certaines étapes à fin de simplifier le processus de construction. L'objectif de cette généralisation est de minimiser l'impact de l'ajout, suppression ou modification d'une étape dans le processus global de construction d'ontologies, tout en préservant la cohérence globale du processus de construction. Pour y parvenir, notre approche consiste à utiliser l'Ingénierie Dirigée par les Modèles pour caractériser chaque étape au sein d'un modèle et ensuite ramener le problème du passage d'une étape à l'autre à un problème de mapping de modèles. Les mappings établis entre les modèles sont ensuite utilisés pour semi-automatiser le processus de construction d'ontologies. L'originalité du langage MQL se trouve dans sa capacité, au travers de requêtes syntaxiquement compactes, à explorer transitivement tout ou partie du graphe de mappings lors d'une recherche d'informations.
La problématique traitée par la Reconnaissance de la Langue (LR) porte sur la définition découverte de la langue contenue dans un segment de parole. Cette thèse se base sur des paramètres acoustiques de courte durée, utilisés dans une approche d'adaptation de mélanges de Gaussiennes (GMM-UBM). Le problème majeur de nombreuses applications du vaste domaine de la re- problème connaissance de formes consiste en la variabilité des données observées. Dans le contexte de la Reconnaissance de la Langue (LR), cette variabilité nuisible est due à des causes diverses, notamment les caractéristiques du locuteur, l'évolution de la parole et de la voix, ainsi que les canaux d'acquisition et de transmission. Dans le contexte de la reconnaissance du locuteur, l'impact de la variabilité solution peut sensiblement être réduit par la technique d'Analyse Factorielle (Joint Factor Analysis, JFA). Dans ce travail, nous introduisons ce paradigme à la Reconnaissance de la Langue. La deuxième hypothèse, plus technique, est que la variabilité nuisible se situe dans un sous-espace de faible dimension, qui est défini de manière globale. Dans ce travail, nous analysons le comportement de la JFA dans le contexte d'un dispositif de LR du type GMM-UBM. Nous introduisons et analysons également sa combinaison avec des Machines à Vecteurs Support (SVM). Les premières publications sur la JFA regroupaient toute information qui est amélioration nuisible à la tâche (donc ladite variabilité) dans un seul composant. Celui-ci est supposé suivre une distribution Gaussienne. En pratique, nous observons que cette hypothèse n'est pas toujours vérifiée. Nous avons, par exemple, le cas où les données peuvent être groupées de manière logique en deux sous-parties clairement distinctes, notamment en données de sources téléphoniques et d'émissions radio. Dans ce cas-ci, nos recherches détaillées montrent un certain avantage à traiter les deux types de données par deux systèmes spécifiques et d'élire comme score de sortie celui du système qui correspond à la catégorie source du segment testé. Afin de sélectionner le score de l'un des systèmes, nous avons besoin d'un analyses détecteur de canal source. Nous proposons ici différents nouveaux designs pour engendrées de tels détecteurs automatiques. Dans ce cadre, nous montrons que les facteurs de variabilité (du sous-espace) de la JFA peuvent être utilisés avec succès pour la détection de la source. Ceci ouvre la perspective intéressante de subdiviser les5données en catégories de canal source qui sont établies de manière automatique. L'approche JFA permet une réduction de la mesure de coûts allant jusqu'à généraux 72% relatives, comparé au système GMM-UBM de base. En utilisant des systèmes spécifiques à la source, suivis d'un sélecteur de scores, nous obtenons une amélioration relative de 81%.
Depuis l'explosion du Web, la Recherche d'Information (RI) s'est vue étendue et les moteurs de recherche sur le Web ont vu le jour. Les méthodes classiques de la RI, surtout destinées à des recherches textuelles simples, se sont retrouvées face à des documents de différents formats et des contenus riches. L'utilisateur, en réponse à cette avancée, est devenu plus exigeant quant aux résultats retournés par les systèmes de RI. La personnalisation tente de répondre à ces exigences en ayant pour objectif principal l'amélioration des résultats retournés à l'utilisateur en fonction de sa perception et de ses intérêts ainsi que de ses préférences. Le présent travail de thèse se situe à la croisée des différents aspects présentés et couvre cette problématique. Elle a pour objectif principal de proposer des solutions nouvelles et efficaces à cette problématique. Pour atteindre cet objectif, un système de personnalisation de la recherche spatiale et sémantique sur le Web et intégrant la modélisation de l'utilisateur, a été proposé. Ce système comprend deux volets : 1/ la modélisation de l'utilisateur ; 2/ la collaboration implicite des utilisateurs à travers la construction d'un réseau de modèles utilisateurs, construit itérativement lors des différentes recherches effectuées en ligne. Ainsi, nous avons effectué un ensemble d'évaluation, dont les principales sont : a) l'évaluation de la qualité du modèle de l'utilisateur ; b) l'évaluation de l'efficacité de la recherche d'information ; c) l'évaluation de l'efficacité de la recherche d'information intégrant les informations spatiales ; d) l'évaluation de la recherche exploitant le réseau d'utilisateurs. Les expérimentations menées montrent une amélioration de la personnalisation des résultats présentés par rapport à ceux obtenus par d'autres moteurs de recherche.
Des connaissances sémantiques sont obligatoires pour le Traitement Automatique des Langues. Malheureusement, les classifications à visée universelle sont une utopie. Il existe des systèmes d'extraction de connaissances sémantiques des textes de spécialité par des approches terminologiques mais il est largement reconnu qu'il n'est pas possible d'effectuer une telle extraction de textes de la langue dite " générale ". Cette thèse a pour but de montrer que cette idée est fausse. Nous montrons qu'une analyse thématique de textes non spécialisés (journaux, dépêches de presse en texte intégral ou pages HTML moissonnées sur le Web) permet la plupart du temps de se ramener dans le cadre d'un problème classique de traitement de corpus spécialisé, tout en nécessitant des interventions humaines très réduites. Dans notre approche, le thème des segments de textes est détecté par l'analyse statistique des distributions des mots. Après avoir défini des notions de similarité et d'agrégation, les mots des segments similaires sont agrégés pour former des domaines thématiques dans lesquels les mots de poids élevés décrivent un thème. On regroupe les noms qui apparaissent comme argument d'un même verbe dans les divers segments de texte appartenant à un certain thème, ce qui forme des classes. SVETLAN', qui a été testé sur des corpus de plusieurs millions de mots en français et en anglais. L'analyse empirique des résultats montre que, comme prévu, les mots sont très souvent en relation sémantique forte les uns avec les autres dans les classes obtenues, et ce dans le contexte déterminé par le thème.
Ce projet de thèse porte sur la modélisation des variations acoustico-prosodique des unités informationnelles courtes de la parole spontanée pour permettre leur classification en grandes catégories sémantiques reliées à leur utilisation pragmatique. Le travail se fonde sur la Théorie du langage en action (Language into Act Theory, désormais L-AcT, Cresti 2000 ; Moneglia and Raso, 2014 ; Cavalcante 2016, 2020), pour laquelle les fonctions informationnelles, y compris illocutoires, des interactions parlées spontanées sont essentiellement orientées par la prosodie. La parole est segmentée en unités intonatives encapsulant un certain nombre de mots dans la même enveloppe prosodique. Cela établit un contraste entre les mots des différentes enveloppes prosodiques : la segmentation guide ainsi un découpage fonctionnel du discours (Barth-Weingarten, 2016 ; Barbosa and Raso, 2018 ; Izre'el et al., à paraître-a ; Izre'el et al., à paraître-b). La L-AcT propose qu'il existe une relation isomorphe entre l'unité intonative et l'unité informationnelle : chaque unité intonative composant l'énoncé acquiert une valeur informationnelle, à l'exception des unités de scansion, produites volontairement (pour relever un point) ou involontairement (problèmes de performance). La compositionnalité syntaxique est une propriété des unités informationnelles, qui établissent des relations fonctionnelles guidées par les contours intonatifs, sans que la compositionnalité syntaxique y joue un rôle nécessaire (Cresti 2014). Dans le flux discursif, on distingue les frontières prosodiques à valeur terminale ou non-terminale, qui peuvent être détectées automatiquement de bons niveaux de rappel et de précision, sur la base de leurs corrélats prosodiques (Teixeira, 2018 ; Teixeira, Barbosa and Raso, 2018 ; Raso, Teixeira and Barbosa, à paraître). Les frontières terminales marquent la fin d'une séquence composée d'unités intonatives non-terminales, marquées par les frontières non-terminales et délimitant les unités informationnelles. Cette thèse traite de l'analyse des unités informationnelles courtes, c'est-à-dire celles réalisées sur un seul mot phonologique et encapsulées dans une unité intonative (précédées ou suivies d'au moins une frontière prosodique non-terminale). Les unités courtes peuvent théoriquement avoir pour fonction toutes les valeurs informationnelles, couvrant tous les marqueurs discursifs ainsi que la plupart des unités textuelles : ceci permettra l'observation d'une large gamme de fonctions informationnelles. En appliquant le processus de classification à ces seules unités courtes, on évite les variations prosodiques dues à d'autres niveaux linguistiques (hiérarchisation, etc.). De plus, les mots et expressions apparaissant sur les unités courtes sont souvent fréquents dans les corpus oraux, ce qui permet de baser les analyses sur de plus grandes quantités de données. Les données seront extraites des corpus C-ORAL en portugais brésilien, italien et anglais américain (Cresti and Moneglia, 2005 ; Raso and Mello, 2012 ; Cavalcante and Ramos, 2016), avec une possible application au français (le corpus existe mais doit être resegmenté et informationnellement annoté). Ces corpus sont segmentés prosodiquement et informationnellement, conformément aux prémisses de la L-AcT. Des méthodes de TAL ont déjà été appliquées pour l'analyse de différentes unités informationnelles, notamment dans ce cadre théorique (Raso and Vieira, 2016 ; Moneglia and Cresti, 2018 ; Gobbo, 2019 ; Cavalcante, 2020), et ont démontré la faisabilité de cette tâche. Il s'agira de caractériser les différentes structures prosodiques observées sur les unités cibles, dans la cadre d'une analyse acoustico-prosodique qui aura pour but une discrétisation des formes et une estimation des meilleurs moyens de représentation de celle-ci (les niveaux phonétique et phonologique proposant des formats de descriptions variables). Sur la base de ces descriptions d'unités prosodiques, un processus d'apprentissage devra modéliser les liens entre formes prosodiques et fonctions linguistiques des unités cibles. Ces deux processus auront pour but de tester le rôle joué par la prosodie dans l'attribution du signifié informationnel en parole spontanée, et aussi la similarité inter-linguistique des formes signifiantes, si leur usage leur distribution et leur fréquence peuvent varier d'une langue à l'autre.
Dans la première partie de cette thèse, nous présentons une nouvelle méthode de production de variantes de prononciations qui adapte des prononciations standards, c'est-à-dire issues d'un dictionnaire, à un style spontané. Cette méthode utilise une vaste gamme d'informations linguistiques, articulatoires et acoustiques, ainsi qu'un cadre probabiliste d'apprentissage automatique, à savoir les champs aléatoires conditionnels (CAC) et les modèles de langage. Nos expériences poussées sur le corpus Buckeye démontrent l'efficacité de l'approche à travers des évaluations objectives et perceptives. Des tests d'écoutes sur de la parole synthétisée montrent que les prononciations adaptées sont jugées plus spontanées que les prononciations standards, et même que celle réalisées par les locuteurs du corpus étudié. La seconde partie de la thèse explore une nouvelle approche de production automatique de disfluences dans les énoncés en entrée d'un système de synthèse de la parole. L'approche proposée offre l'avantage de considérer plusieurs types de disfluences, à savoir des pauses, des répétitions et des révisions. Pour cela, nous présentons une formalisation novatrice du processus de production de disfluences à travers un mécanisme de composition de ces disfluences. Nous présentons une première implémentation de notre processus, elle aussi fondée sur des CAC et des modèles de langage, puis conduisons des évaluations objectives et perceptives. Celles-ci nous permettent de conclure à la bonne fonctionnalité de notre proposition et d'en discuter les pistes principales d'amélioration.
La personnalisation est une tendance envisagée par les musées pour faire face à la diversité des visiteurs et de leurs pratiques de visite. Afin de favoriser la mise en œuvre de visites personnalisées, nous questionnons l'apport des interactions tangibles, non seulement pour les visiteurs, mais également pour les professionnels du musée. Comment aider les médiateurs culturels à concevoir des parcours personnalisés prenant en compte la diversité des profils de visiteurs ? Comment aider les visiteurs à choisir et suivre le parcours qui correspond à leurs envies et besoins ? Nous avons appliqué un processus de conception centrée utilisateurs avec des musées partenaires afin de concevoir, implémenter et évaluer des outils tangibles permettant de répondre à ces questions. L'analyse des besoins des utilisateurs (médiateurs culturels et visiteurs) nous a permis de définir six caractéristiques principales à prendre en compte pour la personnalisation des visites ainsi que de faire émerger le concept de personnalisation multicritères. Nous proposons un concept d'interface permettant d'associer le choix des caractéristiques visiteur constituant un profil et le suivi dynamique de l'avancement de la création des parcours personnalisés pour chaque combinaison pour les médiateurs. Ce concept a été instancié selon deux modalités d'interaction tangible et tactile, que nous avons comparées à travers une étude expérimentale auprès de médiateurs de musée. Nous décrivons également la conception itérative de prototypes d'aide au choix de parcours personnalisés pour les visiteurs, ainsi que l'étude pilote menée dans un musée partenaire. Les prototypes conçus pendant cette thèse mettent en œuvre le paradigme d'interaction token+constraint. Nous proposons une analyse systématique de la littérature référençant le paradigme d'interaction token+constraint, ainsi qu'une grille heuristique de 24 propriétés réparties en cinq catégories reprenant, synthétisant et illustrant les concepts de l'article séminal, que nous appliquons aux prototypes conçus durant la thèse.
Leur réutilisation peut prendre différentes formes (évolution, alignement, fusion, etc.), et présente plusieurs verrous scientifiques. L'un des plus importants est la préservation de la consistance de l'ontologie lors de son changement. Afin d'y répondre, nous nous intéressons dans cette thèse à étudier les changements ontologiques et proposons un cadre formel capable de faire évoluer et de fusionner des ontologies sans affecter leur consistance. Premièrement, nous proposons TGGOnto (Typed Graph Grammars for Ontologies), un nouveau formalisme permettant la représentation des ontologies et leurs changements par les grammaires de graphes typés. Deuxièmement, nous proposons EvOGG (Evolving Ontologies with Graph Grammars), une approche d'évolution d'ontologies qui se base sur le formalisme GGTOnto et traite les inconsistances d'une manière a priori. Nous nous intéressons aux ontologies OWL et nous traitons à la fois : (1) l'enrichissement d'ontologies en étudiant leur niveau structurel et (2) le peuplement d'ontologies en étudiant les changements qui affectent les individus et leurs assertions. L'approche proposée se décompose en trois étapes : (1) la recherche de similarité entre concepts en se basant sur des techniques syntaxiques, structurelles et sémantiques ; (2) la fusion d'ontologies par l'approche algébrique SPO ; (3) l'adaptation de l'ontologie globale résultante par le biais des règles de réécriture de graphes. Afin de valider les travaux menés dans cette thèse, nous avons développé plusieurs outils open source basés sur l'outil AGG (Attributed Graph Grammar). Ces outils ont été appliqués sur un ensemble d'ontologies, essentiellement sur celles développées dans le cadre du projet européen CCAlps (Creatives Companies in Alpine Space) qui a financé les travaux de cette thèse.
Cette thèse fait partie du projet RAPSODIE dont l'objectif est de proposer une reconnaissance vocale spécialisée sur les besoins des personnes sourdes et malentendantes. Deux axes sont étudiées : la modélisation lexicale et l'extraction d'informations para Cette détection a comme but de signaler aux personnes sourdes ou malentendantes quand une question leur est adressée
Les travaux de recherche de la thèse portent sur l'étude et la formalisation des interactions émotionnelles Humain-Machine. Au delà d'une détection d'informations paralinguistiques (émotions, disfluences,...) ponctuelles, il s'agit de fournir au système un profil interactionnel et émotionnel de l'utilisateur dynamique, enrichi pendant l'interaction. Ce profil permet d'adapter les stratégies de réponses de la machine au locuteur, et il peut également servir pour mieux gérer des relations à long terme. Le profil est fondé sur une représentation multi-niveau du traitement des indices émotionnels et interactionnels extraits à partir de l'audio via les outils de détection des émotions du LIMSI. Ainsi, des indices bas niveau (variations de la F0, d'énergie, etc.), fournissent des informations sur le type d'émotion exprimée, la force de l'émotion, le degré de loquacité, etc. Ces éléments à moyen niveau sont exploités dans le système afin de déterminer, au fil des interactions, le profil émotionnel et interactionnel de l'utilisateur. Ce profil est composé de six dimensions : optimisme, extraversion, stabilité émotionnelle, confiance en soi, affinité et domination (basé sur le modèle de personnalité OCEAN et les théories de l'interpersonal circumplex). Le comportement social du système est adapté en fonction de ce profil, de l'état de la tâche en cours, et du comportement courant du robot. Les règles de création et de mise à jour du profil émotionnel et interactionnel, ainsi que de sélection automatique du comportement du robot, ont été implémentées en logique floue à l'aide du moteur de décision développé par un partenaire du projet ROMEO. L'implémentation du système a été réalisée sur le robot NAO. Ces systèmes ont permis de recueillir des données en contrôlant plusieurs paramètres d'élicitation des émotions au sein d'une interaction ; nous présentons les résultats de ces expérimentations, et des protocoles d'évaluation de l'Interaction Humain-Robot via l'utilisation de systèmes à différents degrés d'autonomie.
Grâce aux avancées récentes de l'intelligence artificielle et du Traitement Automatique de la Langue Naturelle, l'objectif du projet pluridisciplinaire IA4Allergie, est de convertir les grandes masses de données textuelles contenues dans les entrepôts de données médicales pour en extraire et déduire de nouvelles connaissances dans l'objectif d'adapter la prise en charge des patients souffrant d'allergiques respiratoires.
Dans cette thèse nous étudions le rôle de la motivation intrinsèque dans l'émergence et le développement des systèmes communicationnels. Notre objectif est d'explorer comment des populations d'agents artificiels peuvent utiliser un système de motivation computationnel particulier, appelé l'autotelic principle, pour réguler leur développement linguistique et les dynamiques qui en résultent au niveau de la population. Nous proposons d'abord une mise en œuvre concrète de l'autotelic principle. La relation entre les deux éléments n'est pas stable mais se déstabilise régulièrement lorsque de nouvelles compétences sont acquises, ce qui permet au système de tenter des défis de plus grande complexité. Ensuite, nous testons l'utilité de ce système de motivation dans une série d'expériences sur l'évolution du langage. Dans le premier ensemble d'expériences, une population d'agents artificiels doit développer une langue pour se référer à des objets ayant des caractéristiques discrètes. Ces expériences se concentrent sur la façon dont les systèmes communicatifs non ambigus peuvent émerger lorsque l'autotelic principle est utilisé pour réguler le développement du langage en étapes de difficulté croissante. Dans le deuxième ensemble d'expériences, les agents doivent créer un langage artificiel pour communiquer sur des couleurs. Dans cette partie, on explore comment le système de motivation peut contrôler la complexité linguistique des interactions pour un domaine continu et on examine aussi la validité de l'autotelic principle en tant que mécanisme permettant de réguler simultanément plusieurs stratégies linguistiques de difficulté similaire. En résumé, nous avons démontré à travers de notre travail que l'autotelic principle peut être utilisé comme un mécanisme général pour réguler la complexité du langage développé de manière autonome en domaines discrets et continus.
La gastronomie et l'onomastique n'ont encore jamais fait l'objet d'étude, de manière conjointe, d'un point de vue linguistique. L'objectif de cette thèse est de proposer un point de départ à une réflexion sur la structure, la nature et la place du nom propre dans la gastronomie française. Pour ce faire, après avoir exposé les différentes problématiques liées à la définition des concepts de gastronomie et de nom propre, nous avons réalisé une synthèse des principaux éléments théoriques qui servent de base à la recherche sur les noms de plats, tant sous un angle linguistique, historique qu'artistique ou législatif. Sera d'abord traitée l'évolution du nom propre dans la gastronomie à travers le prisme de la norme dictionnairique au cours des 70 dernières années, au moyen d'une comparaison lexicographique des deux éditions du Larousse Gastronomique (1938 et 2007). Pour ce faire, une catégorisation des noms propres relevés dans le corpus a été établie. Dans un deuxième temps, la place des noms propres dans un corpus témoignant de l'usage a été examinée, en rassemblant des menus de restaurants parisiens ainsi que des dépliants publicitaires de la restauration livrée.
Le travail effectué au cours de ma thèse s'inscrit dans le cadre du Web Sémantique pour rendre l'annotation sémantique. La vision du Web Sémantique a pour son objectif d'avoir les informations disponibles pour que les utilisateurs puissent les exploiter selon leurs besoins. Pour cela, les données doivent être étiquetées sémantiquement. En plus, comparé aux langues flexionnelles comme le Français, la technologie dans le traitement de langue agglutinative comme le Coréen a toujours des manques à cause de la complexité des morphologies et syntaxe.
Cette thèse s'intéresse à la modélisation de la syntaxe et de l'interface syntaxe-sémantique de la phrase, et explore la possibilité de contrôler au niveau des structures de dérivation la surgénération que produit le traitement des dépendances à distance par des types d'ordre supérieur. À cet effet, nous étudions la possibilité d'étendre le système de typage des Grammaires Catégorielles Abstraites avec les constructions de la somme disjointe, du produit cartésien et du produit dépendant, permettant d'étiqueter les catégories syntaxiques par des structures de traits. Nous prouvons dans un premier temps que le calcul résultant de cette extension bénéficie des propriétés de confluence et de normalisation, permettant d'identifier les termes beta-équivalents dans le formalisme grammatical. Nous réduisons de plus le même problème pour la beta-eta-équivalence à un ensemble d'hypothèse de départ. Dans un second temps, nous montrons comment cette introduction de structures de traits peut être appliquée au contrôle des dépendances à distances, à travers les exemples des contraintes de cas, des îlots d'extraction pour les mouvements explicites et implicites, et des extractions interrogatives multiples, et nous discutons de la pertinence de placer ces contrôles sur les structures de dérivation
Cette thèse porte sur l'identification des expressions polylexicales, abordée au moyen d'une analyse par transitions. Une expression polylexicale (EP) est une construction linguistique composée de plusieurs éléments dont la combinaison montre une irrégularité à un ou plusieurs niveaux linguistiques. La tâche d'identification d'EPs consiste à annoter en contexte les occurrences d'EPs dans des textes, i.e à détecter les ensembles de tokens formant de telles occurrences. L'analyse par transitions est une approche célèbre qui construit une sortie structurée à partir d'une séquence d'éléments, en appliquant une séquence de « transitions » choisies parmi un ensemble prédéfini, pour construire incrémentalement la sortie. Dans cette thèse, nous proposons un système par transitions dédié à l'identification des EPs au sein de phrases représentées comme des séquences de tokens, et étudions diverses architectures pour le classifieur qui sélectionne les transitions à appliquer, permettant de construire l'analyse de la phrase. La première variante de notre système utilise un classifieur linéaire de type machine à vecteur support. Les variantes suivantes utilisent des modèles neuronaux : un simple perceptron multicouche, puis des variantes intégrant une ou plusieurs couches récurrentes. Le scénario privilégié est une identification d'EPs n'utilisant pas d'informations syntaxiques, alors même que l'on sait les deux tâches liées. Nous étudions ensuite une approche par apprentissage multitâche, réalisant conjointement l'étiquetage morphosyntaxique, l'identification des EPs par transitions et l'analyse syntaxique en dépendances par transitions. La thèse comporte une partie expérimentale importante. Nous avons d'une part étudié quelles techniques de ré-échantillonnage des données permettent une bonne stabilité de l'apprentissage malgré des initialisations aléatoires. D'autre part, nous avons proposé une méthode de réglage des hyperparamètres de nos modèles par analyse de tendances au sein d'une recherche aléatoire de combinaison d'hyperparamètres. Nos variantes produisent de très bons résultats, et notamment les scores d'état de l'art pour de nombreuses langues de PARSEME. L'une des variantes s'est classée première pour la plupart des langues de PARSEME 1.0. Pourtant, nos modèles ont des performances faibles sur les EPs non vues à l'apprentissage.
Le véhicule autonome est un véhicule qui se conduira, à terme, sans aucune intervention du conducteur, quelle que soit la situation de conduite. Ce véhicule comprend une nouvelle fonction, nommée fonction AD, pour Autonomous Driving, en charge de la conduite autonome. Cette fonction peut se trouver dans des états différents (Active, Disponible par exemple) selon l'évolution des conditions environnementales. Le changement de ses états est géré par une fonction de Supervision, nommée Supervision AD. Le principal objet de ces travaux consiste à garantir que la fonction AD se trouve constamment dans un état sûr. Ceci revient à s'assurer que la Supervision AD respecte l'ensemble des exigences fonctionnelles et de sûreté qui spécifient son comportement. Ces deux disciplines d'ingénierie, bien qu'elles contribuent à la conception d'une même fonction, se distinguent en de nombreux points : objectifs, contraintes, planning, outils... Dans notre cas d'étude, ces différences s'illustrent par les exigences considérées : les exigences fonctionnelles sont allouées à la fonction AD globale, tandis que les exigences de sûreté spécifient le comportement de sous-fonctions locales redondantes assurant une continuité de service en cas de défaillance. La mise en cohérence de ces deux perspectives métier au plus tôt dans le cycle de conception et dans un contexte industriel, est la problématique centrale traitée. Les enjeux de SdF soulevés par le véhicule autonome rendent ce problème primordial pour les constructeurs automobiles. Afin de répondre à ces préoccupations, nous avons proposé une démarche outillée et collaborative de conception sûre de la Supervision AD. Cette démarche est intégrée dans les processus normatifs en vigueur (normes ISO 15288 et ISO 26262) ainsi que dans les processus de conception internes chez Renault. Elle est fondée sur la vérification formelle par model checking, la composition parallèle d'automates finis et l'expertise métier. Cette démarche prône l'utilisation d'un même formalisme (l'automate à états finis) par les deux métiers pour mener à bien des activités partageant un objectif de modélisation commun : la vérification d'exigences de comportement en phase amont de conception. Une méthode pour traduire les exigences en propriétés formelles et construire les modèles d'état a été déployée. Il en résulte une consolidation progressive des exigences traitées, initialement rédigées en langage naturel. Les potentielles ambigüités, incohérences et incomplétudes sont exhibées et traitées.
Les approches de gestion des processus métier font aujourd'hui partie intégrante de la vie et de la recherche de performance des entreprises. Néanmoins, ce domaine demeure historiquement cloisonné en particulier vis-à-vis du rôle du système d'information dans l'entreprise. Les résultats de ces travaux ont été appliqués à un cas industriel réel ce qui a permis de démontrer la pertinence et la nécessité d'une convergence entre un système d'information et la démarche processus d'une entreprise.
La lexicographie contemporaine met à disposition des ressources offrant de multiples possibilités d'exploitation automatique. Elle débute par une revue de la formalisation et de l'informatisation de l'analogie pour l'étude du lexique, qui définit les principes de l'exploration : les sommets sont des objets disposant d'Attributs, les arcs représentent des Relations. Deux séries d'expériences viennent ensuite. La première montre que la formalisation de la ressource permet de détecter des analogies conformes à l'intuition, que différents types d'exploration sont possibles et que l'approche permet de vérifier la cohérence du réseau et de faire émerger des règles lexicales. La seconde série porte sur la notion de configurations de dérivations lexicales. Elle montre que le regroupement de sous-graphes analogues fait émerger des connexions récurrentes. L'état d'avancement du réseau ne permet pas d'obtenir des règles et des modèles aboutis, mais les résultats sont encourageants. Elle permet d'identifier des phénomènes linguistiques et d'instrumenter l'activité lexicologique.
Cette thèse explique et présente notre démarche de la réalisation d'un système à base de règles de reconnaissance et de classification automatique des EN en arabe. C'est un travail qui implique deux disciplines : la linguistique et l'informatique. L'outil informatique et les règles la linguistiques s'accouplent pour donner naissance à une nouvelle discipline ; celle de « traitement automatique des langues » , qui opère sur des niveaux différents (morphosyntaxique, syntaxique, sémantique, syntactico-sémantique etc.). Ce travail de thèse s'inscrit donc dans un cadre général de traitement automatique des langues, mais plus particulièrement dans la continuité des travaux réalisés au niveau de l'analyse morphosyntaxique par la conception et la réalisation des bases des données lexicales SAMIA et ensuite DIINAR avec l'ensemble de résultats de recherches qui en découlent. Pour comprendre de quoi il s'agit, il nous était important de commencer par la définition de l'entité nommée. Et pour mener à bien notre démarche, nous avons distingué entre deux types principaux : pur nom propre et EN descriptive. Nous avons aussi établi une classification référentielle en se basant sur diverses classes et sous-classes qui constituent la référence de nos annotations sémantiques. Cependant, nous avons dû faire face à deux difficultés majeures : l'ambiguïté lexicale et les frontières des entités nommées complexes. Notre système adopte une approche à base de règles syntactico-sémantiques. Il est constitué, après le Niveau 0 d'analyse morphosyntaxique, de cinq niveaux de construction de patrons syntaxiques et syntactico-sémantiques basés sur les informations linguistique nécessaires (morphosyntaxiques, syntaxiques, sémantique, et syntactico-sémantique). Ce travail, après évaluation en utilisant deux corpus, a abouti à de très bons résultats en précision, en rappel et en F–mesure. Les résultats de notre système ont un apport intéressant dans différents application du traitement automatique des langues notamment les deux tâches de recherche et d'extraction d'informations. En effet, on les a concrètement exploités dans les deux applications (recherche et extraction d'informations). En plus de cette expérience unique, nous envisageons par la suite étendre notre système à l'extraction et la classification des phrases dans lesquelles, les entités classifiées, principalement les entités nommées et les verbes, jouent respectivement le rôle d'arguments et de prédicats. Un deuxième objectif consiste à l'enrichissement des différents types de ressources lexicales à l'instar des ontologies.
La réutilisation des données de soins pour la recherche s'est largement répandue avec le développement d'entrepôts de données cliniques. Ces entrepôts de données sont modélisés pour intégrer et explorer des données structurées liées à des thesaurus. Ces données proviennent principalement d'automates (biologie, génétique, cardiologie, etc) mais aussi de formulaires de données structurées saisies manuellement. La production de soins est aussi largement pourvoyeuse de données textuelles provenant des comptes rendus hospitaliers (hospitalisation, opératoire, imagerie, anatomopathologie etc.), des zones de texte libre dans les formulaires électroniques. Cette masse de données, peu ou pas utilisée par les entrepôts classiques, est une source d'information indispensable dans le contexte des maladies rares. En effet, le texte libre permet de décrire le tableau clinique d'un patient avec davantage de précisions et en exprimant l'absence de signes et l'incertitude. Particulièrement pour les patients encore non diagnostiqués, le médecin décrit l'histoire médicale du patient en dehors de tout cadre nosologique. Cette richesse d'information fait du texte clinique une source précieuse pour la recherche translationnelle. Cela nécessite toutefois des algorithmes et des outils adaptés pour en permettre une réutilisation optimisée par les médecins et les chercheurs. Nous présentons dans cette thèse l'entrepôt de données centré sur le document clinique, que nous avons modélisé, implémenté et évalué. À travers trois cas d'usage pour la recherche translationnelle dans le contexte des maladies rares, nous avons tenté d'adresser les problématiques inhérentes aux données textuelles : (i) le recrutement de patients à travers un moteur de recherche adapté aux données textuelles (traitement de la négation et des antécédents familiaux), (ii) le phénotypage automatisé à partir des données textuelles et (iii) l'aide au diagnostic par similarité entre patients basés sur le phénotypage. Ces méthodes et algorithmes ont été intégrés dans le logiciel Dr Warehouse développé pendant la thèse et diffusé en Open source depuis septembre 2017.
Les réseaux sociaux ont transformé le Web d'un mode lecture, où les utilisateurs pouvaient seulement consommer les informations, à un mode interactif leur permettant de les créer, partager et commenter. Un défi majeur du traitement d'information dans les médias sociaux est lié à la taille réduite des contenus, leur nature informelle et le manque d'informations contextuelles. D'un autre côté, le web contient des bases de connaissances structurées à partir de concepts d'ontologies, utilisables pour enrichir ces contenus. Cette thèse explore le potentiel d'utiliser les bases de connaissances du Web de données, afin de détecter, classifier et suivre des événements dans les médias sociaux, particulièrement Twitter. On a abordé 3 questions de recherche : i) Comment extraire et classifier les messages qui rapportent des événements ? ii) Comment identifier des événements précis ? iii) Étant donné un événement, comment construire un fil d'actualité représentant les différents sous-événements ? Les travaux de la thèse ont contribué à élaborer des méthodes pour la généralisation des entités nommées par des concepts d'ontologies pour mitiger le sur-apprentissage dans les modèles supervisés ; une adaptation de la théorie des graphes pour modéliser les relations entre les entités et les autres termes et ainsi caractériser des événements pertinents ;  l'utilisation des ontologies de domaines et les bases de connaissances dédiées, pour modéliser les relations entre les caractéristiques et les acteurs des événements. Nous démontrons que l'enrichissement sémantique des entités par des informations du Web de données améliore la performance des modèles d'apprentissages supervisés et non supervisés.
Pour obtenir des hauts taux de détection, les CNNs requièrent d'un grand nombre de paramètres à stocker, et en fonction de l'application, aussi un grand nombre d'opérations. Cela complique gravement le déploiement de ce type de solutions dans les systèmes embarqués. Ce manuscrit propose plusieurs solutions à ce problème en visant une coadaptation entre l'algorithme, l'application et le matériel. Dans ce manuscrit, les principaux leviers permettant de fixer la complexité computationnelle d'un détecteur d'objets basé sur les CNNs sont identifiés et étudies. Cela devient très coûteux lorsque des petits objets doivent être trouvés dans des images en haute résolution. Pour rendre la solution efficiente et ajustable, le processus est divisé en deux étapes. Un premier CNN s'especialise à trouver des régions d'intérêt de manière efficiente, ce qui permet d'obtenir des compromis flexibles entre le taux de détection et le nombre d'opérations. De plus, les CNN exhibent plusieurs propriétés qui confirment leur surdimensionnement. Ce surdimensionnement est une des raisons du succès des CNN, puisque cela facilite le processus d'optimisation en permettant un ample nombre de solutions équivalentes. Cependant, cela complique leur implémentation dans des systèmes avec fortes contraintes computationnelles. Dans ce sens, une méthode de compression de CNN basé sur une Analyse en Composantes Principales (ACP) est proposé. L'ACP permet de trouver, pour chaque couche du réseau, une nouvelle représentation de l'ensemble de filtres appris par le réseau en les exprimant à travers d'une base ACP plus adéquate. Cette base ACP est hiérarchique, ce qui veut dire que les termes de la base sont ordonnés par importance, et en supprimant les termes moins importants, il est possible de trouver des compromis optimales entre l'erreur d'approximation et le nombre de paramètres. À travers de cette méthode il es possible d'obtenir, par exemple, une réduction x2 sur le nombre de paramètres et opérations d'un réseau du type ResNet-32, avec une perte en accuracy &lt ; 2%. Il est aussi démontré que cette méthode est compatible avec d'autres méthodes connues de l'état de l'art, notamment le pruning, winograd et la quantification. Dans le cas de la détection de vissages, la parallélisation du réseau comprimé par ACP sûr 8 processeurs incrémente la vitesse de calcul d'un facteur x11.68 par rapport au réseau original sûr un seul processeur.
Nous nous intéressons, dans cette thèse, à l'étude des modèles de mélange et des modèles linéaires généralisés, avec une application aux données de co-infection entre les arbovirus et les parasites du paludisme. Après une première partie consacrée à l'étude de la co-infection par un modèle logistique multinomial, nous proposons dans une deuxième partie l'étude des mélanges de modèles linéaires généralisés. La méthode proposée pour estimer les paramètres du mélange est une combinaison d'une méthode des moments et d'une méthode spectrale. Nous proposons à la fin une dernière partie consacrée aux mélanges de valeurs extrêmes en présence de censure. La méthode d'estimation proposée dans cette partie se fait en deux étapes basées sur la maximisation d'une vraisemblance.
Avec l'émergence du Web de données, notamment les données ouvertes liées, une abondance de données est devenue disponible sur le web. Cependant, les ensembles de données LOD et leurs sous-graphes inhérents varient fortement par rapport a leur taille, le thème et le domaine, les schémas et leur dynamicité dans le temps au niveau des données. Dans ce contexte, l'identification des jeux de données appropriés, qui répondent a des critères spécifiques, est devenue une tâche majeure, mais difficile a soutenir, surtout pour répondre a des besoins spécifiques tels que la recherche d'entités centriques et la recherche des liens sémantique des données liées. Notamment, en ce qui concerne le problème de liage des données, le besoin d'une méthode efficace pour la recommandation des jeux de données est devenu un défi majeur, surtout avec l'état actuel de la topologie du LOD, dont la concentration des liens est très forte au niveau des graphes populaires multi-domaines tels que DBpedia et YAGO, alors qu'une grande liste d'autre jeux de données considérés comme candidats potentiels pour le liage est encore ignorée. Ce problème est dû a la tradition du web sémantique dans le traitement du problème de " identification des jeux de données candidats pour le liage ". Bien que la compréhension de la nature du contenu d'un jeu de données spécifique est une condition cruciale pour les cas d'usage mentionnées, nous adoptons dans cette thèse la notion de " profil de jeu de données " - un ensemble de caractéristiques représentatives pour un jeu de données spécifique, notamment dans le cadre de la comparaison avec d'autres jeux de données. Notre première direction de recherche était de mettre en œuvre une approche de recommandation basée sur le filtrage collaboratif, qui exploite à la fois les prols thématiques des jeux de données, ainsi que les mesures de connectivité traditionnelles, afin d'obtenir un graphe englobant les jeux de données du LOD et leurs thèmes. Cette approche a besoin d'apprendre le comportement de la connectivité des jeux de données dans le LOD graphe. Cependant, les expérimentations ont montré que la topologie actuelle de ce nuage LOD est loin d'être complète pour être considéré comme des données d'apprentissage. Face aux limites de la topologie actuelle du graphe LOD, notre recherche a conduit a rompre avec cette représentation de profil thématique et notamment du concept "apprendre pour classer" pour adopter une nouvelle approche pour l'identification des jeux de données candidats basée sur le chevauchement des profils intensionnels entre les différents jeux de données. Par profil intensionnel, nous entendons la représentation formelle d'un ensemble d'étiquettes extraites du schéma du jeu de données, et qui peut être potentiellement enrichi par les descriptions textuelles correspondantes. Cette représentation fournit l'information contextuelle qui permet de calculer la similarité entre les différents profils d'une manière efficace. Nous identifions le chevauchement de différentes profils à l'aide d'une mesure de similarité semantico-fréquentielle qui se base sur un classement calcule par le tf*idf et la mesure cosinus. Les expériences, menées sur tous les jeux de données lies disponibles sur le LOD, montrent que notre méthode permet d'obtenir une précision moyenne de 53% pour un rappel de 100%. Afin d'assurer des profils intensionnels de haute qualité, nous introduisons Datavore - un outil oriente vers les concepteurs de métadonnées qui recommande des termes de vocabulaire a réutiliser dans le processus de modélisation des données. Datavore fournit également les métadonnées correspondant aux termes recommandés ainsi que des propositions des triples utilisant ces termes. L'outil repose sur l'écosystème des Vocabulaires Ouverts Lies (LOV) pour l'acquisition des vocabulaires existants et leurs métadonnées.
Les travaux présentés dans ce mémoire s'inscrivent dans le contexte de systèmes d'entreprises collaboratives. En effet, la non-interopérabilité engendre des coûts non négligeables dus principalement au temps et aux ressources mises en place pour développer des interfaces d'échange des informations. Ceci influe sur la performance globale des entreprises et précisément sur les coûts et les délais d'obtention des services attendus. Nous proposons ainsi une approche pour mesurer, a priori, le degré d'interopérabilité (ou de non interopérabilité) entre modèles conceptuels de systèmes d'information d'entreprise, afin de donner à une entreprise la possibilité d'évaluer sa propre capacité à interopérer et donc de prévoir les éventuels problèmes avant la mise en place d'un partenariat. Il s'agit ainsi de définir des indicateurs et des métriques tant quantitatifs que qualitatifs, permettant de qualifier l'interopérabilité entre les systèmes d'entreprises et de proposer des stratégies d'amélioration lorsque le niveau d'interopérabilité est évalué comme insuffisant
Cette thèse a deux objectifs principaux. En premier lieu, on voudrait offrir un aperçu des connaissances académiques actuelles, tant théoriques qu'empiriques, des processus d'accommodation linguistique entre interlocuteurs, au sens général, et des caractéristiques rythmiques de la langue espagnole, en particulier. En second lieu, on présente deux études empiriques conçues pour analyser l'influence de la régularité rythmique au niveau des phrases et de l'arrangement phonologique sur les processus d'accommodation linguistique. Dans l'ensemble, les données rassemblées dans cette thèse indiquent que les phrases avec un rythme régulier, disposées en groupes accentuels, produisent une plus grande ressemblance entre les hispanophones en matière de rythme et de l'étendue de la F0, par rapport aux phrases avec un rythme irrégulier et aux phrases disposées en pieds accentuels. De plus, certains faits connus concernant les femmes ayant une moyenne de F0 supérieure, une étendue de F0 plus large, et un débit de parole plus lent quant aux hommes ont également été observés au cours de la première expérience. En outre, une valeur inférieure de la moyenne de F0 et une étendue de F0 plus étroite ont été observées lors de l'utilisation de phrases avec un rythme régulier et de phrases disposées en groupes accentuels, par rapport aux conditions expérimentales opposées.
Les entreprises, les administrations, et parfois les particuliers, doivent faire face à de nombreuses fraudes sur les documents qu'ils reçoivent de l'extérieur ou qu'ils traitent en interne. Les factures, les notes de frais, les justificatifs... tout document servant de preuve peut être falsifié dans le but de gagner plus d'argent ou de ne pas en perdre. En France, on estime les pertes dues aux fraudes à plusieurs milliards d'euros par an. Étant donné que le flux de documents échangés, numériques ou papiers, est très important, il serait extrêmement coûteux en temps et en argent de les faire tous vérifier par des experts de la détection des fraudes. C'est pourquoi nous proposons dans notre thèse un système de détection automatique des faux documents. Si la plupart des travaux en détection automatique des faux documents se concentrent sur des indices graphiques, nous cherchons quant à nous à vérifier les informations textuelles du document afin de détecter des incohérences ou des invraisemblances. Pour cela, nous avons tout d'abord constitué un corpus de tickets de caisse que nous avons numérisés et dont nous avons extrait le texte. Après avoir corrigé les sorties de l'OCR et fait falsifier une partie des documents, nous en avons extrait les informations et nous les avons modélisées dans une ontologie, afin de garder les liens sémantiques entre elles. Les informations ainsi extraites, et augmentées de leurs possibles désambiguïsations, peuvent être vérifiées les unes par rapport aux autres au sein du document et à travers la base de connaissances constituée. Les liens sémantiques de l'ontologie permettent également de chercher l'information dans d'autres sources de connaissances, et notamment sur Internet.
Nous présentons dans cette thèse deux modèles informatiques développés pour délivrer de l'information structurée et applicables à de grandes bases de données de textes médiévaux. Les deux modèles, l'un appliqué à la reconnaissance des entités nommées, l'autre à la détection des parties du discours diplomatique, ont suivi un apprentissage supervisé utilisant la méthode des Champs aléatoires conditionnelles (CRF) sur un corpus manuellement annoté de actes médiévaux (Corpus Burgundiae Medii Aevi ou CBMA). Notre modèle principal de reconnaissance d'entités nommées a prouvé sa robustesse lorsqu'il a été appliqué sur des échantillons de corpus de taille, chronologie et origine très variés. Le modèle secondaire détectant les parties du discours diplomatique, bien que moins performant, s'est montré valide comme outil de structuration. Ils peuvent à présent être utilisés pour l'indexation et l'étude d'une grande variété de sources diplomatiques, économisant, ainsi des considérables efforts humains. Nous avons développé différentes solutions destinées à trouver un juste équilibre entre la dépendance du modèle à son corpus d'origine et sa capacité à être appliqué à d'autres corpus. De même, différents ajouts et corrections ont été opérés sur le corpus de référence à partir de plusieurs observations de type historique et linguistique concernant les documents utilisés, ce qui a permis d'améliorer la performance initiale. Nous avons ensuite appliqué les outils ainsi générés à la reconnaissance de noms de personnes, de lieux et de parties du discours diplomatique sur des milliers d'actes du CBMA afin d'étudier différentes questions intéressant la science historique et la diplomatique. Ces études concernent la datation semi-automatique d'un cartulaire qui en était dépourvu ; l'évolution du vocabulaire spatial dans les actes du Moyen Âge Central ; et l'indexation des documents à partir des modules les intégrant, notamment les formules du protocole des actes. Par ces études nous poursuivons un double objectif : illustrer différentes stratégies permettant d'abstraire et d'adapter au traitement automatique des données des méthodes de recherche classiques en Histoire ; démontrer que nos outils de traitement massif permettent la génération de connaissances pertinentes pour la science historique.
Les besoins croissants continuels de la médecine et de la santé accentuent la nécessité de systèmes d'alertes médicales bien adaptés. Ces systèmes d'alerte peuvent être utilisés par une variété de patients et d'acteurs médicaux. Ils devraient permettre de contrôler un large éventail de variables médicales. Aux alertes détectées on associées deux indices de qualité. L'indice d'applicabilité qui exprime dans quelle mesure un patient est concerné par l'alerte, et l'indice de confiance qui exprime la fiabilité de l'alerte concernant la fraicheur des données utilisées lors de sa détection. Les indices de qualité associés à une alerte détectée se calculent grâce aux informations liées à une situation d'alerte paramétrée par l'utilisateur. Une situation d'alerte est définie à partir d'une composition de conditions d'activation. Une condition d'activation est construite à partir d'une valeur linguistique exprimant l'état (par exemple température élevée) ou la tendance (par exemple tension systolique à la hausse) d'une entité observable (la température, la tension systolique, etc.). Lorsque la situation d'alerte est évaluée, le système utilise les connaissances préparées préalablement par les utilisateurs concernant les valeurs linguistiques. C'est-à-dire, quelle valeur linguistique représente le mieux une valeur quantitative sous un contexte spécifique. Les alertes détectées pouvant être nombreuses, nous définissons un politique de notification pour ne notifier que les alertes pertinentes dans le but de ne pas fatiguer les utilisateurs. D'abord les alertes sont filtrées à partir des indices de qualité. Parmi les alertes restantes, le système filtre par expressivité : choisir les tendances plus durables et les valeurs linguistiques les plus expressives. Ensuite, dans le cas d'alertes consécutives, le système ne garde que les alertes qui accomplissent les préférences des utilisateurs, comme celles dont l'indice d'applicabilité augmente. L'objectif final est de fidéliser l'utilisateur au système d'alertes en lui fournissant un service de qualité. Il s'approprie le système en même temps qu'il définit les situations d'alerte. Ainsi, il est capable de l'adapter lui-même au contexte afin d'obtenir des alertes de meilleure qualité. L'adaptation est guidée par les indices de qualité utilisés pour réduire les faux-positifs et faux-négatifs ainsi que pour contrôler la sur-notification d'alertes. Nous proposons de nous appuyer sur les systèmes existants en apportant des fonctionnalités de dynamisme et d'évolution, ainsi que des facilités de paramétrage et d'adaptation en temps réel au contexte d'utilisation afin d'exploiter au mieux les observations.
L'extraction d'information de textes médicaux fournit des renseignements très utiles pour identifier les effets indésirables dans la surveillance après consommation (Pharmacovigilance), qui sont plus difficiles à découvrir à travers des études médicales typiques puisque les patients prennent plusieurs traitements en même temps. Récemment, les techniques de Data Mining ont permis de découvrir les connaissances enregistrées dans de grands ensembles de données, comme les dossiers cliniques collectés par les hôpitaux tout au long de la vie du patient. Pour cela, nous devons extraire les relations entre les médicaments et Adverses Ce problème est divisé en tâches de reconnaissance d'entités nommées (NER) et d'extraction de relations. Aujourd'hui, les approches supervisées basées sur des algorithmes de Deep Learning et Machine Learning résolvent ce problème dans l'état de l'art. Les méthodes supervisées ont besoin de caractéristiques riches afin d'apprendre des modèles efficaces au cours de la formation, par conséquent, nous nous concentrons sur la construction de représentations de mots larges (l'entrée du réseau neuronal) La représentation proposée améliore la performance du modèle de référence et le modèle final a atteint les performances des méthodes de pointe. Ensuite, nous avons extrait des informations contextuelles à travers des modèles de Deep Learning, afin d'identifier les réactions indésirables aux médicaments. Le modèle proposé a amélioré la précision globale et l'extraction des réactions indésirables aux médicaments obtenu avec le modèle de base, ce qui indique l'efficacité de combiner des modèles de Deep Learning et une vaste ingénierie des caractéristiques.
Cette thèse s'intéresse au Résumé Automatique de texte et plus particulièrement au résumé mis-à-jour. Cette problématique de recherche vise à produire un résumé différentiel d'un ensemble de nouveaux documents par rapport à un ensemble de documents supposés connus. Elle intègre ainsi dans la problématique du résumé à la fois la question de la dimension temporelle de l'information et celle de l'historique de l'utilisateur. Dans ce contexte, le travail présenté s'inscrit dans les approches par extraction fondées sur une optimisation linéaire en nombres entiers (ILP) et s'articule autour de deux axes principaux : la détection de la redondance des informations sélectionnées et la maximisation de leur saillance. Pour le premier axe, nous nous sommes plus particulièrement intéressés à l'exploitation des similarités inter-phrastiques pour détecter, par la définition d'une méthode de regroupement sémantique de phrases, les redondances entre les informations des nouveaux document set celles présentes dans les documents déjà connus. Concernant notre second axe, nous avons étudié l'impact de la prise en compte de la structure discursive des documents, dans le cadre de la Théorie de la Structure Rhétorique (RS), pour favoriser la sélection des informations considérées comme les plus importantes. L'intérêt des méthodes ainsi définies a été démontré dans le cadre d'évaluations menées sur les données des campagnes TAC et DUC. Enfin, l'intégration de ces critères sémantique et discursif au travers d'un mécanisme de fusion tardive a permis de montrer dans le même cadre la complémentarité de ces deux axes et le bénéfice de leur combinaison.
Depuis l'expansion du web, de nombreuses applications cherchent à répondre aux besoins d'utilisateurs ou de machines aux origines culturelles variées. De ce contexte de diversité culturelle émerge de nombreux conflits liés à des conceptions du monde différentes. Proposer des services adaptés requiert l'intégration au sein du système d'une forme de conscience culturelle. Une conscience culturelle artificielle est composée de représentations et de médiations culturelles formelles offrant au système les moyens pour interpréter les cultures représentées et déterminer leurs différences. Ces modèles grossiers, bien qu'ils soient adaptés, limitent la compréhension possible des cultures représentées. Par conséquent ils constituent un goulot d'étranglement dans le développement des systèmes culturellement conscients. J'étudie la construction, la formalisation et la médiation des représentations culturelles émiques. Mes contributions principales sont la conception et la validation, d'une part, d'un nouveau processus ethnographique semi-automatique de construction de modèles émiques via la fouille de textes et, d'autre part, d'une conscience culturelle artificielle émique fondée sur l'alignement d'ontologies culturelles issues de ces modèles.
Le premier problème qu'on considère est de détecter un sous graphe Erdős–Rényi G(m,p) plante dans un graphe Erdős–Rényi G(n,q). Nous dérivons les distributions d'une statistique basée sur les propriétés spectrales d'une matrice définie du graphe. Pour cela nous utilisons l'algorithme « Belief Propagation » . Le BP sans informations supplémentaires ne réussit à la récupération qu'avec un SNR effectif lambda au-delà d'un seuil. Nous prouvons qu'en présence des informations supplémentaires, ce seuil disparaît et le BP réussi pour n'importe quel lambda. Finalement, nous dérivons des expressions asymptotiques pour PageRank sur une classe de graphes aléatoires non dirigés appelés « fast expanders » , en utilisant des techniques théoriques à la matrice aléatoire. Nous montrons que PageRank peut être approché pour les grandes tailles du graphe comme une combinaison convexe du vecteur de dégré normalisé et le vecteur de personnalisation du PageRank, lorsque le vecteur de personnalisation est suffisamment délocalisé. Par la suite, nous caractérisons les formes asymptotiques de PageRank sur le Stochastic Block Model (SBM) et montrons qu'il contient un terme de correction qui est fonction de la structure de la communauté.
L'apprentissage de modèles stochastiques générant des séquences a de nombreuses applications en traitement de la parole, du langage ou bien encore en bio-informatique. Les algorithmes traditionnels d'apprentissage comme celui de Baum-Welch sont itératifs, lent et peuvent converger vers des optima locaux. Une alternative récente consiste à utiliser la méthode des moments (MoM) pour concevoir des algorithmes rapides et consistent avec des garanties pseudo-PAC. Cependant, les algorithmes basés sur la MoM ont deux inconvénients principaux. Tout d'abord, les garanties PAC ne sont valides que si la dimension du modèle appris correspond à la dimension du modèle cible. Deuxièmement, bien que les algorithmes basés sur la MoM apprennent une fonction proche de la distribution cible, la plupart ne contraignent pas celle-ci à être une distribution. Ainsi, un modèle appris à partir d'un nombre fini d'exemples peut renvoyer des valeurs négatives et qui ne somment pas à un. Ainsi, cette thèse s'adresse à ces deux problèmes. D'abord, nous proposons un élargissement des garanties théoriques pour les modèles compressés, ainsi qu'un algorithme spectral régularisé qui adapte la taille du modèle aux données. Puis, une application en guerre électronique est aussi proposée pour le séquencement des écoutes du récepteur superhétérodyne.
Elle s'intéresse En particulier à deux sous-groupes : les dénominaux dont la paraphrase est « faire devenir S » , où S est le substantif de base (appelés BN) ; et les désadjectivaux dont la paraphrase est « faire quelque chose plus A » , où A est l'adjective de base (appelés DPV). La thèse s'ouvre avec des chapitres de caractère général. Le premier plaide pour une amélioration des méthodes de collecte de données dans le domaine génératif. Les deuxième et troisième chapitres décrivent les cadres formels pertinents ainsi que la parasynthèse. La première partie de la thèse porte sur les BN. Cette enquête se poursuit par une comparaison avec les données du français, qui se comporte différemment. La deuxième partie s'ouvre avec une réflexion sur les diagnostics de la stativité, et se poursuit avec l'analyse des DPVs. Enfin, une application au TALN des diagnostics de stativité est décrite dans le dernier chapitre.
La représentation d'image sous une forme hiérarchique a été largement utilisée dans un contexte de classification. Une telle représentation est capable de modéliser le contenu d'une image à travers une structure arborescente. Dans cette thèse, nous étudions les méthodes à noyaux qui permettent de prendre en entrée des données sous une forme structurée et de tenir compte des informations topologiques présentes dans chaque structure en concevant des noyaux structurés. Nous présentons un noyau structuré dédié aux structures telles que des arbres non ordonnés et des chemins (séquences de noeuds) équipés de caractéristiques numériques. Le noyau proposé, appelé Bag of Subpaths Kernel (BoSK), est formé en sommant les noyaux calculés sur les sous-chemins (un sac de tous les chemins et des noeuds simples) entre deux sacs. Nous proposons également une version rapide de notre algorithme, appelé Scalable BoSK (SBoSK), qui s'appuie sur la technique des Random Fourier Features pour projeter les données structurées dans un espace euclidien, où le produit scalaire du vecteur transformé est une approximation de BoSK. Cette stratégie permet la classification des tuiles ou parties d'image. En poussant plus loin l'utilisation de (S)BoSK, nous introduisons une nouvelle approche de classification multi-source qui effectue la classification directement à partir d'une représentation hiérarchique construite à partir de deux images de la même scène prises à différentes résolutions, éventuellement selon différentes modalités. Les évaluations sur plusieurs jeux de données de télédétection disponibles dans la communauté illustrent la supériorité de (S)BoSK par rapport à l'état de l'art en termes de précision de classification, et les expériences menées sur une tâche de classification urbaine montrent la pertinence de l'approche de classification multi-source proposée.
Le principal objectif de cette thèse est d'étudier l'incidence des erreurs prépositionnelles sur l'intelligibilité de productions en L2 anglais par des apprenants francophones. Nous proposons également des solutions pédagogiques pour améliorer l'apprentissage des prépositions en anglais. Les résultats de cette analyse permettent de voir dans quelle mesure les erreurs prépositionnelles affectent l'intelligibilité du message.
Dans de nombreux domaines, les nouvelles technologies d'acquisition de l'information ou encore de mesure (e.g. serres de phénotypage robotisées) ont engendré une création phénoménale de données. Nous nous appuyons en particulier sur deux cas d'application réels : les observations de plantes en botanique et les données de phénotypage en biologie. Cependant, nos contributions peuvent être généralisées aux données du Web. Par ailleurs, s'ajoute à la quantité des données leur distribution. Chaque utilisateur stocke en effet ses données sur divers sites hétérogènes (e.g. Que ce soit pour les observations de botanique ou pour les données de phénotypage en biologie, des solutions collaboratives, comprenant des outils de recherche et de recommandation distribués, bénéficieraient aux utilisateurs. Les techniques de diversification permettent de présenter aux utilisateurs des résultats offrant une meilleure nouveauté tout en évitant de les lasser par des contenus redondants et répétitifs. Grâce à la diversité, une distance entre toutes les recommandations est en effet introduite afin que celles-ci soient les plus représentatives possibles de l'ensemble des résultats pertinents. Peu de travaux exploitent la diversité des profils des utilisateurs partageant les données. Dans ce travail de thèse, nous montrons notamment que dans certains scénarios, diversifier les profils des utilisateurs apporte une nette amélioration en ce qui concerne la qualité des résultats~ : des sondages montrent que dans plus de 75% des cas, les utilisateurs préfèrent la diversité des profils à celle des contenus. Par ailleurs, afin d'aborder les problèmes de distribution des données sur des sites hétérogènes, deux approches sont possibles. La première, les réseaux P2P, consiste à établir des liens entre chaque pair (noeud du réseau) : étant donné un pair p, ceux avec lesquels il a établi un lien représentent son voisinage. Celui-ci est utilisé lorsque p soumet une requête q, pour y répondre. Cependant, dans les solutions de l'état de l'art, la redondance des profils des pairs présents dans les différents voisinages limitent la capacité du système à retrouver des résultats pertinents sur le réseau, étant donné les requêtes soumises par les utilisateurs. Nous montrons, dans ce travail, qu'introduire de la diversité dans le calcul du voisinage, en augmentant la couverture, permet un net gain en termes de qualité. La seconde approche de la distribution est le multisite. Généralement, dans les solutions de l'état de l'art, les sites sont homogènes et représentés par de gros centres de données. Deux versions du prototype ont été réalisées afin de répondre aux deux cas d'application, en s'adaptant notamment aux types des données.
Dans cette thèse, nous nous sommes intéressés à l'impact de la quantité et de la qualité de l'information échangée entre individus d'un groupe sur leurs performances collectives dans deux types de tâches bien spécifiques. Dans une première série d'expériences, les sujets devaient estimer des quantités séquentiellement, et pouvaient réviser leurs estimations après avoir reçu comme information sociale l'estimation moyenne d'autres sujets. Nous contrôlions cette information sociale à l'aide de participants virtuels (dont nous contrôlions le nombre) donnant une information (dont nous contrôlions la valeur), à l'insu des sujets. Nous avons montré que lorsque les sujets ont peu de connaissance préalable sur une quantité à estimer, (les logarithmes de) leurs estimations suivent une distribution de Laplace. La médiane étant un bon estimateur du centre d'une distribution de Laplace, nous avons défini la performance collective comme la proximité de la médiane (du logarithme) des estimations à la vraie valeur. Nous avons trouvé qu'après influence sociale, et lorsque les agents virtuels fournissent une information correcte, la performance collective augmente avec la quantité d'information fournie (fraction d'agents virtuels). Nous avons aussi analysé la sensibilité à l'influence sociale des sujets, et trouvé que celle-ci augmente avec la distance entre l'estimation personnelle et l'information sociale. Ces analyses ont permis de définir 5 traits de comportement : garder son opinion, adopter celle des autres, faire un compromis, amplifier l'information sociale ou au contraire la contredire. Nos résultats montrent que les sujets qui adoptent l'opinion des autres sont ceux qui améliorent le mieux leur performance, car ils sont capables de bénéficier de l'information apportée par les agents virtuels. Nous avons ensuite utilisé ces analyses pour construire et calibrer un modèle d'estimation collective, qui reproduit quantitativement les résultats expérimentaux et prédit qu'une quantité limitée d'information incorrecte peut contrebalancer un biais cognitif des sujets consistant à sous-estimer les quantités, et ainsi améliorer la performance collective. D'autres expériences ont permis de valider cette prédiction. Dans une seconde série d'expériences, des groupes de 22 piétons devaient se séparer en clusters de la même "couleur", sans indice visuel (les couleurs étaient inconnues), après une courte période de marche aléatoire. Pour les aider à accomplir leur tâche, nous avons utilisé un système de filtrage de l'information disponible (analogue à un dispositif sensoriel tel que la rétine), prenant en entrée l'ensemble des positions et couleurs des individus, et retournant un signal sonore aux sujets (émit par des tags attachés à leurs épaules) lorsque la majorité de leurs k plus proches voisins était de l'autre couleur que la leur.
Cette thèse, réalisée en coopération avec l'ONERA, concerne la reconnaissance active d'objets 3D par un agent autonome muni d'une caméra d'observation. Alors qu'en reconnaissance passive les modalités d'acquisitions des observations sont imposées et génèrent parfois des ambiguïtés, la reconnaissance active exploite la possibilité de contrôler en ligne ces modalités d'acquisition au cours d'un processus d'inférence séquentiel dans le but de lever l'ambiguïté. L'objectif des travaux est d'établir des stratégies de planification dans l'acquisition de l'information avec le souci d'une mise en œuvre réaliste de la reconnaissance active. La première partie des travaux se consacre à apprendre à planifier. La deuxième partie des travaux s'attache à exploiter au mieux les observations au cours de la reconnaissance. La possibilité d'une reconnaissance active multi-échelles est étudiée pour permettre une interprétation au plus tôt dans le processus séquentiel d'acquisition de l'information. Les observations sont également utilisées pour estimer la pose de l'objet de manière robuste afin d'assurer la cohérence entre les modalités planifiées et celles réellement atteintes par l'agent visuel.
Cette thèse porte sur la reconnaissance visuelle "zero-shot", qui vise à classifier des images de catégories non rencontrées par le modèle pendant la phase d'apprentissage. Après avoir classé les méthodes existantes en trois grandes catégories, nous défendons l'idée que les méthodes dites de classement se basent habituellement sur plusieurs hypothèses implicites préjudiciables. Nous proposons d'adapter leur fonction de coût pour leur permettre d'intégrer des relations inter et intra-classe. Nous proposons également un processus permettant de diminuer l'écart entre les performances sur les classes vues et non vues dont souffrent fréquemment ces méthodes. Dans notre évaluation expérimentale, ces contributions permettent à notre modèle d'égaler ou surpasser les performances des méthodes génératives, tant en étant moins restrictif. Dans un second temps, nous nous intéressons aux représentations sémantiques utilisées dans un contexte d' Dans ce contexte, l'information sémantique provient généralement de plongements lexicaux des noms de classe. Nous soutenons que les plongements habituels souffrent d'un manque de contenu visuel dans les corpus servant à leur apprentissage. Nous proposons donc de nouveaux corpus de texte davantage connotés visuellement, ainsi qu'une méthode permettant d'adapter les modèles de plongement à ces corpus. Nous proposons en outre de compléter ces représentations non supervisées par de courtes descriptions en langage naturel, dont la production ne requiert qu'un effort minimal comparé à des attributs génériques.
La construction d'ontologies est un processus fastidieux qui nécessite un travail manuel conséquent. Les textes, en tant que sources de connaissances, peuvent optimiser les recours aux experts du domaine. Le passage des textes à l'ontologie requiert un double changement de perspective. Tout d'abord du niveau du discours vers le niveau linguistique (terminologie, hyperonymie, synonymie, etc.), à l'aide d'outils de traitement automatique des langues. La conceptualisation, manuelle, permet ensuite d'entrer dans le monde des modèles. Nous montrons que, dans l'état actuel des connaissances, la construction d'ontologies à partir de textes ne peut s'effectuer de manière totalement automatique.
Cette thèse se situe au croisement de deux domaines de recherche, le Traitement Automatique des Langues (TAL) d'une part, et l'Apprentissage Automatique (AP) d'autre part. Le sujet propose d'exploiter les connaissances expertes et les ressources linguistiques disponibles pour améliorer la performance des modèles de réseaux de neurones profonds pour les domaines et langues peu dotées en données d'apprentissage. Le sujet de thèse proposé a pour but d'explorer et d'expérimenter de nouvelles approches pour l'intégration de connaissances expertes et de ressources linguistiques dans des modèles neuronaux profonds en vue d'améliorer leur performance pour les domaines et langues peu dotées en données d'apprentissage. Un premier défi de cette thèse réside dans la définition d'un mécanisme de représentation de ces connaissances expertes et de ces ressources linguistiques de nature variées pour une intégration optimale dans le modèle neuronal. Le problème s'avère plus difficile face à des connaissances expertes ou des ressources linguistiques hétérogènes. Nous proposons d'aborder cette problématique selon les axes suivants, dans le prolongement des travaux déjà réalisés au laboratoire LASTI (Laboratoire Analyse Sémantique Texte et Image) :  - Prise en compte de connaissances expertes et de ressources linguistiques hétérogènes : Ontologies, Bases de données terminologiques, Lexiques, Règles de reconnaissance d'entités nommées, Règles d'identification de relations de dépendance syntaxique, etc. - Développement d'un formalisme permettant de décrire les connaissances expertes et les ressources linguistiques dans un ensemble de représentations disposées par étages. L'idée est que chaque type de connaissance experte ou de ressource linguistique a une représentation séparée mais que l'ensemble des représentations est décrit dans le même format (modèle). - Exploration de nouvelles stratégies pour l'intégration des connaissances expertes et des ressources linguistiques dans les réseaux de neurones profonds. L'idée sous-jacente est de proposer un mécanisme d'intégration qui s'adapte à chaque type de connaissance externe ou de ressource linguistique.
Menée dans le cadre de la Construction Morphology, ma thèse propose une analyse des verbes parasynthétiques de l'italien. La définition de parasynthèse courante en littérature correspond à 'double affixation simultanée sur une base de dérivation' ([préf+[X]N/A+suff]V, cf. par exemple IMBARCARE 'embarquer'). Cette particularité est motivée par l'impossibilité d'attester 'l'étape intermédiaire'de dérivation entre la base et le verbe construit (cf. BARCA, *IMBARCA, *BARCARE). Cette définition relève d'une analyse morphemique, incrémentale et concatenative et présuppose que les procédés dérivationnels sont conçus comme des règles orientées. Dans mon analyse, au contraire, sont définis parasynthétiques tous les verbes construits par préfixation. Cette définition s'appuie uniquement sur le paramètre d'appartenance au schéma [préf+[X]N/A]V (le suffixe étant de nature flexionnelle). Selon mon point de vue, la non-attestation d'une forme est un paramètre qui est non seulement insuffisamment fiable d'un point de vue empirique, mais aussi négligeable du point de vue d'une théorie basée sur l'idée de procédés non-orientés. La base de données utilisée contient 1674 lexèmes tirés de façon automatique du corpus ItWaC. Les variables structurelles des verbes correspondent (i) au préfixe employé (a-, in-, s-, de-ou dis-), (ii) à la classe flexionnelle (-are ou-ire) et (iii) à la catégorie de la base (N ou A). Chaque lexème, définit comme construction, est l'association d'une forme (le résultat de la combinaison des variables) et d'une valeur sémantique holistique. Les valeurs sémantiques suivantes ont été identifiées : (i) changement d'état, (ii) changement de relation locative et (iii) valeur intensive/itérative. Pour les deux premières valeurs il a été proposé une analyse unifiée dans une même composante sémantique qui exprime un changement (formalisé au moyen du prédicat BECOME), alors que la classe des verbes qui expriment la valeur (iii) est exclue de cette généralisation.
Nos travaux de recherches présentés dans ce manuscrit ont pour objectif, l'optimisation des performances des algorithmes de suivi des formants. Pour ce faire, nous avons commencé par l'analyse des différentes techniques existantes utilisées dans le suivi automatique des formants. Cette analyse nous a permis de constater que l'estimation automatique des formants reste délicate malgré l'emploi de diverses techniques complexes. Vue la non disponibilité des bases de données de référence en langue arabe, nous avons élaboré un corpus phonétiquement équilibré en langue arabe tout en élaborant un étiquetage manuel phonétique et formantique. Ensuite, nous avons présenté nos deux nouvelles approches de suivi de formants dont la première est basée sur l'estimation des crêtes de Fourier (maxima de spectrogramme) ou des crêtes d'ondelettes (maxima de scalogramme) en utilisant comme contrainte de suivi le calcul de centre de gravité de la combinaison des fréquences candidates pour chaque formant, tandis que la deuxième approche de suivi est basée sur la programmation dynamique combinée avec le filtrage de Kalman. Finalement, nous avons fait une étude exploratrice en utilisant notre corpus étiqueté manuellement comme référence pour évaluer quantitativement nos deux nouvelles approches par rapport à d'autres méthodes automatiques de suivi de formants. Nous avons testé la première approche par détection des crêtes ondelette, utilisant le calcul de centre de gravité, sur des signaux synthétiques ensuite sur des signaux réels de notre corpus étiqueté en testant trois types d'ondelettes complexes (CMOR, SHAN et FBSP). Suite à ces différents tests, il apparaît que le suivi de formants et la résolution des scalogrammes donnés par les ondelettes CMOR et FBSP sont meilleurs qu'avec l'ondelette SHAN. Afin d'évaluer quantitativement nos deux approches, nous avons calculé la différence moyenne absolue et l'écart type normalisée. Nous avons fait plusieurs tests avec différents locuteurs (masculins et féminins) sur les différentes voyelles longues et courtes et la parole continue en prenant les signaux étiquetés issus de la base élaborée comme référence. Les résultats de suivi ont été ensuite comparés à ceux de la méthode par crêtes de Fourier en utilisant le calcul de centre de gravité, de l'analyse LPC combinée à des bancs de filtres de Mustafa Kamran et de l'analyse LPC dans le logiciel Praat. Cette méthode donne donc un suivi de formants (F1, F2 et F3) pertinent et plus proche de suivi référence. Elles sont aussi très proches de la méthode par détection de crêtes de Fourier utilisant le calcul de centre de gravité. Les résultats obtenus dans le cas des locutrices féminins confirment la tendance observée sur les locuteurs
Cette thèse porte sur l'étude de méthodes aléatoires pour l'apprentissage de données en grande dimension. Nous proposons d'abord une approche non supervisée consistant en l'estimation des composantes principales, lorsque la taille de l'échantillon et la dimension de l'observation tendent vers l'infini. Cette approche est basée sur les matrices aléatoires et utilise des estimateurs consistants de valeurs propres et vecteurs propres de la matrice de covariance. Ensuite, dans le cadre de l'apprentissage supervisé, nous proposons une approche qui consiste à, d'abord réduire la dimension grâce à une approximation de la matrice de données originale, et ensuite réaliser une LDA dans l'espace réduit. La réduction de dimension est basée sur l'approximation de matrices de rang faible par l'utilisation de matrices aléatoires. Un algorithme d'approximation rapide de la SVD, puis une version modifiée permettant l'approximation rapide par saut spectral sont développés. Les approches sont appliquées à des données réelles images et textes. Elles permettent, par rapport à d'autres méthodes, d'obtenir un taux d'erreur assez souvent optimal, avec un temps de calcul réduit. Enfin, dans le cadre de l'apprentissage par transfert, notre contribution consiste en l'utilisation de l'alignement des sous-espaces caractéristiques et l'approximation de matrices de rang faible par projections aléatoires.
Cette thèse en linguistique contrastive se propose d'étudier les spécificités syntactico-sémantiques des verbes inchoatifs en arabe et les comparer avec celles de commencer (à/par/de) et de se mettre à. Nous nous sommes appuyé sur un corpus de textes littéraires francophones et arabophones (Mahfouz, Pagnol et Camus). Notre corpus montre que les verbes inchoatifs peuvent apparaître dans des constructions simples suivantes : (sujet + verbe inchoatif + complément d'objet direct, complément d'objet indirect, adverbe ou sans complément), ainsi que dans des constructions composées (sujet + V1 inchoatif accompli + V2 inaccompli) en arabe, et (S + V1 conjugué + V2 infinitif) en français. Notre analyse s'inspire des cadres syntaxiques proposés par Peeters (1993). En effet, l'application de cette théorie et la variation syntaxique des éléments grammaticaux de la phrase arabe nous amènent, tout d'abord, à proposer différents cadres syntaxiques de certains verbes inchoatifs arabes relevés dans notre corpus en déterminant leurs spécificités syntaxiques et sémantiques. Concernant la forme simple des inchoatifs en arabe, بدأ (bada'a) est le verbe le plus fréquemment utilisé. Celui-ci est compatible avec certaines prépositions et accepte n'importe quel type du sujet et du complément. Par conséquent, il se distingue par six constructions grammaticales et quatre cadres syntaxiques. En revanche, les verbes هم (hamma), شرع (šara῾a) et أخذ ('aẖaḏa), dans leur emploi simple, chacun de ceux-ci ne se régit que par une préposition, de ce fait, leur construction simple ne se caractérise que par un cadre syntaxique. Par ailleurs, le verbe راح (rāḫa) se combine avec deux cadres syntaxiques. En ce qui concerne leur forme composée, notons que tous les verbes inchoatifs en arabe se partagent par une construction composée (S + V1 accompli + V2 inaccompli). Sémantiquement, la majorité de ces verbes se distingue par une valeur imperfective. Notons que le verbe بدأ (bada'a) véhicule une valeur déterminative ainsi que non déterminative. Par contre, le verbe شرع (šara῾a) se défini que par une valeur perfective, alors que هم (hamma) par une valeur semi-perfective. En conséquence, d'après les résultats de Peeters ainsi que les nôtres, nous constatons que, d'une part, le verbe بدأ (bada'a) et commencer (à/par/de) sont presque équivalents syntaxiquement et sémantiquement. D'autre part, les verbes هم (hamma) et se mettre à servent toujours à indiquer la rapidité et la soudaineté, mais l'action du premier est toujours semi-perfective. Nous pouvons dire qu'il existe toujours des points de divergence et convergence entre les verbes inchoatifs dans les deux langues (arabe et français). Enfin, d'après des productions écrites d'un échantillon d'apprenants libyens du FLE, nous avons tenté de dégager des pistes pédagogiques destinées à savoir l'origine des difficultés dans l'emploi des verbes commencer (à/par/de) et se mettre à.
Aujourd'hui, la plupart des internautes ont leurs données dispersées dans plusieurs appareils, applications et services. La gestion et le contrôle de ses données sont de plus en plus difficiles. Dans cette thèse, nous adoptons le point de vue selon lequel l'utilisateur devrait se voir donner les moyens de récupérer et d'intégrer ses données, sous son contrôle total. À ce titre, nous avons conçu un système logiciel qui intègre et enrichit les données d'un utilisateur à partir de plusieurs sources hétérogènes de données personnelles dans une base de connaissances RDF. Le logiciel est libre, et son architecture innovante facilite l'intégration de nouvelles sources de données et le développement de nouveaux modules pour inférer de nouvelles connaissances. Nous présentons un algorithme pour retrouver les points de séjour d'un utilisateur à partir de son historique de localisation. À l'aide de ces données et de données provenant d'autres capteurs de son téléphone, d'informations géographiques provenant d'OpenStreetMap, et des horaires de transports en commun, nous présentons un algorithme de reconnaissance du mode de transport capable de retrouver les différents modes et lignes empruntés par un utilisateur lors de ses déplacements. L'algorithme reconnaît l'itinéraire pris par l'utilisateur en retrouvant la séquence la plus probable dans un champ aléatoire conditionnel dont les probabilités se basent sur la sortie d'un réseau de neurones artificiels. Nous montrons également comment le système peut intégrer les données du courrier électronique, des calendriers, des carnets d'adresses, des réseaux sociaux et de l'historique de localisation de l'utilisateur dans un ensemble cohérent. Pour ce faire, le système utilise un algorithme de résolution d'entité pour retrouver l'ensemble des différents comptes utilisés par chaque contact de l'utilisateur, et effectue un alignement spatio-temporel pour relier chaque point de séjour à l'événement auquel il correspond dans le calendrier de l'utilisateur. Enfin, nous montrons qu'un tel système peut également être employé pour faire de la synchronisation multi-système/multi-appareil et pour pousser de nouvelles connaissances vers les sources.
L'incursion de l'Islam en Afrique subsaharienne à partir du IXe siècle, s'est opérée via le commerce transsaharien entre les peuples d'Afrique du Nord et ceux du Sahel. Ce contact entretenu par les caravanes commerciales entre les deux peuples a engendré l'islamisation progressive de la population hausaphone. Sous l'influence de l'arabe, plusieurs vocables sont introduits dans le lexique du hausa. Cet effet islamique s'accompagne d'une révolution dans la production de la littérature arabe-ajami. Sur la base de ces observations, cette thèse se propose d'analyser les emprunts lexicaux arabes dans les œuvres poétiques de l'auteur, et leur intégration dans la langue hausa. Notre corpus comprend 15 œuvres poétiques que nous avons lemmatisées en préalable aux calculs statistiques à l'aide du logiciel Excel. Les principaux résultats obtenus sur les formes graphiques, montrent une fréquence d'utilisation très élevée des emprunts arabes. L'association de l'analyse linguistique et des traitements informatiques, nous a permis ainsi de confirmer, de façon formelle et impartiale, que la plupart des emprunts les plus fréquents relèvent de domaines religieux, et donc liés aux lexiques de situation.
L'utilisation des dossiers médicaux électroniques (DMEs) et la prescription électronique sont des priorités dans les différents plans d'action européens sur la santé connectée. il capture tous les épisodes symptomatiques dans la vie d'un patient et doit permettre l'amélioration des pratiques médicales et de prises en charge, à la condition de mettre en place des procédures de traitement automatique. A ce titre nous travaillons sur la prédiction d'hospitalisation à partir des DMEs et après les avoir représentés sous forme vectorielle, nous enrichissons ces modèles afin de profiter des connaissances issues de référentiels, qu'ils soient généralistes ou bien spécifiques dans le domaine médical, et cela, dans le but d'améliorer le pouvoir prédictif d'algorithmes de classification automatique. Déterminer les connaissances à extraire dans l'objectif de les intégrer aux représentations vectorielles est à la fois une tâche subjective et destinée aux experts, nous verrons une procédure semi-supervisée afin d'automatiser en partie ce processus. Du fruit de nos recherches, nous avons ébauché un produit destiné aux médecins généralistes afin de prévenir l'hospitalisation de leur patient ou du moins améliorer son état de santé. Ainsi, par le biais d'une simulation, il sera possible au médecin d'évaluer quels sont les facteurs impliqués dans le risque d'hospitalisation de son patient et de définir les actions préventives à planifier pour éviter l'apparition de cet événement.
Les recherches passées constituent pourtant une source d'information utile pour les nouveaux utilisateurs (nouvelles requêtes). En raison de l'absence de collections ad-hoc de RI, à ce jour il y a un faible intérêt de la communauté RI autour de l'utilisation des recherches passées. En effet, la plupart des collections de RI existantes sont composées de requêtes indépendantes. Ces collections ne sont pas appropriées pour évaluer les approches fondées sur les requêtes passées parce qu'elles ne comportent pas de requêtes similaires ou qu'elles ne fournissent pas de jugements de pertinence. Par conséquent, il n'est pas facile d'évaluer ce type d'approches. En outre, l'élaboration de ces collections est difficile en raison du coût et du temps élevés nécessaires. Une alternative consiste à simuler les collections. Par ailleurs, les documents pertinents de requêtes passées similaires peuvent être utilisées pour répondre à une nouvelle requête. De plus, ce principe peut également bénéficier d'un clustering des recherches antérieures selon leurs similarités. Quatre algorithmes probabilistes pour la réutilisation des résultats de recherches passées sont ensuite proposés et évalués. Enfin, une nouvelle mesure dans un contexte de clustering est proposée.
ANALYSE DIACHRONIQUE DU TRÉSOR DE LA LANGUE FRANÇAISEET DE L'OXFORD ENGLISH DICTIONARY : LE TRAITEMENT DES EMPRUNTS RÉSUMÉ Il n'est pas de langue dont le lexique ne s'enrichisse au gré des emprunts, qui permettent d'accroître et de renouveler le fonds lexical au fur et à mesure que se développent les relations entre les pays et entre leurs cultures. Les langues anglaise et française, en raison de leur rayonnement sur tous les continents, ont acquis un contingent très important de mots venus d'ailleurs, qu'elles se sont en outre souvent partagé. En effet, du fait de leur proximité géographique et d'une histoire commune d'une grande richesse, l'anglais et le français ont été amenés à s'interpénétrer pendant plus de dix siècles. Nous avons voulu, dans cette étude, montrer l'impact des emprunts sur les deux langues, et analyser la façon dont ils sont traités dans les dictionnaires les plus extensifs qui soient de part et d'autre de la Manche : le Trésor de la Langue Française et l'Oxford English Dictionary. Dans une première partie, nous étudions la constitution des lexiques anglais et français au fil du temps en fonction des apports étrangers, avant de définir la notion même d'emprunt et d'en montrer la complexité. Enfin, nous présentons le corpus sur lequel repose ce travail. La seconde partie est consacrée à la présentation du Trésor de la Langue Française et de l'Oxford English Dictionary. Après avoir retracé l'histoire des dictionnaires de langue et la genèse de ces deux dictionnaires, leurs caractéristiques sont mises en évidence et leur constitution finement analysée, tant sur le plan macrostructurel que sur le plan microstructurel. Nous avons également montré les atouts que représente leur informatisation.
Dans cette recherche, nous abordons le problème de le recherche de services qui répondent à des besoins des utilisateurs exprimés sous forme de requête en texte libre. Notre objectif est de résoudre les problèmes qui affectent l'efficacité des modèles de recherche d'information existant lorsqu'ils sont appliqués à la recherche de services dans un corpus rassemblant des descriptions standard de ces services. Ces problèmes sont issus du fait que les descriptions des services sont brèves. Nous avons adapté une famille de modèles de recherche d'information (IR) dans le but de contribuer à accroître l'efficacité acquise avec les modèles existant concernant la découverte de services. En outre, nous avons mené des expériences systématiques afin de comparer notre famille de modèles IR avec ceux de l'état de l'art portant sur la découverte de service. Des résultats des expériences, nous concluons que notre modèle basé sur l'extension des requêtes via un thésaurus co-occurrence est plus efficace en terme des mesures classiques utilisées en IR que tous les modèles étudiés dans cette recherche. Par conséquent, nous avons mis en place ce modèle dans S3niffer, qui est un moteur de recherche de service basé sur leur description standard.
Les bases de connaissances sont des bases de données déductives où la logique est utilisée pour représenter des connaissances de domaine sur des données existantes. Dans le cadre des règles existentielles, une base de connaissances est composée de deux couches : la couche de données qui représentent les connaissances factuelle et la couche ontologique qui incorpore des règles de déduction et des contraintes négatives. L'interrogation de données à l'aide des ontologies est la fonction de raisonnement principale dans ce contexte. Comme dans la logique classique, les contradictions posent un problème à l'interrogation car « d'une contradiction, on peut déduire ce qu'on veut (ex falso quodlibet) » . Récemment, des approches d'interrogation tolérantes aux incohérences ont été proposées pour faire face à ce problème dans le cadre des règles existentielles. Elles déploient des stratégies dites de réparation pour restaurer la cohérence. Cependant, ces approches sont parfois inintelligibles et peu intuitives pour l'utilisateur car elles mettent souvent en œuvre des stratégies de réparation complexes. Ce manque de compréhension peut réduire l'utilisabilité de ces approches car elles réduisent la confiance entre l'utilisateur et les systèmes qui les utilisent. Par conséquent, la problématique de recherche que nous considérons est comment rendre intelligible à l'utilisateur l'interrogation tolérantes aux incohérences. Pour répondre à cette question de recherche, nous proposons d'utiliser deux formes d'explication pour faciliter la compréhension des réponses retournées par une interrogation tolérante aux incohérences. Ces deux types d'explication prennent la forme d'un dialogue entre l'utilisateur et le raisonneur au sujet des déductions retournées comme réponses à une requête donnée. Nous étudions ces explications dans le double cadre de l'argumentation fondée sur la logique et de la dialectique formelle, comme nous étudions leurs propriétés et leurs impacts sur les utilisateurs en termes de compréhension des résultats.
Ces troubles sont courants dans la maladie de Parkinson et associés à une baisse de la qualité de vie des patients ainsi qu'à une augmentation de la charge des aidants. Pouvoir prédire quels sont les sujets les plus à risque de développer ces troubles et quand ces troubles apparaissent est de grande importance. L'objectif de cette thèse est d'étudier les troubles du contrôle de l'impulsivité dans la maladie de Parkinson à partir des approches statistique et de l'apprentissage automatique, et se divise en deux parties. La première partie consiste à analyser la performance prédictive de l'ensemble des facteurs associés à ces troubles dans la littérature. La seconde partie consiste à étudier l'association et l'utilité d'autres facteurs, en particulier des données génétiques, pour améliorer la performance prédictive.
Depuis de nombreuses années, le déploiement des TIC dans la prise en charge médicale de pathologies chroniques joue un rôle majeur notamment dans l'évolution des pratiques de santé et l'amélioration du bien-être du patient. Les pathologies chroniques sont de nature longue et évolutive et nécessitent un suivi régulier effectué par une équipe pluridisciplinaire où différents acteurs interviennent auprès du patient. Le patient à son tour, est amené à respecter à domicile, un protocole de soins défini et personnalisé par cette équipe. Cependant, la forme dans la quelle le contenu du protocole est représenté n'est pas forcément complète ni facile à comprendre par les patients. De plus, chaque patient est unique, et la définition du protocole de soins doit être personnalisée et appropriée à ses soins et traitements individuels, parfois même à ses souhaits et contraintes personnelles. L'expertise du patient sur sa maladie chronique est une information précieuse que nous souhaitons intégrer dans un protocole de soins personnalisé afin d'améliorer la prise en charge médicale, le suivi à distance de la maladie chronique et à terme améliorer la connaissance de la pathologie chronique.
La recherche développée dans cette thèse introduit une approche qualitative pour représenter et raisonner à partir d'entités spatiales dans un espace géographique à deux dimensions. Les patrons de mouvements entre entités dynamiques sont catégorisés à partir d'un modèle qualitatif de relations topologiques entre une ligne orientée et une région, et de relations d'orientation entre deux lignes orientées, respectivement. Les mouvements qualitatifs sont dérivés à partir de relations spatio-temporelles qui caractérisent des entités dynamiques conceptualisées comme des points ou des régions dans un espace à deux dimensions. Cette architecture de raisonnement permet de dériver des configurations de mouvements basiques dérivées à partir d'entités statiques et dynamiques. L'approche est complétée par une qualification de ces configurations à partir d'expressions du langage naturel. Les compositions de mouvements sont étudiées tout comme les transitions possibles dans des cas de données incomplètes. Les tables de compositions sont également explorées et permettent d'étendre les possibilités de raisonnement. Le modèle est expérimenté dans le contexte de l'analyse de trajectoires aériennes et maritimes.
Les modèles computationnels pour la compréhension automatique des textes ont suscité un vif intérêt en raison de gains de performances inhabituels au cours des dernières années, certains d'entre eux conduisant à des scores d'évaluation surhumains. Ce succès a conduit à affirmer la création de représentations universelles de phrases. Dans cette thèse, nous questionnons cette affirmation au travers de deux angles complémentaires. Premièrement, les réseaux de neurones et les représentations vectorielles sont-ils suffisamment expressifs pour traiter du texte de sorte à pouvoir effectuer un large éventail de tâches complexes ? Dans cette thèse, nous présenterons les modèles neuronaux actuellement utilisés et les techniques d'entraînement associées. Deuxièmement, nous aborderons la question de l'universalité dans les représentation des phrases : que cachent en réalité ces prétentions à l'universalité ? Nous décrivons quelques théories de ce qu'est le sens d'une expression textuelle, et dans une partie ultérieure de cette thèse, nous soutenons que la sémantique (contenu littéral, non situé) par rapport à la pragmatique (la partie du sens d'un texte définie par son rôle et son contexte) est prépondérante dans les données d'entraînement et d'évaluation actuelles des modèles de compréhension du langage naturel. Pour atténuer ce problème, nous montrons que la prédiction de marqueurs de discours (classification de marqueurs de discours initialement présents entre des phrases) peut être considérée comme un signal d'apprentissage centré sur la pragmatique pour la compréhension de textes. Nous proposons également un nouvel outil d'évaluation de la compréhension du langage naturel en se basant sur le discours et la pragmatique. Cet outil pourrait inciter la communauté du traitement des langues à prendre en compte les considérations pragmatiques lors de l'évaluation de modèles de compréhension du langage naturel.
Cette étude porte sur l'analyse du discours identitaire du recueil "Pourquoi as-tu laissé le cheval à sa solitude ?" de Mahmoud Darwich à savoir la nature de la relation que le Moi (celui de Mahmoud Darwich) entretient avec le Moi de l'Autre. Autrement dit, le recueil en question ressemble à un dialogue où le même et l'autre se plongent dans un discours à titre argumentatif. La réponse à la question de la relation entre les deux parties du dialogue est abordée, premièrement, d'un point de vue thématique/théorique : la présentation de la figure emblématique du Moi darwichien perçu en tant qu'un, ayant une identité personnelle, culturelle, sociale et nationale, différente de celle dont dispose l'autre un (celui de l'Autre). Deuxièmement, d'un point de vue pratique, le type de relation entre les deux côtés a été montré à travers l'analyse du corpus du recueil se composant de six groupes.
La communication efficace a tendance à suivre la loi du moindre effort. Selon ce principe, en utilisant une langue donnée les interlocuteurs ne veulent pas travailler plus que nécessaire pour être compris. Ce fait mène à la compression extrême de textes surtout dans la communication électronique, comme dans les microblogues, SMS, ou les requêtes dans les moteurs de recherche. Cependant souvent ces textes ne sont pas auto-suffisants car pour les comprendre, il est nécessaire d'avoir des connaissances sur la terminologie, les entités nommées ou les faits liés. Le premier objectif de notre travail est d'aider l'utilisateur à mieux comprendre un message court par l'extraction du contexte d'une source externe comme le Web ou la Wikipédia au moyen de résumés construits automatiquement. Pour cela nous proposons une approche pour le résumé automatique de documents multiples et nous l'appliquons à la contextualisation de messages, notamment à la contextualisation de tweets. La méthode que nous proposons est basée sur la reconnaissance des entités nommées, la pondération des parties du discours et la mesure de la qualité des phrases. Contrairement aux travaux précédents, nous introduisons un algorithme de lissage en fonction du contexte local. De plus, nous avons développé un algorithme basé sur les graphes pour le ré-ordonnancement des phrases. La méthode a été également adaptée pour la génération de snippets. Les résultats des évaluations attestent une bonne performance de notre approche.
Les traumas, accidentels ou intentionnels, constituent un problème majeur de santé publique. En France, jusqu'à récemment, seule la sécurité routière faisait l'objet d'une attention revendiquant l'exhaustivité nationale de la surveillance épidémiologique, coordonnée par le ministère de l'Intérieur et sans lien fort avec le système de santé. Le système actuel de centralisation des résumés de visites aux services d'urgence (réseau OSCOUR®) constitue un puissant outil de surveillance en temps réel qui permet grâce au codage des diagnostics principaux et secondaires (codes CIM-10) de fournirn des indicateurs sur certaines pathologies saisonnières comme la grippe ou la gastro entérite. Cependant, ce type d'information n'est pas suffisant pour établir des indicateurs de surveillance liés aux traumatismes car le mécanisme ayant conduit au traumatisme (accident de la route, suicide violence, chute...) n'est pas renseigné. L'ajout de mécanismes relatifs aux traumatismes permettrait une surveillance épidémiologique nationale des traumatismes,d'évaluer les stratégies de prévention et d'améliorer la gestion prédictive des flux de patients. Le motif détaillé de la venue aux urgences n'est pas disponible sous forme de base de données standardisée mais se trouve être décrit en détail au sein des notes cliniques rédigée en texte libre et qui sont stockées dans les dossiers médicaux électroniques. L'objectif global du projet est donc de développer un outil qui permettrait de créer les informations standardisées sur les mécanismes traumatiques à partir de ces textes cliniques. À cette fin, les techniques les plus récentes de traitement du langage naturel (NLP : Natural Language Processing), branche de l'Intelligence Artificielle, seront testées, comparées et appliquées.
Notre recherche doctorale, qui s'inscrit dans le champ de l'acquisition d'une troisième langue et du plurilinguisme, a pour objectif d'apporter une meilleure compréhension du fonctionnement de la compétence plurilingue. Elle cherche plus particulièrement à déterminer si les langues d'un plurilingue jouent des rôles distincts dans l'acquisition d'une nouvelle langue au niveau des interactions translinguistiques lexicales et syntaxiques et si ces phénomènes sont associés. Elle examine également les activités métalinguistiques et translinguistiques permettant aux apprenants de gérer et d'appréhender la langue cible. Pour ce faire, nous avons réalisé onze études de cas de locuteurs de l'espagnol et de l'anglais avec des niveaux variés en français L3+. Ces participants ont été soumis à trois tâches de production orale. Afin prendre en compte la nature complexe des interactions lorsque les langues en contact sont typologiquement proches et lorsque différents domaines linguistiques sont examinés, nous avons adopté une approche mêlant travaux menés dans le champ de l'acquisition d'une L3 à ceux menés dans l'étude des contacts de langues. Par le biais d'une analyse quantitative et qualitative, notre travail a mis en évidence le fonctionnement de la compétence plurilingue de nos participants. Ils activent en effet toutes leurs langues à des degrés divers, au niveau de différents types d'ITL, propriétés syntaxiques et domaines linguistiques. Nos participants mobilisent, par ailleurs, leur conscience métalinguistique et translinguistique afin de mieux appréhender la LC et gérer leur production en recourant à des cognats, à des consultations translinguistiques et à des inférences.
La thèse se situe au carrefour des domaines du traitement automatique de la parole, de la reconnaissance des formes et de la recherche d'informations multimédia : l'indexation des émotions en vue de la recherche par le contenu. Le travail de thèse est donc orienté vers la reconnaissance et l'indexation de l'émotion indépendante du locuteur. Une grande partie de la thèse porte sur l'étude des paramètres avec la conclusion de l'efficacité de la combinaison entre la méthode de Sélection Forcée Séquentielle en Avant (SFSA) et la normalisation symbolique proposée. L'autre partie de la thèse s'appuie sur l'étude des techniques de classification appliquées dans la reconnaissance de l'émotion. Des études sur la reconnaissance de l'émotion inter-langue (interculturel) ont aussi été effectuées. Et enfin, sur la base de ces résultats, un moteur de l'indexation sur un corpus réel a été construit.
Dans ces travaux, nous explorons comment les techniques de Générations Automatiques de Langue Naturelle (GLN) peuvent être utilisées pour aborder la tâche de génération (semi-)automatique de matériel et d'activités dans le contexte de l'apprentissage de langues assisté par ordinateur. En particulier, nous montrons comment un Réalisateur de Surface (RS) basé sur une grammaire peut être exploité pour la création automatique d'exercices de grammaire. Notre réalisateur de surface utilise une grammaire réversible étendue, à savoir SemTAG, qui est une Grammaire d'Arbre Adjoints à Structure de Traits (FB-TAG) couplée avec une sémantique compositionnelle basée sur l'unification. Plus précisément, la grammaire FB-TAG intègre une représentation plate et sous-spécifiée des formules de Logique de Premier Ordre (FOL). Dans la première partie de la thèse, nous étudions la tâche de réalisation de surface à partir de formules sémantiques plates et nous proposons un algorithme de réalisation de surface basé sur la grammaire FB-TAG optimisé, qui supporte la génération de phrases longues étant donné une grammaire et un lexique à large couverture. L'approche suivie pour l'optimisation de la réalisation de surface basée sur FB-TAG à partir de sémantiques plates repose sur le fait qu'une grammaire FB-TAG peut être traduite en une Grammaire d'Arbres Réguliers à Structure de Traits (FB-RTG) décrivant ses arbres de dérivation. Le langage d'arbres de dérivation de la grammaire TAG constitue un langage plus simple que le langage d'arbres dérivés, c'est pourquoi des approches de génération basées sur les arbres de dérivation ont déjà été proposées. Notre approche se distingue des précédentes par le fait que notre encodage FB-RTG prend en compte les structures de traits présentes dans la grammaire FB-TAG originelle, ayant de ce fait des conséquences importantes par rapport à la sur-génération et la préservation de l'interface syntaxe-sémantique. L'algorithme de génération d'arbres de dérivation que nous proposons est un algorithme de type Earley intégrant un ensemble de techniques d'optimisation bien connues : tabulation, partage-compression (sharing-packing) et indexation basée sur la sémantique. Dans la seconde partie de la thèse, nous explorons comment notre réalisateur de surface basé sur SemTAG peut être utilisé pour la génération (semi-)automatique d'exercices de grammaire. Habituellement, les enseignants éditent manuellement les exercices et leurs solutions et les classent au regard de leur degré de difficulté ou du niveau attendu de l'apprenant. Un courant de recherche dans le Traitement Automatique des Langues (TAL) pour l'apprentissage des langues assisté par ordinateur traite de la génération (semi-)automatique d'exercices. Principalement, ces travaux s'appuient sur des textes extraits du Web, utilisent des techniques d'apprentissage automatique et des techniques d'analyse de textes (par exemple, analyse de phrases, POS tagging, etc.). Ces approches confrontent l'apprenant à des phrases qui ont des syntaxes potentiellement complexes et du vocabulaire varié. En revanche, l'approche que nous proposons dans cette thèse aborde la génération (semi-)automatique d'exercices du type rencontré dans les manuels pour l'apprentissage des langues. Il s'agit, en d'autres termes, d'exercices dont la syntaxe et le vocabulaire sont faits sur mesure pour des objectifs pédagogiques et des sujets donnés. Les approches de génération basées sur des grammaires associent les phrases du langage naturel avec une représentation linguistique fine de leur propriété morpho-syntaxiques et de leur sémantique grâce à quoi il est possible de définir un langage de contraintes syntaxiques et morpho-syntaxiques permettant la sélection de phrases souches en accord avec un objectif pédagogique donné.
Les définitions des mots « sentiment » , « opinion » et « émotion » sont toujours très vagues comme l'atteste aussi le dictionnaire qui semble expliquer un mot en utilisant le deux autres. Tout le monde est affecté par les opinions : les entreprises pour vendre les produits, les gens pour les acheter et, plus en général, pour prendre des décisions, les chercheurs en intelligence artificielle pour comprendre la nature de l'être humain. Aujourd'hui on a une quantité d'information disponible jamais vue avant, mais qui résulte peu accessible. Les mégadonnées (en anglais « big data » ) ne sont pas organisées, surtout pour certaines langues – dont la difficulté à les exploiter. La recherche française souffre d'une manque de ressources « prêt-à-porter » pour conduire des tests. Cette thèse a l'objectif d'explorer la nature des sentiments et des émotions, dans le cadre du Traitement Automatique du Langage et des Corpus. Les contributions de cette thèse sont plusieurs : création de nouvelles ressources pour l'analyse du sentiment et de l'émotion, emploi et comparaison de plusieurs techniques d'apprentissage automatique, et plus important, l'étude du problème sous différents points de vue : classification des commentaires en ligne en polarité (positive et négative), Aspect-Based Sentiment Analysis des caractéristiques du produit recensé. Enfin, un étude psycholinguistique, supporté par des approches lexicales et d'apprentissage automatique, sur le rapport entre qui juge et l'objet jugé.
Si les moteurs de recherche actuels sont efficaces pour trouver des documents correspondant à un besoin d'information large, c'est-à-dire pour fournir une liste de documents sur un sujet donné, ils répondent moins bien à un besoin d'information précise comme " Qui était le président des Etats-Unis en 1978 ? ". Les systèmes de questions-réponses tentent de répondre à de tels besoins. La requête par mots-clefs d'un moteur de recherche classique est remplacée par une question, donc une requête en langage naturel, et la sortie est constituée de la réponse précise à la question, au lieu d'une liste de documents à parcourir. Le domaine de questions-réponses tire ainsi parti des possibilités actuelles en Recherche d'Information (RI), mais lui apporte une interaction facilitée avec l'utilisateur, grâce à la manipulation et la compréhension du langage naturel hérités du Traitement Automatique des Langues (TAL). L'objectif de ce travail est d'étudier précisément comment accéder à l'information précise recherchée, et en particulier ce qui caractérise une réponse à une question d'un point de vue du TAL. Nous nous demanderons ainsi ce que peut être une réponse à une question et quel niveau de connaissances linguistiques doit être utilisé par le système pour reconnaître le lien entre une question et une réponse potentielle. Nous nous sommes intéressée à la problématique de ce lien entre question et réponse sous un angle syntaxique, afin de déterminer l'impact de ce type de connaissances dans un procesus de recherche d'information précises.
La recherche d'information (RI) dans des documents semi-structurés (écrits en XML en pratique) combine des aspects de la RI traditionnelle et ceux de l'interrogation de bases de données. La structure a une importance primordiale, mais le besoin d'information reste vague. L'unité de recherche est variable (un paragraphe, une figure, un article complet…). Par ailleurs, la flexibilité du langage XML autorise des manipulations du contenu qui provoquent parfois des ruptures arbitraires dans le flot naturel du texte. Les problèmes posés par ces caractéristiques sont nombreux, que ce soit au niveau du pré-traitement des documents ou de leur interrogation. Face à ces problèmes, nous avons étudié les solutions spécifiques que pouvait apporter le traitement automatique de la langue (TAL). Nous avons ainsi proposé un cadre théorique et une approche pratique pour permettre l'utilisation des techniques d'analyse textuelle en faisant abstraction de la structure. Nous avons également conçu une interface d'interrogation en langage naturel pour la RI dans les documents XML, et proposé des méthodes tirant profit de la structure pour améliorer la recherche des éléments pertinents.
Cette thèse traite d'une approche, guidée par une ontologie, conçue pour annoter les documents d'un corpus où chaque document décrit une entité de même type. Dans notre contexte, l'ensemble des documents doit être annoté avec des concepts qui sont en général trop spécifiques pour être explicitement mentionnés dans les textes. De plus, les concepts d'annotation ne sont représentés au départ que par leur nom, sans qu'aucune information sémantique ne leur soit reliée. Enfin, les caractéristiques des entités décrites dans les documents sont incomplètes. La phase de peuplement (1) ajoute dans l'ontologie des informations provenant des documents du corpus mais aussi du Web des données (Linked Open Data ou LOD). Le LOD représente aujourd'hui une source prometteuse pour de très nombreuses applications du Web sémantique à condition toutefois de développer des techniques adaptées d'acquisition de données. Dans le cadre de SAUPODOC, le peuplement de l'ontologie doit tenir compte de la diversité des données présentes dans le LOD : propriétés multiples, équivalentes, multi-valuées ou absentes. Les correspondances à établir, entre le vocabulaire de l'ontologie à peupler et celui du LOD, étant complexes, nous proposons un modèle pour faciliter leur spécification. Puis, nous montrons comment ce modèle est utilisé pour générer automatiquement des requêtes SPARQL et ainsi faciliter l'interrogation du LOD et le peuplement de l'ontologie. Celle-ci, une fois peuplée, est ensuite enrichie(2) avec les concepts d'annotation et leurs définitions qui sont apprises grâce à des exemples de documents annotés. Un raisonnement sur ces définitions permet enfin d'obtenir les annotations souhaitées. Des expérimentations ont été menées dans deux domaines d'application, et les résultats, comparés aux annotations obtenues avec des classifieurs, montrent l'intérêt de l'approche.
Cette nouvelle ère de petits UAV qui peuplent actuellement l'espace aérien soulève de nombreuses préoccupations en matière de sécurité, en raison de l'absence de pilote à bord et de la nature moins précise des capteurs. Cela nécessite des approches intelligentes pour faire face aux situations d'urgence qui se produiront inévitablement pour toutes les catégories d'opérations d'UAV telles que définies par l'AESA (Agence européenne de la sécurité aérienne). Les limitations matérielles de ces petits véhicules suggèrent l'utilisation de la redondance analytique plutôt que la pratique habituelle de la redondance matérielle dans l'aviation humaine. Au cours de cette étude, des pratiques d'apprentissage automatique sont mises en œuvre afin de diagnostiquer les défaillances d'un petit drone à voilure fixe afin d'éviter le fardeau de la modélisation précise nécessaire au diagnostic par le modèle. Une méthode de classification supervisée, SVM (Support Vector Machines), permet de classer les défauts. Les données utilisées pour diagnostiquer les défauts sont les mesures de gyroscope et d'accéléromètre. L'idée de restreindre le jeu de données aux mesures d'accéléromètre et de gyroscope est de vérifier la capacité de classification de la méthode, avec un jeu de puces petit et peu coûteux, sans avoir à accéder aux données du pilote automatique, telles que les informations d'entrée de commande. Ce travail aborde les défauts dans les surfaces de contrôle d'un UAV. Plus précisément, les défauts considérés sont la surface de contrôle coincée en angle et la perte d'efficacité. Tout d'abord, un modèle d'aéronef est simulé. Ce modèle n'est pas utilisé pour la conception d'algorithmes FDD (Fault Detection and Diagnosis), mais est utilisé pour générer des données. Des données simulées sont utilisées à la place des données de vol pour isoler les effets probables du contrôleur sur le diagnostic, ce qui peut compliquer une étude préliminaire sur les FDD pour les drones. Les résultats montrent que pour les mesures simulées, SVM donne des résultats très précis sur la classification des défauts de perte d'efficacité sur les surfaces de contrôle. Ces résultats prometteurs appellent un complément d'investigation afin d'évaluer les performances du SVM en matière de classification des anomalies avec les données de vol. Des vols réels ont été organisés pour générer des données de vol erronées en manipulant le pilote automatique open source, Paparazzi. Toutes les données et le code sont disponibles dans le système de partage de code et de versions, Github. La formation est interrompue en raison du besoin de données étiquetées et du fardeau informatique lié à la phase de réglage des classificateurs. Les résultats montrent que, d'après les données de vol, SVM donne un score F1 de 0,98 pour la classification des failles bloquées à la surface de contrôle. En ce qui concerne les défauts de perte d'efficacité, il est nécessaire de recourir à certaines techniques d'ingénierie, impliquant l'ajout de mesures antérieures, pour obtenir les mêmes performances de classification. Un résultat prometteur est découvert lorsque les spinors sont utilisés comme caractéristiques au lieu de vitesses angulaires. Les résultats montrent qu'en utilisant les spinors pour la classification, la précision de la classification est considérablement améliorée, en particulier lorsque les classificateurs ne sont pas accordés. À l'aide de spinors et d'un noyau Gaussian, un classifieur sans accord donne un score F1 de 0,9555, soit 0,2712 lorsque les mesures gyroscopiques ont été utilisées. En résumé, ce travail montre que SVM donne une performance satisfaisante pour la classification des défauts sur les gouvernes d'un drone à l'aide de données de vol.
L'idée directrice est que la coordination efficace entre les membres d'une équipe améliore la productivité et réduit les erreurs individuelles et collectives. Cette thèse traite de la mise en place et du maintien de la coordination au sein d'une équipe de travail composée d'agents et d'humains interagissant dans un EVCF.L'objectif de ces recherches est de doter les agents virtuels de comportements conversationnels permettant la coopération entre agents et avec l'utilisateur dans le but de réaliser un but commun. Nous proposons une architecture d'agents Collaboratifs et Conversationnels, dérivée de l'architecture Belief-Desire-Intention (C2-BDI), qui gère uniformément les comportements délibératifs et conversationnels comme deux comportements dirigés vers les buts de l'activité collective. Nous proposons un modèle intégré de la coordination fondé sur l'approche des modèles mentaux partagés, afin d'établir la coordination au sein de l'équipe de travail composée d'humains et d'agents. Nous soutenons que les interactions en langage naturel entre les membres d'une équipe modifient les modèles mentaux individuels et partagés des participants. Enfin, nous décrivons comment les agents mettent en place et maintiennent la coordination au sein de l'équipe par le biais de conversations en langage naturel. Afin d'établir un couplage fort entre la prise de décision et le comportement conversationnel collaboratif d'un agent, nous proposons tout d'abord une approche fondée sur la modélisation sémantique des activités humaines et de l'environnement virtuel via le modèle mascaret puis, dans un second temps, une modélisation du contexte basée sur l'approche Information State. Ces représentations permettent de traiter de manière unifiée les connaissances sémantiques des agents sur l'activité collective et sur l'environnement virtuel ainsi que des informations qu'ils échangent lors de dialogues. State nous permet de doter les agents C2BDI de capacités communicatives leur permettant de s'engager pro-activement dans des interactions en langue naturelle en vue de coordonner efficacement leur activité avec les autres membres de l'équipe. De plus, nous définissons les protocoles conversationnels collaboratifs favorisant la coordination entre les membres de l'équipe. Enfin, nous proposons dans cette thèse un mécanisme de prise de décision s'inspirant de l'approche BDI qui lie les comportements de délibération et de conversation des agents. Nous avons mis en oeuvre notre architecture dans trois différents scénarios se déroulant dans des EVCF. Nous montrons que les comportements conversationnels collaboratifs multipartites des agents C2BDI facilitent la coordination effective de l'utilisateur avec les autres membres de l'équipe lors de la réalisation d'une tâche partagée.
De nombreux domaines ont intérêt à étudier les points de vue exprimés en ligne, que ce soit à des fins de marketing, de cybersécurité ou de recherche avec l'essor des humanités numériques. Nous proposons dans ce manuscrit deux contributions au domaine de la fouille de points de vue, axées sur la difficulté à obtenir des données annotées de qualité sur les médias sociaux. Notre première contribution est un jeu de données volumineux et complexe de 22853 profils Twitter actifs durant la campagne présidentielle française de 2017. C'est l'un des rares jeux de données considérant plus de deux points de vue et, à notre connaissance, le premier avec un grand nombre de profils et le premier proposant des communautés politiques recouvrantes. Ce jeu de données peut être utilisé tel quel pour étudier les mécanismes de campagne sur Twitter ou pour évaluer des modèles de détection de points de vue ou des outils d'analyse de réseaux. Nous proposons ensuite deux modèles génériques semi-supervisés de détection de points de vue, utilisant une poignée de profils-graines, pour lesquels nous connaissons le point de vue, afin de catégoriser le reste des profils en exploitant différentes proximités inter-profils. En effet, les modèles actuels sont généralement fondés sur les spécificités de certaines plateformes sociales, ce qui ne permet pas l'intégration de la multitude de signaux disponibles. En construisant des proximités à partir de différents types d'éléments disponibles sur les médias sociaux, nous pouvons détecter des profils suffisamment proches pour supposer qu'ils partagent une position similaire sur un sujet donné, quelle que soit la plateforme. Notre premier modèle est un modèle ensembliste séquentiel propageant les points de vue grâce à un graphe multicouche représentant les proximités entre les profils. En utilisant des jeux de données provenant de deux plateformes, nous montrons qu'en combinant plusieurs types de proximité, nous pouvons correctement étiqueter 98% des profils. Notre deuxième modèle nous permet d'observer l'évolution des points de vue des profils pendant un événement, avec seulement un profil-graine par point de vue. Ce modèle confirme qu'une grande majorité de profils ne changent pas de position sur les médias sociaux, ou n'expriment pas leur revirement.
Face à la masse grandissante des données textuelles présentes sur le Web, le résumé automatique d'une collection de documents traitant d'un sujet particulier est devenu un champ de recherche important du Traitement Automatique des Langues. Les expérimentations décrites dans cette thèse s'inscrivent dans cette perspective. SemEval 2014 pour la langue anglaise et des ressources que nous avons construites pour la langue française. Les bonnes performances des mesures proposées nous ont amenés à les utiliser dans une tâche de résumé multi-documents, qui met en oeuvre un algorithme de type PageRank. Le système a été évalué sur les données de DUC 2007 pour l'anglais et le corpus RPM2 pour le français. Les résultats obtenus par cette approche simple, robuste et basée sur une ressource aisément disponible dans de nombreuses langues, se sont avérés très encourageants
L'importance des systèmes collaboratifs a considérablement augmenté au cours des dernières années. La majorité de nouvelles applications sont conçues de manière distribuée pour répondre aux besoins du travail collaboratif. RCE) qui permettent la manipulation de divers objets partagés, tels que les pages wiki ou les articles scientifiques par plusieurs personnes réparties dans le temps et dans l'espace Dans cette thèse, nous proposons un modèle de contrôle d'accès générique basé sur l'approche de réplication optimiste du document partagé ainsi que sa politique de contrôle d'accès. Pour cela, nous proposons une approche optimiste de contrôle d'accès dans la mesure où un utilisateur peut violer temporairement la politique de sécurité. Vu l'absence d'une solution d'annulation générique et correcte, nous proposons une étude théorique du problème d'annulation et nous concevons une solution générique basée sur une nouvelle sémantique de l'opération identité. Afin de valider notre approche tous nos algorithmes ont été implémentés en Java et testés sur la plateforme distribuée Grid'5000
Cette thèse s'inscrit dans le domaine de la recherche d'information (RI) et la recommandation de lecture. Elle a pour objets : — La création de nouvelles approches de recherche de documents utilisant des techniques de combinaison de résultats, d'agrégation de données sociales et de reformulation de requêtes ; — La création d'une approche de recommandation utilisant des méthodes de RI et les graphes entre les documents. Deux collections de documents ont été utilisées. Une collection qui provient de l'évaluation CLEF (tâche Social Book Search-SBS) et la deuxième issue du domaine des sciences humaines et sociales (OpenEdition, principalement Revues.org). La modélisation des documents de chaque collection repose sur deux types de relations : — Dans la première collection (CLEF SBS), les documents sont reliés avec des similarités calculées par Amazon qui se basent sur plusieurs facteurs (achats des utilisateurs, commentaires, votes, produits achetés ensemble, etc.) ; — Dans la deuxième collection (OpenEdition), les documents sont reliés avec des relations de citations (à partir des références bibliographiques). Le manuscrit est structuré en deux parties. La première partie « état de l'art » regroupe une introduction générale, un état de l'art sur la RI et sur les systèmes de recommandation. La deuxième partie « contributions » regroupe un chapitre sur la détection de comptes rendus de lecture au sein de la collection OpenEdition (Revues.org), un chapitre sur les méthodes de RI utilisées sur des requêtes complexes et un dernier chapitre qui traite l'approche de recommandation proposée qui se base sur les graphes.
La recherche de réponses à des questions relève de deux disciplines : le traitement du langage naturel et la recherche d'information. L'émergence de l'apprentissage profond dans plusieurs domaines de recherche tels que la vision par ordinateur, le traitement du langage naturel etc. a conduit à l'émergence de modèles de bout en bout. Dans le cadre du projet GoASQ, l'objectif est d'étudier, comparer et combiner différentes approches pour répondre à des questions formulées en langage naturel sur des données textuelles, en domaine ouvert et en domaine biomédical. Ce travail se concentre principalement sur 1) la construction de modèles permettant de traiter des ensembles de données à petite et à grande échelle, et 2) l'exploitation de connaissances sémantiques pour répondre aux questions par leur intégration dans les différents modèles. Nous visons à fusionner des connaissances issues de textes libres, d'ontologies, de représentations d'entités, etc. Afin de faciliter l'utilisation des modèles neuronaux sur des données de domaine de spécialité, nous nous plaçons dans le cadre de l'adaptation de domaine. Nous avons proposé deux modèles de tâches de QR différents, évalués sur la tâche BIOASQ de réponse à des questions biomédicales. Nous montrons par nos résultats expérimentaux que le modèle de QR ouvert convient mieux qu'une modélisation de type Compréhension machine. Nous pré-entrainons le modèle de Compréhension machine, qui sert de base à notre modèle, sur différents ensembles de données pour montrer la variabilité des performances. Nous constatons que l'utilisation d'un ensemble de données particulier pour le pré-entraînement donne les meilleurs résultats lors du test et qu'une combinaison de quatre jeux de données donne les meilleurs résultats lors de l'adaptation au domaine biomédical. Nous avons testé des modèles de langage à grande échelle, comme BERT, qui sont adaptés à la tâche de réponse aux questions. Les performances varient en fonction du type des données utilisées pour pré-entrainer BERT. Ainsi, le modèle de langue appris sur des données biomédicales, BIOBERT, constitue le meilleur choix pour le QR biomédical. Nous avons annoté manuellement et automatiquement un jeu de données par les variantes des réponses de BIOASQ et montré l'importance d'apprendre un modèle de QR avec ces variantes. Ces types sont ensuite utilisés pour mettre en évidence les entités dans les jeux de données, ce qui montre des améliorations sur l'état de l'art. Par ailleurs l'exploitation de représentations vectorielles d'entités dans les modèles se montre positif pour le domaine ouvert. Nous faisons l'hypothèse que les résultats obtenus à partir de modèles d'apprentissage profond peuvent être encore améliorés en utilisant des traits sémantiques et des traits collectifs calculés à partir des différents paragraphes sélectionnés pour répondre à une question. Nous utilisons des modèles de classification binaires pour améliorer la prédiction de la réponse parmi les K candidats à l'aide de ces caractéristiques, conduisant à un modèle hybride qui surpasse les résultats de l'état de l'art. Enfin, nous avons évalué des modèles de QR ouvert sur des ensembles de données construits pour les tâches de Compréhension machine et Sélection de phrases. Nous montrons la différence de performance lorsque la tâche à résoudre est une tâche de QR ouverte et soulignons le fossé important qu'il reste à franchir dans la construction de modèles de bout en bout pour la tâche complète de réponse aux questions.
Cette thèse présente : 1) le développement d'une nouvelle approche pour trouver des associations directes entre des paires d'éléments liés indirectement à travers diverses caractéristiques communes, 2) l'utilisation de cette approche pour associer directement des fonctions biologiques aux domaines protéiques (ECDomainMiner et GODomainMiner) et pour découvrir des interactions domaine-domaine, et enfin 3) l'extension de cette approche pour annoter de manière complète à partir des domaines les structures et les séquences des protéines. En utilisant des associations de domaines ayant acquis des annotations fonctionnelles inférées, et en tenant compte des informations de taxonomie, des milliers de règles d'annotation ont été générées automatiquement. Ensuite, ces règles ont été utilisées pour annoter des séquences de protéines dans la base de données TrEMBL
Le travail comprend la définition d'un modèle pour les prédicats d'opinion et leurs arguments (la source, le sujet et le message), la création d'un lexique de prédicats d'opinions auxquels sont associées des informations provenant du modèle et la réalisation de trois systèmes informatiques. En effet ces différents travaux obtiennent des scores qui se situent entre 63% et 89,5%.Par ailleurs, en sus des systèmes réalisés pour l'identification de l'opinion, notre travail a débouché sur la construction de plusieurs ressources pour l'espagnol : un lexique de prédicats d'opinions, un corpus de 13000 mots avec des annotations sur les opinions et un corpus de 40000 mots avec des annotations sur les prédicats d'opinion et les sources.
Transmettre une émotion ou exprimer une sensation par le discours nécessite de l'interlocuteur qu'il construise des stratégies discursives situées aussi bien dans une situation de communication spécifique que dans une langue-culture particulière. Cette étude sur les langues de spécialité en contexte équatorien met l'accent sur le locuteur et les stratégies qu'il met en place pour exprimer ses émotions pendant une session de dégustation. En effet, plusieurs interrogations se posent dont celle de la caractérisation d'un genre discursif lié à une praxis sociale : « Existe-t-il un genre de discours dont le protocole de création et le prototype discursif seraient spécifiques à la dégustation ? » ;  celle d'une production discursive liée à l'identité de son locuteur utilisant différents lexiques-grammaires pour communiquer sur un même produit : « Les experts et les consommateurs utilisent-ils les mêmes stratégies discursives ? » ;  celle d'une terminologie située : « Comment les représentations sociales et les pratiques discursives modulent-elles les contributions des locuteurs impliqués ? » . En effet, ces interrogations doivent permettre de caractériser l'expression de la sensorialité située dans un contexte professionnel spécifique-la dégustation en espagnol dans la culture équatorienne-tout en proposant une lecture scientifique des choix terminologiques utilisés comme descripteurs sensoriels et hédoniques incluant une dimension émotionnelle. Cette recherche s'inscrit donc dans le cadre théorique de la sémantique cognitive, de la linguistique de corpus et de l'analyse textométrique des pratiques discursives représentatives de praxis socioculturelles. La méthodologie proposée utilise les outils informatiques et la technique de fouille de texte propres au traitement automatique des langues appliquée au trois corpus suivants. 1. La compilation d'un corpus de textes diachroniques à partir de la production écrite de la presse spécialisée des dix dernières années dans lequel seront identifiés les descripteurs actuellement utilisés pour décrire les sensations et les émotions qui peuvent être perçues lors de la dégustation d'un carré de chocolat. 2. La compilation d'un corpus de textes produits par les locuteurs s'exprimant sur la conceptualisation et la signification qu'ils possèdent de ces mêmes descripteurs. Ce corpus sera compilé sans contact avec le produit, le chocolat, de façon à mettre l'accent sur la dimension cognitive de la représentation sociale des descripteurs sélectionnés. 3. Et la compilation d'un corpus de textes à partir de productions orales visant la mise en discours de ces descripteurs pour exprimer des émotions et des sensations perçues lors d'une session de dégustation de chocolat. L'analyse de ces différents corpus offre un objet d'étude authentique et représentatif de la perception que peut avoir le locuteur lorsqu'il met en place des stratégies discursives visant à communiquer une émotion ou exprimer une sensation. Cette méthode permet une interprétation qualitative à partir d'une analyse quantitative de données authentiques de façon à proposer une description des différentes stratégies d'expression de l'expérience sensorielle. Parallèlement à la constitution de ces corpus, l'application d'un questionnaire avant la session de dégustation permet de caractériser le locuteur et de collecter des informations sur les représentations sociales qu'il pourrait convoquer dans ses choix discursifs. Au-delà de la constitution d'un corpus inédit et de son analyse discursive, cette recherche présente l'intérêt de proposer une étude des stratégies utilisées pour exprimer les émotions et les sensations dans un contexte de dégustation, elle permet ainsi d'offrir une description d'un genre discursif spécifique qui peut apporter des améliorations au discours produit pour s'adresser aux consommateurs potentiels.
Le principal objectif du projet constitue également sa principale originalité. Il consiste à extraire automatiquement la structure narrative des séries TV. Les séries télévisées actuelles sont basées sur des structures complexes impliquant plusieurs arcs d'histoires entrelacés dans le même épisode. La première barrière scientifique est donc liée à l'écart dit sémantique entre l'histoire réelle véhiculée par les séries TV et le type d'information qui peut être automatiquement extrait et traité par les programmes informatiques. Une approche naïve pourrait être basée uniquement sur la recherche d'informations telles que la structure temporelle de la collection, les personnages (qui sont-ils ? Mais, ce problème difficile ne peut pas être résolu en utilisant uniquement un seul de ces aspects (Qui ?
La thèse, effectuée dans le cadre d'une bourse CIFRE, et prolongeant un des aspects du projet ANR Traouiero, aborde d'abord la production, l'extension et l'amélioration de corpus multilingues par traduction automatique (TA) et post-édition contributive (PE). Des améliorations fonctionnelles et techniques ont été apportées aux logiciels SECTra et iMAG, et on a progressé vers une définition générique de la structure d'un corpus multilingue, multi-annoté et multimédia, pouvant contenir des documents classiques aussi bien que des pseudo-documents et des méta-segments. Dans le cadre d'un projet interne sur le site du LIG et d'un projet (TABE-FC) en coopération avec l'université de Xiamen, on a pu démontrer l'intérêt de l'apprentissage incrémental en TA statistique, sous certaines conditions, grâce à une expérience qui s'est étalée sur toute la thèse. La troisième partie est consacrée à des contributions et mises à disposition de supports informatiques et de ressources. Les principales se placent dans le cadre du projet COST MUMIA de l'EU et résultent de l'exploitation de la collection CLEF-2011 de 1,5 M de brevets partiellement multilingues. De grosses mémoires de traductions en ont été extraites (17,5 M segments), 3 systèmes de TA en ont été tirés, et un site Web de support à la RI multilingue sur les brevets a été construit. On décrit aussi la réalisation en cours de JianDan-eval, une plate-forme de construction, déploiement et évaluation de systèmes de TA.
La modélisation de processus complexes peut impliquer un grand nombre de variables ayant entre elles une structure de corrélation compliquée. Par exemple, les phénomènes spatiaux possèdent souvent une forte régularité spatiale, se traduisant par une corrélation entre variables d'autant plus forte que les régions correspondantes sont proches. Le formalisme des graphes pondérés permet de capturer de manière compacte ces relations entre variables, autorisant la formalisation mathématique de nombreux problèmes d'analyse de données spatiales. Nous présentons une stratégie de préconditionnement pour l'algorithme generalized forward-backward, spécifiquement adaptée à la résolution de problèmes structurés par des graphes pondérés présentant une grande variabilité de configurations et de poids. Ces algorithmes présentent des performances supérieures à l'état de l'art pour des tâches d'agrégations de données geostatistiques. La seconde partie de ce document se concentre sur le développement d'un nouveau modèle qui étend les chaînes de Markov à temps continu au cas des graphes pondérés non orientés généraux. Ce modèle autorise la prise en compte plus fine des interactions entre noeuds voisins pour la prédiction structurée, comme illustré pour la classification supervisée de tissus urbains.
Cette thèse présente une approche d'analyse des textes non-standardisé qui consiste à modéliser une chaine de traitement permettant l'annotation automatique de textes à savoir l'annotation grammaticale en utilisant une méthode d'étiquetage morphosyntaxique et l'annotation sémantique en mettant en œuvre un système de reconnaissance des entités nommées. Dans ce contexte, nous présentons un système d'analyse du Moyen Français qui est une langue en pleine évolution dont l'orthographe, le système flexionnel et la syntaxe ne sont pas stables. Les textes en Moyen Français se singularisent principalement par l'absence d'orthographe normalisée et par la variabilité tant géographique que chronologique des lexiques médiévaux. L'objectif est de mettre en évidence un système dédié à la construction de ressources linguistiques, notamment la construction des dictionnaires électroniques, se basant sur des règles de morphologie. Ensuite, nous présenterons les instructions que nous avons établies pour construire un étiqueteur morphosyntaxique qui vise à produire automatiquement des analyses contextuelles à l'aide de grammaires de désambiguïsation. Finalement, nous retracerons le chemin qui nous a conduits à mettre en place des grammaires locales permettant de retrouver les entités nommées. De ce fait, nous avons été amenés à constituer un corpus MEDITEXT regroupant des textes en Moyen Français apparus entre le fin du XIIIème et XVème siècle.
Nous traitons dans cette thèse la modélisation de la coarticulation par les réseaux de neurones, dans l'objectif de synchroniser l'animation d'un visage virtuel 3D à de la parole. Nous proposons dans cette thèse un modèle de coarticulation, c'est-à-dire un modèle qui prédit les trajectoires spatiales des articulateurs à partir de la parole. Nous exploiterons pour cela un modèle séquentiel, les réseaux de neurones récurrents (RNN), et plus particulièrement les Gated Recurrent Units, capables de considérer la dynamique de l'articulation au cœur de leur modélisation. Malheureusement, la quantité de données classiquement disponible dans les corpus articulatoires et audiovisuels semblent de prime-abord faibles pour une approche deep learning. Pour pallier cette difficulté, nous proposons une stratégie permettant de fournir au modèle des connaissances sur les gestes articulatoires du locuteur dès son initialisation. La robustesse des RNNs nous a permis d'implémenter notre modèle de coarticulation pour prédire les mouvements des lèvres pour le français et l'allemand, et de la langue pour l'anglais et l'allemand. L'évaluation du modèle fut réalisée par le biais de mesures objectives de la qualité des trajectoires et par des expériences permettant de valider la bonne réalisation des cibles articulatoires critiques. Nous avons également réalisé une évaluation perceptive de la qualité de l'animation des lèvres du visage parlant. Enfin, nous avons conduit une analyse permettant d'explorer les connaissances phonétiques acquises par le modèle après apprentissage.
Les systèmes de traduction automatique obtiennent aujourd'hui de bons résultats sur certains couples de langues comme anglais – français, anglais – chinois, anglais – espagnol, etc. Toutefois, la recherche sur la traduction automatique pour des paires de langues dites « peu dotés » doit faire face au défi du manque de données. Nous avons ainsi abordé le problème d'acquisition d'un grand corpus de textes bilingues parallèles pour construire le système de traduction automatique probabiliste. L'originalité de notre travail réside dans le fait que nous nous concentrons sur les langues peu dotées, où des corpus de textes bilingues parallèles sont inexistants dans la plupart des cas. Ce manuscrit présente notre méthodologie d'extraction d'un corpus d'apprentissage parallèle à partir d'un corpus comparable, une ressource de données plus riche et diversifiée sur l'Internet. Nous proposons trois méthodes d'extraction. La première méthode suit l'approche de recherche classique qui utilise des caractéristiques générales des documents ainsi que des informations lexicales du document pour extraire à la fois les documents comparables et les phrases parallèles. Cependant, cette méthode requiert des données supplémentaires sur la paire de langues. La deuxième méthode est une méthode entièrement non supervisée qui ne requiert aucune donnée supplémentaire à l'entrée, et peut être appliquée pour n'importe quelle paires de langues, même des paires de langues peu dotées. La dernière méthode est une extension de la deuxième méthode qui utilise une troisième langue, pour améliorer les processus d'extraction de deux paires de langues. Les méthodes proposées sont validées par des expériences appliquées sur la langue peu dotée vietnamienne et les langues française et anglaise.
Cette thèse décrit les applications du traitement automatique des langues (TAL) à la gestion des risques industriels. Elle se concentre sur le domaine de l'aviation civile, où le retour d'expérience (REX) génère de grandes quantités de données, sous la forme de rapports d'accidents et d'incidents. Nous commençons par faire un panorama des différentes types de données générées dans ce secteur d'activité. Nous analysons les documents, comment ils sont produits, collectés, stockés et organisés ainsi que leurs utilisations. Nous montrons que le paradigme actuel de stockage et d'organisation est mal adapté à l'utilisation réelle de ces documents et identifions des domaines problématiques ou les technologies du langage constituent une partie de la solution. Répondant précisément aux besoins d'experts en sécurité, deux solutions initiales sont implémentées : la catégorisation automatique de documents afin d'aider le codage des rapports dans des taxonomies préexistantes et un outil pour l'exploration de collections de rapports, basé sur la similarité textuelle. En nous basant sur des observations de l'usage de ces outils et sur les retours de leurs utilisateurs, nous proposons différentes méthodes d'analyse des textes issus du REX et discutons des manières dont le TAL peut être appliqué dans le cadre de la gestion de la sécurité dans un secteur à haut risque. En déployant et évaluant certaines solutions, nous montrons que même des aspects subtils liés à la variation et à la multidimensionnalité du langage peuvent être traités en pratique afin de gérer la surabondance de données REX textuelles de manière ascendante
Cette thèse porte sur l'analyse et la génération de mouvements expressifs pour des personnages humains virtuels. Sur la base de résultats d'état de l'art issus de trois domaines de recherche différents (la perception des émotions et du mouvement biologique, la reconnaissance automatique des émotions et l'animation de personnages), une représentation en faible dimension des mouvements a été proposée. Cette représentation est constituée de trajectoires spatio-temporelles des extrémités des chaînes articulées (tête, mains et pieds) et du pelvis. Nous avons soutenu que cette représentation est à la fois appropriée et suffisante pour caractériser le contenu expressif du mouvement humain et pour contrôler la génération de mouvements corporels expressifs. Pour étayer ces affirmations, cette thèse propose :  i.) Une nouvelle base de données de mouvements capturés. Cette base de données a été inspirée par la théorie du théâtre physique et contient des exemples de différentes catégories de mouvements (à savoir des mouvements périodiques, des mouvements fonctionnels, des mouvements spontanés et des séquences de mouvements théâtraux), produit avec des états émotionnels distincts (joie, tristesse, détente, stress et neutre) et interprétés par plusieurs acteurs. ii.) Une étude perceptuelle et une approche basée classification automatique conçus pour évaluer qualitativement et quantitativement la quantité d'information liée aux émotions encore véhiculée et codée dans la représentation proposée. Nous avons observé que, bien que de légères différences dans la performance aient été trouvées par rapport à la situation dans laquelle le corps entier a été utilisé, notre représentation conserve la plupart des qualités de mouvements liées à l'expression de l'affect et d'émotions. iii.) Un système de synthèse de mouvement capable : (a) de reconstruire des mouvements du corps entier à partir de la représentation à faible dimension proposée, (b) de produire de nouvelles trajectoires extrémités expressives (incluant la trajectoire du pelvis). Une évaluation quantitative et qualitative des mouvements du corps entier générés montre que ces mouvements sont aussi expressifs que les mouvements enregistrés à partir d'acteurs humains.
L'extraction d'information spatiale à partir de données textuelles est désormais un sujet de recherche important dans le domaine du Traitement Automatique du Langage Naturel (TALN). Elle répond à un besoin devenu incontournable dans la société de l'information, en particulier pour améliorer l'efficacité des systèmes de Recherche d'Information (RI) pour différentes applications (tourisme, aménagement du territoire, analyse d'opinion, etc.). De tels systèmes demandent une analyse fine des informations spatiales contenues dans les données textuelles disponibles (pages web, courriels, tweets, SMS, etc.). Cependant, la multitude et la variété de ces données ainsi que l'émergence régulière de nouvelles formes d'écriture rendent difficile l'extraction automatique d'information à partir de corpus souvent peu standards d'un point de vue lexical voire syntaxique. Afin de relever ces défis, nous proposons, dans cette thèse, des approches originales de fouille de textes permettant l'identification automatique de nouvelles variantes d'entités et relations spatiales à partir de données textuelles issues de la communication médiée. Ces approches sont fondées sur trois principales contributions qui sont cruciales pour fournir des méthodes de navigation intelligente. Notre première contribution se concentre sur la problématique de reconnaissance et d'extraction des entités spatiales à partir de corpus de messages courts (SMS, tweets) marqués par une écriture peu standard. La deuxième contribution est dédiée à l'identification de nouvelles formes/variantes de relations spatiales à partir de ces corpus spécifiques. Enfin, la troisième contribution concerne l'identification des relations sémantiques associées à l'information spatiale contenue dans les textes.
Dans cette thèse, nous étudions plusieurs modèles de collaboration entre l'ingénierie logiciel et le web sémantique. L'objectif principal de notre travail est de fournir au développeur des outils pour concevoir la matière déclarative une couche de métier "exécutable" d'une application afin de simuler son fonctionnement et de montrer ainsi la conformité de l'application par rapport aux exigences du client au début du cycle de vie du logiciel. Un autre avantage de cette approche est de permettre au développeur de partager et de réutiliser la description de la couche de métier d'une application dans un domaine en utilisant l'ontologie. Celle-ci est appelée "patron d'application". La réutilisation de la description de la couche de métier d'une application est un aspect intéressant à l'ingénier logiciel. C'est le point-clé que nous voulons considérer dans cette thèse. Dans la première partie de notre travail, nous traitons la modélisation de la couche de métier. Nous présentons d'abord une approche fondée sur l'ontologie pour représenter les processus de métiers et les règles de métiers et nous montrons comment vérifier la cohérence du processus et de l'ensemble des règles de métier. Puis, nous présentons le mécanisme de vérification automatique de la conformité d'un processus de métier avec un ensemble de règles de métier. La deuxième partie de cette thèse est consacrée à définir une méthodologie, dite de personnalisation, de création une application à partir d'un "patron d'application". Cette méthode permettra à l'utilisateur d'utiliser un patron d'application pour créer sa propre application en évitant les erreurs de structures et les erreurs sémantiques. Nous introduisons à la fin de cette partie, la description d'une plateforme expérimentale permettant d'illustrer la faisabilité des mécanismes proposés dans cette thèse.
A travers cette étude, se présente une perspective dynamique de l'annotation sémantique. Cette perspective considère le passage du temps et les flux permanents de documents qui font croître les collections et étendre leurs systèmes d'annotation. Nous apportons également une vision de la qualité des systèmes d'annotations basée sur la notion d'accès à l'information et de cohérence. Dans notre vision de la qualité, l'information de vocabulaire d'annotation est la complexité à parcourir par un utilisateur à la recherche d'un certain sujet. Pour répondre au problème de la dynamique dans l'annotation sémantique, cette thèse propose une architecture modulaire pour l'annotation sémantique dynamique. Cette architecture modélise les activités impliquées dans le processus d'annotation sémantique en modules abstraits avec des considérations particulières en fonction de la tâche spécifique. Comme cas d'étude, nous prenons l'annotation de blogs. Nous rassemblâmes un corpus contenant jusqu'à 10 ans de billets de blog annotés avec des catégories et des tags et analysé les habitudes d'annotation observées. Nous explorons la suggestion automatique de tags et de catégories afin de mesurer l'impact de la dynamique dans le système d'annotation. Certaines stratégies pour faire face à cet impact ont été évaluées pour caractériser l'importance de l'âge des exemples. Enfin, nous proposons un cadre de trois mesures de qualité et une méthode interactive pour récupérer la qualité d'un système d'indexation basé sur des annotations sémantiques appuyée par les métriques. Les mesures ont été évaluées au fil du temps pour observer la dégradation de la qualité de l'indexation. Une série d'exemples étudiés sont présentés pour observer la performance des mesures visant à guider la restructuration du système d'annotation de l'indexation.
Bien que les vaccins représentent une avancée majeure pour la santé publique, le risque d'effets secondaires constitue une menace réelle pour leur acceptation par le grand public et les professionnels de santé. La France se classe, d'ailleurs, comme le pays manifestant la plus grande défiance envers le vaccin. Cela s'est souvent traduit pas des couvertures vaccinales faibles. L'origine de cette perte de confiance est, entre autres, liée à la polémique intense autour du vaccin anti-hépatite B (HB) et le risque de sclérose en plaques dans les années 1990. Le but de cette thèse est d'évaluer le lien potentiel entre vaccination et démyélinisation, en considérant deux exemples : les vaccins anti-VHB et anti-papillomavirus (HPV). Une approche méthodologique, progressive, fondée sur les preuves a été utilisée pour les deux vaccins. La génération d'hypothèse a considéré la plausibilité biologique, les rapports de cas publiés, les analyses de disproportionnalité conduites dans le système américain de pharmacovigilance des vaccins (i.e., Vaccine Adverse Event Reporting System (VAERS)), et l'analyse des signaux détectés par la surveillance passive. Concernant la vaccination anti-VHB, des analyses attendu/observé ont également été menées à partir des cas confirmés rapportés à la pharmacovigilance française dans les années 1990. Les résultats restent mitigés pour les deux vaccins. Pour la vaccination anti-VHB, une plausibilité biologique faible et indirecte, l'analyse du signal français détecté en 1996 qui a révélé une disjonction complète entre les populations cible et rejointe, ainsi que les résultats des analyses de disproportionnalité dans VAERS sont des éléments en faveur d'une possible association entre démyélinisation centrale et vaccin anti-VHB. Cependant, ni la méta-analyse, ni les analyses attendu/observé (bien que leurs conclusions puissent être renversées par un facteur modéré de sous-notification), n'ont fourni de résultat statistiquement significatif. En tout état de cause, si un risque en excès existait, il serait faible et ne concernerait que l'adulte. Les recommandations actuelles qui minimisent la probabilité d'exposition à l'âge adulte, sont donc plus que justifiées. Pour la vaccination anti-HPV, le risque de démyélinisation centrale semble, à ce jour, écarté. Néanmoins, un doute subsiste concernant un possible risque en excès pour le syndrome de Guillain et Barré. En conclusion, une association forte avec un risque de démyélinisation semble à exclure pour les deux vaccins, rendant la balance bénéfice/risque largement positive pour ces produits, dès lors qu'ils sont utilisés dans leurs populations cibles. Dans ce contexte, une communication scientifique, indépendante et claire est la clé pour promouvoir les programmes de vaccination et créer la confiance et l'adhésion du grand public. Le futur de la pharmacovigilance des vaccins pourrait résider dans la mise en place d'un réseau collaboratif entre le patient et son médecin, via l'utilisation de SMS et smartphones, comme cela existe déjà en Australie. En plus de collecter les effets secondaires des vaccins, cela représenterait une opportunité unique de placer le patient au coeur du système de surveillance, lui offrant une voix et contribuant à restaurer sa confiance envers les vaccins, et même envers les décideurs de santé publique.
Considérant un point de vue général de cette thèse aborde le problème de trouver, à partir d'un ensemble de blocs de construction, un sous-ensemble qui procure une solution à un problème donné. Ceci est fait en tenant compte de la compatibilité de chacun des blocs de construction par rapport au problème et l'aptitude d'interaction entre ces parties pour former ensemble une solution. Dans la perspective notamment de la thèse sont les blocs de construction de méta-modèles et le problème donné est une description d'un problème peut être résolu en utilisant un logiciel et d'être résolu en utilisant un système multi-agents. Il peut également indiquer que le problème ne peut être résolu par ce paradigme. Le processus adressée par la thèse consiste en les étapes principales suivantes :  (1) A travers un processus de caractérisation on analyse la description du problème pour localiser le domaine de solutions, puis choisissez une liste de candidats des méta-modèles. (3) On crée un système multi-agents où chaque agent représente un candidat méta-modèle. Dans cette société les agents interagissent les uns avec les autres pour trouver un groupe de méta-modèles qui est adapté pour représenter une solution donnée. Les agents utilisent des critères appropriés pour chaque méta-modèle à représenter. Cette thèse se concentre sur la fourniture d'un processus et un outil prototype pour résoudre plutôt la dernière étape de la liste. Par conséquent, le chemin proposé a été créé à l'aide de plusieurs concepts de la méta-analyse, l'intelligence artificielle de coopération, de la cognition bayésienne, incertitude, la probabilité et statistique.
La thèse a pour objectif d'observer le fonctionnement des verbes de manière de mouvement en tant que prédicats en polonais et en français. Il s'avère que les structures argumentales des prédicats sont responsables de certaines caractéristiques et des « comportements » grammaticaux des membres de cette classe verbale dans les deux langues. Le premier chapitre expose les bases théoriques et méthodologiques adoptées dans les analyses. Les chapitres suivants sont consacrés spécifiquement aux verbes déterminés polonais (chapitre 2), aux verbes indéterminés en polonais (chapitre 3) et aux verbes de manière de mouvement français (chapitre 4). Le chapitre 5 est un bilan comparatif. Par là même, ils acceptent de modifier les propriétés aspectuelles déterminées par le sens du lexème verbal.
La vérification automatique est aujourd'hui devenue un domaine central de recherche en informatique. Depuis plus de 25 ans, une riche théorie a été développée menant à de nombreux outils, à la fois académiques et industriels, permettant la vérification de propriétés booléennes-celles qui peuvent être soit vraies soit fausses. Les besoins actuels évoluent vers une analyse plus fine, c'est-à-dire plus quantitative. L'extension des techniques de vérification aux domaines quantitatifs a débuté depuis 15 ans avec les systèmes probabilistes. Cependant, de nombreuses autres propriétés quantitatives existent, telles que la durée de vie d'un équipement, la consommation énergétique d'une application, la fiabilité d'un programme, ou le nombre de résultats d'une requête dans une base de données. Exprimer ces propriétés requiert de nouveaux langages de spécification, ainsi que des algorithmes vérifiant ces propriétés sur une structure donnée. Cette thèse a pour objectif l'étude de plusieurs formalismes permettant de spécifier de telles propriétés, qu'ils soient dénotationnels-expressions régulières, logiques monadiques ou logiques temporelles-ou davantage opérationnels, comme des automates pondérés, éventuellement étendus avec des jetons. Un premier objectif de ce manuscript est l'étude de résultats d'expressivité comparant ces formalismes. En particulier, on donne des traductions efficaces des formalismes dénotationnels vers celui opérationnel. Ces objets, ainsi que les résultats associés, sont présentés dans un cadre unifié de structures de graphes. Ils peuvent, entre autres, s'appliquer aux mots et arbres finis, aux mots emboîtés (nested words), aux images ou aux traces de Mazurkiewicz. Par conséquent, la vérification de propriétés quantitatives de traces de programmes (potentiellement récursifs, ou concurrents), les requêtes sur des documents XML (modélisant par exemple des bases de données), ou le traitement des langues naturelles sont des applications possibles. On s'intéresse ensuite aux questions algorithmiques que soulèvent naturellement ces résultats, tels que l'évaluation, la satisfaction et le model checking. En particulier, on étudie la décidabilité et la complexité de certains de ces problèmes, en fonction du semi-anneau sous-jacent et des structures considérées (mots, arbres...). Finalement, on considère des restrictions intéressantes des formalismes précédents. Certaines permettent d'étendre l'ensemble des semi-anneau sur lesquels on peut spécifier des propriétés quantitatives. Une autre est dédiée à l'étude du cas spécial de spécifications probabilistes : on étudie en particulier des fragments syntaxiques de nos formalismes génériques de spécification générant uniquement des comportements probabilistes.
Il est bien connu qu'une nouvelle étude, menée au sein d'une entreprise, est souvent semblable à une étude précédente et par conséquent, peut être structurée selon un processus de référence commun au type d'étude correspondant. La difficulté majeure réside dans la formalisation de cette démarche métier. Les approches traditionnelles de capitalisation des connaissances s'appuyant sur des verbalisations d'experts ont montré leur limite. Souvent réalisées en dehors de l'activité réelle, les experts omettent des détails qui peuvent être d'importance. Notre thèse repose sur l'idée qu'il est possible de construire le processus opérationnel mis en œuvre lors des activités collaboratives de conception, à partir des traces enregistrées lors de l'utilisation de l'outil numérique par les acteurs métier. Le processus opérationnel ainsi construit pourra aider les acteurs métiers et les experts à prendre du recul sur le travail réel et à formaliser et enrichir les démarches métier de l'entreprise. Notre travail s'est déroulé au sein du laboratoire ERPI (Équipe de Recherche sur les Processus Innovatifs) de l'Université de Lorraine et en collaboration avec la société TDC Software dans le cadre d'une thèse CIFRE. Les contributions que nous proposons sont les suivantes :  • Un double cycle de capitalisation pour des activités instrumentées, • Une approche globale de gestion des démarches métier, • Une ontologie OntoProcess modélisant des aspects organisationnels génériques (séparant clairement des concepts liés aux traces et d'autres liés aux démarches métier) et des extensions métiers spécifiques à l'outil utilisé, • Un système multi-agents supportant l'approche globale de gestion des démarches métiers et s'appuyant sur l'ontologie OntoProcess, • Un système à base de traces permettant de construire un processus opérationnel à partir des traces enregistrées lors d'une étude
La popularisation des réseaux sociaux et des documents numériques a rapidement accru l'information disponible sur Internet. Cependant, cette quantité massive de données ne peut pas être analysée manuellement. Nous analysons également d'autres tâches du TALN (la représentation des mots, la similarité sémantique ou encore la compression de phrases et de groupes de phrases) pour générer des résumés cross-lingues plus stables et informatifs. La plupart des applications du TALN, celle du résumé automatique y compris, utilisent une mesure de similarité pour analyser et comparer le sens des mots, des séquences de mots, des phrases et des textes. L'une des façons d'analyser cette similarité est de générer une représentation de ces phrases tenant compte de leur contenu. Le sens des phrases est défini par plusieurs éléments, tels que le contexte des mots et des expressions, l'ordre des mots et les informations précédentes. En analysant ces problèmes, nous proposons un modèle de réseau de neurones combinant des réseaux de neurones récurrents et convolutifs pour estimer la similarité sémantique d'une paire de phrases (ou de textes) en fonction des contextes locaux et généraux des mots. Sur le jeu de données analysé, notre modèle a prédit de meilleurs scores de similarité que les systèmes de base en analysant mieux le sens local et général des mots mais aussi des expressions multimots. Afin d'éliminer les redondances et les informations non pertinentes de phrases similaires, nous proposons de plus une nouvelle méthode de compression multiphrase, fusionnant des phrases au contenu similaire en compressions courtes. Pour ce faire, nous modélisons des groupes de phrases semblables par des graphes de mots. Notre approche surpasse les systèmes de base en générant des compressions plus informatives et plus correctes pour les langues française, portugaise et espagnole. Enfin, nous combinons les méthodes précédentes pour construire un système de résumé de texte cross-lingue. Notre système génère des résumés cross-lingue de texte en analysant l'information à la fois dans les langues source et cible, afin d'identifier les phrases les plus pertinentes. Inspirés par les méthodes de résumé de texte par compression en analyse monolingue, nous adaptons notre méthode de compression multiphrase pour ce problème afin de ne conserver que l'information principale. En analysant les résumés cross-lingues depuis l'anglais, le français, le portugais ou l'espagnol, vers l'anglais ou le français, notre système améliore les systèmes par extraction de l'état de l'art pour toutes ces langues.
La popularité des médias sociaux en ligne (Online Social Media-OSM) est fortement liée à la qualité du contenu généré par l'utilisateur (User Generated Content-UGC) et la protection de la vie privée des utilisateurs. En se basant sur la définition de la qualité de l'information, comme son aptitude à être exploitée, la facilité d'utilisation des OSM soulève de nombreux problèmes en termes de la qualité de l'information ce qui impacte les performances des applications exploitant ces OSM. Ces problèmes sont causés par des individus mal intentionnés (nommés spammeurs) qui utilisent les OSM pour disséminer des fausses informations et/ou des informations indésirables telles que les contenus commerciaux illégaux. La propagation et la diffusion de telle information, dit spam, entraînent d'énormes problèmes affectant la qualité de services proposés par les OSM. La majorité des OSM (comme Facebook, Twitter, etc.) sont quotidiennement attaquées par un énorme nombre d'utilisateurs mal intentionnés. Cependant, les techniques de filtrage adoptées par les OSM se sont avérées inefficaces dans le traitement de ce type d'information bruitée, nécessitant plusieurs semaines ou voir plusieurs mois pour filtrer l'information spam. En effet, plusieurs défis doivent être surmontées pour réaliser une méthode de filtrage de l'information bruitée. Les défis majeurs sous-jacents à cette problématique peuvent être résumés par : (i) données de masse ; (ii) vie privée et sécurité ; (iii) hétérogénéité des structures dans les réseaux sociaux ; (iv) diversité des formats du UGC ; (v) subjectivité et objectivité. Notre travail s'inscrit dans le cadre de l'amélioration de la qualité des contenus en termes de messages partagés (contenu spam) et de profils des utilisateurs (spammeurs) sur les OSM en abordant en détail les défis susmentionnés. Comme le spam social est le problème le plus récurant qui apparaît sur les OSM, nous proposons deux approches génériques pour détecter et filtrer le contenu spam :  i) La première approche consiste à détecter le contenu spam (par exemple, les tweets spam) dans un flux en temps réel. ii) La seconde approche est dédiée au traitement d'un grand volume des données relatives aux profils utilisateurs des spammeurs (par exemple, les comptes Twitter).
Le besoin croissant en assistance humaine a poussé les chercheurs à développer des systèmes de dialogue automatiques, intelligents et infatigables qui conversent avec les humains dans un langage naturel pour devenir soit leurs assistants virtuels ou leurs compagnons. L'industrie des systèmes de dialogue est devenue populaire cette dernière décennie, ainsi, plusieurs systèmes ont été développés par des industriels comme des académiques. Dans le cadre de cette thèse, nous étudions les systèmes de dialogue basés sur la recherche de réponse qui cherchant la réponse la plus appropriée à la conversation parmi un ensemble de réponses prédéfini. Le défi majeur de ces systèmes est la compréhension de la conversation et l'identification des éléments qui décrivent le problème et la solution qui sont souvent implicites. La plupart des approches récentes sont basées sur des techniques d'apprentissage profond qui permettent de capturer des informations implicites. Souvent, ces approches sont complexes ou dépendent fortement du domaine. Nous proposons une approche de recherche de réponse de bout en bout, simple, efficace et indépendante du domaine et qui permet de capturer ces informations implicites. Nous effectuons également plusieurs analyses afin de déterminer des pistes d'amélioration.
L'objectif de notre travail, qui émane d'une demande de la sous-direction Assurance Qualité du CNES (Centre National d'Études Spatiales), est d'augmenter la clarté des spécifications techniques rédigées par les ingénieurs préalablement à la réalisation de systèmes spatiaux. L'importance des spécifications (et en particulier des exigences qui les composent) pour la réussite des projets de grande envergure est en effet désormais largement reconnue, de même que les principaux problèmes liés à l'utilisation de la langue naturelle (ambiguïtés, flou, incomplétude) sont bien identifiés. L'originalité de notre démarche consiste à systématiquement vérifier nos hypothèses sur un corpus d'exigences (constitué à partir d'authentiques spécifications de projets spatiaux) à l'aide de techniques et d'outils de traitement automatique du langage existants, dans l'optique de proposer un ensemble cohérent de règles (nouvelles ou inspirées de règles plus anciennes) qui puissent ainsi être vérifiées semi-automatiquement lors de l'étape de spécification et qui soient conformes aux pratiques de rédaction des ingénieurs du CNES. Pour cela, nous nous appuyons notamment sur l'hypothèse de l'existence d'un genre textuel, que nous tentons de prouver par une analyse quantitative, ainsi que sur les notions de normalisation et normaison. Notre méthodologie combine les approches corpus-based et corpus-driven en tenant compte à la fois des règles imposées par deux autres langues contrôlées (dont l'adéquation avec des données réelles est discutée au travers d'une analyse plus qualitative) et des résultats offerts par des outils de text mining.
L'évaluation de l'analyseur a été effectuée sur des textes malais et un texte indonésien. Cet analyseur utilise : un ensemble de règles, une liste d'exceptions, une liste restreinte de bases dépourvues de toute information linguistique et des techniques de reconnaissance des formes. L'algorithme d'analyse est non déterministe. Les bases analysées sont traitées hors contexte. L'évaluation des résultats de l'analyseur a donné environ 97% d'analyses correctes et un taux d'erreur inférieur à 2%. Très peu de bases affixées n'ont pas été analysées (taux inférieur à 0,5%).
Le texte a été le moyen dominant de stocker des données dans des systèmes informatiques et d'envoyer des informations sur le Web. L'extraction de représentations significatives hors du texte a été un élément clé de la modélisation de langage afin de traiter des tâches de la NLP telles que la classification de texte. Ces représentations peuvent ensuite former des groupes que l'on peut utiliser pour des problèmes d'apprentissage supervisé. Plus spécifiquement, on peut utiliser ces groupes linguistiques à des fins de régularisation. Enfin, ces structures peuvent être utiles dans un autre domaine important, le calcul de distance entre documents texte. Tout d'abord, en examinant de nouvelles représentations de texte basées sur des graphes. Ensuite, nous avons étudié comment des groupes de ces représentations peuvent aider à la régularisation dans des modèles d'apprentissage automatique pour la classification de texte. Enfin, nous avons traité des ensembles et de la mesure des distances entre les documents, en utilisant les groupes linguistiques que nous avons proposés, ainsi que des approches basées sur des graphes. Dans la première partie de la thèse, nous avons étudié les représentations de texte basées sur des graphes. Transformer le texte en graphiques n'est pas anodin et existait avant même que les mots incorporés ne soient introduits dans la communauté NLP. Dans notre travail, nous montrons que les représentations graphiques de texte peuvent capturer efficacement des relations telles que l'ordre, la sémantique ou la structure syntaxique. De plus, ils peuvent être créés rapidement tout en offrant une grande polyvalence pour de multiples tâches. Dans la deuxième partie, nous nous sommes concentrés sur la régularisation structurée du texte. Les données textuelles souffrent du problème de dimensionnalité, créant de grands espaces de fonctionnalités. La régularisation est essentielle pour tout modèle d'apprentissage automatique, car elle permet de remédier au surajustement. Dans notre travail, nous présentons de nouvelles approches pour la régularisation de texte, en introduisant de nouveaux groupes de structures linguistiques et en concevant de nouveaux algorithmes. Dans la dernière partie de la thèse, nous étudions de nouvelles méthodes pour mesurer la distance dans le mot englobant l'espace. Premièrement, nous présentons diverses méthodes pour améliorer la comparaison entre des documents constitués de vecteurs de mots. Ensuite, en présentant la comparaison des documents comme une correspondance bipartite pondérée, nous montrons comment nous pouvons apprendre des représentations cachées et améliorer les résultats pour la tâche de classification de texte. Enfin, nous conclurons en résumant les principaux points de la contribution totale et en discutant des orientations futures.
Comment les structures sociales et sémantiques d'une communauté scientifique guident-elles les dynamiques de collaboration à venir ? Dans cette thèse, nous combinons des techniques de traitement automatique des langues et des méthodes provenant de l'analyse de réseaux complexes pour analyser une base de données de publications scientifiques dans le domaine de la linguistique computationnelle : l'ACL Anthology. Notre objectif est de comprendre le rôle des collaborations entre les chercheurs dans la construction du paysage sémantique du domaine, et, symétriquement, de saisir combien ce même paysage influence les trajectoires individuelles des chercheurs et leurs interactions. Nous employons des outils d'analyse du contenu textuel pour extraire des textes des publications les termes correspondant à des concepts scientifiques. Ces termes sont ensuite connectés aux chercheurs pour former un réseau socio-sémantique, dont nous modélisons la dynamique à différentes échelles. Nous construisons d'abord un modèle statistique, à base de régressions logistiques multivariées, qui permet de quantifier le rôle respectif des propriétés sociales et sémantiques de la communauté sur la dynamique microscopique du réseau socio-sémantique. Nous reconstruisons par la suite l'évolution du champ de la linguistique computationelle en créant différentes cartographies du réseau sémantique, représentant les connaissances produites dans le domaine, mais aussi le flux d'auteurs entre les différents champs de recherche du domaine. En résumé, nos travaux ont montré que la combinaison des méthodes issues du traitement automatique des langues et de l'analyse des réseaux complexes permet d'étudier d'une manière nouvelle l'évolution des domaines scientifiques.
La focalisation prosodique désigne le soulignement d'un constituant dans un énoncé au moyen de différentes ressources prosodiques, en particulier l'accentuation et l'intonation. Plusieurs fonctions sont attribuées à la focalisation : le marquage des différentes catégories de focus, ainsi que des fonctions emphatiques (ici appelées insistance et expressivité). Cette thèse a pour principal but de savoir si la focalisation et ses fonctions présentent des propriétés spécifiques dans le phonogenre de la parole interprétée, c'est-à-dire l'oralisation d'un texte écrit mémorisé au préalable par le locuteur (généralement un comédien). Cette question présente un intérêt pour la linguistique et la phonétique à plusieurs titres. Tout d'abord, les différences de réalisation prosodique entre les fonctions de la focalisation sont encore mal connues. Par ailleurs, peu d'études ont été consacrées aux caractéristiques prosodiques de la parole interprétée. Enfin, notre thèse présente un apport sur le plan méthodologique à travers le protocole relativement novateur de ses deux expériences. Dans une expérience de production, des locuteurs ont reproduit des conversations spontanées en parole lue et en parole interprétée. Un groupe d'experts en prosodie a ensuite relevé les occurrences de focalisation dans le corpus et a effectué une classification fonctionnelle de ces occurrences. Nous avons également mené une expérience de perception afin de comparer la réalisation des fonctions de la focalisation indépendamment du phonogenre. Malgré un taux d'accord entre experts relativement faible (ce qui soulève plusieurs questions méthodologiques et théoriques), nos analyses révèlent plusieurs résultats originaux. La fréquence d'occurrence de la focalisation est la plus élevée en parole interprétée, suivie de la parole lue. Ce résultat confirme notre prédiction et suggère que la parole interprétée est un phonogenre favorable à l'étude de la focalisation. Une forte relation est observée entre la fonction d'insistance et le trait d'accentuation initiale, ce qui confirme de nombreuses études précédentes. Le phonogenre se révèle en revanche avoir très peu d'influence sur la réalisation de la focalisation et de ses fonctions. Ce résultat est dû selon nous à un manque de données et au fait que certains traits prosodiques n'ont pas été pris en compte dans l'analyse.
Cette thèse fournit une description du kakabé, une langue mandée parlée en Guinée, basée sur un corpus et avec un focus sur le système phonologique. Elle contient une brève esquisse grammaticale et deux parties qui portent sur l'analyse phonologique : la phonologie segmentale et la phonologie suprasegmentale. Le kakabé applique diverses stratégies d'adaptation des emprunts (principalement, du poular et du français), telles que l'épenthèse vocalique, la simplification d'agglomérations consonantiques. Le kakabé est une langue à ton (H vs. L), avec downdrift, relèvement du ton H, un ton flottant L, et un certain nombre de processus tonals, tels que l'insertion du ton H, la propagation du ton,l'aplatissement du contour HLH. En conséquence, la distance entre les tons lexicaux sous‐jacents et leur réalisation de surface peut être assez importante. Chacun des processus tonals est appliqué dans une unité prosodique particulière. Par conséquent, les processus tonals participent au découpage du discours en unités prosodiques. Le kakabé comporte des tons de frontière qui servent à signaler la force illocutoire de l'énoncé. Les tons lexicaux et les tons de frontièrecoexistent avec des opérations intonatives sur la courbe F0. Les appendices comprennent un dictionnaire kakabé-français, composé de 3400 entrées, et le corpus de 12 heures de textes en kakabé, transcrits, glosés, traduits etaccompagnés des fichiers vidéos et audios.
Dans le cadre du développement international du mouvement du libre accès aux publications scientifiques, cette thèse analyse plus précisément la situation française dans le contexte européen. Cette analyse a été menée à travers une démarche de recherche-action, au sein d'un groupe d'acteurs du Groupement français des industries de l'information (GFII) concernés par le libre accès. Nous cherchons tout d'abord à mettre en évidence les forces motrices du développement du libre accès en nous appuyant sur une méthodologie prospective développée au LIPSOR/CNAM.Les résultats nous ont conduit à contribuer à la conception d'un site d'information dont la finalité est l'affichage des politiques des éditeurs nationaux en matière d'auto-archivage afin d'accompagner les pratiques de dépôts au niveau national. L'analyse prospective a en effet révélé l'importance des embargos pour les équilibres financiers des éditeurs. De façon plus distanciée, nous amorçons également une réflexion sur l'impact réel du libre accès sur deux moteurs semblant jouer un rôle croissant dans l'économie de la connaissance, à savoir la créativité et l'interdisciplinarité.
Apprendre, c'est extraire et condenser de l'information pertinente à partir d'un certain nombre d'indices concordants. Pourtant, les indices auxquels un apprenant a, ou pourrait avoir, accès, peuvent sembler insuffisants pour ce à quoi il veut parvenir. Le caractère partiel ou insuffisant des indices disponibles est souvent évoqué dans le contexte de l'acquisition du langage : le sens des mots et la grammaire que les individus maîtrisent semblent nécessiter davantage d'information que celle présente dans leur environnement. Si une grande partie de la recherche se focalise sur l'identification de sources d'information négligées ou sous-estimées jusqu'alors, ici nous déplaçons le point de vue pour explorer plutôt les connaissances que ces sources d'information doivent permettre de construire. Nous proposons dans ce travail que les informations disponibles restent souvent insuffisantes pour atteindre le niveau de connaissance qu'un locuteur compétent pense avoir ; toutefois, la même information peut être suffisante pour qu'un apprenant extraie l'information nécessaire au traitement cognitif du langage. Plus généralement, il pourrait y avoir un décalage entre ce que l'on pense apprendre et ce que l'on apprend véritablement. Dans l'introduction de ce travail, nous commençons par une présentation des études qui suggèrent l'existence d'un décalage entre notre intuition de ce qu'est le sens des mots, et la manière dont le sens des mots est effectivement traité par le cerveau. Nous explorons ce décalage dans le contexte plus général de la manière dont les individus, qui ne disposent que d'informations limitées, tentent de comprendre le monde. Ensuite, dans le premier chapitre, nous examinons comment une faible quantité d'information peut malgré tout favoriser l'acquisition de la grammaire, en utilisant un paradigme écologique. Nos résultats démontrent que la connaissance d'une petite poignée de mots peut générer un cercle vertueux dans l'acquisition de la grammaire et du vocabulaire chez le bébé. Puis, dans le deuxième chapitre, nous tâchons de découvrir si cet ensemble réduit d'informations peut suffire à amorcer l'acquisition d'un savoir productif qui permettra ensuite de généraliser cette connaissance à des situations nouvelles, à travers les âges : du bébé à l'adulte en passant par l'enfant d'âge scolaire. Nos données suggèrent que les bébés et les adultes généralisent les caractéristiques d'un nouvel élément grammatical afin de comprendre de nouveaux mots, mais que les enfants d'âge scolaire ne généralisent pas. Nous expliquons nos résultats dans le contexte de la recherche existante sur la généralisation, en soulignant la possibilité que ce qui est considéré comme savoir est peut-être défini par rapport à un seuil, plutôt que par rapport à une définition idéale objective. Finalement, dans le troisième chapitre, nous étudions si l'information qui donne l'impression de refléter directement l'état des connaissances de quelqu'un d'autre (i.e., entendre une traduction directe : 'Bamoule', ça veut dire 'chat') est véritablement plus utile, ou seulement plus attractive. Nos résultats suggèrent que ce type d'information 'clé-en-main'augmente systématiquement la confiance de l'apprenant, mais a des effets variables sur la performance objective. A travers trois chapitres et dix expériences, nous proposons une série de principes qui définissent la connaissance : (1) une petite quantité d'information peut mener loin, (2) combien d'information suffit semble dépendre d'un seuil adaptable, et (3) le cerveau paraît être avide de certitude. Nous suggérerons que l'ensemble des informations à la disposition d'un individu peut être suffisant pour générer des connaissances, mais pas forcément le type de connaissances que l'on a l'intuition d'avoir. Ainsi, pour mieux comprendre comment on apprend, nous devons étudier ce que signifie vraiment 'savoir'pour l'apprenant.
L'apprentissage profond (deep learning) est une avancée majeure de l'intelligence artificielle de ces dernières années. L'apprentissage profond s'est rapidement imposé comme un standard dans de plusieurs domaines en pulvérisant les records des précédentes méthodes de l'état de l'art. Ses domaines de prédilection sont principalement l'analyse d'image et le traitement du langage naturel. Dans ce projet, nous nous intéressons plus spécifiquement à la prédiction de phénotypes (diagnostique, pronostique, réponse à un traitement) à partir de données transcriptomiques. La grande majorité des articles publiés a moins de deux ans et parmi eux, seulement une poignée s'intéresse à la prédiction de phénotypes. Alors que les réseaux de neurones profonds traitant des images ou du langage naturel sont construits à partir de plusieurs centaines de milliers ou millions d'exemples, les jeux de données transcriptomiques contiennent très peu de patients (&lt ; 5000). À cause de ce faible nombre d'exemples, l'apprentissage des réseaux de neurones profonds se heurte à des problèmes de sur-apprentissage. Nous nous inspirerons des méthodes les plus performantes actuellement sur les images tel que les adversarial autoencoders, les ladder networks, ou les generative adversarial networks, et les adapterons au problème spécifique des données d'expression. Une autre solution serait de pouvoir d'intégrer des jeux de données issus de différentes plateformes de production de données transcriptomiques. Ce problème d'intégration n'a pour le moment pas de solutions satisfaisantes en raison des liens complexes entre les mesures des différentes plateformes. Une mesure d'expression sur une plateforme A peut à la fois se repartir sur plusieurs expressions et se mélanger à d'autres expressions sur une plateforme B. Notre idée est de faire apprendre par un réseau de neurones la correspondance entre deux plateformes. Pour cela, nous nous inspirerons des méthodes de transfert de domaine telles que CycleGAN. Un autre défi majeur concerne l'interprétation des réseaux de neurones et de leurs prédictions. L'Union Européenne a adopté récemment un texte imposant aux utilisateurs d'algorithmes d'apprentissage automatique d'être capables d'expliquer les décisions d'un modèle prédictif. Il y a donc un réel besoin de rendre les réseaux de neurones plus interprétables et cela est particulièrement vrai dans le domaine médical pour deux raisons. Premièrement, il est important de s'assurer que le réseau de neurones base ses prédictions sur une représentation fiable des patients et ne se concentre pas sur des artefacts non pertinents présents dans les données d'apprentissage. Sans explications des prédictions, les médecins ne peuvent pas faire confiance à un réseau de neurones quelles que soient ses performances. L'interprétation des réseaux de neurones se fera avec des méthodes de perturbation et de rétro-propagation du signal de sortie. Dans ce projet, nous collaborons avec l'Institut de Cardiométabolisme et de Nutrition (ICAN) qui nous fournira le profil d'expression génétique de plusieurs milliers de patients atteint de maladies cadiométabolomiques. Ils nous donneront également accès aux données du projet européen Metacardis qui contient les données cliniques et d'expression de gènes de plus de 2000 patients. Résultat attendu : Nous proposerons de nouvelles approches d'apprentissage profond adaptées à la prédiction à partir de données transcriptomiques et pour l'interprétation biologique des réseaux de neurones. Nous mettrons également à disposition de la communauté les réseaux que nous aurons construits sur des grands jeux de données. Par « transfert learning » , les chercheurs pourront utiliser nos réseaux sur des jeux de données plus petits et améliorer leurs résultats. L'objectif est de fournir pour les données transcriptomiques un équivalent des réseaux VGG ou ResNet en analyse d'image.
Le sujet de cette thèse s'inscrit dans le cadre général de la Recherche d'Information et la gestion des données massives et distribuées. Elle a pour but de retourner à l'utilisateur d'un système de recherche d'information des objets résultats qui sont riches et porteurs de connaissances. Ces objets n'existent pas en tant que tels dans les sources. Ils sont construits par assemblage (ou configuration ou agrégation) de fragments issus de diffèrentes sources. Les sources peuvent être non spécifiées dans l'expression de la requête mais découvertes dynamiquement lors de la recherche. Nous nous intéressons particulièrement à l'exploitation des dépendances de données pour optimiser les accès aux sources distribuées. Dans ce cadre, nous proposons une approche pour l'un des sous processus de systèmes de RIA, principalement le processus d'indexation/organisation des documents. Nous considérons dans cette thèse, les systèmes de recherche d'information orientés graphes (graphes RDF). Utilisant les relations dans les graphes, notre travail s'inscrit dans le cadre de la recherche d'information agrégative relationnelle (Relational Aggregated Search) où les relations sont exploitées pour agréger des fragments d'information. Nous proposons d'optimiser l'accès aux sources d'information dans un système de recherche d'information agrégative. Ces sources contiennent des fragments d'information répondant partiellement à la requête. L'objectif est de minimiser le nombre de sources interrogées pour chaque fragment de la requête, ainsi que de maximiser les opérations d'agrégations de fragments dans une même source. Nous proposons d'effectuer cela en réorganisant la/les base(s) de graphes dans plusieurs clusters d'information dédiés aux requêtes agrégatives. Ces clusters sont obtenus à partir d'une approche de clustering sémantique ou structurel des prédicats des graphes RDF. Pour le clustering structurel, nous utilisons les algorithmes d'extraction de sous-graphes fréquents et dans ce cadre nous élaborons une étude comparative des performances de ces algorithmes. Pour le clustering sémantique, nous utilisons les métadonnées descriptives des prédicats dont nous appliquons des outils de similarité textuelle sémantique.
L'augmentation des données disponibles dans presque tous les domaines soulève la nécessité d'utiliser des algorithmes pour l'analyse automatisée des données. Cette nécessité est mise en évidence dans la maintenance prédictive, où l'objectif est de prédire les pannes des systèmes en observant continuellement leur état, afin de planifier les actions de maintenance à l'avance. Ces observations sont générées par des systèmes de surveillance habituellement sous la forme de séries temporelles et de journaux d'événements et couvrent la durée de vie des composants correspondants. Le principal défi de la maintenance prédictive est l'analyse de l'historique d'observation afin de développer des modèles prédictifs. Dans ce sens, l'apprentissage automatique est devenu omniprésent puisqu'il fournit les moyens d'extraire les connaissances d'une grande variété de sources de données avec une intervention humaine minimale. L'objectif de cette thèse est d'étudier et de résoudre les problèmes dans l'aviation liés à la prévision des pannes de composants à bord. La quantité de données liées à l'exploitation des avions est énorme et, par conséquent, l'évolutivité est une condition essentielle dans chaque approche proposée. Cette thèse est divisée en trois parties qui correspondent aux différentes sources de données que nous avons rencontrées au cours de notre travail. Dans la première partie, nous avons ciblé le problème de la prédiction des pannes des systèmes, compte tenu de l'historique des Post Flight Reports. Nous avons proposé une approche statistique basée sur la régression précédée d'une formulation méticuleuse et d'un prétraitement / transformation de données. Notre méthode estime le risque d'échec avec une solution évolutive, déployée dans un environnement de cluster en apprentissage et en déploiement. À notre connaissance, il n'y a pas de méthode disponible pour résoudre ce problème jusqu'au moment où cette thèse a été écrite. La deuxième partie consiste à analyser les données du livre de bord, qui consistent en un texte décrivant les problèmes d'avions et les actions de maintenance correspondantes. Le livre de bord contient des informations qui ne sont pas présentes dans les Post Flight Reports bien qu'elles soient essentielles dans plusieurs applications, comme la prédiction de l'échec. Cependant, le journal de bord contient du texte écrit par des humains, il contient beaucoup de bruit qui doit être supprimé afin d'extraire les informations utiles. Nous avons abordé ce problème en proposant une approche basée sur des représentations vectorielles de mots. Notre approche exploite des similitudes sémantiques, apprises par des neural networks qui ont généré les représentations vectorielles, afin d'identifier et de corriger les fautes d'orthographe et les abréviations. Enfin, des mots-clés importants sont extraits à l'aide du Part of Speech Tagging. Dans la troisième partie, nous avons abordé le problème de l'évaluation de l'état des composants à bord en utilisant les mesures des capteurs. Dans les cas considérés, l'état du composant est évalué par l'ampleur de la fluctuation du capteur et une tendance à l'augmentation monotone. Dans notre approche, nous avons formulé un problème de décomposition des séries temporelles afin de séparer les fluctuations de la tendance en résolvant un problème convexe.
Cette thèse porte sur la capture, l'annotation, la synthèse et l'évaluation des mouvements des mains et des bras pour l'animation d'avatars communiquant en Langues des Signes (LS). Actuellement, la production et la diffusion de messages en LS dépendent souvent d'enregistrements vidéo qui manquent d'informations de profondeur et dont l'édition et l'analyse sont difficiles. Les avatars signeurs constituent une alternative prometteuse à la vidéo. Ils sont généralement animés soit à l'aide de techniques procédurales, soit par des techniques basées données. L'animation procédurale donne souvent lieu à des mouvements peu naturels, mais n'importe quel signe peut être produit avec précision. Avec l'animation basée données, les mouvements de l'avatar sont réalistes mais la variété des signes pouvant être synthétisés est limitée et/ou biaisée par la base de données initiale. Privilégiant l'acceptation de l'avatar, nous avons choisi l'approche basée sur les données mais, pour remédier à sa principale limitation, nous proposons d'utiliser les mouvements annotés présents dans une base de mouvements de LS capturés pour synthétiser de nouveaux signes et énoncés absents de cette base. Pour atteindre cet objectif, notre première contribution est la conception, l'enregistrement et l'évaluation perceptuelle d'une base de données de capture de mouvements en Langue des Signes Française (LSF) composée de signes et d'énoncés réalisés par des enseignants sourds de LSF. Notre deuxième contribution est le développement de techniques d'annotation automatique pour différentes pistes d'annotation basées sur l'analyse des propriétés cinématiques de certaines articulations et des algorithmes d'apprentissage automatique existants. Notre dernière contribution est la mise en œuvre de différentes techniques de synthèse de mouvements basées sur la récupération de mouvements par composant phonologique et sur la reconstruction modulaire de nouveaux contenus de LSF avec l'utilisation de techniques de génération de mouvements, comme la cinématique inverse, paramétrées pour se conformer aux propriétés des mouvements réels.
L'objectif du didacticien est d'élaborer une méthode performante dont le contenu et les outils d'enseignement-apprentissage améliorent les compétences phonétiques en langue étrangère. Concernant le contenu pédagogique, les travaux ont montré que les sons et les phonèmes d'une langue inconnue sont traités selon l'organisation de l'espace phonétique et phonologique de la langue maternelle. Les recherches mettent en avant l'intérêt de confronter les systèmes linguistiques afin de prédire les difficultés et les facilités auxquelles seront exposés les apprenants de langue. S'agissant des outils de transmission, les études montrent les effets bénéfiques de l'interdisciplinarité et le rôle pertinent de la musique sur le développement cognitif et des apprentissages. Notre objectif de recherche s'inscrit dans ce contexte scientifique. Notre intérêt est double. D'abord, nous avons tenté d'identifier quel paramètre, inhérent à l'émission en voix chantée et la différenciant de la voix parlée, pouvait faciliter la perception de voyelles non-natives. Ensuite, nous avons souhaité comparer les effets sur la compétence de production de voyelles non-natives de deux méthodes de corrections phonétique, l'une des deux exploitant l'outil « voix chantée » . À travers les résultats de ces études, nous avons essayé de saisir le rôle de l'italien langue maternelle sur la perception et la production du français langue cible. Nos travaux n'ont pas mis en évidence d'effet des modalités fréquence fondamentale et allongement de la durée vocalique sur la discrimination perceptive des voyelles non natives /y/ et /ø/, mais ils suggèrent un rôle du contexte prévocalique sur la perception de la voyelle non-native /y/ en contraste /u/. Nous avons trouvé un effet favorable de la méthode de correction phonétique incluant la pratique chantée sur la production du spectre sonore des voyelles fermées du français, mais pas sur l'évolution des catégories phonologiques à l'intérieur de l'espace acoustique vocalique. Les résultats de ces études soutiennent la théorie que l'enseignement-apprentissage de la phonétique a sa place en classe de langue, et suggèrent que la voix chantée serait, sous certaines conditions, un outil pertinent pour faciliter la perception et la production de voyelles non-natives.
Dans cette thèse, nous traitons le problème de la séparation de sources audio multicanale par réseaux de neurones profonds (deep neural networks, DNNs). Notre approche se base sur le cadre classique de séparation par algorithme espérance-maximisation (EM) basé sur un modèle gaussien multicanal, dans lequel les sources sont caractérisées par leurs spectres de puissance à court terme et leurs matrices de covariance spatiales. Nous explorons et optimisons l'usage des DNNs pour estimer ces paramètres spectraux et spatiaux. À partir des paramètres estimés, nous calculons un filtre de Wiener multicanal variant dans le temps pour séparer chaque source. Nous étudions en détail l'impact de plusieurs choix de conception pour les DNNs spectraux et spatiaux. Nous considérons plusieurs fonctions de coût, représentations temps-fréquence, architectures, et tailles d'ensembles d'apprentissage. Ces fonctions de coût incluent en particulier une nouvelle fonction liée à la tâche pour les DNNs spectraux : le rapport signal-à-distorsion. Nous présentons aussi une formule d'estimation pondérée des paramètres spatiaux, qui généralise la formulation EM exacte. Sur une tâche de séparation de voix chantée, nos systèmes sont remarquablement proches de la méthode de l'état de l'art actuel et améliorent le rapport source-interférence de 2 dB. Sur une tâche de rehaussement de la parole, nos systèmes surpassent la formation de voies GEV-BAN de l'état de l'art de 14%, 7% et 1% relatifs en terme d'amélioration du taux d'erreur sur les mots sur des données à 6, 4 et 2 canaux respectivement
Cette étude présente une typologie des prédicats de mouvement du hongrois. Elle reflète une perception objective et simple du mouvement et de l'espace. Le travail s'inscrit dans le cadre de la théorie des classes d'objets que nous avons appliquée au hongrois. La classification s'appuie sur des propriétés aspectuelles. Ces propriétés sont complétées par des propriétés morpho-syntaxiques nécessaires au traitement automatique. L'aspect contrastif de notre étude a permis de proposer une meilleure description des classes de prédicats du hongrois et de relever les différences morpho-syntaxiques et combinatoires spécifiques des deux langues dans l'expression du mouvement, comme le rôle des préfixes verbaux, des compléments locatifs ainsi que l'importance des prédicats nominaux.
La masse d'informations du domaine juridique, qui ne cesse de s'accroitre, a généré un be-soin capital d'organiser et de structurer les contenus des documents disponibles, et les trans-former ainsi en un guide intelligent, capable d'apporter des réponses complètes et immé-diates à des requêtes en langage naturel. De ce fait, les systèmes de questions-réponses (QASs) répondent parfaitement à ce besoin en offrant les différents mécanismes pour four-nir des réponses adéquates et précises à des questions exprimées en langage naturel. En effet, ce type de systèmes permet à l'utilisateur de poser une question en langue naturel et de recevoir une réponse précise à sa demande au lieu d'un ensemble de documents jugés relevant, comme c'est le cas pour les moteurs de recherche. Notre objectif, est de mettre en place un système de questions-réponses opérant dans le domaine juridique au Maroc qui utilise dans la plupart du temps les langues française et arabe, voir la langue anglaise. Il a pour but de donner une réponse pertinente et concise à des questions dans le domaine juridique, énoncées en langue naturelle par un utilisateur, sans que ce dernier ait à parcourir les documents juridiques pour trouver une réponse à sa ques-tion, ce qui peut être couteux en terme de temps.
Cette thèse a pour objet l'étude de modèles à base d'analogies dans un cadre d'Apprentissage Automatique pour le Traitement Automatique des Langues Naturelles. L'approche analogique apporte une alternative à la fois aux méthodes déductives (inférence de connaissances particulières à partir de connaissances générales) et aux méthodes inductives (inférence de connaissances générales à partir de connaissances particulières). Selon ce mode de raisonnement, l'analyse d'une nouvelle entité s'effectue par comparaison avec les données disponibles ; l'inférence s'effectue directement du particulier au particulier. Dans cette approche, l'abstraction que constitue la connaissance générale impliquée à la fois dans les approches déductives et inductives n'apparaît plus comme une composante nécessaire du modèle. Par ailleurs, cette approche s'accorde bien avec l'organisation paradigmatique des données linguistiques, qui permet de mettre aisément une entité linguistique en relation avec d'autres selon des schémas spécifiques ; la connaissance linguistique reste alors implicitement représentée dans le corpus accumulé et les relations systématiques qu'entretiennent les entités le composant. Cette organisation paradigmatique invite en particulier à considérer des proportions analogiques. Un modèle d'apprentissage est présenté, qui repose sur l'exploitation de proportions analogiques. Nous introduisons la notion d'extension analogique, qui permet d'exprimer la méthode et d'identifier clairement son biais d'apprentissage. Nous proposons également un cadre algébrique formel permettant de donner un sens à la notion de proportion analogique entre objets structurés.
Notre perception est par nature multimodale, i.e. fait appel à plusieurs de nos sens. Pour résoudre certaines tâches, il est donc pertinent d'utiliser différentes modalités, telles que le son ou l'image. Cette thèse s'intéresse à cette notion dans le cadre de l'apprentissage neuronal profond. Pour cela, elle cherche à répondre à une problématique en particulier : comment fusionner les différentes modalités au sein d'un réseau de neurones ? Nous proposons tout d'abord d'étudier un problème d'application concret : la reconnaissance automatique des émotions dans des contenus audio-visuels. Cela nous conduit à différentes considérations concernant la modélisation des émotions et plus particulièrement des expressions faciales. Nous proposons ainsi une analyse des représentations de l'expression faciale apprises par un réseau de neurones profonds. De plus, cela permet d'observer que chaque problème multimodal semble nécessiter l'utilisation d'une stratégie de fusion différente. Enfin, nous nous intéressons à une vision multimodale du transfert de connaissances. En effet, nous détaillons une méthode non traditionnelle pour effectuer un transfert de connaissances à partir de plusieurs sources, i.e. plusieurs modèles pré-entraînés. Pour cela, une représentation neuronale plus générale est obtenue à partir d'un modèle unique, qui rassemble la connaissance contenue dans les modèles pré-entraînés et conduit à des performances à l'état de l'art sur une variété de tâches d'analyse de visages.
À STMicroelectronics, l'équipe de Business Intelligence est confrontée à exploiter quotidiennement des données et des informations pour créer des rapports d'activité afin de superviser la production. Dans une telle organisation industrielle, les produits changent régulièrement et les données peuvent rapidement devenir obsolètes. Par conséquent, au fil du temps, le nombre de rapports crées est de plus en plus important, tandis que les connaissances sur leur création sont perdues. Ceci est illustré dans une évaluation qualitative et quantitative de la partie principale du système de connaissances à STMicroelectronics. Ainsi, des problèmes d'obsolescence, de duplication, de non-centralisation et de prolifération continuent à surgir. Son objectif est de capitaliser efficacement et en permanence les connaissances, tout en ciblant les besoins métier et assurant une solution évolutive. Un système de Business Intelligence pour la Business Intelligence (BI4BI) est proposé. Comme la connaissance est intégrée non seulement dans les systèmes et les outils, mais aussi détenue par les humains et leurs pratiques, notre solution de capitalisation de connaissances proposée implique aussi les utilisateurs et les organisations : elle propose de recueillir les points de vue des utilisateurs pour les intégrer dans la représentation des connaissances et dans notre système BI4BI.
La théorie des automates est apparue pour résoudre des problèmes aussi bien pratiques que théoriques, et ceci dès le début de l'informatique. Désormais, les automates font partie des notions fondamentales de l'informatique, et se retrouvent dans la plupart des logiciels. En 1974, Samuel Eilenberg proposa un modèle de calcul qui unifie la plupart des automates (transducteurs, automates à pile et machines de Turing) et qui a une propriété de modularité intéressante au vu d'applications reposant sur différentes couches d'automates ;  Nous proposons de rendre effectif ce modèle en étudiant des techniques de simulation. Le simulateur est défini par un programme fonctionnel énumérant progressivement les solutions en explorant un espace de recherche selon différentes stratégies. Nous introduisons la notion de machines d'Eilenberg finies pour lesquelles nous fournissons une preuve formelle de correction de la simulation. Récemment, un ensemble de travaux approfondissant la notion de dérivées de Brzozowski, a été la source d'algorithmes efficaces de synthèse d'automates non-déterministes à partir d'expressions régulières. Ces algorithmes sont de nature algébrique et nous en faisons un état de l'art, tout en donnant une implémentation en OCaml permettant de les comparer les uns aux autres dans un cadre commun.
Le Traitement Automatique du Langage (TAL) est un domaine de recherche qui s'est particulièrement développé ces dernières années. En effet, la démocratisation des modes de communication à large échelle a entraîné l'apparition d'une immense quantité de donnée à forte valeur ajoutée qu'il est impossible de traiter manuellement. L'extraction du sens de ces échanges représente un enjeu commercial de premier plan pour beaucoup d'acteurs privés, comme les GAFAMI (réseaux sociaux, retours clients sur des produits distribués par des boutiques en ligne, etc.), et publics. La recherche académique et privée s'est donc naturellement orientée sur le développement de moyens permettant de déterminer le sens d'un énoncé à moindre coût et avec le moins de contraintes de fonctionnement possibles. Ces technologies sont notamment intéressantes pour mettre en place un système d'écoute des réseaux sociaux concernant un lieu particulier, comme une gare, un musée ou un aéroport. En effet, les informations qui peuvent circuler à leur sujet peuvent être exploitées pour répondre à différents besoins. Par exemple, il est possible d'anticiper les futurs pics d'affluence et de les localiser sur un site afin d'améliorer la gestion de son personnel, ou de détecter des cibles pour d'éventuelles actions malveillantes. L'élaboration de schémas de circulation en fonction de la langue des usagers peut également permettre d'optimiser l'affichage d'informations destinées aux clients pour le rendre plus facile d'accès. L'extraction du sens de données textuelles peut cependant se heurter à une barrière linguistique, en particulier si elles sont écrites en différentes langues. Leur acquisition et leur analyse doivent alors pouvoir s'opérer indépendamment du langage utilisé pour que l'information contenue dans le texte puisse être exploitée. Dans ce contexte, le développement d'un outil capable d'écouter les échanges sur différents réseaux sociaux pour y identifier des éléments que l'on souhaite quantifier et synthétiser dans différentes langues se révèlerait particulièrement intéressant pour aider à améliorer les services et la sécurité dans un lieu tel qu'un aéroport. Les évènements présentant un caractère d'urgence sont identifiés car pouvant impacter directement l'activité aéroportuaire. Par exemple une tempête empêchera les avions d'atterrir ou de décoller et il faut le prévoir pour gérer les passager, les personnels, les correspondances etc. Un embouteillage quant à lui peut empêcher les passagers d'accéder à l'avion, de sortir de l'aéroport, empêcher les pilotes et les personnels de prendre poste, bloquer les véhicules de service selon leur localisation et leur trajets prévus, etc. C'est dans ces problématiques que s'inscrit ce projet de recherche.
Nous abordons le problème de la recherche indépendante du modèle pour la Nouvelle Physique (NP), au Grand Collisionneur de Hadrons (LHC) en utilisant le détecteur ATLAS. Une attention particulière est accordée au développement et à la mise à l'essai de nouvelles techniques d'apprentissage automatique à cette fin. Le présent ouvrage présente trois résultats principaux. Tout d'abord, nous avons mis en place un système de surveillance automatique des signatures génériques au sein de TADA, un outil logiciel d'ATLAS. Nous avons exploré plus de 30 signatures au cours de la période de collecte des données de 2017 et aucune anomalie particulière n'a été observée par rapport aux simulations des processus du modèle standard. Deuxièmement, nous proposons une méthode collective de détection des anomalies pour les recherches de NP indépendantes du modèle au LHC. Nous proposons l'approche paramétrique qui utilise un algorithme d'apprentissage semi-supervisé. Cette approche utilise une probabilité pénalisée et est capable d'effectuer simultanément une sélection appropriée des variables et de détecter un comportement anormal collectif possible dans les données par rapport à un échantillon de fond donné. Troisièmement, nous présentons des études préliminaires sur la modélisation du bruit de fond et la détection de signaux génériques dans des spectres de masse invariants à l'aide de processus gaussiens (GPs) sans information préalable moyenne. Deux méthodes ont été testées dans deux ensembles de données : une procédure en deux étapes dans un ensemble de données tiré des simulations du modèle standard utilisé pour ATLAS General Search, dans le canal contenant deux jets à l'état final, et une procédure en trois étapes dans un ensemble de données simulées pour le signal (Z′) et le fond (modèle standard) dans la recherche de résonances dans le cas du spectre de masse invariant de paire supérieure. Notre étude est une première étape vers une méthode qui utilise les GPs comme outil de modélisation qui peut être appliqué à plusieurs signatures dans une configuration plus indépendante du modèle.
De par leurs rôles écologiques multiples, les macrophytes sont une composante importante des hydrosystèmes, qu'il est essentiel de conserver. Toutefois, durant la saison estivale, des densités importantes des espèces submergées entraînent des problèmes récurrents dans certains cours d'eau, notamment en milieu urbain pour les usagers et les gestionnaires, et peuvent avoir des conséquences négatives pour la santé des écosystèmes. Dans un contexte de changements globaux, les enjeux liés à la prolifération des macrophytes submergés incitent à proposer des outils permettant de mieux comprendre la dynamique des herbiers et de prédire leur évolution selon différents scénarios environnementaux. Dans cette optique, la présente thèse a pour objectif le développement d'une boîte à outils accompagnant un modèle mécaniste multispécifique de production des végétaux aquatiques submergés, le modèle DEMETHER. Pour ce faire, il nécessite de connaître certains paramètres écophysiologiques et de disposer de données spatialisées de biomasse pour sa calibration, ainsi que de données bathymétriques et de substrat. La première phase de ce travail a alors consisté à réaliser des relevés sur le terrain pour caractériser le site d'étude et à développer des outils numériques ou expérimentaux pour l'acquisition de ces données. Le premier outil développé a pour objet le suivi des macrophytes submergés par télédétection. La méthode explorée a confirmé le potentiel de l'imagerie multispectrale à haute résolution spatiale (50 cm) des satellites Pléiades, traitée par des algorithmes d'apprentissage automatique, pour cartographier la distribution des herbiers et quantifier leur biomasse in situ. Cette approche nous a par ailleurs conduits à proposer une stratégie d'échantillonnage optimisée des macrophytes en grand cours d'eau pour de futures investigations. Ce travail ouvre des perspectives intéressantes pour appliquer la méthode à de l'imagerie drone, et poursuivre son développement pour un suivi mensuel automatisé. En parallèle, un outil de mesure de paramètres écophysiologiques par oxymétrie a été développé et appliqué aux deux espèces d'intérêt. Les données obtenues renseignent en particulier sur les capacités photosynthétique et respiratoire de chaque espèce en réponse à des facteurs limitants (lumière, température). La seconde phase de ce travail a consisté en l'application du modèle DEMETHER pour l'exploration de différents scénarios d'évolution climatique. Des simulations de la dynamique des herbiers en termes de biomasse ont été réalisées pour les conditions thermiques actuelles et pour une hausse des températures prévisible à l'horizon 2041-2070. Les résultats ont montré l'importance de la sensibilité de certains processus physiologiques à la température pour expliquer les patrons de distribution des deux espèces étudiées, soulignant l'intérêt de la modélisation mécaniste pour comprendre la structuration des communautés de macrophytes. Les premiers résultats obtenus avec cette boîte à outils ont confirmé sa fonctionnalité. Toutefois, en vue d'étendre son champ d'application, chacun des outils développés durant la thèse devra encore être amélioré, notamment pour affiner la calibration du modèle DEMETHER. Des propositions précises ont été formulées dans ce sens
Cette thèse de doctorat traite de la reconnaissance automatique du Langage français Parlé Complété (LPC), version française du Cued Speech (CS), à partir de l'image vidéo et sans marquage de l'information préalable à l'enregistrement vidéo. Afin de réaliser cet objectif, nous cherchons à extraire les caractéristiques de haut niveau de trois flux d'information (lèvres, positions de la main et formes), et fusionner ces trois modalités dans une approche optimale pour un système de reconnaissance de LPC robuste. Dans ce travail, nous avons introduit une méthode d'apprentissage profond avec les réseaux neurono convolutifs (CNN)pour extraire les formes de main et de lèvres à partir d'images brutes. Un modèle de mélange de fond adaptatif (ABMM) est proposé pour obtenir la position de la main. Toutes ces méthodes constituent des contributions significatives pour l'extraction de caractéristiques du LPC. En outre, en raison de l'asynchronie des trois flux caractéristiques du LPC, leur fusion est un enjeu important dans cette thèse. Afin de le résoudre, nous avons proposé plusieurs approches, y compris les stratégies de fusion au niveau données et modèle avec une modélisation HMM dépendant du contexte. Pour obtenir le décodage, nous avons proposé trois architectures CNNs-HMMs. Toutes ces architectures sont évaluées sur un corpus de phrases codées en LPC en parole continue sans aucun artifice, et la performance de reconnaissance CS confirme l'efficacité de nos méthodes proposées. Le résultat est comparable à l'état de l'art qui utilisait des bases de données où l'information pertinente était préalablement repérée. En même temps, nous avons réalisé une étude spécifique concernant l'organisation temporelle des mouvements de la main, révélant une avance de la main en relation avec l'emplacement dans la phrase.
Néanmoins, jusqu'à présent, aucune théorie systématique n'a pas été construite qui peut fournir en une manière formelle la composition et la gamme des types de conversation actuels et possibles. Dans cette thèse, nous adoptons une approche topologique pour classifier les conversations et développons une théorie formelle des types conversationnels dans le cadre de Type Theory with Records. En revanche, nous testerons l'hypothèse que la variation entre des distributions des énoncés non phrastiques peut servir à structurer l'espace des types conversationnels.
Cette recherche porte sur la réalisation linguistique de l'explication en anglais contemporain. À la suite d'un travail définitoire de la notion sont identifiés deux types d'explication : la clarification et l'explication causale. L'étude adopte une approche énonciative tout en offrant des considérations d'ordre pragmatique. L'analyse linguistique se fonde sur un corpus de textes didactiques et examine plus particulièrement le rôle des connecteurs dans le discours explicatif. Enfin, ce travail vise également la formalisation de quelques relations discursives à l'œuvre dans l'explication et l'écriture de règles linguistiques en vue de la reconnaissance automatique de ces relations.
Cette thèse s'inscrit dans un mouvement de reconnaissance de l'importance accrue de l'institution judiciaire, et de questionnement actuel sur la légitimité démocratique du juge. Dans ce cadre, elle enquête sur le rôle, dans la fonction et la pratique judiciaire, de l'opinion publique, largement considérée comme un élément de légitimité démocratique. Pour obtenir un éclairage plus complet sur cette question, une approche comparative est adoptée et appliquée à l'œuvre protectrice d'une cour nationale constitutionnelle et d'une cour internationale dans le domaine des droits et des libertés : la Cour suprême des États-Unis et la Cour européenne des droits de l'Homme. Le raisonnement suivi est le suivant. Au niveau théorique, il s'agit de clarifier le concept protéiforme d' « opinion publique » et d'établir les différentes sources de la légitimité judiciaire, afin de déterminer si l'opinion publique peut en faire partie. On se penche enfin sur la substance des décisions de justice, qui révèlent la manière dont les juges conçoivent le rôle de l'opinion publique dans la démocratie et dans l'évolution judiciaire des droits et libertés. L'étude de la substance des décisions se concentre d'une part sur la relation entre opinion publique et démocratie dans la protection de la liberté d'expression, et d'autre part sur le rôle de l'opinion publique dans l'évolution des droits des personnes homosexuelles.
Nous proposons dans ce travail de thèse une étude des constructions en parce que à l'oral, à l'interface entre linguistique de corpus et linguistique théorique. Le corpus analysé est composé de conversations enregistrées dans le cadre de quatre enquêtes du projet PFC (Phonologie du Français Contemporain). La première phase de l'annotation permet la caractérisation syntaxique desdites constructions (Debaisieux, 1994). Nous proposons pour ce faire une nouvelle relation de type Explication permettant l'annotation des parce que en lien avec la modalité d'énoncé. Des éléments pour l'identification des termes mis en relation par parce que sont donnés, qui montrent l'intérêt d'une double analyse du corpus, en syntaxe et en discours.
En Interaction Homme-Machine, la qualité est une utopie : malgré toutes les précautions prises en conception, il existe toujours des utilisateurs et des situations d'usage pour lesquels l'Interface Homme-Machine (IHM) est imparfaite. Cette thèse explore l'auto-explication des IHM pour améliorer la qualité perçue par les utilisateurs. L'approche s'inscrit dans une Ingénierie Dirigée par les Modèles. Elle consiste à embarquer à l'exécution les modèles de conception pour dynamiquement augmenter l'IHM d'un ensemble de questions et de réponses. Les questions peuvent être relatives à l'utilisation de l'IHM (par exemple, "A quoi sert ce bouton ? ", "Pourquoi telle action n'est pas possible ?) et à sa justification (par exemple, "Pourquoi les items ne sont-ils pas rangés par ordre alphabétique ?"). Cette thèse propose une infrastructure logicielle UsiExplain basée sur les méta-modèles UsiXML. L'évaluation sur un cas d'étude d'achat de voitures montre que l'approche est pertinente pour les questions d'utilisation de l'IHM.
L'augmentation constante du nombre de documents disponibles et des moyens d'accès transforme les pratiques de recherche d'information. Depuis quelques années, de plus en plus de plateformes de recherche d'information à destination des chercheurs ou du grand public font leur apparition sur la toile. Auparavant, la principale problématique des chercheurs était de savoir si une information existait. Aujourd'hui, il est plutôt question de savoir comment accéder à une information pertinente. Pour résoudre ce problème, deux leviers d'action seront étudiés dans cette thèse. Nous pensons qu'il est avant tout important d'identifier l'usage qui est fait des principaux moyens d'accès à l'information. Être capable d'interpréter le comportement des utilisateurs est une étape nécessaire pour d'abord identifier ce que ces derniers comprennent des systèmes de recherche, et ensuite ce qui doit être approfondi. En effet, la plupart de ces systèmes agissent comme des boîtes noires qui masquent les différents processus sous-jacents. Pourquoi le moteur de recherche me renvoie-t-il ces résultats ? Pourquoi ce document est-il plus pertinent qu'un autre ? Ces questions apparemment banales sont pourtant essentielles à une recherche d'information critique. Nous pensons que les utilisateurs ont le droit et le devoir de s'interroger sur la pertinence des outils informatiques mis à leur disposition. Pour les aider dans cette tâche, nous avons développé une plateforme de recherche d'information en ligne à double usage. Elle peut tout d'abord être utilisée pour l'observation et la compréhension du comportement des utilisateurs. De plus, elle peut aussi être utilisée comme support pédagogique, pour mettre en évidence les différents biais de recherche auxquels les utilisateurs sont confrontés. Dans le même temps, ces outils doivent être améliorés. Nous prenons dans cette thèse l'exemple de la qualité des documents qui a un impact certain sur leur accessibilité. La quantité de documents disponibles ne cessant d'augmenter, les opérateurs humains sont de moins en moins capables de les corriger manuellement et de s'assurer de leur qualité. Il est donc nécessaire de mettre en place de nouvelles stratégies pour améliorer le fonctionnement des systèmes de recherche. Nous proposons dans cette thèse une méthode pour automatiquement identifier et corriger certaines erreurs générées par les processus automatiques d'extraction d'information (en particulier l'OCR).
Le présent travail introduit en phonétique la décomposition atomique du signal, appelée aussi Matching Pursuit, traite les fichiers d'atomes par compression sans perte et enfin mesure la distance des fichiers comprimés par des algorithmes de Kolmogorov. L'étalonnage est basé sur une première analyse classique de la coarticulation de séquences sonores VCV et CV, (ou V ∈ {[i] [u] [a]} et C ∈ {[t] [d] [s] [δ]}∪{[tʕ] [dʕ] [sʕ [δʕ]}, extraites d'un corpus issu de quatre régions arabophones. L'équation de locus de CV vs CʕV, permet de différencier les variétés de langue. La deuxième analyse applique un algorithme de décomposition atomique adaptative ou Matching Pursuit sur des séquences VCV et VCʕV du même corpus. Les séquences atomiques représentant VCV et VCʕV sont ensuite compressées sans perte et la distance entre elles est recherchée par des algorithmes de Kolmogorov. La classification des productions phonétiques et des régions arabophones obtenue est équivalente à celle de la première méthode. Ce travail montre l'intérêt de l'introduction de Matching Pursuit en phonétique, la grande robustesse des algorithmes utilisés et suggère d'importantes possibilités d'automatisation des processus mis en oeuvre, tout en ouvrant de nouvelles directions d'investigation
Ce travail de recherche est conjointement effectué dans le cadre d'une cotutelle entre deux universités : en France l'Université Jean Monnet de Saint-Etienne, laboratoire Hubert Curien sous la supervision de Mme Frédérique Laforest, M. Christophe Gravier et M. Julien Subercaze, et au Maroc l'Université Mohamed V de Rabat, équipe LeRMA sous la supervision de Mme Rachida Ajhoun et Mme Mounia Abik. Les connaissances et les apprentissages sont des préoccupations majeures dans la société d'aujourd'hui. Les technologies de l'apprentissage humain visent à promouvoir, stimuler, soutenir et valider le processus d'apprentissage. Notre approche explore les opportunités soulevées en faisant coopérer le Web Social et le Web sémantique pour le e-learning. Plus précisément, nous travaillons sur l'enrichissement des profils des apprenants en fonction de leurs activités sur le Web Social. Le Web social peut être une source d'information très importante à explorer, car il implique les utilisateurs dans le monde de l'information et leur donne la possibilité de participer à la construction et à la diffusion de connaissances. Nous nous focalisons sur le suivi des différents types de contributions, dans les activités de collaboration spontanée des apprenants sur les réseaux sociaux. Le profil de l'apprenant est non seulement basé sur la connaissance extraite de ses activités sur le système de e-learning, mais aussi de ses nombreuses activités sur les réseaux sociaux. En particulier, nous proposons une méthodologie pour exploiter les hashtags contenus dans les écrits des utilisateurs pour la génération automatique des intérêts des apprenants dans le but d'enrichir leurs profils. Cependant les hashtags nécessitent un certain traitement avant d'être source de connaissances sur les intérêts des utilisateurs. Nous avons défini une méthode pour identifier la sémantique de hashtags et les relations sémantiques entre les significations des différents hashtags. Par ailleurs, nous avons défini le concept de Folksionary, comme un dictionnaire de hashtags qui pour chaque hashtag regroupe ses définitions en unités de sens. Les hashtags enrichis en sémantique sont donc utilisés pour nourrir le profil de l'apprenant de manière à personnaliser les recommandations sur le matériel d'apprentissage. L'objectif est de construire une représentation sémantique des activités et des intérêts des apprenants sur les réseaux sociaux afin d'enrichir leurs profils. Nous présentons également notre approche générale de recommandation multidimensionnelle dans un environnement d'e-learning. Notre implémentation s'est focalisée sur la recommandation personnalisée
Les travaux présentés de cette thèse visent à proposer des outils numériques et théoriques pour la résolution de problèmes inverses en imagerie. Une approche plébiscitée pour estimer un opérateur de dégradation consiste à observer un échantillon contenant des sources ponctuelles (microbilles en microscopie, étoiles en astronomie). Une telle acquisition fournit une mesure de la réponse impulsionnelle de l'opérateur en plusieurs points du champ de vue. Le traitement de cette observation requiert des outils robustes pouvant utiliser rapidement les données rencontrées en pratique. Nous proposons une boîte à outils qui estime un opérateur de dégradation à partir d'une image contenant des sources ponctuelles. L'opérateur estimé à la propriété qu'en tout point du champ de vue, sa réponse impulsionnelle s'exprime comme une combinaison linéaire de fonctions élémentaires. Cela permet d'estimer des opérateurs invariants (convolutions) et variants (développement en convolution-produit) spatialement. Une spécificité importante de cette boîte à outils est son caractère automatique : seul un nombre réduit de paramètres facilement accessibles permettent de couvrir une grande majorité des cas pratiques. La taille de la source ponctuelle (e.g. bille), le fond et le bruit sont également pris en compte dans l'estimation. Cet outil se présente sous la forme d'un module appelé PSF-Estimator pour le logiciel Fiji, et repose sur une implémentation parallélisée en C++. En réalité, les opérateurs modélisant un système optique varient d'une expérience à une autre, ce qui, dans l'idéal, nécessite une calibration du système avant chaque acquisition. Pour pallier à cela, nous proposons de représenter un système optique non pas par un unique opérateur de dégradation, mais par un sous-espace d'opérateurs. Cet ensemble doit permettre de représenter chaque opérateur généré par un microscope. Nous introduisons une méthode d'estimation d'un tel sous-espace à partir d'une collection d'opérateurs de faible rang (comme ceux estimés par la boîte à outils PSF-Estimator). Nous montrons que sous des hypothèses raisonnables, ce sous-espace est de faible dimension et est constitué d'éléments de faible rang. Dans un second temps, nous appliquons ce procédé en microscopie sur de grands champs de vue et avec des opérateurs variant spatialement. Cette mise en œuvre est possible grâce à l'utilisation de méthodes complémentaires pour traiter des images réelles (e.g.
L'objectif de cette de thèse est d'étudier à la fois des solutions matérielles et logicielles pour améliorer les conditions de travail des sapeurs-pompiers. Il s'agit de développer un système intelligent basé sur l'internet des objets pour surveiller l'état de santé des pompiers et aider à les localiser lors des interventions. Dans la première partie de la thèse, nous avons étudié et proposé plusieurs approches permettant de réduire la consommation d'énergie du système afin de maximiser sa durée de vie. La première approche présente un modèle de prédiction basé sur la corrélation temporelle entre les mesures collectées par le même capteur. Il permet de réduire la quantité de données collectées et transmises au centre de contrôle. Ce modèle est exécuté à la fois par le capteur et le centre et qui s'auto-adapte en fonction de l'écart constaté entre les mesures réelles collectées et les mesures prédites. Une deuxième version de cette approche a été étudiée pour prendre en considération la perte de message et la synchronisation entre le capteur et le centre de contrôle. D'un autre côté et pour réduire davantage la consommation d'énergie, nous avons couplé l'approche de prédiction avec un algorithme de collecte de données adaptatif permettant de réduire l'activité du capteur et le taux d'échantillonnage. Toutes ces approches ont été testées via des simulations et de l'implémentation réelle. Les résultats obtenus montrent l'efficacité de ces approches en termes de réduction de la consommation d'énergie tout en gardant l'intégrité de données. La deuxième partie de cette thèse est dédiée au traitement des données issues des interventions des sapeurs-pompiers. Nous avons étudié plusieurs méthodes de clustérisation permettant un prétraitement de données avant l'extraction des connaissances. D'un autre côté, nous avons appliqué des méthodes d'apprentissage profond sur un grand ensemble de données concernant 200.000 interventions qui ont eu lieu pendant une période de 6 ans dans le département du Doubs, en France. Le but de cette partie était de prédire le nombre d'interventions futures en fonction de variables explicatives externes, pour aider les pompiers à bien gérer leurs ressources.
Le sujet de la thèse est l'extraction et la segmentation des vêtements à partir d'images en utilisant des techniques de la vision par ordinateur, de l'apprentissage par ordinateur et de la description d'image, pour la recommandation de manière non intrusive aux utilisateurs des produits similaires provenant d'une base de données de vente. Nous proposons tout d'abord un extracteur d'objets dédié à la segmentation de la robe en combinant les informations locales avec un apprentissage préalable. Un détecteur de personne localises des sites dans l'image qui est probable de contenir l'objet. Ensuite, un processus d'apprentissage intra-image en deux étapes est est développé pour séparer les pixels de l'objet de fond. L'objet est finalement segmenté en utilisant un algorithme de contour actif qui prend en compte la segmentation précédente et injecte des connaissances spécifiques sur la courbure locale dans la fonction énergie. Nous proposons ensuite un nouveau framework pour l'extraction des vêtements généraux en utilisant une procédure d'ajustement globale et locale à trois étapes. Un ensemble de modèles initialises un processus d'extraction d'objet par un alignement global du modèle, suivi d'une recherche locale en minimisant une mesure de l'inadéquation par rapport aux limites potentielles dans le voisinage. Les résultats fournis par chaque modèle sont agrégés, mesuré par un critère d'ajustement globale, pour choisir la segmentation finale. De plus, nous introduisons une nouvelle base de données RichPicture, constituée de 1000 images pour l'extraction de vêtements à partir d'images de mode. Les méthodes sont validées sur la base de données publiques et se comparent favorablement aux autres méthodes selon toutes les mesures de performance considérées.
Les données de classement, c.à. d. des listes ordonnées d'objets, apparaissent naturellement dans une grande variété de situations, notamment lorsque les données proviennent d'activités humaines (bulletins de vote d'élections, enquêtes d'opinion, résultats de compétitions) ou dans des applications modernes du traitement de données (moteurs de recherche, systèmes de recommendation). La conception d'algorithmes d'apprentissage automatique, adaptés à ces données, est donc cruciale. Cependant, en raison de l'absence de structure vectorielle de l'espace des classements et de sa cardinalité explosive lorsque le nombre d'objets augmente, la plupart des méthodes classiques issues des statistiques et de l'analyse multivariée ne peuvent être appliquées directement. Par conséquent, la grande majorité de la littérature repose sur des modèles paramétriques. Dans cette thèse, nous proposons une théorie et des méthodes non paramétriques pour traiter les données de classement. Notre analyse repose fortement sur deux astuces principales. La première est l'utilisation poussée de la distance du tau de Kendall, qui décompose les classements en comparaisons par paires. Cela nous permet d'analyser les distributions sur les classements à travers leurs marginales par paires et à travers une hypothèse spécifique appelée transitivité, qui empêche les cycles dans les préférences de se produire. La seconde est l'utilisation des fonctions de représentation adaptées aux données de classements, envoyant ces dernières dans un espace vectoriel. Trois problèmes différents, non supervisés et supervisés, ont été abordés dans ce contexte : l'agrégation de classement, la réduction de dimensionnalité et la prévision de classements avec variables explicatives. La première partie de cette thèse se concentre sur le problème de l'agrégation de classements, dont l'objectif est de résumer un ensemble de données de classement par un classement consensus. Dans cette thèse, nous avons étudié la complexité de ce problème de deux manières. Premièrement, nous avons proposé une méthode pour borner la distance du tau de Kendall entre tout candidat pour le consensus (généralement le résultat d'une procédure efficace) et un consensus de Kemeny, sur tout ensemble de données. Nous avons ensuite inscrit le problème d'agrégation de classements dans un cadre statistique rigoureux en le reformulant en termes de distributions sur les classements, et en évaluant la capacité de généralisation de consensus de Kemeny empiriques. La deuxième partie de cette théorie est consacrée à des problèmes d'apprentissage automatique, qui se révèlent être étroitement liés à l'agrégation de classement. Le premier est la réduction de la dimensionnalité pour les données de classement, pour lequel nous proposons une approche de transport optimal, pour approximer une distribution sur les classements par une distribution montrant un certain type de parcimonie. Le second est le problème de la prévision des classements avec variables explicatives, pour lesquelles nous avons étudié plusieurs méthodes. Notre première proposition est d'adapter des méthodes constantes par morceaux à ce problème, qui partitionnent l'espace des variables explicatives en régions et assignent à chaque région un label (un consensus). Notre deuxième proposition est une approche de prédiction structurée, reposant sur des fonctions de représentations, aux avantages théoriques et computationnels, pour les données de classements.
La connaissance des positions et des mouvements des articulateurs (lèvres, palais, langue...) du conduit vocal lors de la phonation est un enjeu crucial pour l'étude de la parole. Nous décrivons un ensemble de protocoles et de méthodes pour obtenir et fusionner automatiquement un important volume de données échographiques (imageant en 2D la dynamique de la langue), stéréoscopiques (imageant en 3D la dynamique des lèvres), de capteurs électromagnétiques (capturant des points 3D de la langue et du visage), et d'Imagerie par Résonance Magnétique (IRM) pour acquérir en 3D l'ensemble des articulateurs en position statique. Nos contributions concernent plus particulièrement la synchronisation temporelle, le recalage spatial des données et l'extraction automatique des formes à partir des données (suivi de la langue dans les images échographiques). Ces travaux permettent l'obtention de données bien fondées pour la mise en place et l'étude de modèles articulatoires pour des applications en parole.
De nombreux algorithmes d'Apprentissage automatique ont été proposés afin de résoudre les différentes tâches pouvant être extraites des problèmes de prédiction issus d'un contexte réel. Pour résoudre les différentes tâches pouvant être extraites, la plupart des algorithmes d'Apprentissage automatique se basent d'une manière ou d'une autre sur des relations liant les instances. Les relations entre paires d'instances peuvent être définies en calculant une distance entre les représentations vectorielles des instances. En se basant sur la représentation vectorielle des données, aucune des distances parmi celles communément utilisées n'est assurée d'être représentative de la tâche à résoudre. Dans ce document, nous étudions l'intérêt d'adapter la représentation vectorielle des données à la distance utilisée pour une meilleure résolution de la tâche. Nous nous concentrons plus précisément sur l'algorithme existant résolvant une tâche de classification en se basant sur un graphe. Nous décrivons d'abord un algorithme apprenant une projection des données dans un espace de représentation permettant une résolution, basée sur un graphe, optimale de la classification. En projetant les données dans un espace de représentation dans lequel une distance préalablement définie est représentative de la tâche, nous pouvons surpasser la représentation vectorielle des données lors de la résolution de la tâche. Une analyse théorique de l'algorithme décrit est développée afin de définir les conditions assurant une classification optimale. Un ensemble d'expériences nous permet finalement d'évaluer l'intérêt de l'approche introduite et de nuancer l'analyse théorique.
Les éditions critiques numériques sont des ressources patrimoniales annotées, sous une forme numérique. De telles éditions prennent la forme d'une transcription des ressources originales, augmentées d'un apparat critique, c'est-à-dire, la forme de données structurées. Dans un contexte collaboratif, a structure de ces données est définie explicitement par un schéma, document interprétable qui contraint la manière dont les éditeurs vont pouvoir annoter les ressources primaires et va de ce fait garantir une certaine homogénéité dans le respect de la politique éditoriale. Les projets d'édition critique numérique font classiquement face à deux problèmes techniques. Le premier a à voir avec l'expressivité des langages d'annotation, qui empêchent l'expression de certaines informations utiles. La seconde tient au fait que, par expérience, les schémas qui sous-tendent une édition critique vont être amenés à évoluer au cours de la réalisation de cette édition ;  cependant, modifier le schéma implique qu'il faille mettre à jour l'intégralité des données structurées validées par ce schéma, ce qui est habituellement effectué à la main par les éditeurs, au moyen de scripts ad-hoc – si les éditeurs, faute de moyens ou de temps, ne renoncent pas à faire évoluer la structure de données. Dans ce travail de thèse, nous définissons les fondements théoriques pour l'établissement d'un système éditorial dédié à l'édition critique numérique. Nous définissons les eAG, un modèle d'annotation déporté basé sur un formalisme de graphes cycliques, autorisant a plus grande expressivité. Nous définissons un mécanisme de schéma innovant, SeAG, permettant la validation à la volée des eAG au cours de leur manufacture. Nous définissons également une syntaxe de balisage présentant des similarités avec les langages d'annotation classiques comme XML, tout en préservant l'expressivité des eAG. Enfin, nous proposons une algèbre bidirectionnelle pour les eAG de telle sorte que, si un SeAG S est transformé en un SeAG S', alors tout eAG I validé par S est traduit de manière semi-automatique sous la forme d'un eAG I', validé par S', et tel que toute mise à jour de I (respectivement I') soit propagé, de manière semi-automatique, sur I'(resp. I).
La conception des systèmes de contrôle-commande souffre souvent des problèmes de communication et d'interprétation des spécifications entre les différents intervenants provenant souvent de domaines techniques très variés. Afin de cadrer la conception de ces systèmes, plusieurs démarches ont été proposées dans la littérature. Parmi elles, la démarche dite mixte (ascendante/descendante), qui voit la conception réalisée en deux phases. Dans la première phase (ascendante), un modèle du système est défini à partir d'un ensemble de composants standardisés. Ce modèle subit, dans la deuxième phase (descendante), plusieurs raffinages et transformations pour obtenir des modèles plus concrets (codes,applicatifs, etc.). Afin de garantir la qualité des systèmes conçus par cette démarche, nous proposons dans cette thèse, deux approches de vérification formelle basées sur le Model-Checking. La première approche porte sur la vérification des composants standardisés et permet la vérification d'une chaîne de contrôle-commande élémentaire complète. La deuxième approche consiste en la vérification des modèles d'architecture (PandID) utilisés pour la génération des programmes de contrôle-commande. Cette dernière est basée sur la définition d'un style architectural en Alloy pour la norme ANSI/ISA-5.1. Pour supporter les deux approches, deux flots de vérification formelle semi-automatisés basés sur les concepts de l'IDM ont été proposés. L'intégration des méthodes formelles dans un contexte industriel est facilitée, ainsi, par la génération automatique des modèles formels à partir des modèles de conception maîtrisés par les concepteurs métiers. Nos deux approches ont été validées sur un cas industriel concret concernant un système de gestion de fluide embarqué dans un navire.
Dans ce contexte, les requêtes sont résolues par la composition de plusieurs services DaaS. Définir la sémantique des services cloud DaaS est la première étape vers l'automatisation de leur composition. Une approche intéressante pour définir la sémantique des services DaaS est de les décrire comme étant des vues sémantiques à travers une ontologie de domaine. Cependant, la définition de ces vues sémantiques ne peut pas être toujours faite avec certitude, surtout lorsque les données retournées par un service sont trop complexes. Dans cette thèse, nous proposons une approche probabiliste pour représenter les services DaaS à sémantique incertaine. Dans notre approche, un service DaaS dont la sémantique est incertaine est décrit par plusieurs vues sémantiques possibles, chacune avec une probabilité. En se basant sur nos modèles probabilistes, nous étudions le problème de l'interprétation d'une composition existante impliquant des services à sémantique incertaine. Nous étudions aussi le problème de la réécriture de requêtes à travers les services DaaS incertains, et nous proposons des algorithmes efficaces permettant de calculer les différentes compositions possibles ainsi que leurs probabilités. Nous menons une série d'expérimentation pour évaluer la performance de nos différents algorithmes de composition. Les résultats obtenus montrent l'efficacité et la scalabilité de nos solutions proposées
Cette thèse s'inscrit dans le domaine du traitement automatique des langues et concerne l'analyse sémantique de la structure du discours. Nous nous attachons plus particulièrement au problème de l'analyse thématique, qui vise l'étude de la structure des textes selon des critères relatifs à la répartition de leur contenu informationnel. Cette tâche revêt une importance capitale dans la perspective de l'accès assisté à l'information, qui constitue notre principale visée applicative. Le concept même de "thème" étant à la fois complexe et assez rarement considéré en tant qu'objet d'étude dans le domaine de la recherche d'information, la première partie du mémoire est consacrée à une vaste étude bibliographique autour des notions de thème, de topique, de sujet ou encore d'à propos, tant en linguistique qu'en sciences de l'information ou en traitement des langues. Nous en dégageons les lignes de force qui fondent notre approche du thème comme objet discursif, sémantique et structuré. Nous proposons sur cette base différents modèles et procédés s'attachant d'abord au traitement sémantique des documents géographiques, puis à l'analyse automatique des cadres de discours spatio-temporels au sens de Michel Charolles. Nous généralisons ces travaux en introduisant les notions de thème discursif composite et d'axe sémantique. Nous terminons en présentant LinguaStream, environnement d'expérimentation intégré que nous avons conçu pour faciliter l'élaboration de modèles linguistiques opérationnels, et qui nous conduit à proposer des principes méthodologiques originaux.
Notre projet de recherche vise à comprendre comment les organisations peuvent s'adapter à leur environnement et le faire évoluer en tirant partie de la richesse d' Ils montrent par ailleurs que l'objectif du transfert d'informations entre les chargés d'études et le marketing opérationnel, ainsi que le degré de contrôle sur l'interprétation des informations par le marketing opérationnel influence la coordination des activités qui composent l'agilité.
La prolifération des réseaux sociaux et des données à caractère personnel apporte de nombreuses possibilités de développement de nouvelles applications. Au même temps, la disponibilité de grandes quantités de données à caractère personnel soulève des problèmes de confidentialité et de sécurité. Dans cette thèse, nous développons des méthodes pour identifier les différents comptes d'un utilisateur dans des réseaux sociaux. Nous étudions d'abord comment nous pouvons exploiter les profils publics maintenus par les utilisateurs pour corréler leurs comptes. Nous identifions quatre propriétés importantes-la disponibilité, la cohérence, la non-impersonabilite, et la discriminabilité (ACID)-pour évaluer la qualité de différents attributs pour corréler des comptes. On peut corréler un grand nombre de comptes parce-que les utilisateurs maintiennent les mêmes noms et d'autres informations personnelles à travers des différents réseaux sociaux. Pourtant, il reste difficile d'obtenir une précision suffisant pour utiliser les corrélations dans la pratique à cause de la grandeur de réseaux sociaux réels. Nous développons des schémas qui obtiennent des faible taux d'erreur même lorsqu'elles sont appliquées dans les réseaux avec des millions d'utilisateurs. Ensuite, nous montrons que nous pouvons corréler les comptes d'utilisateurs même si nous exploitons que leur activité sur un les réseaux sociaux. Ça sa démontre que, même si les utilisateurs maintient des profils distincts nous pouvons toutefois corréler leurs comptes. Enfin, nous montrons que, en identifiant les comptes qui correspondent à la même personne à l'intérieur d'un réseau social, nous pouvons détecter des imitateurs.
L'analyse de la littérature académique sur l'influence de la couleur débouche sur la proposition d'un cadre conceptuel des effets de la couleur de fond dans le contexte des avis de consommateurs en ligne (chapitre 2). La phase empirique de cette recherche s'articule autour de trois études expérimentales. Une première s'attache à examiner l'influence d'une couleur de fond de valence négative (rouge) pour un avis de consommateur en ligne négatif sur les évaluations des consommateurs exposés (chapitre 3). Une seconde étude expérimentale porte sur le rôle de la valence perçue de la couleur comme un mécanisme d'influence de la couleur de fond d'un avis de consommateur en ligne (chapitre 4). La recherche d'une plus grande validité écologique débouche sur une troisième étude expérimentale. Les participants sont alors exposés à plusieurs avis présentés simultanément sur une page type d'avis de consommateurs en ligne (chapitre 5). Cette recherche présente un certain nombre d'apports, et n'est pas dépourvue de limites qui ouvrent d'importantes voies de recherches additionnelles (conclusion).
Cette thèse aborde la conception d'un Système de Gestion Énergétique (EMS), prenant en compte les contraintes de trafic, pour un véhicule hybride électrique. Actuellement, les EMS sont habituellement classé en deux catégories ceux proposant une architecture en temps réel cherchant un optimum local, et ceux qui recherchent un optimum global, plus coûteux en temps de calcul et donc plus approprié à un usage hors ligne. Cette thèse repose sur le fait que la consommation énergétique peut être modélisée précisément à l'aide de distributions de probabilité sur la vitesse et l'accélération. Dans le but de réduire la taille des données, une classification est proposé, basé sur la distance de Wasserstein, les barycentres des classes pouvant être calculés grâce aux itérations de Sinkhorn ou la méthode du Gradient Stochastique Alterné. Cette modélisation trafic a permis à une optimisation hors ligne de déterminer le contrôle optimal (le couple du moteur électrique) qui minimise la consommation de carburant du véhicule hybride sur un segment routier.
Les réseaux de neurones ont obtenu ces dernières années des succès notables dans des domaines tels que la vision par ordinateur ou bien le traitement du langage naturel. Ceux-ci bénéficient des grandes quantités de données annotées. Toutefois, régulariser ces neurones quand les données annotées sont rares reste un problème ouvert, la plupart des approches existantes n'étant pas satisfaisantes. Le but de cette thèse est donc de développer des algorithmes d'optimisation et des stratégies de régularisation adaptés aux réseaux de neurones profonds, avec un accent mis sur les régimes où les données annotées sont plus rares. Les pistes possibles comprennent les normes spectrales et les normes d'espaces à noyau reproduisant. L'architecture joue également un rôle important dans la complexité de l'entraînement d'un réseau de neurones. Ce problème présente une structure particulière qu'il convient d'exploiter lors de la phase d'optimisation. En particulier, le théorème de Shapley-Folkman montre que ce type de problème présente un faible écart de dualité dans certaines configurations.
Plusieurs ressources publiées sur le Web de données sont dotées de références spatiales qui décrivent leur localisation géographique. Ces références spatiales sont un moyen favori pour interconnecter et visualiser les ressources sur le Web de données. Cependant, les hétérogénéités des niveaux de détail et de modélisations géométriques entre les sources de données constituent un défi majeur pour l'utilisation de la comparaison des références spatiales comme critère pour l'interconnexion des ressources. Ce défi est amplifié par la nature ouverte et collaborative des sources de données du Web qui engendre des hétérogénéités géométriques internes aux sources de données. En outre, les applications de visualisation cartographique des ressources géoréférencées du Web de données ne fournissent pas une visualisation lisible à toutes les échelles. Dans cette thèse, nous proposons un vocabulaire pour formaliser les connaissances sur les caractéristiques de chaque géométrie dans un jeu de données. Nous proposons également une approche semi-automatique basée sur un référentiel topographique pour acquérir ces connaissances. Nous proposons de mettre en oeuvre ces connaissances dans une approche d'adaptation dynamique du paramétrage de la comparaison des géométries dans un processus d'interconnexion. Nous proposons une approche complémentaire s'appuyant sur un référentiel topographique pour la détection des liens de cardinalité n : m. Nous proposons finalement des applications qui s'appuient sur des données topographiques de référence et leurs liens avec les ressources géoréférencées du Web pour offrir une visualisation cartographique multiéchelle lisible et conviviale
Cette thèse porte sur l'utilisation d'une langue contrôlée pour les spécifications des besoins du logiciel en thaï. L'étude décrit les ambiguïtés syntaxiques et sémantiques ainsi que les problèmes rencontrés dans les spécifications des besoins du logiciel en thaï. Ce travail explique également la nature de la langue thaïe. Le modèle de la langue contrôlée pour les spécifications des besoins du logiciel en thaï, proposé dans cette étude, comprend trois composantes : l'analyse lexicale, l'analyse syntaxique et l'analyse sémantique. Pour l'analyse syntaxique, une syntaxe contrôlée est conçue en utilisant la forme du Backus-Naur (BNF). Quant à l'analyse lexicale, nous créons une ressource lexicale sous forme de langage XML pour stocker tous les mots classés selon leur domaine. Les mots reçus de la ressource XML sont corrects d'un point de vue conceptuel mais ne sont pas pertinents d'un point de vue sémantique. Pour résoudre ce problème, nous faisons alors usage de matrices booléennes pour aligner les phrases sémantiquement. Ainsi les phrases produites par le modèle seront syntaxiquement et sémantiquement correctes. Après avoir créé le modèle, nous avons construit un logiciel pour tester son efficacité. Il est ainsi évalué par quatreméthodes d'évaluation : 1. le test de fonctionnement syntaxique pour vérifier la syntaxe de la phrase ; 2. le test defonctionnement sémantique pour tester la sémantique de la phrase ; 3. le test d'acceptation en terme de satisfaction desutilisateurs avec le logiciel ; et 4. le test d'acceptation en terme d'acception des données de sortie. Des résultats positifs montrent que : 1.les phrases produites par le modèle proposé sont syntaxiquement correctes ; 2. les phrases produites par le modèle proposé sont sémantiquement correctes ; 3. les utilisateurs sont satisfaits et acceptent lelogiciel ; et 4. les utilisateurs acceptent et comprennent les phrases produites par ce modèle.
Dans cette thèse, nous présentons notre travail sur le développement d'algorithmes de traitement automatique des langues (TAL) pour aider les lecteurs et les auteurs d'articles scientifiques (biomédicaux) à détecter le spin (présentation inadéquate des résultats de recherche). Notre algorithme se concentre sur le spin dans les résumés d'articles rapportant des essais contrôlés randomisés. Nous avons étudié le phénomène de ” spin ” du point de vue linguistique pour créer une description de ses caractéristiques textuelles. Nous avons annoté des corpus pour les tâches principales de notre chaîne de traitement pour la détection de spin : extraction des résultats — en anglais ” outcomes ” — déclarés (primaires) et rapportés, évaluation de la similarité sémantique des paires de résultats d'essais et extraction des relations entre les résultats rapportés et leurs niveaux de signification statistique. En outre, nous avons annoté deux corpus plus petits pour identifier les déclarations de similarité des traitements et les comparaisons intra-groupe. Nous avons développé et testé un nombre d'algorithmes d'apprentissage automatique et d'algorithmes basés sur des règles pour les tâches principales de la détection de spin (extraction des résultats, évaluation de la similarité des résultats et extraction de la relation résultat-signification statistique). La meilleure performance a été obtenues par une approche d'apprentissage profond qui consiste à adapter les représentations linguistiques pré-apprises spécifiques à un domaine (modèles de BioBERT et SciBERT) à nos tâches. Cette approche a été mise en oeuvre dans notre système prototype de détection de spin, appelé DeSpin, dont le code source est librement accessible sur un serveur public. Notre prototype inclut d'autres algorithmes importants, tels que l'analyse de structure de texte (identification du résumé d'un article,identification de sections dans le résumé), la détection de déclarations de similarité de traitements et de comparaisons intra-groupe, l'extraction de données de registres d'essais. L'identification des sections des résumés est effectuée avec une approche d'apprentissage profond utilisant le modèle BioBERT, tandis que les autres tâches sont effectuées à l'aide d'une approche basée sur des règles. Notre système prototype a une interface simple d'annotation et de visualisation.
Avec l'émergence et la généralisation des systèmes d'intelligence artificielle, comprendre le comportement des agents artificiels, ou robots intelligents, devient essentiel pour garantir une collaboration fluide entre l'homme et ces agents. De récentes études dans le domaine l'intelligence artificielle explicable, particulièrement sur les modèles utilisant des objectifs, ont confirmé qu'expliquer le comportement d'un agent à un humain favorise la compréhensibilité de l'agent par ce dernier et augmente son acceptabilité. Cependant, fournir des informations trop nombreuses ou inutiles peut également semer la confusion chez les utilisateurs humains et provoquer des malentendus. Pour ces raisons, la parcimonie des explications a été présentée comme l'une des principales caractéristiques facilitant une interaction réussie entre l'homme et l'agent. Une explication parcimonieuse est définie comme l'explication la plus simple et décrivant la situation de manière adéquate. Si la parcimonie des explications fait l'objet d'une attention croissante dans la littérature, la plupart des travaux ne sont réalisés que de manière conceptuelle. Pour fournir des explications parcimonieuses, HAExA s'appuie d'abord sur la génération d'explications normales et contrastées, et ensuite sur leur mise à jour et leur filtrage avant de les communiquer à l'humain. Nous validons nos propositions en concevant et menant des études empiriques d'interaction homme-machine utilisant la simulation orientée-agent. Nos études reposent sur des mesures bien établies pour estimer la compréhension et la satisfaction des explications fournies par HAExA. Les résultats sont analysés et validés à l'aide de tests statistiques paramétriques et non paramétriques.
Pour cela, nous introduisons au Chapitre 3, trois nouveaux modèles prenant en compte les dépendances entre les thèmes relatifs à chaque document pour deux documents successifs. Le deuxième modèle utilise les copules, outil générique servant à modéliser les dépendances entre variables aléatoires. La famille de copules utilisée est la famille des copules Archimédiens et plus précisément la famille des copules de Franck qui vérifient de bonnes propriétés (symétrie, associativité) et qui sont donc adaptés à la modélisation de variables échangeables. Nos expériences numériques, réalisées sur cinq collections standard, mettent en évidence les performances de notre approche, par rapport aux approches existantes dans la littérature comme les dynamic topic models, le temporal LDA et les Evolving Hierarchical Processes, et ceci à la fois sur le plan de la perplexité et en terme de performances lorsqu'on cherche à détecter des thèmes similaires dans des flux de documents. Notre approche, comparée aux autres, se révèle être capable de modéliser un plus grand nombre de situations allant d'une dépendance forte entre les documents à une totale indépendance. Par ailleurs, l'hypothèse d'échangeabilité sous jacente à tous les topics models du type du LDA amène souvent à estimer des thèmes différents pour des mots relevant pourtant du même segment de phrase ce qui n'est pas cohérent. Dans le Chapitre 4, nous introduisons le copulaLDA (copLDA), qui généralise le LDA en intégrant la structure du texte dans le modèle of the text et de relaxer l'hypothèse d'indépendance conditionnelle. Nous montrons de manièreempirique l'efficacité du modèle copLDA pour effectuer à la fois des tâches de natureintrinsèque et extrinsèque sur différents corpus accessibles publiquement. Pour compléter le modèle précédent (copLDA), le chapitre 5 présente un modèle de type LDA qui génére des segments dont les thèmes sont cohérents à l'intérieur de chaque document en faisant de manière simultanée la segmentation des documents et l'affectation des thèmes à chaque mot. La cohérence entre les différents thèmes internes à chaque groupe de mots est assurée grâce aux copules qui relient les thèmes entre eux. De plus ce modèle s'appuie tout à la fois sur des distributions spécifiques pour les thèmes reliés à chaque document et à chaque groupe de mots, ceci permettant de capturer les différents degrés de granularité. Nous montrons que le modèle proposé généralise naturellement plusieurs modèles de type LDA qui ont été introduits pour des tâches similaires. Par ailleurs nos expériences, effectuées sur six bases de données différentes mettent en évidence les performances de notre modèle mesurée de différentes manières : à l'aide de la perplexité, de la Pointwise Mutual Information Normalisée, qui capture la cohérence entre les thèmes et la mesure Micro F1 measure utilisée en classification de texte.
Dans cette thèse, nous proposons une nouvelle approche de l'apprentissage profond pour la classification des flux de données de grande dimension. Au cours des dernières années, les réseaux de neurones sont devenus la référence dans diverses applications d'apprentissage automatique. Cependant, la plupart des méthodes basées sur les réseaux de neurones sont conçues pour résoudre des problèmes d'apprentissage statique. Effectuer un apprentissage profond en ligne est une tâche difficile. La principale difficulté est que les classificateurs basés sur les réseaux de neurones reposent généralement sur l'hypothèse que la séquence des lots de données utilisées pendant l'entraînement est stationnaire ; ou en d'autres termes, que la distribution des classes de données est la même pour tous les lots (hypothèse i.i.d.). Lorsque cette hypothèse ne tient pas les réseaux de neurones ont tendance à oublier les concepts temporairement indisponibles dans le flux. Dans la littérature scientifique, ce phénomène est généralement appelé oubli catastrophique. Les approches que nous proposons ont comme objectif de garantir la nature i.i.d. de chaque lot qui provient du flux et de compenser l'absence de données historiques. Pour ce faire, nous entrainons des modèles génératifs et pseudo-génératifs capable de produire des échantillons synthétiques à partir des classes absentes ou mal représentées dans le flux, et complètent les lots du flux avec ces échantillons. Nous testons nos approches dans un scénario d'apprentissage incrémental et dans un type spécifique de l'apprentissage continu. Nos approches effectuent une classification sur des flux de données dynamiques avec une précision proche des résultats obtenus dans la configuration de classification statique où toutes les données sont disponibles pour la durée de l'apprentissage. En outre, nous démontrons la capacité de nos méthodes à s'adapter à des classes de données invisibles et à de nouvelles instances de catégories de données déjà connues, tout en évitant d'oublier les connaissances précédemment acquises.
Les Véhicules Autonomes (VA) sont des systèmes émergents et considérés comme une pierre angulaire de la mobilité du futur. Leur conception est à l'origine de nombreux efforts de recherche universitaires et industrielles. L'industrialisation des VAs est un moyen pour les acteurs de la mobilité de renforcer le positionnement futur. Les VAs fonctionnent en interagissant avec leur contexte opérationnel (CO) et doivent être adaptés à celui-ci. Ce travail de recherche vise à soutenir les activités d'architecture des Véhicules Autonomes pour aboutir à des architectures adaptées à leurs contextes opérationnels. Une ontologie du CO pour Véhicules Autonomes est proposée pour soutenir l'identification et la définition de scénarios dans la phase initiale de conception, suivant une approche de conception basée sur les scénarios. En utilisant cette ontologie, une méthode de conception de l'architecture logique des VAs basée sur l'OC est proposée. La prise en compte du CO dans les activités de conception d'architecture des VAs est renforcée par une deuxième méthode visant à évaluer l'impact du changement du CO sur l'architecture durant la phase de conception. Les contributions proposées sont validées par des études de cas industriels sur la conception d'architectures AV tenant en compte du CO et de son évolution.
L'établissement de la similarité entre séries temporelles est au cœur de nombreuses tâches d'analyse de données. Les mesures permettant d'établir des similitudes entre les séries temporelles sont spécifiques en ce sens qu'elles doivent pouvoir prendre en compte les différences entre les valeurs constituant la série, ainsi que les distorsions selon l'axe du temps. La mesure de similarité la plus répandue est la mesure Dynamic Time Warping (DTW). Cependant, son calcul est coûteux et son application à des séries temporelles nombreuses et/ou très longues est difficile en pratique. Il s'agit de plonger les séries temporelles dans un espace euclidien construit de telle manière que les distances entre les séries selon la métrique DTW s'y trouvent préservées. Ce manuscrit apporte des contributions majeures : (1) il explique comment les shapelets préservant la DTW peuvent être utilisées dans le contexte spécifique de la recherche de séries temporelles similaires ; (2) il propose des stratégies de sélection de ces shapelets pour faire face à l'échelle, c'est-à-dire pour traiter une collection extrêmement vaste de séries temporelles ; (3) il explique en détail comment gérer les séries temporelles univariées et multivariées, couvrant ainsi tout le spectre des problèmes de recherches et facilitant la moise au point d'applications très diverses. Le coeur de la contribution présentée dans ce manuscrit permet de compenser facilement la complexité du processus de plongement par un jeu sur la précision de la recherche. Des expérimentations utilisant les jeux de données UCR et UEA démontrent l'amélioration considérable des performances par rapport aux techniques de pointe.
Les systèmes intégrant des traitements venant du traitement automatique des langues reposent souvent sur des lexiques et des grammaires, parfois indirectement sur des corpus. A cause de la quantité et de la complexité des informations qu'elles contiennent, ces ressources linguistiques deviennent facilement une source d'incohérence. Dans cette thèse, nous explorons les moyens d'améliorer la gestion des nombreuses ressources linguistiques d'un moteur de recherche industriel en dix-neuf langues qui fait appel à une analyse textuelle élaborée. Nous proposons une méthode pour formaliser l'architecture linguistique des traitements linguistiques et des ressources utilisées par ceux-ci. Cette formalisation explicite la façon dont les connaissances contenues dans les ressources sont exploitées. Grâce à elle, nous pouvons construire des outils de gestion qui respectent l'architecture du système. L'environnement ainsi mis en place se concentre sur la mise à jour et l'acquisition des ressources linguistiques, leur exploitation étant figée par des contraintes industrielles.
Nous proposons une contribution empirique aux tentatives récentes d'unification des sciences cognitives et des sciences sociales. La Théorie de l'Attraction Culturelle (CAT) propose de s'atteler à des questions interdisciplinaires en utilisant une ontologie commune faite de représentations. D'après la CAT, malgré des transformations au niveau micro, la distribution globale des représentations peut rester stable grâce à des attracteurs culturels. Nous présentons deux études de cas sur de courts énoncés écrits. La première examine les changements que des citations subissent lorsqu'elles sont copiées en ligne. En combinant psycholinguistique et fouille de données, nous montrons que les substitutions de mots sont cohérentes avec l'hypothèse des attracteurs culturels, et avec les effets connus de variables lexicales. La deuxième étude étend ces résultats, et utilise une expérience web permettant de récolter des chaînes de transmission de qualité et en grande quantité. En étendant un algorithme bioinformatique, nous décomposons les transformations en des opérations plus simples, et proposons un premier modèle descriptif du processus qui relie les connaissances psycholinguistiques sur la transformation de phrases aux tendances de haut niveau identifiées dans la littérature sur l'évolution culturelle. Enfin, nous montrons que la compréhension de l'évolution de telles représentations nécessite une théorie du sens des énoncés, une tâche pour laquelle nous explorons les approches empiriques possibles.
De nombreuses applications de flux de données ont vu le jour au cours des dernières années. Lorsque l'environnement évolue, il est nécessaire de s'appuyer sur un apprentissage en ligne pouvant s'adapter aux conditions changeantes, alias dérives de concept. L'adaptation aux dérives de concept implique d'oublier une partie ou la totalité des connaissances acquises lorsque le concept change, tout en accumulant des connaissances sur le concept sous-jacent supposé stationnaire. Les méthodes d'ensemble ont été parmi les approches les plus réussies. Cependant, la gestion de l'ensemble qui détermine les informations à oublier n'a pas été complètement étudiée jusqu'ici. Notre travail montre l'importance de la stratégie de l'oubli en comparant plusieurs approches. Les résultats ainsi obtenus nous amènent à proposer une nouvelle méthode d'ensemble avec une stratégie d'oubli conçue pour s'adapter aux dérives de concept. Des évaluations empiriques montrent que notre méthode se compare favorablement aux systèmes adaptatifs de l'état de l'art. Dans ce travail, nous allons plus loin en introduisant un mécanisme d'anticipation capable de détecter des états pertinents de l'environnement, de reconnaître les contextes récurrents et d'anticiper les changements de concept susceptibles. Par conséquent, la méthode que nous proposons traite à la fois le défi d'optimiser le dilemme stabilité-plasticité, l'anticipation et la reconnaissance des futurs concepts. Ceci est accompli grâce à une méthode d'ensemble qui contrôle un comité d'apprenants.
Cette thèse étudie l'intégration de repères phonétiques dans la reconnaissance automatique de la parole (RAP) continue à grand vocabulaire. Les repères sont des événements à temps discret indiquant la présence d'événements phonétiques dans le signal de parole. Le but est de développer des détecteurs de repères qui sont motivés par la connaissance phonétique afin de modéliser quelques événements phonétiques plus précisément. La thèse présente deux approches de détection de repères, qui utilisent l'information extraite par segments et étudie deux méthodes différentes pour intégrer les repères dans le décodage, qui sont un élagage basé sur les repères et une approche reposant sur les combinaisons pondérées. Alors que les deux approches de détection de repères présentées améliorent les performance de reconnaissance de la parole comparées à l'approche de référence, elles ne surpassent pas les prédictions phonétiques standards par trame. Ces résultats indiquant que la RAP guidée par des repères nécessite de l'information phonétique très hétérogène pour être efficace, la thèse présente une troisième méthode d'intégration conçue pour intégrer un nombre arbitraire de flux de repères hétérogènes et asynchrones dans la RAP. Les résultats indiquent que cette méthode est en effet en mesure d'améliorer le système de référence, pourvu que les repères fournissent de l'information complémentaire aux modèles acoustiques standards.
Les industriels et les opérateurs des flottes de systèmes cyber-physiques (CPS) sont soumis à de fortes exigences exprimées en termes de disponibilité, fiabilité des produits et des services fournis lors de l'exploitation de ces flottes dans des environnements dynamiques. Ces attentes incitent les industriels, et notamment dans le secteur du transport, à développer des mécanismes efficaces de planification réactive des opérations de maintenance au niveau de la flotte. Dans cette thèse, un système multi-agent (SMA) pour la planification réactive de la maintenance d'une flotte de CPS est proposé. Ce SMA est construit en utilisant la méthode de conception ANEMONA et a pour objectif d'optimiser la planification de la maintenance au niveau flotte afin de répondre aux exigences spécifiées. Les expériences réalisées au cours de ces travaux démontrent la capacité de ce SMA à planifier la maintenance de la flotte de manière efficace (c'est-à-dire satisfaire les exigences de disponibilité et de fiabilité de la flotte dans un environnement statique) et de manière réactive (c'est-à-dire être capable d'adapter/de modifier les décisions de planification de la maintenance à la suite des perturbations). L'efficacité de ce modèle SMA est validée par un modèle mathématique et sa réactivité est testée par simulation de perturbations. Une application dans le domaine ferroviaire au sein de Bombardier Transport France est proposée. Le SMA est intégré à un système d'aide à la décision dénommé « MainFleet » . Le développement de MainFleet est en cours.
Ce travail porte sur la segmentation jointe d'un ensemble d'images dans un cadre bayésien. Le modèle proposé combine le processus de Dirichlet hiérarchique (HDP) et le champ de Potts. Ainsi, pour un groupe d'images, chacune est divisée en régions homogènes et les régions similaires entre images sont regroupées en classes. D'une part, grâce au HDP, il n'est pas nécessaire de définir a priori le nombre de régions par image et le nombre de classes, communes ou non. D'autre part, le champ de Potts assure une homogénéité spatiale. Les lois a priori et a posteriori en découlant sont complexes rendant impossible le calcul analytique d'estimateurs. Un algorithme de Gibbs est alors proposé pour générer des échantillons de la loi a posteriori. De plus,un algorithme de Swendsen-Wang généralisé est développé pour une meilleure exploration dela loi a posteriori. Enfin, un algorithme de Monte Carlo séquentiel a été défini pour l'estimation des hyperparamètres du modèle. Le choix de la meilleure partition se fait par minimisation d'un critère indépendant de la numérotation. Ces méthodes ont été évaluées sur des images-test et sur des images naturelles. Les performances de l'algorithme sont évaluées via des métriques connues en statistiques mais peu utilisées en segmentation d'image.
Comment les mots sont-ils reconnus ? Comment avons-nous accès à la signification des mots ? Ces questions ont été explorées dans des études sur l'accès lexical et la reconnaissance des mots durant le demi-siècle dernier dans les domaines de la psycho-, neuro- et de la linguistique. Le traitement morphologique est un niveau essentiel de traitement pour l'extraction d'information lors de la reconnaissance de mots. A un extrême, les modèles de pleine-entrée proposent le stockage du mot entier dans la mémoire et un traitement morphologique post-lexical paradigmatiques ;  à l'autre extrême, les modèles décompositionnels proposent une décomposition pré-lexicale et une activation morphologique basée sur des règles ;  entre les deux, les modèles à double-mécanismes postulent deux voies pour la reconnaissance des mots, une route associative avec les mots entiers et une route combinatoire basée sur des règles. Dans la présente thèse, le traitement morphologique des verbes fléchis en français a été étudié en modalité visuelle dans cinq études. L'étude 1 a recherché à mettre à jour l'organisation du lexique mental en utilisant les effets de fréquences de surface et les effets de fréquences cumulée ;  l'étude 2 a exploré l'impact des différents processus de formation du radical verbal ;  l'étude 3 a étudié les opérations morphologiques au travers des suffixes flexionnels ;  l'étude 4 a testé le traitement morphologique verbal pour des locuteurs de français comme L2 ;  et l'étude 5 a exploré les violations morphologiques verbales via des mesures électro-encéphalographiques. Globalement les résultats suggèrent que tous les verbes français fléchis sont traités par un mécanisme unique avec décomposition morphologique pré-lexicale pour l'accès lexical et la reconnaissance des mots. Il est proposé un traitement différent pour les morphèmes lexicaux et fonctionnels. Les mots sont décomposés en morphèmes atomiques, les représentations morphologiques sont activées dans le lexique mental, et les constituants de mots sont recombinés pour la vérification de mot
La maîtrise de la qualité de la production est un objectif particulièrement important pour la croissance des industries. Contrôler la qualité d'un produit nécessite de la mesurer. Le contrôle de cent pourcent des produits est un objectif important pour dépasser les limites du contrôle par prélèvement, dans le cas de défauts liés à des causes exceptionnelles. Cependant, les contraintes industrielles ont limité le déploiement de la mesure des caractéristiques des produits directement au sein des lignes de production. Le déploiement du contrôle visuel humain est limité par sa durée incompatible avec la durée du cycle des productions à haute cadence, par son coût et par sa variabilité. L'intégration de systèmes de vision informatique présente un coût qui les réservent aux productions à hautes valeurs ajoutées. De plus, le contrôle automatique de la qualité de l'aspect des produits reste une thématique de recherche ouverte. Notre travail a pour objectifs de répondre à ces contraintes, dans le cadre du procédé d'injection Nous proposons un système de contrôle qui est non invasif pour le procédé de production. Les pièces sont contrôlées dès la sortie de la presse à injecter. Nous étudierons l'apport de l'imagerie non-conventionnelle. La thermographie d'une pièce moulée chaude permet d'obtenir une information sur sa géométrie, qui est complémentaire de l'imagerie conventionnelle. De plus, les cahiers des charges des produits présentent de plus en plus d'exigences tant sur les géométries complexes que sur l'aspect. Cependant, les caractéristiques d'aspect sont difficiles à formaliser. Pour automatiser le contrôle d'aspect, il est nécessaire de modéliser la notion de qualité d'une pièce. Afin d'exploiter les mesures réalisées sur les pièces chaudes, notre approche utilise des méthodes d'apprentissage statistique. Ainsi, l'expert humain qui connait la notion de qualité d'une pièce transmet son savoir au système, par l'annotation d'un jeu de données d'apprentissage. Notre système de contrôle apprend alors une métrique de la qualité d'une pièce, à partir des données brutes issues capteurs. Nous avons privilégier une approche par réseaux de convolution profonds (textit{Deep Learning}) afin d'obtenir les meilleurs performances en justesse de discrimination des pièces conformes. La faible quantité d'échantillons annotés disponible dans notre contexte industrielle nous ont amenée à utiliser des méthodes d'apprentissage par transfert de domaine. Enfin, afin de répondre à l'ensemble des contraintes, nous avons réalisé l'intégration verticale d'une prototype de dispositif de mesure des pièces et de la solution logicielle de traitement par apprentissage statistique. Le dispositif intègre l'imagerie thermique, polarimétrique, l'éclairage et le système de traitement embarqué nécessaire à l'envoi des données sur un serveur d'analyse distant. Deux cas d'applications permettent d'évaluer les performances et la viabilité de la solution proposée
Cette thèse comprend trois essais dont l'objectif est d'étudier les déterminants et les implications de la qualité de l'information narrative des entreprises cotées. Afin d'appréhender la qualité de cette information nous recourons à des techniques de traitement automatique de langage naturel qui permettent de construire des indices de lisibilité des rapports annuels. Le premier essai étudie l'effet de la complexité textuelle des rapports annuels sur la liquidité des titres. L'utilisation d'un échantillon d'entreprises françaises cotées en bourse sur la période 2002-2013 montre l'existence d'une relation positive entre le degré de lisibilité des rapports annuels et la liquidité des titres. Ces résultats suggèrent que la complexité textuelle de l'information narrative affecte les investisseurs sur le marché des actions. Le deuxième essai étudie l'effet de la lisibilité des rapports annuels sur le coût des fonds propres des entreprises. Le troisième essai examine l'effet des pratiques de réduction d'impôt des entreprises sur la lisibilité de leurs divulgations financières. La littérature mobilisant la théorie d'agence montre que ces pratiques de réduction d'impôt créent un cadre permettant aux dirigeants d'extraire des bénéfices privés aux dépens des autres parties prenantes. En utilisant un échantillon d'entreprises américaines cotées en bourse pour la période 1995-2012, nous constatons que les entreprises qui s'engagent dans des politiques de diminution d'impôt publient des rapports annuels moins lisibles et plus ambigus.
En s'appuyant sur un corpus constitué de six-cents articles de recherche et comptabilisant plus de cinq-millions de mots, l'objectif est double. Il s'agit d'abord de comprendre les mécanismes et procédés par lesquels les SHS se construisent leur appareillage terminologique pour ensuite dégager les profils combinatoires des termes ainsi que leur fonctionnement interactif contextuel. La démarche s'inscrivant dans le cadre de la linguistique de corpus et celle du traitement automatique des langues naturelles (TALN), l'approche se veut donc sémasiologique, combinant socioterminologie et lexicométrie. L'analyse du comportement de ces items en contexte permet en effet de relever les glissements sémantiques et leur relation. Nous partons du postulat que le terme, bien que souvent présenté comme monosémique et stable dans le rapport biunivoque qu'il entretient avec le concept qu'il dénomme, semble être sujet à des variations dénominatives et est, de ce fait, révélateur de stratégies d'énonciation qui se manifestent dans sa lexicogenèse.
La disponibilité de grandes masses de textes sous forme informatique rend possible aujourd'hui la mise en œuvre de méthodes qui construisent des représentations du sens des mots par exploration automatique exhaustive de leur usage dans ces textes. Ces représentations sont au cœur de nombreuses applications de traitement automatique des langues, de l'extraction d'information à la traduction automatique en passant par la recherche de réponses à des questions. Cependant, dans les domaines spécialisés, deux facteurs rendent ces méthodes plus difficilement applicables. D'une part, les corpus spécialisés à un domaine sont nécessairement de taille moins grande que les corpus non restreints à un domaine, alors que la taille ces corpus employés est un facteur clé dans la qualité des représentations construites. D'autre part, les termes complexes (formés de plusieurs mots) ont une importance particulière dans les domaines spécialisés, alors que les méthodes standard sont conçues pour traiter des termes simples (formés d'un mot). Le projet ADDICTE vise à surmonter ces difficultés en donnant une meilleure représentation des termes complexes, en enrichissant l'analyse des textes d'un domaine par des ressources additionnelles (terminologies du domaine et textes hors domaine), et en mettant au point des représentations qui peuvent tirer parti de ces informations supplémentaires. Dans ce contexte, la thèse proposée porte spécifiquement sur la conception et le test de méthodes visant à faire bénéficier de ressources externes la construction de représentations distributionnelles.
Cette thèse s'intègre dans le cadre du traitement automatique du langage naturel. La problématique du résumé automatique de documents arabes qui a été abordée, dans cette thèse, s'est cristallisée autour de deux points. Le premier point concerne les critères utilisés pour décider du contenu essentiel à extraire. Le deuxième point se focalise sur les moyens qui permettent d'exprimer le contenu essentiel extrait sous la forme d'un texte ciblant les besoins potentiels d'un utilisateur. Afin de montrer la faisabilité de notre approche, nous avons développé le système "L.A.E", basé sur une approche hybride qui combine une analyse symbolique avec un traitement numérique. Les résultats d'évaluation de ce système sont encourageants et prouvent la performance de l'approche hybride proposée. Ces résultats, ont montré, en premier lieu, l'applicabilité de l'approche dans le contexte de documents sans restriction quant à leur thème (Éducation, Sport, Science, Politique, Reportage, etc.), leur contenu et leur volume. Ils ont aussi montré l'importance de l'apprentissage dans la phase de classement et sélection des phrases forment l'extrait final.
Cette thèse s'inscrit dans la problématique de l'extraction de sens à partir de textes et flux textuels, produits dans notre cas lors de processus collaboratifs. Plus précisément, nous nous intéressons aux courriels de travail et aux documents textuels objets de collaboration, avec une première application aux documents éducatifs. La motivation de cet intérêt est d'aider les utilisateurs à accéder plus rapidement aux informations utiles ; nous cherchons donc à les repérer dans les textes. Ainsi, nous nous intéressons aux tâches dans les courriels, et aux fragments de documents éducatifs qui concernent les thèmes de leurs intérêts. Deux corpus, un de courriels et un de documents éducatifs, principalement en français, ont été constitués. Cela était indispensable, car il n'y a pratiquement pas de travaux antérieurs sur ce type de données en français. Notre première contribution théorique est une modélisation générique de la structure de ces données. Nous l'utilisons pour spécifier le traitement formel des documents, prérequis au traitement sémantique. Nous démontrons la difficulté du problème de segmentation, normalisation et structuration de documents en différents formats source, et présentons l'outil SEGNORM, première contribution logicielle de cette thèse. SEGNORM segmente et normalise les documents (en texte brut ou balisé), récursivement et en unités de taille paramétrable. Dans le cas des courriels, il segmente les messages contenant des messages cités en messages individuels, en conservant l'information du chaînage entre les fragments entremêlés. Il analyse également les métadonnées des messages pour reconstruire les fils de discussions, et retrouve dans les citations les messages dont on ne possède pas le fichier source. Nous abordons ensuite le traitement sémantique de ces documents. Nous proposons une modélisation (ontologique) de la notion de tâche, puis décrivons l'annotation d'un corpus de plusieurs centaines de messages issus du contexte professionnel de VISEO et du GETALP. Nous présentons alors la deuxième contribution logicielle de cette thèse, un outil de repérage de tâches et d'extraction de leurs attributs (contraintes temporelles, assignataires, etc.). Cet outil, basé sur une combinaison d'une approche experte et d'apprentissage automatique, est évalué selon des critères classiques de précision, rappel et F-mesure, ainsi que selon la qualité d'usage. Enfin, nous présentons nos travaux sur la plate-forme MACAU-CHAMILO, troisième contribution logicielle, qui aide à l'apprentissage par (1) structuration de documents pédagogiques selon deux ontologies (forme et contenu), (2) accès multilingue à du contenu initialement monolingue. Il s'agit donc de nouveau de structuration selon les deux axes, forme et sens. (1) L'ontologie des formes permet d'annoter les fragments des documents par des concepts comme théorème, preuve, exemple, par des niveaux de difficulté et d'abstraction, et par des relations comme élaboration_de, illustration_de. L'ontologie de domaine modélise les objets formels de l'informatique, et plus précisément les notions de complexité calculatoire. Cela permet de suggérer aux utilisateurs des fragments utiles pour la compréhension de notions d'informatique perçues comme abstraites ou difficiles. (2) L'aspect relatif à l'accès multilingue a été motivé par le constat que nos universités accueillent un grand nombre d'étudiants étrangers, qui ont souvent du mal à comprendre nos cours à cause de la barrière linguistique. Nous avons proposé une approche pour multilingualiser du contenu pédagogique avec l'aide d'étudiants étrangers, par post-édition en ligne de pré-traductions automatiques, puis, si besoin, amélioration incrémentale de ces post-éditions. (Nos expériences ont montré que des versions multilingues de documents peuvent être produites rapidement et sans coût.) Ce travail a abouti à un corpus de plus de 500 pages standard (250 mots/page) de contenu pédagogique post-édité vers le chinois.
Nous présentons, dans ce manuscrit, un dispositif informatique d'aide à la formation des futurs enseignants de FLE en Colombie. Il prend ses sources dans la linguistique textuelle et cherche à améliorer le niveau linguistique des étudiants universitaires actuellement en formation. Pour ce faire, le dispositif est fondé sur un corpus textuel spécifiquement annoté et étiqueté grâce aux outils de traitement automatique de langues (TAL) et à des annotations manuelles en format XML. Ceci permet de développer des activités à visée formative, en tenant compte des besoins exprimés par les publics cibles (enseignants-formateurs et leurs étudiants en formation). Comme nous l'exposons tout au long de cette thèse, l'élaboration d'un système comme le nôtre est le produit de la mise en œuvre de connaissances et de compétences issues de plusieurs disciplines et/ou domaines  : didactique des langues, ingénierie pédagogique, linguistique générale, linguistique textuelle, linguistique de corpus, TAL et ALAO. Il se veut, principalement, un dispositif pédagogique pour la formation des étudiants en FLE dans le contexte de l'éducation supérieure en Colombie, un outil pensé en fonction des besoins et des objectifs de cet apprentissage. L'originalité de notre système repose sur le type de public choisi, le modèle didactique de formation mis en œuvre et la spécificité du corpus utilisé. À notre connaissance, il s'agit d'un des premiers systèmes d'ALAO fondé sur la linguistique textuelle s'adressant à la formation des futurs enseignants de FLE dans un contexte exolingue.
Internet propose aujourd'hui aux utilisateurs de services en ligne de commenter, d'éditer et de partager leurs points de vue sur différents sujets de discussion. Ce type de contenu est maintenant devenu la ressource principale pour les analyses d'opinions sur Internet. Néanmoins, à cause des abréviations, du bruit, des fautes d'orthographe et toutes autres sortes de problèmes, les outils de traitements automatiques des langues, y compris les reconnaisseurs d'entités nommées et les étiqueteurs automatiques morphosyntaxiques, ont des performances plus faibles que sur les textes bien-formés (Ritter et al., 2011). Cette thèse a pour objet la reconnaissance d'entités nommées sur les contenus générés par les utilisateurs sur Internet. Nous avons établi un corpus d'évaluation avec des textes multi-sources et multi-domaines. Ensuite, nous avons développé un modèle de champs conditionnels aléatoires, entrainé sur un corpus annoté provenant des contenus générés par les utilisateurs. Dans le but d'améliorer les résultats de la reconnaissance d'entités nommées, nous avons d'abord développé un étiqueteur morpho-syntaxique sur les contenus générés par les utilisateurs et nous avons utilisé les étiquettesprédites comme un attribut du modèle des champs conditionnels aléatoire. Enfin, pour transformer les contenus générés par les utilisateurs en textes bien-formés, nous avons développé un modèle de normalisation lexicale basé sur des réseaux de neurones pour proposer une forme correcte pour les mots non-standard.
L'accès à l'information textuelle est devenu une composante essentielle de nombreux systèmes d'information. Le paradigme de la lecture automatique a été récemment proposé. Son objectif est de formaliser le processus de lecture comme un apprentissage supervisé de 'bout-en-bout'où l'objectif d'un modèle est de répondre à une question sur un texte donné. Actuellement, les modèles proposés nécessitent une grande quantité de données et il a été montré qu'ils se révèlent particulièrement sensibles au bruit. L'objectif de ce projet est d'étudier comment un processus d'apprentissage adversarial permettrais d'appréhender ces limitations.
Différentes composantes des systèmes de traduction automatique statistique sont considérées comme des problèmes d'optimisations. En effet, l'apprentissage du modèle de traduction, le décodage et l'optimisation des poids de la fonction log-linéaire sont trois importants problèmes d'optimisation. Savoir définir les bons algorithmes pour les résoudre est l'une des tâches les plus importantes afin de mettre en place un système de traduction performant. Plusieurs algorithmes d'optimisation sont proposés pour traiter les problèmes d'optimisation du décodeur. Ils sont combinés pour résoudre, d'une part, le problème de décodage qui produit une traduction dans la langue cible d'une phrase source, d'autre part, le problème d'optimisation des poids des scores combinés dans la fonction log-linéaire pour d'évaluation des hypothèses de traduction au cours du décodage. Le système de traduction statistique de référence est basé sur un algorithme de recherche en faisceau pour le décodage, et un algorithme de recherche linéaire pour l'optimisation des poids associés aux scores. Nous proposons un nouveau système de traduction avec un décodeur entièrement basé sur les algorithmes génétiques. Les algorithmes génétiques sont des algorithmes d'optimisation bio-inspirés qui simulent le processus de l'évolution naturelle des espèces. Ils permettent de manipuler un ensemble de solutions à travers plusieurs itérations pour converger vers des solutions optimales. Ce travail, nous permet d'étudier l'efficacité des algorithmes génétiques pour la traduction automatique statistique. Ce travail, nous a permis de proposer un nouveau système de traduction automatique statistique ayant un décodeur entièrement basé sur des algorithmes génétiques
La sécurité de l'information repose sur la bonne interaction entre différents niveaux d'abstraction : les composants matériels, systèmes d'exploitation, algorithmes, et réseaux de communication. Cependant, protéger ces éléments a un coût ; ainsi de nombreux appareils sont laissés sans bonne couverture. Cette thèse s'intéresse à ces différents aspects, du point de vue de la sécurité et de la cryptographie. Nous décrivons ainsi de nouveaux algorithmes cryptographiques (tels que des raffinements du chiffrement de Naccache–Stern), de nouveaux protocoles (dont un algorithme d'identification distribuée à divulgation nulle de connaissance), des algorithmes améliorés (dont un nouveau code correcteur et un algorithme efficace de multiplication d'entiers),ainsi que plusieurs contributions à visée systémique relevant de la sécurité de l'information et à l'intrusion. En outre, plusieurs de ces contributions s'attachent à l'amélioration des performances des constructions existantes ou introduites dans cette thèse.
Elle consiste à identifier certains objets textuels tels que les noms de personne, d' organisation et de lieu. Le travail de cette thèse se concentre sur la tâche de reconnaissance des entités nommées pour la modalité orale. Dans un premier temps, nous étudions les spécificités de la reconnaissance des entités nommées en aval du système de reconnaissance automatique de la parole. Nous présentons une méthode pour la reconnaissance des entités nommées dans les transcription de la parole en adoptant une taxonomie hiérarchique et compositionnelle. Nous mesurons l'impact des différents phénomènes spécifiques à la parole sur la qualité de reconnaissance des entités nommées. Dans un second temps, nous proposons d'étudier le couplage étroit entre la tâche de transcription de la parole et la tâche de reconnaissance des entités nommées. Dans ce but, nous détournons les fonctionnalités de base d'un système de transcription de la parole pour le transformer en un système de reconnaissance des entités nommées. Ainsi, en mobilisant les connaissances propres au traitement de la parole dans le cadre de la tâche liée à la reconnaissance des entités nommées, nous assurons une plus grande synergie entre ces deux tâches. Nous menons différents types d'expérimentations afin d'optimiser et d'évaluer notre approche.
Dans cette thèse, nous avons utilisé les données de l'IRM du conduit vocal pour étudier la production de la parole. La première partie consiste en l'étude de l'impact que le vélum, l'épiglotte et la position de la tête a sur la phonation de cinq voyelles françaises. Des simulations acoustiques ont été utilisées pour comparer les formants des cas étudiés avec la référence afin de mesurer leur impact. Par conséquent, la deuxième partie présente quelques algorithmes que l'on peut utiliser pour améliorer les données de production de la parole. Plusieurs transformations d'images ont été combinées afin de générer des estimations des formes du conduit vocal qui sont plus informatives que les originales. À ce stade, nous avons envisagé, outre l'amélioration des données de production de la parole, de créer un modèle de référence générique qui pourrait fournir des informations améliorées non pas pour un sujet spécifique, mais globalement pour la parole. Enfin, la dernière partie de la thèse, fait référence à une sélection de questions ouvertes du domaine qui restent encore sans réponse, quelques pistes intéressantes que l'on peut développer à partir de cette thèse et quelques approches potentielles qui pourraient être envisager afin de répondre à ces questions.
Le but de cette these consiste en l'extraction d'un echantillon de textes pour les modeles markoviens. Pour cela, nous avons utilise la stratification. Nous avons developpe un logiciel "echantillonnage multivarie" qui extrait un echantillon d'un corpus stratifie categorise et ambigu, minimisant une "perte d'information" selon un certain sens. Les resultats obtenus sont tres satisfaisants puisque nous avons ameliore le nombre de levees d'ambiguites morphologiques. Cette probabilite n'est pas unique quelque soit la topologie consideree. Grace aux isometries de la norme li, nous avons demontre qu'il est impossible d'obtenir une solution unique a ce probleme. L'obtention d'une solution unique contraint les representation du "vrai" et du "faux" a se confondre. Il faut egalement distinguer le "vrai" associe a une formule logique d'un "evenement certain" de la theorie des probabilites. Finalement, nous avons propose un nouveau modele de markov capable de tenir compte du contexte associe a une categorie morphologique.
Nous avons travaillé sur un formalisme appelé grammaires d'arbres adjoints (tag) qui appartient a la famille des grammaires d'unification mais avec la particularité d'être basé sur des structures arborescente. L'intérêt théorique est double : d'un point de vue linguistique, les tag élargissent la notion de localité et permettent de redéfinir les interactions multiples entre lexique, syntaxe et sémantique ;  nous avons ajouté une contrainte de lexicalisation qui les rend a même de formaliser les hypothèses qui sous-tendent les lexiques-grammaires de M. GROSS et peuvent contribuer à les préciser. D'un point de vue informatique, les tags sont un peu plus puissants que les grammaires hors contexte mais gardent des propriétés informatiques intéressantes (temps de traitement, décidabilité). En particulier, nous remettons en cause la notion de groupe verbal. Nous comparons à chaque fois les représentations retenues en tag avec celles utilisées dans d'autres formalismes (gpsg, lfg, grammaires en chaine). Nous présentons enfin la stratégie d'analyse spécifique aux grammaires lexicalisées et l'état actuel de l'implémentation de notre analyseur.
La majorité des systèmes embarqués récents sont basées sur des architectures massivement parallèles MPSoC, d'où la nécessité de développer des applications parallèles embarquées. La conception et le développement d'une application parallèle embarquée devient de plus en plus difficile notamment pour les architectures multiprocesseurs hétérogènes ayant différents types de contraintes de communication et de conception tels que le coût du matériel, la puissance et la rapidité. Cela est particulièrement important pour les systèmes embarqués de type MPSoC, où les applications doivent fonctionner correctement sur de nombreux cœurs. En outre, la performance d'une application ne s'améliore pas forcément lorsque l'application tourne sur un nombre de cœurs encore plus grand. La performance d'une application peut être limitée en raison de multiples goulot d'étranglement notamment la contention sur des ressources partagées telles que les caches et la mémoire. Cela devient contraignant etune perte de temps pour un développeur de faire un profilage de l'application parallèle embarquée et d'identifier des goulots d'étranglement dans le code source qui diminuent la performance de l'application. Pour surmonter ces problèmes, dans cette thèse, nous proposons trois méthodes automatiques qui détectent les instructions du code source qui ont conduit à une diminution de performance due à la contention et à l'évolutivité des processeurs sur une puce. Les méthodes sont basées sur des techniques de fouille de données exploitant des gigaoctets de traces d'exécution de bas niveau produites par les platesformes MPSoC. Nos approches de profilage permettent de quantifier et de localiser automatiquement les goulots d'étranglement dans le code source afin d'aider les développeurs à optimiserleurs applications parallèles embarquées. Nous avons effectué plusieurs expériences sur plusieurs applications parallèles embarquées. Nos expériences montrent la précision des techniques proposées, en quantifiant et localisant avec précision les hotspots dans le code source.
La thèse porte sur le développement d'architectures neuronales profondes permettant d'analyser des contenus textuels ou visuels, ou la combinaison des deux. Réseaux récurrents pour la compréhension de la parole : différentes architectures de réseaux sont comparées pour cette tâche sur leurs facultés à modéliser les observations ainsi que les dépendances sur les étiquettes à prédire. L'approche été étudiée principalement en structuration de collections de vidéos, dons le cadre d'évaluations internationales où l'approche proposée s'est imposée comme l'état de l'art.
Les réseaux sociaux et les communautés en ligne sont devenus des acteurs majeurs dans le domaine du tourisme. Les informations échangées sur ces communautés influencent de plus en plus le comportement du consommateur. Dans le même temps, le nombre des utilisateurs des téléphones mobiles est aussi en pleine croissance. Les mobiles proposent à leurs utilisateurs différents services. Les réseaux sociaux associés à la technologie mobile pourront ainsi constituer une source d'information pour le touriste et être ainsi considérés comme un facteur clé qui influence son comportement durant son voyage. Notre propos serait ainsi d'étudier le comportement du touriste dans le cadre de l'utilisation d'une application mobile qui se base sur le partage des avis et des retours d'
Ce travail a pour objet l'adaptation des toponymes thaïlandais en français dans un corpus de quatre guides touristiques francophones. Les analyses linguistiques et traductologiques montrent que les toponymes thaïlandais sont bien intégrés en français aux différents niveaux de leur adaptation. Ils sont d'abord romanisés par divers systèmes, parfois avec la francisation graphématique. Au niveau sémantico-référentiel, leur valeur fondamentale est locative mais dans certains contextes, ils peuvent subir une interprétation métonymique et métaphorique. Ainsi le transfert sémantique est possible par les divers procédés traductologiques. Avec la traduction libre, l'auteur peut modifier la traduction de la dénomination d'origine ou créer une nouvelle forme dénominative en présentant la caractérisation dominante du référent. Ces stratégies soulignent une fonction pragmatique spécifique du guide touristique : permettre au lecteur d'identifier des lieux qui lui sont inconnus en suscitant son intérêt pour une langue-culture étrangère.
Cette recherche vise à comprendre comment le bouche-à-oreille électronique visuel émis par un youtubeur influence le sentiment et l'intention d'achat des consommateurs. Nous étudions l'impact des caractéristiques du bouche-à-oreille et l'effet de la contagion sociale. Les résultats principaux sont : 1. Le statut professionnel du youtubeur, l'existence de liens commerciaux entre marques et youtubeurs et la valence de la revue ont un impact sur le sentiment exprimé et par conséquent l'intention d'achat.2. Il existe une contagion sociale, à la fois émotionnelle et comportementale au sein de l'audience d'un bouche-à-oreille électronique et visuel qui impacte son efficacité.3. Le nombre d'abonnés d'un individu (caractéristique individuelle du commentateur) impacte sa sensibilité à la contagion sociale.
Le volume de données disponible croît de manière très importante et ouvre d'importants défis pour les exploiter. Les domaines scientifiques du Web sémantique et des ontologies sont alors une réponse pour aider à traiter les données de manière efficace. Le domaine que nous considérons dans nos travaux est celui des Interventions Pour ce faire, il est essentiel de disposer d'une classification évolutive et consensuelle effectuée au niveau international pour les spécialistes. Dans ce domaine, le développement d'une ontologie est crucial pour faciliter les recherches bibliographiques et mettre en place des bonnes pratiques. La construction manuelle de l'ontologie est en effet fastidieuse et longue. En particulier, la collecte des termes liés au domaine des INM nécessite beaucoup d'efforts et de temps tant le champ du vocabulaire est large. Ainsi le terme INM lui-même est parfois remplacé par d'autres (médecines alternatives, médecines douces, etc). Ainsi, un outil de visualisation doit être proposé pour les experts des INM. Des contributions sont proposées dans cette thèse sur ces deux sujets (construction du vocabulaire et visualisation). Deux approches sont présentées pour la construction, l'une reposant sur la connaissance experte et l'autre sur un corpus. Une mesure de similarité est introduite et évaluée. Pour la visualisation, notre proposition repose sur l'utilisation de cartes conceptuelles. Un outil a été implémenté, permettant de transformer les ontologies décrites en OWL pour les visualiser.
En prenant en compte le contexte industriel, nous avons centré notre recherche sur certains problèmes, informatiques et lexicographiques. Maurel, D., 2006]. Nous montrons aussi comment l'étendre pour répondre à de nouveaux besoins, comme ceux du projet INNOVALANGUES. Enfin, nous avons créé un "intergiciel de lemmatisation", LEXTOH, qui permet d'appeler plusieurs analyseurs morphologiques ou lemmatiseurs, puis de fusionner et filtrer leurs résultats. Combiné à un nouvel outil de création de dictionnaires, CREATDICO, LEXTOH permet de construire à la volée un "mini-dictionnaire" correspondant à une phrase ou à un paragraphe d'un texte en cours de "post-édition" en ligne sous IMAG/SECTRA, ce qui réalise la fonctionnalité d'aide lexicale proactive prévue dans [Huynh, C.-P., 2010]. On pourra aussi l'utiliser pour créer des corpus parallèles "factorisés" pour construire des systèmes de TA en MOSES.
Grâce aux progrès impressionnants qui ont été réalisés dans la transcription du langage parlé, il est de plus en plus possible d'exploiter les données transcrites pour des tâches qui requièrent la compréhension de ce que l'on dit dans une conversation. Le travail présenté dans cette thèse, réalisé dans le cadre d'un projet consacré au développement d'un assistant de réunion, contribue aux efforts en cours pour apprendre aux machines à comprendre les dialogues des réunions multipartites. Nous nous sommes concentrés sur le défi de générer automatiquement les résumés abstractifs de réunion. Nous présentons tout d'abord nos résultats sur le Résumé Abstractif de Réunion (RAR), qui consiste à prendre une transcription de réunion comme entrée et à produire un résumé abstractif comme sortie. Nous introduisons une approche entièrement non-supervisée pour cette tâche, basée sur la compression multi-phrases et la maximisation sous-modulaire budgétisée. Nous tirons également parti des progrès récents en vecteurs de mots et dégénérescence de graphes appliqués au TAL, afin de prendre en compte les connaissances sémantiques extérieures et de concevoir de nouvelles mesures de diversité et d'informativité. Ensuite, nous discutons de notre travail sur la Classification en Actes de Dialogue (CAD), dont le but est d'attribuer à chaque énoncé d'un discours une étiquette qui représente son intention communicative. La CAD produit des annotations qui sont utiles pour une grande variété de tâches, y compris le RAR. Nous proposons une couche neuronale modifiée de Champ Aléatoire Conditionnel (CAC) qui prend en compte non seulement la séquence des énoncés dans un discours, mais aussi les informations sur les locuteurs et en particulier, s'il y a eu un changement de locuteur d'un énoncé à l'autre. Nous proposons une nouvelle approche de la DCA dans laquelle nous introduisons d'abord un encodeur neuronal contextuel d'énoncé qui comporte trois types de mécanismes d'auto-attention, puis nous l'entraînons en utilisant les méta-architectures siamoise et triplette basées sur l'énergie. Nous proposons en outre une méthode d'échantillonnage générale qui permet à l'architecture triplette de capturer des motifs subtils (p. ex., des groupes qui se chevauchent et s'emboîtent).
La thèse a pour objectif d'étudier la causalité à partir de son expression discursive dans des textes français. Cette étude linguistique est effectuée dans la perspective du traitement automatique des langues. Ce travail s'intègre dans un projet de filtrage sémantique des textes orienté vers la production de synthèses et de résumes (projet SAFIR : système automatique de filtrage d'informations pour le résumé). Mais sa portée s'étend à l'acquisition des connaissances à partir de textes. Notre premier objectif consiste à répertorier les différents procédés linguistiques employés par les auteurs pour communiquer des relations causales. Nous utilisons une méthode informatique originale d'exploration contextuelle qui ne se base pas sur une "représentation profonde" du texte, mais sur une identification automatique de marqueurs considérés comme étant pertinents. Nous proposons une carte de 1500 marqueurs (verbes, locutions, adverbes...) qui sont des indices automatiquement identifiables des relations causales prises en charge par l'énonciateur ou par des tiers.
Nous adoptons le formalisme des classes d'objets pour décrire la métonymie en français. L'objectif de cette description est double : rendre compte des régularités du fonctionnement de ce mécanisme, et constituer des bases lexicales dédiées au traitement automatique des langues. Il est postulé que le cadre minimal d'analyse des unités lexicales est la phrase simple définie en termes de prédicats et d'arguments. Partant de ce principe, la métonymie est analysée comme un mécanisme sémantique signalé par un transfert de prédicats entre des classes de mots corrélés à des prédicats appropriés. Quand il porte sur des noms élémentaires, le transfert opère une recatégorisation sémantique et engendre de nouveaux emplois. La catégorisation des mots en arguments et prédicats nous permet de proposer une classification tripartite des métonymies. Les métonymies de type métonymique et celles qui sont de type argumental mettent en relation des noms élémentaires, tandis que les métonymies de type prédicatif opèrent une recatégorisation structurelle. Dans ce dernier cas, nous distinguons d'abord une double actualisation (prédicative et argumentale) des unités lexicales polysémiques, avant de faire état d'un transfert de prédicats qui rend compte de la recatégorisation structurelle. Transfert et actualisation sont les deux principes organisateurs qui mettent en évidence le caractère systématique de la métonymie.
Le principal objectif de cette thèse vise à estimer de manière automatique la qualité de la traduction de langue parlée (Spoken Language Translation ou SLT), appelée estimation de confiance (Confidence Estimation ou CE). En raison de multiples facteurs, la sortie de SLT, ayant une qualité insatisfaisante, pourrait causer différents problèmes pour les utilisateurs finaux. Par conséquent, il est utile de savoir combien de confiance les tokens corrects pourraient être trouvés au sein de l'hypothèse. Dans le cadre de cette thèse, nous avons proposé un boîte à outils, qui consiste en un framework personnalisable, flexible et en une plate-forme portative, pour l'estimation de confiance au niveau de mots (Word-level Confidence Estimation ou WCE) de SLT. Cette tâche, relativement nouvelle, est définie et formalisée comme un problème d'étiquetage séquentiel dans lequel chaque mot, dans l'hypothèse de SLT, est annoté comme bon ou mauvais selon un ensemble des traits importants. Nous proposons plusieurs outils servant d'estimer la confiance des mots (WCE) en fonction de notre évaluation automatique de la qualité de la transcription (ASR), de la qualité de la traduction (MT), ou des deux (combiner ASR et MT). Ce travail de recherche est réalisable parce que nous avons construit un corpus spécifique, qui contient 6.7k des énoncés pour lesquels un quintuplet est normalisé comme suit : (1) sortie d'ASR, (2) transcription en verbatim, (3) traduction textuelle, (4) traduction vocale et (5) post-édition de la traduction. La conclusion de nos multiples expérimentations, utilisant les traits conjoints entre ASR et MT pour WCE, est que les traits de MT demeurent les plus influents, tandis que les traits de ASR peuvent apporter des informations intéressantes complémentaires. En deuxième lieu, nous proposons deux méthodes pour distinguer des erreurs susceptibles d'ASR et de celles de MT, dans lesquelles chaque mot, dans l'hypothèse de SLT, est annoté comme good (bon), asr_error (concernant les erreurs d'ASR) ou mt_error (concernant les erreurs de MT). Nous contribuons donc à l'estimation de confiance au niveau de mots (WCE) pour SLT par trouver la source des erreurs au sein des systèmes de SLT. En troisième lieu, nous proposons une nouvelle métrique, intitulée Word Error Rate with Embeddings (WER-E), qui est exploitée afin de rendre cette tâche possible. Cette approche génère de meilleures hypothèses de SLT lors de l'optimisation de l'hypothèse de N-meilleure hypothèses avec WER-E. En somme, nos stratégies proposées pour l'estimation de la confiance se révèlent un impact positif sur plusieurs applications pour SLT. Les outils robustes d'estimation de la qualité pour SLT peuvent être utilisés dans le but de re-calculer des graphes de la traduction de parole ou dans le but de fournir des retours d'information aux utilisateurs dans la traduction vocale interactive ou des scénarios de parole aux textes assistés par ordinateur. Mots-clés : Estimation de la qualité, Estimation de confiance au niveau de mots (WCE), Traduction de langue parlée (SLT), traits joints, Sélection des traits.
Les publications présentées couvrent un champ relativement vaste de la linguistique appliquée, depuis la didactique des langues étrangères et l'allemand enseigné en tant que langue étrangère jusqu'à la traduction automatique, en passant par des questions de syntaxe d'ordre général concernant la linguistique computationnelle dans son ensemble et les problèmes concernant les systèmes d'information (indexation automatique). Dans ce travail, il y a une minutieuse réévaluation des systèmes existants et une élaboration d'une toute nouvelle approche concernant les problèmes fondamentaux. Dans une série d'articles rédigés en Portugais brésilien, il est question de l'analyse automatique de textes portugais. Cette œuvre contient plusieurs aspects : une formalisation détaillée de la morphologie et de la syntaxe, les problèmes d'application concernant l'indexation automatique, les systèmes d'information pour le parlement et la traduction automatique. Depuis 1985, la traduction automatique occupe une place prépondérante. Certains problèmes spéciaux ont été analysés avec des exemples concrets et finalement des systèmes concrets ont été mis à l'épreuve. Cela a abouti à des conclusions qui vont fortement influencer le développement ultérieur de la partie "software" en EUROTRA.
L'événement est un concept central dans plusieurs tâches du Traitement Automatique des Langues, en dépit de l'absence d'une définition unifiée de ce que recouvre cette notion. La création de ces schémas requiert une connaissance experte et est donc longue, coûteuse et difficile à étendre à un large ensemble de domaines de spécialité. En parallèle de ces travaux, la quantité de données produites par les individus et les organisations a crû de manière exponentielle, ouvrant des perspectives applicatives inédites. Pour ce faire, nous suivons une approche ascendante divisée en trois grandes étapes. Dans la première étape, nous groupons ensemble les nombreuses mentions textuelles relatant la même réalisation d'un événement, identifiée dans le temps et l'espace et appelée instance. La deuxième étape vise à s'abstraire des caractéristiques spatio-temporelles de chaque instance pour les grouper en grands types d'événements. Enfin, la dernière étape de cette contribution vise à extraire les éléments caractéristiques de chaque type d'événement induit afin d'en proposer une représentation synthétique assimilable à un schéma d'événement.
La plateforme Weblab est un environnement de définition et d'exécution de chaines de traitements média Avant le début de cette thèse, aucun outil n'existait pour l'analyse et l'amélioration de la qualité de workflows WebLab. La problématique principale de la thèse repose sur le fonctionnement dit boite noire des services WebLab. L'approche choisie est non-intrusive : nous complétons la définition du workflow WebLab par des règles de provenance et de propagation de qualité. Les règles de provenance génèrent des liens de dépendance dit grains-fins entre les données et les services après l'exécution d'une chaine de traitements WebLab. Les règles de propagation de qualité profitent des liens inférés précédemment pour raisonner sur l'influence de la qualité d'une donnée utilisée par un service sur la qualité d'une donnée produite...
Ce travail de thèse a pour objectif de proposer plusieurs méthodes d'identification non-supervisées des personnes présentes dans les flux télévisés à l'aide des noms écrits à l'écran. Comme l'utilisation de modèles biométriques pour reconnaître les personnes présentes dans de larges collections de vidéos est une solution peu viable sans connaissance a priori des personnes à identifier, plusieurs méthodes de l'état de l'art proposent d'employer d'autres sources d'informations pour obtenir le nom des personnes présentes. Ces méthodes utilisent principalement les noms prononcés comme source de noms. Cependant, on ne peut avoir qu'une faible confiance dans cette source en raison des erreurs de transcription ou de détection des noms et aussi à cause de la difficulté de savoir à qui fait référence un nom prononcé. Les noms écrits à l'écran dans les émissions de télévision ont été peu utilisés en raison de la difficulté à extraire ces noms dans des vidéos de mauvaise qualité. Toutefois, ces dernières années ont vu l'amélioration de la qualité des vidéos et de l'incrustation des textes à l'écran. Nous avons donc ré-évalué, dans cette thèse, l'utilisation de cette source de noms. Nous avons d'abord développé LOOV (pour Lig Overlaid OCR in Vidéo), un outil d'extraction des textes sur-imprimés à l'image dans les vidéos. Nous obtenons avec cet outil un taux d'erreur en caractères très faible. Ce qui nous permet d'avoir une confiance importante dans cette source de noms. Nous avons ensuite comparé les noms écrits et les noms prononcés dans leurs capacités à fournir le nom des personnes présentes dans les émissions de télévisions. Il en est ressorti que deux fois plus de personnes sont nommables par les noms écrits que par les noms prononcés extraits automatiquement. Un autre point important à noter est que l'association entre un nom et une personne est intrinsèquement plus simple pour les noms écrits que pour les noms prononcés. Cette très bonne source de noms nous a donc permis de développer plusieurs méthodes de nommage non-supervisé des personnes présentes dans les émissions de télévision. Nous avons commencé par des méthodes de nommage tardives où les noms sont propagés sur des clusters de locuteurs. Ces méthodes remettent plus ou moins en cause les choix fait lors du processus de regroupement des tours de parole en clusters de locuteurs. Nous avons ensuite proposé deux méthodes (le nommage intégré et le nommage précoce) qui intègrent de plus en plus l'information issue des noms écrits pendant le processus de regroupement. Pour identifier les personnes visibles, nous avons adapté la méthode de nommage précoce pour des clusters de visages. Enfin, nous avons aussi montré que cette méthode fonctionne aussi pour nommer des clusters multi-modaux voix-visage. Avec cette dernière méthode, qui nomme au cours d'un unique processus les tours de paroles et les visages, nous obtenons des résultats comparables aux meilleurs systèmes ayant concouru durant la première campagne d'évaluation REPERE
La métabolomique permet une étude à large échelle du profil métabolique d'un individu, représentatif de son état physiologique. La comparaison de ces profils conduit à l'identification de métabolites caractéristiques d'une condition donnée. La métabolomique présente un potentiel considérable pour le diagnostic, mais également pour la compréhension des mécanismes associés aux maladies et l'identification de cibles thérapeutiques. Cependant, ces dernières applications nécessitent d'inclure ces métabolites caractéristiques dans un contexte plus large, décrivant l'ensemble des connaissances relatives au métabolisme, afin de formuler des hypothèses sur les mécanismes impliqués. Cette mise en contexte peut être réalisée à l'aide des réseaux métaboliques, qui modélisent l'ensemble des transformations biochimiques opérables par un organisme. L'une des limites de cette approche est que la métabolomique ne permet pas à ce jour de mesurer l'ensemble des métabolites, et ainsi d'offrir une vue complète du métabolome. Les travaux présentés dans cette thèse proposent une méthode pour pallier ces limitations, en suggérant des métabolites pertinents pouvant aider à la reconstruction de scénarios mécanistiques. Cette méthode est inspirée des systèmes de recommandations utilisés dans le cadre d'activités en ligne, notamment la suggestion d'individus d'intérêt sur les réseaux sociaux numériques. La méthode a été appliquée à la signature métabolique de patients atteints d'encéphalopathie hépatique. Elle a permis de mettre en avant des métabolites pertinents dont le lien avec la maladie est appuyé par la littérature scientifique, et a conduit à une meilleure compréhension des mécanismes sous-jacents et à la proposition de scénarios alternatifs.
Cette thèse de Traitement Automatique des Langues a pour objectif l'annotation automatique en rôles sémantiques du français en domaine spécifique. Cette tâche désambiguïse le sens des prédicats d'un texte et annote les syntagmes liés avec des rôles sémantiques tels qu'Agent, Patient ou Destination. Elle aide de nombreuses applications dans les domaines où des corpus annotés existent, mais est difficile à utiliser quand ce n'est pas le cas. Nous avons d'abord évalué sur le corpus FrameNet une méthode existante d'annotation basée uniquement sur VerbNet et donc indépendante du domaine considéré. Pour utiliser cette méthode en français, nous traduisons deux ressources lexicales anglaises. Nous commençons par la base de données lexicales WordNet. Nous traduisons ensuite le lexique VerbNet dans lequel les verbes sont regroupés sémantiquement grâce à leurs traits syntaxiques. La traduction, VerbNet, a été obtenue en réutilisant deux lexiques verbaux du français (le Lexique-Grammaire et Les Verbes Français) puis en modifiant manuellement l'ensemble des informations obtenues. Enfin, une fois ces briques en place, nous évaluons la faisabilité de l'annotation en rôles sémantiques en anglais et en français dans trois domaines spécifiques. Nous évaluons quels sont les avantages et inconvénients de se baser sur VerbNet et VerbuNet pour annoter ces domaines, avant d'indiquer nos perspectives pour poursuivre ces travaux.
La recherche de réponses précises à des questions formulées en langue naturelle renouvelle le champ de la recherche d'information. De nombreux travaux ont eu lieu sur la recherche de réponses à des questions factuelles en domaine ouvert. Moins de travaux ont porté sur la recherche de réponses en domaine de spécialité, en particulier dans le domaine médical ou biomédical. Plusieurs conditions différentes sont rencontrées en domaine de spécialité comme les lexiques et terminologies spécialisés, les types particuliers de questions, entités et relations du domaine ou les caractéristiques des documents ciblés. Dans une première partie, nous étudions les méthodes permettant d'analyser sémantiquement les questions posées par l'utilisateur ainsi que les textes utilisés pour trouver les réponses. Dans une seconde partie, nous étudions l'apport des technologies du web sémantique pour la portabilité et l'expressivité des systèmes de questions-réponses. Enfin, nous présentons notre système de questions-réponses, appelé MEANS, qui utilise à la fois des techniques de TAL, des connaissances du domaine et les technologies du web sémantique pour répondre automatiquement aux questions médicales.
Les flux de contenus audiovisuels peuvent être représentés sous forme de séquences d'événements (par exemple, des suites d'émissions, de scènes, etc.). Dans le contexte d'une chaîne TV, la programmation des émissions suit une cohérence définie par cette même chaîne, mais peut également être influencée par les programmations des chaînes concurrentes. Dans de telles conditions,les séquences d'événements des flux parallèles pourraient ainsi fournir des connaissances supplémentaires sur les événements d'un flux considéré. La modélisation de séquences est un sujet classique qui a été largement étudié, notamment dans le domaine de l'apprentissage automatique. Les réseaux de neurones récurrents de type Long Short-Term Memory (LSTM) ont notamment fait leur preuve dans de nombreuses applications incluant le traitement de ce type de données. Néanmoins,ces approches sont conçues pour traiter uniquement une seule séquence d'entrée à la fois. Notre contribution dans le cadre de cette thèse consiste à élaborer des approches capables d'intégrer conjointement des données séquentielles provenant de plusieurs flux parallèles. Le contexte applicatif de ce travail de thèse, réalisé en collaboration avec le Laboratoire Informatique d'Avignon et l'entreprise EDD, consiste en une tâche de prédiction du genre d'une émission télévisée. Cette prédiction peut s'appuyer sur les historiques de genres des émissions précédentes de la même chaîne mais également sur les historiques appartenant à des chaînes parallèles. Nous proposons une taxonomie de genres adaptée à de tels traitements automatiques ainsi qu'un corpus de données contenant les historiques parallèles pour 4 chaînes françaises. Deux méthodes originales sont proposées dans ce manuscrit, permettant d'intégrer les séquences des flux parallèles. La première, à savoir, l'architecture des LSTM parallèles(PLSTM) consiste en une extension du modèle LSTM. Les PLSTM traitent simultanément chaque séquence dans une couche récurrente indépendante et somment les sorties de chacune de ces couches pour produire la sortie finale. Pour ce qui est de la seconde proposition, dénommée MSE-SVM, elle permet de tirer profit des avantages des méthodes LSTM et SVM. D'abord, des vecteurs de caractéristiques latentes sont générés indépendamment, pour chaque flux en entrée, en prenant en sortie l'événement à prédire dans le flux principal. Ces nouvelles représentations sont ensuite fusionnées et données en entrée à un algorithme SVM. Les approches PLSTM et MSE-SVM ont prouvé leur efficacité dans l'intégration des séquences parallèles en surpassant respectivement les modèles LSTM et SVM prenant uniquement en compte les séquences du flux principal. Les deux approches proposées parviennent bien à tirer profit des informations contenues dans les longues séquences. En revanche, elles ont des difficultés à traiter des séquences courtes. Cependant, le problème rencontré avec les séquences courtes est plus prononcé pour le cas de l'approche MSE-SVM. Nous proposons enfin d'étendre cette approche en permettant d'intégrer des informations supplémentaires sur les événements des séquences en entrée (par exemple, le jour de la semaine des émissions de l'historique). Cette extension, dénommée AMSE-SVM améliore remarquablement la performance pour les séquences courtes sans les baisser lorsque des séquences longues sont présentées.
Dans cette thèse, nous nous intéressons particulièrement aux données issues de l'imagerie par résonance magnétique fonctionnelle (IRMf), que nous étudions dans un cadre d'apprentissage statistique. Tout d'abord, nous considérons les données d'IRMf de repos, que nous traitons grâce à des méthodes de factorisation de matrices. Notre méthode principale introduit une réduction aléatoire de la dimension des données dans une boucle d'apprentissage en ligne. L'algorithme proposé converge plus de 10 fois plus vite que les meilleures méthodes existantes, pour différentes configurations et sur plusieurs jeux de données. Nous effectuons une vaste validation expérimentale de notre approche de sous-échantillonnage aléatoire. Nous proposons une étude théorique des propriétés de convergence de notre algorithme. Dans un second temps, nous nous intéressons aux données d'IRMf d'activation. Nous démontrons comment agréger différents études acquises suivant des protocoles distincts afin d'apprendre des modèles joints de décodage plus justes et interprétables. Cela suscite un transfert d'information entre les études. En conséquence, notre modèle multi-étude est plus performant que les modèles de décodage appris sur chaque étude séparément. Notre approche identifie une représentation universellement pertinente de l'activité cérébrale, supportée par un petit nombre de réseaux optimisés pour l'identification de tâches.
Les langages contrôlés sont des langages artificiellement définis utilisant un sous-ensemble du vocabulaire, des formes morphologiques, des constructions syntaxiques d'une langue naturelle tout en en éliminant la polysémie. En quelque sorte, ils constituent le pont entre les langages formels et les langues naturelles. De ce fait, ils remplissent la fonction de communication du médium texte tout en étant rigoureux et analysables par la machine sans ambiguïté. En particulier, ils peuvent être utilisés pour faciliter l'alimentation de bases de connaissances, dans le cadre d'une interface homme-machine. Le Service Hydrographique et Océanographique de la Marine (SHOM) publie depuis 1971 les Instructions nautiques, des recueils de renseignements généraux, nautiques et réglementaires, destinés aux navigateurs. Ces ouvrages complètent les cartes marines. D'autre part, l'Organisation Hydrographique Internationale (OHI) a publié des normes spécifiant l'échange de données liées à la navigation et notamment un modèle universel de données hydrographiques (norme S-100, janvier 2010). Cette thèse se propose d'étudier l'utilisation d'un langage contrôlé pour représenter des connaissances contenues dans les Instructions nautiques, dans le but de servir de pivot entre la rédaction du texte par l'opérateur dédié, la production de l'ouvrage imprimé ou en ligne, et l'interaction avec des bases de connaissances et des outils d'aide à la navigation. En particulier on étudiera l'interaction entre le langage contrôlé des Instructions nautiques et les cartes électroniques correspondantes. Plus généralement, cette thèse se pose la question de l'évolution d'un langage contrôlé et des ontologies sous-jacentes dans le cadre d'une application comme les Instructions nautiques, qui ont la particularité d'avoir des aspects rigides (données numériques, cartes électroniques, législation) et des aspects nécessitant une certaine flexibilité (rédaction du texte par des opérateurs humains, imprévisibilité du type de connaissance à inclure par l'évolution des usages et des besoins des navigants). De manière similaire aux ontologies dynamiques que l'on rencontre dans certains domaines de connaissance, on définit ici un langage contrôlé dynamique. Bien que créé pour le domaine de la navigation maritime, les mécanismes du langage contrôlé présentés dans cette thèse ont le potentiel pour être adaptés à d'autres domaines utilisant des corpus multimodaux. Enfin, les perspectives d'évolution pour un langage contrôlé hybride sont importantes puisqu'elles peuvent exploiter les différents avantages des modes en présence (par exemple, une exploitation de l'aspect visuel pour une extension 3D).
Les techniques d'apprentissage semi-supervisé basées sur des graphes (G-SSL) permettent d'exploiter des données étiquetées et non étiquetées pour construire de meilleurs classifiers. Malgré de nombreuses réussites, leur performances peuvent encore être améliorées, en particulier dans des situations ou` les graphes ont une faible séparabilité de classes ou quand le nombres de sujets supervisés par l'expert est déséquilibrés. Pour aborder ces limitations on introduit une nouvelle méthode pour G-SSL, appelee Lγ - PageRank, qui constitue la principal contribution de cette these. Il s'agit d'une g´en´eralisation de l'algorithme PageRank ´a partir de l'utilisation de puissances positives γ de la matrice Laplacienne du graphe. L'étude théorique de Lγ - PageRank montre que (i) pour γ &lt ; 1, cela correspond à une extension de l'algorithme PageRank aux processus de vol de Lévy : où les marcheurs aléatoires peuvent désormais relier, en un seul saut, des nœuds distants du graphe ; et (ii) pour γ &gt ; 1, la classification est effectué sur des graphes signés : où les noeuds appartenant à une même classe ont plus de chances de partager des liens positifs, tandis que les noeuds de classes différentes ont plus de chances d'être connectés avec des arêtes négatifs. Nous montrons l'existence d'une puissance optimale γ qui maximise la performance de classification, pour laquelle une méthode d'estimation automatique est conçue et évaluée. Des expériences sur plusieurs jeux de données montrent que les marcheurs aléatoires de vols de Lévy peuvent améliorer la détection des classes ayant des structures locales complexes, tandis que les graphes signés permet d'améliorer considérablement la séparabilité des données et de surpasser le problème des données étiquetées non équilibrées. Dans un second temps, nous étudions des implémentations efficaces de Lγ - PageRank. Nous proposons des extensions de Power Iteration et Gauss-Southwell pour Lγ - PageRank, qui sont des algorithmes initialement conçues pour calculer efficacement la solution de la méthode PageRank standard. Ensuite, les versions dynamiques de ces algorithmes sont également étendues à Lγ - PageRank, permettant de mettre `a jour la solution de Lγ - PageRank en complexité sub-linéaire lorsque le graphe évolue ou que de nouvelles données arrivent. Pour terminer, nous appliquons Lγ - PageRank dans le contexte du routage Internet. Nous abordons le problème de l'identification des systèmes autonomes (AS) pour des arêtes inter-AS `a partir du réseau d'adresses IP et des registres publics des AS. Des expériences sur des mesures traceroute d'Internet montrent que Lγ -PageRank peut résoudre cette tâche sans erreurs, même lorsqu'il n'y a pas d'exemples étiquetés par l'expert pour la totalité des classes.
Le but est qu'ils s'expriment en langage naturel et puissent dialoguer avec des interlocuteurs humains. Pour développer un ACA, il faut d'abord comprendre que des aspects tels que personnalité, les émotions et leur apparence sont extrêmement importants. Le travail qui est présenté dans cette thèse a pour objectif d'augmenter l'acceptabilité et la crédibilité des agents au moyen de la personnalité, considérée comme une notion centrale à l'interaction ACA-humain. On propose un modèle qui dote l'ACA de facettes de personnalité et de buts de communication « cachés » et qui module ainsi ses actions conversationnelles.
L'intégration et l'analyse des différentes bases de données liées à la même question de recherche, par exemple la corrélation entre phénotypes et génotypes, sont essentielles pour découvrir de nouvelles connaissances. Elles fournissent un vocabulaire commun pour les humains, et des définitions d'entités formelles pour les machines. Un grand nombre d'ontologies et de terminologies biomédicales a été développé pour représenter et annoter les différentes bases de données existantes. Cependant, celles qui sont représentées avec différentes ontologies qui se chevauchent, c'est à dire qui ont des parties communes, ne sont pas interopérables. Il est donc crucial d'établir des correspondances entre les différentes ontologies utilisées, ce qui est un domaine de recherche actif connu sous le nom d'alignement d'ontologies. Les premières méthodes d'alignement d'ontologies exploitaient principalement le contenu lexical et structurel des ontologies à aligner. Ces méthodes sont moins efficaces lorsque les ontologies à aligner sont fortement hétérogènes lexicalement, c'est à dire lorsque des concepts équivalents sont décrits avec des labels différents. Pour pallier à ce problème, la communauté d'alignement d'ontologies s'est tournée vers l'utilisation de ressources de connaissance externes en tant que pont sémantique entre les ontologies à aligner. Cette approche soulève plusieurs nouvelles questions de recherche, notamment : (1) la sélection des ressources de connaissance à utiliser, (2) l'exploitation des ressources sélectionnées pour améliorer le résultat d'alignement. Plusieurs travaux de recherche ont traité ces problèmes conjointement ou séparément. Dans notre thèse, nous avons fait une revue systématique et une comparaison des méthodes proposées dans la littérature. Les ontologies, autres que celles à aligner, sont les ressources de connaissance externes (Background Knowledge : BK) les plus utilisées. Les travaux apparentés sélectionnent souvent un ensemble d'ontologies complètes en tant que BK même si, seuls des fragments des ontologies sélectionnées sont réellement efficaces pour découvrir de nouvelles correspondances. Nous proposons une nouvelle approche qui sélectionne et construit une ressource de connaissance à partir d'un ensemble d'ontologies. Afin de faire face à ce problème, nous proposons deux méthodes pour sélectionner les correspondances les plus pertinentes parmi les candidates qui se basent sur : (1) un ensemble de règles et (2) l'apprentissage automatique supervisé. Nous avons expérimenté et évalué notre approche dans le domaine biomédical, grâce à la profusion de ressources de connaissances en biomédecine (ontologies, terminologies et alignements existants). Nous avons effectué des expériences intensives sur deux benchmarks de référence de la campagne d'évaluation de l'alignement d'ontologie (OAEI).
La rénovation des bâtiments de logements est aujourd'hui un axe majeur de la lutte pour la réduction des consommations énergétiques et, plus généralement, du réchauffement climatique. Cependant, si la situation semble simple sur le plan technique, il existe encore de nombreux freins (économiques, sociaux, culturels) qui ralentissent la rénovation du parc existant. Ces freins nécessitent d'aborder une rénovation sous un angle global, grâce notamment au travail d'un facilitateur, capable d'établir une relation de confiance avec les habitants. Cette approche globale est encore plus importante lorsqu'il s'agit de rénover les copropriétés. En effet, dans ces ensembles immobiliers, la prise de décision est rendue complexe par le nombre d'acteurs et leur organisation juridique spécifique. Pour permettre aux copropriétés de voter des travaux de rénovation, un travail d'accompagnement long et couteux est nécessaire. Pour permettre de rendre ce travail d'accompagnement le plus efficace possible, nous avons conçu un outil d'aide à la décision, utilisable par le facilitateur, qui permet de réaliser un diagnostic pluridisciplinaire de la copropriété et de préconiser des solutions d'accompagnements sur mesure. Nous avons notamment défini une liste de critères influents sur la décision d'une copropriété d'entreprendre des travaux de rénovations, et nous les avons compilés dans un outil d'évaluation. La copropriété est ainsi étudiée selon toutes ses caractéristiques (qualité d'usage, possibilités techniques, potentiel économique, profil sociologiques des propriétaires, qualité du quartier, état des dynamiques collectives) et des prescriptions spécifiques sont faites en fonction de l'évaluation de chaque critère. Si les premiers résultats sont encourageants, de nombreuses perspectives d'améliorations s'offrent encore à nous pour massifier l'utilisation de cet outil et rendre le diagnostic encore plus précis.
De nombreuses méthodes existent pour résoudre des problèmes d'optimisation multicritère, et il n'est pas aisé de choisir une méthode suffisamment adaptée à un problème multicritère donné. En effet, après le choix d'une méthode multicritère, différents paramètres (e.g. poids, fonctions d'utilité, etc.) doivent être déterminés, soit pour trouver la solution optimale (meilleur compromis) ou pour classer l'ensemble des solutions faisables (alternatives). Justement, vue cette difficulté pour fixer les paramètres, les méthodes d'élicitation sont utilisées pour aider le décideur dans cette tâche de fixation des paramètres. Par ailleurs, nous supposons que nous disposons d'un ensemble de solutions plausibles, et nous faisons aussi l'hypothèse de la disponibilité au préalable, des informations préférentielles obtenues après une interaction avec le décideur. Dans la première contribution de ce travail, nous tirons profit d'une mesure statistique simple et rapidement calculable, à savoir, le coefficient de corrélation ρ de Spearman, afin de développer une approche gloutonne (approchée), et deux approches exactes basées sur la programmation par contraintes (PPC) et la programmation linéaire en nombres entiers (PLNE). Ces méthodes sont ensuite utilisées pour éliciter automatiquement les paramètres appropriés de la méthode multicritère basée sur l'ordre lexicographique. Nous proposons aussi des modèles d'élicitation des paramètres d'autres méthodes multicritère, telles que la méthode MinLeximax issue de la théorie du choix social et du partage équitable, la méthode de la somme pondérée et les opérateurs OWA.
Caractériser la qualité d'une odeur est une tâche complexe qui consiste à identifier un ensemble de descripteurs qui synthétise au mieux la sensation olfactive au cours de séances d'analyse sensorielle. Généralement, cette caractérisation est une liste de descripteurs extraite d'un vocabulaire imposé par les industriels d'un domaine pour leurs analyses sensorielles. Ces analyses représentent un coût significatif pour les industriels chaque année. En effet, ces approches dites orientées reposent sur l'apprentissage de vocabulaires, limitent singulièrement les descripteurs pour un public non initié et nécessitent de couteuses phases d'apprentissage. Si cette caractérisation devait être confiée à des évaluateurs naïfs, le nombre de participants pourrait être significativement augmenté tout en réduisant le cout des analyses sensorielles. Malheureusement, chaque description libre n'est alors plus associée à un ensemble de descripteurs non ambigus, mais à un simple sac de mots en langage naturel (LN). Deux problématiques sont alors rattachées à la caractérisation d'odeurs. Ainsi, la première partie de notre travail se focalise sur la définition et l'évaluation de modèles qui peuvent être utilisés pour résumer un ensemble de mots en un ensemble de descripteurs désambiguïsés. Parmi les différentes stratégies envisagées dans cette contribution, nous proposons de comparer des approches hybrides exploitant à la fois des bases de connaissances et des plongements lexicaux définis à partir de grands corpus de textes. Nos résultats illustrent le bénéfice substantiel à utiliser conjointement représentation symbolique et plongement lexical. Nous définissons ensuite de manière formelle le processus de synthèse d'un ensemble de concepts et nous proposons un modèle qui s'apparente à une forme d'intelligence humaine pour évaluer les résumés alternatifs au regard d'un objectif de synthèse donné. L'approche non orientée que nous proposons dans ce manuscrit apparait ainsi comme l'automatisation cognitive des tâches confiées aux opérateurs des séances d'analyse sensorielle. Elle ouvre des perspectives intéressantes pour développer des analyses sensorielles à grande échelle sur de grands panels d'évaluateurs lorsque l'on essaie notamment de caractériser les nuisances olfactives autour d'un site industriel.
L'apprentissage profond a permis des avancées significatives dans le domaine de la traduction automatique. La traduction automatique neuronale (NMT) s'appuie sur l'entrainement de réseaux de neurones avec un grand nombre de paramètres sur une grand quantité de données parallèles pour apprendre à traduire d'une langue à une autre. Un facteur primordial dans le succès des systèmes NMT est la capacité de concevoir des architectures puissantes et efficaces. À cette fin, nous introduisons Pervasive Attention, un modèle basé sur des convolutions bidimensionnelles qui encodent conjointement les séquences source et cible avec des interactions qui sont omniprésentes dans le réseau neuronal. Pour améliorer l'efficacité des systèmes NMT, nous étudions la traduction automatique simultanée où la source est lue de manière incrémentielle et le décodeur est alimenté en contextes partiels afin que le modèle puisse alterner entre lecture et écriture. Nous abordons également l'efficacité computationnelle des modèles NMT et affirmons qu'ajouter plus de couches à un réseau de neurones n'est pas requis pour tous les cas. Nous concevons des décodeurs Transformer qui peuvent émettre des prédictions à tout moment dotés de mécanismes d'arrêt adaptatifs pour allouer des ressources en fonction de la complexité de l'instance.
Le traitement automatique de la parole est un domaine qui englobe un grand nombre de travaux : de la reconnaissance automatique du locuteur à la détection des entités nommées en passant par la transcription en mots du signal audio. Toutes ces informations peuvent être exploitées par des techniques d'indexation automatique qui vont permettre d'indexer de grandes collections de documents. Les travaux présentés dans cette thèse s'intéressent à l'indexation automatique de locuteurs dans des documents audio en français. Plus précisément nous cherchons à identifier les différentes interventions d'un locuteur ainsi qu'à les nommer par leur prénom et leur nom. Ce processus est connu sous le nom d'identification nommée du locuteur (INL). La particularité de ces travaux réside dans l'utilisation conjointe du signal audio et de sa transcription en mots pour nommer les locuteurs d'un document. Le prénom et le nom de chacun des locuteurs est extrait du document lui même (de sa transcription enrichie plus exactement), avant d'être affecté à un des locuteurs du document. Nous commençons par rappeler le contexte et les précédents travaux réalisés sur l'INL avant de présenter Milesin, le système développé lors de cette thèse. L'apport de ces travaux réside tout d'abord dans l'utilisation d'un détecteur automatique d'entités nommées (LIA_NE) pour extraire les couples prénom / nom de la transcription. Ensuite, ils s'appuient sur la théorie des fonctions de croyance pour réaliser l'affectation aux locuteurs du document et prennent ainsi en compte les différents conflits qui peuvent apparaître. Pour finir, un algorithme optimal d'affectation est proposé. Ce système obtient un taux d'erreur compris entre 12 et 20 % sur des transcriptions de référence (réalisées manuellement) en fonction du corpus utilisé. Nous présentons ensuite les avancées réalisées et les limites mises en avant par ces travaux. Nous proposons notamment une première étude de l'impact de l'utilisation de transcriptions entièrement automatiques sur Milesin.
C'est pourquoi de nombreux projets d'aide aux personnes âgées : techniques, universitaires et commerciaux ont vu le jour ces dernières années. Ce travail de thèse a été eﬀectué sous convention Cifre, conjointement entre l'entreprise KRG Corporate et le laboratoire BMBI (Biomécanique et Bio-ingénierie) de l'UTC (Université de technologie de Compiègne). Elle a pour objet de proposer un capteur de reconnaissance de sons et des activités de la vie courante, dans le but d'étoﬀer et d'améliorer le système de télé-assistance déjà commercialisé par la société. Plusieurs méthodes de reconnaissance de parole ou de reconnaissance du locuteur ont déjà été éprouvées dans le domaine de la reconnaissance de sons, entre autres les techniques : GMM (Modèle de mélange gaussien–Gaussian Mixture Model), SVM-GSL (Machine à vecteurs de support, GMM-super-vecteur à noyau linéaire – Support vector machine GMM Supervector Linear kernel) et HMM (Modèle de Markov caché – Hidden Markov Model). De la même manière, nous nous sommes proposés d'utiliser les i Puis nous avons élargi notre spectre, et utilisé l'apprentissage profond (Deep Learning) qui donne actuellement de très bon résultats en classification tous domaines confondus. Les méthodes précédemment évoquées ont également été testées en conditions bruités puis réelles.
Au début de cette thèse, aucun corpus annoté syntaxiquement (treebank) n'était disponible pour le serbe. Or, les treebanks annotés manuellement sont une condition sine qua non du développement (entraînement et évaluation) d'outils statistiques dédiés à l'annotation syntaxique automatique (parsers). L'existence des parsers performants permet à son tour l'annotation syntaxique de corpus plus larges, qui peuvent ensuite alimenter des recherches en linguistique théorique. Afin de combler cette lacune, nous avons constitué un ensemble de ressources pour le traitement automatique du serbe. Il s'agit en premier lieu du treebank ParCoTrain-Synt, qui contient 101 000 tokens annotés en morphosyntaxe, en lemmes et en syntaxe de dépendances. Nous avons également confectionné le lexique ParCoLex, doté de 7 millions d'entrées provenant de 157 000 lemmes différents. En exploitant ces deux ressources, nous avons développé des modèles pour le parsing, pour l'étiquetage et pour la lemmatisation. Toutes les ressources citées sont librement diffusées à l'adresse suivante : https : //github.com/aleksandra-miletic/serbian-nlp-resources. Les ressources constituées ont également été exploitées dans le cadre de deux études linguistiques, montrant ainsi que le corpus ParCoTrain-Synt ouvre la porte aux études empiriques basées sur des analyses quantitatives dans le domaine de la linguistique serbe.
L'étude de l'extraction de lexiques bilingues à partir de corpus comparables a été souvent circonscrite aux mots simples. Les méthodes classiques ne peuvent gérer les expressions complexes que si elles sont de longueur identique, tandis que les méthodes de plongements de mots modélisent les expressions comme une seule unité. Ces dernières nécessitent beaucoup de données, et ne peuvent pas gérer les expressions hors vocabulaire. Dans cette thèse, nous nous intéressons à la modélisation d'expressions de longueur variable par co-occurrences et par les méthodes neuronales état de l'art. Nous étudions aussi l'apprentissage de représentation d'expressions supervisé et non-supervisé. Nous proposons deux contributions majeures. Notre système améliore significativement l'alignement bilingue des expressions de longueurs différentes.
À partir d'un corpus vidéo de conversation spontanée en anglais, notre travail de thèse s'attache à déterminer si plusieurs types syntaxiques de constructions subordonnées expriment le même degré d'intégration à leur environnement co-textuel, d'une perspective multimodale. La littérature syntaxique décrit les subordonnées comme des formes dépendantes, qui spécifient ou élaborent le contenu d'une autre proposition. En montrant que les constructions sous étude n'expriment pas une dépendance uniforme à leur environnement selon la façon dont les locuteurs utilisent les modalités prosodique et gestuelle pour exprimer plus ou moins de démarcation, les résultats en production comme en perception suggèrent d'une part que les appositives sont produites avec davantage de rupture que les autres types syntaxiques, et d'autre part que la création d'une rupture s'appuie majoritairement sur des moyens davantage prosodiques que gestuels.
Cette thèse présente la réalisation d'un système de compréhension de parole continue sur une architecture informatique modeste. Elle concerne la compréhension de parole continue dans le cadre d'applications précises et des langages artificiels. Le système utilise dans un premier temps, la technique de reconnaissance analytique donnant comme résultat un treillis phonétique de la phrase prononcée. Un outil d'aide de génération de données linguistiques a été ajouté au système, qui permet à l'utilisateur de déterminer la syntaxe de son langage d » application, les mots étant introduits sous leur forme graphémique.
Les graphes sont omniprésents dans de nombreux domaines de recherche, allant de la biologie à la sociologie. Un graphe est une structure mathématique très simple constituée d'un ensemble d'éléments, appelés nœuds, reliés entre eux par des liens, appelés arêtes. Le partitionnement ou clustering de graphe est un problème central en analyse de graphe dont l'objectif est d'identifier des groupes de nœuds densément interconnectés et peu connectés avec le reste du graphe. Ces groupes de nœuds, appelés clusters, sont fondamentaux pour une compréhension fine de la structure des graphes. Il n'existe pas de définition universelle de ce qu'est un bon cluster, et différentes approches peuvent s'avérer mieux adaptées dans différentes situations. Alors que les méthodes classiques s'attachent à trouver des partitions des nœuds de graphe, c'est-à-dire à colorer ces nœuds de manière à ce qu'un nœud donné n'ait qu'une et une seule couleur, des approches plus élaborées se révèlent nécessaires pour modéliser la structure complexe des graphes que l'on rencontre en situation réelle. En particulier, dans de nombreux cas, il est nécessaire de considérer qu'un nœud donné peut appartenir à plus d'un cluster. Par ailleurs, de nombreux systèmes que l'on rencontre en pratique présentent une structure multi-échelle pour laquelle il est nécessaire de partir à la recherche de hiérarchies de clusters plutôt que d'effectuer un partitionnement à plat. De plus, les graphes que l'on rencontre en pratique évoluent souvent avec le temps et sont trop massifs pour être traités en un seul lot. Pour ces raisons, il est souvent nécessaire d'adopter des approches dites de streaming qui traitent les arêtes au fil de l'eau. Enfin, dans de nombreuses applications, traiter des graphes entiers n'est pas nécessaire ou est trop coûteux, et il est plus approprié de retrouver des clusters locaux dans un voisinage de nœuds d'intérêt plutôt que de colorer tous les nœuds. Dans ce travail, nous étudions des approches alternatives de partitionnement de graphe et mettons au point de nouveaux algorithmes afin de résoudre les différents problèmes évoqués ci-dessus.
La présente thèse est dédiée à l'étude de la préposition iz dans la langue russe moderne à travers les constructions qui la contiennent et qui s'organisent en un réseau sémantique complexe. Bien que les prépositions aient toujours été au cœur des recherches linguistiques, iz n'a cependant pas reçu de description très détaillée de sa sémantique ni de son fonctionnement même s'il existe quelques travaux qui lui sont consacrés. Le Chapitre II présente une analyse détaillée des emplois de iz au sein d'une construction, considérée comme une unité sémantico-syntaxique. Cette analyse détaillée permet de mettre en évidence les paramètres qui rapprochent les différents emplois de iz en établissant les liens entre ses différents emplois et en les organisant en un réseau de significations bien structuré. Ce réseau de significations constitue le profil sémantique de iz à proprement parler. Le Chapitre III est un chapitre synthétique qui reprend les principales idées développées dans le Chapitre II et les confronte par le biais d'une étude contrastive aux emplois des prépositions ot et s qui apparaissent dans des contextes proches et, par conséquent, y entrent en concurrence avec iz. Cette analyse contrastive permet d'identifier les particularités propres aux emplois de chacune des trois prépositions et de cerner la manière dont elles départagent les domaines d'emplois. En outre, une place particulière est accordée à l'analyse quantitative des combinatoires des prépositions iz, ot et s avec les verbes à préfixe. Les résultats obtenus confirment grandement l'hypothèse de la corrélation entre les prépositions et les préfixes homonymes (ot et ot-, s et s-) ou « synonymes » (iz et vy-). Enfin, les thèses développées tout au long du travail sont utilisées pour expliquer les cas d'emplois erronés impliquant la préposition iz dans les travaux d'apprenants de russe (francophones et anglophones principalement). Grâce à cette dimension comparative entre les langues à structure différente, l'analyse effectuée met en lumière le fait que certaines constructions avec iz (notamment, « appartenance » , « repérage d'une entité dans un groupe d'entités » ) ont des paramètres particuliers qui ne sont pas présents dans des constructions « équivalentes » en français et en anglais. La conclusion générale expose l'ensemble des résultats et données obtenus, tout comme l'Annexe qui regroupe les principales données quantitatives issues des recherches sur les prépositions étudiées principalement dans le Corpus National de la Langue Russe (ruscorpora.ru). Les résultats obtenus peuvent aussi bien contribuer aux études linguistiques sur les prépositions et aux entrées lexicographiques, qu'aux recherches portant sur l'acquisition du russe par des adultes non-russophones ou encore sur la didactique du russe langue étrangère.
Dans ce manuscrit, nous étudions la diffusion d'information dans les réseaux sociaux en ligne. Des sites comme Facebook ou Twitter sont en effet devenus aujourd'hui des media d'information à part entière, sur lesquels les utilisateurs échangent de grandes quantités de données. La plupart des modèles existant pour expliquer ce phénomène de diffusion sont des modèles génératifs, basés sur des hypothèses fortes concernant la structure et la dynamique temporelle de la diffusion d'information. Nous considérerons dans ce manuscrit le problème de la prédiction de diffusion dans le cas où le graphe social est inconnu, et où seules les actions des utilisateurs peuvent être observées. - Nous proposons, dans un premier temps, une méthode d'apprentissage du modèle independent cascade consistant à ne pas prendre en compte la dimension temporelle de la diffusion. Des résultats expérimentaux obtenus sur des données réelles montrent que cette approche permet d'obtenir un modèle plus performant et plus robuste. - Nous proposons ensuite plusieurs méthodes de prédiction de diffusion reposant sur des technique d'apprentissage de représentations. Celles-ci nous permettent de définir des modèles plus compacts, et plus robustes à la parcimonie des données. - Enfin, nous terminons en appliquant une approche similaire au problème de détection de source, consistant à retrouver l'utilisateur ayant lancé une rumeur sur un réseau social.
Cette thèse en informatique présente un travail de modélisation cognitive computationnelle, à partir de données de mouvements oculaires lors de tâches de recherche d'information dans des textes. Parce que le temps est souvent une contrainte, les textes ne sont souvent pas entièrement lus Plus précisément, nous avons analysé les mouvements des yeux dans deux tâches de recherche d'information consistant à lire un paragraphe et à décider rapidement i) s'il est associé à un but donné et ii) s'il est plus associé à un but donné qu'un autre paragraphe traité auparavant. Un modèle est proposé pour chacune de ces situations. Nos simulations sont réalisées au niveau des fixations et des saccades oculaires. En particulier, nous prédisons le moment auquel les participants décident d'abandonner la lecture du paragraphe parce qu'ils ont suffisamment d'information pour prendre leur décision. Les modèles font ces prédictions par rapport aux mots qui sont susceptibles d'être traités avant que le paragraphe soit abandonné. Nous avons suivi une approche statistique paramétrique dans la construction de nos modèles. Ils sont basés sur un classifieur bayésien. Nous proposons un seuil linéaire bi-dimensionnel pour rendre compte de la décision d'arrêter de lire un paragraphe, utilisant le Rang de la fixation et i) la similarité sémantique (Cos) entre le paragraphe et le but ainsi que ii) la différence de similarité sémantique (Gap) entre chaque paragraphe et le but. Pour chacun des modèles, les performances montrent que nous sommes capables de reproduire en moyenne le comportement des participants face aux tâches de recherche d'information étudiées durant cette thèse. Cette thèse comprend deux parties principales : 1) la conception et la passation d'expériences psychophysiques pour acquérir des données de mouvements oculaires et 2) le développement et le test de modèles cognitifs computationnels.
Alors que les sources multimédias sont massivement disponibles en ligne, aider les utilisateurs à comprendre la grande quantité d'information générée est devenu un problème majeur. Une façon de procéder consiste à résumer le contenu multimédia, générant ainsi des versions abrégées et informatives des sources. Cette thèse aborde le sujet du résumé automatique (texte et parole) dans un contexte multilingue. Le résumé multimédia basé sur le texte utilise des transcriptions pour produire des résumés qui peuvent être présentés sous forme textuelle ou dans leur format d'origine. La transcription des sources multimédia peut être effectuée manuellement ou automatiquement par un système de Reconnaissance automatique de la parole (RAP). Les transcriptions peuvent différer de la langue écrite car la source étant parlée. De plus, ces transcriptions manquent d'informations syntaxiques. Par exemple, les majuscules et les signes de ponctuation sont absents, ce qu'implique des phrases inexistantes. Enfin, nous étendons ARTEX, un résumeur textuel extractif état de l'art, pour traiter de documents en arabe standard en adaptant ses modules de prétraitement. Les résumés peuvent être présentés sous une forme textuelle ou dans leur format multimédia original en alignant les US sélectionnées. En ce qui concerne le résumé multimédia basée sur l'audio, nous introduisons une méthode extractive qui représente l'informativité de la source à partir de ses caractéristiques audio pour sélectionner les segments les plus pertinents pour le résumé. Pendant la phase d'entraînement, notre méthode utilise les transcriptions des documents audio pour créer un modèle informatif qui établit une correspondance entre un ensemble de caractéristiques audio et une mesure de divergence. Dans notre système, les transcriptions ne sont plus nécessaires pour résumer des nouveaux documents audio. Les résultats obtenus sur un schéma multi-évaluation montrent que notre approche génère des résumés compréhensibles et informatifs. Nous avons étudié également les mesures d'évaluation et nous avons développé la méthode Window-based Nous explorons également la possibilité de mesurer la qualité des transcriptions automatiques en fonction de leur informativité. De plus, nous étudions dans quelle mesure le résumé automatique peut compenser les problèmes posés au cours de la transcription. Enfin, nous étudions comment les mesures d'évaluation d'informativité peuvent être étendues pour l'évaluation de l'intérêt des passages textuels.
Il est donc important de prendre en compte ces représentations ou vues des données. Ce problème d'apprentissage automatique est appelé apprentissage multivue. Il est utile dans de nombreux domaines d'applications, par exemple en imagerie médicale, nous pouvons représenter le cerveau humains via des IRM, t-fMRI, EEG, etc. Dans cette cette thèse, nous nous concentrons sur l'apprentissage multivue supervisé, où l'apprentissage multivue est une combinaison de différents modèles de classifications ou de vues. Par conséquent, selon notre point de vue, il est intéressant d'aborder la question de l'apprentissage à vues multiples dans le cadre PAC-Bayésien. C'est un outil issu de la théorie de l'apprentissage statistique étudiant les modèles s'exprimant comme des votes de majorité. Un des avantages est qu'elle permet de prendre en considération le compromis entre précision et diversité des votants, au cœur des problématiques liées à l'apprentissage multivue. La première contribution de cette thèse étend la théorie PAC-Bayésienne classique (avec une seule vue) à l'apprentissage multivue (avec au moins deux vues). Pour ce faire, nous définissons une hiérarchie de votants à deux niveaux : les classifieurs spécifiques à la vue et les vues elles-mêmes. Sur la base de cette stratégie, nous avons dérivé des bornes en généralisation PAC-Bayésiennes (probabilistes et non-probabilistes) pour l'apprentissage multivue. Le premier algorithme appelé PB-MVBoost est un algorithme itératif qui apprend les poids sur les vues en contrôlant le compromis entre la précision et la diversité des vues. Le second est une approche de fusion tardive où les prédictions des classifieurs spécifiques aux vues sont combinées via l'algorithme PAC-Bayésien CqBoost proposé par Roy et al. Enfin, nous montrons que la minimisation des erreurs pour le vote de majorité multivue est équivalente à la minimisation de divergences de Bregman. De ce constat, nous proposons un algorithme appelé MωMvC2 pour apprendre un vote de majorité multivue.
Aujourd'hui, nous recueillons des données de différentes sources, en ligne et hors ligne. Habituellement, ces données se réfèrent principalement à des actes déjà commis et sont généralement analysées a posteriori. Étant donné que l'alerte précoce conduit à une action rapide (et, espérons-le, plus efficace), nous suggérons dans ce travail d'essayer d'identifier les signaux d'alerte précoce des actions ou activités qui se produiront à l'avenir. Un signal faible est une preuve passée ou actuelle avec des interprétations ambiguës pouvant être corrélée à d'autres événements et tendances plus importants, présents ou futurs. Les signaux faibles sont des indices peu clairs qui peuvent nous avertir d'autres modèles émergents. Le problème clé est de détecter ces signaux pertinents cachés dans la masse de données disponibles. Avant d'être sûr que l'attention des analystes humains est nécessaire, il est important de détecter, mais également de vérifier et de valider ces signaux au moyen d'une série de méthodes différentes. Une fois identifié et validé, un signal faible devient un signe précoce qu'il convient de surveiller attentivement. Dans ce travail, nous souhaitons établir des méthodes d'identification des signaux faibles au sein d'ensembles de données volumineux, tels que des ensembles de données provenant de médias sociaux ou d'autres sources accessibles au public. Des algorithmes basés sur des méthodes d'apprentissage statistique ou automatique ou d'intelligence artificielle, combinés à des descriptions sémantiques formelles des événements qui nous intéressent, nous permettraient d'identifier les signaux d'alerte précoce et de les transformer en indications d'actes futurs. De plus, les méthodes liées à la détection d'entités nommées nous permettraient de lier ces signaux d'alerte aux entités participantes (par exemple, des personnes, des lieux, etc.) et d'utiliser les descriptions d'entités pour améliorer et élargir la description des événements possibles. Le travail sera validé en appliquant ses résultats aux données réelles fournies par PJGN, partenaire de ce travail. Identifier des références à des événements et résoudre les informations autour de ces événements est un problème important pour PJGN et le travail sera directement applicable.
Cette thèse s'inscrit dans le cadre de l'analyse d' Notre étude se concentre sur l'utilisation d'une approche neuronale pour améliorer la détection de polarité, en exploitant la puissance des embeddings. En effet, ceux-ci se sont révélés un atout fondamental dans différentes tâches de traitement automatique des langues naturelles (TALN). Notre contribution dans le cadre de cette thèse porte plusieurs axes. Nous commençons, d'abord, par une étude préliminaire des différentes ressources d'embeddings de mots pré-entraînés existants en langue arabe. Ces embeddings considèrent les mots comme étant des unités séparées par des espaces afin de capturer, dans l'espace de projection, des similarités sémantiques et syntaxiques. Les phénomènes comme l'agglutination et la richesse morphologique de l'arabe sont alors pris en compte. Ces embeddings spécifiques ont été utilisés, seuls et combinés, comme entrée à deux réseaux neuronaux (l'un convolutif et l'autre récurrent) apportant une amélioration des performances dans la détection de polarité sur un corpus de revues. Nous proposons une analyse poussée des embeddings proposées. Dans une évaluation intrinsèque, nous proposons un nouveau protocole introduisant la notion de la stabilité de polarités (sentiment stability) dans l'espace d'embeddings. Puis, nous proposons une analyse qualitative extrinsèque de nos embeddings en utilisant des méthodes de projection et de visualisation.
La combinaison du traitement sémantique des connaissances (Semantic Processing of Knowledge) et de la modélisation des étapes de raisonnement (Modeling Steps of Reasoning), utilisés dans le domaine clinique, offrent des possibilités intéressantes, nécessaires aussi, pour l'élaboration des ontologies médicales, utiles à l'exercice de cette profession. Dans ce cadre, l'interrogation de banques de données médicales multiples, comme MEDLINE, PubMed… constitue un outil précieux mais insuffisant car elle ne permet pas d'acquérir des connaissances facilement utilisables lors d'une démarche clinique. En effet, l'abondance de citations inappropriées constitue du bruit et requiert un tri fastidieux, incompatible avec une pratique efficace de la médecine. Dans un processus itératif, l'objectif est de construire, de façon aussi automatisée possible, des bases de connaissances médicales réutilisables, fondées sur des ontologies et, dans cette thèse, nous développons une série d'outils d'acquisition de connaissances qui combinent des opérateurs d'analyse linguistique et de modélisation de la clinique, fondés sur une typologie des connaissances mises en œuvre, et sur une implémentation des différents modes de raisonnement employés. La connaissance ne se résume pas à des informations issues de bases de données ; elle s'organise grâce à des opérateurs cognitifs de raisonnement qui permettent de la rendre opérationnelle dans le contexte intéressant le praticien. Un système multi-agents d'aide à la décision clinique (SMAAD) permettra la coopération et l'intégration des différents modules entrant dans l'élaboration d'une ontologie médicale et les sources de données sont les banques médicales, comme MEDLINE, et des citations extraites par PubMed ; les concepts et le vocabulaire proviennent de l'Unified Medical Language System (UMLS). Concernant le champ des bases de connaissances produites, la recherche concerne l'ensemble de la démarche clinique : le diagnostic, le pronostic, le traitement, le suivi thérapeutique de différentes pathologies, dans un domaine médical donné. Sur l'ensemble, nous avons travaillé les aspects logiques liés aux opérateurs cognitifs de raisonnement utilisés et nous avons organisé la coopération et l'intégration des connaissances exploitées durant les différentes étapes du processus clinique (diagnostic, pronostic, traitement, suivi thérapeutique). Cette intégration s'appuie sur un SMAAD : système multi-agent d'aide à la décision.
Le but de ce travail de thèse est de rendre sincère un Agent Conversationnel Animé (ACA) pour, d'une part, améliorer sa crédibilité du point de vue de l'humain, et d'autre part contribuer à le rendre acceptable dans une relation privilégiée compagnon artificiel - humain. Les états mentaux portés par les ACM sont formalisés en logique : la volonté de représenter des états mentaux issus de raisonnements complexes (basés d'une part sur le raisonnement contrefactuel et d'autre part sur les normes et les buts de l'agent), dont l'expression se fait avant tout par le langage (Oatley 1987), a amené à mettre en place le modèle BIGRE (Beliefs, Ideals, Goals, Responsibility, Emotions). Ce modèle, basé sur une logique de type BDI (Belief, Desire, Intention), permet de représenter également des émotions que nous appelons complexes, telles que la réjouissance, la gratitude ou le regret. Le LCM est implémenté dans l'ACA Greta, ce qui permet une évaluation de ce langage en termes de crédibilité et de sincérité perçues par l'humain. La deuxième partie de ce travail porte sur les capacités de raisonnement de l'ACA : dans le but de permettre à l'agent de raisonner dans le dialogue, c'est-à-dire mettre à jour ses états mentaux et ses émotions et sélectionner son intention communicative, un moteur de raisonnement a été mis en place. Ce moteur de raisonnement est basé sur le cycle de comportement BDI - Perception, Décision, Action - et les opérateurs du modèle BIGRE, permettant ainsi la manipulation d'états mentaux issus de raisonnements complexes (dont les émotions complexes). Les ACM qui composent notre langage sont intégrés dans le moteur, et sont utilisés pour atteindre l'intention communicative de l'ACA : par exemple, si l'agent a l'intention d'exprimer sa gratitude, il construit un plan pour satisfaire son intention, formé des ACM remercier ou féliciter, selon le degré de l'émotion. Un type d'intention communicative, déclenché par des règles d'obligation du discours, participe à la régulation locale du dialogue. L'ACA étant de plus affectif, sa sincérité l'amène à exprimer toutes ses émotions. La généricité de ce moteur de raisonnement permet de l'implémenter dans l'ACA Greta (où il est en lien avec le LCM) et dans l'agent MARC. L'expression multimodale des ACM avec l'agent MARC a été rendue possible par l'intégration des checks de Scherer dans le moteur de raisonnement que nous avons adapté au contexte du dialogue. Une évaluation du moteur de raisonnement avec l'agent MARC montre que les états mentaux déduits par le moteur sont appropriés à la situation, et que leur expression (l'expression de la sincérité de l'agent) est également appropriée.
L'Intelligence Artificielle est présente dans tous les aspects de notre vie à l'ère du Big Data. Elle a entraîné des changements révolutionnaires dans divers secteurs, dont le commerce électronique et la finance. Dans cette thèse, nous présentons quatre applications de l'IA qui améliorent les biens et services existants, permettent l'automatisation et augmentent considérablement l'efficacité de nombreuses tâches dans les deux domaines. Tout d'abord, nous améliorons le service de recherche de produits offert par la plupart des sites de commerce électronique en utilisant un nouveau système de pondération des termes pour mieux évaluer l'importance des termes dans une requête de recherche. Ensuite, nous construisons un modèle prédictif sur les ventes quotidiennes en utilisant une approche de prévision des séries temporelles et tirons parti des résultats prévus pour classer les résultats de recherche de produits afin de maximiser les revenus d'une entreprise. Ensuite, nous proposons la difficulté de la classification des produits en ligne et analysons les solutions gagnantes, consistant en des algorithmes de classification à la pointe de la technologie, sur notre ensemble de données réelles. Nous effectuons une étude approfondie sur chaque titre de l'indice S&amp ; P 500 en utilisant quatre algorithmes de classification à la pointe de la technologie et nous publions des résultats très prometteurs
Les produits et réseaux électriques sont soumis à un environnement réglementaire et normatif fractionné selon la géographie, le niveau de tension et le secteur d'application. L'importance croissante des nouvelles technologies associées à la transition énergétique (énergies renouvelables, véhicule électrique, stockage et optimisation de l'énergie) entraîne une accélération de la production de référentiels normatifs et réglementaires. Dans ce contexte, la conception de nouveaux produits, systèmes et installations, requiert une veille permanente sur les textes de référence à prendre en compte, l'évolution du vocabulaire et des concepts employés et, évidemment, les contraintes qu'il est nécessaire de respecter. L'objectif de la thèse proposée est d'extraire de collections documentaires les règles et contraintes de conception à appliquer en les traduisant dans un langage formel actionnable (c'est-à-dire compréhensible et utilisable par les experts du domaine) pour l'aide à la conception. Ceci inclut la formalisation des connaissances du domaine, l'analyse et la compréhension des textes, la construction de synthèse des règles extraites, ainsi que la détection de conflits et doublons
Étant donnée la masse toujours croissante de texte publié, la compréhension automatique des langues naturelles est à présent l'un des principaux enjeux de l'intelligence artificielle. En langue naturelle, les faits exprimés dans le texte ne sont pas nécessairement tous explicites : le lecteur humain infère les éléments manquants grâce à ses compétences linguistiques, ses connaissances de sens commun ou sur un domaine spécifique, et son expérience. Les systèmes de Traitement Automatique des Langues (TAL) ne possèdent naturellement pas ces capacités. Incapables de combler les défauts d'information du texte, ils ne peuvent donc pas le comprendre vraiment. Cette thèse porte sur ce problème et présente notre travail sur la résolution d'inférences pour la compréhension automatique de texte. Une inférence textuelle est définie comme une relation entre deux fragments de texte : un humain lisant le premier peut raisonnablement inférer que le second est vrai. Beaucoup de tâches de TAL évaluent plus ou moins directement la capacité des systèmes à reconnaître l'inférence textuelle. Au sein de cette multiplicité de l'évaluation, les inférences elles-mêmes présentent une grande variété de types. Nous nous interrogeons sur les inférences en TAL d'un point de vue théorique et présentons deux contributions répondant à ces niveaux de diversité : une tâche abstraite contextualisée qui englobe les tâches d'inférence du TAL, et une taxonomie hiérarchique des inférences textuelles en fonction de leur difficulté. La reconnaissance automatique d'inférence textuelle repose aujourd'hui presque toujours sur un modèle d'apprentissage, entraîné à l'usage de traits linguistiques variés sur un jeu d'inférences textuelles étiquetées. Cependant, les données spécifiques aux phénomènes d'inférence complexes ne sont pour le moment pas assez abondantes pour espérer apprendre automatiquement la connaissance du monde et le raisonnement de sens commun nécessaires. Les systèmes actuels se concentrent plutôt sur l'apprentissage d'alignements entre les mots de phrases reliées sémantiquement, souvent en utilisant leur structure syntaxique. Pour étendre leur connaissance du monde, ils incluent des connaissances tirées de ressources externes, ce qui améliore souvent les performances. Mais cette connaissance est souvent ajoutée par dessus les fonctionnalités existantes, et rarement bien intégrée à la structure de la phrase. Nos principales contributions dans cette thèse répondent au problème précédent. En partant de l'hypothèse qu'un lexique plus simple devrait rendre plus facile la comparaison du sens de deux phrases, nous décrivons une méthode de récupération de passage fondée sur une expansion lexicale structurée et un dictionnaire de simplifications. Cette hypothèse est testée à nouveau dans une de nos contributions sur la reconnaissance d'implication textuelle : des paraphrases syntaxiques sont extraites du dictionnaire et appliquées récursivement sur la première phrase pour la transformer en la seconde. Nous présentons ensuite une méthode d'apprentissage par noyaux de réécriture de phrases, avec une notion de types permettant d'encoder des connaissances lexico-sémantiques. Cette approche est efficace sur trois tâches : la reconnaissance de paraphrases, d'implication textuelle, et le question Des tests de compréhension sont utilisés pour son évaluation, sous la forme de questions à choix multiples sur des textes courts, qui permettent de tester la résolution d'inférences en contexte. Notre système est fondé sur un algorithme efficace d'édition d'arbres, et les traits extraits des séquences d'édition sont utilisés pour construire deux classifieurs pour la validation et l'invalidation des choix de réponses. Cette approche a obtenu la deuxième place du challenge "Entrance Exams" à CLEF 2015.
Nous proposons une approche pour détecter les sujets, les communautés d'intérêt non disjointes, l'expertise, les tendances et les activités dans des sites où le contenu est généré par les utilisateurs et en particulier dans des forums de questions-réponses tels que StackOverFlow. Nous décrivons d'abord QASM (Questions &amp ; Réponses dans des médias sociaux), un système basé sur l'analyse de réseaux sociaux pour gérer les deux principales ressources d'un site de questions-réponses : les utilisateurs et le contenu. Nous présentons également le vocabulaire QASM utilisé pour formaliser à la fois le niveau d'intérêt et l'expertise des utilisateurs. Nous proposons ensuite une approche efficace pour détecter les communautés d'intérêts. Elle repose sur une autre méthode pour enrichir les questions avec un tag plus général en cas de besoin. Nous comparons trois méthodes de détection sur un jeu de données extrait du site populaire StackOverflow. Notre méthode basée sur le se révèle être beaucoup plus simple et plus rapide, tout en préservant la qualité de la détection. Nous proposons en complément une méthode pour générer automatiquement un label pour un sujet détecté en analysant le sens et les liens de ses mots-clefs. Nous menons alors une étude pour comparer différents algorithmes pour générer ce label. Enfin, nous étendons notre modèle de graphes probabilistes pour modéliser conjointement les sujets, l'expertise, les activités et les tendances. Nous le validons sur des données du monde réel pour confirmer l'efficacité de notre modèle intégrant les comportements des utilisateurs et la dynamique des sujets.
Nous sommes amenés chaque jour à prendre un nombre important de décisions : quel nouveau livre lire ? Quel film regarder ce soir ou où aller ce week-end ? De plus en plus, nous utilisons les ressources en ligne pour nous aider à prendre des décisions. Comme la prise de décision est assistée par le domaine en ligne, l'utilisation de systèmes de recommandation est devenue essentielle dans la vie quotidienne. Dans le même temps, les réseaux sociaux sont devenus une partie indispensable de ce processus ; partout dans le monde on les utilise quotidiennement pour récupérer des données de personne et de sources d'information en qui on a confiance. Quand les internautes passent du temps sur les réseaux sociaux, ils laissent de précieuses informations sur eux-mêmes. Comme le domaine de la recommandation est un domaine qui a assisté à des changements de grande ampleur attribuable à des réseaux sociaux, il y a un intérêt évident pour les systèmes de recommandation sociale. Cependant, dans la littérature de ce domaine, nous avons constaté que de nombreux systèmes de recommandation sociale ont été évalués en utilisant des réseaux sociaux spécialisés comme Epinions, Flixter et d'autres types des réseaux sociaux de recommandation, qui tendent à être composées d'utilisateurs, d'articles, de notes et de relations.
Les enceintes intelligentes offrent la possibilité d'interagir avec les systèmes informatiques de la maison. Elles permettent d'émettre un éventail de requêtes sur des sujets divers et représentent les premières interfaces vocales disponibles couramment dans les environnements domestiques. La compréhension des commandes vocales concerne des énoncés courts ayant une syntaxe simple, dans le domaine des habitats intelligents destinés à favoriser le maintien à domicile des personnes âgées. Ils les assistent dans leur vie quotidienne, améliorant ainsi leur qualité de vie, mais peuvent aussi leur porter assistance en situations de détresse. La conception de ces habitats se concentre surtout sur les aspects de la sécurité et du confort, ciblant fréquemment sur la détection de l'activité humaine., en particulier pour des langues autres que l'anglais, alorsqu'ils sont essentiels pour développer les systèmes de communication entre l'habitat et ses habitants. La disponibilité de tels corpus, pourrait contribuer au développement d'une génération d'enceintes intelligentes qui soient capables d'extraire des commandes vocales plus complexes. Pour contourner une telle contrainte, une partie de notre travail consiste à développer un générateur de corpus, produisant des commandes vocales spécifiques au domaine domotique, automatiquement annotées d'étiquettes d'intentions et de concepts. Un système de compréhension de la parole (SLU - Spoken Language Understanding) est nécessaire afin d'extraire les intentions et les concepts des commandes vocales avant de les fournir au module de prise de décision en charge de l'exécution des commandes. Comme plusieurs études l'ont montré, l'enchaînement entre RAP et NLU dans une approche séquentielle de SLU cumule les erreurs. À cette fin, nous élaborons d'abord une approche SLU séquentielle comme approche de référence, dans laquelle une méthode classique de RAP génère des transcriptions qui sont transmises au module NLU, avant de poursuivre par le développement d'un module de SLU de bout en bout. Ces deux systèmes de SLU sont évalués sur un corpus enregistré spécifiquement au domaine de la domotique. Nous étudions si l'information prosodique, à laquelle la SLU de bout en bout a accès, contribue à augmenter les performances. Nous comparons aussi la robustesse des deux approches lorsqu'elles sont confrontées à un style de parole aux niveaux sémantiques et syntaxiques plus varié. Cette étude est menée dans le cadre du projet VocADom financé par l'appel à projets génériques de l'ANR.
En raison de ses enjeux sociétaux, économiques et culturels, l'intelligence artificielle (dénotée IA) est aujourd'hui un sujet d'actualité très populaire. L'un de ses principaux objectifs est de développer des systèmes qui facilitent la vie quotidienne de l'homme, par le biais d'applications telles que les robots domestiques, les robots industriels, les véhicules autonomes et bien plus encore. La montée en popularité de l'IA est fortement due à l'émergence d'outils basés sur des réseaux de neurones profonds qui permettent d'apprendre simultanément, la représentation des données (qui était traditionnellement conçue à la main), et la tâche à résoudre (qui était traditionnellement apprise à l'aide de modèles d'apprentissage automatique). Ceci résulte de la conjonction des avancées théoriques, de la capacité de calcul croissante ainsi que de la disponibilité de nombreuses données annotées. Celle-ci consiste `a l'apprentissage d'un modèle qui une fois appris résoud une certaine tâche, et est généralement composée de deux sous-modules, l'un représentant la donnée (nommé ”représentation”) et l'autre prenant des décisions (nommé ”résolution de tâche”). Alors que la spécialisation a été largement explorée par la communauté de l'apprentissage profond, seules quelques tentatives implicites ont été réalisée vers la seconde catégorie, à savoir, l'universalité.
Lorsqu'il s'agit la construction de ressources lexico-sémantiques multilingues, la première chose qui vient à l'esprit, et la nécessité que les ressources à alignées partagent le même format de données et la même représentations (interopérabilité représentationnelle). Avec l'apparition de standard tels que LMF et leur adaptation au web sémantique pour la production de ressources lexico- sémantiques multilingues en tant que données lexicales liées ouvertes (Ontolex), l'interopérabilité représentationnelle n'est plus un verrou majeur. Cependant, en ce qui concerne l'interopérabilité des alignements multilingues, le choix et la construction du pivot interlingue est l'un des obstacles principaux. L'utilisation d'une pivot à acceptions interlingues, solution proposée il y a déjà plus de 20 ans, pourrait être viable. Néanmoins, leur construction manuelle est trop ardue du fait du manque d'experts parlant assez de langues et leur construction automatique pose problème du fait de l'absence d'une formalisation et d'une caractérisation axiomatique permettant de garantir leur propriétés. Nous proposons dans cette thèse de d'abord formaliser l'architecture à pivot interlingue par acceptions, en développant une axiomatisation garantissant leurs propriétés. Nous proposons ensuite des algorithmes de construction initiale automatique en utilisant les propriétés combinatoires du graphe des alignements bilingues, mais aussi des algorithmes de mise à jour garantissant l'interopérabilité dynamique. Dans un deuxième temps, nous étudions de manière plus pratique sur DBNary, un extraction périodique de Wiktionary dans de nombreuses éditions de langues, afin de cerner les contraintes pratiques à l'application des algorithmes proposés.
Cette thèse examine les convergences et divergences le long de l'histoire de la documentation muséale dans trois galeries nationales en Angleterre, au Canada et aux États-Unis. La lutte continue afin d'intégrer véritablement la technologie dans la documentation d'art trahit l'héritage difficile entre ces deux modèles opposés. Tout au long des trajectoires uniques à chacune des institutions étudiées, peu de preuves montrent que les technologies optiques ou numériques ont eu des répercussions importantes sur les méthodologies ou les philosophies de la documentation des œuvres d'art. Au contraire, on observe que la documentation de forme numérique repose toujours sur une approche minimale de recueil de données, sur un groupe restreint de personnes habilitées à les collecter et sur un accès limité à ces données. Cette recherche renforce l'argumentation pour une redéfinition de la documentation des œuvres d'art pour repenser ses stratégies et ses philosophies directrices, pour poser un nouveau regard sur la recherche dans les collections et pour élargir l'intégration des technologies numériques dans ces processus.
Cette étude est une analyse dialectométrique des parlers berbères de Kabylie. Le présent travail inclut un échantillon de 168 parlers kabyles répartis sur tout le territoire kabylophone. Le corpus analysé compte 130 entrées (lexèmes et syntagmes) recueillies dans chacune des variétés prises en compte. Nous avons opté pour la méthode Levenshtein afin de calculer la distance entre les variantes. Nous avons choisi l'algorithme de Ward's Method pour regrouper les variétés. Nous avons testé trois méthodes pour calculer la distance entre les sons : la méthode binaire, la distance d'Euclide et la distance de Manhattan. L'analyse des résultats nous a permis de montrer le continuum dialectal en Kabylie et de classifier les parlers kabyles en cinq zones infradialectales principales.
L'activité scientifique de cette thèse consiste à :  Comprendre la nature des relations d'anaphore et de coréférence dans les messages issus de la communication électronique médiée, avec une priorité donnée aux e-mails, de sorte à mener vers des méthodes capables de tisser des liens sémantiques entre différentes phrases et différents messages. Etudier plus précisément les anaphores événementielles (Danlos, 2004) et la nature des relations entre les événements. Il s'agit d'un verrou scientifique majeur pour la compréhension du langage naturel avec une retombée directe sur la suite du développement de la solution Prevyo de l'entreprise d'accueil Emvista. L'avancée scientifique et l'appropriation des technologies en résultant permettront à Emvista de se positionner comme un acteur important de la compréhension du langage naturel.
Au cours des dernières années, le secteur touristique a été caractérisé par toute une série de changements fondamentaux. Notre mémoire de recherche se situe à l'intersection de la terminologie thématique, de la linguistique de corpus et du traitement automatique des langues. Dans le premier chapitre du travail que nous allons présenter, nous chercherons à introduire aux domaines d'études théoriques sur lesquels notre recherche s'appuie. Premièrement, on traitera de la linguistique de corpus et on examinera les différentes catégories de corpus existantes. On mettra l'accent sur deux notions fondamentales dans la conception de l'outil corpus en général et dans la création de notre corpus en particulier : représentativité et contexte. Avant tout, on fournira les indications théoriques et pragmatiques nécessaires pour réaliser un corpus trilingue en langue de spécialité : la collecte des textes, l'homogénéisation des échantillons textuels repérés et l'annotation. Au cours de ce chapitre, nous présenterons Alinea, l'instrument qu'on a utilisé pour l'alignement de textes recueillis et pour la consultation simultanée des traductions trilingues. Dans le troisième et dernier chapitre, on passera à l'interrogation du corpus créé. Sur la base d'un terme pris comme exemple, le terme ville, on lancera la recherche dans le CTT. Ensuite, on analysera les collocations les plus usitées contenant le mot ville. Pour conclure, l'objectif général de notre étude sera d'explorer la chaîne de gestion terminologique à travers la création d'un glossaire trilingue dans le domaine du tourisme.
Les différences entre conditions d'apprentissage et conditions de test peuvent considérablement dégrader la qualité des transcriptions produites par un système de reconnaissance automatique de la parole (RAP). L'adaptation est un moyen efficace pour réduire l'inadéquation entre les modèles du système et les données liées à un locuteur ou un canal acoustique particulier. Il existe deux types dominants de modèles acoustiques utilisés en RAP : les modèles de mélanges gaussiens (GMM) et les réseaux de neurones profonds (DNN). L'approche par modèles de Markov cachés (HMM) combinés à des GMM (GMM-HMM) a été l'une des techniques les plus utilisées dans les systèmes de RAP pendant de nombreuses décennies. Plusieurs techniques d'adaptation ont été développées pour ce type de modèles. L'objectif principal de cette thèse est de développer une méthode de transfert efficace des algorithmes d'adaptation des modèles GMM aux modèles DNN. La technique proposée fournit un cadre général pour le transfert des algorithmes d'adaptation développés pour les GMM à l'adaptation des DNN. Elle est étudiée pour différents systèmes de RAP à l'état de l'art et s'avère efficace par rapport à d'autres techniques d'adaptation au locuteur, ainsi que complémentaire.
Associer une orientation temporelle au sens des mots pour capter l'information temporelle en langue est une tâche relativement directe pour les humains utilisant leurs connaissances sur le monde. Une base de connaissances lexicales associant automatiquement cette orientation au sens des mots serait de fait cruciale pour les tâches automatiques visant à interpréter la temporalité dans les textes. Dans cette recherche, nous présentons une ontologie temporelle, TempoWordNet, où les synsets de WordNet sont enrichis avec une information sur leur temporalité intrinsèque : atemporel, passé, présent et futur. TempoWordNet est évalué de manière intrinsèque et extrinsèque, une ressource fiable devant à la fois contenir un étiquetage temporel de haute qualité et améliorer les performances de certaines tâches externes. Les deux types d'évaluations montrent la qualité et l'intérêt de la ressource. Pour compléter nos travaux, nous étudions aussi comment une application de recherche telle un moteur de recherche peut tirer parti de cette ressource. Le retour des utilisateurs de TempoWordNet a encouragé à améliorer encore la ressource. Nous terminons donc en proposant une nouvelle stratégie de construction permettant d'améliorer de manière conséquente TempoWordNet.
Après s'être interrogée sur la notion de sens, cette thèse se fixe pour objectif de dégager un certain nombre d'éléments linguistiques allant dans le sens d'une caractérisation abstraite à même de rendre compte des différents emplois du verbe anglais want. Il est proposé que ce verbe conserve aujourd'hui quelque chose de son emploi diachronique initial en tant que « verbe de manque » à partir d'une première opération de localisation par laquelle le sujet grammatical est mis en relation avec un objet ou une propriété non-immédiatement disponible pour lui dans la situation d'énonciation. Cette mise en relation particulière accompagne un processus de subjectivisation culminant dans une tendance forte à la modalisation de l'énoncé en want, en lien avec des effets déontiques et épistémiques. Il semble intéressant de le traiter comme un domaine notionnel organisé autour de p, lequel sert de repère permettant d'envisager une seconde opération de localisation qui autorise le repérage d'une relation permettant d'atteindre p. Ainsi, la distinction désir / volonté est pensable en termes de degrés dans les déterminations que l'on peut construire à partir du manque. Ce travail met donc en relation les constructions syntaxiques possibles du second argument de want et ces déterminations.
Cette hétérogénéité des sources de connaissances pose le problème de l'utilisation secondaire des données, et en particulier de l'exploitation de données hétérogènes dans le cadre de la médecine personnalisée ou translationnelle. En effet, les données à utiliser peuvent être codées par des sources de connaissances décrivant la même notion clinique de manière différente ou décrivant des notions distinctes mais complémentaires. Pour répondre au besoin d'utilisation conjointe des sources de connaissances encodant les données de santé, nous avons étudié trois processus permettant de répondre aux conflits sémantiques (difficultés résultant de leur mise en relation) : (1) Dans un premier travail, nous avons aligné la terminologie d'interface du laboratoire d'analyses du CHU de Bordeaux à la LOINC. Deux étapes principales ont été mises en place : (i) le prétraitement des libellés de la terminologie locale qui comportaient des troncatures et des abréviations, ce qui a permis de réduire les risques de survenue de conflits de nomenclature, (ii) le filtrage basé sur la structure de la LOINC afin de résoudre les différents conflits de confusion. Ainsi, les médicaments dans RxNorm ont été décrits en OWL grâce à leurs éléments définitionnels (substance, unité de mesure, dose, etc.). Nous avons ensuite fusionné cette représentation de RxNorm à la structure de la SNOMED CT, résultant en une nouvelle source de connaissances. Notre méthode a résolu des conflits de nomenclature mais s'est confrontée à certains conflits de confusion et d'échelle, ce qui a mis en évidence le besoin d'améliorer RxNorm et SNOMED CT.Finalement, nous avons réalisé une intégration sémantiquement enrichie de la CIM10 et de la CIMO3 en utilisant la SNOMED CT comme support. La CIM10 décrivant des diagnostics et la CIMO3 décrivant cette notion suivant deux axes différents (celui des lésions histologiques et celui des localisations anatomiques), nous avons utilisé la structure de la SNOMED CT pour retrouver des relations transversales entre les concepts de la CIM10 et de la CIMO3 (résolution de conflits ouverts). Au cours du processus, la structure de la SNOMED CT a également été utilisée pour supprimer les mappings erronés (conflits de nomenclature et de confusion) et désambiguïser les cas de mappings multiples (conflits d'échelle).
Les récentes avancées en matière d'intelligence artificielle ont vu une adoption limitée dans la communauté des auteurs de revues systématiques, et une grande partie du processus d'élaboration des revues systématiques est toujours manuelle, longue et coûteuse. Les auteurs de revues systématiques rencontrent des défis tout au long du processus d'élaboration d'une revue. Il est long et difficile de chercher, d'extraire, de collecter des données, de rédiger des manuscrits et d'effectuer des analyses statistiques. L'automatisation de la sélection d'articles a été proposé comme un moyen de réduire la charge de travail, mais son adoption a été limitée en raison de différents facteurs,notamment l'investissement important de prise en main, le manque d'accompagnement et les décalages par rapport au flux de travail. Il est nécessaire de mieux harmoniser les méthodes actuelles avec les besoins de la communauté des revues systématiques. Les études sur l'exactitude des tests diagnostiques sont rarement indexées de façon à pouvoir être facilement retrouvées dans les bases de données bibliographiques. Les requêtes de recherche méthodologique visant à repérer les études diagnostiques ont donc tendance à être peu précises, et leur utilisation dans les études méthodiques est déconseillée. Par conséquent, il est particulièrement nécessaire d'avoir recours à d'autres méthodes pour réduire la charge de travail dans les études méthodiques sur l'exactitude des tests diagnostiques. Dans la présente thèse, nous avons examiné l'hypothèse selon laquelle les méthodes d'automatisation peuvent offrir un moyen efficace de rendre le processus d'élaboration des revues systématique plus rapide et moins coûteux, à condition de pouvoir cerner et surmonter les obstacles à leur adoption. Les travaux réalisés montrent que les méthodes automatisées offrent un potentiel de diminution des coûts tout en améliorant la transparence et la reproductibilité du processus.
Cette étude consiste en une analyse comparée des unités syntaxiques (les syntagmes) et des constructions fondamentales du fiançais et du persan, en ayant un regard sur les Langues Contrôlées (LC) et les cas problématiques et ambigus pour la traduction. Après un passage sur l'histoire de ces langues et une brève présentation du système d 'écriture et phonétique du persan,les classes de mots et leurs classifications traditionnelle et moderne sont comparées. Ensuite, les structures des syntagmes déterminant, nominal, adjectival, prépositionnel, adverbial et verbal et la nature de leurs composants, ainsi que les constructions fondamentales de la phrase de base dans ces deux langues sont analysées. Tout au long du parcours, en faisant quelques tests de traduction avec des étudiants persanophones, certains cas problématiques pour la traduction sont repérés et traités pour une langue contrôlée français-persan éventuelle. Dans la synthèse finale, sont rassemblées, les structures syntagmatiques et certaines instructions pour élaborer une LC concernant les langues française et persane.
Le marché de l'aviation fait face aujourd'hui à une croissance rapide des technologies innovantes. Les drones cargo, les taxis drones, les dirigeables, les ballons stratosphériques, pour n'en citer que quelques-uns, pourraient faire partie de la prochaine génération de transport aérien. Dans le même temps, les Petites et Moyennes Entreprises (PMEs) s'impliquent de plus en plus dans la conception et le développement de nouvelles formes de système aéroporté, passant du rôle traditionnel de fournisseur à celui de concepteur et intégrateur. Cette situation modifie considérablement la portée de la responsabilité des PMEs. En tant qu'intégrateurs, elles deviennent responsables de la certification des composants et du processus de fabrication, domaine dans lequel elles n'ont encore que peu d'expérience. La certification, qui requiert une connaissance très spécifique des réglementations, des normes et standards, demeure un processus obligatoire et une activité critique pour les entreprises de l'industrie aéronautique. C'est aussi un défi majeur pour les PMEs qui doivent assumer cette responsabilité de certification avec des moyens limités. Dans cette thèse, deux besoins majeurs sont identifiés : le soutien méthodologique n'est pas facilement disponible pour les PMEs ; et les exigences de certification ne sont pas facilement compréhensibles et adaptables à chaque situation. Nous examinons donc des voies alternatives pour réduire la complexité de la situation des PMEs. L'objectif est de fournir un soutien afin qu'elles puissent être plus efficaces pour comprendre et intégrer les règles, les législations et les lignes directrices à leurs processus internes de manière plus simple. Cette thèse propose ainsi une approche méthodologique pour soutenir ces organisations. Développée en étroite collaboration avec une PME française, l'approche est composée d'un ensemble de modèles (métamodèle, modèles structurels et comportementaux) couverts par un mécanisme de gouvernance.
Cependant, cette mise en commun de ressources, données et savoir-faire implique de nouvelles exigences en termes de sécurité. En effet, le manque de confiance dans les structures du Cloud est souvent vu comme un frein au développement de tels services. L'objectif de cette thèse est d'étudier les concepts d'orchestration de services, de confiance et de gestion des risques dans le contexte du Cloud. La contribution principale est un framework permettant de déployer des processus métiers dans un environnement Cloud, en limitant les risques de sécurité liés à ce contexte. La contribution peut être séparée en trois partie distinctes qui prennent la forme d'une méthode, d'un modèle et d'un framework. La méthode catégorise des techniques pour transformer un processus métier existant en un modèle sensibilisé (ou averti) qui prend en compte les risques de sécurité spécifiques aux environnements Cloud. Le modèle formalise les relations et les responsabilités entre les différents acteurs du Cloud. Ce qui permet d'identifier les différentes informations requises pour évaluer et quantifier les risques de sécurité des environnements Cloud. Le framework est une approche complète de décomposition de processus en fragments qui peuvent être automatiquement déployés sur plusieurs Clouds. Ce framework intègre également un algorithme de sélection qui combine les information de sécurité avec d'autres critère de qualité de service pour générer des configuration optimisées. Finalement, les travaux sont implémentés pour démontrer la validité de l'approche. Le framework est implémenté dans un outil. Le modèle d'évaluation des risques de sécurité Cloud est également appliqué dans un contexte de contrôle d'accès. La dernière partie présente les résultats de l'implémentation de nos travaux sur un cas d'utilisation réel.
Le contexte de cette thèse s'inscrit dans le domaine de la fouille de textes médicaux. L'intérêt central concerne l'extraction des informations sur les interactions entre l'alimentation et les médicaments. Les aliments peuvent en effet avoir des interactions avec les médicaments et cela peut mener à des conséquences malheureuses pour le patient. Ces interactions sont très peu étudiées. Il est ainsi nécessaire de proposer des méthodes de fouille de texte permettant d'analyser des articles scientifiques d'en extraire les informations relatives aux interactions entre aliment et médicament. On s'appuiera également sur les informations proposées par les ressources terminologiques et de données liées existantes.
Notre thèse se donne comme objectif le traitement de l'adjectif polysémique applicable au domaine du traitement automatique des langues(TAL) et plus particulièrement à la traduction automatique (TA) (français-coréen). La variabilité du sens des lexiques boursiers génère plusieurs interprétations sémantiques différentes et donc plusieurs traductions. Pour résoudre ce genre de problèmes dans le cadre du TAL, nous proposons un modèle de désambiguïsation en mettant en œuvre des informations co-textuelles et plus particulièrement le nom. Ce dernier constitue un syntagme nominal en se combinant avec un adjectif et permet de lever l'ambiguïté sémantique de l'adjectif et déterminer son sens adéquat. Les indices co-textuels sur lesquels nous nous appuyons sont les suivants : l'ordre syntaxique de l'adjectif, la propriété du nom et la classe sémantique à laquelle le nom appartient. Notre travail consiste à attribuer à chacun des adjectifs polysémiques sélectionnés les informations co-textuelles ci-dessus pour que la machine puisse trouver leur sens correct. La méthodologie proposée est généralisable à d'autres parties du discours, nous l'avons nous-même appliquée au verbe et à son co-texte, le nom
L'utilisation répandue des smartphones dans notre vie quotidienne et l'essor technologique que connait le monde aujourd'hui - offrant l'accès mobile à très haut débit - ont exponentiellement augmenté la demande sur les services de vidéo streaming mobile, ce qui justifie la tendance à explorer de nouvelles approches pour la distribution des contenus média. Afin d'assurer une qualité de streaming constante et acceptable, la majorité des approches proposent aujourd'hui d'adapter la distribution des flux média au contexte de l'utilisateur. Pour assurer une bonne QoE, les solutions de vidéo streaming mobile exigent la connaissance au préalable du contexte de l'utilisateur, comme par exemple la capacité de son lien physique ou la disponibilité de sa bande passante. L'acquisition de telles informations contextuelles est devenue possible aujourd'hui grâce à l'utilisation des capteurs sans fils dans les appareils mobiles et à l'existence de plusieurs applications intelligentes dédiées, le principe étant majoritairement d'exploiter la forte corrélation entre le contexte de l'utilisateur et sa position géographique. En outre, plusieurs études menées sur les modèles de mobilité des usagers ont exhibé une quasi-régularité spatio-temporelle dans leurs trajets quotidiens, soit en prenant les transports publics ou en allant vers des endroits fréquemment visités. Couplés avec les cartes radio, ces études permettent une haute précision dans la prédiction du contexte de l'utilisateur le long de son trajet. Dans cette thèse, nous nous intéressons à analyser l'impact de l'adaptation du service vidéo streaming au contexte de l'utilisateur sur sa QoE finale. Nous commençons par proposer CAMS (Context Aware Mode Switching), un mécanisme d'allocation de ressources qui dépend du contexte et qui s'applique à la distribution du vidéo streaming réel (non-adaptatif), pour assurer le minimum d'interruptions de vidéo. CAMS est conçu pour être déployé dans une topologie de réseau spécifique avec un modèle de mobilité particulier. Par la suite, nous explorons l'impact de la connaissance à l'avance du débit futur de l'utilisateur sur l'adaptation de la qualité de sa vidéo et sur le coût de sa transmission dans un contexte de streaming adaptatif. Nous proposons NEWCAST (aNticipating qoE With threshold sCheme And aScending biTrate levels), un algorithme proactif pour l'ajustement du coût et l'adaptation de la qualité sous réserve d'une prédiction parfaite du débit. Nous étendons cette étude, dans un deuxième temps, pour le cas où la prédiction du débit est imparfaite. Nous proposons, donc, d'autres algorithmes adaptatifs en nous inspirant de l'approche de NEWCAST. Pour étudier la faisabilité de ces algorithmes sur le plan pratique, nous menons quelques expérimentations dans un environnement émulé à l'aide du lecteur média DASH-IF-Reference. Finalement, nous explorons l'idée de coupler la connaissance parfaite du débit futur de l'utilisateur avec l'usage d'un mécanisme d'apprentissage automatique, pour améliorer la QoE dans un contexte de streaming adaptatif. Nous proposons, donc, un système à boucle fermée, basé sur le retour des utilisateurs, pour apprendre progressivement leurs préférences et pour optimiser adéquatement la transmission des futures vidéos. Ce système est particulièrement conçu pour être utilisé dans des populations hétérogènes avec des profils de QoE différents et inconnus à l'avance.
Cette thèse propose un modèle conventionnel de la structuration d'une tâche collaborative humain-machine pour concevoir un système interactif assistant un humain sur la réalisation de tâches complexes. Plus spécifiquement, nous nous focalisons sur des tâches dont la résolution est hautement opportuniste et ne peut pas être planifiée. Nous introduisons un modèle de représentation de la tâche s'appuyant sur les jeux de dialogue, des motifs d'interaction dialogique permettant de décrire la structure de l'interaction à l'aide d'enchaînements d'actes de dialogue conventionnellement acceptables. Nous organisons ces motifs dialogiques en états, des structures décrivant les comportements attendus de la part de chaque interlocuteur au cours des différentes sous-tâches de la tâche collaborative. Ces états regroupent les motifs dialogiques en un ensemble cohérent vis-à-vis de la sous-tâche à laquelle ils sont associés et les enrichissent avec des règles localement cohérentes. Celles-ci permettent de décrire les effets de la réalisation d'un motif dialogique par les interlocuteurs dans un contexte particulier de la tâche. Les états permettent au système qui les utilise de lier un énoncé de l'humain à l'état actuel du dialogue et de la tâche et d'initier des comportements cohérents avec l'interaction et constructifs pour la tâche. Les décisions prises par le système sont basées sur la notion de maturité, une valeur associée à chaque état représentant la capacité du système à prendre des initiatives dans cet état. Le modèle décisionnel du système est conçu pour être résilient et laisser un maximum de liberté à l'utilisateur. Ce modèle est implémenté dans CoCoA, un système qui collabore avec un humain pour l'assister lors de la réalisation d'une tâche complexe. À partir de l'étude d'un corpus de dialogues humain-humain sur une tâche de recherche d'information collaborative médicale, nous décrivons la tâche à l'aide de notre modèle. Ce cas d'utilisation est implémenté à l'aide de CoCoA et une évaluation est réalisée en simulant le comportement d'un utilisateur ayant un besoin d'information et en faisant varier ses actions à partir de comportements identifiés dans le corpus.
Cette thèse propose une analyse morphosyntaxique, prosodique et conversationnelle des marqueurs discursifs dans le débat présidentiel et dans le talkshow politique télévisuel. Les marqueurs discursifs, en corrélation avec la gestualité et l'intonation, participent considérablement à la construction de l'échange entre les différents participants au débat. Pour l'analyse, l'approche proposée par Haselow (2017) a été adoptée car elle émane de la prise en compte de toutes les autres approches citées plus haut tout en y ajoutant les théories cognitives. Sur le plan de l'organisation de l'interaction pendant le débat présidentiel ou les talkshows,les marqueurs discursifs, accompagnés de la gestualité et de l'intonation, jouent un rôle important dans la gestion des tours de parole et dans l'organisation séquentielle des actions. Sur le plan énonciatif, ils interviennent, accompagnés des gestes manuels et du regard, dans l'expression de l'emphase, de la reformulation et de l'opposition. Ils permettent également aux candidats d'attirer l'attention de leurs interlocuteurs dans le but d'introduire une justification, un changement d'optique, un discours rapporté, ou dans l'optique d'interrompre ces derniers, ou tout simplement pour rassurer les potentiels électeurs. Ils permettent aussi aux candidats d'anticiper ou de prendre en compte le point de vue de leur interlocuteurs (passifs ou actifs), en s'inscrivant ou en se positionnant dans une logique d'acceptation ou de rupture avec ces derniers.
Vidéo d'interprétation et de compréhension est l'un des objectifs de recherche à long terme dans la vision par ordinateur. Toutefois, afin de déployer des systèmes de reconnaissance visuelle à grande échelle dans la pratique, il devient important d'aborder l'évolutivité des techniques. L'objectif principal est cette thèse est de développer des méthodes évolutives pour l'analyse de contenu vidéo (par exemple pour le classement ou la classification).
Restorer la faculté de parler chez des personnes paralysées et aphasiques pourrait être envisagée via l'utilisation d'une interface cerveau L'objectif de cette thèse était de développer trois aspects nécessaires à la mise au point d'une telle preuve de concept. Premièrement, un synthétiseur permettant de produire en temps-réel de la parole intelligible et controlé par un nombre raisonable de paramètres est nécessaire. Nous avons donc développé un synthétiseur produisant de la parole intelligible à partir de données articulatoires. Dans un premier temps, nous avons enregistré un large corpus de données articulatoire et acoustiques synchrones chez un locuteur. Ensuite, nous avons utilisé des techniques d'apprentissage automatique, en particulier des réseaux de neurones profonds, pour construire un modèle permettant de convertir des données articulatoires en parole. Ce synthétisuer a été construit pour fonctionner en temps réel. Enfin, comme première étape vers un contrôle neuronal de ce synthétiseur, nous avons testé qu'il pouvait être contrôlé en temps réel par plusieurs locuteurs, pour produire de la parole inetlligible à partir de leurs mouvements articulatoires dans un paradigme de boucle fermée. Deuxièmement, nous avons étudié le décodage de la parole et de ses propriétés articulatoires à partir d'activités neuronales essentiellement enregistrées dans le cortex moteur de la parole. Nous avons construit un outil permettant de localiser les aires corticales actives, en ligne pendant des chirurgies éveillées à l'hôpital de Grenoble, et nous avons testé ce système chez deux patients atteints d'un cancer du cerveau. Les résultats ont montré que le cortex moteur exhibe une activité spécifique pendant la production de parole dans les bandes beta et gamma du signal, y compris lors de l'imagination de la parole. Les données enregistrées ont ensuite pu être analysées pour décoder l'intention de parler du sujet (réelle ou imaginée), ainsi que la vibration des cordes vocales et les trajectoires des articulateurs principaux du conduit vocal significativement au dessus du niveau de la chance. Enfin, nous nous sommes intéressés aux questions éthiques qui accompagnent le développement et l'usage des interfaces cerveau
Les documents papiers sont à la base de nos connaissances et renferment une myriade d'informations dont certaines sont très précieuses pour notre société. Dans un but de préservation et afin de les rendre plus accessibles, de nombreux projets de numérisation visent à convertir ce type de documents en textes numérisés, notamment en utilisant des logiciels de reconnaissance optique de caractères (OCR). Ainsi, une œuvre sera numérisée page par page, ce qui ne correspond généralement qu'à une organisation physique et pas à l'intention rédactionnelle des auteurs. Un second verrou du processus de numérisation, qui en est également le plus important, correspond aux performances des moteurs d'OCR. En effet, celles-ci sont substantiellement réduites pour les documents patrimoniaux qui ont généralement subis des dégradations. Les erreurs d'OCR que cela induit ont un impact non négligeable sur la performance des outils de recherches et sur les systèmes de traitement du langage naturel puisqu'il faut par exemple apparier des besoins bien écrits à des textes mal reconnus. Cette thèse a pour objectif de faciliter l'accès aux documents historiques numérisés en étudiant les problèmes précédemment mentionnés. En vue de faciliter l'accès aux documents historiques, plusieurs approches sont proposées, visant à reconstruire les structures logiques des ouvrages et à améliorer la qualité des textes numérisés par OCR. Nos expériences ont démontré que cette approche surpasse l'état de l'art. La contribution majeure de cette thèse fournit, quant à elle, des méthodes pour la détection et la correction des erreurs d'OCR. Les caractéristiques communes et divergentes entre les erreurs d'OCR et celles des utilisateurs sont clarifiées pour mieux concevoir les traitements post-OCR. Normalement, un système de post-traitement détecte et rectifie les erreurs résiduelles. Toutefois, il peut être préférable de gérer ces erreurs séparément grâce à des applications qui permettent de filtrer, d'étiqueter, ou de traiter sélectivement de telles données. Les résultats montrent que les performances de nos méthodes sont comparables à plusieurs méthodes de référence sur des jeux de données en anglais utilisés lors des deux premières éditions de la compétition sur la correction des textes post-OCR organisée durant les conférence ICDAR en 2017 et 2019.
Les médias sociaux ont changé notre manière de communiquer entre individus, au sein des organisations et des communautés. La disponibilité de ces données sociales ouvre de nouvelles opportunités pour comprendre et influencer le comportement des utilisateurs. De ce fait, la fouille des médias sociaux connait un intérêt croissant dans divers milieux scientifiques et économiques. Dans cette thèse, nous nous intéressons spécifiquement aux utilisateurs de ces réseaux et cherchons à les caractériser selon deux axes : (i) leur expertise et leur réputation et (ii) les sentiments qu'ils expriment. De manière classique, les données sociales sont souvent fouillées selon leur structure en réseau. Cependant, le contenu textuel des messages échangés peut faire émerger des connaissances complémentaires qui ne peuvent être connues via la seule analyse de la structure. Jusqu'à récemment, la majorité des travaux concernant l'analyse du contenu textuel était proposée pour l'Anglais. L'originalité de cette thèse est de développer des méthodes et des ressources basées sur le contenu pour la fouille des réseaux sociaux pour la langue Française. Dans le premier axe, nous proposons d'abord d'identifier l'expertise des utilisateurs. Pour cela, nous avons utilisé des forums qui recrutent des experts en santé pour apprendre des modèles de classification qui servent à identifier les messages postés par les experts dans n'importe quel autre forum. Nous démontrons que les modèles appris sur des forums appropriés peuvent être utilisés efficacement sur d'autres forums. Puis, dans un second temps, nous nous intéressons à la réputation des utilisateurs dans ces forums. L'idée est de rechercher les expressions de confiance et de méfiance exprimées dans les messages, de rechercher les destinataires de ces messages et d'utiliser ces informations pour en déduire la réputation des utilisateurs. Nous proposons une nouvelle mesure de réputation qui permet de pondérer le score de chaque réponse selon la réputation de son auteur. Des évaluations automatiques et manuelles ont démontré l'efficacité de l'approche. Dans le deuxième axe, nous nous sommes focalisés sur l'extraction de sentiments (polarité et émotion). Pour cela, dans un premier temps, nous avons commencé par construire un lexique de sentiments et d'émotions pour le Français que nous appelons FEEL (French Expanded Emotion Lexicon). Ce lexique est construit de manière semi-automatique en traduisant et en étendant son homologue Anglais NRC EmoLex. Nous avons ensuite comparé FEEL avec les lexiques Français de la littérature sur des benchmarks de référence. Les résultats ont montré que FEEL permet d'améliorer la classification des textes Français selon leurs polarités et émotions. Dans un deuxième temps, nous avons proposé d'évaluer de manière assez exhaustive différentes méthodes et ressources pour la classification de sentiments en Français. Les expérimentations menées ont permis de déterminer les caractéristiques utiles dans la classification de sentiments pour différents types de textes. Les systèmes appris se sont montrés particulièrement efficaces sur des benchmarks de référence. De manière générale, ces travaux ont ouvert des perspectives prometteuses sur diverses tâches d'analyse des réseaux sociaux pour la langue française incluant : (i) combiner plusieurs sources pour transférer la connaissance sur les utilisateurs des réseaux sociaux ; 
Les méthodes à noyaux sont connues pour être efficaces pour l'analyse d'objets complexes en les plongeant implicitement dans un espace de caractéristiques (feature Ce travail propose une méthode d'estimation de la pré-image pour rendre interprétable les méthodes d'analyse de séries temporelles à base de noyaux. Dans la première étape, une fonction de déformation temporelle, supervisée par des contraintes de distances, est définie pour plonger les séries dans un espace métrique où des analyses pratiques peuvent être menées. Dans la deuxième étape, l'estimation de la pré-image des séries temporelles est obtenue par l'apprentissage d'une transformation linéaire (ou non linéaire) assurant une isométrie locale entre le nouvel espace métrique des séries et l'espace des caractéristiques. La méthode proposée est comparée aux méthodes de l'état de l'art au travers de trois tâches principales requérant l'estimation de la pré-image : 1) le centrage des séries temporelles, 2) la reconstruction et le débruitage des séries temporelles et 3) l'apprentissage de représentations pour des séries temporelles.
Ensuite, tout en introduisant les jeux de données qui ont aussi bien servi lors des études empiriques que motivé les choix de modélisation, nous essayons de donner des informations intéressantes sur l'état du marché des couvertures de défaillance peu connu du grand public sinon pour son rôle lors de la crise financière mondiale de 2007-2008. De nouvelles distances entre séries temporelles financières prenant mieux en compte leur nature stochastique et pouvant être mis à profit dans les méthodes de partitionnement automatique existantes sont proposées. Nous étudions empiriquement leur impact sur les résultats. Les résultats de ces études peuvent être consultés sur www.datagrapple.com.
Les compétences linguistiques sont fondamentales à l'exercice du métier de traducteur. La connaissance de la culture d'origine et d'arrivée est elle aussi d'une importance primordiale pour transmettre la richesse d'un ouvrage. La question de la place du culturel dans la traduction est abordée par les chercheurs en traductologie depuis une trentaine d'années, et les critères définis sont de plus en plus précis. Cependant, le couple spécifique que constitue le polonais vers le français n'est encore que peu étudié. Nous nous interrogeons donc sur l'expression des connaissances culturelles et contextuelles dans la traduction lors du passage du polonais vers le français. Pour ce faire, nous nous appuyons sur un corpus issu majoritairement des littératures de grande diffusion : notre questionnement porte sur la place de la culture, tant individuelle que nationale, quand l'objectif premier du texte est autre que culturel, ainsi que sur l'importance apportée à sa traduction. Notre réflexion s'appuie sur les oeuvres d'un grand auteur de fantasy, Andrzej Sapkowski, Polonais à la renommée mondiale,et aux traductions ver s le français de ses oeuvres. Nos observations sont par ailleurs complétées par une analyse de la même problématique dans le polar et le roman historique. La question que nous posons est la suivante : dans des textes polonais dont l'objectif premier n'est pas la transmission de la culture, mais le divertissement du public, quels types de connaissances culturelles et contextuelles sont présents, quelle place occupent-ils et comment sont-ils transmis en français ? Nous nous demanderons également quelle est l'incidence s'ils sont occultés. Afin de répondre à ces interrogations, nous proposons une classification des références culturelles et contextuelles qui correspondent à une typologie possible du culturème dans la traduction polono-française, applicable à d'autres littératures que celles de grande diffusion. Pour chacune des catégories et sous-catégories distinguées, nous analysons des exemples précis d'application dans notre corpus. Ceci nous permet de mettre à jour les pratiques les plus courantes dans la traduction des littératures de grande diffusion, et peut-être d'envisager d'autres approches.
Cette etude, portant sur l'analyse litteraire assistee par ordinateur, se situe au carrefour de trois differentes disciplines : la litterature, la linguistique et l'informatique.
Presque tous les internautes sont susceptibles d'être victimes de clickbait, supposant à tort qu'il s'agit d'informations légitimes. Un type important de clickbait se présente sous la forme de spam et de publicités qui sont utilisés pour rediriger les utilisateurs vers des sites web. Un autre type de "clickbait" est conçu pour faire la une des journaux et rediriger les lecteurs vers leurs sites en ligne, mais ces nouvelles sensationnelles peuvent être trompeuses. Dans cette thèse, on propose deux approches innovantes pour explorer le clickbait généré par les médias d'information dans les médias sociaux.
La recherche d'images basée sur le contenu visuel est un domaine très actif de la vision par ordinateur, car le nombre de bases d'images disponibles ne cesse d'augmenter. L'objectif de ce type d'approche est de retourner les images les plus proches d'une requête donnée en terme de contenu visuel. Notre travail s'inscrit dans un contexte applicatif spécifique qui consiste à indexer des petites bases d'images expertes sur lesquelles nous n'avons aucune connaissance a priori. L'une de nos contributions pour palier ce problème consiste à choisir un ensemble de descripteurs visuels et de les placer en compétition directe. Nous utilisons deux stratégies pour combiner ces caractéristiques : la première, est pyschovisuelle, et la seconde, est statistique. Dans ce contexte, nous proposons une approche adaptative non supervisée, basée sur les sacs de mots et phrases visuels, dont le principe est de sélectionner les caractéristiques pertinentes pour chaque point d'intérêt dans le but de renforcer la représentation de l'image. Les tests effectués montrent l'intérêt d'utiliser ce type de méthodes malgré la domination des méthodes basées réseaux de neurones convolutifs dans la littérature. Nous proposons également une étude, ainsi que les résultats de nos premiers tests concernant le renforcement de la recherche en utilisant des méthodes semi-interactives basées sur l'expertise de l'utilisateur.
Les progrès des technologies informatiques et l'augmentation continue des capacités de stockage ont permis de disposer de masses de données de trés grandes tailles et de grandes dimensions. Le volume et la nature même des données font qu'il est de plus en plus nécessaire de développer de nouvelles méthodes capables de traiter, résumer et d'extraire l'information contenue dans de tels types de données. D'un point de vue extraction des connaissances, la compréhension de la structure des grandes masses de données est d'une importance capitale dans l'apprentissage artificiel et la fouille de données. En outre, contrairement à l'apprentissage supervisé, l'apprentissage non supervisé peut fournir des outils pour l'analyse de ces ensembles de données en absence de groupes (classes). Dans cette thèse, nous nous concentrons sur des méthodes fondamentales en apprentissage non supervisé notamment les méthodes de réduction de la dimension, de classification simple (clustering) et de classification croisée (co-clustering). Notre contribution majeure est la proposition d'une nouvelle manière de traiter simultanément la classification et la réduction de dimension. L'idée principale s'appuie sur une fonction objective qui peut être décomposée en deux termes, le premier correspond à la réduction de la dimension des données, tandis que le second correspond à l'objectif du clustering et celui du co-clustering. Nous avons en outre proposé des versions régularisées de nos approches basées sur la régularisation du Laplacien afin de mieux préserver la structure géométrique des données. Enfin, nous avons aussi étudié comment intégrer des contraintes dans les Laplaciens utilisés pour la régularisation à la fois dans l'espace des objets et l'espace des variables. De cette façon, nous montrons comment des connaissances a priori peuvent contribuer à l'amélioration de la qualité du co-clustering.
Cette thèse de doctorat aborde les problématiques de l'estimation de confiance pour la traduction automatique, et de la traduction automatique statistique de la parole spontanée à grand vocabulaire. Je propose une évaluation des performances des différentes méthodes évoquées, présente les résultats obtenus lors d'une campagne d'évaluation internationale et propose une application à la post-édition par des experts de documents traduits automatiquement. J'aborde ensuite le problème de la traduction automatique de la parole. Après avoir passé en revue les spécificités du medium oral et les défis particuliers qu'il soulève, je propose des méthodes originales pour y répondre, utilisant notamment les réseaux de confusion phonétiques, les mesures de confiances et des techniques de segmentation de la parole.
Le déploiement massif de ressources d'énergie renouvelable distribuée (RED) représente une opportunité majeur pour atteindre les objectifs de réduction des émissions de carbone en Europe, mais aussi dans le monde entier. Visant à mobiliser des capitaux publics et privés, plusieurs plans ont été développés pour placer les clients finaux au cœur de la transition énergétique, dans l'espoir d'accélérer l'adoption de l'énergie verte en augmentant son attractivité et sa rentabilité. Pour ce faire, nous concevons un cadre permettant de concevoir et de comparer divers paradigmes « d'investissements partagés et d'échanges monétisés locaux de l'énergie » , dont le potentiel de « gains » se traduit par une incitation forte à leur mise en œuvre. Dans le cadre d'échanges monétisés locaux d'énergie, nous étudions les interactions entre prosommateurs (consommateurs avec capacité de production et éventuellement de stockage) situés dans le même réseau Basse Tension, éventuellement derrière le même départ. Dans nos systèmes, ces prosommateurs seront toujours connectés au réseau électrique principal et ils auront la possibilité, comme ils le font aujourd'hui, d'acheter et de vendre à un opérateur de services de distribution d'électricité, suivant une politique tarifaire connue à l'avance (un taux forfaitaire ou un temps d'utilisation, pour exemple). Dans la première partie de la thèse, nous étudions des modèles concurrentiels dans lesquels les prosommateurs vendent leur surplus à leurs voisins via un marché local d'énergie. Nous analysons différents types de marchés et donc différentes stratégies que les acteurs pourraient utiliser pour participer à ces marchés, ainsi que leur impact sur le réseau électrique et sur le gestionnaire du réseau de distribution. Dans la deuxième partie de la thèse, nous explorons les incitations qui peuvent être mises en œuvre par la coopération. À cet égard, nous utilisons la théorie des jeux coopératifs pour modéliser l'investissement partagé dans l'acquisition de dispositifs de stockage énergie et de panneaux photovoltaïques (PV) par un groupe de prosommateurs.
Cette thèse, effectuée dans le cadre d'un contrat CIFRE avec la société OctopusMind, est centrée sur le développement d'un outillage informatique dédié et optimisé pour l'assistance à l'exploitation de la base d'appels d'offres, dans une finalité de veille stratégique. Elle contient plus de deux millions de documents traduits dans 24 langues publiées durant les 9 dernières années. Le deuxième chapitre concerne une étude sur les questions de vectorisation de mots, phrases et documents susceptibles de capturer au mieux la sémantique selon différentes échelles. Nous avons proposé deux approches : la première est basée sur une combinaison entre word2vec et LSA. La deuxième est basée sur une architecture neuronale originale basée sur des réseaux d'attention convolutionnels à deux niveaux. Ces vectorisations sont exploitées à titre de validation sur des tâches de classification et de clustering de textes. La fin de ce chapitre concerne la mise en production dans l'environnement logiciel d'OctopusMind des différentes solutions, notamment l'extraction d'informations, le système de recommandation, ainsi que la combinaison de ces différents modules pour résoudre des problèmes plus complexes
L'opérationnalisation des terminologies à des fins de traitement de l'information requière une représentation computationnelle du système conceptuel. La théorie du concept sur laquelle se fonde la Terminologie ISO ne permet pas aujourd'hui une telle représentation informatique [Roche TKE 2012]. Même si les normes ISO sur la Terminologie n'ont pas pour objectif l'opérationnalisation de terminologies mais la communication entre humains, elles doivent pouvoir – elles le précisent – servir à la modélisation des informations et des données [ISO 704 : 2009]. Les résultats de disciplines telles que l'ingénierie des connaissances ont permis de mettre en évidence la nécessité de disposer d'une théorie du concept qui puisse donner lieu à une représentation informatique. Dans ce cadre, les ontologies, issues de l'ingénierie des connaissances, constituent une des perspectives les plus intéressantes pour la modélisation du système conceptuel d'une terminologie [Roche 2015, Handbook of Terminology].
Cette thèse présente de nouveaux outils informatiques pour quantifier les déformations et le mouvement de structures anatomiques à partir d'images médicales dans le cadre d'une grande variété d'applications cliniques. Des outils génériques de recalage déformable sont présentés qui permettent l'analyse de la déformation de tissus anatomiques pour améliorer le diagnostic, le pronostic et la thérapie. Ces outils combinent des méthodes avancées d'analyse d'images médicales avec des méthodes d'apprentissage automatique performantes. Dans un premier temps, nous nous concentrons sur les problèmes de recalages inter-sujets difficiles. Dans un second temps, nous développons un modèle de déformation difféomorphe qui permet un recalage multi-échelle précis et une analyse de déformation en apprenant une représentation de faible dimension des déformations intra-sujet. La méthode non supervisée utilise un modèle de variable latente sous la forme d'un autoencodeur variationnel conditionnel (CVAE) pour apprendre une représentation probabiliste des déformations qui est utile pour la simulation, la classification et la comparaison des déformations. Troisièmement, nous proposons un modèle de mouvement probabiliste dérivé de séquences d'images d'organes en mouvement. Ce modèle génératif décrit le mouvement dans un espace latent structuré, la matrice de mouvement, qui permet le suivi cohérent des structures ainsi que l'analyse du mouvement. Ainsi cette approche permet la simulation et l'interpolation de modèles de mouvement réalistes conduisant à une acquisition et une augmentation des données plus rapides. Enfin, nous démontrons l'intérêt des outils développés dans une application clinique où le modèle de mouvement est utilisé pour le pronostic de maladies et la planification de thérapies. Il est démontré que le risque de survie des patients souffrant d'insuffisance cardiaque peut être prédit à partir de la matrice de mouvement discriminant avec une précision supérieure par rapport aux facteurs de risque classiques dérivés de l'image.
La recherche d'experts consiste en l'identification d'un ensemble d'individus que l'on considère comme experts d'une thématique particulière. Il s'agit d'une problématique essentielle dans le milieu académique. En effet, il est constamment nécessaire d'identifier des chercheurs appropriés lors de la constitution de comités de lecture ou d'évaluation de projets de recherche, par exemple. De même, il est particulièrement utile d'identifier automatiquement les experts d'un domaine de recherche à partir de la littérature scientifique. Nous proposons une approche de découverte et d'enrichissement de connaissances basée sur une annotation sémantique des articles scientifiques, sur leur représentation sous forme de réseaux de collaboration scientifique et leur fouille à l'aide d'une méthode d'abstraction de graphe. Cette méthode permet de se focaliser sur les zones denses des réseaux et de découvrir des experts et leurs expertises associées à l'aide de contraintes de connectivité. Ces dernières permettent de prendre en compte une validation par les pairs, matérialisée par la densité des relations de collaboration scientifique que les individus entretiennent entre eux. Nous expérimentons notre approche sur un corpus de publications scientifiques, proposons une méthode d'évaluation originale de nos résultats et comparons nos performances aux méthodes de recherche d'experts implémentées dans le cadre d'évaluation LT ExpertFinder. Nous obtenons des performances supérieures à l'état de l'art et découvrons que les indicateurs d'expertise les plus déterminants sont la rédaction d'articles fortement cités mais également la capacité à citer la littérature scientifique appropriée.
Le succès de l'apprentissage profond de ces dernières années dépend en grande partie de grands jeux de données annotées, extrêmement couteux à collecter et disponible en quantité limitée. Les algorithmes d'apprentissage auto-supervisés sont devenus populaires ces 10 dernières années dans les communautés de la vision artificielle, l'apprentissage machine, et du traitement du langage naturel. Ils bénéficient des avancées en apprentissage supervisé mais nécessite beaucoup moins d'intervention humaine, car la supervision provient des données elles-mêmes (par exemple, prédire les frames manquantes d'une vidéo) sans annotation manuelle supplémentaire. Une alternative prometteuse est de considérer une famille de techniques qui mesurent les contradictions entre entrées prédites et réelles en utilisant une fonction paramétrée par une variable latente, régularisée pour éviter les solutions triviales. En particulier, le modèle implicite correspondant permet de réaliser des prédictions multi-modales qui reflètent les aspects non déterministes du monde réel. Cette thèse développera de nouvelles techniques adaptant ce cadre général à la vision artificielle et les appliquera à des tâches telles que l'apprentissage auto-supervisé de fonctions de photocohérence pour la stéréo multi-vues et de modèles d'objets pour la reconnaissance visuelle.
Alors que les méthodes de reconnaissance visuelle sont de plus en plus évoluées, la communauté scientifique s'intéresse désormais à des systèmes aux capacités de raisonnement plus poussées. Dans cette thèse, nous nous intéressons au Visual Question Answering (VQA), qui consiste en la conception de systèmes capables de répondre à une question portant sur une image. Ce problème difficile est habituellement abordé par des techniques d'apprentissage profond. Dans la première partie de cette thèse, nous développons des stratégies de fusion multimodales permettant de modéliser des interactions entre les représentations d'image et de question. Nous explorons des techniques de fusion bilinéaire, et assurons l'expressivité et la simplicité des modèles en utilisant des techniques de factorisation tensorielle. Dans la seconde partie, on s'intéresse au raisonnement visuel qui encapsule ces fusions. Après avoir présenté les schémas classiques d'attention visuelle, nous proposons une architecture plus avancée qui considère les objets ainsi que leurs relations mutuelles. Tous les modèles sont expérimentalement évalués sur des jeux de données standards et obtiennent des résultats compétitifs avec ceux de la littérature.
L'évaluation de requêtes sur des données probabilistes(probabilistic query evaluation, ou PQE) est généralement très coûteuse en ressources et ce même à requête fixée. Bien que certaines restrictions sur les requêtes et les données aient été proposées pour en diminuer la complexité, les résultats existants ne s'appliquent pas à la complexité combinée, c'est-à-dire quand la requête n'est pas fixe. Ma thèse s'intéresse à la question de déterminer pour quelles requêtes et données l'évaluation probabiliste est faisable en complexité combinée. La première contribution de cette thèse est d'étudier PQE pour des requêtes conjonctives sur des schémas d'arité 2. Nous imposons que les requêtes et les données aient la forme d'arbres et montrons l'importance de diverses caractéristiques telles que la présence d'étiquettes sur les arêtes, les bifurcations ou la connectivité. Les restrictions imposées dans ce cadre sont assez sévères, mais la deuxième contribution de cette thèse montre que si l'on est prêts à augmenter la complexité en la requête, alors il devient possible d'évaluer un langage de requête plus expressif sur des données plus générales. Plus précisément, nous montrons que l'évaluation probabiliste d'un fragment particulier de Datalog sur des données de largeur d'arbre bornée peut s'effectuer en temps linéaire en les données et doublement exponentiel en la requête. Ce résultat est prouvé en utilisant des techniques d'automates d'arbres et de compilation de connaissances. La troisième contribution de ce travail est de montrer les limites de certaines de ces techniques, en prouvant des bornes inférieures générales sur la taille de formalismes de représentation utilisés en compilation de connaissances et en théorie des automates.
L'enjeu de la normalisation du reporting RSE occupe une place croissante dans la communication des entreprises à l'égard de leurs parties prenantes. La norme GRI constitue, à l'heure actuelle, le référentiel de reporting RSE le plus utilisé à l'échelle internationale pour la rédaction des rapports RSE. Le corpus G3C constitué est représentatif du genre discursif « 
De nombreuses données médicales au format électronique sont produites dans les établissements de santé au moment du soin. Ces données sont riches en information et présentent un intérêt majeur pour la recherche médicale. Il est cependant très difficile pour un chercheur de réutiliser les données collectées durant le soin. En 2015, le CHU de Bordeaux a mis en place un entrepôt de données clinique basé sur la solution i2b2 pour faciliter la réutilisation des données. Même intégrées dans un entrepôt, de nombreux verrous restent encore à résoudre comme la présence de données textuelles et une hétérogénéité sémantique des différentes sources de données.
L'architecture système cherche à se distinguer de son domaine d'origine, l'ingénierie système, en devenant un domaine émergent. Loin d'être reconnue en tant que science ou discipline à proprement parler, sa pratique est de plus en plus répandue de nos jours. Cependant, cette pratique reste encore peu formalisée et peu enseignée, faute d'un corpus de connaissances, de techniques ou de démarches établi et accessible. Notre thèse contribue à combler ce manque en proposant un paradigme de la conception architecturale des systèmes artificiels complexes. Ce dernier est construit en se basant sur des paradigmes existants, en les combinant, puis en les complétant. Il vise à doter l'architecte de systèmes artificiels complexes d'un cadre opérant, voire performatif. Il se traduit par une structuration de la démarche de conception en quatre niveaux. Un niveau dit archétypal condense les grands principes de toute démarche de conception architecturale de systèmes artificiels complexes. Ces principes sont dérivés de diverses démarches déjà appliquées, principalement à la conception de systèmes ou de produits, mais également à la conception architecturale de bâtiments. Cette vision impacte directement la nature des artéfacts sur lesquels il travaille. Nous proposons ensuite d'agréger ces artéfacts en des modèles, reflétant soit sa perception du présent, soit son élaboration des futurs, évoluant suivant des processus identifiés. Un niveau dit particulier a pour objectif de permettre la narration d'une conception particulière. Nous proposons pour cela une notation de la conception.
De nombreux efforts ont été faits ces dernières années pour faciliter la gestion et la représentation des entités culturelles. Toutefois, il existe encore un grand nombre de systèmes souvent isolés et encore utilisés dans les institutions culturelles reposant sur des modèles non sémantiques qui rendent difficile la validation et l'enrichissement des données. Cette thèse a pour but de proposer de nouvelles solutions pour améliorer la représentation et l'enrichissement sémantique de données culturelles en utilisant les principes du Web Sémantique. Toutefois, la qualité d'une telle transformation est cruciale et c'est pourquoi des améliorations doivent être faites au niveau de la configuration et de l'évaluation d'un tel processus. Cependant, l'agrégation d'informations depuis des sources hétérogènes implique des étapes d'alignement à la fois au niveau du schéma et au niveau des entités
Les collections de manuscrits sur feuilles de palmier sont devenues une partie intégrante de la culture et de la vie des peuples de l'Asie du Sud-Est. Avec l'augmentation des projets de numérisation des documents patrimoniaux à travers le monde, les collections de manuscrits sur feuilles de palmier ont finalement attiré l'attention des chercheurs en analyse d'images de documents (AID). Les travaux de recherche menés dans le cadre de cette thèse ont porté sur les manuscrits d'Indonésie, et en particulier sur les manuscrits de Bali. Nos travaux visent à proposer des méthodes d'analyse pour les manuscrits sur feuilles de palmier. En effet, ces collections offrent de nouveaux défis car elles utilisent, d'une part, un support spécifique : les feuilles de palmier, et d'autre part, un langage et un script qui n'ont jamais été analysés auparavant. Ces systèmes rendront ces manuscrits plus accessibles, lisibles et compréhensibles à un public plus large ainsi que pour les chercheurs et les étudiants du monde entier. Cette thèse a permis de développer un système d'AID pour les images de documents sur feuilles de palmier, comprenant plusieurs tâches de traitement d'images : numérisation du document, construction de la vérité terrain, binarisation, segmentation des lignes de texte et des glyphes, la reconnaissance des glyphes et des mots, translittération et l'indexation de document. Nous avons également développé un système de reconnaissance des glyphes et un système de translittération automatique des manuscrits balinais. Cette thèse propose un schéma complet de reconnaissance de glyphes spatialement catégorisé pour la translittération des manuscrits balinais sur feuilles de palmier. Le schéma proposé comprend six tâches : la segmentation de lignes de texte et de glyphes, un processus de classification de glyphes, la détection de la position spatiale pour la catégorisation des glyphes, une reconnaissance globale et catégorisée des glyphes, la sélection des glyphes et la translittération basée sur des règles phonologiques. La translittération automatique de l'écriture balinaise nécessite de mettre en œuvre des mécanismes de représentation des connaissances et des règles phonologiques.
Dans cette thèse, j'étudie la manière dont les ressources lexicales basées sur l'organisation de la connaissance lexicale dans des classes qui partagent des propriétés communes (syntactiques, sémantiques, etc.) permettent le traitement automatique de la langue naturelle et en particulier la reconnaissance symbolique d'implications textuelles. Tout d'abord, je présente une approche robuste et à large couverture sur la reconnaissance de paraphrases verbales lexico-structurelle basée sur la classification de verbes anglais par Levin (1993). Puis, je montre qu'en étendant le cadre proposé par Levin pour traiter les modèles d'inférence généraux, on obtient une classification d'adjectifs anglais qui, comparée à des approches antérieures, propose une caractérisation sémantique à grain plus fin de leurs propriétés déductives. De plus, je développe un cadre sémantique compositionnel pour assigner à des adjectifs une représentation sémantique sur la base d'une approche ontologiquement variée (Hobbs, 1985) et qui permet ainsi l'inférence de premier ordre pour tous les types d'adjectifs, y compris les adjectifs extensionnels. Enfin, je présente un corpus de test pour l'inférence basée sur les adjectifs que j'ai développée comme ressource pour l'évaluation de systèmes de traitement automatique de l'inférence de la langue naturelle.
Avec l'augmentation exponentielle de nombre d'images disponibles sur Internet, le besoin en outils efficaces d'indexation et de recherche d'images est devenu important. Dans cette thèse, nous nous baserons sur le contenu visuel des images comme source principale d'informations pour leur représentation. Dans un premier temps, nous améliorons l'approche des sacs de mots visuels en caractérisant la constitution spatio-colorimétrique d'une image par le biais d'un mélange de n Gaussiennes dans l'espace de caractéristiques. Cela permet de proposer un nouveau descripteur de contour qui joue un rôle complémentaire avec le descripteur SURF. Dans un deuxième temps, nous introduisons un nouveau modèle probabiliste basé sur les catégories : le modèle MSSA (Multilayer Semantic Significance Analysis ou Analyse multi-niveaux de la pertinence sémantique) dans le but d'étudier la sémantique des mots visuels construits. Ce modèle permet de construire des mots visuels sémantiquement cohérents (SSVW - Semantically Significant Visual Word). Enfin, nous proposons un nouveau schéma de pondération spatiale ainsi qu'un classifieur multi-classes basé sur un vote. Nos résultats expérimentaux extensifs démontrent que la représentation visuelle proposée permet d'atteindre de meilleures performances comparativement aux représentations traditionnelles utilisées dans le domaine de la recherche, la classification et de la reconnaissance d'objets.
Notre thèse se situe dans le contexte du projet ANR GEONTO qui porte sur la constitution, l'alignement, la comparaison et l'exploitation d'ontologies géographiques hétérogènes. Dans ce contexte, notre objectif est d'extraire automatiquement des termes topographiques à partir des récits de voyage afin d'enrichir une ontologie géographique initialement conçue par l'IGN. La méthode proposée permet de repérer et d'extraire des termes à connotation topographiques contenus dans un texte. Notre méthode est basée sur le repérage automatique de certaines relations linguistiques afin d'annoter ces termes. Sa mise en œuvre s'appuie sur le principe des relations n-aires et passe par l'utilisation de méthodes ou de techniques de TAL (Traitement Automatique de la Langue). Il s'agit de relations n-aires entre les termes à extraire et d'autres éléments du textes qui peuvent être repérés à l'aide de ressources externes prédéfinies, telles que des lexiques spécifiques : les verbes de récit de voyage (verbes de déplacement, verbes de perceptions, et verbes topographiques), les pré-positions (prépositions de lieu, adverbes, adjectifs), les noms toponymiques, des thésaurus génériques, des ontologies de domaine (ici l'ontologie géographique initialement conçue par l'IGN). Le point fort de notre approche est que la méthode proposée permet d'extraire non seulement des termes rattachés directement aux noms toponymiques mais également dans des structures de phrase où d'autres termes s'intercalent. L'expérimentation sur un corpus comportant 12 récits de voyage (2419 pages, fournit par la médiathèque de Pau) a montré que notre méthode est robuste. En résultat, elle a permis d'extraire 2173 termes distincts dont 1191 termes valides, soit une précision de 0,55. Cela démontre que l'utilisation des relations proposées est plus efficace que celle des couples (termes, nom toponymique)(qui donne 733 termes distincts valides avec une précision de 0,38). Notre méthode peut également être utilisée pour d'autres applications telles que la reconnaissance des entités nommées géographiques, l'indexation spatiale des documents textuels.
L'intégration des technologies numériques dans l'éducation (TICE) est souvent décrite comme difficile. Parce que leur déploiement est généralisé et leur utilisation rendue obligatoire par l'institution, les environnements numériques de travail (ENT) constituent un outil à part dans le champ des TICE. Ce travail propose d'investiguer comment cet outil a intégré la pensée professionnelle enseignante à partir d'une approche reposant sur la théorie des représentations sociales. Nous proposons par une approche lexicométrique d'identifier les thématiques transversales à ces corpus et celles qui sont spécifiques de chacun de ces types de discours. Le second temps de ce travail repose sur une enquête par questionnaire menée auprès de 625 enseignants du secondaire de l'académie de Toulouse. À partir de tests d'évocations hiérarchisées, nous étudions les contenus des représentations professionnelles de quatre objets : l'ENT, le métier d'enseignant, la notion d'information et la notion de communication. L'analyse de la structure des réponses, sur chacun de ces objets puis dans les relations entre les objets, met notamment en évidence la particularité de leur lecture par les enseignants au regard du discours social identifié dans la partie précédente. Le système complexe formé par ces quatre objets permet d'observer que les représentations du métier d'enseignant et de l'ENT ont peu de contenus communs, mais qu'elles partagent toutes les deux de nombreux éléments avec les représentations de la communication et de l'information. Ces résultats dessinent également deux grandes tendances dans la façon de décrire les objets de représentation investigués : une partie de la population semble systématiquement privilégier des éléments fonctionnels pour décrire chacun des objets alors qu'une autre partie de la population associe préférentiellement des éléments évaluatifs.
Une carte marine est un type de carte utilisé pour décrire la morphologie du fond marin et du littoral adjacent. Un de ses principaux objectifs est de garantir la sécurité de la navigation maritime. En conséquence, la construction d'une carte marine est contrainte par des règles très précises. Le cartographe doit choisir et mettre en évidence les formes du relief sous-marin en fonction de leur intérêt pour la navigation. Au sein d'un processus automatisé, le système doit être en mesure d'identifier et de classifier ces formes de relief à partir d'un modèle de terrain. Un relief sous-marin est une individuation subjective d'une partie du fond océanique. La reconnaissance de la morphologie du fond sous-marin est une tâche difficile, car les définitions des formes de relief reposent généralement sur une description qualitative et floue. Obtenir la reconnaissance automatique des formes de relief nécessite donc une définition formelle des propriétés des reliefs et de leur modélisation. Cette terminologie a été utilisée ici comme point de départ pour la classification automatique des formes de relief sous-marines d'un modèle numérique de terrain. Afin d'intégrer les connaissances sur le relief sous-marin et sa représentation sur une carte nautique, cette recherche vise à définir des ontologies du relief sous-marin et des cartes marines. Les ontologies sont ensuite utilisées à des fins de généralisation de carte marine. Dans la première partie de la recherche, une ontologie est définie afin d'organiser la connaissance géographique et cartographique pour la représentation du relief sous-marin et la généralisation des cartes marines. Tout d'abord, une ontologie de domaine du relief sous-marin présente les différents concepts de formes de relief sous-marines avec leurs propriétés géométriques et topologiques. Cette ontologie est requise pour la classification des formes de relief. Deuxièmement, une ontologie de représentation est présentée, qui décrit la façon dont les entités bathymétriques sont représentées sur la carte. Troisièmement, une ontologie du processus de généralisation définit les contraintes et les opérations usitées pour la généralisation de carte marine. Dans la deuxième partie de la recherche, un processus de généralisation fondé sur l'ontologie est conçu en s'appuyant sur un système multi-agents (SMA). Quatre types d'agents (isobathe, sonde, forme de relief et groupe de formes de relief) sont définis pour gérer les objets cartographiques sur la carte. Un modèle de base de données a été généré à partir de l'ontologie. Le système proposé classe automatiquement les formes de relief sous-marines extraites à partir de la bathymétrie, et évalue les contraintes cartographiques. Ensuite, les conflits de distance et de superficie sont évalués dans le SMA et des plans de généralisation sont proposés au cartographe.
L'objectif de cette thèse est de proposer un système de reconnaissance automatique des émotions (RAE) par analyse de la voix pour une application dans un contexte pédagogique d'orchestration de classe. Ce système s'appuie sur l'extraction de nouvelles caractéristiques, par démodulation en amplitude et en fréquence, de la voix ; considérée comme un signal multi-composantes modulé en amplitude et en fréquence (AM-FM), non-stationnaire et issue d'un système non-linéaire. Cette démodulation est basée sur l'utilisation conjointe de la décomposition en modes empiriques (EMD) et de l'opérateur d'énergie de Teager-Kaiser (TKEO). Dans ce système, le modèle discret (ou catégoriel) a été retenu pour représenter les six émotions de base (la tristesse, la colère, la joie, le dégoût, la peur et la surprise) et l'émotion dite neutre. La reconnaissance automatique a été optimisée par la recherche de la meilleure combinaison de caractéristiques, la sélection des plus pertinentes et par comparaison de différentes approches de classification. Deux bases de données émotionnelles de référence, en allemand et en espagnol, ont servi à entrainer et évaluer ce système. Une nouvelle base de données en Français, plus appropriée pour le contexte pédagogique a été construite, testée et validée.
L'évaluation de la gravité et la surveillance des maladies pulmonaires chroniques représentent deux challenges importants pour la prise en charge des patients et l'évaluation des traitements. La surveillance repose principalement sur les données fonctionnelles respiratoires mais l'évaluation morphologique reste un point essentiel pour le diagnostic et l'évaluation de sévérité. Une approche simple par seuillage adaptatif et une méthode plus sophistiquée de radiomique sont évaluées Dans la seconde partie, nous évaluons une méthode d'apprentissage profond pour contourer automatiquement l'atteinte fibrosante de la sclérodermie en scanner. Dans la dernière partie, nous démontrons que l'étude de la déformation pulmonaire en IRM entre inspiration et expiration peut être utilisée pour repérer les régions pulmonaires en transformation fibreuse, moins déformables au cours de la respiration, et qu'en scanner, l'évaluation de la déformation entre des examens successifs de suivi peut diagnostiquer l'aggravation fibreuse chez les patients sclérodermiques.
Dans ce travail, nous nous intéressons aux mouvements de sourcils dans l'interaction en face- à-face en tant que ressource utilisée par les participants pour signaler un problème de compréhension. Notre objectif est de décrire la séquence interactionnelle dans laquelle apparait ce mouvement et les trajectoires interactionnelles dont use le locuteur pour résoudre le trouble de la compréhension qui a surgi. Nous nous appuyons sur trois corpus présentant différents degrés de spontanéité et diverses activités interactionnelles. Après avoir identifié et annoté les mouvements de sourcils (haussement et froncement) et les séquences présentant un problème de compréhension (Weigand, 1999 ; Antaki, 2012), nous avons mené une analyse séquentielle sur une soixantaine d'extraits. Cette recherche met en lumière le rôle des mouvements de sourcils en tant que ressource qui participe à rendre mutuellement reconnaissable un problème de compréhension, mais également en tant qu'indice de désalignement et de réalignement lors de ces dîtes séquences.
Ce travail s'inscrit dans le cadre du traitement automatique des langues naturelles, et s'interesse au probleme de la coherence dans la representation des connaissances issues de textes. Deux resultats ont ete degages : soit la negation est source de coherence dans le sens ou elle est definie comme operation au sein des domaines &lt ; &lt ; notionnels &gt ; &gt ; introduits par le discours. La structure de &lt ; &lt ; domaine &gt ; &gt ; facilite l'interpretation des enonces negatifs parce qu'elle permet de les exprimer de facon positive. Soit, la negation est source d'incoherence et elle peut etre un facteur de construction d'un nouvel espace ou monde. Nous avons propose une formalisation des deux types de negations dans le formalisme oriente-objet, dont les fondements theoriques sont les systemes logiques de lesniewski. La notion de domaine a ete introduite dans les deux univers (intensionnel et extensionnel) du modele, en termes du calcul des noms et de la mereologie. Cette formalisation peut nous permettre de rendre compte divers mecanismes d'inferences de la negation.
La majorité des méthodes de classification de textes utilisent le paradigme du sac de mots pour représenter les textes. Pourtant cette technique pose différents problèmes sémantiques : certains mots sont polysémiques, d'autres peuvent être des synonymes et être malgré tout différenciés, d'autres encore sont liés sémantiquement sans que cela soit pris en compte et enfin, certains mots perdent leur sens s'ils sont extraits de leur groupe nominal. Pour pallier ces problèmes, certaines méthodes ne représentent plus les textes par des mots mais par des concepts extraits d'une ontologie de domaine, intégrant ainsi la notion de sens au modèle. Afin d'améliorer les performances de ces modèles, plusieurs méthodes ont été proposées pour enrichir les caractéristiques des textes à l'aide de nouveaux concepts extraits de bases de connaissances. A l'aide de l'algorithme du classifieur naïf Bayésien, j'ai testé et comparé mes contributions sur le corpus de textes labéllisés Ohsumed et l'ontologie de domaine Disease Ontology. Les résultats satisfaisants m'ont amené à analyser plus précisément le rôle des relations sémantiques dans l'enrichissement des modèles. Ces nouveaux travaux ont été le sujet d'une seconde expérience où il est question d'évaluer les apports des relations hiérarchiques d'hyperonymie et d'hyponymie.
Face à cette évolution technologique vertigineuse, l'utilisation des dispositifs de l'Internet des Objets (IdO), les capteurs, et les réseaux sociaux, d'énormes flux de données IdO sont générées quotidiennement de différentes applications pourront être transformées en connaissances à travers l'apprentissage automatique. En pratique, de multiples problèmes se posent afin d'extraire des connaissances utiles de ces flux qui doivent être gérés et traités efficacement. Dans ce contexte, cette thèse vise à améliorer les performances (en termes de mémoire et de temps) des algorithmes de l'apprentissage supervisé, principalement la classification à partir de flux de données en évolution. En plus de leur nature infinie, la dimensionnalité élevée et croissante de ces flux données dans certains domaines rendent la tâche de classification plus difficile. La deuxième partie de la thèse détaille nos contributions en classification pour les flux de données. Il s'agit de nouvelles approches basées sur les techniques de réduction de données visant à réduire les ressources de calcul des classificateurs actuels, presque sans perte en précision. Pour traiter les flux de données de haute dimension efficacement, nous incorporons une étape de prétraitement qui consiste à réduire la dimension de chaque donnée (dès son arrivée) de manière incrémentale avant de passer à l'apprentissage. Dans ce contexte, nous présentons plusieurs approches basées sur : Bayesien naïf amélioré par les résumés minimalistes et hashing trick, k-NN qui utilise compressed sensing et UMAP, et l'utilisation d'ensembles d'apprentissage également.
Ces dernières années, des méthodes issues de l'optimisation combinatoire ont été appliquées avec succès pour résoudre des problèmes algorithmiques difficiles en Traitement Automatique des Langues (TAL). Nous suivons cette méthodologie dans le cadre de l'analyse syntaxique avec des Grammaires d'Arbres Adjoints Lexicalisés ;  Plus précisément, un problème d'analyse est d'abord réduit à un problème de sélection de sous-graphe. Ensuite nous formulons ce dernier sous forme de Programme Linéaire en Nombres Entiers. Beaucoup d'algorithmes ont été proposés pour ces formulations. Nous nous concentrons sur la Relaxation Lagrangienne qui a reçu beaucoup d'attention de la part de la communauté du TAL.
Les systèmes de fusion d'informations sont principalement composés, d'outils mathématiques permettant de réaliser la représentation et la combinaison des données. L'objectif de ces systèmes peut s'exprimer comme un problème de décision sur la vérité ou la vraisemblance d'une proposition étant donné une ou plusieurs informations issues de différentes sources. Les systèmes de fusion cherchent à exploiter au mieux les propriétés des sources de données en tenant compte de l'imperfection de l'information (imprécis, incomplet, ambigu, incertain, etc.) ainsi que l'aspect redondant, complémentaire et conflictuel des informations. Le système de fusion concerné par cette thèse a la capacité d'intégrer dans ses traitements de la connaissance experte. On le nomme système de fusion coopératif. Puisque ce système cherche à intégrer pleinement les experts dans son fonctionnement, il est important de mettre à disposition des utilisateurs des informations aidant à mieux comprendre la fusion réalisée. Une des grandes problématiques liées à ces systèmes de fusion d'informations porte sur l'évaluation de leurs performances. L'évaluation doit permettre d'améliorer la qualité de la fusion, d'améliorer l'interaction expert/système et d'aider à mieux ajuster les paramètres du système. En général, l'évaluation de tels systèmes est réalisée en fin de chaîne par une évaluation globale du résultat. Mais, celle-ci ne permet pas de savoir précisément l'endroit de la chaîne qui nécessite une intervention. Une autre difficulté réside dans le fait qu'une vérité terrain complète sur le résultat n'est pas toujours disponible, ce qui complique la tâche d'évaluation de performances de ce type de systèmes. Le contexte applicatif de ce travail est l'interprétation d'images tridimensionnelles (images tomographiques, images sismiques, images synthétiques,...). Dans ce contexte une évaluation locale des systèmes de fusion d'informations, a été mise en place. L'approche a montré son intérêt dans l'ajustement efficace des paramètres et dans la coopération avec l'expert.
De nos jours, il est très fréquent de représenter un système en termes de relations entre objets. Parmi les applications les plus courantes de telles données relationnelles, se situent les systèmes de recommandation (RS), qui traitent généralement des relations entre utilisateurs et items à recommander. Les modèles relationnels probabilistes (PRM) sont un bon choix pour la modélisation des dépendances probabilistes entre ces objets. Une tendance croissante dans les systèmes de recommandation est de rajouter une dimension spatiale à ces objets, que ce soient les utilisateurs, ou les items. Cette thèse porte sur l'intersection peu explorée de trois domaines connexes - modèles probabilistes relationnels (et comment apprendre les dépendances probabilistes entre attributs d'une base de données relationnelles), les données spatiales et les systèmes de recommandation. La première contribution de cette thèse porte sur le chevauchement des PRM et des systèmes de recommandation. Nous avons proposé un modèle de recommandation à base de PRM capable de faire des recommandations à partir des requêtes des utilisateurs, mais sans profils d'utilisateurs, traitant ainsi le problème du démarrage à froid. Notre deuxième contribution aborde le problème de l'intégration de l'information spatiale dans un PRM.
L'extraction automatique des intuitions et la construction de modèles computationnels à partir de connaissances sur des systèmes complexes repose largement sur le choix d'une représentation appropriée. Ce travail s'efforce de construire un cadre adapté pour la représentation de connaissances fragmentées sur des systèmes complexes et sa curation semi-automatisé. Un système de représentation des connaissances basé sur des hiérarchies de graphes liés à l'aide d'homomorphismes est proposé. Les graphes individuels représentent des fragments de connaissances distincts et les homomorphismes permettent de relier ces fragments. Nous nous concentrons sur la conception de mécanismes mathématiques, basés sur des approches algébriques de la réécriture de graphes, pour la transformation de graphes individuels dans des hiérarchies qui maintient des relations cohérentes entre eux. De tels mécanismes fournissent une piste d'audit transparente, ainsi qu'une infrastructure pour maintenir plusieurs versions des connaissances. La théorie développée est appliquée à la conception des schémas pour les bases de données orientée graphe qui fournissent des capacités de co-évolution schémas-données. Ensuite, cette théorie est utilisée dans la construction du cadre KAMI, qui permet la curation des connaissances sur la signalisation dans les cellules. KAMI propose des mécanismes pour une agrégation semi-automatisée de faits individuels sur les interactions protéine-protéine en corpus de connaissances, la réutilisation de ces connaissances pour l'instanciation de modèles de signalisation dans différents contextes cellulaires et la génération de modèles exécutables basés sur des règles.
Les graphes sont présents dans de nombreux domaines de recherche, que ce soit pour représenter des molécules, des réseaux sociaux ou des réseaux de transport. Il est composé de nœuds reliés entre eux par des liens, appelés arêtes. Récemment, les techniques d'apprentissage profond ont prouvé leur efficacité dans de nombreux domaines tels que le traitement de texte ou l'analyse d'images. Ce constat a motivé de nombreux travaux de recherche visant à généraliser les techniques d'apprentissage profond à l'analyse de graphes. Ainsi des algorithmes se basant notamment sur des réseaux de neurones et des convolutions ont été développés afin de répondre à des problématiques de classification de nœuds et de graphes. Ces représentations à différentes échelles, encodent des informations hierarchiques sur le graphe. En ce basant sur ces vectorialisations de graphes, nous proposons de nouvelles architectures afin de répondre à des tâches de classification de nœuds et classification de graphes.
Pour la recherche en santé publique, réutiliser les bases médicoadministratives est pertinent et ouvre de nouvelles perspectives Cette thèse porte sur l'utilisation conjointe de bases de données médicoadministratives et de connaissances biomédicales pour l'étude des trajectoires de soin. Cela recouvre à la fois (1) l'exploration et l'identification des trajectoires de soins pertinentes dans des flux volumineux au moyen de requêtes et (2) l'analyse des trajectoires retenues. Les technologies du Web Sémantique et les ontologies du Web des données ont permis d'explorer efficacement les données médicoadministratives, en identifiant dans des trajectoires de soins des interactions, ou encore des contre-indications. Nous avons également développé le package R queryMed afin de rendre plus accessible les ontologies médicales aux chercheurs en santé publique. Après avoir permis d'identifier les trajectoires intéressantes, les connaissances relatives aux nomenclatures médicales de ces bases de données ont permis d'enrichir des méthodes d'analyse de trajectoires de soins pour mieux prendre en compte leurs complexités. Cela s'est notamment traduit par l'intégration de similarités sémantiques entre concepts médicaux. Les technologies du Web Sémantique ont également été utilisées pour explorer les résultats obtenus.
Il s'agit d'étudier le discours onusien sur la crise nucléaire iranienne pendant les dix années entre 2005 et 2015. Une étude basée sur l'Analyse de Discours, est menée sur un corpus clos et prédéfini afin de discerner les différents procédés linguistiques et discursifs qui commandent le discours. Il s'agit également d'appréhender les enjeux et les origines juridiques et politiques de cette crise diplomatique. Notre défi majeur est ainsi de saisir le discours sous ses multiples dimensions, linguistiques, discursives, politiques et juridiques. Telles sont les interrogations auxquelles nous répondons dans ce travail. L'appréhension des incidences linguistiques et discussives se réalise à la lumière des données politiques et juridiques qui constituent un cadre interprétatif à l'analyse. L'objectif est d'identifier la construction d'une identité onusienne à travers des notions de valeurs, par des mécanismes discursifs.
La quantité de contenu généré par l'utilisateur sur le Web croît à un rythme rapide. Une grande partie de ce contenu est constituée des opinions et avis sur des produits et services. Vu leur impact, ces avis sont un facteur important dans les décisions concernant l'achat de ces produits ou services. Les utilisateurs ont tendance à faire confiance aux autres utilisateurs, surtout s'ils peuvent se comparer à ceux qui ont écrit les avis, ou, en d'autres termes, ils sont confiants de partager certaines caractéristiques. Par exemple, les familles préféreront voyager dans les endroits qui ont été recommandés par d'autres familles. Nous supposons que les avis qui contiennent des expériences vécues sont plus précieuses, puisque les expériences donnent aux avis un aspect plus subjective, permettant aux lecteurs de se projeter dans le contexte de l'écrivain. En prenant en compte cette hypothèse, dans cette thèse, nous visons à identifier, extraire et représenter les expériences vécues rapportées dans les avis des utilisateurs en hybridant les techniques d'extraction des connaissances et de traitement du langage naturel,afin d'accélérer le processus décisionnel. Pour cela, nous avons défini opérationnellement une expérience vécue d'un utilisateur comme un événement mentionné dans un avis, où l'auteur est présent parmi les participants. Cette définition considère que les événements mentionnés dans le texte sont les éléments les plus importants dans les expériences vécues : toutes les expériences vécues sont basées sur des événements, qui sont clairement définis dans le temps et l'espace. Par conséquent, nous proposons une approche permettant d'extraire les événements à partir des avis des utilisateurs, qui constituent la base d'un système permettant d'identifier et extraire les expériences vécues. Pour l'approche d'extraction d'événements, nous avons transformé les avis des utilisateur sen leurs représentations sémantiques en utilisant des techniques de machine reading. Nous avons effectué une analyse sémantique profonde des avis et détecté les cadres linguistiques les plus appropriés capturant des relations complexes exprimées dans les avis. Le système d'extraction des expériences vécues repose sur trois étapes. La première étape opère un filtrage des avis, basé sur les événements, permettant d'identifier les avis qui peuvent contenir des expériences vécues. La deuxième étape consiste à extraire les événements pertinents avec leurs participants. Afin de tester notre hypothèse, nous avons effectué quelques expériences pour vérifier si les expériences vécues peuvent être considérées comme des motivations pour les notes attribuées par les utilisateurs dans le système de notation. Par conséquent, nous avons utilisé les expériences vécues comme des caractéristiques dans un système de classification, en comparant avec les notes associées avec des avis dans un ensemble de données extraites et annotées manuellement de Tripadvisor. Les résultats montrent que les expériences vécues sont corrélées avec les notes. Cette thèse fournit des contributions intéressantes dans le domaine de l'analyse d'opinion. Tout d'abord, l'application avec succès de machine reading afin d'identifier les expériences vécues. Ensuite, La confirmation que les expériences vécues sont liées aux notations. Enfin, l'ensemble de données produit pour tester notre hypothèse constitue également une contribution importante de la thèse.
Bien que les réseaux de neurones soient à présent utilisés dans la quasi-totalité des composants d'un système de reconnaissance de la parole, du modèle acoustique au modèle de langue, l'entrée de ces systèmes reste une représentation analytique et fixée de la parole dans le domaine temps-fréquence, telle que les mel-filterbanks. Cela se distingue de la vision par ordinateur, un domaine où les réseaux de neurones prennent en entrée les pixels bruts. Les mel-filterbanks sont le produit d'une connaissance précieuse et documentée du système auditif humain, ainsi que du traitement du signal, et sont utilisées dans les systèmes de reconnaissance de la parole les plus en pointe, systèmes qui rivalisent désormais avec les humains dans certaines conditions. Cependant, les mel-filterbanks, comme toute représentation fixée, sont fondamentalement limitées par le fait qu'elles ne soient pas affinées par apprentissage pour la tâche considérée. Nous formulons l'hypothèse qu'apprendre ces représentations de bas niveau de la parole, conjontement avec le modèle, permettrait de faire avancer davantage l'état de l'art. Nous explorons tout d'abord des approches d'apprentissage faiblement supervisé et montrons que nous pouvons entraîner un unique réseau de neurones à séparer l'information phonétique de celle du locuteur à partir de descripteurs spectraux ou du signal brut et que ces représentations se transfèrent à travers les langues. De plus, apprendre à partir du signal brut produit des représentations du locuteur significativement meilleures que celles d'un modèle entraîné sur des mel-filterbanks. Ces résultats encourageants nous mènent par la suite à développer une alternative aux mel-filterbanks qui peut être entraînée à partir des données. Dans la seconde partie de cette thèse, nous proposons les Time-Domain filterbanks, une architecture neuronale légère prenant en entrée la forme d'onde, dont on peut initialiser les poids pour répliquer les mel-filterbanks et qui peut, par la suite, être entraînée par rétro-propagation avec le reste du réseau de neurones. Au cours d'expériences systématiques et approfondies, nous montrons que les Time-Domain filterbanks surclassent systématiquement les melfilterbanks, et peuvent être intégrées dans le premier système de reconnaissance de la parole purement convolutif et entraîné à partir du signal brut, qui constitue actuellement un nouvel état de l'art. Les descripteurs fixes étant également utilisés pour des tâches de classification non-linguistique, pour lesquelles elles sont d'autant moins optimales, nous entraînons un système de détection de dysarthrie à partir du signal brut, qui surclasse significativement un système équivalent entraîné sur des mel-filterbanks ou sur des descripteurs de bas niveau. Enfin, nous concluons cette thèse en expliquant en quoi nos contributions s'inscrivent dans une transition plus large vers des systèmes de compréhension du son qui pourront être appris de bout en bout.
Le travail présenté dans ce mémoire vise à proposer des solutions aux problèmes d'entreposage des données textuelles. L'intérêt porté à ce type de données est motivé par le fait qu'elles ne peuvent être intégrées et entreposées par l'application de simples techniques employées dans les systèmes décisionnels actuels. Elle couvre les principales phases d'un processus classique d'entreposage des données et utilise de nouvelles méthodes adaptées aux données textuelles. Dans ces travaux de thèse, nous nous sommes focalisés sur les deux premières phases qui sont l'intégration des données textuelles et leur modélisation multidimensionnelle. Pour mettre en place une solution d'intégration de ce type de données, nous avons eu recours aux techniques de recherche d'information (RI) et du traitement automatique du langage naturel (TALN). Pour cela, nous avons conçu un processus d'ETL (Extract-Transform-Load) adapté aux données textuelles. Il s'agit d'un framework d'intégration, nommé ETL-Text, qui permet de déployer différentes tâches d'extraction, de filtrage et de transformation des données textuelles originelles sous une forme leur permettant d'être entreposées. Certaines de ces tâches sont réalisées dans une approche, baptisée RICSH (Recherche d'information contextuelle par segmentation thématique de documents), de prétraitement et de recherche de données textuelles. Celui-ci étend le modèle en constellation classique pour prendre en charge la représentation des textes dans un environnement multidimensionnel. Dans TWM, il est défini une dimension sémantique conçue pour structurer les thèmes des documents et pour hiérarchiser les concepts sémantiques. Pour cela, TWM est adossé à une source sémantique externe, Wikipédia, en l'occurrence, pour traiter la partie sémantique du modèle. De plus, nous avons développé WikiCat, un outil pour alimenter la dimension sémantique de TWM avec des descripteurs sémantiques issus de Wikipédia. Ces deux dernières contributions complètent le framework ETL-Text pour constituer le dispositif d'entreposage des données textuelles. Pour valider nos différentes contributions, nous avons réalisé, en plus des travaux d'implémentation, une étude expérimentale pour chacune de nos propositions. Face au phénomène des données massives, nous avons développé dans le cadre d'une étude de cas des algorithmes de parallélisation des traitements en utilisant le paradigme MapReduce que nous avons testés dans l'environnement Hadoop.
L'objectif principal de cette thèse est de proposer un framework complet pour une découverte, modélisation et reconnaissance automatiques des activités humaines dans les vidéos. Afin de modéliser et de reconnaître des activités dans des vidéos à long terme, nous proposons aussi un framework qui combine des informations perceptuelles globales et locales issues de la scène, et qui construit, en conséquence, des modèles d'activités hiérarchiques. Dans la première catégorie du framework, un classificateur supervisé basé sur le vecteur de Fisher est formé et les étiquettes sémantiques prédites sont intégrées dans les modèles hiérarchiques construits. Dans la seconde catégorie, pour avoir un framework complètement non supervisé, plutôt que d'incorporer les étiquettes sémantiques, les codes visuels formés sont stockés dans les modèles. Nous évaluons les frameworks sur deux ensembles de données réalistes sur les activités de la vie quotidienne enregistrées auprés des patients dans un environnement hospitalier. Pour modéliser des mouvements fins du corps humain, nous proposons quatre différents frameworks de reconnaissance de gestes où chaque framework accepte une ou une combinaison de différentes modalités de données en entrée. Nous évaluons les frameworks développés dans le contexte du test de diagnostic médical, appelé Praxis. Nous proposons un nouveau défi dans la reconnaissance gestuelle qui consiste à obtenir une opinion objective sur les performances correctes et incorrectes de gestes très similaires. Les expériences montrent l'efficacité de notre approche basée sur l'apprentissage en profondeur dans la reconnaissance des gestes et les tâches d'évaluation de la performance.
La présente thèse a pour but d'exploiter le potentiel des données spatiales et temporelles présentes dans les textes des articles scientifiques, afin de proposer une ontologie d'informations spatio-temporelles contenues dans les textes scientifiques dans une perspective du Web Sémantique. Ces nouvelles métadonnées pourront servir comme support pour la production de nouvelles représentations sous forme graphique ou sous forme de synthèses textuelles, avec des applications en cartographie, en analyse chronologique et visualisations. Ce projet répond au besoin d'exploitation des informations des corpus scientifiques à grande échelle par des analyses et traitement sémantique en plein texte des articles scientifiques.
Le principal sujet de cette thèse CIFRE, en partenariat avec le cabinet de conseil en IT Retail OneTeam, est le Natural Language Processing (NLP). Sa spécificité est la conception et la mise en œuvre d'un ChatBot à usage interne. Plus précisément, ses cas d'utilisation seront d'aider à la relation entre les clients de OneTeam et son service client en ligne, d'aider à gérer les tickets techniques concernant les difficultés des clients avec le logiciel de vente au détail. Un ChatBot est une interface NLP entre un humain et une machine. Il est censé comprendre un sous-ensemble limité de langage naturel pertinent pour le contexte de l'application, fournir une d'interaction sous forme de dialogue et être capable de traduire les demandes humaines dans un langage soit semi-formel (à communiquer à un responsable technique), ou un langage formel (par exemple une requête de recherche), ou un contexte pragmatique (par exemple implémenter un ensemble d'actions tel que lancer un logiciel ou exécuter une requête SQL), ou une combinaison de ceux-ci. Dans le cadre de cette thèse, le ChatBot doit comprendre une description informelle d'un problème technique côté client lié à l'une des applications des suites logicielles de OneTeam, et fournir une interface conditionnelle basée sur un choix booléen : soit la description correspond à un problème existant dans les tickets fermés stockés dans la base de données, ou non. Dans le premier cas, le ChatBot devrait construire la description pragmatique formelle nécessaire pour résoudre le problème client à portée de main. Dans le deuxième cas, le ChatBot doit traduire la description informelle en un ticket ouvert semi-formel à écrire dans la base de données des tickets. Bien que la conception et le déploiement de ChatBots ne soient pas en soi nouveau, le système global requis par OneTeam présente des fonctionnalités qui offre du challenge :  1. la description informelle en entrée peut être une transcription d'un message téléphonique, ce qui rend le texte beaucoup plus entaché d'erreurs, de signes de ponctuation et donc plus difficile à comprendre ;  2. le langage limité à comprendre comprend plusieurs acronymes qui peuvent être mal orthographiés dans la description informelle, ainsi que des phrases non grammaticales ;  3. dans le cas où le ChatBot doit engendrer directement la solution, il n'y a aucune marge d'erreur dans la compréhension et la bonne traduction formelle pragmatique. Aborder ces caractéristiques nécessitera des recherches scientifiques entre l'algorithmique et la linguistique computationnelle. Un autre défi est que le texte peut être en français ou en anglais, le français étant de loin plus probable. Bien que cette fonctionnalité en elle-même ne soit pas nécessairement originale, il existe une quantité considérablement plus limitée de ressources logicielles NLP disponibles en français par rapport à l'anglais. Cela nécessitera sans aucun doute une conception et une mise en œuvre supplémentaires des tâches de NLP de bas niveau.
Ce travail porte sur les représentations de l'adolescence dans le cinéma indépendant contemporain. Il s'agit d'examiner la mise en images et en discours de ce phénomène social dans six films issus d'Europe et d'Amérique et sortis en salle entre 2009 et 2014. En nous inscrivant en Sciences de l'Information et de la Communication, nous articulons une approche sémiotique et pragmatique afin de rendre compte des aspects plastiques, esthétiques et communicationnels des films du corpus. Notre analyse poétique et socio-politique explore simultanément le langage cinématographique, les conditions de production et l'horizon d'attente auxquels renvoient les œuvres. La première partie rend compte de la construction du cadre analytique et de la démarche analytique qui articule adolescence, cinéma et représentation. La deuxième partie de la thèse saisit les formes et les effets de sens des films. Le corps y est analysé comme le fondement de la représentation de l'adolescence, un objet sémiotique, social et politique. La troisième partie de la recherche s'intéresse à l'émergence d'images des corps adolescents et de discours susceptibles d'engendrer un regard sur l'adolescence cinématographique en tant que construction symbolique et sociale. La thèse conclut que ces films en tant que discours sur l'univers adolescent, invitent à une interprétation délibérative sur le monde social, historique et culturel qui est convoqué dans les films.
Les lignes de produits logiciels (LdPs) permettent la dérivation d'une famille de produits basés sur une gestion de la variabilité. Les LdPs utilisent des configurations de caractéristiques afin de satisfaire les besoins de chaque client et, de même, permettre une réutilisation systématique en utilisant des assets réutilisables. L'approche capitalisant sur des variantes des produits existants est appelé une approche extractive pour l'adoption de LdPs. Il est également nécessaire de localiser les éléments associés à ces caractéristiques. Les contraintes entre ces caractéristiques doivent être identifiées afin de garantir la sélection de configurations valides. Cette thèse présente BUT4Reuse (Bottom-Up Technologies for Reuse), un framework unifié, générique et extensible pour l'adoption extractive de LdPs. Une attention particulière est accordée à des scénarios de développement dirigée par les modèles. Nous nous concentrons aussi sur l'analyse des techniques en proposant un benchmark pour la localisation de caractéristiques et une technique d'identification de familles de variantes. Nous présentons des paradigmes de visualisation pour accompagner les experts du domaine dans le nommage de caractéristiques et aider à la découverte de contraintes. Finalement, nous étudions l'exploitation des variantes pour l'analyse de la LdP après sa création. Nous présentons une approche pour trouver des variantes pertinentes guidée par des évaluations des utilisateurs finaux.
Notre thèse propose l'analyse de l'information selon la théorie de la Grammaire Cognitive (Langacker 1987, 1991). En fournissant des exemples du français, del 'anglais et du grec moderne, nous avons recherché les unités pertinentes pour unfiltrage de l'information. Une fonction est définie comme une opération impliquant un morphème dépendant, p. ex. un verbe ou un suffixe, et un morphème autonome, p. ex. un nom ;  certaines fonctions sont primaires, dans le sens qu'elles sontpsychologiquement plus saillantes ;  il s'agit des morphèmes dépendants verbaux ou déverbaux, qui profilent différemment une même scène processuelle. Afin d'en arriver àun filtrage automatique de l'information au niveau phrastique, il faut d'abord préparerdes dictionnaires de flexion et de dérivation déverbale. Nous faisons les premiers pas en analysant exhaustivement la flexion verbale de l'anglais, du français et du grec moderne et en constituant des dictionnaires électroniques de toutes les formes fléchies verbales.
Ce travail s'inscrit dans le cadre des recherches en conception de systemes d'information textuelle effectuees dans le groupe sydo-lyon. Une indexation d'un corpus homogene de textes francais est construite a partir de la description des syntagmes nominaux extraits des textes et des liens syntaxiques entre ces syntagmes nominaux. L'anaphore peut etre un de ces liens syntaxiques entre syntagmes nominaux, et de plus, elle est une relation discursive qui contribue a representer le contenu d'un texte dans le sens ou elle met en valeur certains syntagmes nominaux (themes). Le but principal de ce travail est d'identifier et de formaliser, en vue d'un traitement automatique, les principes de resolution d'anaphores et d'etudier les rapports entre l'anaphore et la coreference. Nous avons etabli une typologie des unites anaphoriques et des unites antecedentes, suivie d'une proposition pour reconnaitre automatiquement ces unites dans un texte. Enfin, un essai de mise en oeuvre automatique en prolog de la resolution des pronoms clitiques a ete realise. En conclusion, ce travail montre l'interet d'une analyse linguistique approfondie pour la recherche en conception de systeme d'information textuelle.
Les systèmes de question-réponse renvoient une réponse précise à une question formulée en langue naturelle. Les systèmes de question-réponse actuels, ainsi que les campagnes d'évaluation les évaluant, font en général l'hypothèse qu'une seule réponse est attendue pour une question. Or nous avons constaté que, souvent, ce n'était pas le cas, surtout quand on cherche les réponses sur le Web et non dans une collection finie de documents. Nous nous sommes donc intéressés au traitement des questions attendant plusieurs réponses à travers un système de question-réponse sur le Web en français. Pour cela, nous avons développé le système Citron capable d'extraire des réponses multiples différentes à des questions factuelles en domaine ouvert, ainsi que de repérer et d'extraire le critère variant (date, lieu) source de la multiplicité des réponses. Nous avons montré grâce à notre étude de différents corpus que les réponses à de telles questions se trouvaient souvent dans des tableaux ou des listes mais que ces structures sont difficilement analysables automatiquement sans prétraitement. C'est pourquoi, nous avons également développé l'outil Kitten qui permet d'extraire le contenu des documents HTML sous forme de texte et aussi de repérer, analyser et formater ces structures. Enfin, nous avons réalisé deux expériences avec des utilisateurs. La première expérience évaluait Citron et les êtres humains sur la tâche d'extraction de réponse multiples : les résultats ont montré que Citron était plus rapide que les êtres humains et que l'écart entre la qualité des réponses de Citron et celle des utilisateurs était raisonnable. La seconde expérience a évalué la satisfaction des utilisateurs concernant la présentation de réponses multiples : les résultats ont montré que les utilisateurs préféraient la présentation de Citron agrégeant les réponses et y ajoutant un critère variant (lorsqu'il existe) par rapport à la présentation utilisée lors des campagnes d'évaluation.
MATLAB est un environnement informatique doté d'un langage de programmation simple et d'une vaste bibliothèque de fonctions couramment utilisées en science et ingénierie (CSE) pour le prototypage rapide. Cependant, certaines caractéristiques de son environnement, comme son langage dynamique ou son style de programmation interactif, affectent la rapidité d'exécution des programmes. Les approches actuelles d'amélioration des programmes MATLAB traduisent le code dans des langages statiques plus rapides comme C ou Fortran, ou bien appliquent systématiquement des transformations de code au programme MATLAB sans considérer leur impact sur les performances. Dans cette thèse, nous comblons cette lacune en développant des techniques d'analyse et de transformation de code des programmes MATLAB afin d'augmenter leur performance. Plus précisément, nous analysons et modélisons le comportement d'un environnement MATLAB black-box uniquement en mesurant l'exécution caractéristique des programmes sur CPU. À partir des données obtenues, nous formalisons un modèle statique qui prédit le type et l'ordonnancement des instructions programmées lors de l'exécution par le compilateur Just-In-Time (JIT). Ce modèle nous permet de proposer plusieurs transformations de code qui améliorent les performances des programmes MATLAB en influençant la façon dont le compilateur JIT génère le code machine. Les résultats obtenus démontrent les avantages pratiques de la méthodologie présentée.
Avec le développement de la robotique cognitive, le besoin d'outils avancés pour représenter, manipuler, raisonner sur les connaissances acquises par un robot a clairement été mis en avant. Mais stocker et manipuler des connaissances requiert tout d'abord d'éclaircir ce que l'on nomme connaissance pour un robot, et comment celle-ci peut-elle être représentée de manière intelligible pour une machine. Dans un second temps, nous présentons en profondeur ORO, une instanciation particulière d'un système de représentation et manipulation des connaissances, conçu et implémenté durant la préparation de cette thèse. Nous détaillons le fonctionnement interne du système, ainsi que son intégration dans plusieurs architectures robotiques complètes. Un éclairage particulier est donné sur la modélisation de la prise de perspective dans le contexte de l'interaction, et de son interprétation en terme de théorie de l'esprit. La troisième partie de l'étude porte sur une application importante des systèmes de représentation des connaissances dans ce contexte de l'interaction homme-robot : le traitement du dialogue situé. Nous concluons cette thèse sur un certain nombre de considérations sur la viabilité et l'importance d'une gestion explicite des connaissances des agents, ainsi que par une réflexion sur les éléments encore manquant pour réaliser le programme d'une robotique “de niveau humain”
Le cadre de nos recherches est la diffusion d'informations en Langue des Signes Française via un signeur virtuel, par combinaison de segments d'énoncés préenregistrés. Notre étude porte sur une proposition de modèle de coarticulation pour ce système de diffusion. Nous détaillons les différents aspects de la création et de l'annotation de ce corpus, et de l'analyse de ces annotations. Des calculs statistiques quantitatifs et qualitatifs nous permettent de proposer un modèle de coarticulation, basé sur des relâchements et des tensions de configurations des mains. Nous proposons et mettons en œuvre une méthodologie d'évaluation de notre modèle. Enfin nous proposons des perspectives autour des utilisations potentielles de ce modèle pour des recherches en traitement d'image et en animation de personnages 3d s'exprimant en langue des signes française.
Quand les robots doivent affronter le monde réel, ils doivent s'adapter à diverses situations imprévues en acquérant de nouvelles compétences le plus rapidement possible. Les algorithmes d'apprentissage par renforcement (par exemple, l'apprentissage par renforcement profond) pourraient permettre d'apprendre de telles compétences, mais les algorithmes actuels nécessitent un temps d'interaction trop important. Notre objectif principal est de combiner des connaissances acquises sur un simulateur avec les expériences réelles du robot afin d'obtenir un apprentissage et une adaptation rapides. Dans notre première contribution, nous proposons un nouvel algorithme de recherche de politiques basé sur un modèle, appelé Multi-DEX, qui (1) est capable de trouver des politiques dans des scénarios aux récompenses rares, (2) n'impose aucune contrainte sur le type de politique ou le type de fonction de récompense et (3) est aussi efficace en termes de données que l'algorithme de recherche de politiques de l'état de l'art dans des scénarios de récompenses non rares. Dans notre troisième contribution, nous présentons un algorithme de méta-apprentissage basé sur les gradients appelé FAMLE. FAMLE permet d'entraîner le modèle dynamique du robot à partir de données simulées afin que le modèle puisse être adapté rapidement à diverses situations invisibles grâce aux observations du monde réel. En utilisant FAMLE pour améliorer un modèle pour la commande prédictive, nous montrons que notre approche surpasse plusieurs algorithmes d'apprentissage basés ou non sur un modèle, et résout les tâches données en moins de temps d'interaction que les algorithmes avec lesquels nous l'avons comparé.
Avec l'avancée du Web Sémantique et des initiatives Open Linked Data, une grande quantité de documents RDF sont disponibles sur Internet. L'objectif est de rendre ces données lisibles pour les humains et les machines, en adoptant des formats spéciaux et en les connectant à l'aide des IRIs (International Resource Identifier), qui sont des abstractions de ressources réelles du monde. Cependant, les études sur l'anonymisation dans le contexte des documents RDF sont très limitées. Ces études sont les travaux initiaux de protection des individus sur des documents RDF, puisqu'ils montrent les approches pratiques d'anonymisation pour des scénarios simples comme l'utilisation d'opérations de généralisation et d'opérations de suppression basées sur des hiérarchies. Cependant, pour des scénarios complexes, où une diversité de données est présentée, les approches d'anonymisations existantes n'assurent pas une confidentialité suffisante. Ainsi, dans ce contexte, nous proposons une approche d'anonymisation, qui analyse les voisins en fonction des connaissances antérieures, centrée sur la confidentialité des entités représentées comme des nœuds dans les documents RDF. Notre approche de l'anonymisation est capable de fournir une meilleure confidentialité, car elle prend en compte la condition de la diversité de l'environnement ainsi que les voisins (nœuds et arêtes) des entités d'intérêts. En outre, un processus d'anonymisation automatique est assuré par l'utilisation d'opérations d'anonymisations associées aux types de données.
Les buts de cette thèse sont doubles, et concernent les circuits métaboliques synthétiques, qui permettent de détecter des composants chimiques par transmission de signal et de faire du calcul en utilisant des enzymes. La première partie a consisté à développer des outils d'apprentissage actif et par renforcement pour améliorer la conception de circuits métaboliques et optimiser la biodétection et la bioproduction. Pour atteindre cet objectif, un nouvel algorithme (RetroPath3.0) fondé sur une recherche arborescente de Monte Carlo guidée par similarité est présenté. Cet algorithme, combiné à des règles de réaction apprises sur des données et des niveaux différents de promiscuité enzymatique, permet de focaliser l'exploration sur les composés et les chemins les plus prometteurs en bio-rétrosynthèse. La deuxième partie a consisté à développer des méthodes d'analyse, pour générer des connaissances à partir de données biologiques, et modéliser les réponses de biocapteurs. Dans un premier temps, l'effet du nombre de copies de plasmides sur la sensibilité d'un biocapteur utilisant un facteur de transcription a été modélisé. Ensuite, en utilisant des systèmes acellulaires qui permettent un meilleur contrôle des variables expérimentales comme la concentration d'ADN, l'utilisation des ressources a été modélisée pour assurer que notre compréhension actuelle des phénomènes sous-jacents est suffisante pour rendre compte du comportement du circuit, en utilisant des modèles empiriques ou mécanistiques. Dans l'ensemble, cette thèse présente des outils de conception et d'analyse pour les circuits métaboliques synthétiques. Ces outils ont été utilisés pour développer une nouvelle méthode permettant d'effectuer des calculs en biologie synthétique.
Le développement de systèmes d'analyse discursive automatique des documents est un enjeu actuel majeur en Traitement Automatique des Langues. La difficulté principale correspond à l'étape d'identification des relations (comme Explication, Contraste...) liant les segments constituant le document. Dans cette thèse, nous utilisons des données brutes pour améliorer des systèmes d'identification automatique des relations implicites. Nous proposons d'abord d'utiliser les connecteurs pour annoter automatiquement de nouvelles données. Nous rapportons des améliorations sur le corpus anglais du Penn Discourse Treebank et montrons notamment que cette méthode permet de limiter le recours à des ressources riches, disponibles seulement pour peu de langues.
Les réseaux de neurones sont devenus un algorithme phare de l'intelligence artificielle, avec des succès majeurs dans la résolution de tâches complexes telles que la reconnaissance d'images et le jeu. Les ordinateurs et les cartes graphiques consomment malheureusement une quantité très élevée d'énergie lorsqu'ils exécutent des réseaux de neurones. Par exemple, l'entraînement d'un modèle de traitement du langage naturel de pointe sur un superordinateur moderne consomme 1000 kW.h [1], soit l'énergie consommée par un cerveau humain pour l'ensemble de ses tâches sur une durée de six ans. Dans le cerveau, les neurones - qui peuvent être considérés comme effectuant le calcul - ont un accès direct à la mémoire, portée par les synapses. L'électronique actuelle, sur laquelle reposent les GPU et les CPU utilisés en IA, sépare intrinsèquement la mémoire et le calcul dans des unités physiques distinctes, entre lesquelles les données doivent être transportées. Ce "goulot d'étranglement de von Neuman" est un problème pour les algorithmes d'intelligence artificielle qui nécessitent la lecture de quantités considérables de données à chaque étape, l'exécution d'opérations avancées sur ces données, puis la réécriture des résultats en mémoire [2]-[4]. Ce processus ralentit le calcul et augmente considérablement la consommation d'énergie pour l'apprentissage et l'inférence. Le paradigme général du calcul neuromorphique consiste donc à s'inspirer de la topologie du cerveau pour construire des circuits composés de neurones physiques interconnectés par des synapses physiques qui mettent en œuvre la mémoire in-situ, de manière non volatile, ce qui réduit considérablement la nécessité de déplacer les données dans le circuit et permet d'énormes gains en termes de vitesse et d'efficacité énergétique. L'un des défis majeurs du calcul neuromorphique est de parvenir à former les réseaux sur puce de manière efficace. La mise en œuvre d'un algorithme d'entraînement traditionnel tel que la rétropropagation du gradient nécessite des circuits de grande surface et gourmands en énergie pour stocker les activations, calculer les gradients, stocker les gradients, puis modifier séquentiellement chaque synapse physique du réseau. C'est un obstacle immense au développement d'une IA intégrée capable d'apprendre. C'est pourquoi les puces d'IA développées aujourd'hui dans les universités et l'industrie visent principalement l'inférence avec un apprentissage hors puce [5]-[7], ou mettent en œuvre des algorithmes d'entraînement dont les performances sont loin des méthodes de descente de gradient les plus modernes [8]. Dans ce contexte, les travaux de pionniers de l'IA, tels que Geoffrey Hinton [9] et Yoshua Bengio [10]-[12], visant à comprendre comment la descente de gradient peut être effectuée dans le cerveau, donnent des indications inspirantes pour atteindre le même résultat dans les systèmes neuromorphiques. Des algorithmes tels que la propagation de l'équilibre, proposée pour la première fois par Scellier et Bengio [12], et la propagation de l'éligibilité (E-Prop) de Wolfgang Maas [13] montrent que, dans les réseaux de neurones de pointe, les gradients peuvent être transportés à travers le réseau directement par l'activité des neurones. Le C2N et le CNRS/Thales, en collaboration avec l'équipe de Yoshua Bengio au Mila, ont montré que les gradients calculés par la propagation de l'équilibre sont égaux à ceux dérivés par la rétropropagation du gradient dans le temps, et ont souligné le potentiel de ce résultat pour la formation de systèmes neuromorphiques avec des performances de pointe dans une publication dans NeurIPS l'année dernière (acceptée comme présentation orale) [14]. L'objectif ultime de la thèse est de fabriquer des circuits neuromorphiques qui apprennent localement et de manière autonome grâce à des algorithmes tels que la propagation de l'équilibre, qui codent des gradients dans l'activité impulsionnelle des neurones, et peuvent être directement appliqués aux synapses qu'ils connectent. À cette fin, le/la doctorant.e devra d'abord développer, coder et tester par des simulations de nouveaux algorithmes pour entraîner des réseaux de neurones à impulsions récurrents, inspirés de la propagation de l'équilibre mais adaptés au matériel neuromorphique. Il/elle réalisera ensuite ces réseaux avec des nanocomposants, par des expériences à l'échelle du laboratoire, puis dans des systèmes plus importants. Les éléments de base qui seront utilisés pour développer ce matériel sont des composants électroniques à l'échelle nanométrique appelés dispositifs à commutation résistive ou memristors (abréviation de 'memory resistors') [15]. Ils sont constitués d'un oxyde isolant pris en sandwich entre deux électrodes métalliques (Fig. 1(a-b). Ces dispositifs à l'échelle nanométrique sont très prometteurs pour l'informatique neuromorphique car ils peuvent imiter à la fois les synapses et les neurones avec une très faible énergie, et ont de très petites dimensions. Ils présentent des comportements différents selon les matériaux utilisés pour l'oxyde et les électrodes. Ils peuvent être passifs et présenter une commutation de résistance non-volatile (par exemple, avec des hétérostructures Pt/TaOx/Pt, Fig. 1(d)), ce qui est utile pour les caractéristiques synaptiques (mémoire et plasticité à plusieurs niveaux). Ils peuvent également être actifs et présenter une commutation volatile par résistance différentielle négative (par exemple, avec des hétérostructures Pt/NbOx/Pt), ce qui peut générer des caractéristiques neuronales oscillantes (comme des impulsions et des trains d'impulsions, Fig. 1(c)). Le C2N et le CNRS/Thales ont une expertise de longue date dans la fabrication et l'étude de ces nanodispositifs [3], [16]-[20]. Pour l'instant, les études ont surtout porté sur les circuits qui utilisent ces dispositifs soit comme synapses, soit comme neurones, mais pas les deux à la fois. Il a été démontré que les synapses mémorielles peuvent apprendre grâce à l'apprentissage bio-inspiré appelé Spike Timing Dependent Plasticity [3], [21]. Il a aussi été prouvé que les neurones mémoriels peuvent imiter de nombreuses caractéristiques des neurones comme leur réponse impulsionnelle et les trains d'impulsions [15]. Le but de cette thèse est d'assembler ces composants individuels pour former des réseaux de neurones à impulsions capables d'apprendre de manière autonome grâce à des algorithmes de pointe reproduisant la retro-propagation du gradient *in materio*.
Cette thèse de recherche aborde les champs de la créativité, de l'innovation et du processus créatif collaboratif. Il est nécessaire de construire de nouveaux systèmes techniques et innovants qui peuvent imiter le comportement humain précieux consistant à visualiser et à donner vie à leurs idées. Cette approche fait appel à la créativité et implique, en tant que processus, de nombreux concepts tels que la découverte, la création, la sociabilité, le raffinement et la communication. L'objectif de cette recherche est de concevoir des outils informatiques et technologiques pour soutenir la créativité au cours des phases de création dans un atelier de créativité. Pour opérationnaliser cet outil, il y a plusieurs objectifs à atteindre pendant le temps requis pour cette recherche. Le contexte de la recherche est un atelier de créativité « 48H » , mobilisant des étudiants, des professeurs, des industriels, des experts travaillant avec des idées au moyen d'une combinatoire exploratoire, de techniques de transformation et de méthodes de création collaborative. Pour construire un outil pour ce genre d'événement créatif, nous avons besoin d'étudier comment prendre en compte le modèle humain de partage des connaissances et comment un système proposé peut développer une approche sémantique pour comparer les cartes d'idées dans un atelier de créativité de 48 heures. Les principaux objectifs de cette recherche sont de comprendre le partage des connaissances lors d'un atelier de créativité et d'étudier l'organisation humaine dans un atelier de créativité et son processus général, en particulier. Nous étudions l'événement 48 Heures de créativité (48H) comme un processus créatif et innovant où l'idée est le produit final. Cet événement a lieu à Nancy, en France, à l'Université de Lorraine (Laboratoire ERPI). Nous concevons un système intelligent basé en multi-agents (MAS) également pour développer et annoter le système. Nous proposons une approche sémantique pour gérer les idées et le système de design global. Les implications pratiques de notre modèle permettent d'utiliser la créativité, les méthodes créatives collaboratives et les nouvelles méthodologies technologiques modernes, comme de nouveaux outils conçus pour construire des systèmes créatifs et innovants. Quelques domaines et modèles pertinents participent à cette recherche tels que le système multi-agent (MAS), le web sémantique, l'ontologie, le modèle organisationnel pour mettre en évidence les connaissances et le modèle organisationnel de réutilisation des connaissances KROM.
Avec l'augmentation massive du contenu vidéo sur Internet et au-delà, la compréhension automatique du contenu visuel pourrait avoir un impact sur de nombreux domaines d'application différents tels que la robotique, la santé, la recherche de contenu ou le filtrage. Le but de cette thèse est de fournir des contributions méthodologiques en vision par ordinateur et apprentissage statistique pour la compréhension automatique du contenu des vidéos. Nous mettons l'accent sur les problèmes de la reconnaissance de l'action humaine à grain fin et du raisonnement visuel à partir des interactions entre objets. Dans la première partie de ce manuscrit, nous abordons le problème de la reconnaissance fine de l'action humaine. Nous introduisons deux différents mécanismes d'attention, entrainés sur le contenu visuel à partir de la pose humaine articulée. Une première méthode est capable de porter automatiquement l'attention sur des points pré-sélectionnés importants de la vidéo, conditionnés sur des caractéristiques apprises extraites de la pose humaine articulée. Nous montrons qu'un tel mécanisme améliore les performances sur la tâche finale et fournit un bon moyen de visualiser les parties les plus discriminantes du contenu visuel. Une deuxième méthode va au-delà de la reconnaissance de l'action humaine basée sur la pose. Nous développons une méthode capable d'identifier automatiquement un nuage de points caractéristiques non structurés pour une vidéo à l'aide d'informations contextuelles. De plus, nous introduisons un système distribué entrainé pour agréger les caractéristiques de manière récurrente et prendre des décisions de manière distribuée. Nous démontrons que nous pouvons obtenir de meilleures performances que celles illustrées précédemment, sans utiliser d'informations de pose articulée au moment de l'inférence. Dans la deuxième partie de cette thèse, nous étudions les représentations vidéo d'un point de vue objet. Étant donné un ensemble de personnes et d'objets détectés dans la scène, nous développons une méthode qui a appris à déduire les interactions importantes des objets à travers l'espace et le temps en utilisant uniquement l'annotation au niveau vidéo. Cela permet d'identifier une interaction inter-objet importante pour une action donnée ainsi que le biais potentiel d'un ensemble de données. Enfin, dans une troisième partie, nous allons au-delà de la tâche de classification et d'apprentissage supervisé à partir de contenus visuels, en abordant la causalité à travers les interactions, et en particulier le problème de l'apprentissage contrefactuel. Nous introduisons une nouvelle base de données, à savoir CoPhy, où, après avoir regardé une vidéo, la tâche consiste à prédire le résultat après avoir modifié la phase initiale de la vidéo. Nous développons une méthode basée sur des interactions au niveau des objets capables d'inférer les propriétés des objets sans supervision ainsi que les emplacements futurs des objets après l'intervention.
Cette etude sur la classification et l'analyse syntaxique des phrases figees s'inscrit dans la perspective de l'elaboration d'un lexique-grammaire du grec moderne,en d'autres termes un dictionnaire syntaxique de la langue. Le cadre syntaxique theorique est celui de la grammaire transformationnelle de z. S. Harris et de m. Gross. Les phrases figees se definissent par le fait qu'un ou plusieurs de leurs actants sont lexicalement invariables par rapport au verbe. Les phrases figees, s'analysent, dans la majorite des cas, de facon syntaxiquement correcte et les regles qu'elles subissent sont les regles de la syntaxe des phrases libres. Ceci est valable aussi bien pour leurs parties libres que pour leurs parties figees. Notre classification est donc basee sur le fait que le nombre et la position des parties libres et figees est variable. Quant aux eventuelles applications pratiques nous pensons aux differents traitements automatiques du langage ainsi qu'a l'enseignement des langues et a la traduction.
La reconnaissance des opinions d'un locuteur dans une interaction orale est une étape cruciale pour améliorer la communication entre un humain et un agent virtuel. Dans cette thèse, nous nous situons dans une problématique de traitement automatique de la parole (TAP) sur les phénomènes d'opinions dans des interactions orales spontanées naturelles. L'analyse d'opinion est une tâche peu souvent abordée en TAP qui se concentrait jusqu'à peu sur les émotions à l'aide du contenu vocal et non verbal. De plus, la plupart des systèmes récents existants n'utilisent pas le contexte interactionnel afin d'analyser les opinions du locuteur. Dans cette thèse, nous nous penchons sur ces sujet. Nous nous situons dans le cadre de la détection automatique en utilisant des modèles d'apprentissage statistiques. Après une étude sur la modélisation de la dynamique de l'opinion par un modèle à états latents à l'intérieur d'un monologue, nous étudions la manière d'intégrer le contexte interactionnel dialogique, et enfin d'intégrer l'audio au texte avec différents types de fusion. Nous avons travaillé sur une base de données de Vlogs au niveau d'un sentiment global, puis sur une base de données d'interactions dyadiques multimodales composée de conversations ouvertes, au niveau du tour de parole et de la paire de tours de parole. Pour finir, nous avons fait annoté une base de données en opinion car les base de données existantes n'étaient pas satisfaisantes vis-à-vis de la tâche abordée, et ne permettaient pas une comparaison claire avec d'autres systèmes à l'état de l'art. A l'aube du changement important porté par l'avènement des méthodes neuronales, nous étudions différents types de représentations : les anciennes représentations construites à la main, rigides mais précises, et les nouvelles représentations apprises de manière statistique, générales et sémantiques. Nous étudions différentes segmentations permettant de prendre en compte le caractère asynchrone de la multi-modalité. Dernièrement, nous utilisons un modèle d'apprentissage à états latents qui peut s'adapter à une base de données de taille restreinte, pour la tâche atypique qu'est l'analyse d'opinion, et nous montrons qu'il permet à la fois une adaptation des descripteurs du domaine écrit au domaine oral, et servir de couche d'attention via son pouvoir de clusterisation. La fusion multimodale complexe n'étant pas bien gérée par le classifieur utilisé, et l'audio étant moins impactant sur l'opinion que le texte, nous étudions différentes méthodes de sélection de paramètres pour résoudre ces problèmes.
Leur qualité joue par conséquent un rôle déterminant dans le succès du système final. Des études ont montré que la majeure partie des changements que subit un SI concerne des manques ou des défaillances liés aux fonctionnalités attendues. Sachant que la définition de ses fonctionnalités incombe à la phase de l'analyse et conception dont les MC constituent les livrables, il apparaît indispensable pour une méthode de conception de veiller à la qualité des MC qu'elle produit. Notre approche vise les problèmes liés à la qualité de la modélisation conceptuelle en proposant une solution intégrée au processus de développement qui à l'avantage d'être complète puisqu'elle adresse à la fois la mesure de la qualité ainsi que son amélioration. La proposition couvre les aspects suivants :  En effet, un des manques constaté dans le domaine de la qualité des MC est l'absence de consensus sur les concepts et leurs définitions. Un pattern de qualité sert à aider un concepteur de SI dans l'identification des critères de qualité applicables à sa spécification, puis de le guider progressivement dans la mesure de la qualité ainsi que dans son amélioration. Sachant que la plupart des approches existantes s'intéresse à la mesure de la qualité et néglige les moyens de la corriger. iii. Formulation d'une méthode orientée qualité incluant à la fois des concepts, des guides et des techniques permettant de définir les concepts de qualité souhaités, leur mesure et l'amélioration de la qualité des MC. iv. Développement d'un prototype "CM-Quality".
Il existe dans le sud ouest algérien plusieurs variétés de berbère. Certaines d'entre elles sont situées dans la région dite du Sud-Oranais et peuvent être cataloguées comme des langues en danger. Nous avons donc entrepris de décrire ces variétés avant qu'elles ne disparaissent. Cela a été mené à bien en réalisant plusieurs enquêtes de terrain. Par ailleurs, ce travail de documentation linguistique et de conservation du patrimoine culturel n'est qu'un des aspects de cette thèse. Nous avons eu recours aux méthodes en usage en Sciences de l'Information Géographique (SIG) et en Sciences Des Données (SDD) pour mener une étude dialectologique. Grâce aux SIG, nous avons réalisé une étude géolinguistique qui nous a permis de visualiser sur des cartes linguistiques la distribution de la variation linguistique de certaines consonnes. À partir de ces données, nous avons discuté de la réalité phonologique de ces consonnes simples et géminées. Dans le prolongement, une étude dialectométrique a été effectuée en nous basant sur des méthodes de partitions des données. Nous avons utilisé les méthodes d'Apprentissage Non Supervisé (PHA, k-moyenne, MDS,...) et les méthodes d'Apprentissage Supervisé (CART) connues en SDD. Puis, nous avons entrepris une analyse phonétique fondée sur une étude acoustique des rhotiques alvéolaires : [ɾ], [r], [ɾˤ] et [rˤ]. Ces unités phoniques se distinguent par leur temporalité et leur réalisation articulatoire. Ainsi, les spectrogrammes nous ont permis d'examiner la distribution de ces sons.
Les discours normés que produisent les institutions sont concurrencés par les discours informels ou faiblement formalisés issus du web social. La démocratisation de la prise de parole redistribue l'autorité en matière de connaissance et modifie les processus de construction des savoirs. Ces discours spontanés sont accessibles par tous et dans des volumes exponentiels, ce qui offre aux sciences humaines et sociales de nouvelles possibilités d'exploration. Pourtant elles manquent encore de méthodologies pour appréhender ces données complexes et encore peu décrites. L'objectif de la thèse est de montrer dans quelle mesure les discours du web social peuvent compléter les discours institutionnels. Nous y développons une méthodologie de collecte et d'analyse adaptée aux spécificités des discours natifs du numérique (massivité, anonymat, volatilité, caractéristiques structurelles, etc.). Ce terrain applicatif recouvre plusieurs enjeux de société : sanitaire et social, évolutions des moeurs, concurrence des discours. L'étude est complétée par l'analyse d'un corpus comparable de langue française, relevant des mêmes thématique, genre et discours que le corpus vietnamien, de manière à mettre en évidence les spécificités de contextes socioculturels distincts.
L'objectif de ce projet est d'exploiter le flux audio en profondeur pour enrichir de façon automatique les scripts et sous-titres de séries télévisées et de films, en y ajoutant automatiquement les noms et positions des personnages. locuteur A : 'Nice to meet you, I am Leonard, and this is Sheldon. We live across the hall.' locuteur B : 'Oh. Hi. I'm Penny.' locuteur A : 'Sheldon, what the hell are you doing ?' locuteur C : 'I am not quite sure yet. Do you know where Howard lives ?' En lisant ces deux conversations, un humain peut facilement déterminer que 'locuteur A' s'appele 'Leonard', 'locuteur B' est 'Penny' et 'locuteur C' est 'Sheldon'. L'objectif de ce projet est de combiner des approches de traitement automatique de la langue naturelle et de la parole pour arriver à ce résultat automatiquement.
De manière simple, il peut être présenté ainsi : imaginez que vous êtes dans un labyrinthe, dont vous connaissez toutes les routes menant à chacune des portes de sortie. Pour cela, il vous indiquera la direction à prendre à chaque intersection. Malheureusement, cet homme ne parle pas votre langue, et les mots qu'il utilise pour dire ``droite'' ou ``gauche'' vous sont inconnus. Est-il possible de trouver le trésor et de comprendre l'association entre les mots du vieil homme et leurs significations ? Ce problème, bien qu'en apparence abstrait, est relié à des problématiques concrètes dans le domaine de l'interaction homme-machine. Remplaçons le vieil homme par un utilisateur souhaitant guider un robot vers une sortie spécifique du labyrinthe. Ce robot ne sait pas en avance quelle est la bonne sortie mais il sait où se trouvent chacune des portes et comment s'y rendre. Imaginons maintenant que ce robot ne comprenne pas a priori le langage de l'humain; en effet, il est très difficile de construire un robot à même de comprendre parfaitement chaque langue, accent et préférence de chacun. Il faudra alors que le robot apprenne l'association entre les mots de l'utilisateur et leur sens, tout en réalisant la tâche que l'humain lui indique (i.e.trouver la bonne porte). En effet, le résoudre reviendrait à créer des interfaces ne nécessitant pas de phase de calibration car la machine pourrait s'adapter,automatiquement et pendant l'interaction, à différentes personnes qui ne parlent pas la même langue ou qui n'utilisent pas les mêmes mots pour dire la même chose. Cela veut aussi dire qu'il serait facile de considérer d'autres modalités d'interaction (par exemple des gestes, des expressions faciales ou des ondes cérébrales). Dans cette thèse, nous présentons une solution à ce problème. Nous appliquons nos algorithmes à deux exemples typiques de l'interaction homme robot et de l'interaction cerveau machine : une tâche d'organisation d'une série d'objets selon les préférences de l'utilisateur qui guide le robot par la voix, et une tâche de déplacement sur une grille guidé par les signaux cérébraux de l'utilisateur. Ces dernières expériences ont été faites avec des utilisateurs réels.
L'objectif principal de cette thèse est de proposer des techniques d'apprentissage de représentation avares en données étiquetées : apprentissage non-supervisé, supervision distante, ou encore apprentissage actif sont autant de pistes qui pourront être étudiées. L'apprentissage non-supervisé est le cas extrême où les données non-étiquetées sont disponibles en abondance, mais aucune donnée étiquetée n'est disponible. L'appellation 'auto-supervisé'(self-supervised, en anglais) est de plus en plus utilisée pour décrire ce type d'approches car il s'agit généralement d'approches visant à reconstruire les données originales à partir d'une version altérée (BERT [Devlin et al., 2018], auto-encodeurs, etc.). On parle de supervision distante quand des étiquettes (parfois imparfaites) sont effectivement disponibles, mais pas pour la tâche visée. Par exemple, pour la tâche de vérification du locuteur, il est possible d'utiliser un système de transcription de la parole automatique pour obtenir des étiquettes phonétiques du signal de parole. Cette information orthogonale à la tâche principale doit permettre de démêler des signaux contradictoires pour améliorer la représentation des locuteurs [Zeghidour et al., 2016]). L'apprentissage actif est un modèle d'apprentissage semi-supervisé où un oracle (généralement, un humain) intervient au cours du processus. Plus précisément, à partir de données non-étiquetés, l'algorithme d'apprentissage détermine quelles données doivent être annotées par l'oracle pour obtenir les meilleures performances à moindre coût [Lowell et al., 2018, Drugman et al., 2019, Feyisetan et al., 2019, Settles and Craven, 2008, Duong et al., 2018, Kholghi et al., 2016,Settles, 2009].
Avec l'évolution des systèmes d'information des établissements de santé, les praticiens sont tenus de saisir de plus en plus d'informations de manière numérique à travers différents logiciels spécialisés. La prise en main de ces logiciels impliquent toujours une étape de formation et d'adaptation nécessite l'accès immédiat à un poste de travail, ce qui prend du temps sur leur mission de soin. Par ailleurs, ces contraintes modifient grandement les habitudes de travail des médecins habitués à saisir leurs ordonnances directement sur une page blanche ou dans un autre logiciel. Le but de ce travail de thèse est de libérer les prescripteurs d'un certain nombre de ces contraintes, et de leur proposer un outil leur permettant de se rapprocher le plus possible d'une construction la plus *naturelle* et *culturelle* possible de l'ordonnance, dans un langage quasi-naturel. Pour répondre à ce problème, nous nous plaçons dans un contexte d'interaction vocale sur smartphone qui permet de répondre à deux problèmes observés : l'interaction non naturelle avec les logiciel d'aide à la prescription et le manque d'accès immédiat à un système informatique connecté.
Les systèmes de recommandation essayent de déduire les intérêts de leurs utilisateurs afin de leurs suggérer des items pertinents. Ces systèmes offrent ainsi aux utilisateurs un service utile car ils filtrent automatiquement les informations non-pertinentes, ce qui évite le problème de surcharge d'information qui est courant de nos jours. C'est pourquoi les systèmes de recommandation sont aujourd'hui populaires, si ce n'est omniprésents dans certains domaines tels que le World Wide Web. Cependant, les intérêts d'un individu sont des données personnelles et privées, comme par exemple son orientation politique ou religieuse. Les systèmes de recommandation recueillent donc des données privées et leur utilisation répandue nécessite des mécanismes de protection de la vie privée. Dans cette thèse, nous étudions la protection de la confidentialité des intérêts des utilisateurs des systèmes de recommandation appelés systèmes de filtrage collaboratif (FC). Notre première contribution est Hide &amp ; Share, un nouveau mécanisme de similarité, respectueux de la vie privée, pour la calcul décentralisé de graphes de K-Plus-Proches-Voisins (KPPV). C'est un mécanisme léger, conçu pour les systèmes de FC fondés sur les utilisateurs et décentralisés (ou pair-à-pair), qui se basent sur les graphes de KPPV pour fournir des recommandations. Notre seconde contribution s'applique aussi aux systèmes de FC fondés sur les utilisateurs, mais est indépendante de leur architecture. Cette contribution est double : nous évaluons d'abord l'impact d'une attaque active dite « Sybil » sur la confidentialité du profil d'intérêts d'un utilisateur cible, puis nous proposons une contre-mesure. Celle-ci est 2-step, une nouvelle mesure de similarité qui combine une bonne précision, permettant ensuite de faire de bonnes recommandations, avec une bonne résistance à l'attaque Sybil en question.
Le Center for Data Science de l'Université Paris-Saclay a déployé une plateforme compatible avec le Linked Data en 2016. Or, les chercheurs rencontrent face à ces technologies de nombreuses difficultés. Pour surmonter celles-ci, une approche et une plateforme appelée LinkedWiki, ont été conçues et expérimentées au-dessus du cloud de l'université (IAAS) pour permettre la création d'environnements virtuels de recherche (VRE) modulaires et compatibles avec le Linked Data. Nous avons ainsi pu proposer aux chercheurs une solution pour découvrir, produire et réutiliser les données de la recherche disponibles au sein du Linked Open Data, c'est-à-dire du système global d'information en train d'émerger à l'échelle du Web. Cette expérience nous a permis de montrer que l'utilisation opérationnelle du Linked Data au sein d'une université est parfaitement envisageable avec cette approche. Cependant, certains problèmes persistent, comme (i) le respect des protocoles du Linked Data et (ii) le manque d'outils adaptés pour interroger le Linked Open Data avec SPARQL. Nous proposons des solutions à ces deux problèmes. Afin de pouvoir vérifier le respect d'un protocole SPARQL au sein du Linked Data d'une université, nous avons créé l'indicateur SPARQL Score qui évalue la conformité des services SPARQL avant leur déploiement dans le système d'information de l'université. De plus, pour aider les chercheurs à interroger le LOD, nous avons implémenté le démonstrateur SPARQLets-Finder qui démontre qu'il est possible de faciliter la conception de requêtes SPARQL à l'aide d'outils d'autocomplétion sans connaissance préalable des schémas RDF au sein du LOD.
Parmi les désignations qui fleurissent dans la presse économique sous l'appellation attractive de « new business models » , deux ont pour point commun de s'appuyer sur une logique de partage de biens : l' « économie de (la) fonctionnalité » et l ' « économie collaborative » . Elles articulent l'exploitation d'innovations technologiques récentes avec l'évolution de pratiques sociales. Ces approches entendent tirer profit d'une transformation contemporaine des modes de consommation, caractérisée par une désacralisation du rôle accordé aux biens matériels. Notre axe de recherche questionne la construction et la signification de plusieurs modèles socio-économiques a priori tournés vers un développement durable. Bien que la multiplication des désignations sème le trouble dans leurs définitions, chacune d'entre elles peut être reliée à des réseaux d'acteurs distincts. Si les expressions « économie de fonctionnalité » et « économie de la fonctionnalité » ne se distinguent que par un déterminant, elles renvoient à deux approches en tension. De même, alors que le terme « économie collaborative » évoque au moment de son émergence la bannière « peer-to-peer » , il se diffuse rapidement pour qualifier une forme de capitalisme connexionniste. Le déploiement des modèles étudiés permet de capter certaines transformations des représentations contemporaines. Le relatif succès des modèles est fonction de la correspondance des idéaux qui y sont attachés avec les faits socio-économiques éprouvés par les acteurs. Évolution des formes de travail, modification des contours de
Ce travail de recherche s'inscrit en sciences de l'information et de la communication et s'intéresse à l'émotion dans la recherche d'information à travers l'exemple des forums de santé. Le succès de ces dispositifs résulte d'une motivation informationnelle et émotionnelle des participants qui peuvent accéder à des témoignages, des informations ponctuelles ou encore des informations médicales filtrées par le vécu du malade (ou du proche de malade) qui s'exprime. Les messages sont donc souvent empreints d'émotion. La problématique s'attache aux évolutions de l'activité d'information et notamment au rôle que peuvent jouer les marqueurs d'émotion dans la structuration des informations mais également lors de leur évaluation. Une première analyse vise à mettre en évidence l'organisation des messages et des indices d'émotion grâce à une analyse de corpus (de fils de discussion provenant de différents forums de santé). Une seconde enquête s'attache à l'analyse de données recueillies pendant une phase d'entretiens et d'expérimentations sur l'utilisation des forums de santé et sur la manière dont les participants évaluent les informations. Les résultats montrent que les informations médicales sont très présentes et majoritairement entremêlées d'indices d'émotion de peur. Toutefois, la joie est l'émotion la plus présente dans l'ensemble du corpus. Enfin, si les marqueurs d'émotion sont des critères d'évaluation, il apparaît que les informations médicales sont également des indices d'évaluation et non les informations évaluées.
Cette thèse aborde le problème de l'apprentissage avec des fonctions de perte nonmodulaires. Pour les problèmes de prédiction, où plusieurs sorties sont prédites simultanément, l'affichage du résultat comme un ensemble commun de prédiction est essentiel afin de mieux incorporer les circonstances du monde réel. Dans la minimisation du risque empirique, nous visons à réduire au minimum une somme empirique sur les pertes encourues sur l'échantillon fini avec une certaine perte fonction qui pénalise sur la prévision compte tenu de la réalité du terrain. Dans cette thèse, nous proposons des méthodes analytiques et algorithmiquement efficaces pour traiter les fonctions de perte non-modulaires. D'abord, nous avons introduit une méthode pour les fonctions de perte supermodulaires, qui est basé sur la méthode d'orientation alternée des multiplicateurs, qui ne dépend que de deux problémes individuels pour la fonction de perte et pour l'infèrence. Deuxièmement, nous proposons une nouvelle fonction de substitution pour les fonctions de perte submodulaires, la Lovász hinge, qui conduit à une compléxité en O(p log p) avec O(p) oracle pour la fonction de perte pour calculer un gradient ou méthode de coupe. Enfin, nous introduisons un opérateur de fonction de substitution convexe pour des fonctions de perte nonmodulaire, qui fournit pour la première fois une solution facile pour les pertes qui ne sont ni supermodular ni submodular. Cet opérateur est basé sur une décomposition canonique submodulaire-supermodulaire.
Le travail que nous présentons dans cette thèse s'inscrit dans une problématique générale d'étude et de conception des MOOCs (Massive Open Online Courses). Elle s'intéresse plus particulièrement à l'étude didactique d'un MOOC d'algorithmique conçu au profit des étudiants de premier cycle de l'université d'Hassan Premier au Maroc. Ce travail se situe dans une approche compréhensive et vise plus précisément à comprendre le processus d'élaboration du contenu d'algorithmique véhiculé par le dispositif MOOC et la manière dont les étudiants le construisent au sein et en relation avec ce dispositif. En considérant le MOOC comme un dispositif didactique, deux approches didactique et épistémologique des activités d'apprentissage en algorithmique ont été articulées. La notion de performance didactique est mobilisée pour examiner les stratégies d'apprentissage adoptées par les étudiants. En recourant aux forums de discussion et en mobilisant un questionnaire et des entretiens semi-directifs, les discours des étudiants ont été analysés en vue de caractériser les contenus construits, les performances didactiques et les difficultés rencontrées. La caractérisation de la conception du MOOC met en évidence deux étapes :  1) l'identification des concepts incontournables en algorithmique : variable, instructions de base, conditions, boucles et leur organisation en unités d'apprentissage 2) la scénarisation pédagogique décrivant les tâches d'apprentissage des unités pédagogiques et leur organisation ; le cours est par ailleurs adapté à la massification des audiences notamment en diminuant la charge horaire des semaines du MOOC et en exigeant peu de prérequis. L'analyse des pratiques identifiées montre que les étudiants ont construit deux types de contenus, d'une part, des savoirs conceptuels (condition et boucle) et d'autre part des savoirs procéduraux (démarche de résolution d'un problème, exécution d'un algorithme) ; que les étudiants ont manifesté plus de performances didactiques cognitives et techniques que sociales pour construire le contenu. Plus particulièrement, la démarche de construction du contenu consiste : 1) en des stratégies cognitives d'élaboration telle que la mise en lien du contenu avec les connaissances antérieures et d'organisation à savoir l'utilisation des organigrammes pour construire pas à pas le savoir algorithmique ; 2) en des stratégies techniques en termes de mobilisations des vidéos du MOOC. Les résultats montrent également que même si les étudiants se sont avérés particulièrement performants dans l'analyse des problèmes (détermination des objets d'entrées et sorties), certaines difficultés subsistent, tels que le passage de la phase d'analyse d'un problème à celle d'élaboration de l'algorithme. Ce travail de thèse a l'ambition de proposer aux concepteurs pédagogiques des MOOCs des principes utiles pour l'élaboration d'un contenu, d'une part, et ouvrir une voie de recherche en didactiques, sur les dispositifs MOOCs, qui tient compte de la spécificité des contenus véhiculés.
Notre thèse se situe dans le domaine interdisciplinaire de la stylistique computationnelle, à savoir l'application des méthodes statistiques et computationnelles à l'étude du style littéraire. Historiquement, la plupart des travaux effectués en stylistique computationnelle se sont concentrés sur les aspects lexicaux. Dans notre thèse, l'accent est mis sur l'aspect syntaxique du style qui est beaucoup plus difficile à analyser étant donné sa nature abstraite. Comme contribution principale, dans cette thèse, nous travaillons sur une approche à l'étude stylistique computationnelle de textes classiques de littérature française d'un point de vue herméneutique, où découvrir des traits linguistiques intéressants se fait sans aucune connaissance préalable. Suivant la ligne de pensée herméneutique, nous proposons un processus de découverte de connaissances pour la caractérisation stylistique accentué sur la dimension syntaxique du style et permettant d'extraire des motifs pertinents à partir d'un texte donné. Ce processus proposé consiste en deux étapes principales, une étape d'extraction de motifs séquentiels suivi de l'application de certaines mesures d'intérêt. En particulier, l'extraction de tous les motifs syntaxiques possibles d'une longueur donnée est proposée comme un moyen particulièrement utile pour extraire des caractéristiques intéressantes dans un scénario exploratoire. Nous proposons, évaluons et présentons des résultats sur les trois mesures d'intérêt proposées, basée chacune sur un raisonnement théorique linguistique et statistique différent.
Cette thèse est une contribution à la description du kmhmouʔ – langue à tradition orale parlée au Laos. Elle présente les caractéristiques générales de cette langue peu décrite dans la région de l'Asie du Sud-Est comme le montre notre référence bibliographique. Le kmhmouʔ, langue isolante, est dépourvue de marqueurs grammaticaux morphologiques. Les mots du kmhmouʔ sont pour la plupart pluricatégoriels et plurifonctionnels : la distinction verbo-nominale se fonde essentiellement sur des critères combinatoires. La description grammaticale s'appuie sur des données spontanées recueillies lors d'un travail de terrain dans les villages kmhmouʔ et à la radio nationale lao pour l'émission en kmhmouʔ. Le travail de terrain a été conduit en kmhmouʔ, et l'analyse et l'interprétation du corpus ont bénéficié du fait que l'auteur est locuteur natif. Cette thèse, outre qu'elle rend disponible pour la première fois les données et la grammaire du dialecte du kmhmouʔ de l'Est, ouvre des pistes de réflexion particulièrement intéressantes pour les travaux en typologie des langues isolantes et de la transcatégorialité, ainsi que la réflexion sur le rôle du contact de langue dans la grammaticalisation.
Au cours de ces dernières années, la sécurité des Systèmes d'Information (SI) est devenue une préoccupation importante, qui doit être prise en compte dans toutes les phases du développement du SI, y compris dans la phase initiale de l'ingénierie des exigences (IE). Prendre en considération la sécurité durant les premières phases du développement des SI permet aux développeurs d'envisager les menaces, leurs conséquences et les contre-mesures avant qu'un système soit mis en place. Les exigences de sécurité sont connues pour être "les plus difficiles des types d'exigences", et potentiellement celles qui causent le plus de risque si elles ne sont pas correctes. De plus, les ingénieurs en exigences ne sont pas principalement intéressés à, ou formés sur la sécurité. Leur connaissance tacite de la sécurité et leur connaissance primitive sur le domaine pour lequel ils élucident des exigences de sécurité rendent les exigences de sécurité résultantes pauvres et trop génériques. Cette thèse explore l'approche de l'élucidation des exigences fondée sur la réutilisation de connaissances explicites. Tout d'abord, la thèse propose une étude cartographique systématique et exhaustive de la littérature sur la réutilisation des connaissances dans l'ingénierie des exigences de sécurité identifiant les différentes formes de connaissances. Suivi par un examen et une classification des ontologies de sécurité comme étant la principale forme de réutilisation. Dans la deuxième partie, AMAN-DA est présentée. AMAN-DA est la méthode développée dans cette thèse. Elle permet l'élucidation des exigences de sécurité d'un système d'information spécifique à un domaine particulier en réutilisant des connaissances encapsulées dans des ontologies de domaine et de sécurité. En outre, la thèse présente les différents éléments d'AMAN-DA : (I) une ontologie de sécurité noyau, (II) une ontologie de domaine multi-niveau, (iii) des modèles syntaxique de buts et d'exigences de sécurité, (IV) un ensemble de règles et de mécanismes nécessaires d'explorer et de réutiliser la connaissance encapsulée dans les ontologies et de produire des spécifications d'exigences de sécurité. La dernière partie rapporte l'évaluation de la méthode. AMAN-DA a été implémenté dans un prototype d'outil. Sa faisabilité a été évaluée et appliquée dans les études de cas de trois domaines différents (maritimes, applications web, et de vente). La facilité d'utilisation et l'utilisabilité de la méthode et de son outil ont également été évaluées dans une expérience contrôlée. L'expérience a révélé que la méthode est bénéfique pour l'élucidation des exigences de sécurité spécifiques aux domaines, et l'outil convivial et facile à utiliser.
Le développement et la multiplication des systèmes et plateformes informatiques pour accéder à de l'information ne fait que s'accentuer depuis une trentaine d'années. Le grand volume d'information disponible a soulevé de nombreux défis scientifiques dans des domaines tel que la recherche d'information. Pour accéder à des documents regroupés dans un corpus numérique, il faut être en mesure d'exprimer son besoin en information, souvent sous la forme d'une requête, d'y associer les documents pertinents et de les présenter de la meilleure manière possible aux utilisateurs. La recherche documentaire dans un corpus documentaire thématique présentant un haut niveau de technicité dans la discipline concernée s'apparente à un processus de navigation guidé par un besoin d'information d'un utilisateur. Cette navigation nécessite l'usage d'outils classiques de recherche d'information pour sélectionner des documents pertinents en fonction d'une requête, mais ils doivent être complétés par des mécanismes de personnalisation et d'adaptation capable de faire évoluer la représentation du besoin en fonction des spécificités d'un utilisateur, de sa navigation en cours ou du corpus considéré. En effet, l'accès aux documents d'un corpus numérique soulève des problèmes liés à la recherche d'information, à la visualisation des résultats d'une requête et à la navigation entre les documents. Le processus de recherche d'information nécessite des améliorations et surtout l'intégration de l'utilisateur comme facteur principal à prendre en compte dans la recherche de satisfaction de son besoin informationnel. Nous considérons plusieurs approches pour aider un utilisateur dans sa recherche de documents. Une première assistance porte sur la reformulation de requêtes en visant un public d'utilisateurs peu familier avec les termes techniques du domaine et en difficulté pour exprimer sous la forme d'une requête leur besoin. La deuxième approche que nous proposons consiste à ne pas considérer l'utilisateur isolément mais à le rapprocher de ceux ayant exprimé des recherches similaires pour retrouver les documents qu'ils avaient jugés pertinents. Enfin, nous incluons des travaux issus du domaine de la recommandation afin de mieux cerner le besoin informationnel de l'utilisateur et l'aider à trouver plus facilement ce qu'il cherche en lui recommandant des ressources documentaires. Nous proposons dans cette thèse de traiter cette diversité d'influence par un système multi-agent interagissant par un environnement partagé représentant la navigation des utilisateurs, de manière à pouvoir adapter le système en utilisant l'une ou l'autre des techniques d'assistance proposées en fonction de l'expertise de l'utilisateur. Ce travail a été appliqué à une recherche documentaire dans un corpus numérique de documents juridiques.
Les robots sont de plus en plus utilisés dans un cadre social. Il ne suffit plusde partager l'espace avec des humains, mais aussi d'interagir avec eux. Dans ce cadre, il est attendu du robot qu'il comprenne un certain nombre de signaux ambiguës, verbaux et visuels, nécessaires à une interaction humaine. En particulier, on peut extraire beaucoup d'information, à la fois sur l'état d'esprit des personnes et sur la dynamique de groupe à l'œuvre, en connaissant qui ou quoi chaque personne regarde. On parle de la Cible d'attention visuelle, désignée par l'acronyme anglais VFOA. Dans cette thèse, nous nous intéressons aux données perçues par un robot humanoı̈de qui participe activement à une in-teraction sociale, et à leur utilisation pour deviner ce que chaque personne regarde. D'une part, le robot doit “regarder les gens”, à savoir orienter sa tête (et donc la caméra) pour obtenir des images des personnes présentes. Nous présentons une méthode originale d'apprentissage par renforcement pour contrôler la direction du regard d'un robot. Cette méthode utilise des réseaux de neurones récurrents. Le robot s'entraı̂ne en autonomie à déplacer sa tête enfonction des données visuelles et auditives. Il atteint une stratégie efficace, qui lui permet de cibler des groupes de personnes dans un environnement évolutif. D'autre part, les images du robot peuvent être utilisée pour estimer les VFOAs au cours du temps. Pour chaque visage visible, nous calculons laposture 3D de la tête (position et orientation dans l'espace) car très fortement corrélée avec la direction du regard. Nous l'utilisons dans deux applications. Premièrement, nous remarquons que les gens peuvent regarder des objets qui ne sont pas visible depuis le point de vue du robot. Sous l'hypothèse que les dits objets soient regardés au moins une partie du temps, nous souhaitons estimer leurs positions exclusivement à partir de la direction du regard des personnes visibles. Nous utilisons une représentation sous forme de carte de chaleur. Nous avons élaboré et entraı̂né plusieurs réseaux de convolutions afin d'estimer la régression entre une séquence de postures des têtes, et les positions des objets. Nous présentons alors un modèle probabiliste, suggéré par des résultats en psychophysique, afin de modéliser la relation entre les postures des têtes, les positions des objets, la direction du regard et les VFOAs. La formulation utilise un modèle markovien à dynamiques multiples. En appliquant une approches bayésienne, nous obtenons un algorithme pour calculer les VFOAs au fur et à mesure, et une méthode pour estimer les paramètres du modèle. Nos contributions reposent sur la possibilité d'utiliser des données, afin d'exploiter des approches d'apprentissage automatique. Toutes nos méthodes sont validées sur des jeu de données disponibles publiquement. De plus, la génération de scénarios synthétiques permet d'agrandir à volonté la quantité de données disponibles ; les méthodes pour simuler ces données sont explicitement détaillée.
Acquérir l'adjectif épithète pose deux problèmes majeurs en français. D'abord, l'adjectif dénote une propriété à propos d'un nom, les enfants doivent donc pouvoir concevoir un objet comme un tout et comme un ensemble de propriétés pour manier un SN avec épithète. De plus, bien que les locuteurs connaissent cette possibilité, ils optent plutôt pour un placement fixe en usage. Ces faits nous ont amenée à nous demander si l'input permet à l'enfant de se construire la notion d'adjectif épithète sans avoir recours à des connaissances langagières innées. Pour y répondre, nous proposons une étude comparant les usages de trois enfants à ceux de leur famille à deux temps de leur acquisition (T1 : 3 ; 8, T2 : 4 ; 6). Nous étudions quatre aspects de l'usage de l'épithète (lexique, placement, combinaison avec d'autres modifieurs ou un dépendant adjectival) et nous confrontons l'adjectif aux autres modifieurs nominaux. Ces phénomènes montrent tous la même évolution. À T1, les enfants emploient la construction la plus fréquente des adultes, avec un fort degré de spécificité lexicale. À T2, d'autres constructions émergent selon leur ordre de fréquence chez les adultes. Le lexique de la construction de T1 s'est en outre élargi dans le champ de la classe sémantique des usages de T1. Les enfants montrent ainsi une sensibilité aux informations quantitatives et une abstraction graduelle des structures par analogie sémantique, qui plaident pour une construction progressive de la notion d'adjectif épithète à partir de l'input.
En conséquence, une grande partie des événements quotidiens a vocation à être numérisée. Dans ce cadre, le Web contient des descriptions de divers événements du monde réel et provenant du monde entier. L'ampleur de ces événements peut varier, allant de ceux pertinents uniquement localement à ceux qui retiennent l'attention du monde entier. La presse et les médias sociaux permettent d'atteindre une diffusion presque mondiale. L'ensemble de toutes ces données décrivant des événements sociétaux potentiellement complexes ouvre la porte à de nombreuses possibilités de recherche pour analyser et mieux comprendre l'état de notre société. Dans cette thèse, nous étudions diverses tâches d'analyse de l'impact des événements sociétaux. Plus précisément, nous abordons trois facettes dans le contexte des événements et du Web, à savoir la diffusion d'événements dans des communautés de langues étrangères, la classification automatisée des contenus Web et l'évaluation et la visualisation de la viralité de l'actualité. Nous émettons l'hypothèse que les entités nommées associées à un événement ou à un contenu Web contiennent des informations sémantiques précieuses, qui peuvent être exploitées pour créer des modèles de prédiction précis. À l'aide de nombreuses études, nous avons montré que l'élévation du contenu Web au niveau des entités saisissait leur essence essentielle et offrait ainsi une variété d'avantages pour obtenir de meilleures performances dans diverses tâches. Nous exposons de nouvelles découvertes sur des tâches disparates afin de réaliser notre objectif global en matière d'analyse de l'impact des événements sociétaux.
L'apprentissage machine est l'étude de la conception d'algorithmes qui apprennent à partir des données d'apprentissage pour réaliser une tâche spécifique. Le modèle résultant est ensuite utilisé pour prédire de nouveaux points de données (invisibles) sans aucune aide extérieure. Ces données peuvent prendre de nombreuses formes telles que des images (matrice de pixels), des signaux (sons,...), des transactions (âge, montant, commerçant,...), des journaux (temps, alertes,...). Les ensembles de données peuvent être définis pour traiter une tâche spécifique telle que la reconnaissance d'objets, l'identification vocale, la détection d'anomalies, etc. Dans ces tâches, la connaissance des résultats escomptés encourage une approche d'apprentissage supervisé où chaque donnée observée est assignée à une étiquette qui définit ce que devraient être les prédictions du modèle. Par exemple, dans la reconnaissance d'objets, une image pourrait être associée à l'étiquette "voiture" qui suggère que l'algorithme d'apprentissage doit apprendre qu'une voiture est contenue dans cette image, quelque part. Cela contraste avec l'apprentissage non supervisé où la tâche à accomplir n'a pas d'étiquettes explicites. Par exemple, un sujet populaire dans l'apprentissage non supervisé est de découvrir les structures sous-jacentes contenues dans les données visuelles (images) telles que les formes géométriques des objets, les lignes, la profondeur, avant d'apprendre une tâche spécifique. Ce type d'apprentissage est évidemment beaucoup plus difficile car il peut y avoir un nombre infini de concepts à saisir dans les données. Dans cette thèse, nous nous concentrons sur un scénario spécifique du cadre d'apprentissage supervisé : 1) l'étiquette d'intérêt est sous-représentée (p. ex. anomalies) et 2) l'ensemble de données augmente avec le temps à mesure que nous recevons des données d'événements réels (p. ex. transactions par carte de crédit). En fait, ces deux problèmes sont très fréquents dans le domaine industriel dans lequel cette thèse se déroule.
Avec le développement de la robotique grand public apparaît une nouvelle forme de télécommunication  : la robotique de téléprésence. Le principe consiste à représenter une personne à distance par l'intermédiaire d'un robot mobile, dont elle peut contrôler librement les déplacements. L'objectif n'est pas simplement de lui permettre de communiquer à distance, mais de lui donner une présence physique et sociale, que le téléphone ou la visioconférence ne suffisent pas à transmettre. Dans ce contexte, il est particulièrement important de parvenir à transmettre au mieux le «  toucher social  » du pilote du robot  : c'est-à-dire lui permettre d'échanger avec ses interlocuteurs un vaste ensemble de signaux socio-affectifs, qui sont les vecteurs du lien social. En particulier, cette thèse s'intéresse à un élément fondamental du toucher social et fortement impacté par la téléprésence  : la portée vocale, à travers laquelle un locuteur contrôle qui peut l'entendre, et s'adapte en permanence aux conditions acoustiques de l'environnement. À travers une première étude, nous nous intéresserons au lien entre toucher vocal et proxémie, en nous demandant si la manière dont un auditeur perçoit à l'aveugle un interlocuteur dans l'espace peut être influencée par les socio-affects produits par celui-ci. Ensuite, nous montrerons que la portée vocale peut-être affectée par effet Lombard en cas de téléprésence ubiquïte  : le pilote, qui perçoit à la fois son environnement local, et l'environnement du robot, s'adapte au niveau de bruit ambiant, même lorsque ce bruit n'est pas perçu par ses interlocuteurs. Enfin, nous présenterons notre participation à un projet Arts et Sciences  : le spectacle Aporia, au cours duquel un acteur unique, aidé d'un logiciel de transformation vocale, incarne plusieurs personnages.
Un grand nombre de méthodes de mesure ont ainsi été mises au point afin de la mesurer, mais demeurent immatures, voire contradictoires entre elles. C'est pourquoi un travail mérite d'être mené afin d'en augmenter la validité et la fiabilité. Ce travail de thèse s'est attaché donc à utiliser plusieurs de ces méthodes, de les combiner et les articuler selon différentes techniques afin d'améliorer la qualité de la mesure. Nous nous sommes appuyés pour cela sur un large spectre d'indicateurs, d'ordre physiologiques, comportementaux et auto rapportés et de deux stratégies de triangulation en particulier : multi-facettes et multi-mesures. Enfin, ces méthodes ont testées dans des cas d'application réels et selon une complexification croissante des procédures et traitements statistiques. Cela a donné lieu à trois études distinctes. La première a consisté à évaluer la pertinence d'un algorithme de recommandation de films face à son concurrent en utilisant une stratégie d'évaluation multi-facettes. Une deuxième étude a été élaborée afin de tester la pertinence de modèles d'évaluation multi-mesures, en évaluant l'utilisabilité de sites universitaires grâce à un logiciel de test utilisateur à distance (Evalyzer) et la combinaison multimodale de divers indicateurs d'utilisabilité. Enfin, une dernière étude a été réalisée afin de valider un protocole de mesure d'immersion multi-mesure (questionnaire, expression faciale, conductance de la peau, rythme cardiaque, comportement oculaire). Ces trois études nous ont permis d'évaluer la pertinence d'un certain nombre de mesures (d'utilisabilité et d'expérience utilisateur), la valeur ajoutée de certaines de leurs combinaisons, ainsi qu'un retour critique sur la procédure de validation multi-facettes utilisée dans cette thèse
De nos jours, il existe de nombreuses bases de données géographiques (BDG) couvrant le même territoire. Les données géographiques sont modélisées différemment (par exemple une rivière peut être modélisée par une ligne ou bien par une surface), elles sont destinées à répondre à plusieurs applications (visualisation, analyse) et elles sont créées suivant des modes d'acquisition divers (sources, processus). Tous ces facteurs créent une indépendance entre les BDG, qui pose certains problèmes à la fois aux producteurs et aux utilisateurs. Ainsi, une solution est d'expliciter les relations entre les divers objets des bases de données, c'est-à-dire de mettre en correspondance des objets homologues représentant la même réalité. La complexité du processus d'appariement fait que les approches existantes varient en fonction des besoins auxquels l'appariement répond, et dépendent des types de données à apparier (points, lignes ou surfaces) et du niveau de détail. Nous avons remarqué que la plupart des approches sont basées sur la géométrie et les relations topologiques des objets géographiques et très peu sont celles qui prennent en compte l'information descriptive des objets géographiques. De plus, pour la plupart des approches, les critères sont enchaînés et les connaissances sont à l'intérieur du processus. Suite à cette analyse, nous proposons une approche d'appariement de données qui est guidée par des connaissances et qui prend en compte tous les critères simultanément en exploitant à la fois la géométrie, l'information descriptive et les relations entre eux. Afin de formaliser les connaissances et de modéliser leurs imperfections (imprécision, incertitude et incomplétude), nous avons utilisé la théorie des fonctions de croyance [Shafer, 1976]. Notre approche d'appariement de données est composée de cinq étapes : après une sélection des candidats, nous initialisons les masses de croyance en analysant chaque candidat indépendamment des autres au moyen des différentes connaissances exprimées par divers critères d'appariement. Ensuite, nous fusionnons les critères d'appariement et les candidats. Enfin, une décision est prise. Nous avons testé notre approche sur des données réelles ayant des niveaux de détail différents représentant le relief (données ponctuelles) et les réseaux routiers (données linéaires)
Les travaux de cette thèse portent sur la modélisation des émotions pour la synthèse audiovisuelle expressive de la parole à partir du texte. Aujourd'hui, les résultats des systèmes de synthèse de la parole à partir du texte sont de bonne qualité, toutefois la synthèse audiovisuelle reste encore une problématique ouverte et la synthèse expressive l'est encore d'avantage. Nous proposons dans le cadre de cette thèse une méthode de modélisation des émotions malléable et flexible, permettant de mélanger les émotions comme on mélange les teintes sur une palette de couleurs. Dans une première partie, nous présentons et étudions deux corpus expressifs que nous avons construits. La stratégie d'acquisition ainsi que le contenu expressif de ces corpus sont analysés pour valider leur utilisation à des fins de synthèse audiovisuelle de la parole. Dans une seconde partie, nous proposons deux architectures neuronales pour la synthèse de la parole. Nous avons utilisé ces deux architectures pour modéliser trois aspects de la parole : 1) les durées des sons, 2) la modalité acoustique et 3) la modalité visuelle. Dans un premier temps, nous avons adopté une architecture entièrement connectée. Cette dernière nous a permis d'étudier le comportement des réseaux de neurones face à différents descripteurs contextuels et linguistiques. Nous avons aussi pu analyser, via des mesures objectives, la capacité du réseau à modéliser les émotions. La deuxième architecture neuronale proposée est celle d'un auto-encodeur variationnel. Cette architecture est capable d'apprendre une représentation latente des émotions sans utiliser les étiquettes des émotions. Après analyse de l'espace latent des émotions, nous avons proposé une procédure de structuration de ce dernier pour pouvoir passer d'une représentation par catégorie vers une représentation continue des émotions. Nous avons pu valider, via des expériences perceptives, la capacité de notre système à générer des émotions, des nuances d'émotions et des mélanges d'émotions, et cela pour la synthèse audiovisuelle expressive de la parole à partir du texte.
De nos jours, les informations liées au déplacement et à la mobilité dans un réseau de transport représentent sans aucun doute un potentiel important. Ces travaux visent à mettre en œuvre un Système d'Information de Service d'Aide à la Mobilité Urbaine (SISAMU). Le SISAMU doit pouvoir procéder par des processus de décomposition des requêtes simultanées en un ensemble de tâches indépendantes. Chaque tâche correspond à un service qui peut être proposé par plusieurs fournisseurs d'information en concurrence, avec différents coûts, temps de réponse et formats. L'aspect dynamique, distribué et ouvert du problème, nous a conduits à adopter une modélisation multi-agent pour assurer au système une évolution continue et une flexibilité pragmatique. Pour ce faire, nous avons proposé d'automatiser la modélisation des services en utilisant la notion d'ontologie. Notre SISAMU prend en considération les éventuelles perturbations sur le RETM. Ansi, nous avons créé un protocole de négociation entre les agents. Le protocole de négociation proposé qui utilise l'ontologie de la cartographie se base sur un système de gestion des connaissances pour soutenir l'hétérogénéité sémantique. Nous avons détaillé l'Algorithme de Reconstruction Dynamique des Chemins des Agents (ARDyCA) qui est basé sur l'approche de l'ontologie cartographique. Finalement, les résultats présentés dans cette thèse justifient l'utilisation de l'ontologie flexible et son rôle dans le processus de négociation
Cette thèse vise à comprendre le risque de fuite d'informations personnelles sur un réseau social. Nous étudions les violations potentielles de la vie privée, concevons des attaques, prouvons leur faisabilité et analysons leur précision. Cette approche nous aide à identifier l'origine des menaces et constitue un premier pas vers la conception de contre-mesures efficaces. Nous avons d'abord introduit une mesure de sensibilité des sujets à travers une enquête par questionnaire. Puis, nous avons conçu des attaques de divulgation (avec certitude) des liens d'amitié et des liens d'appartenance aux groupes sur “Facebook”. Ces attaques permettent de découvrir le réseau local d'une cible en utilisant uniquement des requêtes légitimes. Nous avons également conçu une technique d'échantillonnage pour collecter rapidement des données utiles autour d'une cible. Les données collectées sont ensuite représentées par des graphes et utilisées pour effectuer des inférences d'attributs (avec incertitude). Pour augmenter la précision des attaques, nous avons conçu des algorithmes de nettoyage. Ces algorithmes quantifient la corrélation entre les sujets, sélectionnent les plus pertinents et permettent de gérer la rareté (sparsity) des données. Enfin, nous avons utilisé un réseau de neurones pour classer les données et déduire les valeurs secrètes d'un attribut sensible d'une cible donnée avec une précision élevée mesurée par AUC sur des données réelles. Les algorithmes proposés dans ce travail sont inclus dans un système appelé SONSAI qui aide les utilisateurs finaux à contrôler la collecte d'informations sur leur vie privée
Lors du développement de logiciels, la maintenance et l'évolution constituent une partie importante du cycle de vie du développement représentant 80% du coût et des efforts globaux. Au cours de la maintenance, il arrive que les développeurs aient copier-coller des fragments de code source afin de les réutiliser. Une telle pratique, apparemment inoffensive, est plus fréquente qu'on ne le pense. Communément appelés « clones » dans la littérature, ces doublons de code source sont un sujet bien connu et étudié en génie logiciel. Dans cette thèse, nous visons à mettre en lumière les pratiques du copier-coller sur les artefacts logiciels. En particulier, nous avons choisi de concentrer nos contributions sur deux types d'artefacts logiciels : Documentation d'API et fichiers de compilation (c.-à-d. Dockerfiles). Pour les deux contributions, nous suivons une méthodologie d'étude empirique commune. Tout d'abord, nous montrons que les documentations d'API et les fichiers de construction de logiciels (c.-à-d. Dockerfiles) sont confrontés à des problèmes de doublons et que de tels doublons sont fréquents. Deuxièmement, nous identifions les raisons derrière l'existence de ces doublons. Enfin, nous montrons que les deux artefacts logiciels manquent de mécanismes de réutilisation pour faire face aux doublons, et que certains développeurs ont même recours à des outils ad-hoc pour les gérer.
Ce document propose d'apprendre le comportement d'un système à partir d'un ensemble de dialogues annotés. Le système apprend un comportement optimal via l'apprentissage par renforcement. Nous montrons qu'il n'est pas nécessaire de définir une représentation de l'espace d'état ni une fonction de récompense. En effet, ces deux paramètres peuvent être appris à partir du corpus de dialogues annotés. Nous montrons qu'il est possible pour un développeur de systèmes de dialogue d'optimiser la gestion du dialogue en définissant seulement la logique du dialogue ainsi qu'un critère à maximiser (par exemple, la satisfaction utilisateur). La première étape de la méthodologie que nous proposons consiste à prendre en compte un certain nombre de paramètres de dialogue afin de construire une représentation de l'espace d'état permettant d'optimiser le critère spécifié par le développeur. Par exemple, si le critère choisi est la satisfaction utilisateur, il est alors important d'inclure dans la représentation des paramètres tels que la durée du dialogue et le score de confiance de la reconnaissance vocale. L'espace d'état est modélisé par une mémoire sparse distribuée. Notre modèle, Genetic Sparse Distributed Memory for Reinforcement Learning (GSDMRL), permet de prendre en compte de nombreux paramètres de dialogue et de sélectionner ceux qui sont importants pour l'apprentissage par évolution génétique. L'espace d'état résultant ainsi que le comportement appris par le système sont aisément interprétables. Ces deux méthodes interprètent le critère à optimiser comme étant la récompense globale pour chaque dialogue. Nous comparons ces deux fonctions sur un ensemble de dialogues simulés et nous montrons que l'apprentissage est plus rapide avec ces fonctions qu'en utilisant directement le critère comme récompense finale. Nous avons développé un système de dialogue dédié à la prise de rendez-vous et nous avons collecté un corpus de dialogues annotés avec ce système. Ce corpus permet d'illustrer la capacité de mise à l'échelle de la représentation de l'espace d'état GSDMRL et constitue un bon exemple de système industriel sur lequel la méthodologie que nous proposons pourrait être appliquée
Au vu de l'émergence rapide des nouvelles technologies mobiles et la croissance des offres et besoins d'une société en mouvement en formation, les travaux se multiplient pour identifier de nouvelles plateformes d'apprentissage pertinentes afin d'améliorer et faciliter le processus d'apprentissage à distance. La prochaine étape de l'apprentissage à distance est naturellement le port de l'apprentissage électronique vers les nouveaux systèmes mobiles. Jusqu'à présent l'environnement d'apprentissage était soit défini par un cadre pédagogique soit imposé par le contenu d'apprentissage. Maintenant, nous cherchons, à l'inverse, à adapter le cadre pédagogique et le contenu d'apprentissage au contexte de l'apprenant.
Une plateforme de conception est une solution totale qui permet à une équipe de conception de développer un système sur puce. Une telle plateforme se compose d'un ensemble de bibliothèques et de circuits réutilisables (IPs), d'outils de CAO et de kits de conception en conformité avec les flots de conception et les méthodologies supportés. Les spécifications de ce type de plateforme offrent un large éventail d'informations, depuis des paramètres de technologie, jusqu'aux informations sur les outils. En outre, les développeurs de bibliothèque/IP ont des difficultés à obtenir les données nécessaires à partir ces spécifications en raison de leur informalité et complexité. Dans cette thèse, nous proposons des méthodologies, des flots et des outils pour formaliser les spécifications d'une plateforme de conception et les traiter. Cette description proposée vise à être utilisée comme une référence pour générer et valider les bibliothèques et les IPs. De plus, nous présentons une méthode basée sur des références pour créer une spécification fiable en LDSpecX et des mots-clés basés sur des tâches pour en extraire les données efficacement. A l'aide des solutions proposées, nous développons une plateforme de spécification. Nous développons une bibliothèque de cellules standard en utilisant cette plateforme de spécification. Nous montrons ainsi que notre approche permet de créer une spécification complète et cohérente avec une réduction considérable du temps. Cette proposition comble également l'écart entre les spécifications et le système automatique existant pour le développement rapide de bibliothèques/IPs.
Tous les travaux de cette thèse ont été développés dans le contexte du Cherenkov Telescope Array (CTA), qui sera le principal observatoire de la prochaine génération pour l'astronomie gamma à très haute énergie au sol. Le plan de ce travail est d'utiliser les GPU et le Cloud Computing afin d'accélérer les tâches de calcul exigeantes, en développant et en optimisant les pipelines d'analyse de données. La thèse se compose de deux parties : la première est destinée à l'estimation des performances du CTA pour l'observation de phénomènes violents tels que ceux générant des sursauts de rayons gamma (GRB) et des ondes gravitationnelles, avec un premier travail effectué pour la création des modèles pour le premier CTA Data Challenge (DC1). La deuxième partie de la thèse est liée au développement des pipelines pour la reconstruction des données de bas niveau provenant des simulations de Monte Carlo. Dans le chapitre 1, je présente les détails du projet CTA, les télescopes et les performances du réseau, ainsi que les méthodes utilisées pour les dériver des simulations de Monte Carlo. Plus de 500 AGNs ont été modélisées pour le DC1, qui a été important à la fois pour impliquer davantage de personnes dans l'analyse des données du CTA et pour calculer le temps d'observation nécessaire aux différents KSP. Les simulations pour les papier du Consortium sur les ondes gravitationnelles et les sursauts gamma ont été créés avec le pipeline ctools_pipe (présenté au chapitre 4), mis en œuvre autour des bibliothèques ctools et gammalib. Le pipeline est composé de deux parties : la tâche à exécuter (simulation de fond, création de modèle et la partie qui effectue la détection) et dans quel centre de calcul. La deuxième partie de la thèse est axée sur le développement et l'optimisation des pipelines d'analyse à utiliser pour la reconstruction d'événements à partir de données brutes simulées et pour la visualisation des événements dans un espace 3D. Ces analyses ont été réalisées à l'aide de ctapipe, un framework pour le prototypage des algorithmes de traitement de données de bas niveau pour CTA. La structure de la bibliothèque est présentée dans le chapitre 5, avec un accent particulier sur les méthodes de reconstruction qui sont mises en œuvre dans ctapipe, y compris le système ImPACT. Cette méthode utilise un modèle d'images créé à partir des simulations de Monte Carlo et une "seed" de la méthode de reconstruction standard pour s'adapter entre les modèles afin de trouver une meilleure estimation des paramètres de la gerbe atmosphérique. Le profilage temporel et les stratégies adoptées pour optimiser le pipeline ImPACT sont présentés au chapitre 6. L'implémentation d'un pipeline pour l'analyse de l'observation du Large Size Telescope en mode monoscopique et son implémentation GPU avec PyTorch est également présentée. ctapipe a également été utilisé et développé pour estimer les performances du CTA lors d'observations en mode "pointage divergent", dans lequel les directions de pointage sont légèrement différentes par rapport au mode de pointage parallèle, de sorte que l'hyper champ de vision final de tous les télescopes est plus grand par rapport au mode de pointage parallèle. Les résolutions angulaires et énergétiques ainsi que la sensibilité sont moins bonnes dans ce scénario, mais le fait d'avoir un hyper field-of-view plus large peut être bénéfique pour d'autres sujets, comme la recherche de sources transitoires. Les modifications du code de reconstruction introduites dans ctapipe et certains tracés de résolution angulaire pour les gammas de source ponctuelle simulés sont présentés au chapitre 7.Les résultats présentés dans cette thèse sont une démonstration de l'utilisation de techniques logicielles avancées en astrophysique de très haute énergie.
Les discrétisations adaptatives sont importantes dans les problèmes de flux compressible/incompressible puisqu'il est souvent nécessaire de résoudre desdétails sur plusieurs niveaux, en permettant de modéliser de grandes régions d'espace en utilisant un nombre réduit de degrés de liberté (et en réduisant le temps de calcul). Il existe une grande variété de méthodes de discrétisation adaptative, maisles grilles cartésiennes sont les plus efficaces, grâce à leurs stencils numériques simples et précis et à leurs performances parallèles supérieures. Et telles performance et simplicité sont généralement obtenues en appliquant un schéma de différences finies pour la résolution des problèmes, mais cette approche de discrétisation ne présente pas, au contraire, un chemin facile d'adaptation. Dans un schéma de volumes finis, en revanche, nous pouvons incorporer différent stypes de maillages, plus appropriées aux raffinements adaptatifs, en augmentant la complexité sur les stencils et en obtenant une plus grande flexibilité. L'opérateur de Laplace est un élément essentiel des équations de Navier-Stokes,un modèle qui gouverne les écoulements de fluides, mais il se produit également dans des équations différentielles qui décrivent de nombreux autres phénomènes physiques, tels que les potentiels électriques et gravitationnels. Il s'agit donc d'un opérateur différentiel très important, et toutes les études qui ont été effectuées sur celui-ci, prouvent sa pertinence. Dans ce travail seront présentés des approches de différences finies et devolumes finis 2D pour résoudre l'opérateur laplacien, en appliquant des patchs de grilles superposées où un niveau plus fin est nécessaire, en laissant des maillages plus grossiers dans le reste du domaine de calcul. Ces grilles superposées auront des formes quadrilatérales génériques. Plus précisément, les sujets abordés seront les suivants : 1) introduction à la méthode des différences finies, méthode des volumes finis, partitionnement des domaines, approximation de la solution ; 2) récapitulatif des différents types de maillages pour représenter de façon discrète la géométrie impliquée dans un problème, avec un focussur la structure de données octree, présentant PABLO et PABLitO. Le premier est une bibliothèque externe utilisée pour gérer la création de chaque grille, l'équilibrage de charge et les communications internes, tandis que la seconde est l'API Python de cette bibliothèque, écrite ad hoc pour le projet en cours ;  3) la présentation de l'algorithme utilisé pour communiquer les données entreles maillages (en ignorant chacune l'existence de l'autre) en utilisant lesintercommunicateurs MPI et la clarification de l'approche monolithique appliquéeà la construction finale de la matrice pour résoudre le système, en tenantcompte des blocs diagonaux, de restriction et de prolongement ;  4) la présentation de certains résultats ; conclusions, références. Il est important de souligner que tout est fait sous Python comme framework de programmation, en utilisant Cython pour l'écriture de PABLitO, MPI4Py pour les communications entre grilles, PETSc4py pour les parties assemblage et résolution du système d'inconnues, NumPy pour les objets à mémoire continue. Le choix de ce langage de programmation a été fait car Python, facile à apprendre et à comprendre, est aujourd'hui un concurrent significatif pour l'informatique numérique et l'écosystème HPC, grâce à son style épuré, ses packages, ses compilateurs et pourquoi pas ses versions optimisées pour des architectures spécifiques.
Les invasions biologiques font partie des changements globaux qui contribuent à la perte de biodiversité. Les plantes invasives peuvent aussi provoquer des pertes économiques, notamment d'importants coûts pour leur contrôle. Dans l'archipel des Mascareignes plusieurs programmes de gestion ont été mis en place pour contrôler les espèces invasives. La perception du public sur les espèces invasives varie fortement entre les parties prenantes. En raison d'opinions divergentes sur la gestion des invasions, les travaux de recherche et de mise en œuvre de programme de lutte ont récemment généré des conflits d'usage. Ce travail de thèse a permis de conduire une étude pluridisciplinaire sur les dimensions socio-écologiques et économiques de la gestion de Rubus alceifolius, objet d'un programme de contrôle biologique à l'île de La Réunion. Nous avons mené une analyse économique des différentes options de gestion de R. alceifolius et des coûts futurs de son invasion. Nous avons évalué l'impact de la lutte biologique sur le rétablissement des espèces indigènes dans une aire protégée. Le succès économique et écologique du programme de contrôle biologique de R. alceifolius a été démontré dans les habitats d'altitude &lt ; 800 m. Afin de comprendre la raison des conflits entre les parties prenantes nous avons parallèlement développé un travail de recherche socio-anthropologique. Nous avons pu mettre en évidence des faiblesses dans le processus de prise de décision et de mise en œuvre collective de ce programme de lutte.
Nous abordons d'abord le problème de la représentation sous forme d'un graphe de l'image et de ses applications en reconnaissance des formes, en mettant l'accent sur les applications de recherche d'images basées sur le contenu (CBIR). Les images utilisées dans cette thèse sont des images de bandes dessinées, qui possèdent des spécificités qui sont des freins pour les méthodes de recherche d'information par le contenu utilisées dans la littérature. Nous proposons ainsi une représentation qui permet d'obtenir des graphes stables et qui conserve des informations structurelles de haut niveau pour les objets d'intérêt dans les images de bandes dessinées. Ensuite, nous étendons le problème d'indexation et d'appariement aux structures de graphes représentant les images d'une bande dessinée et nous l'appliquons au problème de la recherche d'information. Un album de bandes dessinées est ainsi transformé en une base de graphes, chaque graphe correspondant à la description d'une seule case. La stratégie utilisée pour retrouver un objet ou un personnage donné, consiste donc à rechercher des motifs fréquents (ou des sous-structures fréquentes) dans cette base de graphes. Il apparait donc un écart sémantique entre le graphe et le contenu de l'image de bande dessinée.
Nous étudions des données de nature diverse sous forme de flux, en particulier :  Base de données, Réseaux sociaux, Données de texte. Pour une base de données qui suit un schéma relationnel, un schéma d'analyse OLAP (Online Analytical Processing) définit une des tables de la base de données comme une table d'analyse. Nous supposons que les tuples de la table d'analyse arrivent sous forme d'un flux. Nous étudions l'approximation des requêtes OLAP, en échantillonnant de manière non uniforme les tuples du flux sans stocker les données d'analyse et donnons un modèle de préférence dans ce cadre. Dans le cas du réseau social Twitter, nous observons un flux de tweets qui contiennent un tag donné et le transformons en un flux d'arêtes d'un graphe. Nous souhaitons étudier l'existence des grands clusters dans le graphe ainsi obtenu. Nous proposons une méthode d'échantillonnage uniforme qui va associer au graphe un sous-graphe aléatoire et étudions les composantes géantes de ce sous-graphe aléatoire comme témoin des grands clusters du graphe d'origine. Pour un flux de texte, nous considérons les paires de mots dans une phrase lemmatisée comme des arêtes d'un graphe où les nœuds sont les mots. Nous transformons le flux de texte en flux d'arêtes. Nous échantillonnons les arêtes proportionnellement à la similarité Word2vec des mots. Nous analysons ensuite les composantes géantes. Nous étendons les vecteurs Word2vec en prenant en compte la morphologie d'une langue, en particulier la structure des préfixes et des suffixes d'un mot.
Cette étude s'appuie sur l'enseignement de la lecture-compréhension en français pour un public universitaire débutant en FLE au Brésil. En effet, malgré la diffusion de cet enseignement parmi les étudiants en Amérique latine et sa constante réélaboration afin de rendre compte de nouvelles situations d'apprentissage, il présente des lacunes importantes. Ces lacunes se trouvent autour d'un point névralgique pour une grande partie des lecteurs : le déficit lexical. Un déficit qui n'est pas lié qu'à un simple manque de vocabulaire, mais qui passe aussi par un savoir-apprendre les mots. Cette situation est mise à l'épreuve dans cette thèse par deux recherches expérimentales. Une première recherche à caractère plutôt prospectif vise à repérer le rôle du dictionnaire, et particulièrement du dictionnaire bilingue lors de la lecture et l'effet de son usage sur la construction du sens. Une deuxième recherche met en rapport l'emploi de deux outils lexicographiques, un dictionnaire bilingue et un dictionnaire pédagogique destiné à des apprenants de FLE. Ces recherches nous donnent des pistes essentielles pour l'intégration de l'étude lexicale à ce genre d'enseignement et sont complétées par l'analyse des dictionnaires disponibles pour ce public particulier. Ce parcours nous aide à tracer les principes méthodologiques pour l'élaboration d'un dictionnaire pédagogique d'appui à la lecture et à l'acquisition lexicale fondés sur la lexicographie fonctionnelle.
Notre but dans cette thèse est de construire un système qui réponde à une question en langue naturelle (NL) en représentant sa sémantique comme une forme logique (LF) et ensuite en calculant une réponse en exécutant cette LF sur une base de connaissances. La partie centrale d'un tel système est l'analyseur sémantique qui transforme les questions en formes logiques. Notre objectif est de construire des analyseurs sémantiques performants en apprenant à partir de paires (NL, LF). Nous proposons de combiner des réseaux neuronaux récurrents (RNN) avec des connaissances préalables symboliques exprimées à travers des grammaires hors-contexte (CFGs) et des automates. En intégrant des CFGs contrôlant la validité des LFs dans les processus d'apprentissage et d'inférence des RNNs, nous garantissons que les formes logiques générées sont bien formées ; en intégrant, par le biais d'automates pondérés, des connaissances préalables sur la présence de certaines entités dans la LF, nous améliorons encore la performance de nos modèles. Expérimentalement, nous montrons que notre approche permet d'obtenir de meilleures performances que les analyseurs sémantiques qui n'utilisent pas de réseaux neuronaux, ainsi que les analyseurs à base de RNNs qui ne sont pas informés par de telles connaissances préalables.
La classification automatique des messages courts est de plus en plus employée de nos jours dans diverses applications telles que l'analyse des sentiments ou la détection des « spams » . Nous présentons dans cette thèse deux nouvelles approches visant à améliorer la classification de ce type de message. Notre première approche est nommée « forêts sémantiques » . Dans le but d'améliorer la qualité des messages, cette approche les enrichit à partir d'une source externe construite au préalable. Puis, pour apprendre un modèle de classification, contrairement à ce qui est traditionnellement utilisé, nous proposons un nouvel algorithme d'apprentissage qui tient compte de la sémantique dans le processus d'induction des forêts aléatoires. Notre deuxième contribution est nommée « IGLM » (Interactive Generic Learning Method). C'est une méthode interactive qui met récursivement à jour les forêts en tenant compte des nouvelles données arrivant au cours du temps, et de l'expertise de l'utilisateur qui corrige les erreurs de classification. L'ensemble de ce mécanisme est renforcé par l'utilisation d'une méthode d'abstraction permettant d'améliorer la qualité des messages. Les différentes expérimentations menées en utilisant ces deux méthodes ont permis de montrer leur efficacité. Enfin, la dernière partie de la thèse est consacrée à une étude complète et argumentée de ces deux prenant en compte des critères variés tels que l'accuracy, la rapidité, etc.
La littérature recense de nombreux traits linguistiques faisant partie de ces idées associées à un genre ou l'autre tels que la tournure interrogative (tag question) ou la déférence par exemple. De toutes les caractéristiques linguistiques genrées, celle qui a probablement été le plus débattue est celle concernant l'utilisation des jurons. A cause d'une interaction complexe entre pression et pouvoir social, la vulgarité a traditionnellement été associée à l'idée de masculinité avant tout. Utiliser des jurons est souvent considéré comme étant l'affirmation linguistique d'une forme de pouvoir social. Par conséquent, l'association intrinsèque de la vulgarité comme forme de pouvoir à un genre ou l'autre pourrait conduire à l'association d'autres caractéristiques sociales aux questions de masculinité ou de féminité, qu'elles soient fondées ou non. Certaines études ont démontré que, contrairement aux idées longtemps répandues, les femmes n'utilisent pas la vulgarité moins souvent que les hommes, pas plus qu'elles n'utilisent un registre linguistique fondamentalement différent. Certaines ont même prédit que l'utilisation de jurons dits « forts » (i.e. « strong swear words » ) chez les femmes augmenterait dans certains contextes, et en particulier sur les réseaux sociaux (Thelwall, 2008) ; ceci s'appliquerait particulièrement aux jeunes générations de femmes. En d'autres termes, l'utilisation de certains jurons chez ces jeunes générations de femmes deviendrait à terme plus fréquente que celle des hommes du même âge. Par conséquent, la question suivante se pose : les prévisions faites par Thelwall en 2008 se sont-elles réalisées près de dix ans plus tard, dans une société où les médias sociaux n'ont jamais eu autant d'importance dans notre vie quotidienne ? Cette étude est basée sur un corpus composé d'un peu plus de dix-huit millions de tweets représentatifs de près de 739 000 utilisateurs. Le corpus a été constitué à partir de tweets provenant du Royaume Unis, émis par des utilisateurs masculins et féminins, et appartenant à différentes tranches d'âge. Une méthodologie et des outils d'analyse issus de la linguistique dite de corpus ont été utilisés pour mener à bien ce projet et tenter de répondre aux problématiques soulevées précédemment. Aussi, en raison du manque d'informations démographiques directement associées aux profils Twitter (e.g. le genre et l'âge des utilisateurs), il fût nécessaire de recourir à des techniques issues de la programmation informatique afin d'inférer le genre et l'âge de ces personnes.
Par exemple, plusieurs millions de photos sont partagées quotidiennement sur les réseaux sociaux. Les méthodes d'interprétation d'images vise à faciliter l'accès à ces données visuelles, d'une manière sémantiquement compréhensible. Dans ce manuscrit, nous définissons certains buts détaillés qui sont intéressants pour les taches d'interprétation d'images, telles que la classification ou la recherche d'images, que nous considérons dans les trois chapitres principaux. Tout d'abord, nous visons l'exploitation de la nature multimodale de nombreuses bases de données, pour lesquelles les documents sont composés d'images et de descriptions textuelles. Dans ce but, nous définissons des similarités entre le contenu visuel d'un document, et la description textuelle d'un autre document. Ces similarités sont calculées en deux étapes, tout d'abord nous trouvons les voisins visuellement similaires dans la base multimodale, puis nous utilisons les descriptions textuelles de ces voisins afin de définir une similarité avec la description textuelle de n'importe quel document. Ensuite, nous présentons une série de modèles structurés pour la classification d'images, qui encodent explicitement les interactions binaires entre les étiquettes (ou labels). Un scenario interactif comme celui-ci offre un compromis intéressant entre la précision, et l'effort d'annotation manuelle requis. Nous explorons les modèles structurés pour la classification multi-étiquette d'images, pour la classification d'image basée sur les attributs, et pour l'optimisation de certaines mesures de rang spécifiques. Enfin, nous explorons les classifieurs par k plus proches voisins, et les classifieurs par plus proche moyenne, pour la classification d'images à grande échelle. Nous proposons des méthodes d'apprentissage de métrique efficaces pour améliorer les performances de classification, et appliquons ces méthodes à une base de plus d'un million d'images d'apprentissage, et d'un millier de classes. Comme les deux méthodes de classification permettent d'incorporer des classes non vues pendant l'apprentissage à un coût presque nul, nous avons également étudié leur performance pour la généralisation. Nous montrons que la classification par plus proche moyenne généralise à partir d'un millier de classes, sur dix mille classes à un coût négligeable, et les performances obtenus sont comparables à l'état de l'art.
Entailment vise à capturer les principaux besoins d'inférence sémantique dans les applications de Traitement du Langage Naturel. Depuis 2005, dans la Textual Entailment reconnaissance tâche (RTE), les systèmes sont appelés à juger automatiquement si le sens d'une portion de texte, le texte - T, implique le sens d'un autre texte, l'hypothèse - H Cette thèse nous nous intéressons au cas particulier de l'implication, l'implication de généralité. Pour nous, il y a différents types d'implication, nous introduisons le paradigme de l'implication textuelle en généralité, qui peut être définie comme l'implication d'une peine spécifique pour une phrase plus générale, dans ce contexte, le texte T implication Hypothèse H, car H est plus générale que T. Nous proposons des méthodes sans surveillance indépendante de la langue de reconnaissance de l'implication textuelle par la généralité, pour cela, nous présentons une mesure asymétrique informatif appelée Asymmetric simplifié InfoSimba, que nous combinons avec différentes mesures d'association asymétriques à reconnaître le cas spécifique de l'implication textuelle par la généralité. Cette thèse, nous introduisons un nouveau concept d'implication, les implications de généralité, en conséquence, le nouveau concept d'implications de la reconnaissance par la généralité, une nouvelle orientation de la recherche en Traitement du Langage Naturel.
Cette thèse de doctorat traite de l'inférence variationnelle et de la robustesse en statistique et en machine learning. Plus précisément, elle se concentre sur les propriétés statistiques des approximations variationnelles et sur la conception d'algorithmes efficaces pour les calculer de manière séquentielle, et étudie les estimateurs basés sur le Maximum Mean Discrepancy comme règles d'apprentissage qui sont robustes à la mauvaise spécification du modèle. Ces dernières années, l'inférence variationnelle a été largement étudiée du point de vue computationnel, cependant, la littérature n'a accordé que peu d'attention à ses propriétés théoriques jusqu'à très récemment. Dans cette thèse, nous étudions la consistence des approximations variationnelles dans divers modèles statistiques et les conditions qui assurent leur consistence. En particulier, nous abordons le cas des modèles de mélange et des réseaux de neurones profonds. Nous justifions également d'un point de vue théorique l'utilisation de la stratégie de maximisation de l'ELBO, un critère numérique qui est largement utilisé dans la communauté VB pour la sélection de modèle et dont l'efficacité a déjà été confirmée en pratique. En outre, l'inférence Bayésienne offre un cadre d'apprentissage en ligne attrayant pour analyser des données séquentielles, et offre des garanties de généralisation qui restent valables même en cas de mauvaise spécification des modèles et en présence d'adversaires. Malheureusement, l'inférence Bayésienne exacte est rarement tractable en pratique et des méthodes d'approximation sont généralement employées, mais ces méthodes préservent-elles les propriétés de généralisation de l'inférence Bayésienne ? Dans cette thèse, nous montrons que c'est effectivement le cas pour certains algorithmes d'inférence variationnelle (VI). Nous proposons de nouveaux algorithmes tempérés en ligne et nous en déduisons des bornes de généralisation. Notre résultat théorique repose sur la convexité de l'objectif variationnel, mais nous soutenons que notre résultat devrait être plus général et présentons des preuves empiriques à l'appui. Notre travail donne des justifications théoriques en faveur des algorithmes en ligne qui s'appuient sur des méthodes Bayésiennes approchées. Une autre question d'intérêt majeur en statistique qui est abordée dans cette thèse est la conception d'une procédure d'estimation universelle. Cette question est d'un intérêt majeur, notamment parce qu'elle conduit à des estimateurs robustes, un thème d'actualité en statistique et en machine learning. Nous abordons le problème de l'estimation universelle en utilisant un estimateur de minimisation de distance basé sur la Maximum Mean Discrepancy. Nous montrons que l'estimateur est robuste à la fois à la dépendance et à la présence de valeurs aberrantes dans le jeu de données. Nous mettons également en évidence les liens qui peuvent exister avec les estimateurs de minimisation de distance utilisant la distance L2. Enfin, nous présentons une étude théorique de l'algorithme de descente de gradient stochastique utilisé pour calculer l'estimateur, et nous étayons nos conclusions par des simulations numériques. Nous proposons également une version Bayésienne de notre estimateur, que nous étudions à la fois d'un point de vue théorique et d'un point de vue computationnel.
Cette thèse propose d'examiner les façons dont l'expérience de visite peut être transformée dans le contexte d'une culture numérique qui touche les publics réels et les publics potentiels du musée. On s'interroge plus précisément sur les processus d'appropriation et de partage qu'entraine la pratique muséale à l'époque de la culture numérique. Pour explorer cet objet de recherche, nous construisons un appareil méthodologique interdisciplinaire Il permet d'élaborer un cadre analytique susceptible de saisir des pratiques qui se partagent et se combinent entre un espace numérique et un espace physique. De la sorte, nous proposons des protocoles de recherches hybrides et exploratoires qui offrent la possibilité de saisir, par leur association, des pratiques multiples de l'expérience de visite dans la triple temporalité de l'avant, du pendant et de l'après visite du musée. À travers cette thèse nous mettons en évidence la façon dont le numérique intervient dans les pratiques de l'expérience de visite comme un objet technique spécifique et un système d'échange avec et dans lequel le visiteur et le visiteur potentiel reconfigurent leur relation à l'institution muséale, à sa collection, aux expositions et aux autres visiteurs et visiteurs potentiels.
Cette étude est une nouvelle adaptation de la théorie linguistique systémique, créée en 1987 par Sylviane CARDEY, dans laquelle nous proposons un système de règles basé sur les faits et les définitions linguistiques et présenté sous forme de modèle qui prend en considération les principes de la modélisation mathématique. L'objectif de cette thèse est de démontrer aux apprenants de la langue étrangère le mécanisme de son fonctionnement, notamment celui de la grammaire, par des méthodes adéquates et des programmes informatiques soigneusement travaillés pour un style d'apprentissage cohérent. Notre propos porte donc sur une analyse précise et détaillée concernant les règles de la grammaire, ce qui nous permet d'obtenir une matière didactique claire et exhaustive avec laquelle l'apprenant peut étudier le sujet dans sa globalité, en identifiant ses différentes règles et la corrélation entres elles et entre ses domaines. Ces derniers vont par la suite pouvoir être utilisés dans un cadre pratique, celui de l'enseignement de la grammaire. Nous avons également dans ce but construit un site accessible aux enseignants susceptibles de l'utiliser pour leurs cours. Dans un second temps, après avoir traité la partie théorique, nous avons appliqué la théorie dans le deuxième chapitre sur la conjugaison des verbes arabes. Le corpus que nous avons choisi pour cet objectif est constitué par les 127 verbes modèles du Bescherelle arabe. Dans ce chapitre, nous avons utilisé notre système de règles pour décrire le système de la conjugaison arabe. L'application a également permis de mettre en évidence certains défauts que l'on retrouve dans divers manuels de grammaire linguistiques ou autres
La mesure des différents phénomènes terrestres et l'échange d'informations ont permis l'émergence d'un type de données appelé série temporelle. Celle-ci se caractérise par un grand nombre de points la composant et surtout par des interactions entre ces points. En outre, une série temporelle est dite multivariée lorsque plusieurs mesures sont captées à chaque instant de temps. Bien que l'analyse des séries temporelles univariées, une mesure par instant, soit très développée, l'analyse des séries multivariées reste un challenge ouvert. Or les méthodes mises à disposition, aujourd'hui, pour la classification supervisée de ces séries, ne permettent pas de répondre de manière satisfaisante à cette problématique en plus d'une gestion rapide et efficace des données. Cette approche emploie donc un nouvel outil, qui n'a jamais été utilisé dans le domaine de la classification de séries temporelles multivariées, qui est le M-histogramme pour répondre à cette question. Un M-histogramme est à la base une méthode de visualisation sur M axes de la fonction de densité sous-jacente à un échantillon de données. Son utilisation ici permet de produire une nouvelle représentation de nos données afin de mettre en évidence les interactions entre dimensions. Cette recherche de liens entre dimensions correspond aussi tout particulièrement à un sous-domaine d'apprentissage, appelé l'apprentissage multi-vues. Où une vue est une extraction de plusieurs dimensions d'un ensemble de données, de même nature ou type. L'objectif est alors d'exploiter le lien entre ces dimensions afin de mieux classifier les dites données, au travers d'un modèle ensembliste permettant d'agréger les prédictions émises à partir de chaque vue. Dans cette thèse, nous proposons donc une méthode multi-vues ensembliste de M-histogrammes afin de classifier les Séries Temporelles Multivariées (STM). Cela signifie que plusieurs M-histogrammes sont créés à partir de plusieurs vues des STM exploitées. Une prédiction est ensuite réalisée grâce à chaque M-histogramme. Enfin ces prédictions sont ensuite agrégées afin de produire une prédiction finale.
Cette thèse a pour objectif de démontrer l'existence d'une quatrième vague féministe ayant émergé en France au début des années 2010. Le concept de vague est défini comme un cycle de mobilisation féministe qui peut être constaté par la conjonction de trois critères interdépendants et empiriquement testables. Le premier critère requiert d'observer une hausse du traitement médiatique de la cause des femmes sur une période donnée. D'un point de vue qualitatif, ce traitement, bien qu'hétérogène, doit être globalement favorable à l'égalité entre les femmes et les hommes. Le deuxième critère implique de constater une transformation des idées et / ou pratiques des mouvements féministes. Ces transformations sont le signe d'une adaptation réussie des mouvements aux évolutions sociales et techniques, elles prouvent qu'ils ont été capables de rendre leur discours audible dans un espace-tempsdonné – ce dont atteste l'existence du premier critère. Enfin, le troisième critère est celui du renouvellement générationnel : il a pour intérêt essentiel d'historiciser l'analyse en confrontant l'émergence d'un cycle de mobilisation à l'évolution des grammaires mobilisatrices et des conditions produisant des incitations à la mobilisation.
Cette recherche s'inscrit dans le domaine de la didactique du FLE et s'intéresse à l'intégration de la pratique théâtrale dans l'enseignement-apprentissage de l'expression orale en FLE, en s'appuyant sur l'utilisation du téléphone portable et des réseaux sociaux. Elle interroge la potentialité du jeu théâtral pour le développement et l'amélioration des compétences d'expression orale des apprenants, avec une focalisation sur la prononciation. Dans le cadre d'une recherche action à caractère ethnographique s'appuyant sur l'approche par les tâches, un dispositif hybride d'apprentissage alliant séances en présentiel et activités à distance a été mis en place en faveur d'étudiants marocains inscrits en Licence d'Etudes Françaises. En vue de mesurer l'impact du dispositif sur le développement des compétences langagières à l'oral, nous avons analysé la précision phonologique et la fluidité des productions orales des apprenants par le biais d'un pré-test et d'un post test, et lors de l'observation directe des séances (analyse d'enregistrements audio) dans lesquelles les apprenants s'entraînaient à répéter des extraits de scène d'une pièce de théâtre afin d'en présenter une mise en scène à l'issue de la formation. Les résultats obtenus ont fait ressortir des acquisitions langagières avec une nette amélioration de la précision phonologique pour l'ensemble des apprenants, mais un gain en fluidité pour seulement une partie des apprenants. Dans l'ensemble ils ont évalué positivement le dispositif et en sont globalement satisfaits. Cependant, ils n'ont pas tous donné la même importance au travail en groupe que suscite le projet de mise en scène d'une pièce de théâtre, tout comme ils n'ont pas tous montré la même implication.
Le domaine du traitement automatique de la parole regroupe un très grand nombre de tâches parmi lesquelles on trouve la reconnaissance de la parole, l'identification de la langue ou l'identification du locuteur. Ce domaine de recherche fait l'objet d'études depuis le milieu du vingtième siècle mais la dernière rupture technologique marquante est relativement récente et date du début des années 2010. C'est en effet à ce moment qu'apparaissent des systèmes hybrides utilisant des réseaux de neurones profonds (DNN) qui améliorent très notablement l'état de l'art. Dans cette thèse, nous nous intéressons tout particulièrement aux RNN à mémoire court-terme persistante (Long Short Term Memory (LSTM) qui permettent de s'affranchir d'un certain nombre de difficultés rencontrées avec des RNN standards. Nous augmentons ce modèle et nous proposons des processus d'optimisation permettant d'améliorer les performances obtenues en segmentation parole/non-parole et en identification de la langue.
Cette recherche s'intéresse à la possibilité d'établir des points de comparaison entre le prélinguistique et le linguistique dans la période des premiers mots. Le constat d'un flou régnant autour des notions de mot et de proto-mot nous a fait considérer différentes approches : historique, épistémologique et expérimentale. L'apport de l'approche historique est essentiel pour cerner la problématique et considérer la façon dont parler est envisagé par une société et une époque. Cettepartie nous permet de mettre en avant deux éléments : la question de l'émergence de la parole implique la notion de représentation sociale, et l'émergence de la parole, aujourd'hui, se situe durant la période des premiers mots. L'analyse de cette période nous conduit à une partie épistémologique permettant de définir le type d'unités caractéristiques de cette période : les proto-mots et les mots. Une fois nos unités identifiées, nous effectuons une analyse longitudinale de quatre enfants, de un an à deux ans. Nous avons d'abord identifié un phénomène de substitution des mots aux proto-mots. Ensuite, nous avons observé les deux éléments communs à ces productions : la prosodie et la phonologie. Il découle de notre analyse que la prosodie fournit un cadre commun assurant la transition entre les proto-mots et les mots, et que la phonologie est le domaine où s'observent les différences : les mots sont le lieu du développement des structures phonologiques complexes, contrairement aux proto-mots. Nous avons pu considérer que parler, c'était privilégier les motscomme support de communication verbale, par rapport aux proto-mots, et que cette particularité est l'objet du développement phonologique.
Alors que Twitter évolue vers un outil omniprésent de diffusion de l'information, la compréhension des tweets en langues étrangères devient un problème important et difficile. En raison de la nature intrinsèquement à commutation de code, discrète et bruitée des tweets, la traduction automatique (MT) à l'état de l'art n'est pas une option viable (Farzindar &amp ; Inkpen, 2015). En effet, au moins pour le hindi et le japonais, nous observons que le pourcentage de tweets « compréhensibles » passe de 80% pour les locuteurs natifs à moins de 30% pour les lecteurs monolingues cible (anglais ou français) utilisant Google Translate. Notre hypothèse de départ est qu'il devrait être possible de créer des outils génériques, permettant aux étrangers de comprendre au moins 70% des « tweets locaux » , en utilisant une interface polyvalente de « lecture active » (LA, AR en anglais) tout en déterminant simultanément le pourcentage de tweets compréhensibles en-dessous duquel un tel système serait jugé inutile par les utilisateurs prévus. Nous avons donc spécifié un « SUFT » (système d'aide à la compréhension des tweets étrangers) générique, et mis en œuvre SUFT-1, un système interactif à mise en page multiple basé sur la LA, et facilement configurable en ajoutant des dictionnaires, des modules morphologiques et des plugins de TA. Il est capable d'accéder à plusieurs dictionnaires pour chaque langue source et fournit une interface d'évaluation. Pour les évaluations, nous introduisons une mesure liée à la tâche induisant un coût négligeable, et une méthodologie visant à permettre une « évaluation continue sur des données ouvertes » , par opposition aux mesures classiques basées sur des jeux de test liés à des ensembles d'apprentissage fermés. Nous proposons de combiner le taux de compréhensibilité et le temps de décision de compréhensibilité comme une mesure de qualité à deux volets, subjectif et objectif, et de vérifier expérimentalement qu'une présentation de type lecture active, basée sur un dictionnaire, peut effectivement aider à comprendre les tweets mieux que les systèmes de TA disponibles. En plus de rassembler diverses ressources lexicales, nous avons construit une grande ressource de "formes de mots" apparaissant dans les tweets indiens, avec leurs analyses morphologiques (à savoir 163221 formes de mots hindi dérivées de 68788 lemmes et 72312 formes de mots marathi dérivées de 6026 lemmes) pour créer un analyseur morphologique multilingue spécialisé pour les tweets, capable de gérer des tweets à commutation de code, de calculer des traits unifiés, et de présenter un tweet en lui attachant un graphe de LA à partir duquel des lecteurs étrangers peuvent extraire intuitivement une signification plausible, s'il y en a une.
Les systèmes informatiques impliquant la détection d'anomalies émergent aussi bien dans le domaine de la recherche que dans l'industrie. Ainsi, des domaines aussi variés que la médecine (identification de tumeurs malignes), la finance (détection de transactions frauduleuses), les technologies de l'information (détection d'intrusion réseau) et l'environnement (détection de situation de pollution) sont largement impactés. L'apprentissage automatique propose un ensemble puissant d'approches qui peuvent aider à résoudre ces cas d'utilisation de manière efficace. Il implique également plusieurs experts qui travailleront ensemble pour trouver les bonnes approches. De plus, les possibilités ouvertes aujourd'hui par le monde de la sémantique montrent qu'il est possible de tirer parti des technologies du web afin de raisonner intelligemment sur les données brutes pour en extraire de l'information à forte valeur ajoutée. L'absence de systèmes combinant les approches numériques d'apprentissage automatique et les techniques sémantiques du web des données constitue la motivation principale derrière les différents travaux proposés dans cette thèse. Enfin, les anomalies détectées ne signifient pas nécessairement des situations de réalité anormales. En effet, la présence d'informations externes pourrait aider à la prise de décision en contextualisant l'environnement dans sa globalité. Exploiter le domaine spatial et les réseaux sociaux permet de construire des contextes enrichis sur les données des capteurs. Ces contextes spatio-temporels deviennent ainsi une partie intégrante de la détection des anomalies et doivent être traités en utilisant une approche Big Data. Son originalité tient dans sa capacité à raisonner intelligemment sur des données brutes afin d'inférer des informations implicites à partir d'informations explicites et d'aider dans la prise de décision. Cette plateforme a été développée dans le cadre d'un projet FUI dont le principal cas d'usage est la détection d'anomalies dans un réseau d'eau potable. RAMSSES :  Système hybride d'apprentissage automatique dont l'originalité est de combiner des approches numériques avancées ainsi que des techniques sémantiques éprouvées. Système intelligent de "scrapping web" permettant la contextualisation des singularités liées à l'Internet des Objets en exploitant aussi bien des informations spatiales que le web des données
La prise en compte des enjeux environnementaux est un sujet très présent dans notre société actuelle. Dans le domaine du développement de produits, l'Eco-conception est une démarche qui permet la considération de ces enjeux en proposant de réduire les impacts environnementaux des produits tout au long de leur cycle de vie. La phase d'utilisation du cycle de vie est une étape cruciale puisque le mode d'utilisation des produits peut avoir des conséquences non négligeables sur leur performance environnementale. Nous proposons dans cette thèse d'enrichir la compréhension de cette phase d'utilisation en mettant en évidence les composantes Kansei qui jouent un rôle dans l'interaction de l'utilisateur avec le produit et qui peuvent être intégrées en amont de la conception de produits à faible impact environnemental. Notre démarche permet de mieux renseigner la phase d'utilisation, contribuant ainsi à la maitrise de la performance environnementale des produits. Nous démontrons à travers notre recherche que l'utilisateur peut être défini, non seulement par des informations basiques que nous retrouvons communément dans la Conception Centrée Utilisateur, mais également à partir d'informations subjectives apportées par la dimension Kansei. A travers nos expérimentations, nous mettons en application deux attributs EcoKansei correspondants aux valeurs et aux émotions afin d'illustrer la modélisation utilisateur pour la conception de produits à faible impact environnemental. L'approche que nous proposons vient en complément au projet EcoUse qui vise à développer une méthodologie d'Eco-conception centrée utilisateur. Les apports de notre démarche sont multiples. Du point de vue de la recherche, une mise en relation de l'Eco-conception et du Kansei, qui sont à la base complètement déconnectées entraine un enrichissement mutuel entre ces deux approches. Du point de vue industriel, une démarche d'Eco-conception appuyée par les études Kansei doit permettre la conception de nouveaux produits qui auront l'avantage d'être à la fois mieux acceptés par les utilisateurs car en adéquation avec leur sensibilité environnementale et en même temps moins impactants pour l'environnement.
Toutes les sources envisageables seront potentiellement mises à contribution en plus des données de l'AMF, par exemple, les informations publiques (agences de presses spécialisées ou non, réseaux sociaux, WEB...) et les sources d'informations professionnelles, comme par exemple les « carnets d'ordres » (transactions boursières spécifiées en langue naturelle). L'identification s'accompagnera de l'appréciation qualitative du risque considéré, d'un classement par ordre d'importance des documents justifiant l'alerte ainsi que de la génération d'une synthèse des informations extraites justifiant l'alerte, sous des formes qui restent à définir (résumés automatiques, des bases de faits, des graphes...) en fonction des spécificités des systèmes (hypercubes de données, interface de veille) alimentés par l'algorithme de détection. La conception de l'algorithme prendra en compte le fait qu'il devra être opérationnel dans un contexte « Big Data » nécessitant un maintien des performances d'utilisabilité lors du passage à l'échelle. La fonction de l'algorithme objet de la thèse étant de fournir aux experts qui établissent le diagnostic du risque et/ou infraction aux réglements, une pré-analyse (détection de signaux faibles [Sidhom11]) avec tous les éléments d'information justificatifs et complémentaires.
L'accessibilité numérique joue un rôle décisif pour l'éducation, l'inclusion sociale et l'autonomie des individus souffrant d'une déficience. Dans ces travaux, nous nous sommes intéressés à une composante universelle des documents numériques : la mise en forme des textes. L'utilisation de couleurs, polices et dispositions de texte peut paraître anodin, mais il se trouve qu'au-delà de l'esthétique du texte, la mise en forme a non seulement du sens, mais elle permet aux lecteurs d'optimiser leur activité de lecture. Par exemple des couleurs et une police particulière peuvent suffire à nous indiquer un titre, qui va permettre au lecteur de se représenter globalement le contenu. Ces travaux visaient donc à rendre accessible la signification de la mise en forme aux déficients visuels, afin qu'ils puissent accéder aux mêmes informations que les lecteurs voyants mais aussi bénéficier des mêmes optimisations quand ils accèdent aux documents à l'aide de voix de synthèse.
Dans le contexte actuel, l'Intelligence Artificielle (IA) est largement répandue et s'applique à de nombreux domaines tels que les transports, la médecine et les véhicules autonomes. Parmi les algorithmes d'IA, on retrouve principalement les réseaux de neurones, qui peuvent être répartis en deux familles : d'une part, les Réseaux de Neurones Impulsionnels (SNNs) qui sont issus du domaine des neurosciences ; d'autre part, les Réseaux de Neurones Analogiques (ANNs) qui sont issus du domaine de l'apprentissage machine. Les ANNs connaissent un succès inédit grâce à des résultats inégalés dans de nombreux secteurs tels que la classification d'images et la reconnaissance d'objets. Cependant, leur déploiement nécessite des capacités de calcul considérables et ne conviennent pas à des systèmes très contraints. Afin de pallier ces limites, de nombreux chercheurs s'intéressent à un calcul bio-inspiré, qui serait la parfaite alternative aux calculateurs conventionnels basés sur l'architecture de Von Neumann. Ce paradigme répond aux exigences de performance de calcul, mais pas aux exigences d'efficacité énergétique. Il faut donc concevoir des circuits matériels neuromorphiques adaptés aux calculs parallèles et distribués. Dans ce contexte, nous avons établi un certain nombre de critères en termes de précision et de coût matériel pour différencier les SNNs et ANNs. Dans le cas de topologies simples, nous avons montré que les SNNs sont plus efficaces en termes de coût matériel que les ANNs, et ce, avec des précisions de prédiction quasiment similaires. Ainsi, dans ce travail, notre objectif est de concevoir une architecture neuromorphique basée sur les SNNs. Dans un contexte d'efficacité énergétique, nous avons réalisé une étude approfondie sur divers paradigmes de codage neuronal utilisés avec les SNNs. Par ailleurs, nous avons proposé de nouvelles versions dérivées du codage fréquentiel, visant à se rapprocher de l'activité produite avec le codage temporel, qui se caractérise par un nombre réduit d'impulsions (spikes) se propageant dans le SNN. En faisant cela, nous sommes en mesure de réduire le nombre de spikes, ce qui se traduit par un SNN avec moins d'événements à traiter, et ainsi, réduire la consommation énergétique sous-jacente. Pour cela, deux techniques nouvelles ont été proposées : "First Spike", qui se caractérise par l'utilisation d'un seul spike au maximum par donnée ; "Spike Select", qui permet de réguler et de minimiser l'activité globale du SNN.Dans la partie d'exploration RTL, nous avons comparé de manière quantitative un certain nombre d'architectures de SNN avec différents niveaux de parallélisme et multiplexage de calculs. En effet, le codage "Spike Select" engendre une régulation de la distribution des spikes, avec la majorité générée dans la première couche et peu d'entre eux propagés dans les couches profondes. Nous avons constaté que cette distribution bénéficie d'une architecture hybride comportant une première couche parallèle et les autres multiplexées. Par conséquent, la combinaison du "Spike Select" et de l'architecture hybride serait une solution efficace, avec un compromis efficace entre coût matériel, consommation et latence. Enfin, en se basant sur les choix architecturaux et neuronaux issus de l'exploration précédente, nous avons élaboré une architecture évènementielle dédiée aux SNNs mais suffisamment programmable pour supporter différents types et tailles de réseaux de neurones. L'architecture supporte les couches les plus utilisées : convolution, pooling et entièrement connectées. En utilisant cette architecture, nous serons bientôt en mesure de comparer les ANNs et les SNNs sur des applications réalistes et enfin conclure sur l'utilisation des SNNs pour l'IA embarquée.
Le présent travail étudie une situation de communication nouvelle : la communication via l'e-mail. Notre étude s'est centrée plus précisément sur les mails envoyés par des clients vers l'entreprise et ce dans le domaine du tourisme aérien (notion de e-crm). Pour mener nos analyses linguistiques, nous avons constitué un important corpus de messages récoltés sur des forums Internet et traitant de voyages. Notre but est d'automatiser la gestion, la catégorisation et la thématisation des mails. Nous avons donc rassemblé un ensemble de traits lexicaux, syntaxiques, morpho-syntaxiques et sémantiques spécifiques à la notion de déplacement, de toponymie et propres au sous-langage du tourisme aérien. Nous montrons également comment une analyse linguistique des informations spatiales et indissociable d'un traitement des éléments temporels de la phrase. En outre, nous choisissons d'analyser les informations de type émotionnel contenues dans les messages. Dans la dernière partie de notre travail, nous replaçons notre étude dans un système de veille appliqué aux mails. Nous montrons comment les techniques à base de statistiques sont limitées dès qu'il s'agit de traiter des énoncés linguistiquement complexes tels que les nôtres. Notre approche est hybride : à base de mots clés, dictionnaires de synonymes, scripts sur le modèle de SCHANK et ABELSON, mais surtout à base de modélisation des connaissances. Finalement, nous proposons un traitement de haute qualité des connaissances et donnons quelques exemples d'informatisation de notre système grâce à XML, PROLOG et PERL
La vidéosurveillance est d'une grande valeur pour la sécurité publique. En tant que l'un des plus importantes applications de vidéosurveillance, la ré-identification de personnes est définie comme le problème de l'identification d'individus dans des images captées par différentes caméras de surveillance à champs non-recouvrants. Dans la première approche, nous utilisons les attributs des piétons tels que genre, accessoires et vêtements. Nous fusionnons ensuite ces deux branches pour la ré-identification. Deuxièmement, nous proposons un CNN prenant en compte différentes orientations du corps humain. Comme troisième contribution de cette thèse, nous proposons une nouvelle fonction de coût basée sur une liste d'exemples. Elle introduit une pondération basée sur le désordre du classement et permet d'optimiser directement les mesures d'évaluation. Enfin, pour un groupe de personnes, nous proposons d'extraire une représentation de caractéristiques visuelles invariante à la position d'un individu dans une image de group. Cette prise en compte de contexte de groupe réduit ainsi l'ambigüité de ré-identification. Pour chacune de ces quatre contributions, nous avons effectué de nombreuses expériences sur les différentes bases de données publiques pour montrer l'efficacité des approches proposées.
Il est important d'évaluer régulièrement les produits de l'innovation technologique afin d'estimer le niveau de maturité atteint par les technologies et d'étudier les cadres applicatifs dans lesquels elles pourront être exploitées. Pendant longtemps, les différentes briques technologiques issues du TAL étaient développées séparément. Le nouveau défi en terme d'évaluation est alors de pouvoir évaluer les différents modules (ou briques) tout en prenant en compte le contexte applicatif. Nous y décrivons les tâche de RAP et de REN proposées dans les campagnes d'évaluation ainsi que les protocoles mis en place pour leurs évaluation. Nous y discutons également les limites des approches d'évaluations modulaires et nous y exposons les mesures alternatives proposées dans la littératures. En deuxième partie, nous décrivons la tâche de détection, classification et décomposition d'entités nommées étudiée et nous proposons une nouvelle métriques ETER (Entity Tree Error Rate) permettant de prendre en compte les spécificité de cette tâche et le contexte applicatif lors de l'évaluation. ETER permet également de supprimer les biais observés avec les métriques existantes. ATENE consiste à comparer les probabilités de présence d'entités sur les transcriptions de référence et d'hypothèse plutôt qu'une comparaison directe des graphèmes. Elle est composée de deux mesures élémentaires.
Cette thèse présente une méthode générique de reconnaissance automatique des émotions à partir d'un système bimodal basé sur les expressions faciales et les signaux physiologiques. Cette approche de traitement des données conduit à une extraction d'information de meilleure qualité et plus fiable que celle obtenue à partir d'une seule modalité. L'algorithme de reconnaissance des expressions faciales qui est proposé, s'appuie sur la variation de distances des muscles faciaux par rapport à l'état neutre et sur une classification par les séparateurs à vastes marges (SVM). La reconnaissance des émotions à partir des signaux physiologiques est, quant à elle, basée sur la classification des paramètres statistiques par le même classifieur. Afin d'avoir un système de reconnaissance plus fiable, nous avons combiné les expressions faciales et les signaux physiologiques. La combinaison directe de telles informations n'est pas triviale étant donné les différences de caractéristiques (fréquence, amplitude de variation, dimensionnalité). Pour y remédier, nous avons fusionné les informations selon différents niveaux d'application. Au niveau de la fusion des caractéristiques, nous avons testé l'approche par l'information mutuelle pour la sélection des plus pertinentes et l'analyse en composantes principales pour la réduction de leur dimensionnalité. Au niveau de la fusion de décisions, nous avons implémenté une méthode basée sur le processus de vote et une autre basée sur les réseaux Bayésien dynamiques. Les meilleurs résultats ont été obtenus avec la fusion des caractéristiques en se basant sur l'Analyse en Composantes Principales. Ces méthodes ont été testées sur une base de données conçue dans notre laboratoire à partir de sujets sains et de l'inducteur par images IAPS. Une étape d'auto évaluation a été demandée à tous les sujets dans le but d'améliorer l'annotation des images d'induction utilisées. Les résultats ainsi obtenus mettent en lumière leurs bonnes performances et notamment la variabilité entre les individus et la variabilité de l'état émotionnel durant plusieurs jours
Une définition très générale de la structure musicale consiste à considérer tout ce qui distingue la musique d'un bruit aléatoire comme faisant partie de sa structure. Dans cette thèse, nous nous intéressons à l'aspect macroscopique de cette structure, en particulier la décomposition de passages musicaux en unités autonomes (typiquement, des sections) et à leur caractérisation en termes de groupements d'entités élémentaires conjointement compressibles. Un postulat de ce travail est d'établir un lien entre l'inférence de structure musicale et les concepts de complexité et d'entropie issus de la théorie de l'information. Nous travaillons ainsi à partir de l'hypothèse que les segments structurels peuvent être inférés par des schémas de compression de données. Dans une première partie, nous considérons les grammaires à dérivation unique (GDU), conçues à l'origine pour la découverte de structures répétitives dans les séquences biologiques (Gallé, 2011), dont nous explorons l'utilisation pour modéliser les séquences musicales. Cette approche permet de compresser les séquences en s'appuyant sur leurs statistiques d'apparition, leur organisation hiérarchique étant modélisée sous forme arborescente. Nous développons plusieurs adaptations de cette méthode pour modéliser des répétitions inexactes et nous présentons l'étude de plusieurs critères visant à régulariser les solutions obtenues. La seconde partie de cette thèse développe et explore une approche novatrice d'inférence de structure musicale basée sur l'optimisation d'un critère de compression tensorielle. Celui-ci vise à compresser l'information musicale sur plusieurs échelles simultanément en exploitant les relations de similarité, les progressions logiques et les systèmes d'analogie présents dans les segments musicaux. La méthode proposée est introduite d'un point de vue formel, puis présentée comme un schéma de compression s'appuyant sur une extension multi-échelle du modèle Système &amp ; Contraste (Bimbot et al., 2012) à des patrons tensoriels hypercubiques. Nous généralisons de surcroît l'approche à d'autres patrons tensoriels, irréguliers, afin de rendre compte de la grande variété d'organisations structurelles des segments musicaux. Les méthodes étudiées dans cette thèse sont expérimentées sur une tâche de segmentation structurelle de données symboliques correspondant à des séquences d'accords issues de morceaux de musique pop (RWC-Pop). Les méthodes sont évaluées et comparées sur plusieurs types de séquences d'accords, et les résultats établissent l'attractivité des approches par critère de complexité pour l'analyse de structure et la recherche d'informations musicales, les meilleures variantes fournissant des performances de l'ordre de 70% de F-mesure.
Extraire de l'information de données langagières est un sujet de plus en plus d'actualité compte tenu de la quantité toujours croissante d'information qui doit être régulièrement traitée et analysée, et nous assistons depuis les années 90 à l'essor des recherches sur des données de parole également. La parole pose des problèmes supplémentaires par rapport à l'écrit, notamment du fait de la présence de phénomènes propres à l'oral (hésitations, reprises, corrections) mais aussi parce que les données orales sont traitées par un système de reconnaissance automatique de la parole qui génère potentiellement des erreurs. Ainsi, extraire de l'information de données audio implique d'extraire de l'information tout en tenant compte du « bruit » intrinsèque à l'oral ou généré par le système de reconnaissance de la parole. Il ne peut donc s'agir d'une simple application de méthodes qui ont fait leurs preuves sur de l'écrit. L'utilisation de techniques adaptées au traitement des données issues de l'oral et prenant en compte à la fois leurs spécificités liées au signal de parole et à la transcription –manuelle comme automatique – de ce dernier représente un thème de recherche en plein développement et qui soulève de nouveaux défis scientifiques. Ces défis sont liés à la gestion de la variabilité dans la parole et des modes d'expressions spontanés. Par ailleurs, l'analyse robuste de conversations téléphoniques a également fait l'objet d'un certain nombre de travaux dans la continuité desquels s'inscrivent ces travaux de thèse. Cette thèse porte plus spécifiquement sur l'analyse des disfluences et de leur réalisation dans des données conversationnelles issues des centres d'appels EDF, à partir du signal de parole et des transcriptions manuelle et automatique de ce dernier. Ce travail convoque différents domaines, de l'analyse robuste de données issues de la parole à l'analyse et la gestion des aspects liés à l'expression orale. L'objectif de la thèse est de proposer des méthodes adaptées à ces données, qui permettent d'améliorer les analyses de fouille de texte réalisées sur les transcriptions (traitement des disfluences). Pour répondre à ces problématiques, nous avons analysé finement le comportement de phénomènes caractéristiques de l'oral spontané (disfluences) dans des données orales conversationnelles issues de centres d'appels EDF, et nous avons mis au point une méthode automatique pour leur détection, en utilisant des indices linguistiques, acoustico-prosodiques,discursifs et para-linguistiques. Les apports de cette thèse s'articulent donc selon trois axes de recherche. Premièrement, nous proposons une caractérisation des conversations en centres d'appels du point de vue de l'orals pontané et des phénomènes qui le caractérisent. Deuxièmement, nous avons mis au point (i) une chaîne d'enrichissement et de traitement des données orales effective sur plusieurs plans d'analyse (linguistique, prosodique, discursif, para-linguistique) ; (ii) un système de détection automatique des disfluences d'édition adapté aux données orales conversationnelles, utilisant le signal et les transcriptions (manuelles ou automatiques). Troisièmement, d'un point de vue « ressource » , nous avons produit un corpus de transcriptions automatiques de conversations issues de centres d'appels annoté en disfluences d'édition (méthode semi-automatique).
Système de recommandation personnalisé pour les visites touristiques
L'imagerie de résonance magnétique de diffusion (dMRI) est une technique très sensible pour la tractographie des fibres de substance blanche et la caractérisation de l'intégrité et de la connectivité axonale. A travers la mesure des mouvements des molécules d'eau dans les trois dimensions de l'espace, il est possible de reconstruire des cartes paramétriques reflétant l'organisation tissulaire. Parmi ces cartes, la fraction d'anisotropie (FA) et les diffusivités axiale (λa), radiale (λr) et moyenne (MD) ont été largement utilisés pour caractériser les pathologies du système nerveux central. L'emploi de ces cartes paramétriques a permis de mettre en évidence la survenue d'altérations micro structurelles de la substance blanche (SB) et de la substance grise (SG) chez les patients atteints d'une sclérose en plaques (SEP). Cependant, il reste à déterminer l'origine de ces altérations qui peuvent résulter de processus globaux comme la cascade inflammatoire et les mécanismes neurodégénératifs ou de processus plus localisés comme la démyélinisation et l'inflammation. De plus, ces processus pathologiques peuvent survenir le long de faisceaux de SB afférents ou efférents, conduisant à une dégénérescence antero- ou rétrograde. Ainsi, pour une meilleure compréhension des processus pathologiques et de leur progression dans l'espace et dans le temps, une caractérisation fine et précise des faisceaux de SB est nécessaire. En couplant l'information spatiale de la tractographie des fibres aux cartes paramétriques de diffusion, obtenues grâce à un protocole d'acquisitions longitudinal, les profils des faisceaux de SB peuvent être modélisés et analysés. Une telle analyse des faisceaux de SB peut être effectuée grâce à différentes méthodes, partiellement ou totalement non-supervisées. Dans la première partie de ce travail, nous dressons l'état de l'art des études déjà présentes dans la littérature. Cet état de l'art se focalisera sur les études montrant les effets de la SEP sur les faisceaux de SB grâce à l'emploi de l'imagerie de tenseur de diffusion. Dans la seconde partie de ce travail, nous introduisons deux nouvelles méthodes,“string-based”, l'une semi-supervisée et l'autre non-supervisée, pour extraire les faisceaux de SB. Nous montrons comment ces algorithmes permettent d'améliorer l'extraction de faisceaux spécifiques comparé aux approches déjà présentes dans la littérature. De plus, dans un second chapitre, nous montrons une extension de la méthode proposée par le couplage du formalisme “string-based” aux informations spatiales des faisceaux de SB. Dans la troisième et dernière partie de ce travail, nous décrivons trois algorithmes automatiques permettant l'analyse des changements longitudinaux le long des faisceaux de SB chez des patients atteints d'une SEP. Ces méthodes sont basées respectivement sur un modèle de mélange Gaussien, la factorisation de matrices non-négatives et la factorisation de tenseurs non-négatifs. De plus, pour valider nos méthodes, nous introduisons un nouveau modèle pour simuler des changements longitudinaux réels, base sur une fonction de probabilité Gaussienne généralisée. Des hautes performances ont été obtenues avec ces algorithmes dans la détection de changements longitudinaux d'amplitude faible le long des faisceaux de SB chez des patients atteints de SEP. En conclusion, nous avons proposé dans ce travail des nouveaux algorithmes non supervisés pour une analyse précise des faisceaux de SB, permettant une meilleure caractérisation des altérations pathologiques survenant chez les patients atteints de SEP
Les événements distribués, se déroulant sur plusieurs jours et/ou sur plusieurs lieux, tels que les conventions, festivals ou croisières, sont de plus en plus populaires ces dernières années et attirant des milliers de participants. Les programmes de ces événements sont généralement très denses, avec un grand nombre d'activités se déroulant en parallèle. Les systèmes de recommandation peuvent constituer une solution privilégiée dans ce genre d'environnement. De nombreux travaux en recommandation se sont concentrés sur la recommandation personnalisée d'objets spatiaux (points d'intérêts immuables dans le temps ou événements éphémères) indépendants les uns des autres. Ensuite, nous proposons ANASTASIA, une approche de recommandation personnalisée de séquences d'activités lors des événements distribués. Notre approche est basée sur trois composants clés : (1) l'estimation de l'intérêt d'un utilisateur pour une activité, prenant en compte différentes influences, (2) l'intégration de motifs comportementaux d'utilisateurs basés sur leurs historiques d'activités et (3) la construction d'un planning ou séquence d'activités prenant en compte les contraintes spatio-temporelles de l'utilisateur et des activités. Nous explorons ainsi des méthodes issus de l'apprentissage de séquences et de l'optimisation discrète pour résoudre le problème. Enfin, nous démontrons le manque de jeu de données librement accessibles pour l'évaluation des algorithmes de recommandation d'événements et de séquences d'événements. Nous pallions à ce problème en proposant deux jeux de données, librement accessibles, que nous avons construits au cours de la thèse : Fantasy_db et DEvIR.
Les interactions sociales se trouvent au cœur des activités économiques. Pourtant en sciences économiques, elles ne sont traitées que d'une manière limitée en se concentrant uniquement aux rapports de qu'elles entretiennent avec le marché (Mankiw and Reis, 2002). Le rôle que jouent les interactions sociales vis-à-vis des comportements des agents, ainsi que la formation de leurs attentes sont souvent négligé. Cette négligence reste d'actualité malgré que les premières contributions dans la littérature économique les ont depuis longtemps déjà identifiées comme étant de déterminants importants pour la prise des décisions des agents économiques, comme par exemple Sherif (l936), Hyman (1942), Asch (1951), Jahoda (1959) ou Merlon (1968). En revanche, dans les études de consommation (une spécialité au croisement entre les sciences économiques, de la sociologie et de la psychologie), les interactions sociales (influences sociales) sont con­sidérées comme les "... déterminants dominants[...] du comportement de l'individu... " (Burnkrant and Cousineau, 1975). Le but de cette thèse est de construire un pont entre les interactions sociales et leur influence sur la formation des anticipations et le comportement des agents.
Cette thèse s'inscrit dans les domaines des systèmes Question Réponse en domaine restreint, la recherche d'information ainsi que TALN. Les systèmes de Question Réponse (QR) ont pour objectif de retrouver un fragment pertinent d'un document qui pourrait être considéré comme la meilleure réponse concise possible à une question de l'utilisateur. Le but de cette thèse est de proposer une approche de localisation de réponses dans des masses de données complexes et évolutives décrites ci-dessous. De nos jours, dans de nombreux domaines d'application, les systèmes informatiques sont instrumentés pour produire des rapports d'événements survenant, dans un format de données textuelles généralement appelé fichiers log. Les fichiers logs représentent la source principale d'informations sur l'état des systèmes, des produits, ou encore les causes de problèmes qui peuvent survenir. Les fichiers logs peuvent également inclure des données sur les paramètres critiques, les sorties de capteurs, ou une combinaison de ceux-ci. Ces fichiers sont également utilisés lors des différentes étapes du développement de logiciels, principalement dans l'objectif de débogage et le profilage. Bien que le processus de génération de fichiers logs est assez simple et direct, l'analyse de fichiers logs pourrait être une tâche difficile qui exige d'énormes ressources de calcul, de temps et de procédures sophistiquées [Valdman, 2004]. En effet, il existe de nombreux types de fichiers logs générés dans certains domaines d'application qui ne sont pas systématiquement exploités d'une manière efficace en raison de leurs caractéristiques particulières. Dans cette thèse, nous nous concentrerons sur un type des fichiers logs générés par des systèmes EDA (Electronic Design Automation). Ces fichiers logs contiennent des informations sur la configuration et la conception des Circuits Intégrés (CI) ainsi que les tests de vérification effectués sur eux. Ces informations, très peu exploitées actuellement, sont particulièrement attractives et intéressantes pour la gestion de conception, la surveillance et surtout la vérification de la qualité de conception. Cependant, la complexité de ces données textuelles complexes, c.-à-d. des fichiers logs générés par des outils de conception de CI, rend difficile l'exploitation de ces connaissances. Plusieurs aspects de ces fichiers logs ont été moins soulignés dans les méthodes de TALN et Extraction d'Information (EI). Le problème est accentué lorsque nous devons également prendre leurs structures évolutives et leur vocabulaire spécifique en compte. Dans ce contexte, un défi clé est de fournir des approches qui prennent les spécificités des fichiers logs en compte tout en considérant les enjeux qui sont spécifiques aux systèmes QR dans des domaines restreints. Ainsi, les contributions de cette thèse consistent brièvement en :  Au sein de cette approche, nous proposons un type original de descripteur qui permet de modéliser la structure textuelle et le layout des documents textuels. > Proposer une approche de la localisation de réponse (recherche de passages) dans les fichiers logs. Afin d'améliorer la performance de recherche de passage ainsi que surmonter certains problématiques dûs aux caractéristiques des fichiers logs, nous proposons une approches d'enrichissement de requêtes. Cela dit, nous proposons également une nouvelle fonction originale de pondération (scoring), appelée TRQ (Term Relatedness to Query) qui a pour objectif de donner un poids élevé aux termes qui ont une probabilité importante de faire partie des passages pertinents. Cette approche est également adaptée et évaluée dans les domaines généraux. > Etudier l'utilisation des connaissances morpho-syntaxiques au sein de nos approches. A cette fin, nous nous sommes intéressés à l'extraction de la terminologie dans les fichiers logs. Ainsi, nous proposons la méthode Exterlog, adaptée aux spécificités des logs, qui permet d'extraire des termes selon des patrons syntaxiques. Afin d'évaluer les termes extraits et en choisir les plus pertinents, nous proposons un protocole de validation automatique des termes qui utilise une mesure fondée sur le Web associée à des mesures statistiques, tout en prenant en compte le contexte spécialisé des logs.
Cette thèse examine la construction du sens et l'influence translangagière dans la représentation de la signification lexicale et du sens discursif. Nous effectuons cette étude à travers des discours définitionnels et des significations telles qu'elles sont proposées par des locuteurs amenés à exprimer leurs savoirs métalinguistiques sémantiques dans le cadre d'une recherche expérimentale. Le concept culturel de travail est analysé en adoptant le cadre théorique de la Sémantique des Possibles Argumentatifs de Galatanu. Une étude comparative du français et de l'anglais a permis de déterminer les ressemblances, ainsi que les divergences entre les représentations de la signification lexicale et du sens discursif des mots « travail » et « work » proposées par quatre groupes de locuteurs. Chaque groupe de locuteurs ayant son propre profil langagier, spécifiquement deux groupes de locuteurs monolingues (francophone et anglophone), un groupe constitué de locuteurs de langue maternelle française qui parlent l'anglais, et un groupe de locuteurs de langue maternelle anglaise qui parlent le français, présente l'opportunité de non seulement comparer les variations entre les représentations de la signification lexicale et du sens discursif des mots « travail » et « work » , mais aussi de constater l'influence potentielle d'une compétence dans une deuxième langue sur ces représentations. Dans un deuxième temps, cette thèse comporte une analyse de la représentation de l'insulte de loser aux États-Unis. Inspiré par la présence d'un élément liant le concept de work et ladite insulte, le choix de présenter loser rend possible une vision de la mobilisation discursive des valeurs sociales portées par les mots work et loser.
Les bases de connaissances sont des ensembles de faits, souvent sur des sujets encyclopédiques. Elles sont souvent utilisées pour la reconnaissance d'entités nommées, la recherche structurée, la réponse automatique à des questions, etc. Ces bases de connaissances doivent être maintenues, ce qui est une tâche cruciale mais coûteuse. Le sujet de cette thèse est la maintenance automatique de bases de connaissances à l'aide de contraintes. La première contribution de cette thèse est à propos de la découverte automatique de contraintes. Elle améliore les approches classiques d'apprentissage de règles en utilisant des méta-informations de complétude des données. Elle montre que que ces informations permettent d'améliorer de manière significative la qualité des règles trouvées. La seconde contribution est la création d'une base de connaissance, YAGO 4, qui assure le respect d'une série de contraintes en supprimant les faits qui n'y correspondent pas. La troisième contribution est une méthode pour corriger automatiquement les violations de contraintes. Cette méthode utilise l'historique des modifications de la base de connaissance afin de proposer des corrections, ceci à partir de la manière avec laquelle les utilisateurs de la base de connaissance ont déjà corrigé des violations similaires.
L'Intelligence Artificielle (IA) et les Interfaces Homme-Machine (IHM) sont deux champs de recherche avec relativement peu de travaux communs. Les spécialistes en IHM conçoivent habituellement les interfaces utilisateurs directement à partir d'observations et de mesures sur les interactions humaines, optimisant manuellement l'interface pour qu'elle corresponde au mieux aux attentes des utilisateurs. Ce processus est difficile à optimiser : l'ergonomie, l'intuitivité et la facilité d'utilisation sont autant de propriétés clé d'une interface utilisateur (IU) trop complexes pour être simplement modélisées à partir de données d'interaction. Ce constat restreint drastiquement les utilisations potentielles de l'apprentissage automatique dans ce processus de conception. A l'heure actuelle, l'apprentissage automatique dans les IHMs se cantonne majoritairement à la reconnaissance de gestes et à l'automatisation d'affichage, par exemple à des fins publicitaires ou pour suggérer une sélection. L'apprentissage automatique peut également être utilisé pour optimiser une interface utilisateur existante, mais il ne participe pour l'instant pas à concevoir de nouvelles façons d'intéragir. Notre objectif avec cette thèse est de proposer grâce à l'apprentissage automatique de nouvelles stratégies pour améliorer le processus de conception et les propriétés des IUs. Notre but est de définir de nouvelles IUs intelligentes – comprendre précises, intuitives et adaptatives – requérant un minimum d'interventions manuelles. Nous proposons une nouvelle approche à la conception d'IU : plutôt que l'utilisateur s'adapte à l'interface, nous cherchons à ce que l'utilisateur et l'interface s'adaptent mutuellement l'un à l'autre. Le but est d'une part de réduire le biais humain dans la conception de protocoles d'interactions, et d'autre part de construire des interfaces co-adaptatives capables de correspondre d'avantage aux préférences individuelles des utilisateurs. Pour ce faire, nous allons mettre à contribution les différents outils disponibles en apprentissage automatique afin d'apprendre automatiquement des comportements, des représentations et des prises de décision. Nous expérimenterons sur les interfaces tactiles pour deux raisons majeures : celles-ci sont largement utilisées et fournissent des problèmes facilement interprétables. La première partie de notre travail se focalisera sur le traitement des données tactiles et l'utilisation d'apprentissage supervisé pour la construction de classifieurs précis de gestes tactiles. La seconde partie détaillera comment l'apprentissage par renforcement peut être utilisé pour modéliser et apprendre des protocoles d'interaction en utilisant des gestes utilisateur. Enfin, nous combinerons ces modèles d'apprentissage par renforcement avec de l'apprentissage non supervisé pour définir une méthode de conception de nouveaux protocoles d'interaction ne nécessitant pas de données d'utilisation réelles.
La prolifération des données numériques a permis aux communautés de scientifiques et de praticiens de créer de nouvelles technologies basées sur les données pour mieux connaître les utilisateurs finaux et en particulier leur comportement. L'objectif est alors de fournir de meilleurs services et un meilleur support aux personnes dans leur expérience numérique. La majorité de ces technologies créées pour analyser le comportement humain utilisent très souvent des données de logs générées passivement au cours de l'interaction homme-machine. Une particularité de ces traces comportementales est qu'elles sont enregistrées et stockées selon une structure clairement définie. En revanche, les traces générées de manière proactive sont très peu structurées et représentent la grande majorité des données numériques existantes. À ce jour, malgré la prédominance des données textuelles et la pertinence des connaissances comportementales dans de nombreux domaines, les textes numériques sont encore insuffisamment étudiés en tant que traces du comportement humain pour révéler automatiquement des connaissances détaillées sur le comportement. Plusieurs contributions originales sont faites. Il y est menée la seule revue systématique existante à ce jour sur la modélisation automatique des conversations asynchrones avec des actes de langage. Une méthode automatique, indépendante du corpus, pour annoter les énoncées de communication asynchrone avec la taxonomie des intentions de discours proposée, est conçue sur la base d'un apprentissage automatique supervisé. Pour cela, deux corpus "ground-truth" validés sont créés et trois groupes de caractéristiques (discours, contenu et conversation) sont conçus pour être utilisés par les classificateurs. En particulier, certaines des caractéristiques du discours sont nouvelles et définies en considérant des moyens linguistiques pour exprimer des intentions de discours,sans s'appuyer sur le contenu explicite du corpus, le domaine ou les spécificités des types de communication asynchrones. Une méthode automatique basée sur la fouille de processus est conçue pour générer des modèles de processus d'intentions de discours interdépendantes à partir de tours de parole, annotés avec plusieurs labels par phrase. Comme la fouille de processus repose sur des logs d'événements structurés et bien définis, un algorithme est proposé pour produire de tels logs d'événements à partir de conversations. Par ailleurs, d'autres solutions pour transformer les conversations annotées avec plusieurs labels par phrase en logs d'événements, ainsi que l'impact des différentes décisions sur les modèles comportementaux en sortie sont analysées afin d'alimenter de futures recherches.
Nous présentons une solution pour la création d'un corpus numérisé en utilisant une approche crowdsourcing pour annoter des bandes dessinées (BD). Les encodages XML qui en résultent aident également les chercheurs, les éditeurs, les bibliothécaires et les conservateurs de collections BDs. Pour atteindre notre objectif de recueil de données, nous développons un moteur de crowdsourcing en ligne pour annoter les BDs. Les tâches sont conçues pour reproduire l'expérience de lecture des pages des BDs, en demandant aux participants d'identifier et d'annoter les éléments structurels (cases, splash-pages) et de contenu (personnages, lieux, événements, onomatopées, objets, lignes de mouvement) des BDs. Les bibliothécaires et les conservateurs de collections des BDs disposeront d'un contenu structuré qui pourrait permettre la création d'artefacts spécifiques, tels que des dictionnaires de BDs, des indices de recherche ou des dictionnaires d'onomatopée. Du point de vue de l'édition, les standards actuels pour les BDs numériques se chargent exclusivement de la couche de présentation (c'est-à-dire, rendre tout simplement une publication sur l'écran d'un dispositif numérique). Mais la nature artistique de la BD et le grand potentiel des BDs numériques nous permettent d'aller au-delà de la simple présentation du contenu. À cet égard, nous contribuons avec des améliorations aux standards sémantiques (CBML) et de présentation (EPUB).
Les mécanismes de compréhension chez l'être humain sont par essence multimodaux. Comprendre le monde qui l'entoure revient chez l'être humain à fusionner l'information issue de l'ensemble de ses récepteurs sensoriels. Le but de cette thèse est de proposer des traitements joints s'appliquant principalement au texte et à l'image pour le traitement de documents multimodaux à travers deux études : l'une portant sur la fusion multimodale pour la reconnaissance du rôle du locuteur dans des émissions télévisuelles, l'autre portant sur la complémentarité des modalités pour une tâche d'analyse linguistique sur des corpus d'images avec légendes. Pour la première étude nous nous intéressons à l'analyse de documents audiovisuels provenant de chaînes d'information télévisuelle. Nous proposons une approche utilisant des réseaux de neurones profonds pour la création d'une représentation jointe multimodale pour les représentations et la fusion des modalités. Dans la seconde partie de cette thèse nous nous intéressons aux approches permettant d'utiliser plusieurs sources d'informations multimodales pour une tâche monomodale de traitement automatique du langage, afin d'étudier leur complémentarité. Nous proposons un système complet de correction de rattachements prépositionnels utilisant de l'information visuelle, entraîné sur un corpus multimodal d'images avec légendes.
Nos perspectives sont éducatives : créer des exercices de grammaire pour le français. La paraphrase est une opération de reformulation. Nos travaux tendent à attester que les modèles séquence vers séquence ne sont pas de simples répétiteurs mais peuvent apprendre la syntaxe. Nous avons montré, en combinant divers modèles, que la représentation de l'information sous de multiples formes (en utilisant de la donnée formelle (RDF), couplée à du texte pour l'étendre ou le réduire, ou encore seulement du texte) permet d'exploiter un corpus sous différents angles, augmentant la diversité des sorties, exploitant les leviers syntaxiques mis en place. Nous nous sommes penchée sur un problème récurrent, celui de la qualité des données, et avons obtenu des paraphrases avec une haute adéquation syntaxique (jusqu'à 98% de couverture de la demande) et un très bon niveau linguistique. Nous obtenons jusqu'à 83.97 points de BLEU*, 78.41 de plus que la moyenne de nos lignes de base, sans levier syntaxique. Ce taux indique un meilleur contrôle des sorties, pourtant variées et de bonne qualité en l'absence de levier. Le passage à du texte en français était aussi pour nous un impératif. Travailler depuis du texte brut, en automatisant les procédures, nous a permis de créer un corpus de plus de 450 000 couples représentations/phrases, grâce auquel nous avons appris à générer des textes massivement corrects (92% sur la validation qualitative). Anonymiser ce qui n'est pas fonctionnel a participé notablement à la qualité des résultats (68.31 de BLEU, soit +3.96 par rapport à la ligne de base, qui était la génération depuis des données non anonymisées). La représentation formelle de l'information dans un cadre linguistique particulier à une langue est une tâche ardue. Cette thèse offre des pistes de méthodes pour automatiser cette opération. Par ailleurs, nous n'avons pu traiter que des phrases relativement courtes. L'utilisation de modèles neuronaux plus récents permettrait sans doute d'améliorer les résultats. Enfin, l'usage de traits adéquats en sortie permettrait des vérifications poussées. *BLEU (Papineni et al., 2002) : qualité d'un texte sur une échelle de 0 (pire) à 100 (meilleur)
Un nombre incalculable de documents est imprimé, numérisé, faxé, photographié chaque jour. Ces documents sont hybrides : ils existent sous forme papier et numérique. De plus les documents numériques peuvent être consultés et modifiés simultanément dans de nombreux endroits. Avec la disponibilité des logiciels d'édition d'image, il est devenu très facile de modifier ou de falsifier un document. Cela crée un besoin croissant pour un système d'authentification capable de traiter ces documents hybrides. D'autres solutions reposent sur une vérification visuelle et offrent seulement Afin de surmonter tous ces problèmes, nous proposons de créer un algorithme de hachage sémantique pour les images de documents. Cet algorithme de hachage devrait fournir une signature compacte pour toutes les informations visuellement significatives contenues dans le document. Ce condensé permettra la création de systèmes de sécurité hybrides pour sécuriser tout le document. Ceci peut être réalisé grâce à des algorithmes d'analyse du document. Après avoir défini le contexte de l'étude et ce qu'est un algorithme stable, nous nous sommes attachés à produire des algorithmes stables pour la description de la mise en page, la segmentation d'un document, la reconnaissance de caractères et la description des zones graphiques.
L'objectif de la thèse sera de développer des modèles liant modèles de Markov et réseaux de neurones et d'en étudier des applications, notamment pour les systèmes de Questions-Réponses, pour ensuite les mettre en place sur des agents conversationnels. Les algorithmes les plus utilisés pour les systèmes de Questions-Réponses aujourd'hui reposent sur des modèles de Réseaux de Neurones. Ceux-ci peuvent s'interpréter comme des modèles probabilistes cachés, dont le même problème peut être abordés par des modèles de Markov cachés. Ceux-ci ont, depuis une quinzaine d'année, été étendu en modèles de Markov « couples » et « triplets » , permettant d'obtenir des résultats parfois spectaculaires. Le doctorant s'inspirera alors des démarches amenant à la création des modèles de Markov couples et triplets à partir du modèle de Markov cachés pour proposer des modèles de réseaux de neurones « couples » et « triplets » , et des modèles hybrides entre réseau de neurones et modèles de Markov. Il en recherchera des applications, notamment pour les systèmes de Questions-Réponse, et observera les apports de ces nouveaux modèles sur les réseaux de neurones classiques.
La création d'un centre d'appel d'urgence pour les sourds et malentendants a nécessité la conception et la mise en place d'un dispositif sociotechnique ad hoc respectant la diversité de leurs pratiques de communication. Ancrée dans ce projet technique mené afin de rendre accessible une institution organisée autour de l'interaction téléphonique, cette recherche fut l'occasion de conduire une réflexion croisée sur les dispositifs techniques de communication appréhendés comme outil de travail et outil d'accessibilité. À partir de matériaux issus d'une enquête ethnographique réalisée dans les centres d'appel d'urgence que nous faisons dialoguer avec des traces de l'activité de conception, nous interrogeons, dans un même élan, le rôle que tiennent ces dispositifs dans l'écologie des situations ainsi que la manière dont ils définissent en actes la dynamique contemporaine de prise en compte environnementale du handicap. Au travers de ce questionnement deux grandes thématiques sont explorées : le travail en situation de communication médiatisée et le processus reformulation des pratiques engagé par la conception et la mise en service d'un dispositif technique d'accessibilité. Détaillant finement les multiples cours d'action en nous attachant notamment à rematérialiser les pratiques interactionnelles, nous démontrons en quoi les dispositifs techniques de communication participent d'une armature invisible qui fait système et contribue à organiser et pérenniser les pratiques ; armature que nous appréhendons à travers la notion d'infrastructure communicationnelle. Ainsi, nous proposons cette dernière notion comme outil conceptuel pour appréhender l'introduction de nouveaux dispositifs dans les organisations et pour accompagner une réflexion sur l'accessibilité-en-pratiques.
Les systèmes de traitement automatique des langues reposent souvent sur l'idée que le langage est compositionnel, c'est-à-dire que le sens d'une entité linguistique peut être déduite à partir du sens de ses parties. Cette supposition ne s'avère pas vraie dans le cas des expressions polylexicales (EPLs). Par exemple, une "poule mouillée" n'est ni une poule, ni nécessairement mouillée. Les techniques pour déduire le sens des mots en fonction de leur distribution dans le texte ont obtenu de bons résultats sur plusieurs tâches, en particulier depuis l'apparition des word embeddings. Cependant, la représentation des EPLs reste toujours un problème non résolu. En particulier, on ne sait pas comment prédire avec précision, à partir des corpus, si une EPL donnée doit être traitée comme une unité indivisible (p.ex. "carton plein") ou comme une combinaison du sens de ses parties (p.ex. "eau potable"). Cette thèse propose un cadre méthodologique pour la prédiction de compositionnalité d'EPLs fondé sur des représentations de la sémantique distributionnelle, que nous instancions à partir d'une variété de paramètres. Nous présenterons une évaluation complète de l'impact de ces paramètres sur trois nouveaux ensembles de données modélisant la compositionnalité d'EPLs, en anglais, français et portugais. Finalement, nous présenterons une évaluation extrinsèque des niveaux de compositionnalité prédits par le modèle dans le contexte d'un système d'identification d'EPLs. Les résultats suggèrent que le choix spécifique de modèle distributionnel et de paramètres de corpus peut produire des prédictions de compositionnalité qui sont comparables à celles présentées dans l'état de l'art.
En préparation à l'utilisation d'infrastructures distribuées pour accélérer le calcul, cette étude vise à explorer la possibilité d'exécuter l'algorithme de Frank-Wolfe dans un réseau en étoile avec le modèle BSP (Bulk Synchronous Parallel) et à étudier son efficacité théorique et empirique. Concernant l'aspect théorique, cette étude revisite le taux de convergence déterministe de Frank-Wolfe et l'étend à des cas non déterministes. En particulier, il montre qu'avec le sous-problème linéaire résolu de manière appropriée, Frank-Wolfe peut atteindre un taux de convergence sous-linéaire à la fois en espérance et avec une probabilité élevée. Cette contribution pose la fondation théorique de l'utilisation de la méthode de la puissance itérée ou de l'algorithme de Lanczos pour résoudre le sous-problème linéaire de Frank-Wolfe associé à la minimisation de la norme de trace. Concernant l'aspect algorithmique, dans le cadre de BSP, cette étude propose et analyse quatre stratégies pour le sous-problème linéaire ainsi que des méthodes pour la recherche linéaire. En outre, remarquant la propriété de mise à jour de rang-1 de Frank-Wolfe, il met à jour le gradient de manière récursive, avec une représentation dense ou de rang faible, au lieu de le recalculer de manière répétée à partir de zéro. Toutes ces conceptions sont génériques et s'appliquent à toutes les infrastructures distribuées compatibles avec le modèle BSP. Concernant l'aspect empirique, cette étude teste les conceptions algorithmiques proposées dans un cluster Apache SPARK. Selon les résultats des expériences, pour le sous-problème linéaire, la centralisation des gradients ou la moyenne des vecteurs singuliers est suffisante dans le cas de faible dimension, alors que la méthode de la puissance itérée distribuée, avec aussi peu qu'une ou deux itérations par époque, excelle dans le cas de grande dimension. La librairie Python développée pour les expériences est modulaire, extensible et prête à être déployée dans un contexte industriel. Cette étude a rempli sa fonction de preuve de concept. Suivant le chemin qu'il met en place, des solveurs peuvent être implémentés pour différentes infrastructures, parmi lesquelles des clusters GPU, pour résoudre des problèmes pratiques dans des contextes spécifiques. En outre, ses excellentes performances dans le jeu de données ImageNet le rendent prometteur pour l'apprentissage en profondeur.
Un mapping d'ontologies est un ensemble de correspondances. Chaque correspondance relie des artefacts, typiquement concepts et propriétés, d'une ontologie avec ceux d'une autre ontologie. Le mapping entre ontologies a suscité beaucoup d'intérêt durant ces dernières années. En effet, le mapping d'ontologies est largement utilisé pour mettre en oeuvre de l'interopérabilité et intégration (transformation de données, réponse à la requête, composition de web service) dans les applications, et également dans la création de nouvelles ontologies. D'une part, vérifier l'exactitude (logique) d'un mapping est devenu un prérequis fondamentale à son utilisation. D'autre part, pour deux ontologies données, plusieurs mappings peuvent être établis, obtenus par différentes méthodes d'alignement, ou définis manuellement. L'utilisation de plusieurs mappings entre deux ontologies dans une seule application ou pour synthétiser un seul mapping tirant profit de ces plusieurs mappings, peut générer des erreurs dans l'application ou dans le mapping synthétisé car ces plusieurs mappings peuvent être contradictoires. Dans les deux situations décrites ci-dessus, l'exactitude, la non-contradiction et autres propriétés sont généralement exprimées de façon formelle et vérifiées dans le contexte des ontologies formelles (par exemple, lorsque les ontologies sont représentées en logique) La vérification de ces propriétés est généralement effectuée à l'aide d'un seul formalisme, exigeant d'une part que les ontologies soient représentées par ce seul formalisme et, d'autre part, qu'une représentation formelle des mappings soit fournie, complétée par des notions formalisant les propriétés recherchées. Cependant, il existe une multitude de formalismes hétérogènes pour exprimer les ontologies, allant des plus informels (par exemple, du texte contrôlé, des modèles en UML) aux formels (par exemple, des logiques de description ou des catégories). Ceci implique que pour appliquer les approches existantes, les ontologies hétérogènes doivent être traduites (ou juste transformées, si l'ontologie source est exprimée de façon informelle ou si la traduction complète pour maintenir l'équivalence n'est pas possible) dans un seul formalisme commun et les mappings sont reformulés à chaque fois : seulement à l'issu de ce processus, les propriétés recherchées peuvent être établies. Même si cela est possible, ce processus peut produire à la fois des mappings corrects et incorrects vis-à-vis de ces propriétés, en fonction de la traduction (transformation) opérée. En effet, les propriétés recherchées dépendent du formalisme employé pour exprimer les ontologies et les mappings. Dans ce contexte, les ontologies sont représentées comme treillis, et les mappings sont reformulés comme fonctions entre ces treillis. Les treillis sont des structures naturelles pour la représentation directe d'ontologies sans obligation de traduire ou transformer les formalismes dans lesquels les ontologies sont exprimées à l'origine. Cette reformulation unifiée a permis d'introduire une nouvelle notion de mappings compatibles et incompatibles. Il est ensuite formellement démontré que cette nouvelle notion couvre plusieurs parmi les propriétés recherchées de mappings, mentionnées dans l'état de l'art. L'utilisation directe de mappings compatibles et incompatibles est démontrée par l'application à des mappings d'ontologies de haut niveau. La notion de mappings compatibles et incompatibles est aussi appliquée sur des ontologies de domaine, mettant en évidence comment les mappings incompatibles génèrent des résultats incorrects pour la fusion d'ontologies.
Plusieurs découvertes récentes, provenant notamment des neurosciences cognitives, ont eu des retombées dans les sciences humaines et plus particulièrement dans le champ de la linguistique. Elles ont suscité un renouveau de l'intérêt pour la thématique de l'iconicité phonologique, qui s'intéresse à l'ensemble des phénomènes de similarité entre signifiant et signifié à l'intérieur d'une langue. Une multitude d'études a alors vu le jour, attestant l'existence de phénomènes phonosymboliques dans les langues du monde. Malgré cet essor considérable, les contenus de ces travaux, principalement rédigés en anglais, demeurent à ce jour assez méconnu du public francophone, encore relativement ancré dans la tradition structuraliste. Cette thèse présente tout d'abord une synthèse des travaux internationaux menés dans le champ de l'iconicité phonologique en vue d'en retenir les principaux acquis. Sur ces bases, elle apporte de nouvelles preuves empiriques de l'existence de phénomènes iconiques dans un corpus de verbes monosyllabiques français à travers deux méthodes.
Les systèmes de traduction automatique (TA), qui génèrent automatiquement la phrase de la langue cible pour chaque entrée de la langue source, ont obtenu plusieurs réalisations convaincantes pendant les dernières décennies et deviennent les aides linguistiques efficaces pour la communauté entière dans un monde globalisé. Néanmoins, en raison de différents facteurs, sa qualité en général est encore loin de la perfection, constituant le désir des utilisateurs de savoir le niveau de confiance qu'ils peuvent mettre sur une traduction spécifique. La construction d'une méthode qui est capable d'indiquer des bonnes parties ainsi que d'identifier des erreurs de la traduction est absolument une bénéfice pour non seulement les utilisateurs, mais aussi les traducteurs, post-éditeurs, et les systèmes de TA eux-mêmes. Nous appelons cette méthode les mesures de confiance (MC). Cette thèse se porte principalement sur les méthodes des MC au niveau des mots (MCM). Le système de MCM assigne à chaque mot de la phrase cible un étiquette de qualité. Aujourd'hui, les MCM jouent un rôle croissant dans nombreux aspects de TA. Tout d'abord, elles aident les post-éditeurs d'identifier rapidement les erreurs dans la traduction et donc d'améliorer leur productivité de travail. De plus, elles informent les lecteurs des portions qui ne sont pas fiables pour éviter leur malentendu sur le contenu de la phrase. Troisièmement, elles sélectionnent la meilleure traduction parmi les sorties de plusieurs systèmes de TA. Finalement, et ce qui n'est pas le moins important, les scores MCM peuvent aider à perfectionner la qualité de TA via certains scénarios : ré-ordonnance des listes N-best, ré-décodage du graphique de la recherche, etc. Dans cette thèse, nous visons à renforcer et optimiser notre système de MCM, puis à l'exploiter pour améliorer TA ainsi que les mesures de confiance au niveau des phrases (MCP). Comparer avec les approches précédentes, nos nouvelles contributions étalent sur les points principaux comme suivants. Tout d'abord, nous intégrons différents types des paramètres : ceux qui sont extraits du système TA, avec des caractéristiques lexicales, syntaxiques et sémantiques pour construire le système MCM de base. L'application de différents méthodes d'apprentissage nous permet d'identifier la meilleure (méthode : "Champs conditionnels aléatoires") qui convient le plus nos donnés. En suite, l'efficacité de touts les paramètres est plus profond examinée en utilisant un algorithme heuristique de sélection des paramètres. Troisièmement, nous exploitons l'algorithme Boosting comme notre méthode d'apprentissage afin de renforcer la contribution des sous-ensembles des paramètres dominants du système MCM, et en conséquence d'améliorer la capacité de prédiction du système MCM. En outre, nous enquérons les contributions des MCM vers l'amélioration de la qualité de TA via différents scénarios. Dans le re-ordonnance des liste N-best, nous synthétisons les scores à partir des sorties du système MCM et puis les intégrons avec les autres scores du décodeur afin de recalculer la valeur de la fonction objective, qui nous permet d'obtenir un mieux candidat. D'ailleurs, dans le ré-décodage du graphique de la recherche, nous appliquons des scores de MCM directement aux noeuds contenant chaque mot pour mettre à jour leurs coûts. Finalement, les scores de MCM sont aussi utilisés pour renforcer les performances des systèmes de MCP. Au total, notre travail apporte une image perspicace et multidimensionnelle sur des MCM et leurs impacts positifs sur différents secteurs de la TA. Les résultats très prometteurs ouvrent une grande avenue où MCM peuvent exprimer leur rôle, comme : MCM pour la reconnaissance automatique de la parole (RAP), pour la sélection parmi plusieurs systèmes de TA, et pour les systèmes de TA auto-apprentissage.
Les avancées récentes en matière d'animation ont permis le déploiement de personnages virtuels à des fins diverses et variées. Cependant, la génération d'animations pour ces personnages dépend de la description lexicale des signes, modèle linguistique dépendant du système de génération. Les signes décrits par ces modèles sont généralement des réalisations parfaites et géométriques menant à des mouvements robotiques et peu naturels de la part du signeur. Cette thèse s'intéresse à l'addition d'informations anatomiques au squelette de contrôle du personnage virtuel de manière à le faire signer de manière plus humaine et réaliste. Ces informations supplémentaires sont regroupées sous l'appellation de "modèle anatomique" et sont divisées en cinq contributions principales : une nouvelle représentation informatique du squelette, une étude anthropométrique sur la main, l'unification de dépendances articulatoires, un nouveau modèle de complexe carpo-métacarpien permettant l'opposition aisée du pouce et enfin un modèle calculant le confort d'une posture. Ces apports sont intégrés à une plateforme de génération au moyen de techniques adaptées aux contraintes imposées par les modèles linguistiques. Les travaux sont conclus par une évaluation du système ainsi qu'une réflexion sur les travaux futurs pouvant être élaborés à partir de cette thèse.
En linguistique informatique, la relation entre langues différentes est souvent étudiée via des techniques d'alignement automatique. De tels alignements peuvent être établis à plusieurs niveaux structurels. En particulier, les alignements debi-textes aux niveaux phrastiques et sous-phrastiques constituent des sources importantes d'information dans pour diverses applications du Traitement Automatique du Language Naturel (TALN) moderne, la Traduction Automatique étant un exemple proéminent. Cependant, le calcul effectif des alignements de bi-textes peut être une tâche compliquée. Les divergences entre les langues sont multiples,de la structure de discours aux constructions morphologiques. Les alignements automatiques contiennent, majoritairement, des erreurs nuisant aux performances des applications. Dans cette situation, deux pistes de recherche émergent. La première est de continuer à améliorer les techniques d'alignement. La deuxième vise à développer des mesures de confiance fiables qui permettent aux applications de sélectionner les alignements selon leurs besoins. Les techniques d'alignement et l'estimation de confiance peuvent tous les deux bénéficier d'alignements manuels. Des alignements manuels peuvent jouer un rôle de supervision pour entraîner des modèles, et celui des données d'évaluation. Pourtant, la création des telles données est elle-même une question importante, en particulier au niveau sous-phrastique, où les correspondances multilingues peuvent être implicites et difficiles à capturer. Cette thèse étudie des moyens pour acquérir des alignements de bi-textes utiles, aux niveaux phrastiques et sous-phrastiques. Le chapitre 1 fournit une description de nos motivations,la portée et l'organisation du travail, et introduit quelques repères terminologiques et les principales notations. L'état-de-l'art des techniques d'alignement est revu dans la Partie I. Les chapitres 2 et 3 décrivent les méthodes respectivement pour l'alignement des phrases et des mots. Le chapitre 4 présente les bases de données d'alignement manuel,et discute de la création d'alignements de référence. Le reste de la thèse, la Partie II,présente nos contributions à l'alignement de bi-textes, en étudiant trois aspects. Le chapitre 5 présente notre contribution à la collection d'alignements de référence. Pour l'alignement des phrases, nous collectons les annotations d'un genre spécifique de textes : les bi-textes littéraires. Nous proposons aussi un schéma d'annotation de confiance. Pour l'alignement sous-phrastique,nous annotons les liens entre mots isolés avec une nouvelle catégorisation, et concevons une approche innovante de segmentation itérative pour faciliter l'annotation des liens entre groupes de mots. Toutes les données collectées sont disponibles en ligne. L'amélioration des méthodes d'alignement reste un sujet important de la recherche. Nous prêtons une attention particulière à l'alignement phrastique, qui est souvent le point de départ de l'alignement de bi-textes. Le chapitre 6 présente notre contribution. En commençant par évaluer les outils d'alignement d'état-de-l'art et par analyser leurs modèles et résultats,nous proposons deux nouvelles méthodes pour l'alignement phrastique, qui obtiennent des performances d'état-de-l'art sur un jeu de données difficile. L'autre sujet important d'étude est l'estimation de confiance. Dans le chapitre 7, nous proposons des mesures de confiance pour les alignements phrastique et sous-phrastique. Les expériences montrent que l'estimation de confiance des liens d'alignement reste un défi remarquable. Il sera très utile de poursuivre cette étude pour renforcer les mesures de confiance pour l'alignement de bi-textes. Enfin, notons que les contributions apportées dans cette thèse sont employées dans une application réelle : le développement d'une liseuse qui vise à faciliter la lecture des livres électroniques multilingues.
Cette étude est dédiée à un problème d'exploration de données dans les médias sociaux : la prédiction d'activité. Dans ce problème nous essayons de prédire l'activité associée à une thématique pour un horizon temporel restreint. Dans ce problème des contenus générés par différents utilisateurs, n'ayant pas de lien entre eux, contribuent à l'activité d'une même thématique. Trois définitions de la prédiction d'activité sont proposées. Premièrement la prédiction de la magnitude d'activité, un problème de régression qui vise à prédire l'activité exacte d'une thématique. Ces trois problèmes sont étudiés avec les méthodes de l'état de l'art en apprentissage automatique. Les descripteurs proposés pour ces études sont définis en utilisant le cadre d'analyse générique. Ainsi il est facile d'adapter ces descripteurs à différent média sociaux. Plus de 500 millions de contenus générés par les utilisateurs ont été capturé. Une méthode de validation croisée est proposée afin de ne pas introduire de biais expérimental lié au temps. De plus, une méthode d'extraction non-supervisée des candidats au buzz est proposée. En effet, les changements abrupts de popularité sont rares et l'ensemble d'entraˆınement est très déséquilibré. Les problèmes de prédiction de l'activité sont étudiés dans deux configurations expérimentales différentes. La première configuration expérimentale porte sur l'ensemble des données collectées dans les deux médias sociaux, et sur les trois langues observées. La seconde configuration expérimentale porte exclusivement sur Twitter. Cette seconde configuration expérimentale vise à améliorer la reproductibilité de nos expériences. Pour ce faire, nous nous concentrons sur un sous-ensemble des thématiques non ambigu¨es en Anglais.
Ce travail de thèse traite de la découverte de connaissances à partir de Séries Temporelles de Champs de Déplacements (STCD) obtenues par imagerie satellitaire. De telles séries occupent aujourd'hui une place centrale dans l'étude et la surveillance de phénomènes naturels tels que les tremblements de terre, les éruptions volcaniques ou bien encore le déplacement des glaciers. En effet, ces séries sont riches d'informations à la fois spatiales et temporelles et peuvent aujourd'hui être produites régulièrement à moindre coût grâce à des programmes spatiaux tels que le programme européen Copernicus et ses satellites phares Sentinel. Nos propositions s'appuient sur l'extraction de motifs Séquentiels Fréquents Groupés (SFG). Néanmoins, ils ne permettent pas d'utiliser les indices de confiance intrinsèques aux STCD et la méthode de swap randomisation employée pour sélectionner les motifs les plus prometteurs ne tient pas compte de leurs complémentarités spatiotemporelles, chaque motif étant évalué individuellement. Notre contribution est ainsi double. Une première proposition vise tout d'abord à associer une mesure de fiabilité à chaque motif en utilisant les indices de confiance. Cette mesure permet de sélectionner les motifs portés par des données qui sont en moyenne suffisamment fiables. Nous proposons un algorithme correspondant pour réaliser les extractions sous contrainte de fiabilité. Celui-ci s'appuie notamment sur une recherche efficace des occurrences les plus fiables par programmation dynamique et sur un élagage de l'espace de recherche grâce à une stratégie de push partiel, ce qui permet de considérer des STCD conséquentes. Une deuxième contribution visant à sélectionner les motifs les plus prometteurs est également présentée. Celle-ci, basée sur un critère informationnel, permet de prendre en compte à la fois les indices de confiance et la façon dont les motifs se complètent spatialement et temporellement. Pour ce faire, les indices de confiance sont interprétés comme des probabilités, et les STCD comme des bases de données probabilistes dont les distributions ne sont que partielles. Le gain informationnel associé à un motif est alors défini en fonction de la capacité de ses occurrences à compléter/affiner les distributions caractérisant les données. Sur cette base, une heuristique est proposée afin de sélectionner des motifs informatifs et complémentaires. Cette méthode permet de fournir un ensemble de motifs faiblement redondants et donc plus faciles à interpréter que ceux fournis par swap randomisation. Elle a été implémentée au sein d'un prototype dédié. Outre le fait d'être construites à partir de données et de techniques de télédétection différentes, ces séries se différencient drastiquement en termes d'indices de confiance, la série couvrant le massif du Mont-Blanc se situant à des niveaux de confiance très faibles. Pour les deux STCD, les méthodes proposées ont été mises en œuvre dans des conditions standards au niveau consommation de ressources (temps, espace), et les connaissances des experts sur les zones étudiées ont été confirmées et complétées.
Les Agents Conversationnels Animés sont des personnages virtuels dont la fonction principale est d'interagir avec l'utilisateur. Les utilisateurs, conscient d'interagir avec une machine, sont tout de même capable d'analyser et d'identifier des comportements sociaux à travers les signaux émis par les agents. La recherche en ACA s'est longtemps intéressée aux mécanismes de reproduction et de reconnaissance des émotions au sein de ces personnages virtuels et maintenant l'intérêt se porte sur la capacité d'exprimer différentes attitudes sociales. Ces attitudes reflètent un style comportemental et s'expriment à travers différentes modalités du corps comme les expressions faciales, les regards ou les gestes par exemple. Nous avons proposé un modèle permettant à un agent de produire différents comportements non-verbaux traduisant l'expression d'attitudes sociales dans une conversation. L'ensemble des comportements générés par notre modèle permettent à un groupe d'agents animés par celui-ci de simuler une conversation, sans tenir compte du contenu verbal. Deux évaluations du modèle ont été conduites, l'une sur Internet et l'autre dans un environnement de réalité virtuelle, afin de vérifier que les attitudes étaient bien reconnues.
Dans cette thèse, nous nous intéressons à la manière dont les images SAR peuvent être utilisées pour étudier la végétation. La végétation est au coeur de la vie humaine en fournissant à la fois des ressources alimentaires, financières et en participant à la régulation du climat. Traditionnellement, la végétation est classée en trois catégories : les champs, les prairies irriguées et les forêts. Nous utiliserons ces trois catégories dans notre étude. L'objectif de la première partie est de fournir une meilleure compréhension du potentiel des images radar Sentinel-1 (bande C) pour cartographier l'occupation du sol à l'aide des techniques d'apprentissage en profondeur. Nous avons obtenu de bons résultats avec la "F-Measure/Accuracy" supérieure à 86% et le meilleur coefficient Kappa de plus de 0,82. Nous avons constaté que les résultats des deux classificateurs basés sur les réseaux neuronaux récurrents profonds (RNN) dépassaient clairement les approches classiques de Machine Learning. Dans la seconde partie, l'objectif est d'étudier la capacité des images radar multitemporelles pour l'estimation de la hauteur du riz et de la biomasse sèche à l'aide des données Sentinel-1. Pour ce faire, nous avons utilisé les données de Sentinel-1 en appliquant des techniques classiques d'apprentissage de "Machine Learning" (MLR, SVR et RF) pour estimer la hauteur du riz et la biomasse sèche. Ces résultats indiquent que les données radar Sentinel-1 pourraient être exploitées pour la récupération de la biomasse et pourraient être utilisées pour des tâches opérationnelles. Enfin, la réduction des émissions de carbone dues à la déforestation nécessite un aperçu de la façon dont la forêt de biomasse est mesurée et distribuée. Nous utilisons des observations du radar satellitaire ALOS/PALSAR (résolution de 25 m) et des données optiques du capteur Landsat (résolution de 30 m) pour estimer les stocks de biomasse forestière à Madagascar, pour les années 2007-2010. Le signal radar et la biomasse in situ étaient fortement corrélés (R² =0,71) et l'erreur quadratique moyenne était de 30% (pour la biomasse allant de 0 à 500 t/ha). Le signal radar (données SAR en bande L) combiné avec les données optiques semblent être une approche prometteuse pour cartographier la biomasse forestière (et donc du carbone) à de larges échelles géographiques.
Notre travail est présenté en trois parties indépendantes. Tout d'abord, nous proposons trois heuristiques d'apprentissage actif pour les réseaux de neurones profonds : Nous mettons à l'échelle le `query by committee', qui agrège la décision de sélectionner ou non une donnée par le vote d'un comité. Pour se faire nous formons le comité à l'aide de différents masques de dropout. Un autre travail se base sur la distance des exemples à la marge. Nous proposons d'utiliser les exemples adversaires comme une approximation de la dite distance. Nous démontrons également des bornes de convergence de notre méthode dans le cas de réseaux linéaires. Notre méthode sélectionne les données qui minimisent l'énergie libre variationnelle. Dans un second temps, nous nous sommes concentrés sur la distance de Wasserstein. Nous projetons les distributions dans un espace où la distance euclidienne mimique la distance de Wasserstein. Également, nous démontrons les propriétés sous-modulaires des prototypes de Wasserstein et comment les appliquer à l'apprentissage actif. Enfin, nous proposons de nouveaux outils de visualisation pour expliquer les prédictions d'un CNN sur du langage naturel. Deuxièmement, nous profitons des algorithmes de déconvolution des CNNs afin de présenter une nouvelle perspective sur l'analyse d'un texte.
Le but de cette thèse est de caractériser ce qui se passe à la jonction entre le système linguistique et le système conceptuel de notre esprit. Nous défendons la thèse selon laquelle le système linguistique de notre esprit encode des règles permettant de manipuler directement des représentations mentales dotées d'un contenu sémantique. Après avoir examiné les propositions des formalistes du début du XXe siècle (Frege, Russell), des sémanticiens formels (Montague, Lewis) et de la pragmatique de Grice, nous développons notre propre théorie. Nous testons alors notre théorie en l'appliquant au problème des implicatures enchâssées et terminons en signalant ses applications possibles en TALN, à la clarification de l'hypothèse du langage de la pensée et à la distinction sémantique/pragmatique.
Objectif de cette thèse est le développent de méthodes de planification pour la résolution de tâches jointes homme-robot dans des espaces publiques. La thèse décrit l'état de l'art sur la coopération homme-robot dans la robotique de service, et sur les modèles de planification. La thèse introduit une structure hiérarchique qui sépare l'aspect coopératif d'une activité jointe de la tâche en soi. L'approche a été appliquée dans un scénario réel, un robot guide dans un centre commercial. La thèse présente les expériences effectuées pour mesurer la qualité de l'approche proposée, ainsi que les expériences avec le robot réel.
Cette thèse vise à établir une théorie sémantique des textes appelée herméneutique formelle qui applique des méthodes mathématiques rigoureuses dans l'étude des processus d'interprétation des textes en langue naturelle, dits admissibles, que nous dirons écrits « avec bonne volonté » en tant que messages destinés à la compréhension. Dans le paradigme phonocentrique de lecture, une langue est décrite dans une catégorie Logos dite des espaces textuels. Un genre particulier des textes y définit une sous-catégorie pleine des schémas formels discursifs. Définies pour un texte X donné, la catégorie Schl(X) des faisceaux des significations fragmentaires, dite de Schleiermacher, sert à formaliser un principe compositionnel généralisé de Frege, et la catégorie Context(X) des espaces étalés des significations contextuelles sert à formaliser un principe contextuel généralisé de Frege. Établie par le foncteur de sections et le foncteur de germes, une équivalence de catégories Schl(X) ?Context(X), dite dualité de Frege, donne lieu à une représentation fonctionnelle des significations fragmentaires, ce qui permet de décrire le processus de la compréhension d'un texte. Nous considérons comme universaux linguistiques la connexité et la T0–séparabilité de Kolmogoroff de la topologie phonocentrique sous-jacente à un texte.
Les bases de données lexicales jouent un grand rôle dans le TAL, mais, elles nécessitent un développement et un enrichissement permanents via l'exploitation des ressources libres du web sémantique, entre autres, l'encyclopédie Wikipédia, DBpedia, Geonames et Yago2. Prolexbase, comporte à ce jour dix langues, trois parmi elles sont bien couvertes : le francais, l'anglais et le polonais. Il a été conçu manuellement et une première tentative semi-automatique a été réalisée par le projet ProlexFeeder (Savary et al. 2013). L'objectif de notre travail était d'élaborer un outil de mise à jour et d'extension automatiques de ce lexique, et l'ajout de la langue arabe. Un système automatique a également été mis en place pour calculer via la Wikipédia l'indice de notoriété des entrées de Prolexbase ;  cet indice dépend de la langue et participe, d'une part, à la construction d'un module de Prolexbase pour la langue arabe et, d'autre part, à la révision de la notoriété présente pour les autres langues de la base.
Un graphe est un ensemble de noeuds, ensemble de liens reliant des paires de noeuds. Avec la quantité accumulée de données collectées, il existe un intérêt croissant pour la compréhension des structures et du comportement de très grands graphes. Néanmoins, l'augmentation rapide de la taille des grands graphes rend l'étude de tous les graphes de moins en moins efficace. Ainsi, il existe une demande impérieuse pour des méthodes plus efficaces pour étudier de grands graphes sans nécessiter la connaissance de tous les graphes. Une méthode prometteuse pour comprendre le comportement de grands graphes consiste à exploiter des propriétés spécifiques de structures locales, telles que la taille des grappes ou la présence locale d'un motif spécifique, c'est-à-dire un graphe donné (généralement petit). Un exemple classique de la théorie des graphes (cas avérés de la conjecture d'Erdos-Hajnal) est que, si un graphe de grande taille ne contient pas de motif spécifique, il doit alors avoir un ensemble de noeuds liés par paires ou non liés, de taille exponentiellement plus grande que prévue. Cette thèse abordera certains aspects de deux questions fondamentales de la théorie des graphes concernant la présence, en abondance ou à peine, d'un motif donné dans un grand graphe :  - Le grand graphe peut-il être partitionné en copies du motif ? - Le grand graphe contient-il une copie du motif ? Nous discuterons de certaines des conjectures les plus connues de la théorie des graphes sur ce sujet : les conjectures de Tutte sur les flots dans les graphes et la conjecture d'Erdos-Hajnal mentionnée ci-dessus, et présenterons des preuves pour plusieurs conjectures connexes-y compris la conjecture de Barát-Thomassen, une conjecture de Haggkvist et Krissell, un cas particulier de la conjecture de Jaeger-Linial-Payan-Tarsi, une conjecture de Berger et al, et une autre d'Albouker et al.
Image sur le web : analyse de la dynamique des images sur le Web 2.0. Chaque jour, des millions d'individus publient leurs avis sur le Web 2.0 (réseaux sociaux, blogs, etc.). Ces commentaires portent sur des sujets aussi variés que l'actualité, la politique, les résultats sportifs, biens culturels, des objets de consommation, etc. Cette idée porte a priori sur un sujet particulier et n'est valable que dans un contexte, à un instant donné. Cette image perçue est par nature différente de celle que l'entité souhaitait initialement diffuser (par exemple via une campagne de communication). De plus, dans la réalité, il existe au final plusieurs images qui cohabitent en parallèle sur le réseau, chacune propre à une communauté et toutes évoluant différemment au fil du temps (imaginons comment serait perçu dans chaque camp le rapprochement de deux hommes politiques de bords opposés). Enfin, en plus des polémiques volontairement provoquées par le comportement de certaines entités en vue d'attirer l'attention sur elles (pensons aux tenues ou déclarations choquantes), il arrive également que la diffusion d'une image dépasse le cadre qui la régissait et même parfois se retourne contre l'entité (par exemple, « le mariage pour tous » devenu « la manif pour tous » ). Les opinions exprimées constituent alors autant d'indices permettant de comprendre la logique de construction et d'évolution de ces images. Ce travail d'analyse est jusqu'à présent confié à des spécialistes de l'e-communication qui monnaient leur subjectivité. Ces derniers ne peuvent considérer qu'un volume restreint d'information et ne sont que rarement d'accord entre eux. Dans cette thèse, nous proposons d'utiliser différentes méthodes automatiques, statistiques, supervisées et d'une faible complexité permettant d'analyser et représenter l'image de marque d'entité à partir de contenus textuels les mentionnant. Plus spécifiquement, nous cherchons à identifier les contenus(ainsi que leurs auteurs) qui sont les plus préjudiciables à l'image de marque d'une entité. Nous introduisons un processus d'optimisation automatique de ces méthodes automatiques permettant d'enrichir les données en utilisant un retour de pertinence simulé (sans qu'aucune action de la part de l'entité concernée ne soit nécessaire). Nous comparer également plusieurs approches de contextualisation de messages courts à partir de méthodes de recherche d'information et de résumé automatique. Nous tirons également parti d'algorithmes de modélisation(tels que la Régression des moindres carrés partiels), dans le cadre d'une modélisation conceptuelle de l'image de marque, pour améliorer nos systèmes automatiques de catégorisation de documents textuels.
Les thiopurines sont des médicaments cytotoxiques et immunosuppresseurs largement prescrits, notamment dans les maladies inflammatoires chroniques de l'intestin (MICI). La variabilité interindividuelle de la réponse à ces médicaments rend nécessaire leur optimisation thérapeutique. Ce travail de thèse a d'une part, analysé les relations entre activité TPMT et concentrations des métabolites thiopuriniques, et d'autre part, recherché des facteurs associés à la résistance aux thiopurines. De plus, une étude clinique rétrospective dans les MICI pédiatriques a permis d'identifier des facteurs associés à la lymphopénie observée sous thiopurines. Enfin, à partir d'un modèle in vitro fondé sur des lignées cellulaires lymphoblastoïdes (LCL) sélectionnées, nous avons établi une signature transcriptomique, incluant 32 gènes, prédictive de la résistance aux thiopurines. Une analyse fonctionnelle bioinformatique a abouti à l'identification de voies métaboliques liées à la protéine p53 et au cycle cellulaire, ainsi que des mécanismes moléculaires associés à la résistance aux thiopurines. En conclusion, ce travail de thèse, qui a exploré la variabilité de réponse aux thiopurines et tout particulièrement la résistance à ces médicaments, propose des hypothèses pour l'individualisation et l'optimisation thérapeutique des thiopurines.
En effet, bien qu'il existe plusieurs dizaines de modèles de la saillance visuelle, à la fois en termes de contraste et de cognition, il n'existe pas de modèle hybridant les deux mécanismes d'attention : l'aspect visuel et l'aspect cognitif. Pour créer un tel modèle, nous avons exploré les approches existantes dans le domaine de l'attention visuelle, ainsi que plusieurs approches et paradigmes relevant des domaines connexes (tels que la reconnaissance d'objets, apprentissage artificiel, classification, etc.).Une architecture fonctionnelle d'un système d'attention visuelle hybride, combinant des principes et des mécanismes issus de l'attention visuelle humaine avec des méthodes calculatoires et algorithmiques, a été mise en œuvre, expliquée et détaillée. Les études menées ont conclu à la validation expérimentale des modèles proposés, confirmant la pertinence de l'approche proposée dans l'accroissement de l'autonomie des systèmes robotisés – et ceci dans un environnement réel
Les réseaux de neurones profonds ont obtenu des résultats impressionnants dans de nombreux domaines liés à l'IA, allant des jeux informatiques à la vision par ordinateur et au traitement du langage naturel, pour n'en citer que quelques-uns. L'ingénierie d'architecture a permis au taux d'erreur sur ImageNet de passer de 16,4% (AlexNet) à 3,57% (ResNet-152). La conception d'architectures prend du temps et est difficile. Des méthodes automatiques ont été développées pour apprendre des architectures et, ces dernières années, elles ont utilisé une puissance de calcul massive pour atteindre des performances de pointe sur divers problèmes. L'objectif de ce projet de thèse, en collaboration entre Paris Dauphine et Facebook AI Research Paris, comprend :  Contribuer à l'état de l'art en concevant et en développant de nouvelles méthodes d'optimisation sans dérivé pour la recherche d'architecture. Trouvez de nouvelles architectures offrant une meilleure précision ou des compromis intéressants entre la taille, la vitesse de calcul et les performances. Généraliser l'architecture pour apprendre des architectures plus efficaces pour l'apprentissage par renforcement, l'adaptation de domaine ou pour gérer les problèmes survenant lorsque la distribution des données de test est différente de celle des données du train (décalage covariable).
Dans un environnement collaboratif de développement de systèmes complexes, plusieurs entreprises doivent échanger un nombre important de modèles hétérogènes et d'exigences. Durant les phases du cycle de vie du système, ces artefacts, reliés les uns aux autres et issus de différents outils de modélisations, évoluent constamment. Dans un tel environnement, il est crucial de gérer l'impact des différents changements se produisant dans les différents espaces de conception. La traçabilité répond à ce besoin. Toutefois, établir des liens entre des exigences et des modèles en ingénierie des systèmes complexes suppose de faire face à une volumétrie importante des artefacts. Par exemple, pour une spécification d'un véhicule autonome comprenant 3 000 exigences et 400 éléments de modèles, il faudrait en théorie vérifier de l'ordre d'un million de liens potentiels. Bien que plusieurs approches aient été proposées pour l'identification des liens de traçabilité, le processus de validation des liens est toujours chronophage et générateur d'erreurs. Cette approche fournit ainsi une mesure quantitative de confiance sur chaque lien candidat.
Même si l'information est abondante dans le monde, l'information structurée, prête à être utilisée est rare. Ce travail propose l'Extraction d'Information (EI) comme une approche efficace pour la production de l'information structurée, utilisable sur la biologie, en présentant une tâche complète d'EI sur un organisme modèle, Arabidopsis thaliana. Un système d'EI se charge d'extraire les parties de texte les plus significatives et d'identifier leurs relations sémantiques. En collaboration avec des experts biologistes sur la plante A. Thaliana un modèle de connaissance a été conçu. Son objectif est de formaliser la connaissance nécessaire pour bien décrire le domaine du développement de la graine. Ce modèle contient toutes les entités et relations les connectant qui sont essentielles et peut être directement utilisé par des algorithmes. En parallèle ce modèle a été testé et appliqué sur un ensemble d'articles scientifiques du domaine, le corpus nécessaire pour l'entraînement de l'apprentissage automatique. Les experts ont annoté le texte en utilisant les entités et relations du modèle. Le modèle et le corpus annoté sont les premiers proposés pour le développement de la graine, et parmi les rares pour A. Thaliana, malgré son importance biologique. Ce modèle réconcilie les besoins d'avoir un modèle assez complexe pour bien décrirele domaine, et d'avoir assez de généralité pour pouvoir utiliser des méthodes d'apprentissage automatique. Une approche d'extraction de relations (AlvisRE) a également été élaborée et développée. Une fois les entités reconnues, l'extracteur de relations cherche à détecter les cas où le texte mentionne une relation entre elles, et identifier précisément de quel type de relation du modèle il s'agit. L'approche AlvisRE est basée sur la similarité textuelle et utilise à la fois des informations lexiques,syntactiques et sémantiques. Dans les expériences réalisées, AlvisRE donne des résultats qui sont équivalents et parfois supérieurs à l'état de l'art. En plus, AlvisRE a l'avantage de la modularité et adaptabilité en utilisant des informations sémantiques produites automatiquement. Ce dernier caractéristique permet d'attendre des performances équivalentes dans d'autres domaines.
Cependant, cette classe de mots reste à l'heure actuelle la moins exploitée : les règles de la flexion ne décrivent pas tous les cas de figures de la conjugaison macédonienne et leur approche s'effectue de manière trop synthétique pour être opérationnelle dans une optique didactique. Pour toutes ces raisons, le but de cette thèse est d'explorer un grand nombre de verbes fléchis afin de déceler des modèles stables de conjugaison ouvrant de nouvelles pistes pour l'apprentissage du système verbal du macédonien.
La Théorie du Contrôle par Supervision (TCS) est l'un des paradigmes formels les plus importants pour le développement de contrôleurs dans le cadre de l'étude des systèmes à événements discrets (SED). Le grand nombre de contributions scientifiques montre que la TCS suscite un intérêt académique considérable et il a été prouvé que cette théorie était applicable dans divers domaines industriels tels que les systèmes de fabrication, les systèmes embarqués ou les systèmes de transport. La conséquence immédiate est que l'étape de vérification n'a plus lieu d'être, ce qui élimine aussi tous les cycles de reconception et de vérification nécessaires à la mise au point du système. Cependant, cette théorie souffre d'un manque d'intégration dans un processus global de conception dans lequel les exigences initiales sur le système à concevoir sont utilisées pour définir d'une part les exigences sur ses sous-systèmes et d'autre part les contrôleurs locaux associés. L'ingénierie dirigée par les modèles (IDM) fournit les solutions permettant de gérer les limitations de SCT. L'objectif de cette étude est de proposer un nouveau cadre pour la conception des contrôleurs, qui intègre à la fois TCS et IDM afin de combler les lacunes du paradigme formel et du processus d'ingénierie. Dans le cadre proposé, différents diagrammes SysML sont utilisés en tant que modèles complémentaires présentant les vues indispensables du système à étudier dans le processus de modélisation globale. De plus, afin de maintenir la cohérence entre les modèles SysML et les modèles formels, des méthodes de modélisation et de vérification formelles sont proposées. Une étude de cas présentée à la fin de la thèse montre que le cadre proposé, qui fournit un processus de développement global allant de l'analyse des besoins à la mise en œuvre du contrôleur, peut parfaitement répondre aux besoins de la pratique de l'ingénierie.
Le texte est l'une des sources d'informations les plus répandues et les plus persistantes. L'analyse de contenu du texte se réfère à des méthodes d'étude et de récupération d'informations à partir de documents. Aujourd'hui, avec une quantité de texte disponible en ligne toujours croissante l'analyse de contenu du texte revêt une grande importance parce qu' elle permet une variété d'applications. À cette fin, les méthodes d'apprentissage de la représentation sans supervision telles que les modèles thématiques et les word embeddings constituent des outils importants. L'objectif de cette dissertation est d'étudier et de relever des défis dans ce domaine. Dans la première partie de la thèse, nous nous concentrons sur les modèles thématiques et plus précisément sur la manière d'incorporer des informations antérieures sur la structure du texte à ces modèles. Les modèles de sujets sont basés sur le principe du sac-de-mots et, par conséquent, les mots sont échangeables. Bien que cette hypothèse profite les calculs des probabilités conditionnelles, cela entraîne une perte d'information. Pour éviter cette limitation, nous proposons deux mécanismes qui étendent les modèles de sujets en intégrant leur connaissance de la structure du texte. Nous supposons que les documents sont répartis dans des segments de texte cohérents. Le premier mécanisme attribue le même sujet aux mots d'un segment. La seconde, capitalise sur les propriétés de copulas, un outil principalement utilisé dans les domaines de l'économie et de la gestion des risques, qui sert à modéliser les distributions communes de densité de probabilité des variables aléatoires tout en n'accédant qu'à leurs marginaux. En règle générale, une collection de documents pour ces modèles se présente sous la forme de paires de documents comparables. Les documents d'une paire sont écrits dans différentes langues et sont thématiquement similaires. À moins de traductions, les documents d'une paire sont semblables dans une certaine mesure seulement. Pendant ce temps, les modèles de sujets représentatifs supposent que les documents ont des distributions thématiques identiques, ce qui constitue une hypothèse forte et limitante. Pour le surmonter, nous proposons de nouveaux modèles thématiques bilingues qui intègrent la notion de similitude interlingue des documents qui constituent les paires dans leurs processus générateurs et d'inférence. La dernière partie de la thèse porte sur l'utilisation d'embeddings de mots et de réseaux de neurones pour trois applications d'exploration de texte. Tout d'abord, nous abordons la classification du document polylinguistique où nous soutenons que les traductions d'un document peuvent être utilisées pour enrichir sa représentation. À l'aide d'un codeur automatique pour obtenir ces représentations de documents robustes, nous démontrons des améliorations dans la tâche de classification de documents multi-classes. Deuxièmement, nous explorons la classification des tweets à plusieurs tâches en soutenant que, en formant conjointement des systèmes de classification utilisant des tâches corrélées, on peut améliorer la performance obtenue. À cette fin, nous montrons comment réaliser des performances de pointe sur une tâche de classification du sentiment en utilisant des réseaux neuronaux récurrents. La troisième application que nous explorons est la récupération d'informations entre langues. Compte tenu d'un document écrit dans une langue, la tâche consiste à récupérer les documents les plus similaires à partir d'un ensemble de documents écrits dans une autre langue. Dans cette ligne de recherche, nous montrons qu'en adaptant le problème du transport pour la tâche d'estimation des distances documentaires, on peut obtenir des améliorations importantes.
Dans cette thèse, nous considérons le problème d'évaluation de situations dans le cadre plus spécifique des situations de crise. Nous proposons un système d'évaluation de situations automatisé (le système ASAAP) cherchant à pallier ces deux problèmes. Nous y présentons une modélisation dynamique de l'environnement permettant une analyse des variables d'états utiles à l'évaluation de la situation afin de réduire le champ des possibles et maximiser le gain d'informations. Dans le cadre des situations de crise, nous proposons également une application du système à l'évaluation de menace. Cette contribution permet d'analyser et définir la menace à laquelle sont exposées différentes zones tout en définissant la stratégie de l'ennemi. Enfin, nous proposons une approche préliminaire sur l'optimisation de l'utilisation des capteurs pour couvrir les variables d'états utiles dans l'optique de maximisation du gain d'informations. Nous avons généré des scénarios inspirés de situations réelles afin d'évaluer nos approches dans les domaines maritime et militaire. Les résultats du système ASAAP montrent une amélioration de l'évaluation de situation en terme de complexité ainsi que la capacité de décrire la stratégie de l'ennemi dans un temps raisonnable.
Cette thèse se concentre sur les méthodes d'apprentissage pour la transcription automatique de la batterie. Elles sont basées sur un algorithme de transcription utilisant une méthode de décomposition non-négative, la NMD. Cette thèse soulève deux principales problématiques : l'adaptation des méthodes au signal analysé et l'utilisation de l'apprentissage profond. La prise en compte des informations du signal analysé dans le modèle peut être réalisée par leur introduction durant les étapes de décomposition. Une première approche est de reformuler l'étape de décomposition dans un contexte probabiliste pour faciliter l'introduction d'informations a posteriori avec des méthodes comme la SI-PLCA et la NMD statistique. Une deuxième approche est d'implémenter directement dans la NMD une stratégie d'adaptation : l'application de filtres modelables aux motifs pour modéliser les conditions d'enregistrement ou l'adaptation des motifs appris directement au signal en appliquant de fortes contraintes pour conserver leur signification physique. La deuxième approche porte sur la sélection des segments de signaux à analyser. Les résultats obtenus étant très intéressants, le détecteur est entraîné à ne détecter qu'un seul instrument permettant la réalisation de la transcription des trois principaux instruments de batterie avec trois CNN. Finalement, l'utilisation d'un CNN multi-sorties est étudiée pour transcrire la partie de batterie avec un seul réseau.
Depuis leur apparition, les emojis ont une popularité grandissante dans les systèmes de communication. Ces petites images pouvant représenter une idée, un concept ou une émotion, se retrouvent disponibles aux utilisateurs dans de nombreux contextes logiciels : messagerie, courriel, forums et autres réseaux sociaux. Leur usage, en hausse constante, a entraîné l'apparition récurrente de nouveaux emojis. Le parcours de bibliothèques d'emojis ou l'utilisation de moteur de recherche intégré n'est plus suffisant pour permettre à l'utilisateur de maximiser leur utilisation ;  une recommandation d'emojis adaptée est nécessaire. Pour cela nous présentons nous travaux de recherche axés sur le thème de la recommandation d'emojis. Ces travaux ont pour objectifs de créer un système de recommandation automatique d'emojis adapté à un contexte conversationnel informel et privé. Ce système doit améliorer l'expérience utilisateur et la qualité de la communication, et prendre en compte d'éventuels nouveaux emojis créés. Dans le cadre de cette thèse, nous contribuons tout d'abord en montrant les limites d'usage réel d'une prédiction d'emojis ainsi que la nécessité de prédire des notions plus générales. Nous vérifions également si l'usage réel des emojis représentant une expression faciale d'émotion correspond à l'existant théorique. Enfin, nous abordons les pistes d'évaluation d'un tel système par l'insuffisance des métriques, et l'importance d'une interface utilisateur dédiée.
La maladie d'Alzheimer (MA) est la première cause de démence dans le monde, touchant plus de 20 millions de personnes. Son diagnostic précoce est essentiel pour assurer une prise en charge adéquate des patients ainsi que pour développer et tester de nouveaux traitements. La MA est une maladie complexe qui nécessite différentes mesures pour être caractérisée : tests cognitifs et cliniques, neuroimagerie, notamment l'imagerie par résonance magnétique (IRM) et la tomographie par émission de positons (TEP), génotypage, etc. Il y a un intérêt à explorer les capacités discriminatoires et prédictives à un stade précoce de ces différents marqueurs, qui reflètent différents aspects de la maladie et peuvent apporter des informations complémentaires. L'objectif de cette thèse de doctorat était d'évaluer le potentiel et d'intégrer différentes modalités à l'aide de méthodes d'apprentissage statistique, afin de classifier automatiquement les patients atteints de la MA et de prédire l'évolution de la maladie dès ses premiers stades. Plus précisément, nous visions à progresser vers une future application de ces approches à la pratique clinique. La thèse comprend trois études principales. La première porte sur le diagnostic différentiel entre différentes formes de démence à partir des données IRM. Cette étude a été réalisée à l'aide de données de routine clinique, ce qui a permis d'obtenir un scénario d'évaluation plus réaliste. La seconde propose un nouveau cadre pour l'évaluation reproductible des algorithmes de classification de la MA à partir des données IRM et TEP. En effet, bien que de nombreuses approches aient été proposées dans la littérature pour la classification de la MA, elles sont difficiles à comparer et à reproduire. Alzheimer chez les patients atteints de troubles cognitifs légers par l'intégration de données multimodales, notamment l'IRM, la TEP, des évaluations cliniques et cognitives, et le génotypage. En particulier, nous avons systématiquement évalué la valeur ajoutée de la neuroimagerie par rapport aux seules données cliniques/cognitives. Comme la neuroimagerie est plus coûteuse et moins répandue, il est important de justifier son utilisation dans les algorithmes de classification.
La présente thèse recherche une approche sémantique pour l'extraction et l'analyse du discours idéologique au sein de documents textuels sous forme électronique. Cette approche intègre une méthode qualitative d'analyse de textes issue des sciences sociales (analyse critique du discours) avec une méthode quantitative de raisonnement et extraction d'information à base d'ontologies de domaine et de traitement semi-automatique du langage naturel. L'application centrale du projet de thèse vise à étudier le discours marxiste comme représenté par la collection thématique Archive Internet des Marxistes (http : //www.marxists.org) contenant près de 15000 textes. L'objectif de l'application est l'acquisition de schémas émergents, qui pourront contribuer à la classification par point de vue idéologique de textes inconnus.
Toutefois, leur mise en œuvre au sein de laboratoires physiques requiert des infrastructures souvent coûteuses pour les institutions de formation qui peuvent ainsi difficilement faire face à la forte augmentation du nombre d'étudiants. Dans ce contexte, les laboratoires virtuels et distants (VRL) représentent une alternative pour assurer le passage à l'échelle des activités pratiques à moindre coût. De nombreux travaux de recherche ont émergé au cours de la dernière décennie en se focalisant sur les problématiques techniques et technologiques induites par ces nouveaux usages, telles que la fédération, la standardisation, ou la mutualisation des ressources de laboratoires. Cependant, les récentes revues de littérature du domaine mettent en exergue la nécessité de se préoccuper davantage des facettes pédagogiques liées à ces environnements informatiques innovants dédiés à l'apprentissage pratique. Dans cet objectif, nos travaux exploitent les traces issues des activités réalisées par les apprenants lors de sessions d'apprentissage pratique pour mettre en œuvre les théories socio-constructivistes qui sont au cœur de l'apprentissage exploratoire, et ainsi favoriser l'engagement et le processus de réflexion des étudiants. À partir de la littérature traitant des relations sociales entre apprenants, nous identifions dans un premier temps un ensemble de critères pour la conception de systèmes d'apprentissage pratique engageants. En effet, Lab4CE repose sur les Learning Analytics pour supporter différentes formes d'apprentissage telles que la collaboration, la coopération ou l'entraide entre pairs, mais également pour fournir des outils d'awareness et de réflexion visant à promouvoir l'apprentissage en profondeur pendant et après les activités pratiques. De plus, ces expérimentations soulignent l'existence d'une corrélation significative entre l'engagement des étudiants dans la plateforme et les stratégies d'apprentissage qu'ils mettent en œuvre d'une part, et leur performance académique d'autre part. Ces premiers résultats nous permettent d'affirmer que les théories socio-constructivistes sont un levier à l'engagement et à la réflexion dans les VRL. Ils nous invitent à confronter notre approche à d'autres modalités d'apprentissage, mais aussi à intégrer de nouvelles sources d'informations pour approfondir nos analyses du comportement et ainsi renforcer nos contributions à une meilleure prise en compte de l'apprentissage pratique dans les EIAH.
Les études linguistiques les plus récentes admettent que les noms propres sont souvent traduits, ou adaptés, par opposition aux anciennes théories sur la non-traduisibilité des noms propres. Ce qui distingue les toponymes du reste de l'onomastique, ce sont les implications politiques, sociologiques et historiques qui concernent une bonne partie des noms géographiques. Dire « Breslau » pour la ville polonaise « Wrocław » peut avoir une connotation négative selon le contexte. Et pourtant, nous rencontrons très fréquemment cette forme sur Internet. De plus, on observe avec la mondialisation la multiplication de nombreuses versions du même toponyme. Le travail présenté ici est composé de trois parties. La première partie présente de manière brève les théories concernant les noms propres, mais adaptées aux toponymes, et la deuxième décrit leurs fonctions, ainsi que leur statut linguistique par rapport à la normalisation internationale. Le concept de la toponymie synchronique-contrastive et les méthodes d'analyse des toponymes selon cette approche sont introduits dans la partie 2. La partie trois est une analyse du corpus qui a pour le but d'observer les structures et l'intégration des toponymes polonais dans la langue française ainsi que leur usage populaire dans les publications courantes (sur Internet, dans les brochures touristiques, dans les applications et réseaux sociaux, etc.), qui diffère de l'usage officiel, censé être politiquement correct.
Avec l'avènement du « big data » , de nombreuses répercussions ont eu lieu dans tous les domaines de la technologie de l'information, préconisant des solutions innovantes remportant le meilleur compromis entre coûts et précision. Nous abordons dans cette thèse deux problématiques principales : dans un premier temps, le problème du partitionnement des graphes est abordé d'une perspective « big data » , où les graphes massifs sont partitionnés en streaming. Nous étudions et proposons plusieurs modèles de partitionnement en streaming et nous évaluons leurs performances autant sur le plan théorique qu'empirique. Dans un second temps, nous nous intéressons au requêtage des graphes distribués/partitionnés.
Cette thèse étudie la production des nabol, des pratiques narratives de la communauté linguistique nisvaie, située dans le sud-est de Malekula, au Vanuatu. S'appuyant sur la demande de locuteurs nisvais afin que des ressources langagières soient produites pour l'école locale, un corpus de textes oraux a été constitué pour montrer que les nabol sont produites en fonction de la situation d'énonciation et d'enjeux sociaux locaux liés à la classe d'âge de l'orateur. Le corpus de textes oraux annotés résulte de séjours de recherche réalisés entre 2011 et 2015,totalisant 14 mois de terrain au sein de la communauté nisvaie. Les nabol sont étudiées d'une part, à l'aide des concepts issus de la linguistique textuelle afin de décrire les procédés discursifs employés par les locuteurs nisvais. À partir de ces procédés, il a été possible de comparer l'organisation des nabol et de mettre en évidence des variations significatives en fonction de la situation d'énonciation. D'autre part, l'observation participante et des entretiens dirigés ont permis d'identifier des enjeux sociaux que les locuteurs nisvais associent à leurs pratiques narratives. L'emploi des noms propres de personnages ou de lieux lors de la narration répond à un régime de vérité. En fonction de sa classe d'âge, l'orateur doit nommer ou non les personnages qui prennent part à l'intrigue au risque de se faire critiquer par ses pairs. Les pratiques et normes issues du programme de documentation des langues à travers le monde et du traitement automatique des langues ont fourni des outils pour élaborer des ressources langagières pertinentes à l'étude des narrations et à son utilisation par la communauté nisvaie. Deux ressources papier sont jointes en annexe, un lexique bilingue nisvai-français et un recueil bilingue des textes du corpus, conçues pour les locuteurs nisvais et leur école francophone. De plus, deux ressources accessibles en ligne, une interface de lecture-écoute des textes et une interface de consultation des annotations ont été développées pour communiquer les travaux aux chercheurs travaillant sur des pratiques narratives orales ou les langues du Vanuatu.
D'autre part, nous avons construit notre propre gant de données et nous l'utilisons pour confirmer expérimentalement que la solution de reconnaissance gestuelle permet le contrôle temps réel d'un robot en mobilité. Enfin, nous montrons la flexibilité de notre technique en ce sens qu'elle permet de contrôler non seulement des robots, mais aussi des systèmes de natures différentes.
L'Internet des objets, ou IdO (en anglais Internet of Things, ou IoT) conduit à un changement de paradigme du secteur de la logistique. L'avènement de l'IoT a modifié l'écosystème de la gestion des services logistiques. Les fournisseurs de services logistiques utilisent aujourd'hui des technologies de capteurs telles que le GPS ou la télémétrie pour collecter des données en temps réel pendant la livraison. La collecte en temps réel des données permet aux fournisseurs de services de suivre et de gérer efficacement leur processus d'expédition. Le principal avantage de la collecte de données en temps réel est qu'il permet aux fournisseurs de services logistiques d'agir de manière proactive pour éviter des conséquences telles que des retards de livraison dus à des événements imprévus ou inconnus. Les données provenant de ces sources externes enrichissent l'ensemble de données et apportent une valeur ajoutée à l'analyse. De plus, leur collecte en temps réel permet d'utiliser les données pour une analyse en temps réel et de prévenir des résultats inattendus (tels que le délai de livraison, par exemple) au moment de l'exécution. Cependant, les données collectées sont brutes et doivent être traitées pour une analyse efficace. La collecte et le traitement des données en temps réel constituent un énorme défi. La raison principale est que les données proviennent de sources hétérogènes avec une vitesse énorme. La grande vitesse et la variété des données entraînent des défis pour effectuer des opérations de traitement complexes telles que le nettoyage, le filtrage, le traitement de données incorrectes, etc. La diversité des données - structurées, semi-structurées et non structurées - favorise les défis dans le traitement des données à la fois en mode batch et en temps réel. Parce que, différentes techniques peuvent nécessiter des opérations sur différents types de données. Une structure technique permettant de traiter des données hétérogènes est très difficile et n'est pas disponible actuellement. Par conséquent, pour exploiter le Big Data dans les processus de services logistiques, une solution efficace pour la collecte et le traitement des données en temps réel et en mode batch est essentielle. Dans cette thèse, nous avons développé et expérimenté deux méthodes pour le traitement des données : SANA et IBRIDIA. SANA est basée sur un classificateur multinomial Naïve Bayes, tandis qu'IBRIDIA s'appuie sur l'algorithme de classification hiérarchique (CLH) de Johnson, qui est une technologie hybride permettant la collecte et le traitement de données par lots et en temps réel. SANA est une solution de service qui traite les données non structurées. Cette méthode sert de système polyvalent pour extraire les événements pertinents, y compris le contexte (tel que le lieu, l'emplacement, l'heure, etc.). En outre, il peut être utilisé pour effectuer une analyse de texte sur les événements ciblés. IBRIDIA a été conçu pour traiter des données inconnues provenant de sources externes et les regrouper en temps réel afin d'acquérir une connaissance / compréhension des données permettant d'extraire des événements pouvant entraîner un retard de livraison. Selon nos expériences, ces deux approches montrent une capacité unique à traiter des données logistiques
Dans le but d'exploiter au mieux les grandes masses de données hétérogènes produites par les instruments scientifiques modernes de l'astrophysique, les scientifiques ont développé le concept d'Observatoire Virtuel (OV). Il s'agit d'une architecture orientée services, qui a pour objectif de faciliter l'identification et l'interopérabilité des données astrophysiques. Malgré le développement et les avancées permises par l'OV dans l'exploitation de ces données, certains objectifs sont partiellement atteints notamment l'interopérabilité, la sélection de services et l'identification de services connexes, etc. Par ailleurs, l'ergonomie des outils à la disposition de l'utilisateur final reste perfectible. De même l'utilisation actuelle des ressources de l'OV, s'appuyant sur des compétences humaines, gagnerait à être automatisée. Les services de données astrophysiques n'étant pas tous inscrits dans l'OV, il serait aussi souhaitable pour permettre une utilisation plus large de ces outils, qu'ils s'appuient également sur des services disponibles en-dehors de l'OV. En vue d'automatiser l'utilisation des ressources en ligne, les sciences de l'information travaillent depuis 2001 à l'élaboration du Web sémantique. Cette évolution apporte au Web des capacités de raisonnement automatiques, basées sur des algorithmes utilisant une nouvelle forme de description des contenus. Cette nouvelle forme de description sémantique se trouve exprimée dans des représentations informatiques appelées ontologies. Malheureusement, les méthodes actuelles d'élaboration du Web sémantique ne sont pas complètement compatibles avec les services OV qui utilisent des modèles de données, des formats et des protocoles d'accès aux services qui s'éloignent de ceux rencontrés habituellement dans les sciences de l'information. Dans ce contexte, cette thèse décrit une méthodologie générique de composition de services sans état, basée sur la description des services par une ontologie dont la définition est proposée dans ce document. Cette ontologie représente aussi bien des services Web que des services non accessibles par le Web. Elle prend en compte certaines spécificités qui peuvent être rencontrées dans les infrastructures de services préexistantes. La population de cette ontologie, par des services éventuellement éloignés des standards utilisés habituellement dans les sciences de l'information, est aussi traitée. La méthodologie a été appliquée avec succès dans le cadre de l'astrophysique, et a permis de développer une application Web permettant la composition automatique de services utilisable par un public non averti.
Les outils de traduction assistée par ordinateur et de gestion terminologique sont le plus souvent utilisés pour répondre au besoin de la gestion de l'écrit multilingue et monolingue. En effet, ils facilitent l'accès aux termes techniques et aux expressions liés à des domaines de spécialité, et indispensables à tout processus de communication. La compréhension de ces expressions techniques peut être potentialisée au moyen de leur « contextualisation » . Néanmoins, avoir accès à un terme ou à sa traduction ne suffit pas, encore faut-il être capable de l'employer correctement et d'en appréhender le sens exact. Cette contextualisation a donc lieu à deux niveaux : dans les textes et dans la terminologie. Au niveau textuel, l'utilisateur doit avoir accès à des informations concernant l'usage des termes, à savoir des contextes riches en connaissances linguistiques. Au niveau terminologique, il doit avoir accès aux relations sémantiques ou conceptuelles entre termes afin de mieux en saisir le sens, à savoir des contextes riches en connaissances conceptuelles. Nous avons poursuivi nos travaux dans un cadre bilingue et plus particulièrement en phase de révision du processus de traduction spécialisée. Nous avons proposé une méthodologie d'élaboration d'un concordancier bilingue fournissant des CRC alignés à partir de corpus comparables spécialisés. Les évaluations menées montrent que les CRC proposés sont utiles malgré la difficulté de l'exercice étudié.
Les réseaux sociaux numériques ont pris une place prépondérante dans l'espace informationnel, et sont souvent utilisés pour la publicité, le suivi de réputation, la propagande et même la manipulation, que ce soit par des individus, des entreprises ou des états. Alors que la quantité d'information rend difficile son exploitation par des humains, le besoin reste entier d'analyser un réseau social numérique : il faut dégager des tendances à partir des messages postés dont notamment les opinions échangées, qualifier les comportements des utilisateurs, et identifier les structures sociales émergentes. Pour résoudre ce problème, nous proposons un système d'analyse en trois niveaux. Tout d'abord, l'analyse du message vise à en déterminer l'opinion. Nous appliquons ce système d'analyse sur deux corpus provenant de deux médias sociaux différents : le premier est constitué de messages publiés sur Twitter, représentant toutes les activités réalisées par 5 000 comptes liés entre eux sur une longue période. Le second provient d'un réseau social basé sur TOR, nommé Galaxy2. Nous évaluons la pertinence de notre système sur ces deux jeux de données, montrant la complémentarité des outils de caractérisation des comptes utilisateurs (influence, comportement, rôle) et des communautés de comptes (force d'interaction, cohésion thématique), qui enrichissent l'exploitation du graphe social par les éléments issus des contenus textuels échangés.
La méthodologie est fondée sur trois piliers : la définition, la recherche / analyse et l'innovation. La définition exhaustive de la fonction principale du système industriel cible le champ de recherche et permet la récupération de mots clés initiaux grâce à une analyse approfondie de l'existant. La recherche itérative des brevets se base sur la décomposition fonctionnelle et sur l'analyse physique. L'analyse intègre la décomposition fonctionnelle énergétique pour déceler les énergies, les flux fonctionnels transmis et les phénomènes physiques impliqués dans le processus de conversion énergétique afin de sélectionner des effets physiques potentiellement pertinents. Pour délimiter le champ d'exploration nous formulons des requêtes de recherche à partir d'une base de données de mots clés constituée par des mots clés initiaux, des mots clés physiques et des mots clés technologiques. Une matrice des découvertes basée sur les croisements entre ces mots clés permet le classement des brevets pertinents. La recherche des opportunités d'innovation exploite la matrice des découvertes pour déceler les tendances évolutives suivies par les inventions. Les opportunités sont déduites à partir de l'analyse des cellules non pourvues de la matrice des découvertes, de l'analyse par tendances d'évolution et du changement de concept par la substitution du convertisseur énergétique. Nous proposons des tendances d'évolution construites à partir de lois d'évolution de la théorie TRIZ, d'heuristiques de conception et de règles de l'art de l'ingénieur. Un cas d'application concernant l'étude d'évolution et la proposition de nouveaux systèmes de séparation de mélanges bi-phasiques en offshore profond met en valeur la méthode.
L'ouvrage unifie les paradigmes du darwinisme universel, de la psycholinguistique développementale et de la linguistique computationnelle afin de fournir un récit nouveau d'ontogénie de structures linguistiques chez les agents humains ou bien artificiels. La thèse est précédée d'un volume supplémentaire intitulé « Fondements conceptuels » qui présente une théorie de l'évolution intra-mentale qui pose l'hypothèse que l'ontogénie d'un esprit individuel peut être interprétée et même simulée comme un processus impliquant la réplication, la variation et la sélection des structures qui encodent de l'information. La dissertation elle-même présente quatre simulations distinctes abordant quatre problèmes distincts. La 0ème simulation illustre comment l'optimisation évolutionnist pourrait conduire à la découverte d'idées utiles concernant l'énigme cryptologique connue sous le nom de Voynich Manuscript. La première simulation montre comment la théorie des prototypes, les architectures symboliques vectorielles et l'optimisation évolutionniste peuvent être mutuellement combinées afin de produire une nouvelle méthode d'apprentissage automatique supervisée. La deuxième simulation utilise une approche similaire pour démontrer que l'optimisation évolutive peut découvrir des constellations minimalistes et légères de marqueurs de partie de discours. La dernière simulation vise le « saint graal » de la linguistique computationnelle, c'est-à-dire le problème de « l'induction grammaticale » et montre que le problème peut être potentiellement résolu en utilisant une stratégie évolutive capable de combler l'écart entre le domaine subsymbolique et le domaine symbolique.
L'internet et les nouvelles formes de média de communication, d'information, et de divertissement ont entraîné une croissance massive de la quantité des données numériques. Le traitement et l'interprétation automatique de ces données permettent de créer des bases de connaissances, de rendre les recherches plus efficaces et d'effectuer des recherches sur les médias sociaux. Les travaux de recherche sur le traitement automatique du langage naturel concernent la conception et le développement d'algorithmes, qui permettent aux ordinateurs de traiter automatiquement le langage naturel dans les textes, les contenus audio, les images ou les vidéos, pour des tâches spécifiques. De par la complexité du langage humain, le traitement du langage naturel sous forme textuelle peut être divisé en 4 niveaux : la morphologie, la syntaxe, la sémantique et la pragmatique. Les technologies actuelles du traitement du langage naturel ont eu de grands succès sur les tâches liées auxdeux premiers niveaux, ce qui a permis la commercialisation de beaucoup d'applications comme les moteurs de recherche. Cependant, les moteurs de recherches avancés (structurels) nécessitent une interprétation du langage plus avancée. L'extraction d'information consiste à extraire des informations structurelles à partir des ressources non annotées ou semi-annotées, afin de permettre des recherches avancées et la création automatique des bases de connaissances. Cette thèse étudie le problème d'extraction d'information dans le domaine spécifique de l'extraction des événements biomédicaux. Nous proposons une solution efficace, qui fait un compromis entre deux types principaux de méthodes proposées dans la littérature. Cette solution arrive à un bon équilibre entre la performance et la rapidité, ce qui la rend utilisable pour traiter des données à grande échelle. Elle a des performances compétitives face aux meilleurs modèles existant avec une complexité en temps de calcul beaucoup plus faible. Lors la conception de ce modèle, nous étudions également les effets des différents classifieurs qui sont souvent proposés pour la résolution des problèmes de classification multi-classe. Nous testons également deux méthodes permettant d'intégrer des représentations vectorielles des mots appris par apprentissage profond (deep learning). Même si les classifieurs différents et l'intégration des vecteurs de mots n'améliorent pas grandement la performance, nous pensons que ces directions de recherche ont du potentiel et sont prometteuses pour améliorer l'extraction d'information.
Elles servent de cadre au développement de deux outils qui illustrent la mise en œuvre de ces contraintes, associées à l'adaptation de systèmes de production artistique automatique, notamment basés sur des algorithmes de colonie de fourmis artificielles. Le premier programme est un instrument de musique virtuel permettant au plus grand nombre de jouer de la musique, et fournissant un accompagnement automatique. Le second est un atelier de dessin où des outils basés sur des méthodes génératives offrent un résultat complexe à partir d'actions très simples. Cette thèse détaille le développement de ces deux programmes ainsi que leur évaluation, sur le terrain, à la rencontre d'utilisateurs réels.
Cette thèse vise à étudier le lien entre institutions, genre et politique. Elle cherche à répondre à trois questions : les institutions peuvent-elles défaire les normes de genre ? Le premier chapitre de cette thèse vise à étudier le rôle des institutions dans la création des normes de genre. La norme étudiée est celle selon laquelle une femme doit gagner moins que son mari. En utilisant, la division de l'Allemagne comme une expérience naturelle, nous montrons que les institutions égalitaires est-allemandes ont défait le genre. Après la réunification, une femme est-allemande peut gagner plus que son mari sans augmenter son nombre d'heures de travail domestique, risquer de divorcer ou de se retirer du marché du travail. A l'opposé, en Allemagne de l'Ouest, ces comportements sont toujours observables. Le deuxième chapitre étudie si les institutions seraient plus égalitaires avec des femmes à leur tête. En particulier, nous cherchons à déterminer si les femmes politiciennes ont les mêmes priorités que leurs collègues masculins. Le contexte étudié est celui du Parlement Français durant la période 2001-2017. En combinant des méthodes d'analyse de texte avec des variations exogènes dans le sexe des politiciens, ce chapitre montre que, relativement à leurs collègues masculins, les femmes politiciennes à l'Assemblée Nationale défendent plus les intérêts des femmes dans la population. Le thème où les différences sexuées d'activité parlementaire sont les plus marquées est précisément celui de l'égalité femmes-hommes, suivi des thématiques liées à l'enfance et à la santé. Nous montrons que ces différences proviennent de l'intérêt individuel des législateurs. Enfin, nous répliquons ces résultats au Sénat en exploitant l'introduction d'une réforme qui a imposé la parité. Le troisième chapitre s'intéresse aux raisons derrière la sous-représentation des femmes dans les positions de pouvoir. Il cherche à déterminer si dans un contexte où les politiciens sont majoritairement des hommes, la "prime aux sortants" lors d'élections réduit le nombre de femmes élues. Nous montrons que contrairement à ce qu'on peut s'attendre, lorsque les politiciens ne sont pas éligibles à leur réélection, la part de femmes élus n'augmente pas. C'est parce qu'il est plus difficile pour une femme de remplacer une femme que de remplacer un homme.
Les systèmes digitaux jouent un rôle croissant dans le bon fonctionnement de notre société. Au delà de la grande diversité de leur domaines d'utilisations, on confie aujourd'hui des tâches importantes à des algorithmes. Déjà largement utilisés dans des domaines aussi délicat que le transport, la chirurgie ou l'économie, il est aujourd'hui de plus en plus question de faire de la place aux systèmes digitaux dans les domaines sociaux et politiques : vote électronique, algorithmes de sélection, profilage électoral dots. Pour les tâches confiées à des algorithmes, la responsabilité est déplacées de l'exécutant vers les concepteurs, développeurs et testeurs de ces algorithmes. Il incombe aussi aux chercheurs qui étudient ces algorithmes de proposer des techniques de vérifications fiable qui pourront être utilisées à tous les niveaux : conception, développement et test. Les méthodes de vérifications formelles donnent des outils mathématiques pour prévenir des erreurs à chaque niveaux. Parmi elle, le diagnostic d'erreur consiste en la création d'un diagnostiqueur basé sur un modèle formel du système à vérifier. Le diagnostiqueur est exécuté en parallèle du système qu'il doit surveiller et prévient un contrôleur si il détecte un comportement dangereux du système. Pour les systèmes modélisés par des automates temporisés, il n'est pas toujours possible de construire un diagnostiqueur sous la forme d'un autre automate temporisé. En effet les automates temporisés, introduits par cite{AD94} dans les années 90 et largement étudiés et utilisés depuis pour modéliser des systèmes avec contraintes temporelles,ne sont pas déterminisable. Une machine plus puissante qu'un automate temporisé peut cependant être utilisée pour construire le diagnostiqueur d'un automate temporisé comme le montre cite{Tripakis02}. L'aboutissement de ce travail de thèse est la construction automatique d'un diagnostiqueur pour les automates temporisés à une horloge. Ce diagnostiqueur, dans le même esprit que celui de cite{Tripakis02}, est une machine plus puissante qu'un automate temporisé. La partie~I du manuscrit introduit un cadre formel pour ce type de machine et plus généralement pour la modélisation et la déterminisation de systèmes quantitatifs. Y est introduit le modèle des automates sur structures temporisés, qui apporte un nouveau point de vue sur la manière de modéliser les systèmes avec variables quantitatives. La partie~II étudie le problème de la déterminisation des automates sur structures temporises, et plus spécifiquement celui des automates temporisés qui peuvent se traduire dans ce cadre nouveau cadre formel. La partie~III montre comment utiliser les automates sur structure temporisés pour construire de manière générique un diagnostiqueur pour les automate temporisés à une horloge. Cette technique est implémentée dans un outils, DOTA, et comparée à la machine construite par cite{Tripakis02}.
Le "Big Data''et les grands systèmes d'apprentissage sont omniprésents dans les problèmes d'apprentissage automatique aujourd'hui. Contrairement à l'apprentissage de petite dimension, les algorithmes d'apprentissage en grande dimension sont sujets à divers phénomènes contre-intuitifs et se comportent de manière très différente des intuitions de petite dimension sur lesquelles ils sont construits. Cependant, en supposant que la dimension et le nombre des données sont à la fois grands et comparables, la théorie des matrices aléatoires (RMT) fournit une approche systématique pour évaluer le comportement statistique de ces grands systèmes d'apprentissage, lorsqu'ils sont appliqués à des données de grande dimension. L'objectif principal de cette thèse est de proposer un schéma d'analyse basé sur la RMT, pour une grande famille de systèmes d'apprentissage automatique : d'évaluer leurs performances, de mieux les comprendre et finalement les améliorer, afin de mieux gérer les problèmes de grandes dimensions aujourd'hui. Précisément, nous commençons par exploiter la connexion entre les grandes matrices à noyau, les projection aléatoires non-linéaires et les réseaux de neurones aléatoires simples. En considérant que les données sont tirées indépendamment d'un modèle de mélange gaussien, nous fournissons une caractérisation précise des performances de ces systèmes d'apprentissage en grande dimension, exprimée en fonction des statistiques de données, de la dimensionnalité et, surtout, des hyper-paramètres du problème. Lorsque des algorithmes d'apprentissage plus complexes sont considérés, ce schéma d'analyse peut être étendu pour accéder à de systèmes d'apprentissage qui sont définis (implicitement) par des problèmes d'optimisation convexes, lorsque des points optimaux sont atteints. Pour trouver ces points, des méthodes d'optimisation telles que la descente de gradient sont régulièrement utilisées. À cet égard, dans le but d'avoir une meilleur compréhension théorique des mécanismes internes de ces méthodes d'optimisation et, en particulier, leur impact sur le modèle d'apprentissage, nous évaluons aussi la dynamique de descente de gradient dans les problèmes d'optimisation convexes et non convexes. Ces études préliminaires fournissent une première compréhension quantitative des algorithmes d'apprentissage pour le traitement de données en grandes dimensions, ce qui permet de proposer de meilleurs critères de conception pour les grands systèmes d'apprentissage et, par conséquent, d'avoir un gain de performance remarquable lorsqu'il est appliqué à des jeux de données réels.
Les émotions et leurs expressions par des agents virtuels sont deux enjeux importants pour les interfaces homme-machine affectives à venir. En effet, les évolutions récentes des travaux en psychologie des émotions, ainsi que la progression des techniques de l'informatique graphique, permettent aujourd'hui d'animer des personnages virtuels réalistes et capables d'exprimer leurs émotions via de nombreuses modalités. Si plusieurs systèmes d'agents virtuels existent, ils restent encore limités par la diversité des modèles d'émotions utilisés, par leur niveau de réalisme, et par leurs capacités d'interaction temps réel. Dans nos recherches, nous nous intéressons aux agents virtuels capables d'exprimer des émotions via leurs expressions faciales en situation d'interaction avec l'utilisateur. Nos travaux posent de nombreuses questions scientifiques et ouvrent sur les problématiques suivantes : Comment modéliser les émotions en informatique en se basant sur les différentes approches des émotions en psychologie ? Quel niveau de réalisme visuel de l'agent est nécessaire pour permettre une bonne expressivité émotionnelle ? Comment permettre l'interaction temps réel avec un agent virtuel ? Comment évaluer l'impact des émotions exprimées par l'agent virtuel sur l'utilisateur ? A partir de ces problématiques, nous avons axé nos travaux sur la modélisation informatique des émotions et sur leurs expressions faciales par un personnage virtuel réaliste. En effet, les expressions faciales sont une modalité privilégiée de la communication émotionnelle. Notre objectif principal est de contribuer l'amélioration de l'interaction entre l'utilisateur et un agent virtuel expressif. Nos études ont donc pour objectif de mettre en lumière les avantages et les inconvénients des différentes approches des émotions ainsi que des méthodes graphiques étudiées. Nous avons travaillé selon deux axes de recherches complémentaires. D'une part, nous avons exploré différentes approches des émotions (catégorielle, dimensionnelle, cognitive, et sociale). Pour chacune de ces approches, nous proposons un modèle informatique et une méthode d'animation faciale temps réel associée. Notre second axe de recherche porte sur l'apport du réalisme visuel et du niveau de détail graphique à l'expressivité de l'agent. Cet axe est complémentaire au premier, car un plus grand niveau de détail visuel pourrait permettre de mieux refléter la complexité du modèle émotionnel informatique utilisé. Les travaux que nous avons effectués selon ces deux axes ont été évalués par des études perceptives menées sur des utilisateurs. La combinaison de ces deux axes de recherche est rare dans les systèmes d'agents virtuels expressifs existants. L'ensemble des logiciels que nous avons conçus forme notre plateforme d'agents virtuels MARC (Multimodal Affective and MARC a été utilisée dans des applications de natures diverses : jeu, intelligence ambiante, réalité virtuelle, applications thérapeutiques, performances artistiques,
La neuroimagerie permet d'étudier les liens entre la structure et le fonctionnement du cerveau. Des milliers d'études de neuroimagerie sont publiées chaque année. Il est difficile d'exploiter cette grande quantité de résultats. En effet, chaque étude manque de puissance statistique et peut reporter beaucoup de faux positifs. De plus, certains effets sont spécifiques à un protocole expérimental et difficile à reproduire. Les méta-analyses rassemblent plusieurs études pour identifier les associations entre structures anatomiques et processus cognitifs qui sont établies de manière consistante dans la littérature. Les méthodes classiques de méta-analyse commencent par constituer un échantillon d'études focalisées sur un même processus mental ou une même maladie. Ensuite, un test statistique permet de délimiter les régions cérébrales dans lesquelles le nombre d'observations reportées est significatif. Dans cette thèse, nous introduisons une nouvelle forme de méta-analyse, qui s'attache à construire des prédictions plutôt qu'à tester des hypothèses. Nous introduisons des modèles statistiques qui prédisent la distribution spatiale des observations neurologiques à partir de la description textuelle d'une expérience, d'un processus cognitif ou d'une maladie cérébrale. Notre approche est basée sur l'apprentissage statistique supervisé qui fournit un cadre classique pour évaluer et comparer les modèles. Nous construisons le plus grand jeu de données d'études de neuroimagerie et de coordonnées stéréotaxiques existant, qui rassemble plus de 13 000 publications. Cette thèse introduit des méthodes adaptées à la méta-analyse prédictive. Cette approche est complémentaire de la méta-analyse standard, et aide à interpréter les résultats d'études de neuroimagerie ainsi qu'à formuler des hypothèses ou des a priori statistiques.
Face au volume croissant de données audio et multimédia, les technologies liées à l'indexation de données et à l'analyse de contenu ont suscité beaucoup d'intérêt dans la communauté scientifique. D'importants progrès ont été réalisés dans le domaine ces dernières années principalement menés par les évaluations internationales du NIST. En effet, dans un premier temps, nous montrons qu'après avoir introduit une nouvelle composante de purification des clusters dans l'approche top-down, nous obtenons des performances comparables à celles de l'approche bottom-up. De plus, en étudiant en détails les deux types d'approches nous montrons que celles-ci se comportent différemment face à la discrimination des locuteurs et la robustesse face à la composante lexicale. Ces différences sont alors exploitées au travers d'un nouveau système combinant les deux approches. Enfin, nous présentons une nouvelle technologie capable de limiter l'influence de la composante lexicale, source potentielle d'artefacts dans le regroupement et la segmentation en locuteurs.
Avec le développement du commerce électronique, les clients ont publié de nombreux commentaires de produit sur Internet. Ces données sont précieuses pour les concepteurs de produit, car les informations concernant les besoins de client sont identifiables. L'objectif de cette étude est de développer une approche d'analyse automatique des commentaires utilisateurs permettant d'obtenir des informations utiles au concepteur pour guider l'amélioration et l'innovation des produits. L'approche proposée contient deux étapes : structuration des données et analyse des données. Dans la structuration des données, l'auteur propose d'abord une ontologie pour organiser les mots et les expressions concernant les besoins de client décrient dans les commentaires. Ensuite, une méthode de traitement du langage naturelle basée des règles linguistiques est proposé pour structurer automatiquement les textes de commentaires dans l'ontologie proposée. Dans l'analyse des données, deux méthodes sont proposées pour obtenir des idées d'innovation et des visions sur le changement de préférence d'utilisateur avec le temps. Dans ces deux méthodes, les modèles et les méthodes traditionnelles comme affordance base design, l'analyse conjointe, et le Kano model sont étudié et appliqué d'une façon innovante. Pour évaluer la praticabilité de l'approche proposée dans la réalité, les commentaires de client de liseuse numérique Kindle sont analysés. Des pistes d'innovation et des stratégies pour améliorer le produit sont identifiés et construites.
L'objectif principal de l'ingénierie des systèmes est la création d'un ensemble de produits et des services de haute qualité qui permettent l'accomplissement de tâches pour répondre aux besoins des clients. Un projet typique d'ingénierie des systèmes peut être divisé en trois phases : la définition, le développement et le déploiement. La phase de définition comprend les activités de capture des exigences et de leur raffinement. À la fin de la phase de définition du système, nous avons toutes les exigences fonctionnelles et non-fonctionnelles du système. L'un des résultats de la phase de développement est le modèle de travail initiale du système. La phase de déploiement se compose des activités liées à (1) l'évaluation opérationnelle du système, à (2) l'utilisation du système et à (3) son entretien. Dans un cycle de vie du projet, il y a de nombreuses questions qui doivent être traitées au cours des différentes phases pour finalement livrer un produit. Nous avons proposé une solution aux problèmes liés à l'ingénierie des exigences et aux techniques de la détection, de la gestion et de la résolution des conflits entre les parties prenantes. Cette thèse est basée sur les dernières avancées dans les pratiques industrielles et de recherche dans le domaine de l'ingénierie de conception du système. L'objectif de ce travail de thèse est de proposer une méthodologie de conception novatrice et globale en tenant compte de l'environnement multidisciplinaire et de multiples intervenants. Nous avons proposé un langage de modélisation des exigences basé sur les techniques GORE. Nous avons proposé quelques outils pour réduire l'ambiguïté des exigences tels l'utilisation de phrases négatives et de tests à l'aide de négation lorsqu'il s'agit de traiter certaines exigences difficiles à comprendre avec les techniques classiques. Nous avons également proposé des techniques de gestion des exigences pour mieux assurer leur traçabilité. En utilisant la même technique de pondération de critères, une méthode de décision multicritères et multi participants est proposée pour divers problèmes de décision survenant pendant le cycle de vie du projet d'ingénierie systèmes. Enfin, une approche globale de l'ingénierie des systèmes est proposée pour intégrer toutes les contributions faites précédemment et est illustrée sur une étude de cas concernant un projet réel avec la présentation d'un outil SysEngLab que nous avons développé pour mettre en oeuvre la majorité des méthodes et des techniques proposées au cours de thèse.
Dans le cadre de la rééducation orthophonique des troubles de la parole associés à un mauvais positionnement de la langue, il peut être utile au patient et à l'orthophoniste de visualiser la position et les mouvements de cet articulateur naturellement très peu visible. L'imagerie échographique peut pallier ce manque, comme en témoignent de nombreuses études de cas menées depuis plusieurs années dans les pays anglo-saxons. Appuyés par de nombreux travaux sur les liens entre production et perception de la parole, ces études font l'hypothèse que ce retour articulatoire visuel faciliterait la rééducation du patient. Lors des séances orthophoniques, le patient semble, en effet, mieux appréhender les déplacements de sa langue, malgré la difficulté d'interprétation sous-jacente de l'image échographique liée au bruit inhérent à l'image et à l'absence de vision des autres articulateurs. Nous développons dans cette thèse le concept d'échographie linguale augmentée. Nous proposons deux approches afin d'améliorer l'image échographique brute, et présentons une première application clinique de ce dispositif. La première approche porte sur le suivi du contour de la langue sur des images échographiques. Nous proposons une méthode basée sur une modélisation par apprentissage supervisé des relations entre l'intensité de l'ensemble des pixels de l'image et les coordonnées du contour de langue. Une étape de réduction de la dimension des images et des contours par analyse en composantes principales est suivie d'une étape de modélisation par réseaux de neurones. Nous déclinons des implémentations mono-locuteur et multi-locuteur de cette approche dont les performances sont évaluées en fonction de la quantité de contours manuellement annotés (données d'apprentissage). Nous obtenons pour des modèles mono-locuteur une erreur de 1,29 mm avec seulement 80 images, performance meilleure que celle de la méthode de référence EdgeTrak utilisant les contours actifs. Nous construisons tout d'abord un modèle d'association entre les images échographiques et les paramètres de contrôle de la langue acquis sur ce locuteur de référence. Nous adaptons ensuite ce modèle à de nouveaux locuteurs dits locuteurs source. Nous comparons cette approche avec une régression directe par GMR entre données du locuteur source et paramètre de contrôle de la tête parlante. Nous montrons que l'approche par C-GMR réalise le meilleur compromis entre quantité de données d'adaptation d'une part, et qualité de la prédiction d'autre part. Enfin, nous évaluons la capacité de généralisation de l'approche C-GMR et montrons que l'information a priori sur le locuteur de référence exploitée par ce modèle permet de généraliser à des configurations articulatoires du locuteur source non vues pendant la phase d'adaptation. Enfin, nous présentons les premiers résultats d'une application clinique de l'échographie augmentée à une population de patients ayant subi une ablation du plancher de la bouche ou d'une partie de la langue. Les premiers résultats montrent une amélioration des performances des patients, notamment sur le placement de la langue.
Avec la prolifération rapide des plateformes de données qui récoltent des données relatives à plusieurs domaines tels que les données de gouvernements, d'éducation, d'environnement ou les données de notations de produits, plus de données sont disponibles en ligne. Ceci représente une opportunité sans égal pour étudier le comportement des individus et les interactions entre eux. Sur le plan politique, le fait de pouvoir interroger des ensembles de données de votes peut fournir des informations intéressantes pour les journalistes et les analystes politiques. En particulier, ce type de données peut être exploité pour l'investigation des sujet exceptionnellement conflictuels ou consensuels. Considérons des données décrivant les sessions de votes dans le parlement Européen (PE). Ces données offrent la possibilité d'étudier les accords et désaccords de sous-groupes cohérents, en particulier pour mettre en évidence des comportements inattendus. Par exemple, il est attendu que sur la majorité des sessions, les députés votent selon la ligne politique de leurs partis politiques respectifs. Cependant, lorsque les sujets sont plutôt d'intérêt d'un pays particulier dans l'Europe, des coalitions peuvent se former ou se dissoudre. À titre d'exemple, quand une procédure législative concernant la pêche est proposée devant les MPE dans l'hémicycle, les MPE des nations insulaires du Royaume-Uni peuvent voter en accord sans être influencés par la différence entre les lignes politiques de leurs alliances respectives, cela peut suggérer un accord exceptionnel comparé à la polarisation observée habituellement. Dans cette thèse, nous nous intéressons à ce type de motifs décrivant des (dés)accords exceptionnels, pas uniquement sur les données de votes mais également sur des données similaires appelées données comportementales. La première permet la découverte de (dés)accords exceptionnels entre groupes tandis que la seconde permet de mettre en évidence les comportements exceptionnels qui peuvent au sein d'un même groupe. Dans l'esprit d'évaluer la capacité des deux méthodes à réaliser cet objectif, nous évaluons les performances quantitatives et qualitatives sur plusieurs jeux de données réelles. De plus, nous motivons l'utilisation des méthodes proposées dans le contexte du journalisme computationnel.
La dématérialisation des données de santé a permis depuis plusieurs années de constituer un véritable gisement de données provenant de tous les domaines de la santé. Ces données ont pour caractéristiques d'être très hétérogènes et d'être produites à différentes échelles et dans différents domaines. Leur réutilisation dans le cadre de la recherche clinique, de la santé publique ou encore de la prise en charge des patients implique de développer des approches adaptées reposant sur les méthodes issues de la science des données. L'objectif de cette thèse est d'évaluer au travers de trois cas d'usage, quels sont les enjeux actuels ainsi que la place des data sciences pour l'exploitation des données massives en santé. La démarche utilisée pour répondre à cet objectif consiste dans une première partie à exposer les caractéristiques des données massives en santé et les aspects techniques liés à leur réutilisation. La seconde partie expose les aspects organisationnels permettant l'exploitation et le partage des données massives en santé. La troisième partie décrit les grandes approches méthodologiques en science des données appliquées actuellement au domaine de la santé. Enfin, la quatrième partie illustre au travers de trois exemples l'apport de ces méthodes dans les champs suivant : la surveillance syndromique, la pharmacovigilance et la recherche clinique. Nous discutons enfin les limites et enjeux de la science des données dans le cadre de la réutilisation des données massives en santé.
Le Web sémantique propose un ensemble de standards et d'outils pour la formalisation et l'interopérabilité de connaissances partagées sur le Web, sous la forme d'ontologies. Les travaux de cette thèse concernent dans un premier temps une méthode fondée sur les structures de patrons, une extension de l'analyse formelle de concepts pour la découverte de co-occurences de événements indésirables médicamenteux dans des données patients. Cette méthode utilise une ontologie de phénotypes et une ontologie de médicaments pour permettre la comparaison de ces événements complexes, et la découverte d'associations à différents niveaux de généralisation, par exemple, au niveau de médicaments ou de classes de médicaments. Dans un second temps, on utilisera une méthode numérique fondée sur des mesures de similarité sémantique pour la classification de déficiences intellectuelles génétiques. On étudiera deux mesures de similarité utilisant des méthodes de calcul différentes, que l'on utilisera avec différentes combinaisons d'ontologies phénotypiques et géniques. En particulier, on quantifiera l'influence que les différentes connaissances de domaine ont sur la capacité de classification de ces mesures, et comment ces connaissances peuvent coopérer au sein de telles méthodes numériques. Une troisième étude utilise les données ouvertes liées ou LOD du Web sémantique et les ontologies associées dans le but de caractériser des gènes responsables de déficiences intellectuelles. L'ensemble des contributions de cette thèse montre qu'il est possible de faire coopérer avantageusement une ou plusieurs ontologies dans divers processus de fouille de données
Cette thèse porte sur la co-construction de la signification en interaction et les manifestations des processus interprétatifs des participants. En s'intéressant au processus d'explicitation, c'est-à-dire le processus par lequel un contenu informationnel devient explicite dans la conversation, elle propose une étude pluridimensionnelle de la séquence conversationnelle en jeu dans ce processus. La co-construction de la signification est ici abordée comme relevant d'une transformation informationnelle et de l'inférence. Nos analyses ont porté sur un corpus de français parlé en interaction, en contexte de repas et apéritifs entre amis. Ainsi, nous proposons de parcourir cettepratique selon trois axes d'analyse : (a) une analyse séquentielle, s'intéressant au déploiement de la séquence d'explicitation et des éléments la composant, (b) une analyse reposant sur une modélisation de la gestion informationnelle dans cetteséquence, et (c) une analyse des formats linguistiques employés pour l'exhibition du processus inférentiel. Un des enjeux de ce travail l'élaboration d'un modèle conversationnaliste pour la gestion informationnelle et son application à l'analyse desdonnées de langue parlée en interaction.
Notre recherche s'inscrit à la fois dans le domaine des Recherches sur l'acquisition des langues et celui de la Didactique des langues étrangères, et poursuit un double objectif. D'abord, nous avons décrit les influences translinguistiques, issues de l'espagnol L1 comme de l'anglais L2, dans le processus d'acquisition de certaines structures liées au groupe verbal en français L3, à savoir la construction des verbes avec des compléments nominaux, la sélection des prépositions introduisant éventuellement ces compléments et l'acquisition des pronoms compléments. Pour cela, nous avons pris en compte les apports théoriques des recherches sur l'acquisition plurilingue mais également des études ayant dégagé des itinéraires de développement pour le français. Nous nous sommes basée sur des productions écrites réalisées par des étudiants, débutants en français, de première année de la filière de Traduction/Interprétation de la Universidad de Concepción au Chili. Le deuxième objectif de notre recherche correspond à une démarche de transposition didactique de résultats empiriques dans le but de mieux accompagner l'apprentissage du français dans notre contexte de formation. Nos propositions didactiques concernent aussi bien la sélection et le séquençage des contenus grammaticaux, la démarche méthodologique d'enseignement de la grammaire, et la correction/évaluation de la compétence grammaticale des apprenants. In fine, notre objectif didactique est de contribuer à la formation d'apprenants plurilingues conscients des processus à l'œuvre dans leur acquisition, et de les accompagner dans un usage réflexif de leurs connaissances linguistiques, notamment grammaticales.
L'intelligence artificielle (IA), avec les récents progrès de l'apprentissage statistique (ML), vise actuellement à révolutionner la manière dont la science expérimentale est conduite. En physique, chimie, biologie, neurosciences ou médecine, les données sont désormais le moteur de nouvelles théories et de nouvelles hypothèses scientifiques. L'apprentissage supervisé et les modèles prédictifs sont désormais utilisés pour évaluer si quelque chose est 'prévisible' :  La ML est maintenant utilisée en remplacement des tests statistiques d'hypothèses classiques. Dans le domaine des soins de santé, on parle de médecine de précision, de patients virtuels dont la vision est telle que l'intelligence artificielle permettra d'avoir des prédictions individualisées à partir de données génomiques, physiologiques ou d'imagerie. Après des percées pionnières dans le domaine de la vision par ordinateur, du traitement de la parole ou du langage naturel, la ML doit maintenant relever de nouveaux défis afin d'avoir un impact sur diverses disciplines scientifiques et en particulier sur les applications liées à la santé. Lors de l'examen des applications médicales, des problèmes statistiques et informatiques apparaissent. Le problème particulier étudié dans ce projet est lié à l'absence ou à la limitation de la supervision des algorithmes : les modèles prédictifs supervisés ont besoin de ce que l'on appelle des annotations ou des étiquettes pour être estimés et testés, et malheureusement trop peu d'applications médicales peuvent en fournir suffisamment. L'approche envisagée sera basée sur l'apprentissage auto-supervisé et l'ICA non linéaire.
La multiplication de sources textuelles sur le Web offre un champ pour l'extraction de connaissances depuis des textes et à la création de bases de connaissances. Dernièrement, de nombreux travaux dans ce domaine sont apparus ou se sont intensifiés. De ce fait, il est nécessaire de faire collaborer des approches linguistiques, pour extraire certains concepts relatifs aux entités nommées, aspects temporels et spatiaux, à des méthodes issues des traitements sémantiques afin de faire ressortir la pertinence et la précision de l'information véhiculée. Cependant, les imperfections liées au langage naturel doivent être gérées de manière efficace. Enfin, pour présenter un intérêt à l'échelle du Web, les traitements linguistiques doivent être multisources et interlingue. Cette thèse s'inscrit dans la globalité de cette problématique, c'est-à-dire que nos contributions couvrent aussi bien les aspects extraction et représentation de connaissances incertaines que la visualisation des graphes générés et leur interrogation.
L'analyse bioinformatique des données de transcriptomique a pour but d'identifier les gènes qui présentent des variations d'expression entre différentes situations, par exemple entre des échantillons de tissu sain et de tissu malade et de caractériser ces gènes à partir de leurs annotations fonctionnelles. Dans ce travail de thèse, je propose quatre contributions pour la prise en compte des connaissances du domaine dans ces méthodes. Tout d'abord je définis une nouvelle mesure de similarité sémantique et fonctionnelle (IntelliGO) entre les gènes, qui exploite au mieux les annotations fonctionnelles issues de l'ontologie GO ('Gene Ontology'). Je montre ensuite, grâce à une méthodologie d'évaluation rigoureuse, que la mesure IntelliGO est performante pour la classification fonctionnelle des gènes. En troisième contribution je propose une approche différentielle avec affectation floue pour la construction de profils d'expression différentielle (PED). Je définis alors un algorithme d'analyse de recouvrement entre classes fonctionnelles et ensemble des références, ici les PEDs, pour mettre en évidence des gènes ayant à la fois les mêmes variations d'expression et des annotations fonctionnelles similaires. Cette méthode est appliquée à des données expérimentales produites à partir d'échantillons de tissus sains, de tumeur colo-rectale et de lignée cellulaire cancéreuse. Finalement, la mesure de similarité IntelliGO est généralisée à d'autres vocabulaires structurés en graphe acyclique dirigé et enraciné (rDAG) comme l'est l'ontologie GO, avec un exemple d'application concernant la réduction sémantique d'attributs avant la fouille.
À partir d'une étude de cas construite autour de l'analyse des discours produits par les différents acteurs des champs institutionnels étudiés (le champ de l'hébergement touristique pour Airbnb, celui du transport public particulier de personnes pour Uber), nous cherchons à mettre en évidence les stratégies des plateformes pour se constituer une place dans l'espace social. Nous mobilisons différentes grilles d'analyse pour comprendre les stratégies mises en œuvre par Airbnb et Uber : la dynamique des écosystèmes d'affaires des plateformes développée par la recherche en management stratégique, la grille de lecture du mégamarketing définie par Kotler en 1986, la théorie néo-institutionnelle et ses derniers développements concernant le travail institutionnel et la question de la légitimité développée en particulier par Suchman (1995). Airbnb et Uber ont mobilisé chacune à leur manière les compétences de mégamarketing pour constituer leur écosystème d'affaires et leur système de légitimité, véritable support à leur conquête institutionnelle. Ces différentes expressions de leurs stratégies s'incarnent aussi dans le processus de travail institutionnel, tourné vers la négociation pour Airbnb et vers la défiance pour Uber. Les résultats du processus institutionnel ont des similarités entre les cas : constitution des systèmes de légitimité nécessaires à l'interprétation du rôle de ces deux plateformes, reconnaissance par la loi des activités permises par les plateformes et de leurs versants producteurs, ajustement des offres des professionnels établis. Ce travail permet d'entrevoir un modèle de cycle de vie des plateformes, prenant en compte les dynamiques propres à ces formes organisationnelles ainsi que celles découlant du travail institutionnel et de leur quête de légitimité.
Ce projet vise à produire des outils experts afin de détecter automatiquement, voire de proposer, des (portions de) textes adaptées aux capacités de compréhension des enfants. Ces outils s'appuieront sur l'analyse de l'expression linguistique de la temporalité et des émotions dans les textes pour enfants, en lien avec des résultats de travaux psycholinguistiques portant sur les étapes développementales de compréhension de ces deux dimensions.
Extraire de l'information utile de ces données nécessite d'importants prétraitements et des étapes de réduction de bruit. La complexité de ces analyses rend les résultats très sensibles aux paramètres choisis. Le temps de calcul requis augmente plus vite que linéairement : les jeux de données sont si importants qu'il ne tiennent plus dans le cache, et les architectures de calcul classiques deviennent inefficaces. Pour réduire les temps de calcul, nous avons étudié le feature-grouping comme technique de réduction de dimension. Pour ce faire, nous utilisons des méthodes de clustering. Nous proposons un algorithme de clustering agglomératif en temps linéaire : Recursive Nearest Agglomeration (ReNA). ReNA prévient la création de clusters énormes, qui constitue un défaut des méthodes agglomératives rapides existantes. Nous démontrons empiriquement que cet algorithme de clustering engendre des modèles très précis et rapides, et permet d'analyser de grands jeux de données avec des ressources limitées. En neuroimagerie, l'apprentissage statistique peut servir à étudier l'organisation cognitive du cerveau. Des modèles prédictifs permettent d'identifier les régions du cerveau impliquées dans le traitement cognitif d'un stimulus externe. L'entraînement de ces modèles est un problème de très grande dimension, et il est nécéssaire d'introduire un a priori pour obtenir un modèle satisfaisant. Afin de pouvoir traiter de grands jeux de données et d'améliorer la stabilité des résultats, nous proposons de combiner le clustering et l'utilisation d'ensembles de modèles. Nous évaluons la performance empirique de ce procédé à travers de nombreux jeux de données de neuroimagerie. Enfin, nous montrons que l'utilisation d'ensembles de modèles améliore la stabilité des cartes de poids résultantes et réduit la variance du score de prédiction.
Nous proposons deux nouvelles approches pour les systèmes de recommandation et les réseaux. Dans la première partie, nous donnons d'abord un aperçu sur les systèmes de recommandation avant de nous concentrer sur les approches de rang faible pour la complétion de matrice. En nous appuyant sur une approche probabiliste, nous proposons de nouvelles fonctions de pénalité sur les valeurs singulières de la matrice de rang faible. L'algorithme résultant est un algorithme à seuillage doux itératif qui adapte de manière itérative les coefficients de réduction associés aux valeurs singulières. L'algorithme est simple à mettre en œuvre et peut s'adapter à de grandes matrices. Nous fournissons des comparaisons numériques entre notre approche et de récentes alternatives montrant l'intérêt de l'approche proposée pour la complétion de matrice à rang faible. Dans la deuxième partie, nous présentons d'abord quelques prérequis sur l'approche bayésienne non paramétrique et en particulier sur les mesures complètement aléatoires et leur extension multivariée, les mesures complètement aléatoires composées. Nous proposons ensuite un nouveau modèle statistique pour les réseaux creux qui se structurent en communautés avec chevauchement. Le modèle est basé sur la représentation du graphe comme un processus ponctuel échangeable, et généralise naturellement des modèles probabilistes existants à structure en blocs avec chevauchement au régime creux. Notre construction s'appuie sur des vecteurs de mesures complètement aléatoires, et possède des paramètres interprétables, chaque nœud étant associé un vecteur représentant son niveau d'affiliation à certaines communautés latentes. Nous développons des méthodes pour simuler cette classe de graphes aléatoires, ainsi que pour effectuer l'inférence a posteriori. Nous montrons que l'approche proposée peut récupérer une structure interprétable à partir de deux réseaux du monde réel et peut gérer des graphes avec des milliers de nœuds et des dizaines de milliers de connections.
L'imagerie médicale est une ressource de données principale pour différents types d'applications. Bien que les images concrétisent beaucoup d'informations sur le cas étudié, toutes les connaissances a priori du médecin restent implicites. Elles jouent cependant un rôle très important dans l'interprétation et l'utilisation des images médicales. Dans cette thèse, des connaissances anatomiques a priori sont intégrées dans deux applications médicales. Nous proposons d'abord une chaîne de traitement automatique qui détecte, quantifie et localise des anévrismes dans un arbre vasculaire segmenté. Des lignes de centre des vaisseaux sont extraites et permettent la détection et la quantification automatique des anévrismes. Pour les localiser, une mise en correspondance est faite entre l'arbre vasculaire du patient et un arbre vasculaire sain. Les connaissances a priori sont fournies sous la forme d'un graphe. Nous proposons ensuite un nouvel algorithme pour cette tâche, qui profite de toutes les connaissances a priori disponibles dans l'ontologie.
Les hospitalisations potentiellement évitables (HPE) sont les admissions à l'hôpital qui auraient pu être évitées grâce à des traitements rapides et efficaces. Les taux élevés de HPE sont associés à de nombreux facteurs. D'un autre côté, ces hospitalisations évitables sont associées à un coût de plusieurs centaines de millions d'euros pour l'assurance maladie. En d'autres termes, la réduction des HPE améliore non seulement la qualité de vie des patients, mais pourrait également faire économiser des coûts substantiels grâce aux traitements des patients. Par conséquent, les autorités sanitaires sont très intéressées par des solutions améliorant les services de santé pour réduire les HPE. Certaines études récentes en France ont suggéré que l'augmentation du nombre d'infirmières dans certaines zones géographiques pourrait conduire à une réduction des taux des HPE. Dans notre approche, après avoir évalué certaines méthodes de régression courantes, nous avons étendu la machine à vecteurs de support pour la régression à l'information spatiale. Cette approche nous permet de sélectionner non seulement les zones géographiques mais aussi le nombre d'infirmières à ajouter dans ces zones pour la plus forte réduction du nombre des HPE. Concrètement, notre approche est appliquée en Occitanie, en France et les zones géographiques mentionnées ci-dessus sont les bassins de vie (BV). D'un autre côté, la température extrême pourrait être un facteur potentiel associé à des taux élevés de HPE. Par conséquent, une partie de nos travaux consiste à mesurer l'impact de la température extrême sur les HPE ainsi qu'à inclure ces données environnementales dans notre approche ci-dessus. Dans nos travaux, nous avons utilisé les valeurs de température mesurées toutes les heures par des capteurs dans les stations météorologiques. Cependant, ces valeurs sont parfois discontinues et nous avons besoin d'une méthode d'imputation pour ces valeurs manquantes. Dans la littérature, deux approches les plus populaires traitant de cette étape de traitement exploitent soit la composante spatiale soit la composante temporelle des données de température. Respectivement, ces approches sont des méthodes d'interpolation spatiale telles que la pondération de distance inverse (IDW) et des modèles de séries temporelles tels que la moyenne mobile intégrée autorégressive (ARIMA). Les résultats montrent que par rapport aux méthodes IDW et ARIMA, notre approche fonctionne mieux à 100% et 99,8% (604 sur 605) stations météorologiques respectivement. De plus, l'amélioration de la coordination entre les prestataires de soins de santé pourrait conduire à la réduction des HPE. Dans le cas où les patients changeraient d'hôpital pour des traitements, afin d'assurer des traitements efficaces et de haute qualité, les médecins devraient avoir accès au dossier médical des patients des hôpitaux précédents. Par conséquent, nous proposons une approche graphique pour résoudre ce problème. En particulier, nous modélisons d'abord les flux de données des patients entre les hôpitaux sous forme de graphique pondéré non orienté dans lequel les nœuds et les bords présentent respectivement les hôpitaux et la quantité de flux de patients. Ensuite, après avoir évalué deux méthodes de regroupement de graphes courantes, nous personnalisons celle qui convient le mieux à nos besoins. Notre résultat fournit des informations intéressantes par rapport aux approches basées sur les limites administratives.
Cette recherche s'intéresse à l'acquisition du bilinguisme précoce simultané dans un contexte de mixité familiale français Les recherches récentes dans ce domaine ont montré qu'un ensemble de facteurs tels que l'input parental, ainsi que les stratégies discursives familiales, peuvent expliquer comment l'enfant accède à la parole dans ce contexte (Döpke, 1998 ; Lanza, 1997, 2004 ; De Houwer, 2009 ; King et Fogle, 2013 entre autres). L'enfant bilingue a été enregistrée en interaction spontanée et naturelle avec ses deux parents respectifs sur une durée de deux ans (2 ; 00 à 4 ; 00 ans). Le corpus total est constitué de 68 heures d'enregistrement et l'échantillon analysé a été restreint à 28 heures de transcriptions. Les résultats montrent que la fréquence de l'exposition à l'input et les pratiques langagières familiales ont un impact considérable sur les rapports émergents entre les deux langues à un âge précoce. Il découle de notre analyse que l'enfant développe une forme dominante du bilinguisme et passe progressivement à l'usage harmonieux des deux langues vers 3 ans. Ce passage est accompagné des changements du paysage sonore et de la fréquence de l'exposition à l'input en russe. Un décalage dans l'apparition des catégories grammaticales a été noté : l'acquisition du français suit les modalités générales observées chez les enfants monolingues français, tandis que celle du russe connaît un décalage substantiel. Les transferts interlangues au niveau du lexique, de la morphologie (les fillers) et de la syntaxe permettent de soutenir l'existence des compétences sous-jacentes communes.
Les principaux sujets étudiés dans cette thèse concernent le développement d'algorithmes stochastiques d'optimisation sous incertitude, l'étude de leurs propriétés théoriques et leurs applications. On étudie leur convergence en utilisant des outils développés dans la théorie des processus de Markov : on utilise les propriétés du générateur infinitésimal et des inégalités fonctionnelles pour mesurer la distance entre leur distribution et une distribution cible. La première partie est dédiée aux graphes quantiques, munis d'une mesure de probabilité sur l'ensemble des sommets. Les graphes quantiques sont des versions continues de graphes pondérés non-orientés. Le point de départ de cette thèse a été de trouver la moyenne de Fréchet de tels graphes. La moyenne de Fréchet est une extension aux espaces métriques de la moyenne euclidienne et est définie comme étant le point qui minimise la somme des carrés des distances pondérées à tous les sommets. Notre méthode est basée sur une formulation de Langevin d'un recuit simulé bruité et utilise une technique d'homogénéisation. Dans le but d'établir la convergence en probabilité du processus, on étudie l'évolution de l'entropie relative de sa loi par rapport a une mesure de Gibbs bien choisie. En utilisant des inégalités fonctionnelles (Poincaré et Sobolev) et le lemme de Gronwall, on montre ensuite que l'entropie relative tend vers zéro. Notre méthode est testée sur des données réelles et nous proposons une méthode heuristique pour adapter l'algorithme à de très grands graphes, en utilisant un clustering préliminaire. Dans le même cadre, on introduit une définition d'analyse en composantes principales pour un graphe quantique. Ceci implique, une fois de plus, un problème d'optimisation stochastique, cette fois-ci sur l'espace des géodésiques du graphe. Nous présentons un algorithme pour trouver la première composante principale et conjecturons la convergence du processus de Markov associé vers l'ensemble voulu. Dans une deuxième partie, on propose une version modifiée de l'algorithme du recuit simulé pour résoudre un problème d'optimisation stochastique global sur un espace d'états fini. On montre la convergence en probabilité de l'algorithme vers l'ensemble optimal, on donne la vitesse de convergence et un choix de paramètres optimisés pour assurer un nombre minimal d'évaluations pour une précision donnée et un intervalle de confiance proche de 1. Ce travail est complété par un ensemble de simulations numériques qui illustrent la performance pratique de notre algorithme à la fois sur des fonctions tests et sur des données réelles issues de cas concrets.
Aligner des macromolécules telles que des protéines, des ADN et des ARN afin de révéler ou exploiter, leur homologie fonctionnelle est un défi classique en bioinformatique, qui offre de nombreuses applications, notamment dans la modélisation de structures et l'annotation des génomes. Un certain nombre d'algorithmes et d'outils ont été proposés pour le problème d'alignement structure-séquence d'ARN. Cependant, en ce qui concerne les ARN complexes, comportant des pseudo-noeuds, des interactions multiples et des paires de bases non canoniques, de tels outils sont rarement utilisés dans la pratique, en partie à cause de leurs grandes exigences de calcul, et de leur incapacité à supporter des types généraux de structures. Récemment, Rinaudo et al. ont donné un algorithme paramétré général pour la comparaison structure-séquence d'ARN, qui est capable de prendre en entrée n'importe quel type de structures comportant des pseudo-noeuds. L'algorithme paramétré est un algorithme de programmation dynamique basée sur la décomposition arborescente. Afin de l'accélérer sans perte sensible de précision, nous avons introduit une approche de programmation dynamique par bandes. De plus, trois algorithmes ont été développés pour obtenir des alignements sous-optimaux. Tous ces algorithmes ont été implémentés dans un logiciel nommé LiCoRNA (aLignment of Complex RNAs). Les performances de LiCoRNA ont été évaluées d'abord sur l'alignement des graines des familles de de la base de données RFAM qui comportent des pseudo-noeuds. Comparé aux autres algorithmes de l'état de l'art, LiCoRNA obtient généralement des résultats équivalents ou meilleurs que ses concurrents. Grâce à la grande précision démontrée par LiCoRNA, nous montrons que cet outil peut être utilisé pour améliorer les alignements de certaines familles de RFAM qui comportent des pseudo-noeuds.
Le cerveau biologique est composé d'un ensemble d'éléments qui évoluent depuis des millions d'années. Les neurones et autres cellules forment un réseau complexe d'interactions duquel émerge l'intelligence. C'est particulièrement le cas des réseaux neuronaux profonds modernes qui révolutionnent actuellement de nombreux domaines de recherche en informatique tel que la vision par ordinateur, la traduction automatique, le traitement du langage naturel et bien d'autres. Cependant, les réseaux de neurones artificiels ne sont basés que sur un petit sous-ensemble de fonctionnalités biologiques du cerveau. Ils se concentrent souvent sur les fonctions globales, homogènes et à un système complexe et localement hétérogène. Dans cette thèse, nous avons d'examiner le cerveau biologique, des neurones simples aux réseaux capables d'apprendre. Nous avons examiné individuellement la cellule neuronale, la formation des connexions entre les cellules et comment un réseau apprend au fil du temps. Pour chaque composant, nous avons utilisé l'évolution artificielle pour trouver les principes de conception neuronale qui nous avons optimisés pour les réseaux neuronaux artificiels. Notre objectif est d'améliorer la performance des réseaux de neurones artificiels par les moyens inspirés des neurosciences modernes. Cependant, en évaluant les effets biologiques dans le contexte d'un agent virtuel, nous espérons également fournir des modèles de cerveau utiles aux biologistes.
Au fil des dernières années les robots ont fait partie de notre quotidien. Les robots sont utilisés aussi pour l'organisation des produits dans les usines. Un autre domaine de croissance est la robotique sociale. Nous pouvons voir des études tel que des robots d'aide aux enfants autistes. Il y a aussi des robots qui sont utilisés pour accueillir des personnes dans des hôtels ou dans centres commerciaux pour interagir avec les gens. Ainsi, le robot doit comprendre le comportement des personnes. Et, pour les robots mobiles, il faut savoir comment naviguer dans l'environnement humain. En ce qui concerne les environnements humains, ce travail explore la navigation acceptable socialement des robots en direction de personnes. Une personne est une entité qui doit être pris en compte sur la base des normes sociales que nous (en tant que personnes) utilisons tous les jours. Dans cette thèse, nous explorons comment un robot s'approche d'une personne. Celle-ci peut-être gênée si quelque chose ou quelqu'un envahit son espace personnel. La personne se sentira aussi menacée si elle est approchée par derrière. Ces normes sociales doivent être respectées par le robot. C'est pour cela que nous modélisons le comportement du robot à travers des algorithmes d'apprentissage. Nous faisons approcher (manuellement) un robot d'un personne plusieurs fois et le robot apprend à reproduire ce comportement. Un autre travail de cette thèse est la compréhension d'un groupe de personnes. Nous, en tant que humains, avons la capacité de le faire intuitivement. Toutefois, un robot nécessite impérativement un modèle mathématique. Enfin, nous abordons le sujet d'un robot qui s'approche d'un groupe de personnes. Nous utilisons des démonstrations pour faire apprendre le robot. Nous évaluons le bon déroulement du comportement du robot comme par exemple, en observant combien de fois le robot envahit l'espace personnel des personnes pendant la navigation.
Cette étude présente une nouvelle approche pour la génération automatique de résumés, un des principaux défis du Traitement de la Langue Naturelle. Ce sujet, traité pendant un demi-siècle par la recherche, reste encore actuel car personne n'a encore réussi à créer automatiquement des résumés comparables, en qualité, avec ceux produits par des humains. Dans le premier, les phrases sont triées de façon à ce que les meilleures conforment le résumé final. Or, les phrases sélectionnées pour le résumé portent souvent des informations secondaires, une analyse plus fine s'avère nécessaire. Nous proposons une méthode de compression automatique de phrases basée sur l'élimination des fragments à l'intérieur de celles-ci. La méthode proposée est capable de générer des résumés corrects en espagnol. Les résultats de cette étude soulèvent divers aspects intéressants vis-à-vis du résumé de textes par compression de phrases.
La recrudescence de contenus dans lesquels les clients expriment leurs opinions relativement à des produits de consommation a fait de l'analyse d'opinion un sujet d'intérêt pour la recherche en apprentissage automatique. Cependant, prédire une opinion est un tâche difficile et parmi les modèles à disposition, peu sont capables de capturer la complexité de tels objets. Les approches actuelles reposent sur la prédiction de représentations simplifiées d'expressions affectives. Par exemple, il est possible de se restreindre à la reconnaissance de l'attribut de valence. Cette thèse propose d'étudier le problème de la construction de modèles structurés capables de tirer parti des dépendances entre les différentes composantes des opinions. Dans ce contexte, le choix d'un modèle d'opinion a des conséquences sur la complexité du problème d'apprentissage et sur les propriétés statistiques des fonctions de prédiction associées. Nous étudions 2 problèmes classique de l'analyse d'opinion pour lesquels nous mettons en oeuvre des modèles à base de fonctions à noyau de sortie permettant d'illustrer le compromis précision Un second aspect de cette thèse repose sur l'adaptation de méthodes issues de l'état de l'art à un jeu de données comportant des données d'opinion à la structure complexe. Nous proposons une approche basée sur l'apprentissage profond pour prendre en contre jointement les différentes étiquettes du modèle d'opinions. Une nouvelle architecture hiérarchique est introduite issue de la fusion de structures précédemment proposées en les étendant à un jeu de données multimodal. Nous montrons que notre approche fournit des résultats compétitifs par rapport à des architectures traitant séparément les différentes représentations des opinions ce qui soulève des nouvelles questions concernant les stratégies optimales de traitement de données définies selon une hiérarchie.
Le verbe dar est une unité lexicale particulièrement riche en nuances significatives. Parmi les études qui ont abordé cette unité, certaines font l'hypothèse d'une désémantisation lors de la 'grammaticalisation'du verbe. Ce processus serait illustré notamment par l'existence du marqueur discursif dale ainsi que par les emplois de dar en tant que 'support'dans des constructions verbo-nominales plus ou moins lexicalisées ou dans périphrases verbales du type dar un golpe, dar que hablar. L'hypothèse envisagée dans cette étude – menée dans le cadre théorique de la linguistique du signifiant, qui prône l'unicité du signe linguistique – est que le verbe dar exprime une notion d'existence sous-jacente à tous ses emplois discursifs. Le signifié de langue de dar exprimerait l'accès à l'existence d'une entité b, pour le bénéficiaire c d'une opération conduite par une entité a. Cette notion d'existence serait présente dans toutes les occurrences d'un lexème qui n'apparaît jamais vidé – ne serait-ce que partiellement – de son contenu lexical.
Les opérateurs de télécommunications se doivent de maîtriser et d'évaluer la qualité des services qu'ils offrent à leurs clients, dans un contexte en perpétuelle évolution. Comme alternative rapide et à moindre coût aux évaluations fondées sur l'interrogation d'utilisateurs, des outils de mesure ont été développés, qui intègrent des modèles permettant de prédire la qualité perçue. Cette thèse avait pour but de concevoir un outil de diagnostic de qualité vocale (applicable aux services de téléphonie), complémentaire à de tels modèles objectifs, afin d'obtenir des informations spécifiques sur la nature des défauts présents sur le signal audio et d'orienter vers des causes potentielles de ces défauts. En partant de l'hypothèse que la qualité vocale est multidimensionnelle, nous avons fondé l'outil de diagnostic sur la modélisation des quatre dimensions identifiées dans la littérature : la Bruyance, représentative des bruits de fond, la Continuité, relative à la perception des discontinuités dans le signal, la Coloration, liée aux distorsions du spectre de la voix, et la Sonie, traduisant la perception du niveau sonore. Chacune de ces dimensions est quantifiée à l'aide d'indicateurs de qualité issus de l'analyse du signal audio. Notre démarche a consisté, dans un premier temps, à rechercher dans des modèles objectifs récents (notamment la norme P.863 de l'UIT-T) des indicateurs de qualité et à en développer d'autres pour caractériser parfaitement chaque dimension. Finalement, pour chaque dimension, nous avons développé un module de classification automatique de défauts perçus en fonction de la nature du défaut identifié dans le signal, ainsi qu'un module supplémentaire estimant l'impact du défaut sur la qualité vocale. L'outil proposé couvre les trois bandes audio (bande étroite, bande élargie et bande super-élargie) couramment utilisées dans les systèmes de télécommunications avec, toutefois, une priorité pour les signaux en bande super-élargie, plus représentatifs des contenus audio qu'on sera amené à rencontrer dans les futurs services de télécommunications.
Pour estimer la répartition de la puissance au sein d'un réacteur nucléaire, il est nécessaire de coupler des modélisations neutroniques et thermohydrauliques. De telles simulations doivent disposer des valeurs sections efficaces homogénéisées à peu de groupes d'énergies qui décrivent les interactions entre les neutrons et la matière. Cette thèse est consacrée à la modélisation des sections efficaces par des techniques académiques innovantes basées sur l'apprentissage machine. La performance d'un modèle est principalement définie par le nombre de coefficients qui le caractérisent (c'est-à-dire l'espace mémoire nécessaire pour le stocker), la vitesse d'évaluation, la précision, la robustesse au bruit numérique, la complexité, etc. Dans cette thèse, un assemblage standard de combustible UOX REP est analysé avec trois variables d'état : le burnup, la température du combustible et la concentration en bore. Trois techniques d'approximation sont étudiées. Les méthodes de noyaux, qui utilisent le cadre général d'apprentissage machine, sont capables de proposer, dans un espace vectoriel normalisé, une grande variété de modèles de régression ou de classification. Les méthodes à noyaux peuvent reproduire différents espaces de fonctions en utilisant un support non structuré, qui est optimisé avec des techniques d'apprentissage actif. Les approximations sont trouvées grâce à un processus d'optimisation convexe facilité par "l'astuce du noyau”. Le caractère modulaire intrinsèque de la méthode facilite la séparation des phases de modélisation : sélection de l'espace de fonctions, application de routines numériques, et optimisation du support par apprentissage actif. Les réseaux de neurones sont des méthodes d'approximation universelles capables d'approcher de façon arbitraire des fonctions continues sans formuler de relations explicites entre les variables. Une fois formés avec des paramètres d'apprentissage adéquats, les réseaux à sorties multiples (intrinsèquement parallélisables) réduisent au minimum les besoins de stockage tout en offrant une vitesse d'évaluation élevée. Les stratégies que nous proposons sont comparées entre elles et à l'interpolation multilinéaire sur une grille cartésienne qui est la méthode utilisée usuellement dans l'industrie. L'ensemble des données, des outils, et des scripts développés sont disponibles librement sous licence MIT.
Depuis 2004, les médias sociaux en ligne ont connu une croissance considérable. Ce développement rapide a eu des effets intéressants pour augmenter la connexionet l'échange d'informations entre les utilisateurs, mais certains effets négatifs sont également apparus, dont le nombre de faux comptes grandissant jour après jour. Les sockpuppets sont les multiples faux comptes créés par un même utilisateur. Ils sont à l'origine de plusieurs types de manipulations comme la création de faux comptes pour louer, défendre ou soutenir une personne ou une organisation, ou pour manipuler l'opinion publique. Des expérimentations ont été réalisées sur l'étape d'adaptation en utilisant des données réelles extraites de Wikipédia anglais. Afin de trouver le meilleur algorithme d'apprentissage automatique pour la phase de détection de sockpuppet, les résultats de six algorithmes d'apprentissage automatique sont comparés. En outre, ils sont comparés à la littérature où les résultats de la comparaison montrent que notre proposition améliore la précision de la détection des sockpuppets. De plus, les résultats de cinq algorithmes de détection de communauté sont comparés pour la phase de regroupement de Sockpuppet, afin de trouver le meilleur algorithme de détection de communauté qui sera utilisé en temps réel.
La communication affective est au cœur de nos interactions interpersonnelles. Nous communiquons les émotions à travers de multiples canaux non verbaux. Plusieurs travaux de recherche sur l'interaction homme-machine ont exploité ces modalités de communication afin de concevoir des systèmes permettant de reconnaître et d'afficher automatiquement des signaux affectifs. Le toucher est la modalité la moins explorée dans ce domaine de recherche. L'aspect intrusif des interfaces haptiques actuelles est l'un des principaux obstacles à leur utilisation dans la communication affective médiée. En effet, l'utilisateur est physiquement connecté à des systèmes mécaniques pour recevoir la stimulation. Cette configuration altère la transparence de l'interaction médiée et empêche la perception de certaines dimensions affectives comme la valence. Sur la base de l'état de l'art des interfaces haptiques, nous avons proposé une stratégie de stimulation tactile basée sur l'utilisation d'un jet d'air mobile. Cette technique permet de fournir une stimulation tactile non-intrusive sur des zones différentes du corps. De plus, ce dispositif tactile permettrait une stimulation efficace de certains mécanorécepteurs qui jouent un rôle important dans les perceptions d'affects positifs. Nous avons conduit une étude expérimentale pour comprendre les relations entre les caractéristiques physiques de la stimulation tactile par jet d'air et la perception affective des utilisateurs. Les résultats mettent en évidence les effets principaux de l'intensité et de la vitesse du mouvement du jet d'air sur l'évaluation subjective mesurée dans l'espace affectif (à savoir, la valence, l'arousal et de la dominance).La communication des émotions est clairement multimodale. Nous utilisons le toucher conjointement avec d'autres modalités afin de communiquer les différents messages affectifs. C'est dans ce sens que nous avons conduit deux études expérimentales pour examiner la combinaison de la stimulation tactile par jet d'air avec les expressions faciales et vocales pour la perception de la valence. Ces expérimentations ont été conduites dans un cadre théorique et expérimental appelé théorie de l'intégration de l'information. Ce cadre permet de modéliser l'intégration de l'information issue de plusieurs sources en employant une algèbre cognitive. Les résultats de nos travaux suggèrent que la stimulation tactile par jet d'air peut être utilisée pour transmettre des signaux affectifs dans le cadre des interactions homme-machine. Les modèles perceptifs d'intégration bimodales peuvent être exploités pour construire des modèles computationnels permettant d'afficher des affects en combinant la stimulation tactile aux expressions faciales ou à la voix.
Les arguments de sécurité sont couramment utilisés pour montrer que des efforts suffisants ont été faits pour atteindre les objectifs de sécurité. Ainsi, la sécurité du système est souvent justifiée par l'évaluation des arguments de sécurité. L'évaluation de tels arguments repose généralement sur l'avis d'experts sans s'appuyer sur des outils ou des méthodes dédiés. Ceci pose des questions sur la validité des résultats. Dans cette thèse, une approche quantitative est proposée, basé sur la théorie de Dempster-Shafer (théorie D-S) pour évaluer notre confiance dans les arguments de sécurité. Une application dans le domaine ferroviaire conduit à l'estimation des paramètres du cadre par une enquête auprès d'experts en sécurité.
Cette thèse propose une méthode et un système d'identification d'entités (personnes, lieux, organisations) mentionnées au sein des contenus textuels produits par l'Agence France Presse dans la perspective de l'enrichissement automatique de ces contenus. Les différents domaines concernés par cette tâche ainsi que par l'objectif poursuivi par les acteurs de la publication numérique de contenus textuels sont abordés et mis en relation : Web Sémantique, Extraction d'Information et en particulier Reconnaissance d'Entités Nommées (REN), Annotation Sémantique, Liage d'Entités. À l'issue de cette étude, le besoin industriel formulé par l'Agence France Presse fait l'objet des spécifications utiles au développement d'une réponse reposant sur des outils de Traitement Automatique du Langage. L'approche adoptée pour l'identification des entités visées est ensuite décrite : nous proposons la conception d'un système prenant en charge l'étape de REN à l'aide de n'importe quel module existant, dont les résultats, éventuellement combinés à ceux d'autres modules, sont évalués par un module de Liage capable à la fois (i) d'aligner une mention donnée sur l'entité qu'elle dénote parmi un inventaire constitué au préalable, (ii) de repérer une dénotation ne présentant pas d'alignement dans cet inventaire et (iii) de remettre en cause la lecture dénotationnelle d'une mention (repérage des faux positifs). Le système Nomos est développé à cette fin pour le traitement de données en français. Sa conception donne également lieu à la construction et à l'utilisation de ressources ancrées dans le réseau des Linked Data ainsi que d'une base de connaissances riche sur les entités concernées.
Dans un contexte où l'industrie du semi-conducteur explore de nouvelles voies avec la diversification des produits et le paradigme de « More than Moore » , les délais de livraison et la précision de livraison sont des éléments clés pour la compétitivité d'entreprises de semi-conducteur et l'industrie 4.0 en général. La première partie de cette thèse offre une étude approfondie de la variabilité : nous mettons d'abord en avant les conséquences de la variabilité pour mieux la définir, puis nous clarifions que la variabilité concerne les flux de production en introduisant la notion de variabilité des flux de production et en apportant des éléments de mesure associés, et nous clôturons cette première partie par l'étude des sources de variabilité à travers une étude bibliographique et des exemples industriels. La seconde partie est dédiée à l'intégration de la variabilité dans les outils de gestion de production : nous montrons comment une partie des conséquences peut être mesurée et intégrée aux projections d'encours pour améliorer le contrôle et la prévisibilité de la production, proposons un nouvel outil ((the WIP Concurrent) pour mesurer plus précisément les performances des systèmes en environnement complexe, et mettons en avant des effets de dépendances prépondérants sur la variabilité des flux de production et pourtant jamais pris en compte dans les modèles. La troisième et dernière partie de la thèse couvre les perspectives de réduction de la variabilité : en se basant sur les éléments présentés dans la thèse, nous proposons un plan pour réduire la variabilité des flux de production sur le court terme, et une direction pour la recherche à moyen et long terme.
Le doctorant prendra part à l'ANR JCJC MAOI (Analyse Multimodale des Opinions en Interactions) à Telecom-ParisTech. Le principal défi consistera à intégrer le contexte d'interaction dans les méthodes de détection d'opinion par apprentissage automatique.
Dans le premier, nous utilisons le modèle autorégressif de la parole qui décrit les corrélations court et long termes (quasi-périodique) pour formuler un modèle d'état dépendant de paramètres inconnus. EM-Kalman est ainsi utilisé pour estimer conjointement les sources et les paramètres. Dans le deuxième, nous proposons une méthode fréquentielle pour le même modèle de la parole où les sources et les paramètres sont estimés séparément. Les observations sont découpées à l'aide d'un fenêtrage bien conçu pour assurer une reconstruction parfaite des sources après. Les paramètres (de l'enveloppe spectrale) sont estimés en maximisant le critère du GML exprimé avec la matrice de covariance paramétrée que nous modélisons plus correctement en tenant compte de l'effet du fenêtrage. Le filtre de Wiener est utilisé pour estimer les sources. Nous considérons le cas conjointement Gaussien (observations et variables cachées) et trois méthodes itératives d'estimation conjointe : MAP en alternance avec ML, biaisé même asymptotiquement pour les paramètres, EM qui converge asymptotiquement vers ML et VB que nous prouvons converger asymptotiquement vers la solution ML pour les paramètres déterministes.
La diversité linguistique est actuellement menacée : la moitié des langues connues dans le monde pourraient disparaître d'ici la fin du siècle. Cette prise de conscience a inspiré de nombreuses initiatives dans le domaine de la linguistique documentaire au cours des deux dernières décennies, et 2019 a été proclamée Année internationale des langues autochtones par les Nations Unies, pour sensibiliser le public à cette question et encourager les initiatives de documentation et de préservation. Néanmoins, ce travail est coûteux en temps, et le nombre de linguistes de terrain, limité. Par conséquent, le domaine émergent de la documentation linguistique computationnelle (CLD) vise à favoriser le travail des linguistes à l'aide d'outils de traitement automatique. Le projet Breaking the Unwritten Language Barrier (BULB), par exemple, constitue l'un des efforts qui définissent ce nouveau domaine, et réunit des linguistes et des informaticiens. Cette thèse examine le problème particulier de la découverte de mots dans un flot non segmenté de caractères, ou de phonèmes, transcrits à partir du signal de parole dans un contexte de langues très peu dotées. Il s'agit principalement d'une procédure de segmentation, qui peut également être couplée à une procédure d'alignement lorsqu'une traduction est disponible. En utilisant deux corpus en langues bantoues correspondant à un scénario réaliste pour la linguistique documentaire, l'un en Mboshi (République du Congo) et l'autre en Myene (Gabon), nous comparons diverses méthodes monolingues et bilingues de découverte de mots sans supervision. Nous montrons ensuite que l'utilisation de connaissances linguistiques expertes au sein du formalisme des Adaptor Grammars peut grandement améliorer les résultats de la segmentation, et nous indiquons également des façons d'utiliser ce formalisme comme outil de décision pour le linguiste. Nous proposons aussi une variante tonale pour un algorithme de segmentation bayésien non-paramétrique, qui utilise un schéma de repli modifié pour capturer la structure tonale. Pour tirer parti de la supervision faible d'une traduction, nous proposons et étendons, enfin, une méthode de segmentation neuronale basée sur l'attention, et améliorons significativement la performance d'une méthode bilingue existante.
Nous proposons de concevoir et développer un outil permettant d'analyser la diffusion d'information sur les services de réseaux sociaux en ligne grâce au traitement et à la visualisation de données. Fruit d'une réflexion méthodologique, ce dispositif permet d'observer les relations entre les dimensions conversationnelles, sémantiques, temporelles et géographiques des actes de communication en ligne. Courts messages se propageant rapidement sur la Toile selon des modèles encore mal connus, les mèmes Internet comptent parmi les contenus les plus prisés sur les plate-formes web. Les mèmes Internet circulant sur le service de microblog chinois Sina Weibo articulent notamment discussions personnelles, débats sociétaux et vastes campagnes médiatiques. Mobilisant des méthodes issues de l'analyse des réseaux et du traitement automatisé de la langue chinoise, nous procédons à l'analyse d'un vaste corpus de 200 millions de messages représentant l'activité sur Sina Weibo durant l'année 2012. Néanmoins, le volume de calculs nécessaires pour obtenir des résultats fiables sur un large corpus nous amène à abandonner cette approche, montrant par là-même la complexité d'une définition intéressante de l'objet numérique composite mème. Les résultats montrent que les usages majoritaires de Sina Weibo sont similaires à ceux des médias traditionnels (publicité, divertissement, loisirs...). Néanmoins, nous écartons les hashtags comme représentants des mèmes Internet, artefacts d'usages commerciaux et stratégiques à la diffusion cadrée et planifiée. L'organisation de ces informations selon un axe temporel dans un espace de visualisation interactif autorise une lecture détaillée de leur diffusion.
La plupart des systèmes de recommandation actuels se base sur des évaluations sous forme de notes (i.e., chiffre entre 0 et 5) pour conseiller un contenu (film, restaurant...) à un utilisateur. Ce dernier a souvent la possibilité de commenter ce contenu sous forme de texte en plus de l'évaluer. Il est difficile d'extraire de l'information d'un texte brut tandis qu'une simple note contient peu d'information sur le contenu et l'utilisateur. Dans cette thèse, nous tentons de suggérer à l'utilisateur un texte lisible personnalisé pour l'aider à se faire rapidement une opinion à propos d'un contenu. Plus spécifiquement, nous construisons d'abord un modèle thématique prédisant une description de film personnalisée à partir de commentaires textuels. Nous évaluons notre modèle sur une base de données IMDB et illustrons ses performances à travers la comparaison de thèmes. Nous étudions ensuite l'inférence de paramètres dans des modèles à variables latentes à grande échelle, incluant la plupart des modèles thématiques. Nous proposons un traitement unifié de l'inférence en ligne pour les modèles à variables latentes à partir de familles exponentielles non-canoniques et faisons explicitement apparaître les liens existants entre plusieurs méthodes fréquentistes et Bayesiennes proposées auparavant. Enfin, nous proposons une nouvelle classe de processus ponctuels déterminantaux (PPD) qui peut être manipulée pour l'inférence et l'apprentissage de paramètres en un temps potentiellement sous-linéaire en le nombre d'objets. Cette classe, basée sur une factorisation spécifique de faible rang du noyau marginal, est particulièrement adaptée à une sous-classe de PPD continus et de PPD définis sur un nombre exponentiel d'objets. Nous appliquons cette classe à la modélisation de documents textuels comme échantillons d'un PPD sur les phrases et proposons une formulation du maximum de vraisemblance conditionnel pour modéliser les proportions de thèmes, ce qui est rendu possible sans aucune approximation avec notre classe de PPD. Nous présentons une application à la synthèse de documents avec un PPD sur 2 à la puissance 500 objets, où les résumés sont composés de phrases lisibles.
Des milliards de « choses » connectées à l'internet constituent les réseaux symbiotiques de périphériques de communication (par exemple, les téléphones, les tablettes, les ordinateurs portables), les appareils intelligents, les objets (par exemple, la maison intelligente, le réfrigérateur, etc.) et des réseaux de personnes comme les réseaux sociaux. La notion de réseaux traditionnels se développe et, à l'avenir, elle ira au-delà, y compris plus d'entités et d'informations. Ces réseaux et ces dispositifs détectent, surveillent et génèrent constamment une grande uantité de données sur tous les aspects de la vie humaine. L'un des principaux défis dans ce domaine est que le réseau se compose de « choses » qui sont hétérogènes à bien des égards, les deux autres, c'est qu'ils changent au fil du temps, et il y a tellement d'entités dans le réseau qui sont essentielles pour identifier le lien entre eux. Dans cette recherche, nous abordons ces problèmes en combinant la théorie et les algorithmes du traitement des événements avec les domaines d'apprentissage par machine. Notre objectif est de proposer une solution possible pour mieux utiliser les informations générées par ces réseaux. Cela aidera à créer des systèmes qui détectent et répondent rapidement aux situations qui se produisent dans la vie urbaine afin qu'une décision intelligente puisse être prise pour les citoyens, les organisations, les entreprises et les administrations municipales. Les médias sociaux sont considérés comme une source d'information sur les situations et les faits liés aux utilisateurs et à leur environnement social. Au début, nous abordons le problème de l'identification de l'opinion publique pour une période donnée (année, mois) afin de mieux comprendre la dynamique de la ville. Le deuxième défi est de combiner les données du réseau avec diverses propriétés et caractéristiques en format commun qui faciliteront le partage des données entre les services. Pour le résoudre, nous avons créé un modèle d'événement commun qui réduit la complexité de la représentation tout en conservant la quantité maximale d'informations. Ce modèle comporte deux ajouts majeurs : la sémantiques et l'évolutivité. La partie sémantique signifie que notre modèle est souligné avec une ontologie de niveau supérieur qui ajoute des capacités d'interopérabilité. Bien que la partie d'évolutivité signifie que la structure du modèle proposé est flexible, ce qui ajoute des fonctionnalités d'extensibilité. Nous avons validé ce modèle en utilisant des modèles d'événements complexes et des techniques d'analyse prédictive. Pour faire face à l'environnement dynamique et aux changements inattendus, nous avons créé un modèle de réseau dynamique et résilient. Il choisit toujours le modèle optimal pour les analyses et s'adapte automatiquement aux modifications en sélectionnant le meilleur modèle. Nous avons utilisé une approche qualitative et quantitative pour une sélection évolutive de flux d'événements, qui réduit la solution pour l'analyse des liens, l'optimale et l'alternative du meilleur modèle.
A partir des décès certifiés électroniquement de 2012 à 2016 en France, la thèse vise à mettre en œuvre et évaluer les performances de méthodes de traitement automatique des langues, pour classer les causes médicales de décès disponibles en texte libre dans des regroupements syndromiques (RS) pertinents pour la surveillance réactive de la mortalité à visée d'alerte et d'évaluation d'impact sanitaire. Près de 100 RS répondant aux objectifs ont été définis. Deux méthodes de classification ont été développées : une méthode à base de règles linguistiques et une méthode par apprentissage supervisé (SVM). Deux modèles SVM ont été développés utilisant différentes combinaisons de caractéristiques. Le développement et l'évaluation des performances des méthodes se sont appuyés sur 4 500 certificats de décès annotés. L'évaluation a porté dans un premier temps sur 7 RS, puis a été étendue à 60 autres RS. Les évolutions mensuelles des RS attribués par les méthodes ont été comparées sur l'ensemble des décès de 2012 à 2016 (204 000 décès). La méthode par règles et le modèle SVM incluant l'ensemble des caractéristiques ont obtenu des performances élevées (F-mesure≥0,95) pour la classification des causes dans 31 RS. L'évolution temporelle de ces RS obtenus par les deux méthodes était comparable. En moyenne, les causes de décès au sein d'un certificat sont classées dans 3,7 RS. Une méthode de pondération équilibrée des RS a été proposée pour prendre en compte ces causes multiples lors de l'analyse en routine de la mortalité pour la surveillance à visée d'alerte et d'évaluation d'impact. Ces résultats permettent d'améliorer la surveillance réactive actuellement fondée sur des données d'état-civil.
L'acquisition de connaissances relatives aux constructions verbales est une question importante pour le traitement automatique des langues, mais aussi pour la lexicographie qui vise à documenter les nouveaux usages linguistiques. Cette tâche pose de nombreux enjeux, techniques et théoriques. Dans le cadre de cette thèse, nous nous intéressons plus particulièrement à deux aspects fondamentaux de la description du verbe : la notion d'entrée lexicale et la distinction entre arguments et circonstants. A la suite de précédentes études en traitement automatique des langues et en linguistique nous faisons l'hypothèse qu'il n'y a pas de distinction marquée entre homonymes et quasi-synonymes ; de même, nous posons qu'il existe un continuum entre arguments et circonstants. Nous proposons une chaîne de traitement complète pour l'acquisition de schémas prédicatifs verbaux en japonais à partir d'un corpus non étiqueté de textes journalistiques. Cette chaîne de traitement intègre la notion d'argumentalité au processus de création des entrées lexicales et met en œuvre une modélisation de ces deux continuums. La ressource produite a fait l'objet d'une évaluation comparative qualitative, qui a permis de mettre en évidence la difficulté des ressources linguistiques à décrire de nouvelles données, plaidant par là même pour une lexicologie s'inscrivant dans le cadre épistémologique de la linguistique de corpus.
Malgré ses récents succès en vision assistée par ordinateur ou en traduction automatique, l'utilisation de l'apprentissage profond dans le secteur biomédical fait face à de nombreux challenges. Parmi eux, nous comptons l'accès difficile à des données en quantité et qualité suffisantes, ainsi que le besoin en l'interopérabilité et en l'interprétabilité des modèles. Dans cette thèse, nous nous intéressons à ces différentes problématiques à la lueur de la création de modèles prédisant la glycémie future de patients diabétiques. De tels modèles permettraient aux patients d'anticiper les variations de leur glycémie au quotidien, les aidant ainsi à mieux la réguler afin d'éviter les états d'hypoglycémie et d'hyperglycémie. Pour cela, nous utilisons trois ensembles de données. Tandis que le premier a été récolté à l'occasion de cette thèse sur plusieurs patients diabétiques de type 2, les deux autres sont composés de patients diabétiques de type 1, à la fois réels et virtuels. Dans l'ensemble des études, nous utilisons les données passées de glycémie, d'insuline et de glucides de chaque patient pour construire des modèles personnalisés prédisant la glycémie du patient 30 minutes dans le futur. Dans un premier temps, nous faisons une analyse détaillée de l'état de l'art en construisant une base de résultats de référence open source de modèles prédictifs de glycémie. Bien que prometteurs, nous mettons en évidence la difficulté qu'ont les modèles profonds à effectuer des prédictions qui soient à la fois précises et sans danger pour le patient. Afin d'améliorer l'acceptabilité clinique des modèles, nous proposons d'intégrer des contraintes cliniques au sein de l'apprentissage des modèles. Nous explorons son utilisation pratique à travers un algorithme permettant d'obtenir un modèle maximisant la précision des prédictions tout en respectant des contraintes cliniques fixées au préalable. Puis, nous étudions la piste de l'apprentissage par transfert pour améliorer les performances des modèles prédictifs de glycémie. Celui-ci permet de faciliter l'apprentissage des modèles personnalisés aux patients en réutilisant les connaissances apprises sur d'autres patients. En particulier nous proposons le cadre de l'apprentissage par transfert multi-sources adverse. Celui-ci permet de significativement améliorer les performances des modèles en permettant l'apprentissage de connaissances a priori qui sont plus générales, car agnostiques des patients sources du transfert. Nous investiguons différents scénarios de transfert à travers l'utilisation de nos trois jeux de données. Nous montrons qu'il est possible d'effectuer un transfert de connaissance à partir de données provenant de dispositifs expérimentaux différents, de patients de types de diabète différents, mais aussi à partir de patients virtuels. Enfin, nous nous intéressons à l'amélioration de l'interprétabilité des modèles profonds à travers le principe d'attention. En particulier, nous explorons l'utilisation d'un modèle profond et interprétable pour la prédiction de la glycémie. Celui-ci implémente un double mécanisme d'attention lui permettant d'estimer la contribution de chaque variable en entrée au modèle à la prédiction finale. Nous montrons empiriquement l'intérêt d'un tel modèle pour la prédiction de glycémie en analysant son comportement dans le calcul de ses prédictions.
Ce travail de thèse s'inscrit dans la thématique de l'identification automatique des langues. Son objectif est de mettre en évidence des indices linguistiques susceptibles de permettre la distinction des idiomes issus du latin. Les langues romanes ont bénéficié d'une longue tradition descriptive et représentent des langues officielles dans plusieurs pays du monde. L'étude des approches taxinomistes consacrées aux idiomes néo-latins révèle une pertinence particulière de la classification typologique. Les indices vocaliques fournissent des critères appropriés pour une division des langues en deux zones linguistiques, selon leurs complexités respectives. Ces indices séparent l'espagnol et l'italien, langues à vocalisme prototypique du roumain, du français et du portugais, dont les systèmes vocaliques sont riches en oppositions supplémentaires. Afin de tester une pertinence perceptive des critères typologiques, deux paradigmes expérimentaux ont été développés. Une première série d'expériences, de type discrimination, a permis de délimiter le rôle des facteurs " langue maternelle " et " familiarité " des quatre populations participantes, dont deux de langue maternelle romane (français et Roumains) et deux de contrôle (Japonais, Américains). Les résultats ont partiellement convergé vers un regroupement linguistique basé sur la proximité sonore des langues et analogue à la classification typologique fondée sur les spécificités vocaliques. La seconde série d'expériences de type jugement de similarités effectuées par des sujets français et américains a confirmé ce regroupement. Ainsi, les proximités sonores établies de manières perceptive entre les langues romanes permettent leur macro-discrimination en deux groupes principaux : italien, espagnol vs, roumain, français, portugais.
Dans le domaine de la chimie, il est intéressant de pouvoir estimer des propriétés physico-chimiques de molécules, notamment pour des applications industrielles. Celles-ci sont difficiles à estimer par simulations physique, présentant une complexité temporelle prohibitive. L'émergence des données (publiques ou privées) ouvre toutefois de nouvelles perspectives pour le traitement de ces problèmes par des méthodes statistiques et d'apprentissage automatique. La principale difficulté réside dans la caractérisation des molécules : celles-ci s'apparentent davantage à un réseau d'atomes (autrement dit un graphe coloré) qu'à un vecteur. Le but de cette thèse est de tirer parti des corpus publics pour apprendre les meilleures représentations possibles de ces structures, et de transférer cette connaissance globale vers des jeux de données plus restreints. Nous nous inspirons pour ce faire de méthodes utilisées en traitement automatique des langages naturels. Pour les mettre en œuvre, des travaux d'ordre plus théorique ont été nécessaires, notamment sur le problème d'isomorphisme de graphes. Les résultats obtenus sur des tâches de classification/régression sont au moins compétitifs avec l'état de l'art, voire meilleurs, en particulier sur des jeux de données restreints, attestant des possibilités d'apprentissage par transfert sur ce domaine.
Comment concilier l'impératif d'assistance linguistique à toute personne ne parlant pas français et l'absence de ressources linguistiques standardisées pour traduire des combinaisons de langues génétiquement et culturellement distantes~ ? C'est le problème posé par la traduction du hindi et de l'ourdou en France dans le contexte judiciaire. Les systèmes judiciaires dont elles sont le moyen d'expression proviennent de l'héritage colonial britannique qui repose sur la common law. Ce travail propose, à travers l'analyse d'un corpus de documents variés, de créer des ressources terminologiques et phraséologiques afin d'aider le traducteur-interprète à trouver des équivalences de traduction multilingues. Dans un premier temps, nous abordons les différences entre les systèmes judiciaires et le statut des langues de travail dans les trois pays. Enfin, nous proposons une méthode d'extraction des termes et d'alignement par sous-corpus afin de faire ressortir les équivalences terminologiques ou traductionnelles du genre judiciaire entre ces langues. Ce travail, qui met en lumière les relations entre le texte, le contexte et les mots, fournit aux professionnels de la traduction et de l'interprétation des ressources attestées, adaptées au domaine de spécialité et contextualisées.
Cette thèse s'inscrit dans une étude sur l'élaboration d'une tête parlante. Nous nous intéressons tout particulièrement à la prédiction du mouvement de coarticulation des lèvres et de la mâchoire. Après avoir analysé les variations intra et interlocuteur des paramètres labiaux de deux corpora audiovisuels, nous avons conçu un algorithme de prédiction de la coarticulation basé sur des règles phonétiques et prenant en considération l'interaction entre les articulateurs. Le principe de base est la concaténation de séquences élémentaires de type VC...CV qui ont été jugées pertinentes par notre algorithme de prédiction phonétique, et qui sont soit extraites du corpus, soit obtenues par complétion. Afin d'estimer la qualité de notre synthèse, nous avons mesuré les différences entre les signaux réels et synthétisés sur l'ensemble des phrases du corpus et nous avons comparé notre solution avec l'algorithme de Cohen et Massaro. Nous avons montré que notre synthèse est meilleure pour certaines séquences spécifiques de type VCCV où l'anticipation est plus complexe.
Les grandes variations de style de l'écriture et les difficultés de segmenter les mots cursifs sont les raisons principales pour laquelle la reconnaissance de mots cursive manuscrite pour être une tâche si difficile. Un système de lecture des documents postaux indien basé sur le modèle stochastique basé d'un contexte sans segmentation est présenté. L'originalité du travail réside sur une combinaison de caractéristiques conceptuelles à haut niveau avec les renseignements de pixel à basse altitude considérés par ancien modèle et une stratégie d'arrêt dans l'algorithme Viterbi. Pendant que l'information de bas niveau peut être facilement extraite de la forme analysée, le pouvoir discriminatoire de telle information a des limites, car il décrit la forme avec moins de précision. Pour cette raison, nous avons considéré dans le cadre d'une approche analytique, utilisant une segmentation implicite, d'implanter de la haute information an le réduisant à un niveau plus bas. Cet enrichissement peut être perçu comme un poids au niveau de pixel, donnant une importance à chaque pixel analysé fondé sur leurs propriétés conceptuelles. Le défi est de combiner les types différents des caractéristiques considérant une certaine dépendance entre eux. Pour réduire le temps de décodage dans la recherche de Viterbi, un mécanisme de seuil cumulatif est proposé dans une représentation de vocabulaire plate. Au lieu de l'utilisation d'une représentation de trie où les parties de préfixe communes sont partagées nous proposons un mécanisme de seuil dans le vocabulaire plat où basé juste sur une analyse de Viterbi partielle, nous pouvons élaguer un modèle et arrêtons le traitant plus. Les seuils cumulatifs sont fondés sur les scores correspondants prémédités à chaque niveau de lettre, permettant une certaine dynamique et élasticité au modèle. Comme nous sommes intéressés dans un système de reconnaissance d'adresses postaux complet, nous avons convergé aussi notre attention sur la reconnaissance des chiffres, proposant différent solutions neuronaux et stochastiques. Pour augmenter la précision et la robustesse des classifieur, un stratégie de combinaison est aussi proposé.
Les programmes sont partout dans notre vie quotidienne : les ordinateurs et les téléphones, mais aussi les frigo, les avions et ainsi de suite. L'acteur principal dans la création de ces programmes est humain les êtres. Aussi minutie qu'ils peuvent être, les humains sont connus pour faire des erreurs involontaires sans leur conscience. Toute la durée de leur tâche de développement, les développeurs doivent faire face continuellement leurs erreurs (ou leurs collègues). Cette observation clé soulève la nécessité d'aider les développeurs dans leurs tâches de développement / maintenance.
Que ce soit pour des professionnels qui doivent prendre connaissance du contenu de documents en un temps limité ou pour un particulier désireux de se renseigner sur un sujet donné sans disposer du temps nécessaire pour lire l'intégralité des textes qui en traitent, le résumé est une aide contextuelle importante. Avec l'augmentation de la masse documentaire disponible électroniquement, résumer des textes automatiquement est devenu un axe de recherche important dans le domaine du traitement automatique de la langue. La présente thèse propose une méthode de résumé automatique multi-documents fondée sur une classification des phrases à résumer en classes sémantiques. Cette classification nous permet d'identifier les phrases qui présentent des éléments d'informations similaires, et ainsi de supprimer efficacement toute redondance du résumé généré. Cette méthode a été évaluée sur la tâche # résumé d'opinions issues de blogs # de la campagne d'évaluation TAC 2008 et la tâche # résumé incrémental de dépêches # des campagnes TAC 2008 et TAC 2009. Les résultats obtenus sont satisfaisants, classant notre méthode dans le premier quart des participants. Nous avons également proposé d'intégrer la structure des dépêches à notre système de résumé automatique afin d'améliorer la qualité des résumés qu'il génère. Pour finir, notre méthode de résumé a fait l'objet d'une intégration à un système applicatif visant à aider un possesseur de corpus à visualiser les axes essentiels et à en retirer automatiquement les informations importantes.
Selon la représentation d'entrée, cette thèse étudie ces deux types : la génération de texte à partir de représentation de sens et à partir de texte. En la première partie (Génération des phrases), nous étudions comment effectuer la réalisation de surface symbolique à l'aide d'une grammaire robuste et efficace. Cette approche s'appuie sur une grammaire FB-LTAG et prend en entrée des arbres de dépendance peu profondes. Afin nous proposons deux algorithmes de fouille d'erreur : le premier, un algorithme qui exploite les arbres de dépendance plutôt que des données séquentielles et le second, un algorithme qui structure la sortie de la fouille d'erreur au sein d'un arbre afin de représenter les erreurs de façon plus pertinente. Nous montrons que nos réalisateurs combinés à ces algorithmes de fouille d'erreur améliorent leur couverture significativement. En la seconde partie (Simplification des phrases), nous proposons l'utilisation d'une forme de représentations sémantiques (contre à approches basées la syntaxe ou SMT) afin d'améliorer la tâche de simplification de phrase. Nous utilisons les structures de représentation du discours pour la représentation sémantique profonde. Nous proposons alors deux méthodes de simplification de phrase : une première approche supervisée hybride qui combine une sémantique profonde à de la traduction automatique, et une seconde approche non-supervisée qui s'appuie sur un corpus comparable de Wikipedia
La sclérose en plaques (SEP) est une maladie chronique du système nerveux central, principale cause de handicap d'origine non traumatique chez l'adulte jeune. Il se caractérise par de nombreux processus de démyélinisation inflammatoire qui provoquent une vaste gamme de symptômes, notamment des déficits cognitifs et invalidité irréversible. L'imagerie par résonance magnétique (IRM) est aujourd'hui l'outil de référence pour le diagnostic de la maladie. En particulier, de nouvelles approches basées sur la représentation d'images IRM utilisant la théorie des graphes ont été appliquées avec succès pour l'étude et la quantification des dommages à la substance blanche. Grâce à leur capacité à analyser d'énormes quantités de données et à identifier les relations latentes, ce domaine de l'intelligence artificielle a connu un assez grand succès dans la communauté scientifique et s'applique désormais dans de nombreux contextes, notamment le diagnostic médical. Dans ce manuscrit, nous présenterons les différentes techniques d'apprentissage profond développées dans ce travail concernant l'analyse des images biomédicales et, en particulier, pour la classification et la caractérisation des patients atteints de SEP. En fait, la théorie des graphes est devenue un outil sensible pour la détection des altérations causées par les pathologies cérébrales, et peut être combinée à des techniques d'apprentissage automatique afin d'identifier les propriétés structurelles latentes utiles pour étudier la progression de la maladie. La première partie de ce manuscrit est consacré à la description de l'état de l'art. Cet état de l'art se focalisera sur les études montrant les effets de la SEP sur les faisceaux de SB grâce à l'emploi de l'imagerie de tenseur de diffusion. Une description des principales techniques d'apprentissage profond sera également fournie, ainsi que des exemples d'applicabilité dans le contexte biomédical. Dans la seconde partie, deux techniques d'apprentissage profond seront proposées, concernant la génération de nouvelles images IRM du cerveau humain et l'identification automatique du disque optique dans les images du fond oculaire. Dans la troisième partie, nous présenterons les techniques d'apprentissage profond combinées à la théorie des graphiques que développée dans ce travail pour étudier la connectivité structurelle des patients atteints d'une SEP. Enfin, nous explorerons des approches semi-supervisées et non supervisées pour réduire l'intervention humaine dans les processus de décision
Ces graphes peuvent ainsi servir de base à l'évaluation automatisée des risques, en s'appuyant sur l'identification et l'évaluation des actifs essentiels. Cela permet de concevoir des contre-mesures proactives et réactives pour la réduction des risques et peut être utilisé pour la surveillance et le renforcement de la sécurité du réseau. Cette thèse vise à appliquer une approche similaire dans les environnements Cloud, ce qui implique de prendre en compte les nouveaux défis posés par ces infrastructures modernes, la majorité des graphes d'attaque étant conçue pour une application dans des environnements traditionnels. Les nouveaux scénarios d'attaque liés à la virtualisation, ainsi que les propriétés inhérentes du Cloud, à savoir l'élasticité et le caractère dynamique, sont quelques-uns des obstacles à franchir à cette fin. Ainsi, pour atteindre cet objectif, un inventaire complet des vulnérabilités liées à la virtualisation a été effectué, permettant d'inclure cette nouvelle dimension dans les graphes d'attaque existants. Par l'utilisation d'un modèle adapté à l'échelle du Cloud, nous avons pu tirer parti des technologies Cloud et SDN, dans le but de construire des graphes d'attaque et de les maintenir à jour. Des algorithmes capables de faire face aux modifications fréquentes survenant dans les environnements virtualisés ont été conçus et testés à grande échelle sur une plateforme Cloud réelle afin d'évaluer les performances et confirmer la validité des méthodes proposées dans cette thèse pour permettre à l'administrateur de Cloud de disposer d'un graphe d'attaque à jour dans cet environnent.
L'analyse des images satellite et aériennes figure parmi les sujets fondamentaux du domaine de la télédétection. Ces dernières années, les avancées technologiques ont permis d'augmenter la disponibilité à large échelle des images, en comprenant parfois de larges étendues de terre à haute résolution spatiale. En plus des questions évidentes de complexité calculatoire qui en surgissent, un de plus importants défis est l'énorme variabilité des objets dans les différentes régions de la terre. Pour aborder cela, il est nécessaire de concevoir des méthodes de classification qui dépassent l'analyse du spectre individuel de chaque pixel, en introduisant de l'information contextuelle de haut niveau. Dans cette thèse, nous proposons d'abord une méthode pour la classification avec des contraintes de forme, basée sur l'optimisation d'une structure de subdivision hiérarchique des images. Nous explorons ensuite l'utilisation des réseaux de neurones convolutionnels (CNN), qui nous permettent d'apprendre des descripteurs hiérarchiques profonds. Nous étudions les CNN depuis de nombreux points de vue, ce qui nous permettra de les adapter à notre objectif. Parmi les sujets abordés, nous proposons différentes solutions pour générer des cartes de classification à haute résolution et nous étudions aussi la récolte des données d'entrainement. Nous avons également créé une base de données d'images aériennes sur des zones variées, pour évaluer la capacité de généralisation des CNN. Finalement, nous proposons une méthode pour polygonaliser les cartes de classification issues des réseaux de neurones, afin de pouvoir les intégrer dans des systèmes d'information géographique. Au long de la thèse, nous conduisons des expériences sur des images hyperspectrales, satellites et aériennes, toujours avec l'intention de proposer des méthodes applicables, généralisables et qui passent à l'échelle.
Cette thèse propose une description synchronique du système de détermination nominale du créole réunionnais, ainsi qu'une analyse de l'interprétation des syntagmes nominaux (SN). Elle inclut des données nouvelles provenant d'une part d'un ensemble de corpus oraux et d'autre part de jugements de grammaticalité et de félicité. Nous examinons la distribution des différents SN, le statut morphosyntaxique des éléments figurant en position pré- et postnominale, le système du nombre, ainsi que l'expression de la définitude en réunionnais.
Cette thèse propose une étude de l'alternance entre do it, this and do that dans leur emploi comme anaphores verbales (Verb Phrase anaphors, VPAs), où ils renvoient à une action saillante soit évoquée précédemment dans le discours, (la plus souvent via un SV) soit, par exophore, à une action saillante dans la situation discursive mais non évoquée explicitement dans le discours précédent. Do it/this/that ont été peu étudiés dans la littérature par ailleurs conséquente sur l'anaphore and et en particulier l'ellipse du VP (VP ellipsis, VPE, par ex. Kim knows the answer and Pat does too). En effet, on a longtemps considéré que ces trois constructions sont interchangeables entre elles ainsi qu'avec do so et l'ellipse, de sorte qu'un examen détaillé de leur propriétés discursives n'a pas été jugé utile. Les exemples ci-dessous montrent que cette supposition est incorrecte : en (1), un exemple attesté tiré du BNC, do this/that/so pourraient être employés au lieu de do it, mais en (3), do that est nettement préféré. S'agissant de l'ellipse, elle est peu naturelle en (1) et préfère un contexte comme celui en (2). 1.They've been rescuing companies for so long they do it automatically now, I expect. (AB9, ok : they do this/that/so automatically…) 2. They've been rescuing companies for so long that whenever they do, it's always a success. 3. He closes his eyes when he speaks and I don't trust anyone who does that. (AHF ; …anyone who #does this/#it/#so) A partir d'un échantillon d'exemples annotés du British National corpus (BNC, Davies 2004-), notre étude examinera les facteurs qui entrent en jeu dans l'alternance entre do it/this/that. Le choix entre les VPAs est déterminé entre autres par le registre, la présence d'un circonstant après l'anaphore, la mention ou non de l'antécédent avant la phrase antécédent, et dans une moindre mesure, la saillance de l'antécédent et la familiarité supposée qu'en a le destinataire. Do it renvoie en général a des actions très saillantes qui sont ensuite décrites plus en détail par le biais d'un circonstant. Do that est employé le plus souvent sans circonstant, et son usage présente parfois de grandes similarités avec l'ellipse.
L'apprentissage – principe vital de l'évolution – assure la transformation des données primaires captées par nos sens en connaissances utiles ou idées abstraites et générales, exploitables dans de nouvelles situations et contextes. Les neurosciences cognitives montrent que les mécanismes de l'apprentissage reposent sur l'engagement cognitif (e.g. se questionner, évaluer ses erreurs), physique (e.g. manipuler, bouger) et social (e.g. débattre, collaborer). L'apprenant construit ses connaissances par l'expérience, en explorant son environnement, en formulant des hypothèses et en expérimentant. Apprendre est crucial dans un contexte où l'évolution exponentielle des technologies de l'information et de la communication change les objets, les pratiques et les usages. Le développement de l'Internet des Objets (IdO) transforme les objets physiques du quotidien (e.g. ampoule, montre, voiture) en objets connectés (OC) pouvant collecter des données et agir sur l'environnement de l'usager. L'apprentissage devient aussi bien biologique qu'artificiel et permet de créer des systèmes d'Intelligence artificielle (SIA) analysant de grands volumes de données pour automatiser des tâches et assister les individus. Les technologies peuvent favoriser l'apprentissage, lorsque les possibilités techniques qu'elles offrent sont utilisées pour soutenir le processus de construction de connaissances. Ainsi, cette thèse porte sur l'apprentissage dans le contexte de l'IdO et examine la manière dont les spécificités des OC peuvent s'articuler avec les mécanismes de l'apprentissage. Afin d'identifier les caractéristiques de l'apprentissage dans le contexte de l'IdO, nous avons étudié les usages existants d'OC. En s'appuyant sur l'état de l'art, nous avons proposé un outil conceptuel décrivant l'IdO au travers de quatre dimensions d'analyse : Données, Interfaces, Agents et Pervasivité. Cet outil nous a permis d'identifier, de répertorier, de classer et, in fine, d'analyser les usages d'OC au service de l'apprentissage. Dans le cadre ces usages, l'apprentissage est caractérisé par l'engagement physique, la contextualisation des savoirs et le rapprochement des activités pédagogiques avec la réalité. En valorisant les résultats de ce premier travail, nous avons élaboré une approche pour mettre les spécificités des OC au service de l'apprentissage des sciences. L'aspect abstrait et souvent contre-intuitif des savoirs scientifiques freine leur apprentissage, en partie car notre perception de la réalité est subjective et limitée par nos sens. Or, les données collectées par les OC et analysées par des SIA apportent des informations sur l'environnement pouvant être utilisées pour étendre la perception humaine. Ainsi, l'objectif de notre approche, traduite par le modèle Données-Représentations-Interactions (DRI), vise à exploiter les OC et les SIA pour faciliter l'observation de phénomènes physiques. Selon le modèle DRI, l'apprenant interagit avec des représentations d'un phénomène physique générées à partir d'OC et de SIA. En accord avec les mécanismes de l'apprentissage (e.g. constructivisme, rôle de l'expérience), l'apprenant est amené à faire des observations et des manipulations, à formuler des hypothèses et à les tester. Afin d'évaluer les effets et les contraintes du modèle DRI, nous avons conçu les dispositifs LumIoT dédiés à l'apprentissage des grandeurs photométriques (e.g. flux lumineux, intensité lumineuse, éclairement). Puis, nous avons conduit une expérimentation avec 17 étudiants du Master 1 Produits et Services Multimédia de l'Université de Franche-Comté (site de Montbéliard). Les résultats de l'expérimentation montrent que les dispositifs LumIoT, basés sur le modèle DRI, ont facilité l'observation et la compréhension des grandeurs photométriques. En rendant accessibles des savoirs abstraits, le modèle DRI ouvre la voie à des dispositifs d'apprentissage mettant les OC et les SIA au service de la médiation des savoirs.
Cette thèse fait suite à une étude récente, menée par quelques chercheurs de l'Université de Montpellier, dans le but de proposer à la communauté scientifique une procédure d'inversion capable d'estimer de manière non invasive la pression dans les artères cérébrales d'un patient. Son premier objectif est, d'une part, d'examiner la précision et la robustesse de la procédure d'inversion proposée par ces chercheurs, en lien avec diverses sources d'incertitude liées aux modèles utilisés, aux hypothèses formulées et aux données cliniques du patient, et d'autre part, de fixer un critère d'arrêt pour l'algorithme basé sur le filtre de Kalman d'ensemble utilisé dans leur procédure d'inversion. À cet effet, une analyse d'incertitude et plusieurs analyses de sensibilité sont effectuées. Le second objectif est d'illustrer comment l'apprentissage machine, orienté réseaux de neurones convolutifs, peut être une très bonne alternative à la longue et coûteuse procédure mise en place par ces chercheurs pour l'estimation de la pression. Une approche prenant en compte les incertitudes liées au traitement des images médicales du patient et aux hypothèses formulées sur les modèles utilisés, telles que les hypothèses liées aux conditions limites, aux paramètres physiques et physiologiques, est d'abord présentée pour quantifier les incertitudes sur les résultats de la procédure. Les incertitudes liées à la segmentation des images sont modélisées à l'aide d'une distribution gaussienne et celles liées au choix des hypothèses de modélisation sont analysées en testant plusieurs scénarios de choix d'hypothèses possibles. De cette démarche, il ressort que les incertitudes sur les résultats de la procédure sont du même ordre de grandeur que celles liées aux erreurs de segmentation. Par ailleurs, cette analyse montre que les résultats de la procédure sont très sensibles aux hypothèses faites sur les conditions aux limites du modèle du flux sanguin. En particulier, le choix des conditions limites symétriques de Windkessel pour le modèle s'avère être le plus approprié pour le cas du patient étudié. Ensuite, une démarche permettant de classer les paramètres estimés à l'aide de la procédure par ordre d'importance et de fixer un critère d'arrêt pour l'algorithme utilisé dans cette procédure est proposée. Les résultats de cette stratégie montrent, d'une part, que la plupart des résistances proximales sont les paramètres les plus importants du modèle pour l'estimation du débit sanguin dans les carotides internes et, d'autre part, que l'algorithme d'inversion peut être arrêté dès qu'un certain seuil de convergence raisonnable de ces paramètres les plus influents est atteint. Enfin, une nouvelle plateforme numérique basée sur l'apprentissage machine permettant d'estimer la pression artérielle spécifique au patient dans les artères cérébrales beaucoup plus rapidement qu'avec la procédure d'inversion mais avec la même précision, est présentée. L'application de cette plateforme aux données du patient utilisées dans la procédure d'inversion permet une estimation non invasive et en temps réel de la pression dans les artères cérébrales du patient cohérente avec l'estimation de la procédure d'inversion.
Contexte : Un des enjeux majeurs du traitement de pathologies neuro-psychatriques est le suivi des patients chroniques afin de mesurer les rechutes précoces mais également l'observance et l'adhérence des patients au traitement. Un tel suivi est possible grâce à l'utilisation de dispositifs médicaux connectés (mesurant par exemple le poids, la tension artérielle ou l'activité physique) mais des informations cruciales sur le ressenti de la maladie sont complexes à mesurer et nécessitent des entretiens verbaux réguliers entre le médecin et le malade. L'augmentation du nombre de patients à suivre engendre des files d'attente résultant souvent en des suivis épisodiques avec des consultations trop espacées. Outre les entretiens cliniques, il est toutefois également possible de mesurer certains symptômes (ex : tristesse ou somnolence) avec différentes méthodes : en examinant les mouvements oculaires, avec des mesures électro-encéphalographiques, ou en observant les expressions verbales et/ou corporelles. Grâce aux récentes avancées dans le domaine du traitement automatique de la parole, il est désormais possible de détecter des indices précis dans la voix permettant la caractérisation de l'état des patients (par exemple pour mesurer le degré de somnolence). Cette méthode possède les avantages suivants : l'enregistrement de données vocales n'est pas invasif car il ne nécessite pas l'équipement de capteurs spécifiques, ni de calibration complexe. Elle peut donc être mise en place dans des environnements variés, hors conditions de laboratoire et autorise ainsi un suivi plus régulier et moins contraignant des patients. Nous souhaitons dans ce projet mettre en place une solution utilisant l'analyse automatique de la parole qui, couplée à des algorithmes d'intelligence artificielle, permettra de définir de nouveaux biomarqueurs utilisables pour un suivi personnalisé à domicile de la qualité de vie des patients. Objectifs : Depuis 2016, nous avons envisagé la possibilité d'utiliser l'analyse vocale pour le suivi de patients souffrant de somnolence excessive. Cette étude, effectuée en collaboration entre le laboratoire SANPSY USR 6231 et le LaBRI UMR 5800, est actuellement soutenue par la Région Nouvelle Aquitaine dans le cadre du projet IS-OSA. Nous avons ainsi pu mettre en place un protocole d'enregistrement nous permettant d'être à ce jour les seules équipes disposant d'une base de données audio en français ainsi que des diagnostics cliniques associés à chaque patient. Lors cette étude préliminaire, nous avons mis au point une méthode d'analyse utilisant un nombre réduit de paramètres interprétables tout en conservant un niveau de performance élevé. Nous projetons la publication très prochaine de ces recherches dans une revue internationale. Ces résultats prometteurs ouvrent la voie à des améliorations du système nécessaires pour lever les verrous scientifiques du projet :  1. Etudier la cohérence des différentes mesures cliniques de somnolence et leur lien éventuel avec différents paramètres vocaux. 2. Optimiser la recherche des paramètres vocaux pertinents et les lier avec des symptômes médicaux 3. Augmenter la base de données afin de permettre la mise au point d'un système de classification performant utilisant l'intelligence artificielle via des réseaux de neurones profonds. 4. Intégrer le système au compagnon virtuel mis au point par SANPSY et le tester en conditions réelles au domicile des patients.
La méthode textométrique, en revanche, met en œuvre un ensemble de modèles statistiques permettant l'analyse des distributions de mots dans de vastes corpus, afin faire émerger les caractéristiques significatives des données textuelles. Dans cette recherche, la textométrie, traditionnellement considérée comme étant incompatible avec la fouille par l'extraction, est substituée à cette dernière pour obtenir des informations sur des événements économiques dans le discours. Plusieurs analyses textométriques (spécificités et cooccurrences) sont donc menées sur un corpus de flux de presse numérisé. On constate que chacune des approches contribuent différemment au traitement des données textuelles, produisant toutes deux des analyses complémentaires. À l'issue de la comparaison est exposé l'apport des deux méthodes de fouille pour la veille d'événements.
La découverte d'unités linguistiques élémentaires (phonèmes, mots) uniquement à partir d'enregistrements sonores est un problème non-résolu qui suscite un fort intérêt de la communauté du traitement automatique de la parole, comme en témoignent les nombreuses contributions récentes de l'état de l'art. Durant cette thèse, nous nous sommes concentrés sur l'utilisation de réseaux de neurones pour répondre au problème. Nous avons approché le problème en utilisant les réseaux de neurones de manière supervisée, faiblement supervisée et multilingue. Nous avons ainsi développé des outils de segmentation automatique en phonèmes et de classification phonétique fondés sur des réseaux de neurones convolutifs. L'outil de segmentation automatique a obtenu 79% de F-mesure sur le corpus de parole conversationnelle en anglais BUCKEYE. Ce résultat est similaire à un annotateur humain d'après l'accord inter-annotateurs fourni par les créateurs du corpus. De plus, il n'a pas besoin de beaucoup de données (environ une dizaine de minutes par locuteur et 5 locuteurs différents) pour être performant. De plus, il est portable à d'autres langues (notamment pour des langues peu dotées telle que le xitsonga). Le système de classification phonétique permet de fixer les différents paramètres et hyperparamètres utiles pour un scénario non supervisé. Dans le cadre non supervisé, les réseaux de neurones (Auto-Encodeurs) nous ont permis de générer de nouvelles représentations paramétriques, concentrant l'information de la trame d'entrée et ses trames voisines. Nous avons étudié leur utilité pour la compression audio à partir du signal brut, pour laquelle ils se sont montrés efficaces (faible taux de RMS, même avec une compression de 99%). Nous avons également réalisé une pré-étude novatrice sur une utilisation différente des réseaux de neurones, pour générer des vecteurs de paramètres non pas à partir des sorties des couches mais des valeurs des poids des couches. Ces paramètres visent à imiter les coefficients de prédiction linéaire (Linear Predictive Coefficients, LPC). Dans le contexte de la découverte non supervisée d'unités similaires à des phonèmes (dénommées pseudo-phones dans ce mémoire) et la génération de nouvelles représentations paramétriques phonétiquement discriminantes, nous avons couplé un réseau de neurones avec un outil de regroupement (k-means). L'alternance itérative de ces deux outils a permis la génération de paramètres phonétiquement discriminants pour un même locuteur : de faibles taux d'erreur ABx intra-locuteur de 7,3% pour l'anglais, 8,5% pour le français et 8,4% pour le mandarin ont été obtenus. Ces résultats permettent un gain absolu d'environ 4% par rapport à la baseline (paramètres classiques MFCC) et sont proches des meilleures approches actuelles (1% de plus que le vainqueur du Zero Ressource Speech Challenge 2017). Les résultats inter-locuteurs varient entre 12% et 15% suivant la langue, contre 21% à 25% pour les MFCC.
Les travaux présentés dans cette thèse concernent la modélisation des aspects socioculturels et temporels pour permettre aux communautés sénégalaises departager et de co-construire leur connaissances socioculturelles. En effet, avecla mondialisation la nouvelle génération sénégalaise a de moins en moins de connaissances sur son environnement socioculturel. Ainsi, nous avons initié la mise en place d'une application en ligne pour permettre à nos concitoyens de partager et de co-construire leur patrimoine socioculturel. Nos propositions s'appuient sur les technologies du Web social et du Web sémantique. En effet, le Web social propose un cadre à tout utilisateur pour participer à la création de contenu. Le Web sémantique rend accessible les ressources aux agents logiciels pour une meilleure recherche et partage d'informations. La combinaison de ces deux technologies permet aux communautés sénégalaises de partager et de co-construire leur patrimoine culturel dans un cadre collaboratif et sémantique. Nos contributions consistent à (i) proposer des ontologies pour annoter des ressources socioculturelles et (ii) proposer un cadre collaboratif aux communautés sénégalaises. Les ontologies représentent le socle du Web sémantique et permettent de caractériser un domaine. Ainsi, nous avons défini : 1) une ontologie socioculturelle reposant sur la théorie historicoculturelle de l'activité et 2) une ontologie temporelle. Nous avons également défini les communautés de co-élaboration de connaissances culturelles et proposé un prototype qui intègre les différentes contributions.
Les corpus, collections de textes sélectionnés dans un objectif spécifique, occupent une place de plus en plus déterminante en Linguistique comme en Traitement Automatique des Langues (TAL). Considérés à la fois comme source de connaissances sur l'usage authentique des langues, ou sur les entités que désignent des expressions linguistiques, ils sont notamment employés pour évaluer la performance d'applications de TAL. Les critères qui prévalent à leur constitution ont un impact évident, mais encore délicat à caractériser, sur (i) les structures linguistiques majeures qu'ils renferment, (ii) les connaissances qui y sont véhiculées, et, (iii) la capacité de systèmes informatiques à accomplir une tâche donnée. Ce mémoire étudie des méthodologies d'extraction automatique de relations sémantiques dans des corpus de textes écrits. Un tel sujet invite à examiner en détail le contexte dans lequel une expression linguistique s'applique, à identifier les informations qui déterminent son sens, afin d'espérer relier des unités sémantiques. Généralement, la modélisation du contexte est établie à partir de l'analyse de co-occurrence d'informations linguistiques issues de ressources ou obtenues par des systèmes de TAL. Les intérêts et limites de ces informations sont évalués dans le cadre de la tâche d'extraction de relations sur des corpus de genre différent (article de presse, conte, biographie). Les résultats obtenus permettent d'observer que pour atteindre une représentation sémantique satisfaisante ainsi que pour concevoir des systèmes robustes, ces informations ne suffisent pas. Deux problèmes sont particulièrement étudiés. D'une part, il semble indispensable d'ajouter des informations qui concernent le genre du texte. Pour caractériser l'impact du genre sur les relations sémantiques, une méthode de classification automatique, reposant sur les restrictions sémantiques qui s'exercent dans le cadre de relations verbo-nominales, est proposée. La méthode est expérimentée sur un corpus de conte et un corpus de presse. D'autre part, la modélisation du contexte pose des problèmes qui relèvent de la variation discursive de surface. Un texte ne met pas toujours bout à bout des expressions linguistiques en relation et il est parfois nécessaire de recourir à des algorithmes complexes pour détecter des relations à longue portée. Pour répondre à ce problème de façon cohérente, une méthode de segmentation discursive, qui s'appuie sur des indices de structuration de surface apparaissant dans des corpus écrits, est proposée. Elle ouvre le champ à la conception de grammaires qui permettent de raisonner sur des catégories d'ordre macro-syntaxique afin de structurer la représentation discursive d'une phrase. Cette méthode est appliquée en amont d'une analyse syntaxique et l'amélioration des performances est évaluée. Les solutions proposées à ces deux problèmes nous permettent d'aborder l'extraction d'information sous un angle particulier : le système implémenté est évalué sur une tâche de correction d'Entités Nommées dans le contexte d'application des Systèmes de Question-Réponse. Ce besoin spécifique entraîne l'alignement de la définition d'une catégorie sur le type de réponse attendue par une question.
Dans le cadre d'une montée en version logicielle, d'une migration technique ou encore de l'implémentation de connecteurs afin d'assurer une communication avec des services tiers, il est crucial de pouvoir conserver l'ensemble des données clients d'une application, d'assurer leur cohérence et de les communiquer selon des structures (ou méta-modèles) hétérogènes non connus à l'avance. Dans un tel contexte, il est donc important de savoir efficacement gérer la 'conversion'des données d'un méta-modèle à un autre. Cette thèse s'intéresse à l'implémentation d'une transformation de modèle automatisée afin d'assurer une interopérabilité fédérée : il s'agit d'étudier l'inférence à la volée de règles permettant de convertir des données structurées selon un méta-modèle source afin de les adapter à un méta-modèle cible. Pour cela, la thèse vise à étudier les techniques issues de plusieurs disciplines telles que la sémantique, l'apprentissage automatique et le traitement du langage naturel ou encore l'exploitation de base de données de type graphe via des algorithmes d'optimisation par exemple. Cette thèse est financée par la société Forterro Sylob qui amènera en outre des cas d'étude réels.
Ce document rassemble les travaux effectués durant mes années de thèse. Je commence par une présentation concise des résultats principaux, puis viennent trois parties relativement indépendantes. Dans la première partie, je considère des problèmes d'inférence statistique sur un échantillon i.i.d. issu d'une loi inconnue à support dénombrable. Le premier chapitre est consacré aux propriétés de concentration du profil de l'échantillon et de la masse manquante. Il s'agit d'un travail commun avec Stéphane Boucheron et Mesrob Ohannessian. Après avoir obtenu des bornes sur les variances, nous établissons des inégalités de concentration de type Bernstein, et exhibons un vaste domaine de lois pour lesquelles le facteur de variance dans ces inégalités est tendu. Le deuxième chapitre présente un travail en cours avec Stéphane Boucheron et Elisabeth Gassiat, concernant le problème de la compression universelle adaptative d'un tel échantillon. Nous établissons des bornes sur la redondance minimax des classes enveloppes, et construisons un code quasi-adaptatif sur la collection des classes définies par une enveloppe à variation régulière. Dans la deuxième partie, je m'intéresse à des marches aléatoires sur des graphes aléatoires à degrés precrits. Je présente d'abord un résultat obtenu avec Justin Salez, établissant le phénomène de cutoff pour la marche sans rebroussement. Sous certaines hypothèses sur les degrés, nous déterminons précisément le temps de mélange, la fenêtre du cutoff, et montrons que le profil de la distance à l'équilibre converge vers la fonction de queue gaussienne. Puis je m'intéresse à la comparaison des temps de mélange de la marche simple et de la marche sans rebroussement. Enfin, la troisième partie est consacrée aux propriétés de concentration de tirages pondérés sans remise et correspond à un travail commun avec Yuval Peres et Justin Salez.
Pour prévoir la production, les systèmes de surveillance de la sécurité alimentaire doivent être renseignés par des données sur les surfaces cultivées et sur le rendement. Ces données peuvent être estimées par les systèmes d'observations satellitaires à moyenne résolution spatiale, qui, par leur vision synoptique, constituent une source d'information particulièrement adéquate. En Afrique de l'Ouest, l'estimation des surfaces cultivées par télédétection reste cependant problématique en raison d'un domaine cultivé fragmenté, d'une grande hétérogénéité spatiale due aux conditions environnementales et aux pratiques culturales, et de la synchronisation des phénologies des agrosystèmes et des écosystèmes liée au régime des précipitations. Dans ce contexte, cette thèse présente, en trois volets, des développements méthodologiques originaux pour la caractérisation des systèmes agricoles d'Afrique de l'Ouest par télédétection. Les méthodes ont été développées à partir de séries temporelles MODIS (250 m à 500 m de résolutionspatiale) acquises sur le Mali. (i) La cartographie des surfaces cultivées a été réalisée à partir d'indices spectraux, spatiaux, texturaux et temporels dérivés des images. Deux approches ont été appliquées : une approche de type ISODATA consécutive à une segmentation du territoire basée sur les images MODIS et une approche de fouille de données basée sur des « motifs séquentiels » . Les produits cartographiques obtenus présentent une meilleure précision que les produits globaux « occupation du sol » existants (70% vs 50% en moyenne). Cependant, une part importante des erreurs d'omission et de commission (de 20% à 40%) reste incompressible en raison de la fragmentation du domaine cultivé. (ii) La cartographie des types de systèmes agricoles a nécessité un premier travail de typologie effectué à partir d'une BD d'enquêtes de terrain de l'Institut d'Economie Rurale de Bamako sur 100 villages. Trois types de systèmes agricoles ont été déterminés à l'échelle du village : céréales dominantes (mil, sorgho), cultures intensives dominantes (maïs, coton) et mélange de sorgho et de coton. La classification des systèmes agricoles à partir des indicateurs de télédétection précédemment cités a été produite par un algorithme de type Random Forest avec une précision globale de 60%. Les résultats mettent en évidence une combinaison optimale d'indicateurs comprenant le NDVI ainsi que la texture pour la caractérisation des systèmes agricoles. (iii) Enfin, pour le suivi des cultures, le produit phénologique MODIS a été testé et évalué à partir de variables phénologiques obtenues par simulations agro-météorologiques du modèle de plante SARRA-H. Les résultats montrent que ce produit comporte des incohérences dues au fort ennuagement de début de saison des pluies. Après suppression des données aberrantes, on montre que les dates de transition phénologique des surfaces cultivées issues de MODIS sont plus précoces de 20 jours comparées aux sorties du modèle de culture, en raison notamment de la nature mixte « agro-écosystème » des surfaces à l'échelle du pixel MODIS. Les résultats de cette thèse permettent de dégager de nouvelles pistes de couplage entre télédétection, données de terrain et modélisation agro-météorologique en apportant une information continue dans le temps et dans l'espace sur la caractérisation du domaine cultivé au « Sahel » .
Si la parole est une faculté dont l'usage nous semble parfaitement naturel,il reste toutefois beaucoup à comprendre sur la nature des représentations et des processus cognitifs qui la gouvernent. Au cœur de cette thèse se trouve la question des interactions entre perception et action dans la production et la perception de syllabes. Nous adoptons le cadre rigoureux de la programmation bayésienne au sein duquel nous définissons mathématiquement le modèle COSMO (pour "Communicating Objects using Sensori-Motor Operations"), qui permet de formaliser les théories motrice, auditive et perceptuo-motrice de la communication parlée et de les étudier quantitativement. Cette approche conduit à un premier résultat théorique fort : nous démontrons un théorème d'indistinguabilité d'après lequel, lorsque l'on pose certaines hypothèses de conditions idéales d'apprentissage, les théories auditive et motrice font des prédictions identiques pour des tâches de perception, et sont de ce fait indistinguables. Cet algorithme d'apprentissage par imitation de ciblesacoustiques permet l'apprentissage de compétences motrices à partir d'entrées perceptives uniquement, avec la propriété remarquable de se focaliser sur les régions d'intérêt pour l'apprentissage. Nous utilisons des syllabes synthétisées grâce au modèle de conduit vocal VLAM pour analyser les dynamiques d'évolution des modèles appris ainsi que leur robustesse aux dégradations.
Les données métagénomiques du microbiome humain constituent une nouvelle source de données pour améliorer le diagnostic et le pronostic des maladies humaines. L'apprentissage automatique a obtenu de grandes réalisations sur d'importants problèmes de métagénomique liés au regroupement d'OTU, à l'assignation taxonomique, etc. La contribution de cette thèse est multiple : 1) un cadre de sélection de caractéristiques pour approche pour prédire les maladies à l'aide de représentations d'images artificielles. La première contribution, qui est une approche efficace de sélection de caractéristiques basée sur les capacités de visualisation de la carte auto-organisée, montre une précision de classification raisonnable par rapport aux méthodes de pointe. La seconde approche vise à visualiser les données métagénomiques en utilisant une méthode simple de remplissage, ainsi que des approches d'apprentissage de réduction dimensionnelle. La nouvelle représentation des données métagénomiques peut être considérée comme une image synthétique et utilisée comme un nouvel ensemble de données pour une méthode efficace d'apprentissage en profondeur. Les résultats montrent que les méthodes proposées permettent d'atteindre des performances prédictives à la pointe de la technologie ou de les surpasser sur des benchmarks métagénomiques riches en public.
Les sites web de critiques en ligne aident les utilisateurs à décider quoi acheter ou quels hôtels choisir. Ces plateformes permettent aux utilisateurs d'exprimer leurs opinions à l'aide d'évaluations numériques et de commentaires textuels. Les notes numériques donnent une idée approximative du service. D'autre part, les commentaires textuels donnent des détails complets, ce qui est fastidieux à lire. Dans cette thèse, nous développons de nouvelles méthodes et algorithmes pour générer des résumés personnalisés de critiques de films, basés sur les aspects, pour un utilisateur donné. Le premier problème que nous abordons consiste à extraire un ensemble de mots liés à un aspect des critiques de films. Notre évaluation montre que notre méthode est capable d'extraire même des termes impopulaires qui représentent un aspect, tels que des termes composés ou des abréviations. Nous étudions ensuite le problème de l'annotation des phrases avec des aspects et proposons une nouvelle méthode qui annote les phrases en se basant sur une similitude entre la signature d'aspect et les termes de la phrase. Le troisième problème que nous abordons est la génération de résumés personnalisés, basés sur les aspects. Nous proposons un algorithme d'optimisation pour maximiser la couverture des aspects qui intéressent l'utilisateur et la représentativité des phrases dans le résumé sous réserve de contraintes de longueur et de similarité. Enfin, nous réalisons trois études d'utilisateur qui montrent que l'approche que nous proposons est plus performante que la méthode de pointe en matière de génération de résumés.
Le chapitre 1 sert d'introduction à la thèse, pose les problématiques et les méthodes, remet en perspective les enjeux et annonce le plan suivi. Le chapitre 2 définit les principaux types de disfluences (cliniques et naturelles), résume les études principales conduites sur les disfluences, et présente les différents points de vue sur leur rôle dans le discours. Le chapitre 3 dresse l'état de la question sur le statut des deux pauses pleines (fillers) um et uh et montre comment plusieurs études récentes accréditent l'idée d'une différence pragmatique, voire fonctionnelle, entre ces deux "fillers", qu'il convient donc d'envisager comme des marqueurs. Le chapitre 5 caractérise les deux corpus étudiés, ATAROS et Switchboard (SWB), et établit leurs contributions. Ce chapitre présente les méthodologies d'annotation des corpus, les deux versions de SWB, ainsi que la méthode suivie pour construire une interopérabilité de ces deux corpus pour l'analyse de um et uh. Le chapitre 6 analyse la distribution et la durée des deux marqueurs dans SWB et ATAROS en fonction du genre des interlocuteurs, de l'authenticité de la conversation, et du nombre de conversations auxquelles les sujets participent. Ce chapitre montre que um et uh ont des durées et des distributions différentes et indique que les marqueurs ne sont pas utilisés au hasard. Le chapitre 7 se penche sur la production de um et uh dans SWB, et sur la perception des deux marqueurs en comparant les deux versions des transcriptions du corpus. Les principaux résultats montrent que um et uh sont plus souvent oublis que d'autres mots fréquents tels que les mots fonctionnels, et que les transcripteurs de SWB font plus d'erreurs sur uh que sur um, suggérant que um joue un rôle discursif plus important que uh. Le chapitre 8 interroge la relation entre la prise de position ("stance") d'une unité de parole et la présence et la position des marqueurs dans une phrase, et révèle que ces deux dimensions sont dépendantes. Le chapitre 9 évalue la relation entre la prise de position d'une unité de parole et la réalisation acoustique de la voyelle des marqueurs, comparé à la même voyelle dans d'autres mots monosyllabiques. Les résultats indiquent que les valeurs de "stance" affectent avec différents degrés la réalisation acoustique des marqueurs. Le chapitre 10 incorpore les résultats des expériences précédentes dans plusieurs taches de classification qui testent les traits les plus importants pour prédire automatiquement les valeurs de "stance" en fonction des paramètres correspondants à um et uh (traits lexicaux, positionnels et acoustiques). Les résultats aussi indiquent que différentes propriétés acoustiques améliorent les scores de prédictions. Le chapitre 11 conclut la thèse en résumant les résultats des chapitres 6 à 10, en soulignant les impacts de cette recherche, et en indiquant les futures pistes de recherche.
Dans cette thèse, nous collectons sur le web deux types de connaissances. Le premier porte sur le sens commun, i.e. des connaissances intuitives partagées par la plupart des gens comme " le ciel est bleu ". Nous utilisons des logs de requêtes et des forums de questions-réponses pour extraire des faits essentiels grâce à des questions avec une forme particulière. Ensuite, nous validons nos affirmations grâce à d'autres ressources comme Wikipedia, Google Books ou les tags d'images sur Flickr. Finalement, nous groupons tous les signaux pour donner un score à chaque fait. Nous obtenons une base de connaissance, QUASIMODO, qui, comparée à ses concurrents, montre une plus grande précision et collecte plus de faits essentiels. Le deuxième type de connaissances qui nous intéresse sont les connaissances cachées, i.e. qui ne sont pas directement données par un fournisseur de données. En effet, les services web donnent généralement un accès partiel à l'information. Il faut donc combiner des méthodes d'accès pour obtenir plus de connaissances : c'est de la réécriture de requête. Dans un premier scénario, nous étudions le cas où les fonctions ont la forme d'un chemin, la base de donnée est contrainte par des " dépendences d'inclusion unitaires " et les requêtes sont atomiques. Nous montrons que le problème est alors décidable en temps polynomial. Ensuite, nous retirons toutes les contraites et nous créons un nouvelle catégorie pertinente de plans : les " smart plans ". Nous montrons qu'il est décidable de les trouver.
Cette thèse est consacrée à l'étude d'accents régionaux en français, à partir de deux grands corpus de parole face à face (PFC) et de parole téléphonique. Nous avons étudié la perception humaine et les caractéristiques acoustiques de différentes variétés de français (français standard, français du sud de la France, d'Alsace, de Belgique et de Suisse) afin de les modéliser dans leurs aspects segmentaux (articulation des phonèmes) et prosodiques (accentuation et intonation). Dans un premier temps, des tests perceptifs ont permis d'évaluer quels accents sont distingués par des auditeurs français. Les résultats ont été analysés par des techniques de clustering et de scaling. Dans un second temps, nous avons mesuré des paramètres acoustiques (formants, fréquence fondamentale, durée et intensité) en nous appuyant sur les frontières temporelles des segments phonémiques fournies par un système d'alignement standard, ce qui nous a également permis de dégager certains patrons prosodiques spécifiques. Nous avons en outre introduit des variantes dans le dictionnaire de prononciation utilisé pour l'alignement, afin d'observer les variantes choisies par le système. Ces deux méthodes ont permis de mettre en évidence un certain nombre de traits pertinents concernant la réalisation des voyelles nasales, l'antériorisation du /O/, le dévoisement des consonnes sonores et l'accentuation initiale.
Les réseaux de neurones profonds récents possèdent de nombreuses couches cachées ce qui augmente significativement le nombre total de paramètres. L'apprentissage de ce genre de modèles nécessite donc un grand nombre d'exemples étiquetés, qui ne sont pas toujours disponibles en pratique. Le sur-apprentissage est un des problèmes fondamentaux des réseaux de neurones, qui se produit lorsque le modèle apprend par coeur les données d'apprentissage, menant à des difficultés à généraliser sur de nouvelles données. Le problème du sur-apprentissage des réseaux de neurones est le thème principal abordé dans cette thèse. Dans la littérature, plusieurs solutions ont été proposées pour remédier à ce problème, tels que l'augmentation de données, l'arrêt prématuré de l'apprentissage ("early stopping"), ou encore des techniques plus spécifiques aux réseaux de neurones comme le "dropout" ou la "batch normalization". Dans cette thèse, nous abordons le sur-apprentissage des réseaux de neurones profonds sous l'angle de l'apprentissage de représentations, en considérant l'apprentissage avec peu de données. Pour aboutir à cet objectif, nous avons proposé trois différentes contributions. La première contribution, présentée dans le chapitre 2, concerne les problèmes à sorties structurées dans lesquels les variables de sortie sont à grande dimension et sont généralement liées par des relations structurelles. Notre proposition vise à exploiter ces relations structurelles en les apprenant de manière non-supervisée avec des autoencodeurs. Notre approche a montré une accélération de l'apprentissage des réseaux et une amélioration de leur généralisation. La deuxième contribution, présentée dans le chapitre 3, exploite la connaissance a priori sur les représentations à l'intérieur des couches cachées dans le cadre d'une tâche de classification. Cet à priori est basé sur la simple idée que les exemples d'une même classe doivent avoir la même représentation interne. Nous avons formalisé cet à priori sous la forme d'une pénalité que nous avons rajoutée à la fonction de perte. Des expérimentations empiriques sur la base MNIST et ses variantes ont montré des améliorations dans la généralisation des réseaux de neurones, particulièrement dans le cas où peu de données d'apprentissage sont utilisées. Notre troisième et dernière contribution, présentée dans le chapitre 4, montre l'intérêt du transfert d'apprentissage ("transfer learning") dans des applications dans lesquelles peu de données d'apprentissage sont disponibles. Dans cette application, la tâche consiste à localiser la troisième vertèbre lombaire dans un examen de type scanner. L'utilisation du transfert d'apprentissage ainsi que de prétraitements et de post traitements adaptés a permis d'obtenir des bons résultats, autorisant la mise en oeuvre du modèle en routine clinique.
Les systèmes de recommandation sont conçus dans une variété d'applications pour aider à la prise de décision. Dans un environnement collaboratif, le système de recommandation peut guider la collaboration. Les utilisateurs laissent des traces d'interaction lorsqu'ils collaborent sur une plateforme numérique. Ces traces peuvent être analysées pour détecter les signaux forts et les signaux faibles d'une collaboration. Cette thèse porte sur la mise en œuvre d'un système de recommandation exploitant les traces de collaboration dans un environnement informatique. Les travaux réalisés ont été testés au sein de la plateforme web collaborative E-MEMORAe.
Ce travail vise à présenter des méthodes de construction des dictionnaires électroniques de séquences nominales figées du coréen et de leurs formes fléchies, et à justifier leur validité en appliquant notre dictionnaire dans les domaines appliqués de l'analyse automatique de textes coréens. En vue de la reconnaissance des séquences nominales figées par dictionnaire, nous avons classé celles-ci en trois catégories selon les conventions typographiques : noms compacts (NC), noms figés à espacement facultatif (NFF) et noms figés à espacement obligatoire (NFO). Puisque des formes fléchies des séquences nominales figées apparaissent dans les textes coréens, nous avons construit, d'une part, un dictionnaire électronique des NFF à 45000 entrées et d'autre part, un transducteur des séquences de postpositions nominales avec leur segmentation, et enfin fusionné ces deux ensembles de données à partir de codes flexionnels associés à chaque entrée et de la fonctionnalité de flexion d'INTEX. Notre dictionnaire construit d'après ces méthodes a les principaux avantages suivants par rapport aux systèmes préexistants : 1) Le dictionnaire des formes fléchies de NFF permet la reconnaissance automatique de toutes les variantes de NFF liées à l'espacement 2) Le dictionnaire des formes fléchies de NFF permet la segmentation des formes fléchies des NFF en un NFF et une séquence de postpositions nominales 3) Le dictionnaire des séquences de postpositions nominales sous forme de graphes permet leur segmentation en postpositions nominales 4) Le dictionnaire des NFF sert à la segmentation des séquences nominales libres soudées 5) Le dictionnaire des NFF peut être étendu en un dictionnaire bilingue pour la traduction automatique 6) Chaque entrée du dictionnaire de NFF comporte des codes utiles pour les applications dans le traitement automatique : codes indiquant un trait sémantique, le statut de nom prédicatif, le nom tête de chaque entrée, l'origine et la catégorie grammaticale
Nous avons traité dans cette étude de différents types de prédicats nominaux réciproques ou noms de réciprocité dans la perspective de décrire de manière explicite leurs propriétés syntactico-sémantiques. Il s'agit de noms qui se trouvent dans la construction de type "Il y a Npréd entre A et B" mettant en relation au moins deux éléments (humains, concrets, idées, etc.), ces derniers étant obligatoires ;  ils peuvent permuter dans la phrase sans que cette opération provoque de changement de sens. Notre description a comme objectif de contribuer à un objectif plus général qui consiste à établir une typologie sémantique d'une partie du lexique à savoir des noms prédicatifs et ce, dans le but d'un traitement automatique de la langue.
Ces dernières années ont témoigné du succès du projet Linked Open Data (LOD) et de la croissance du nombre de sources de données sémantiques disponibles sur le web. Cependant, il y a encore beaucoup de données qui ne sont pas encore mises à disposition dans le LOD telles que les données sur demande, les données de capteurs etc. Elles sont néanmoins fournies par des API des services Web. L'intégration de ces données au LOD ou dans des applications de mashups apporterait une forte valeur ajoutée. Cependant, chercher de tels services avec les outils de découverte de services existants nécessite une connaissance préalable des répertoires de services ainsi que des ontologies utilisées pour les décrire. Dans cette thèse, nous proposons de nouvelles approches et des cadres logiciels pour la recherche de services web sémantiques avec une perspective d'intégration de données. Premièrement, nous introduisons LIDSEARCH, un cadre applicatif piloté par SPARQL pour chercher des données et des services web sémantiques. Afin d'atteindre ce but, nous utilisons des techniques de traitement automatique de la langue et d'appariement de textes basées sur le deep-learning pour mieux comprendre les descriptions des services.
La thèse aborde un sujet très nouveau à fort enjeu ; les premières publications sont des prépublications qui émanent de grandes compagnies. Prédire des variables cibles structurées dans un cadre rigoureux et générique n'est pas l'option choisie actuellement par ces grands groupes. Au delà des possibles applications que nous prévoyons en biologie computationnelle, de nombreuses autres instances de problèmes de prédiction structurée peuvent aussi être résolues : prédiction de profil client /utilisateur, prédiction d'activités et de comportement, émargeant ainsi sur les thèmes de la société numérique.
Dans cette thèse nous testons l'effet de différents types d'entraînements sur l'acquisition des monophtongues /æ, ʌ, ɑː, ɪ, et iː/ de l'anglais par des apprenants francophones adultes. Des mesures comportementales sont réalisées avant et après entraînements afin d'évaluer les changements sur la perception et la production de ces voyelles. De plus, une étude en potentiels évoqués a été menée afin de mettre en évidence des corrélats neuronaux consécutifs à l'entraînement du contraste /æ-ʌ/. 1) le premier, du type High Variability Phonetic Training (HVPT) comportait des tâches classiques d'identification et de discrimination avec feedback sur la justesse des réponses. Cet entraînement était proposé à 16 participants constituant le groupe PE. En perception, les résultats montrent que l'entraînement HVPT améliore significativement les performances des apprenants en identification et en discrimination comparativement au groupe C, alors que l'amélioration du groupe PR est limitée à l'identification. Les performances en production ont été évaluées par le biais de deux méthodes :  1) une méthode « objective » basée sur l'analyse des paramètres acoustiques des voyelles. 2) une méthode « subjective » basée sur une tâche d'identification des voyelles produites réalisée par des juges natifs. L'entraînement de type HVPT semble être le plus enclin à améliorer les performances en production des apprenants. Cependant, il a été observé que l'amélioration en production était restreinte à l'intelligibilité telle qu'évaluée par des locuteurs natifs et que l'accent étranger mesuré par le biais de mesures acoustiques n'était pas significativement réduit dans les groupes expérimentaux comparés au groupe contrôle. L'étude en électroencéphalographie n'a pas permis d'observer les corrélats attendus mais nous discutons les améliorations potentielles à apporter à notre protocole.
L'apprentissage faiblement supervisé cherche à réduire au minimum l'effort humain requis pour entrainer les modèles de l'état de l'art. Toutefois, dans la pratique, les méthodes faiblement supervisées sont nettement moins efficaces que celles qui sont totalement supervisées. Plus particulièrement, dans l'apprentissage profond, où les approches de vision par ordinateur sont les plus performantes, elles restent entièrement supervisées, ce qui limite leurs utilisations dans les applications du monde réel. Cette thèse tente tout d'abord de combler le fossé entre les méthodes faiblement supervisées et entièrement supervisées en utilisant l'information de mouvement. Puis étudie le problème de la segmentation des objets en mouvement, en proposant l'une des premières méthodes basées sur l'apprentissage pour cette tâche. Le défi est de capturer de manières précises les bordures des objets et d'éviter les optimums locaux (ex : segmenter les parties les plus discriminantes). Contrairement à la plupart des approches de l'état de l'art, qui reposent sur des images statiques, nous utilisons les données vidéo avec le mouvement de l'objet comme informations importantes. Notre méthode utilise une approche de segmentation vidéo de l'état de l'art pour segmenter les objets en mouvement dans les vidéos. Les masques d'objets approximatifs produits par cette méthode sont ensuite fusionnés avec le modèle de segmentation sémantique appris dans un EM-like framework, afin d'inférer pour les trames vidéo, des labels sémantiques au niveau des pixels. Ainsi, au fur et à mesure que l'apprentissage progresse, la qualité des labels s'améliore automatiquement. Nous intégrons ensuite cette architecture à notre approche basée sur l'apprentissage pour la segmentation de la vidéo afin d'obtenir un framework d'apprentissage complet pour l'apprentissage faiblement supervisé à partir de vidéos. Dans la deuxième partie de la thèse, nous étudions la segmentation vidéo non supervisée, plus précisément comment segmenter tous les objets dans une vidéo qui se déplace indépendamment de la caméra. De nombreux défis tels qu'un grand mouvement de la caméra, des inexactitudes dans l'estimation du flux optique et la discontinuité du mouvement, complexifient la tâche de segmentation. Nous abordons le problème du mouvement de caméra en proposant une méthode basée sur l'apprentissage pour la segmentation du mouvement : un réseau de neurones convolutif qui prend le flux optique comme entrée et qui est entraîné pour segmenter les objets qui se déplacent indépendamment de la caméra. Il est ensuite étendu avec un flux d'apparence et un module de mémoire visuelle pour améliorer la continuité temporelle. Le flux d'apparence tire profit de l'information sémantique qui est complémentaire de l'information de mouvement. Le module de mémoire visuelle est un paramètre clé de notre approche : il combine les sorties des flux de mouvement et d'apparence et agréger une représentation spatio-temporelle des objets en mouvement. La segmentation finale est ensuite produite à partir de cette représentation agrégée. L'approche résultante obtient des performances de l'état de l'art sur plusieurs jeux de données de référence, surpassant la méthode d'apprentissage en profondeur et heuristique simultanée.
La conception concourante de produits matériels centrée sur l'homme est basée sur une collaboration entre le concepteur mécanicien, l'ergonome et le designer industriel. Cette collaboration souvent difficile peut être facilitée par l'utilisation d'objets intermédiaires de conception, tels que la Réalité Virtuelle (RV). Néanmoins, bien que largement utilisée dans l'industrie, la RV souffre d'un déficit d'acceptation de la part des concepteurs de produits. Dans le cadre de ces travaux, nous proposons d'utiliser la RV sous la forme d'outils immersifs d'assistance à la convergence multidisciplinaire développés selon une démarche anthropocentrée en fonction des besoins spécifiques à chaque projet de conception de produits. Afin d'optimiser les délais de développement, nous proposons une méthodologie de conception d'applications immersive dédiée : la méthodologie ASAP (As Soon As Possible). Une première série expérimentale a été conduite dans le cadre de contrats industriels d'études et de recherche afin de valider la faisabilité de la méthodologie et l'efficacité des outils développés. Une deuxième série expérimentale a été effectuée sur plus de 50 sujets dans le cadre de projets, cette fois, pédagogiques qui ont nécessité le développement de 12 applications. Elle a permis de valider quantitativement l'influence des outils immersifs sur l'efficacité perçue des phases de convergence interdisciplinaires ainsi que l'influence de l'approche proposée sur l'acceptation de la RV par les concepteurs de produits. Ces travaux de thèse présentent une première approche qui, selon nous, permettra à terme, de faire évoluer l'usage de la RV vers une intégration plus forte au sein des processus de conception de produits avec, par exemple, une plus large utilisation des applications immersives de modélisation 3D, réelles sources d'innovation.
L'annotation manuelle de corpus est devenue un enjeu fondamental pour le Traitement Automatique des Langues (TAL). En effet, les corpus annotés sont utilisés aussi bien pour créer que pour évaluer des outils de TAL. Or, le processus d'annotation manuelle est encore mal connu et les outils proposés pour supporter ce processus souvent mal utilisés, ce qui ne permet pas de garantir le niveau de qualité de ces annotations. Nous proposons dans cette thèse une vision unifiée de l'annotation manuelle de corpus pour le TAL. Ce travail est le fruit de diverses expériences de gestion et de participation à des campagnes d'annotation, mais également de collaborations avec différents chercheur(e)s. Nous proposons dans un premier temps une méthodologie globale pour la gestion de campagnes d'annotation manuelle de corpus qui repose sur deux piliers majeurs : une organisation des campagnes d'annotation qui met l'évaluation au coeur du processus et une grille d'analyse des dimensions de complexité d'une campagne d'annotation. Un second volet de notre travail a concerné les outils du gestionnaire de campagne. Nous avons pu évaluer l'influence exacte de la pré-annotation automatique sur la qualité et la rapidité de correction humaine, grâce à une série d'expériences menée sur l'annotation morpho-syntaxique de l'anglais. Nous avons également apporté des solutions pratiques concernant l'évaluation de l'annotation manuelle, en donnant au gestionnaire les moyens de sélectionner les mesures les plus appropriées. Enfin, nous avons mis au jour les processus en oeuvre et les outils nécessaires pour une campagne d'annotation et instancié ainsi la méthodologie que nous avons décrite.
La tâche de segmentation et de regroupement en locuteurs (speaker diarization) consiste à identifier "qui parle quand" dans un flux audio sans connaissance a priori du nombre de locuteurs ou de leur temps de parole respectifs. Les systèmes de segmentation et de regroupement en locuteurs sont généralement construits en combinant quatre étapes principales. Premièrement, les régions ne contenant pas de parole telles que les silences, la musique et le bruit sont supprimées par la détection d'activité vocale (VAD). Ensuite, les régions de parole sont divisées en segments homogènes en locuteur par détection des changements de locuteurs, puis regroupées en fonction de l'identité du locuteur. Enfin, les frontières des tours de parole et leurs étiquettes sont affinées avec une étape de re-segmentation. Dans cette thèse, nous proposons d'aborder ces quatre étapes avec des approches fondées sur les réseaux de neurones. Au stade du regroupement des régions de parole, nous proposons d'utiliser l'algorithme de propagation d'affinité à partir de plongements neuronaux de ces tours de parole dans l'espace vectoriel des locuteurs. Des expériences sur un jeu de données télévisées montrent que le regroupement par propagation d'affinité est plus approprié que le regroupement hiérarchique agglomératif lorsqu'il est appliqué à des plongements neuronaux de locuteurs. La segmentation basée sur les réseaux récurrents et la propagation d'affinité sont également combinées et optimisées conjointement pour former une chaîne de regroupement en locuteurs. Comparé à un système dont les modules sont optimisés indépendamment, la nouvelle chaîne de traitements apporte une amélioration significative. De plus, nous proposons d'améliorer l'estimation de la matrice de similarité par des réseaux neuronaux récurrents, puis d'appliquer un partitionnement spectral à partir de cette matrice de similarité améliorée. Le système proposé atteint des performances à l'état de l'art sur la base de données de conversation téléphonique CALLHOME. Pour mieux comprendre le comportement du système, une analyse basée sur une architecture de codeur-décodeur est proposée. Sur des exemples synthétiques, nos systèmes apportent une amélioration significative par rapport aux méthodes de regroupement traditionnelles.
La présente recherche, qui s'inscrit à la croisée de la linguistique, de la didactique des langues et du traitement automatique des langues, se focalise sur la problématique d'un côté du classement des supports textuels utilisés dans le cadre de l'apprentissage du français langue étrangère (FLE) et de l'autre côté de l'analyse des divers critères nécessaires à ce classement. Notre analyse se base principalement sur le critère lexical, déterminant dans le processus de la lecture. Ainsi, nous nous sommes interrogé, à partir d'un échantillon de supports textuels, préalablement sélectionnés dans les manuels de FLE, si une telle classification est viable. De ce fait, on a procédé à l'analyse de ces textes du point de vue de leur correspondance avec les niveaux établis par le Cadre européen commun de référence(CECR) (niveaux A1 à B2) et cela à l'aide des inventaires développés dans les Référentiels pour le français. La première partie développe une réflexion sur l'interdisciplinarité, les supports textuels, les typologies des textes et les référentiels, en essayant de voir à quel point l'approche par le lexique se justifie afin de devenir le critère principal d'une telle classification. La seconde partie nous amène à l'analyse de la correspondance des supports textuels avec les niveaux du CECR à partir du critère lexical, à l'aide d'outils informatiques, et cela dans la perspective d'étudier quelles sont les possibilités de pouvoir élaborer un corpus dynamique des supports textuels utilisable dans le cadre du FLE.
De nos jours, il existe de nombreuses applications liées à la vision et à l'audition visant à reproduire par des machines les capacités humaines. Notre intérêt pour ce sujet vient du fait que ces problèmes sont principalement modélisés par la classification de signaux temporels. En fait, nous nous sommes intéressés à deux cas distincts, la reconnaissance de la démarche humaine et la reconnaissance de signaux audio, (notamment environnementaux et musicaux). Dans le cadre de la reconnaissance de la démarche, nous avons proposé une nouvelle méthode qui apprend et sélectionne automatiquement les parties dynamiques du corps humain. Ceci permet de résoudre le problème des variations intra-classe de façon dynamique ; 
Nous proposons la prise en compte d'indices linguistiques et discursifs variés et faisant appel à des niveaux d'analyses différents. L'obsolescence étant un phénomène non linguistique, notre hypothèse est qu'il faut considérer les indices linguistiques et discursifs en termes de combinaisons. Un système d'apprentissage automatique est ensuite mis en place afin de faire émerger les configurations d'indices pertinentes dans les segments obsolescents caractérisés par les experts. Nos objectifs sont remplis : nous proposons une description fine de l'obsolescence dans notre corpus de textes encyclopédiques et ainsi qu'un prototype logiciel d'aide à la mise à jour des textes. Une double évaluation a été menée : par validation croisée sur le corpus d'apprentissage et par les experts sur un corpus de test.
Les radiologues utilisent au quotidien des solutions d'imagerie médicale pour le diagnostic. L'amélioration de l'expérience utilisateur est toujours un axe majeur de l'effort continu visant à améliorer la qualité globale et l'ergonomie des produits logiciels. Les applications de monitoring permettent en particulier d'enregistrer les actions successives effectuées par les utilisateurs dans l'interface du logiciel. Ces interactions peuvent être représentées sous forme de séquences d'actions. Sur la base de ces données, ce travail traite de deux sujets industriels : les pannes logicielles et l'ergonomie des logiciels. Ces deux thèmes impliquent d'une part la compréhension des modes d'utilisation, et d'autre part le développement d'outils de prédiction permettant soit d'anticiper les pannes, soit d'adapter dynamiquement l'interface logicielle en fonction des besoins des utilisateurs. Pour ce faire, nous proposons d'utiliser un test binomial afin de déterminer quel type de pattern est le plus approprié pour représenter les signatures de crash. L'amélioration de l'expérience utilisateur par la personnalisation et l'adaptation des systèmes aux besoins spécifiques de l'utilisateur exige une très bonne connaissance de la façon dont les utilisateurs utilisent le logiciel. Afin de mettre en évidence les tendances d'utilisation, nous proposons de regrouper les sessions similaires. Nous comparons trois types de représentation de session dans différents algorithmes de clustering. La deuxième contribution de cette thèse concerne le suivi dynamique de l'utilisation du logiciel. Les deux méthodologies tirent parti de la structure récurrente des réseaux LSTM pour capturer les dépendances entre nos données séquentielles ainsi que leur capacité à traiter potentiellement différents types de représentations d'entrée pour les mêmes données.
Ce mémoire de thèse de doctorat présente, discute et propose des outils de fouille automatique de mégadonnées dans un contexte de classification supervisée musical. L'application principale concerne la classification automatique des thèmes musicaux afin de générer des listes de lecture thématiques. Le premier chapitre introduit les différents contextes et concepts autour des mégadonnées musicales et de leur consommation. Le deuxième chapitre s'attelle à la description des bases de données musicales existantes dans le cadre d'expériences académiques d'analyse audio. Ce chapitre introduit notamment les problématiques concernant la variété et les proportions inégales des thèmes contenus dans une base, qui demeurent complexes à prendre en compte dans une classification supervisée. Le troisième chapitre explique l'importance de l'extraction et du développement de caractéristiques audio et musicales pertinentes afin de mieux décrire le contenu des éléments contenus dans ces bases de données. Ce chapitre explique plusieurs phénomènes psychoacoustiques et utilise des techniques de traitement du signal sonore afin de calculer des caractéristiques audio. De nouvelles méthodes d'agrégation de caractéristiques audio locales sont proposées afin d'améliorer la classification des morceaux. Le quatrième chapitre décrit l'utilisation des caractéristiques musicales extraites afin de trier les morceaux par thèmes et donc de permettre les recommandations musicales et la génération automatique de listes de lecture thématiques homogènes. Cette partie implique l'utilisation d'algorithmes d'apprentissage automatique afin de réaliser des tâches de classification musicale. Les contributions de ce mémoire sont résumées dans le cinquième chapitre qui propose également des perspectives de recherche dans l'apprentissage automatique et l'extraction de caractéristiques audio multi-échelles.
Notre approche linguistique a été implémentée sur machine à l'aide d'un générateur de système experts "snark".
Fouille des opinion, une sous-discipline dans la recherche d'information (IR) et la linguistique computationnelle, fait référence aux techniques de calcul pour l'extraction, la classification, la compréhension et l'évaluation des opinions exprimées par diverses sources de nouvelles en ligne, social commentaires des médias, et tout autre contenu généré par l'utilisateur. Il est également connu par de nombreux autres termes comme trouver l'opinion, la détection d'opinion, l'analyse des sentiments, la classification sentiment, de détection de polarité, etc. Définition dans le contexte plus spécifique et plus simple, fouille des opinion est la tâche de récupération des opinions contre son besoin aussi exprimé par l'utilisateur sous la forme d'une requête. Il y a de nombreux problèmes et défis liés à l'activité fouille des opinion. Dans cette thèse, nous nous concentrons sur quelques problèmes d'analyse d'opinion.
Les fournisseurs de services géo-localisés (LBS) offrent des données textuelles et spatiales complémentaires, parfois incohérentes et imprécises, représentant les différents points d'intérêt (POI) sur un territoire donné. De plus, chaque fournisseur utilise sa propre convention graphique pour représenter les POIs.
Le développement croissant des réseaux et en particulier l'Internet a considérablement développé l'écart entre les systèmes d'information hétérogènes. En faisant une analyse sur les études de l'interopérabilité des systèmes d'information hétérogènes, nous découvrons que tous les travaux dans ce domaine tendent à la résolution des problèmes de l'hétérogénéité sémantique. Le W3C (World Wide Web Consortium) propose des normes pour représenter la sémantique par l'ontologie. L'ontologie est en train de devenir un support incontournable pour l'interopérabilité des systèmes d'information et en particulier dans la sémantique. La structure de l'ontologie est une combinaison de concepts, propriétés et relations. Cette combinaison est aussi appelée un graphe sémantique. Les langages OWL (Ontology Web Language) et RDF (Resource Description Framework) sont les langages les plus importants du web sémantique, ils sont basés sur XML. Le RDF est la première norme du W3C pour l'enrichissement des ressources sur le Web avec des descriptions détaillées et il augmente la facilité de traitement automatique des ressources Web. Les descriptions peuvent être des caractéristiques des ressources, telles que l'auteur ou le contenu d'un site web. Ces descriptions sont des métadonnées. Enrichir le Web avec des métadonnées permet le développement de ce qu'on appelle le Web Sémantique. Le RDF est aussi utilisé pour représenter les graphes sémantiques correspondant à une modélisation des connaissances spécifiques. Les fichiers RDF sont généralement stockés dans une base de données relationnelle et manipulés en utilisant le langage SQL ou les langages dérivés comme SPARQL. Malheureusement, cette solution, bien adaptée pour les petits graphes RDF n'est pas bien adaptée pour les grands graphes RDF. Ces graphes évoluent rapidement et leur adaptation au changement peut faire apparaître des incohérences. Conduire l'application des changements tout en maintenant la cohérence des graphes sémantiques est une tâche cruciale et coûteuse en termes de temps et de complexité. Un processus automatisé est donc essentiel. Pour ces graphes RDF de grande taille, nous suggérons une nouvelle façon en utilisant la vérification formelle « Le Model checking » . Le Model checking est une technique de vérification qui explore tous les états possibles du système. De cette manière, on peut montrer qu'un modèle d'un système donné satisfait une propriété donnée. Cette thèse apporte une nouvelle méthode de vérification et d'interrogation de graphes sémantiques. Nous proposons une approche nommé ScaleSem qui consiste à transformer les graphes sémantiques en graphes compréhensibles par le model checker (l'outil de vérification de la méthode Model checking). Il est nécessaire d'avoir des outils logiciels permettant de réaliser la traduction d'un graphe décrit dans un formalisme vers le même graphe (ou une adaptation) décrit dans un autre formalisme
Le sujet des travaux concerne l'amélioration du comportement des machines dites \og intelligentes\fg, c'est-à-dire capables de s'adapter à leur environnement, même lorsque celui-ci évolue. Un des domaines concerné est celui des interactions homme-machine. La machine doit alors gérer différents types d'incertitude pour agir de façon appropriée. D'abord, elle doit pouvoir prendre en compte les variations de comportements entre les utilisateurs et le fait que le comportement peut varier d'une utilisation à l'autre en fonction de l'habitude à interagir avec le système. De plus, la machine doit s'adapter à l'utilisateur même si les moyens de communication entre lui et la machine sont bruités. L'objectif est alors de gérer ces incertitudes pour exhiber un comportement cohérent. Une manière habituelle pour gérer les incertitudes passe par l'introduction de modèles : modèles de l'utilisateur, de la tâche, ou encore de la décision. Un inconvénient de cette méthode réside dans le fait qu'une connaissance experte liée au domaine concerné est nécessaire à la définition des modèles. Si l'introduction d'une méthode d'apprentissage automatique, l'apprentissage par renforcement a permis d'éviter une modélisation de la décision \textit{ad hoc} au problème concerné, des connaissances expertes restent toutefois nécessaires. La thèse défendue par ces travaux est que certaines contraintes liées à l'expertise humaine peuvent être relaxées tout en limitant la perte de généricité liée à l'introduction de modèles
Cette thèse s'intéresse particulièrement aux sites de vente en ligne et à leurs réseaux sociaux. La propension des utilisateurs utiliser ces sites Web tels qu'eBay et Amazon est de plus en plus importante en raison de leur fiabilité. Les consommateurs se réfèrent à ces sites Web pour leurs besoins et en deviennent clients. L'un des défis à relever est de fournir les informations utiles pour aider les clients dans leurs achats. Ainsi, une question sous-jacente à la thèse cherche à répondre est de savoir comment fournir une information complète aux clients afin de les aider dans leurs achats. C'est important pour les sites d'achats en ligne car cela satisfait les clients par ces informations utiles. Pour ce faire, les utilisateurs sont classés en fonction de deux scores : optimiste et pessimiste. Dans la deuxième partie, une nouvelle méthodologie de propagation de l'opinion est présentée pour parvenir à un accord et maintenir la cohérence entre les utilisateurs, ce qui rend l'agrégation possible. La propagation se fait en tenant compte des impacts des utilisateurs influents et des voisins. Enfin, dans la troisième partie, l'agrégation des avis est proposée pour rassembler les avis existants et les présenter comme des informations utiles pour les clients concernant chaque produit du site de vente en ligne. Pour ce faire, l'opérateur de calcul de la moyenne pondérée et les techniques floues sont utilisées. La thèse présente un modèle d'opinion consensuelle dans les réseaux. Les travaux peuvent s'appliquer à tout groupe qui a besoin de trouver un avis parmi les avis de ses membres.
Cette thèse en didactique des langues étrangères montre le potentiel des corpus pour l'apprentissage de l'allemand au collège. Conformément aux objectifs poursuivis dans une recherche-action, elle renseigne sur l'évolution des acteurs impliqués dans un dispositif spécifique, en tenant compte de facteurs multiples, tels que le type d'input, les outils, les modes de travail - en présentiel et à distance - et la relation pédagogique. J'ai observé les effets d'un scénario pédagogique, basé sur l'exploitation de corpus spécialisés pour la création de textes du domaine du tourisme. J'ai analysé leurs discours et leurs actions face à l'input, et j'ai décrit l'évolution de certains aspects linguistiques et discursifs de leurs textes. Le projet et les outils proposés ont soutenu le développement de modes de lecture qui ont favorisé la recherche ciblée d'informations dans l'input. Le recyclage d'éléments repérés dans l'input a mené, pour une partie des apprenants, à l'obtention de productions écrites dont les caractéristiques linguistiques et discursives sont proches de celles attestées dans les corpus. L'écriture intertextuelle a permis aux participants, à des degrés variables, de se décentrer et de progresser dans l'apprentissage de l'altérité. L'observation de formes linguistiques à l'aide de lignes de concordances et l'intégration de collocations identifiées dans l'input ont contribué au développement du système linguistique, en particulier dans le domaine de la flexion des adjectifs.
La tâche de Segmentation et Regroupement en Locuteurs (SRL), telle que définie par le NIST, considère le traitement des enregistrements d'un corpus comme des problèmes indépendants. Les enregistrements sont traités séparément, et le tauxd'erreur global sur le corpus correspond finalement à une moyenne pondérée. Dans ce contexte, les locuteurs détectés par le système sont identifiés par des étiquettes anonymes propres à chaque enregistrement. Un même locuteur qui interviendrait dans plusieurs enregistrements sera donc identifié par des étiquettes différentes selon les enregistrements. Cette situation est pourtant très fréquente dans les émissions journalistiques d'information : les présentateurs, les journalistes et autres invités qui animent une émission interviennent généralement de manière récurrente. En conséquence, la tâche de SRL a depuis peu été considérée dans un contexte plus large, où les locuteurs récurrents doivent être identifiés de manière unique dans tous les enregistrements qui composent un corpus. Cette généralisation du problème de regroupement en locuteurs va de pair avec l'émergence du concept de collection, qui se réfère, dans le cadre de la SRL, à un ensemble d'enregistrements ayant une ou plusieurs caractéristiques communes. Le travail proposé dans cette thèse concerne le regroupement en locuteurs sur des collections de documents audiovisuels volumineuses (plusieurs dizaines d'heures d'enregistrements). L'objectif principal est de proposer (ou adapter) des approches de regroupement afin de traiter efficacement de gros volumes de données, tout en détectant les locuteurs récurrents. L'efficacité des approches proposées est étudiée sous deux aspects : d'une part, la qualité des segmentations produites (en termes de taux d'erreur), et d'autre part, la durée nécessaire pour effectuer les traitements. Nous proposons à cet effet deux architectures adaptées au regroupement en locuteurs sur des collections de documents. Nous proposons une approche de simplification où le problème de regroupement est représenté par une graphe non-orienté. La décompositionde ce graphe en composantes connexes permet de décomposer le problème de regroupement en un certain nombre de sous-problèmes indépendants.
Une chaîne de coréférences est l'ensemble des expressions linguistiques — ou mentions — qui font référence à une même entité ou un même objet du discours. La tâche de reconnaissance des chaînes de coréférences consiste à détecter l'ensemble des mentions d'un document et à le partitionner en chaînes de coréférences. Ces chaînes jouent un rôle central dans la cohérence des documents et des interactions et leur identification est un enjeu important pour de nombreuses autres tâches en traitement automatique du langage, comme l'extraction d'informations ou la traduction automatique. Des systèmes automatiques de reconnaissance de chaînes de coréférence existent pour plusieurs langues, mais aucun pour le français ni pour une langue parlée. Nous nous proposons dans cette thèse de combler ce manque par un système de reconnaissance automatique de chaînes de coréférences pour le français parlé. À cette fin, nous proposons un système utilisant des réseaux de neurones artificiels et ne nécessitant pas de resources externes. Ce système est viable malgré le manque d'outils de prétraitements adaptés au français parlé et obtient des performances comparable à l'état de l'art. Nous proposons également des voies d'amélioration de ce système, en y introduisant des connaissances issues de ressources et d'outils conçus pour le français écrit. Enfin, nous proposons un nouveau format de représentation pour l'annotation des chaînes de coréférences dans des corpus de langues écrites et parlées et en nous en donnons une exmple en proposant un nouvelle version d'ANCOR — le premier corpus de français annoté en coréférence.
Dans une première étape, on explicite les enjeux théoriques d'une telle recherche. Cela conduit à accorder une place centrale au concept d'invariant sémantique pour rendre compte de l'identité sémantique d'un nom (en langue) par-delà sa variation de sens (en contexte). Dans une deuxième étape, on circonscrit l'objet empirique – les noms de parties du corps humain en français contemporain – tout en justifiant ce terrain d'étude. La suite de la thèse est consacrée à l'investigation empirique proprement dite. Il s'agit d'abord d'offrir une description générale du potentiel de variation sémantique des noms de parties du corps humain en français. Enfin, quatre autres noms (artère, épaule, bouche et pied) font également l'objet d'une analyse spécifique. Chacune de ces quatre études est l'occasion d'éprouver la pertinence du concept d'invariant sémantique pour rendre compte de la polysémie dans le domaine nominal.
Dans certains environnements sensibles, tels que le domaine de la santé, où les utilisateurs sont généralement de confiance et où des évènements particuliers peuvent se produire, comme les situations d'urgence, les contrôles de sécurité mis en place dans les systèmes d'information correspondants ne doivent pas bloquer certaines décisions et actions des utilisateurs. Cela pourrait avoir des conséquences graves. En revanche, il est important de pouvoir identifier et tracer ces actions et ces décisions afin de détecter d'éventuelles violations de la politique de sécurité mise en place et fixer les responsibilités. Ces fonctionnalités sont assurées par le contrôle d'accès à posteriori qui se base un mécanisme de monitoring à partir des logs. Dans la littérature, ce type de contrôle de sécurité a été divisé en trois étapes qui sont : le traitement des logs, l'analyse des logs, et l'imputabilité. Dans cette thèse, nous couvrons ces trois domaines du contrôle d'accès à posteriori en apportant de nouvelles solutions, et nous introduisons des nouveaux aspects qui n'avaient pas été abordés auparavant.
L'analyse criminelle est une discipline d'appui aux enquêtes pratiquée au sein de la Gendarmerie Nationale. Or, l'analyse criminelle s'appuie entre autres sur le concept d'entités pour formaliser son travail. La présentation du contexte de recherche détaille la pratique de l'analyse criminelle ainsi que la constitution du dossier de procédure judiciaire en tant que corpustextuel. Nous proposons ensuite des perspectives pour l'adaptation des méthodes de traitement automatique de la langue (TAL) et d'extraction d'information au cas d'étude, notamment la mise en parallèle des concepts d'entité en analyse criminelle et d'entité nommée en TAL. Cette comparaison est réalisée sur les plans conceptuels et linguistiques. Une première approche de détection des entités dans les auditions de témoins est présentée. Enfin, le genre textuel étant un paramètre à prendre en compte lors de l'appli-cation de traitements automatiques à du texte, nous construisons une structuration du genre textuel « légal » en discours, genres et sous-genres par le biais d'une étude textométrique visant à caractériser différents types de textes (dont les auditions de témoins) produits par le domaine de la justice.
La vision par ordinateur est un domaine interdisciplinaire étudiant la manière dont les ordinateurs peuvent acquérir une compréhension de haut niveau à partir d'images ou de vidéos numériques. En intelligence artificielle, et plus précisément en apprentissage automatique, domaine dans lequel se positionne cette thèse, la vision par ordinateur passe par l'extraction de caractéristiques présentes dans les images puis par la généralisation de concepts liés à ces caractéristiques. Ce domaine de recherche est devenu très populaire ces dernières années, notamment grâce aux résultats des réseaux de neurones convolutifs à la base des méthodes dites d'apprentissage profond. Aujourd'hui les réseaux de neurones permettent, entre autres, de reconnaître les différents objets présents dans une image, de générer des images très réalistes ou même de battre les champions au jeu de Go. Leurs performances ne s'arrêtent d'ailleurs pas au domaine de l'image puisqu'ils sont aussi utilisés dans d'autres domaines tels que le traitement du langage naturel (par exemple en traduction automatique) ou la reconnaissance de son. Dans cette thèse, nous étudions les réseaux de neurones convolutifs afin de développer des architectures et des fonctions de coûts spécialisées à des tâches aussi bien de bas niveau (la constance chromatique) que de haut niveau (la segmentation sémantique d'image). En vision par ordinateur, l'approche principale consiste à estimer la couleur de l'illuminant puis à supprimer son impact sur la couleur perçue des objets. Les expériences que nous avons menées montrent que notre méthode permet d'obtenir des performances compétitives avec l'état de l'art. Néanmoins, notre architecture requiert une grande quantité de données d'entraînement. Afin de corriger en parti ce problème et d'améliorer l'entraînement des réseaux de neurones, nous présentons plusieurs techniques d'augmentation artificielle de données. Nous apportons également deux contributions sur une problématique de haut niveau : la segmentation sémantique d'image. Cette tâche, qui consiste à attribuer une classe sémantique à chacun des pixels d'une image, constitue un défi en vision par ordinateur de par sa complexité. D'une part, elle requiert de nombreux exemples d'entraînement dont les vérités terrains sont coûteuses à obtenir. D'autre part, elle nécessite l'adaptation des réseaux de neurones convolutifs traditionnels afin d'obtenir une prédiction dite dense, c' Pour résoudre la difficulté liée à l'acquisition de données d'entrainements, nous proposons une approche qui exploite simultanément plusieurs bases de données annotées avec différentes étiquettes. Nous développons aussi une approche dites d'auto-contexte capturant d'avantage les corrélations existantes entre les étiquettes des différentes bases de données. Finalement, nous présentons notre troisième contribution : une nouvelle architecture de réseau de neurones convolutifs appelée GridNet spécialisée pour la segmentation sémantique d'image. Contrairement aux réseaux traditionnels, notre architecture est implémentée sous forme de grille 2D permettant à plusieurs flux interconnectés de fonctionner à différentes résolutions. En outre, nous montrons empiriquement que notre architecture généralise de nombreux réseaux bien connus de l'état de l'art. Nous terminons par une analyse des résultats empiriques obtenus avec notre architecture qui, bien qu'entraînée avec une initialisation aléatoire des poids, révèle de très bonnes performances, dépassant les approches populaires souvent pré-entraînés
Oedipe, le personnage de la tragedie de sophocle, resout l'enigme du sphinx "par sa seule intelligence". Il est ici le point de depart d'une reflexion generale sur le statut linguistique des jeux de langage, dont la pratique est repandue a toutes les epoques et dans toutes les cultures. L'intelligence d'oedipe se fonde sur une capacite a "calculer" l'interpretation de l'enigme en abandonnant un raisonnement inductif (par recurrence) pour adopter un raisonnement analogique. Dans une seconde partie, on montre que le calcul du sens des messages plurivoques permet de proposer un modele d'analyse combinatoire qui est un outil de traitement automatique des langues (tal), capable d'aider au calcul des jeux de charades et a l'interpretation des definitions cryptees des mots croises. Ce modele sert de pierre de touche a une analyse des structures semantiques sous-jacentes aux interpretations et montre quels sont les items lexicaux qui sont concernes par l'isotopie. L'isotopie n'est en l'occurrence pas consideree comme une donnee du message mais comme un construit de l'interpretation. L'ensemble de la demarche adopte donc le point de vue d'une semantique interpretative. La troisieme partie prolonge la reflexion en inscrivant le traitement des messages enigmatiques dans la problematique du dialogue homme-machine (dhm) qui permet de traiter les ambiguites de certains enonces et est capable de comprendre des "messages etranges" a partir des propositions d'interpretation extrapolees du modele. De proche en proche on analyse ainsi le calcul du recepteur des messages comme une activite qui consiste a analyser les traces graphematiques ou acoustiques. La prise en compte des traces est une confrontation avec les attendus du systeme linguistique qui permet de proceder a une serie de decisions aboutissant a l'identification d'un point de vue coherent. La decouverte de cette coherence et de ce point de vue sont compares a la demarche que l'on adopte dans la "lecture" d'une anamorphose (en peinture) ou quand on dechiffre les regles d'organisation des suites de cartes dans le jeu d'eleusis. On retrouve une demarche analogue quand il s'agit d'interpreter la"scriptio continua" des inscriptions paleographiques, dont la technique sert de base a la fois a certaines experiences de production litteraire sous contrainte et au jeux des mots caches.
De nos jours, les médias sociaux ont largement affecté tous les aspects de la vie humaine. Le changement le plus significatif dans le comportement des gens après l'émergence des réseaux sociaux en ligne (OSNs) est leur méthode de communication et sa portée. Avoir plus de connexions sur les OSNs apporte plus d'attention et de visibilité aux gens, où cela s'appelle la popularité sur les médias sociaux. Selon le type de réseau social, la popularité se mesure par le nombre d'adeptes, d'amis, de retweets, de goûts et toutes les autres mesures qui servaient à calculer l'engagement. L'étude du comportement de popularité des utilisateurs et des contenus publiés sur les médias sociaux et la prédiction de leur statut futur sont des axes de recherche importants qui bénéficient à différentes applications telles que les systèmes de recommandation, les réseaux de diffusion de contenu, les campagnes publicitaires, la prévision des résultats des élections, etc. Cette thèse porte sur l'analyse du comportement de popularité des utilisateurs d'OSN et de leurs messages publiés afin, d'une part, d'identifier les tendances de popularité des utilisateurs et des messages et, d'autre part, de prévoir leur popularité future et leur niveau d'engagement pour les messages publiés par les utilisateurs. A cette fin, i) l'évolution de la popularité des utilisateurs de l'ONS est étudiée à l'aide d'un ensemble de données d'utilisateurs professionnels 8K Facebook collectées par un crawler avancé. L'ensemble de données collectées comprend environ 38 millions d'instantanés des valeurs de popularité des utilisateurs et 64 millions de messages publiés sur une période de 4 ans. Le regroupement des séquences temporelles des valeurs de popularité des utilisateurs a permis d'identifier des modèles d'évolution de popularité différents et intéressants. Les grappes identifiées sont caractérisées par l'analyse du secteur d'activité des utilisateurs, appelé catégorie, leur niveau d'activité, ainsi que l'effet des événements externes. Ensuite ii) la thèse porte sur la prédiction de l'engagement des utilisateurs sur les messages publiés par les utilisateurs sur les OSNs. Un nouveau modèle de prédiction est proposé qui tire parti de l'information mutuelle par points (PMI) et prédit la réaction future des utilisateurs aux messages nouvellement publiés. Enfin, iii) le modèle proposé est élargi pour tirer profit de l'apprentissage de la représentation et prévoir l'engagement futur des utilisateurs sur leurs postes respectifs. L'approche de prédiction proposée extrait l'intégration de l'utilisateur de son historique de réaction au lieu d'utiliser les méthodes conventionnelles d'extraction de caractéristiques. La performance du modèle proposé prouve qu'il surpasse les méthodes d'apprentissage conventionnelles disponibles dans la littérature. Les modèles proposés dans cette thèse, non seulement déplacent les modèles de prédiction de réaction vers le haut pour exploiter les fonctions d'apprentissage de la représentation au lieu de celles qui sont faites à la main, mais pourraient également aider les nouvelles agences, les campagnes publicitaires, les fournisseurs de contenu dans les CDN et les systèmes de recommandation à tirer parti de résultats de prédiction plus précis afin d'améliorer leurs services aux utilisateurs
Mes travaux de thèse s'intéressent à l'utilisation de nouvelles technologies d'intelligence artificielle appliquées à la problématique de la classification automatique des séquences audios selon l'état émotionnel du client au cours d'une conversation avec un téléconseiller. En 2016, l'idée est de se démarquer des prétraitements de données et modèles d'apprentissage automatique existant au sein du laboratoire, et de proposer un modèle qui soit le plus performant possible sur la base de données audios IEMOCAP. Nous nous appuyons sur des travaux existants sur les modèles de réseaux de neurones profonds pour la reconnaissance de la parole, et nous étudions leur extension au cas de la reconnaissance des émotions dans la voix. Nous nous intéressons ainsi à l'architecture neuronale bout-en-bout qui permet d'extraire de manière autonome les caractéristiques acoustiques du signal audio en vue de la tâche de classification à réaliser. Pendant longtemps, le signal audio est prétraité avec des indices paralinguistiques dans le cadre d'une approche experte. Nous choisissons une approche naïve pour le prétraitement des données qui ne fait pas appel à des connaissances paralinguistiques spécialisées afin de comparer avec l'approche experte. Exploiter un réseau neuronal pour une tâche de prédiction précise implique de devoir s'interroger sur plusieurs aspects. D'une part, il convient de choisir les meilleurs hyperparamètres possibles. D'autre part, il faut minimiser les biais présents dans la base de données (non discrimination) en ajoutant des données par exemple et prendre en compte les caractéristiques de la base de données choisie. Nous étudions ces aspects pour une architecture neuronale bout-en-bout qui associe des couches convolutives spécialisées dans le traitement de l'information visuelle, et des couches récurrentes spécialisées dans le traitement de l'information temporelle. Nous proposons un modèle d'apprentissage supervisé profond compétitif avec l'état de l'art sur la base de données IEMOCAP et cela justifie son utilisation pour le reste des expérimentations. Notre modèle est évalué sur deux bases de données audios anglophones proposées par la communauté scientifique : IEMOCAP et MSP-IMPROV. Une première contribution est de montrer qu'avec un réseau neuronal profond, nous obtenons de hautes performances avec IEMOCAP et que les résultats sont prometteurs avec MSP-IMPROV. Une autre contribution de cette thèse est une étude comparative des valeurs de sortie des couches du module convolutif et du module récurrent selon le prétraitement de la voix opéré en amont : spectrogrammes (approche naïve) ou indices paralinguistiques (approche experte). À l'aide de la distance euclidienne, une mesure de proximité déterministe, nous analysons les données selon l'émotion qui leur est associée. Nous tentons de comprendre les caractéristiques de l'information émotionnelle extraite de manière autonome par le réseau. L'idée est de contribuer à une recherche centrée sur la compréhension des réseaux de neurones profonds utilisés en reconnaissance des émotions dans la voix et d'apporter plus de transparence et d'explicabilité à ces systèmes dont le mécanisme décisionnel est encore largement incompris.
En traitement automatique des langues, deux grandes approches sont utilisées : l'apprentissage automatique et la fouille de données. Dans cette thèse, nous présentons trois contributions majeures : l'introduction des motifs delta libres,utilisés comme descripteurs de modèle statistiques ; 
Les approches formelles de représentation des signes des langues des signes sont majoritairement paramétriques et nous montrons en quoi celles-ci ne sont pas suffisantes dans l'optique d'une utilisation informatique. Les plus fortes raisons sont le caractère ni nécessaire ni suffisant de l'ensemble de paramètres traditionnellement utilisé, leur nature fixe alors qu'un signe est dynamique et évolue au cours du temps, et le fait que les descriptions ne rendent pas compte de l'adaptabilité des signes décrits à différents contextes, pourtant à l'origine de leur réutilisabilité et de la force de concision des langues des signes. Nous proposons Zebedee, un modèle de description en séquence d'unités temporelles décrivant chacune un ensemble de contraintes nécessaires et suffisantes, appliquées à un squelette. L'espace de signation est vu comme un espace euclidien dans lequel toute construction géométrique annexe est possible. Les dépendances entre éléments des descriptions ou sur des valeurs contextuelles sont non seulement possibles mais pertinentes, et reposent sur des considérations à la fois articulatoires, cognitives et sémantiques. Nous donnons ensuite deux processus complémentaires d'évaluation : en informatique où nous discutons l'implantation de Zebedee dans une plateforme d'animation de signeur virtuel et son utilisation pour la diffusion d'informations en gare, et en linguistique où nous décrivons l'avantage d'une base de données et les nouvelles possibilités de requêtes offertes au linguiste. En perspectives, nous citons plusieurs domaines informatiques où Zebedee sera utile et plusieurs questionnements linguistiques actuels auxquels il offre des pistes de réponse.
La détection sans fil a évolué depuis la découverte de la détection radar en 1886. Cependant, pendant très longtemps, la détection sans fil a rarement été utilisée pour des applications centrées sur l'homme en raison de limitations techniques, d'impraticabilité ou de coût. L'introduction des réseaux sans fil a suscité́ un nouvel intérêt pour le développement de nouveaux services de détection sans fil en raison de leur souplesse et de leur polyvalence. L'intégration de ces fonctionnalités contribuerait à résoudre certains problèmes de société importants. La localisation, la détection de mouvements et la surveillance des signes vitaux ont un grand potentiel pour promouvoir le vieillissement en bonne santé, la sécurité publique et le commerce. La détection sans contact offre un degré de liberté appréciable, permettant de surveiller à distance les personnes âgées isolées sans entraver leur vie quotidienne. Elle pourrait aider les services de sécurité publique à dénombrer les foules et à détecter les survivants à l'intérieur des bâtiments en cas d'urgence. Les commerces de détail et les établissements publics tireraient parti d'une localisation active et passive pour offrir une expérience améliorée à leurs visiteurs et faciliter leurs efforts logistiques. Tandis que d'autres travaux fournissent des solutions à granularité grossière pour résoudre de tels problèmes, nous utilisons les techniques de radar MIMO pour fournir un système d'estimation d'orientation précis pour lesinfrastructures Wi-Fi. Plus précisément, nous analysons les informations de phase des signaux reçus sur le réseau d'antennes afin de calculer le cap d'un terminal Wi Les solutions actuelles sont complexes, coûteuses ou consomment beaucoup d'énergie. Pour résoudre ce problème, nous introduisons les fonctions MIMO dans les systèmes LoRa LPWAN afin de permettre une localisation précise avec des coûts de démarrage limités. Nous activons l'estimation de l'angle d'arrivée en utilisant une deuxième antenne sur la passerelle LoRaWAN. Nous prouvons également l'utilité de ces informations pour augmenter l'efficacité des communications sans fil. Un troisième défi pour la localisation sans fil est l'inefficacité des approches actuelles basées sur un modèle en cas de conditions de non-visibilité et la rigidité des approches basées sur les données en cas de changements d'environnement de propagation. Pour relever ce défi, nous proposons une nouvelle solution de localisation passive pilotée par les données afin de remédier aux limitations des techniques de localisation basées sur un modèle.
Le changement climatique en cours modifie les conditions environnementales et les espèces doivent s'adapter à ces nouvelles conditions, en restant sur place ou en se déplaçant conduisant alors à de nouvelles distributions. Ce repositionnement revêt deux dimensions principales : (i) l'adaptabilité des espèces aux nouvelles conditions (changement de traits d'histoire de vie) liée à la résilience des populations et (ii) leur capacité à explorer de nouveaux habitats favorables. Cette étude avait pour objectif l'élaboration d'un modèle dynamique mécaniste intégrant ces deux dimensions de manière à pouvoir évaluer, comprendre et prédire les possibilités de repositionnement des poissons migrateurs amphihalins européens face au changement climatique. Pour accomplir leurs cycles de vie, les espèces migratrices amphihalines utilisent nécessairement des écosystèmes dulçaquicoles, estuariens et marins. Ces cycles de vie particuliers leur confèrent un plus grand potentiel de repositionnement que les espèces dulçaquicoles. Une base de données sur les traits de vie de ces espèces intégrant notamment ceux pouvant potentiellement être influencés par le changement climatique et ceux pouvant jouer un rôle dans le potentiel de dispersion des espèces a été construite pour l'ensemble des espèces amphihalines européens. Une méthode d'analyse multicritère hiérarchique a été proposée pour définir un indice basé sur les traits de vies visant à caractériser le potentiel de repositionnement des espèces migratrices amphihalines. Le modèle GR3D (Global Repositioning Dynamics for Diadromous fish Distribution) a ensuite été développé pour étudier de façon dynamique le repositionnement potentiel de ces espèces, à large échelle, dans un contexte scénarisé de changement climatique. Il s'agit d'un modèle de simulation stochastique, individus centré, intégrant les principaux processus de dynamique de population d'un poisson migrateur amphihalin (reproduction, mortalité, croissance, migration de montaison avec dispersion, migration de dévalaison). Un premier cas d'étude exploratoire simulant le repositionnement d'une population virtuelle de grande alose (Alosa alosa) de son bassin versant d'origine à un bassin versant voisin inhabité dans un contexte d'augmentation de la température a permis de réaliser une analyse de sensibilité globale du modèle GR3D à la fois aux paramètres incertains de dynamique de population et aux paramètres reliés à la structure de l'environnement. Il a été mis en évidence une sensibilité particulière du modèle aux paramètres liés à la durée de vie et à la mortalité en mer ainsi qu'à la distance entre les deux bassins versants de l'environnement pour déterminer le succès de colonisation. Enfin, l'utilisation du modèle GR3D sur un cas d'application réel a permis de commencer à évaluer l'évolution de la persistance de la grande alose à l'échelle de son aire de répartition (i.e. la façade atlantique) dans un contexte de changement climatique. Les simulations du modèle GR3D devraient ainsi trouver à terme des applications pour la gestion et la conservation des espèces migratrices amphihalines.
Cette thèse introduit différentes méthodes de vérification (ou model-checking) sur des modèles de systèmes à pile. En effet, les systèmes à pile (pushdown systems) modélisent naturellement les programmes séquentiels grâce à une pile infinie qui peut simuler la pile d'appel du logiciel. La première partie de cette thèse se concentre sur la vérification sur des systèmes à pile de la logique HyperLTL, qui enrichit la logique temporelle LTL de quantificateurs universels et existentiels sur des variables de chemin. Il a été prouvé que le problème de la vérification de la logique HyperLTL sur des systèmes d'états finis est décidable ;  nous montrons que ce problème est en revanche indécidable pour les systèmes à pile ainsi que pour la sous-classe des systèmes à pile visibles (visibly pushdown systems). Nous introduisons donc des algorithmes d'approximation de ce problème, que nous appliquons ensuite à la vérification de politiques de sécurité. Dans la seconde partie de cette thèse, dans la mesure où la représentation de la pile d'appel par les systèmes à pile est approximative, nous introduisons les systèmes à surpile (pushdown systems with an upper stack) ; dans ce modèle, les symboles retirés de la pile d'appel persistent dans la zone mémoire au dessus du pointeur de pile, et peuvent être plus tard écrasés par des appels sur la pile. Nous montrons que les ensembles de successeurs post* et de prédécesseurs pre* d'un ensemble régulier de configurations ne sont pas réguliers pour ce modèle, mais que post* est toutefois contextuel (context-sensitive), et que l'on peut ainsi décider de l'accessibilité d'une configuration. Enfin, dans le but d'analyser des programmes avec plusieurs fils d'exécution, nous introduisons le modèle des réseaux à piles dynamiques synchronisés (synchronized dynamic pushdown networks), que l'on peut voir comme un réseau de systèmes à pile capables d'effectuer des changements d'états synchronisés, de créer de nouveaux systèmes à piles, et d'effectuer des actions internes sur leur pile. Nous appliquons ensuite cette méthode à un processus itératif de raffinement des abstractions.
La langue arabe est pauvre en ressources sémantiques électroniques. Il y a bien la ressource Arabic WordNet, mais il est pauvre en mots et en relations. Cette thèse porte sur l'enrichissement d'Arabic WordNet par des synsets (un synset est un ensemble de mots synonymes) à partir d'un corpus général de grande taille. Pour validation, il a fallu créer un corpus de référence (il n'en existe pas en arabe pour ce domaine) à partir d'Arabic WordNet, puis comparer la méthode GraPaVec avec Word2Vec et Glove. Le résultat montre que GraPaVec donne pour ce problème les meilleurs résultats avec une F-mesure supérieure de 25 % aux deux autres. Les classes produites seront utilisées pour créer de nouveaux synsets intégrés à Arabic WordNet
Cette thèse porte sur l'utilisation d'ontologies et de bases de connaissances pour guider différentes étapes du processus d'extraction de connaissances à partir de bases de données (ECBD) et une application dans le domaine de la pharmacogénomique. Cette approche a été implémentée grâce aux technologies du Web sémantique et conduit au peuplement d'une base de connaissances pharmacogénomique. Le fait que les données à fouiller soient alors disponibles dans une base de connaissances entraîne de nouvelles potentialités pour le processus d'extraction de connaissances. Je me suis d'abord intéressé au problème de la sélection des données les plus pertinentes à fouiller en montrant comment la base de connaissances peut être exploitée dans ce but. Ensuite j'ai décrit et appliqué à la pharmacogénomique, une méthode qui permet l'extraction de connaissances directement à partir d'une base de connaissances. Cette méthode appelée Analyse des Assertions de Rôles (ou AAR) permet d'utiliser des algorithmes de fouille de données sur un ensemble d'assertions de la base de connaissances pharmacogénomique et d'expliciter des connaissances nouvelles et pertinentes qui y étaient enfouies.
Avec l'essor du Web 2.0 et des technologies collaboratives qui y sont rattachées,le Web est aujourd'hui devenu une vaste plate-forme d'échanges entre internautes. La majeure partie des sites Web sont actuellement soit dédiés aux interactions sociales de leurs utilisateurs, soit proposent des outils pour développer ces interactions. Nos travaux portent sur la compréhension de ces échanges, ainsi que des structures communautaires qui en découlent, au moyen d'une approche sémantique. Pour répondre aux besoins de compréhension propres aux analystes de site Web et autres gestionnaires de communautés, nous analysons ces structures communautaires pour en extraire des caractéristiques essentielles comme leurs centres thématiques et contributeurs centraux. Notre analyse sémantique s'appuie notamment sur des ontologies légères de référence pour définir plusieurs nouvelles métriques,comme la centralité sémantique temporelle et la probabilité de propagation sémantique. Nous employons une approche « en ligne » afin de suivre l'activité utilisateur en temps réel, au sein de notre outil d'analyse communautaire Web-Tribe. Nous avons implémenté et testé nos méthodes sur des données extraites de systèmes réels de communication sociale sur le Web
Cette thèse se consacre aux enjeux de la polysémie verbale pour l'acquisition des langues secondes. Nous nous intéressons plus particulièrement à la polysémie du verbe prendre. Cette thèse poursuit deux objectifs complémentaires. D'une part, notre premier objectif est de décrire la polysémie du verbe prendre au moyen d'une analyse en sémantique lexicale dans une approche cognitive. D'autre part, notre second objectif est d'évaluer l'incidence de la polysémie du verbe prendre sur les connaissances qu'ont les apprenants de ce verbe et d'isoler les différentes acceptions de prendre mises au jour par notre analyse sémantique, qui s'avèrent problématiques pour les apprenants du français L2. Par ailleurs, nous cherchons également à savoir si les problèmes liés à la polysémie chez les anglophones peuvent être liés à l'influence translangagière. Afin de délimiter le corpus d'acceptions soumis à l'analyse sémantique, nous avons effectué une analyse syntaxique permettant de départager les différents emplois du verbe prendre : les constructions à verbe support, les locutions verbales et les acceptions prédicatives. Chacune des parties du noyau de sens peut faire l'objet d'un fenêtrage de l'attention, qui nous permet de mettre au jour les différents sens du verbe. Nous avons également émis des hypothèses sur les différences entre le verbe prendre et ses équivalents en anglais. Pour atteindre le second objectif de notre travail, nous avons mené une expérimentation auprès de 191 apprenants du français langue seconde. Tous les participants ont complété trois tâches expérimentales : un test de closure afin de mesurer leur niveau de compétence langagière en français, une tâche de production écrite et une tâche de jugement d'acceptabilité. Les résultats des analyses de régression logistique et multinomiale montrent non seulement que l'analyse sémantique que nous avons proposée permet de prédire la connaissance des différentes acceptions du verbe par les apprenants du français L2, mais aussi que les apprenants anglophones et allophones ont un comportement différent par rapport aux types d'acceptions du verbe prendre, comportement que nous avons pu expliquer par l'influence translangagière chez les participants anglophones.
La physique statistique, développée à l'origine pour décrire les systèmes thermodynamiques, a joué pendant les dernières décennies un rôle central dans la modélisation d'un ensemble incroyablement vaste et hétérogène de différents phénomènes qui ont lieu par exemple dans des systèmes sociaux, économiques ou biologiques. Un champ d'applications possibles aussi vaste a été trouvé aussi pour les réseaux, comme une grande variété de systèmes peut être décrite en termes d'éléments interconnectés. Après une partie introductive sur les thèmes abordés ainsi que sur le rôle de la modélisation abstraite dans la science, dans ce manuscrit seront décrites les nouvelles perspectives auxquelles on peut arriver en approchant d'une façon physico-statistique trois problèmes d'intérêt dans la théorie des réseaux : comment une certaine quantité peut se répandre de façon optimale sur un graphique, comment explorer un réseau et comment le reconstruire à partir d'un jeu d'informations partielles. Quelques remarques finales sur l'importance que ces thèmes préserveront dans les années à venir conclut le travail.
Cette thèse propose une analyse contrastive de la notion de défini telle qu'elle est exprimée dans le système de l'article en anglais et en arabe moderne standard. Le récit, The Brook Kerith de l'écrivain irlandais George Moore a été choisi pour des raisons géo-historiques et littéraires : les événements racontés se déroulent en Terre Sainte à l'aube de l'ère chrétienne. Les occurrences du syntagme nominal en anglais et en arabe analysées dans le premier chapitre permettent de dégager les convergences et les divergences des deux systèmes. Les résultats sont soumis à une analyse quantitative et statistique. Il en ressort que la valeur de l'article défini en anglais (“the”) et en arabe (“al”) correspondent dans 76% des emplois. La ressemblance entre la valeur de l'article indéfini (“a / an”) en anglais et son équivalent en arabe s'élève à 96%. Cependant, dans la mesure où l'arabe est une langue sans article indéfini, le fonctionnement de l'article zéro en anglais est sans équivalence ;  En dernière analyse, on constate une grande ressemblance entre les mécanismes cognitifs sous-jacents ;  les différences concernent les transformations sémiotiques de la structure profonde.
Cette thèse porte sur la détection d'événements dans des signaux issus de capteurs sols pour le suivi des personnes âgées. Au vu des questions pratiques, il semble en effet que les capteurs de pression situés au sol soient de bons candidats pour les activités de suivi, notamment la détection de chute. Ainsi, afin de concevoir un détecteur de chutes, nous proposons une approche basée sur les forêts aléatoires, tout en répondant aux contraintes matérielles à l'aide d'une procédure de sélection des variables. Les performances sont améliorées à l'aide d'une méthode d'augmentation des données ainsi qu'à l'agrégation temporelle des réponses du modèle. Nous les testons sur plusieurs ensembles de données, montrant ainsi des résultats encourageants pour la suite, et une implémentation Python est mise à disposition. Enfin, motivés par la question du suivi des personnes âgées tout en traitant un signal unidimensionnel pour une grande zone, nous proposons de distinguer les personnes âgées des individus plus jeunes grâce à un modèle de réseau de neurones convolutifs et un apprentissage de dictionnaire. Les signaux à traiter étant principalement constitués de marches, la première brique du modèle est entraînée pour se focaliser sur les pas dans les signaux, et la seconde partie du modèle est entraînée séparément sur la tâche finale. Cette nouvelle approche de la classification de la marche permet de reconnaître avec efficacité les signaux issus de personnes âgées.
Les systèmes de recommandation visent à présélectionner et présenter en premier les informations susceptibles d'intéresser les utilisateurs. Ceci a suscité l'attention du commerce électronique, où l'historique des achats des utilisateurs sont analysés pour prédire leurs intérêts futurs et pouvoir personnaliser les offres ou produits (appelés aussi items) qui leur sont proposés. Dans ce cadre, les systèmes de recommandation exploitent les préférences des utilisateurs et les caractéristiques des produits et des utilisateurs pour prédire leurs préférences pour des futurs items. Bien qu'ils aient démontré leur précision, ces systèmes font toujours face à de grands défis tant pour le monde académique que pour l'industrie : ces techniques traitent un grand volume de données qui exige une parallélisation des traitements, les données peuvent être également très hétérogènes, et les systèmes de recommandation souffrent du démarrage à froid, situation dans laquelle le système n'a pas (ou pas assez) d'informations sur (les nouveaux) utilisateurs/items pour proposer des recommandations précises. La technique de factorisation matricielle a démontré une précision dans les prédictions et une simplicité de passage à l'échelle. Cependant, cette approche a deux inconvénients : la complexité d'intégrer des données hétérogènes externes (telles que les caractéristiques des items) et le démarrage à froid pour un nouvel utilisateur. Cette thèse présente quatre contributions au domaine des systèmes de recommandation : (1) nous proposons une implémentation d'un algorithme de recommandation de factorisation matricielle parallélisable pour assurer un meilleur passage à l'échelle, (2) nous améliorons la précision des recommandations en prenant en compte l'intérêt implicite des utilisateurs dans les attributs des items, (3) nous proposons une représentation compacte des caractéristiques des utilisateurs/items basée sur les filtres de bloom permettant de réduire la quantité de mémoire utile, (4) nous faisons face au démarrage à froid d'un nouvel utilisateur en utilisant des techniques d'apprentissage actif. La phase d'expérimentation utilise le jeu de données MovieLens et la base de données IMDb publiquement disponibles, ce qui permet d'effectuer des comparaisons avec des techniques existantes dans l'état de l'art. Ces expérimentations ont démontré la précision et l'efficacité de nos approches.
Dans cette thèse, nous présentons notre méthodologie de la connaissance interactive et itérative pour une extraction des textes-le système KESAM : Un outil pour l'extraction des connaissances et le Management de l'Annotation Sémantique. Le KESAM est basé sur l'analyse formelle du concept pour l'extraction des connaissances à partir de ressources textuelles qui prend en charge l'interaction aux experts. Dans le système KESAM, l'extraction des connaissances et l'annotation sémantique sont unifiées en un seul processus pour bénéficier à la fois l'extraction des connaissances et l'annotation sémantique. Les annotations sémantiques sont utilisées pour formaliser la source de la connaissance dans les textes et garder la traçabilité entre le modèle de la connaissance et la source de la connaissance. Le modèle de connaissance est, en revanche, utilisé afin d'améliorer les annotations sémantiques. Le processus KESAM a été conçu pour préserver en permanence le lien entre les ressources (textes et annotations sémantiques) et le modèle de la connaissance. Le noyau du processus est l'Analyse Formelle de Concepts (AFC) qui construit le modèle de la connaissance, i.e. le treillis de concepts, et assure le lien entre le modèle et les annotations des connaissances. Afin d'obtenir le résultat du treillis aussi près que possible aux besoins des experts de ce domaine, nous introduisons un processus itératif qui permet une interaction des experts sur le treillis. Les experts sont invités à évaluer et à affiner le réseau ; ils peuvent faire des changements dans le treillis jusqu'à ce qu'ils parviennent à un accord entre le modèle et leurs propres connaissances ou le besoin de l'application. Grâce au lien entre le modèle des connaissances et des annotations sémantiques, le modèle de la connaissance et les annotations sémantiques peuvent co-évoluer afin d'améliorer leur qualité par rapport aux exigences des experts du domaine. En outre, à l'aide de l'AFC de la construction des concepts avec les définitions des ensembles des objets et des ensembles d'attributs, le système KESAM est capable de prendre en compte les deux concepts atomiques et définis, à savoir les concepts qui sont définis par un ensemble des attributs. Afin de combler l'écart possible entre le modèle de représentation basé sur un treillis de concept et le modèle de représentation d'un expert du domaine, nous présentons ensuite une méthode formelle pour l'intégration des connaissances d'expert en treillis des concepts d'une manière telle que nous pouvons maintenir la structure des concepts du treillis. La connaissance d'expert est codée comme un ensemble de dépendance de l'attribut qui est aligné avec l'ensemble des implications fournies par le concept du treillis, ce qui conduit à des modifications dans le treillis d'origine. La méthode permet également aux experts de garder une trace des changements qui se produisent dans le treillis d'origine et la version finale contrainte, et d'accéder à la façon dont les concepts dans la pratique sont liés à des concepts émis automatiquement à partir des données. Nous pouvons construire les treillis contraints sans changer les données et fournir la trace des changements en utilisant des projections extensives sur treillis. À partir d'un treillis d'origine, deux projections différentes produisent deux treillis contraints différents, et, par conséquent, l'écart entre le modèle de représentation basée sur un treillis de réflexion et le modèle de représentation d'un expert du domaine est rempli avec des projections
Les décès du cancer de la peau sont majoritairement des mélanomes malins. Il est considéré comme l'un des plus dangereux cancer. A ses débuts, les mélanomes malins sont traités avec des simples biopsies et sont complètement curable. Pour cela, une détection précoce est la meilleure solution pour réduire ses conséquences désastreuses. Imagerie médicale telle que la dermoscopie et les caméras à images standard sont les outils disponibles les plus adaptées pour diagnostiquer précocement les mélanomes. Le diagnostic assisté par ordinateur (CAD) est développé dans le but d'accompagner les radiologistes dans la détection et le diagnostic. Cependant, il y a un besoin d'améliorer la précision de la segmentation et de détection des lésions. La deuxième tâche consiste à extraire des caractéristiques afin de discriminer les mélanomes. Deux méthodes ont été développée, une se basant sur l'irrégularité des bords des lésions et l'autre par la fusion des caractéristiques texturales et structurelles. Les résultats ont montrés de bonnes performances avec une précision de 86.54% et de 86.07%, respectivement.
L'apprentissage à distance métrique est une branche de l'apprentissage par re-présentation des algorithmes d'apprentissage automatique. Nous résumons le développement et la situation actuelle de l'algorithme actuel d'apprentissage à distance métrique à partir des aspects de la base de données plate et de la base de données non plate. Pour une série d'algorithmes basés sur la distance de Mahalanobis pour la base de données plate qui ne parvient pas à exploiter l'intersection de trois dimensions ou plus, nous proposons un algorithme d'apprentissage métrique basé sur la fonction sousmodulaire. Pour le manque d'algorithmes d'apprentissage métrique pour les bases de données relationnelles dans des bases de données non plates, nous proposons LSCS (sélection de contraintes relationnelles de force relationnelle) pour la sélection de contraintes pour des algorithmes d'apprentissage métrique avec informations parallèles et MRML (Multi-Relation d'apprentissage métrique) qui somme la perte des contraintes relationnelles et les contraintes d'etiquetage. Grâce aux expériences de conception et à la vérification sur la base de données réelle, les algorithmes proposés sont meilleurs que les algorithmes actuels.
L'enjeu de cette thèse est l'acquisition automatique de nouveaux sens lexicaux. Nous définissons un modèle théorique sur l'émergence d'un nouveau sens pour une unité lexicale ayant déjà un sens codé. Nous la modélisons à partir d'indices quantitatifs articulés à des principes issus de la sémantique textuelle. Le sens codé est représentécomme un ensemble structuré de traits sémantiques. Il est modulé en discours sous l'effet de récurrences d'autres traits. La dynamique du sens est représentée à l'aide de descripteurs de granularité sémantique variable. Ensuite, nous proposons des ressources et outils adaptés, relevant de la linguistique de corpus. En pratique, le Trésor de la Langue Française informatisé fournit les sens codés. Une plateforme transforme ses définitions en ensembles de traits sémantiques. Trois corpus journalistiques des années 2000 servent de ressources textuelles. Les outils mathématiques, essentiellementstatistiques, permettent de jouer sur la structure des ressources, d'extraire des unités saillantes et d'organiser l'information. Enfin, nous établissons les grandes lignes d'une procédure pour allouer de façon semi-automatique un nouveau sens. Elles sont étayées par des expériences illustratives. Il s'appuie sur des jeux de contrastes multiples, permettant de nuancer l'informationsémantique.
L'évaluation de connaissances à travers un support de questions à choix multiples est une méthode fiable et largement utilisée, y compris dans des contextes officiels, comme pour l'examen du code de la route. Cette méthode d'évaluation offre en effet de nombreux avantages, comme une égalité de notation entre les candidats, ou de façon plus pragmatique, une possibilité de correction automatique. L'émergence des MOOCs, des cours dispensés sous un format numérique, a contribué à accroître ce besoin d'évaluation automatique. Les travaux de cette thèse s'inscrivent ainsi dans ce contexte, en proposant une solution permettant de générer des questions thématiques, c'est à dire des questions centrées autour d'un thème prédéfini. Les travaux présentés dans cette thèse utilisent des bases de connaissances comme sources de données pour générer automatiquement des questions à choix multiples thématiques. Cette thèse présente ainsi une méthode basée sur les méta-données de Wikipedia permettant d'identifier et de trier les entités de bases de connaissances en fonction de thèmes prédéfinis.- Pour qu'une question soit intelligible, son énoncé doit être grammaticalement correct, et contenir suffisamment d'informations pour lever toute ambiguïté quand-à la bonne réponse. Dans une dernière contribution, nous présentons la méthode utilisée pour sélectionner des distracteurs qui soient non seulement pertinents vis-à-vis de l'énoncé de la question, mais aussi de son contexte.
Le script est une structure qui décrit une séquence stéréotypée d'événements ou d'actions survenant dans notre vie quotidienne. Les histoires utilisent des scripts, avec une ou plusieurs déviations intéressantes, qui nous permettent de mieux saisir les situations quotidiennes rapportées et les faits saillants du récit. Ainsi, la notion de script est très utile dans de nombreuses applications d'intelligence ambiante telles que la surveillance de la santé et les services d'urgence. La reconnaissance de l'activité humaine (HAR) a ainsi connue un essor important grâce notamment à des approches d'apprentissage automatique telles que le réseau neuronal ou le réseau bayésien. Ces avancées ouvre des perspectives qui vont au delà de la simple reconnaissance d'activités. Ce manuscrit défend la thèse selon laquelle ces données de capteurs portables peuvent être utilisées pour générer des récits articulés autour de scripts en utilisant l'apprentissage automatique. Il ne s'agit pas d'une tâche triviale en raison du grand écart sémantique entre les informations brutes de capteurs et les abstractions de haut niveau présente dans les récits. A notre connaissance, il n'existe toujours pas d'approche pour générer une histoire à partir de données de capteurs en utilisant l'apprentissage automatique, même si de nombreuses approches d'apprentissage automatique (réseaux de neurones convolutifs, réseaux de neurones profonds) ont été proposées pour la reconnaissance de l'activité humaine au cours des dernières années. Deuxièmement, nous présentons un nouveau système permettant de générer automatiquement des scripts à partir de données d'activité humaine à l'aide de l'apprentissage profond. Enfin, nous proposons une approche pour l'apprentissage de scripts à partir de textes en langage naturel capable d'exploiter l'information syntaxique et sémantique sur le contexte textuel des événements. Cette approche permet l'apprentissage de l'ordonnancement d'événements à partir d'histoires décrivant des situations typiques de vie quotidienne.
L'objet de cette thèse est l'étude de la reconnaissance automatique de parole. Ce document débute avec la description des traitements acoustiques les plus répandus en vue de reconnaître la parole. Nous décrivons ensuite les diverses architectures qui ont été utilisées : comparaison dynamique de formes acoustiques, systèmes experts, réseaux neuro-mimétiques et modèles de Markov. Puis ce document se divise en deux parties. Pour cela, nous utilisons des automates qui modélisent le vocabulaire. Celui-ci comporte les dix chiffres anglo-saxons, dont deux prononciations différentes pour le zéro. Le corpus de parole TiDigits a été utilisé par d'autres laboratoires ce qui nous permet de comparer nos résultats. La première étape est consacrée à la reconnaissance de mots isolés. Puis nous présentons une méthode de segmentation de séquences de chiffres. La fin de ce chapitre est consacrée à la reconnaissance de mots enchaînés et à une discussion sur les mérites et les faiblesses de notre approche. La deuxième partie traite de l'utilisation d'un modèle de production qui pourrait être utilisé pour le reconnaissance de la parole. Nous commençons par présenter les équations acoustiques régissant l'écoulement de l'air dans le conduit vocal et divers modèles articulatoires. Ensuite nous justifions le choix du modèle articulatoire de Maeda. Nous décrivons comment nous avons adapté le modèle à un locuteur masculin. Puis nous présentons la méthode variationnelle utilisée pour retrouver les trajectoires des articulateurs en fonction de la parole prononcée. Une dernière section présente les logiciels réalisés. En conclusion, nous résumons les résultats obtenus et donnons quelques perspectives en vue de reconnaître la parole continue quel que soit le locuteur.
Les images satellites représentent de nos jours une source d'information incontournable. Elles sont exploitées dans diverses applications, telles que : la gestion des risques, l'aménagent des territoires, la cartographie du sol ainsi qu'une multitude d'autre taches. Nous proposons des méthodes d'analyse de STIS orientée objets, en opposition aux approches par pixel, qui exploitent des images satellites segmentées. Nous identifions d'abord les profils d'évolution des objets de la série. Ensuite, nous analysons ces profils en utilisant des méthodes d'apprentissage automatique. Afin d'analyser les graphes d'évolution, nous avons proposé trois contributions. La première contribution explore des STIS annuelles. Elle permet d'analyser les graphes d'évolution en utilisant des algorithmes de clustering, afin de regrouper les entités spatio-temporelles évoluant similairement. Nous explorons plusieurs sites d'étude qui sont décrits par des STIS pluri Dans la troisième contribution, nous introduisons une méthode d'analyse semi-supervisée basée sur du clustering par contraintes. Ces contraintes sont utilisées pour guider le processus de clustering et adapter le partitionnement aux besoins de l'utilisateur. Nous avons évalué nos travaux sur diﬀérents sites d'étude. Les résultats obtenus ont permis d'identifier des profils d'évolution types sur chaque site d'étude. En outre, nous avons aussi identifié des évolutions caractéristiques communes à plusieurs sites. Par ailleurs, la sélection de contraintes pour l'apprentissage semi-supervisé a permis d'identifier des entités profitables à l'algorithme de clustering. Ainsi, les partitionnements obtenus en utilisant l'apprentissage non supervisé ont été améliorés et adaptés aux besoins de l'utilisateur.
La pharmacovigilance souffre d'une sous notification chronique des effets indésirables de la part des professionnels de santé. La FDA (US Food and Drug Administration), l'EMA (European Medicines Agency), et d'autres agences sanitaires, suggèrent que les réseaux sociaux pourraient constituer une source de données supplémentaire pour la détection de signaux faibles de pharmacovigilance. L'OMS (Organisation Mondiale de la Santé) a publié un rapport en 2003 exposant le problème que pose la non-observance au traitement sur le long terme et son caractère préjudiciable à l'efficacité des systèmes de santé au niveau mondial. Les données nécessaires à la mise au point d'un système d'extraction d'informations de santé depuis les forums de patients sont mise à disposition par la société́ Kappa Santé. La première approche proposée s'inscrit dans un contexte de détection de cas de pharmacovigilance à partir d'échanges entre patients sur des forums de santé. Nous proposons un filtre basé sur le nombre de mots séparant le nom du médicament évoqué dans le message du terme considéré́ comme un potentiel effet indésirable. Nous proposons une seconde approche basée sur les « topic models » afin de cibler les groupes de messages abordant les thèmes traitant de non-observance. En terme de pharmacovigilance, le filtre gaussien proposé permet d'identifier 50.03% des faux positifs avec une précision de 95.8% et un rappel de 50%. L'approche de détection de cas de non-observance permet l'identification de ces derniers avec une précision de 32.6% et un rappel de 98.5%.
La « Behavioral Strategy » suggère de lier ces erreurs à la psychologie des décideurs, et notamment à leurs biais cognitifs. Toutefois, cette vision suppose de connecter le niveau d'analyse de l'individu et celui de l'organisation. Nous proposons pour ce faire un niveau « méso » , la routine de choix stratégique (RCS), où interagissent la psychologie des décideurs et les décisions stratégiques. Après avoir distingué trois types de RCS, nous formulons des hypothèses d'intervention sur celles-ci visant à prévenir les erreurs stratégiques. Nous illustrons ces hypothèses par six cas pratiques, en testons certaines par une étude quantitative, et analysons les préférences qui conduisent les dirigeants à les adopter ou non. Nous concluons en discutant les implications théoriques et pratiques de notre démarche.
Les récents développements des nouvelles technologies de l'information et de la communication font du Web une véritable mine d'information. Cependant, les pages Web sont très peu structurées. Par conséquent, il est difficile pour une machine de les traiter automatiquement pour en extraire des informations pertinentes pour une tâche ciblée. C'est pourquoi les travaux de recherche s'inscrivant dans la thématique de l'Extraction d'Information dans les pages web sont en forte croissance. Notre travail de thèse se situe à la croisée de ces deux thématiques. Notre objectif principal est de concevoir et de mettre en œuvre des stratégies permettant de scruter le web pour extraire des Entités Nommées (EN) complexes (EN composées de plusieurs propriétés pouvant être du texte ou d'autres EN) de type entreprise ou de type événement, par exemple. Nous proposons ensuite des services d'indexation et d'interrogation pour répondre à des besoins d'informations. Ces travaux ont été réalisés au sein de l'équipe T2I du LIUPPA, et font suite à une commande de l'entreprise Cogniteev, dont le cœur de métier est centré sur l'analyse du contenu du Web. Les problématiques visées sont, d'une part, l'extraction d'EN complexes sur le Web et, d'autre part, l'indexation et la recherche d'information intégrant ces EN complexes. Notre première contribution porte sur l'extraction d'EN complexes dans des textes. Pour cette contribution, nous prenons en compte plusieurs problèmes, notamment le contexte bruité caractérisant certaines propriétés (pour un événement par exemple, la page web correspondante peut contenir deux dates : la date de l'événement et celle de mise en vente des billets). Pour ce problème en particulier, nous introduisons un module de détection de blocs qui permet de focaliser l'extraction des propriétés sur des blocs de texte pertinents. Nos expérimentations montrent une nette amélioration des performances due à cette approche. Nous nous sommes également intéressés à l'extraction des adresses, où la principale difficulté découle du fait qu'aucun standard ne se soit réellement imposé comme modèle de référence. Nous proposons donc un modèle étendu et une approche d'extraction basée sur des patrons et des ressources libres. Notre deuxième contribution porte sur le calcul de similarité entre EN complexes. Dans l'état de l'art, ce calcul se fait généralement en deux étapes : (i) une première calcule les similarités entre propriétés et (ii) une deuxième agrège les scores obtenus pour le calcul de la similarité globale. Notons que nos principales propositions se situent au niveau de la deuxième étape. Ainsi, nous proposons trois techniques pour l'agrégation des scores intermédiaires. Les deux premières sont basées sur la somme pondérée des scores intermédiaires (combinaison linéaire et régression logistique). La troisième exploite les arbres de décisions pour agréger les scores intermédiaires. Son originalité vient du fait qu'elle ne nécessite pas de passer par le calcul de scores de similarités intermédiaires.
Le système de COATIS met en oeuvre une méthode opérationnelle pour passer des textes à des représentations sémantiques impliquant la notion de causalité.
Les robots sous-marins peuvent aujourd'hui évoluer dans des environnements complexes difficilement accessibles à l'Homme pour des raisons de coût ou de sécurité. Or, la complexité de ces milieux impose de doter le vecteur robotique d'une autonomie opérationnelle suffisante afin qu'il puisse mener sa mission à bien tout en préservant son intégrité. Cela nécessite de développer des lois de commande répondant aux spécificités de l'application. Ces lois de commande se basent sur des connaissances provenant de différentes disciplines scientifiques ce qui souligne l'interdisciplinarité inhérente à la robotique. Une fois la loi de commande développée, il faut implémenter le contrôleur sur le robot sous forme de logiciel de contrôle basé sur une architecture logicielle temps-réel. Or la conception actuelle des lois de commande, sous forme de blocs "monolithiques", rend difficile l'évolution d'une loi de commande d'une application à l'autre, l'intégration de connaissances provenant d'autres disciplines scientifiques que ne maitrisent pas forcément les automaticiens et pénalisent son implémentation sur des architectures logicielles qui nécessitent la modularité. Cela permettra en outre une projection plus efficace sur l'architecture logicielle. Nous proposons donc un nouveau formalisme de description des lois de commande selon une composition modulaire d'entités de base appelées Atomes et qui encapsulent les différents éléments de connaissance. Pour cela, nous enrichissons nos Atomes de contraintes chargées de véhiculer les informations relatives à ces aspects temporels. Nous proposons également une méthodologie basée sur notre formalisme afin de guider l'implémentation de nos stratégies de commande sur un Middleware temps-réel, dans notre cas le Middleware ContrACT développé au LIRMM.Nous illustrons notre approche par diverses fonctionnalités devant être mises en oeuvre lors de missions d'exploration de l'environnement aquatique et notamment pour l'évitement de parois lors de l'exploration d'un aquifère karstique.
À l'époque du démarrage des travaux présentés dans ce document (2003) l'annotation d'un génome était une tâche longue et fastidieuse. Avec l'apparition des nouvelles technologies de séquençage, de nombreux outils ont été développés pour faciliter et accélérer ce processus. Pour les meilleurs, l'annotation d'un génome automatique peut prendre moins de 3 minutes, reportant l'activité chronophage sur l'annotation manuelle. Ainsi, de nombreux génomes sont déposés dans les banques de séquences tels quels sans annotation manuelle experte. Il est donc rapidement apparu nécessaire de fournir aux annotateurs la possibilité d'accéder à des bases de données consolidées et spécifiques de leur domaine d'expertise. Nous présentons dans ce document un outil modulaire d'annotation et de visualisation, GenoBrowser, que nous avons créé dans le cadre de nos travaux de recherche dans une équipe de microbiologie. Celui-ci nous permet d'intégrer simplement de nouvelles fonctionnalités liées aux données de Omics générées dans l'équipe. L'architecture de notre outil et la création d'une API (Application Programming Interface) spécifique nous ont permis de développer et de mettre à la disposition de la communauté scientifique deux bases de données (P2CS et P2TF) dédiées aux réseaux de régulation chez les bactéries, ainsi que le serveur web associé pour la prédiction de ces systèmes pour des génomes séquencés de novo. Ce travail a permis de développer, au sein d'une équipe de recherche, un ensemble d'outils d'aide à l'expertise pour la recherche en génomique environnementale. Il nous a permis de travailler sur la consolidation et la réutilisation de la quantité croissante de données de type Omics.
Les méthodes de compréhension de la parole visent à extraire des éléments de sens pertinents du signal parlé. On distingue principalement deux catégories dans la compréhension du signal parlé : la compréhension de dialogues homme/machine et la compréhension de dialogues homme/homme. En fonction du type de conversation, la structure des dialogues et les objectifs de compréhension varient. Cependant, dans les deux cas, les systèmes automatiques reposent le plus souvent sur une étape de reconnaissance automatique de la parole pour réaliser une transcription textuelle du signal parlé. Les systèmes de reconnaissance automatique de la parole, même les plus avancés, produisent dans des contextes acoustiques complexes des transcriptions erronées ou partiellement erronées. Ces erreurs s'expliquent par la présence d'informations de natures et de fonction variées, telles que celles liées aux spécificités du locuteur ou encore l'environnement sonore. Celles-ci peuvent avoir un impact négatif important pour la compréhension. Dans un premier temps, les travaux de cette thèse montrent que l'utilisation d'autoencodeur profond permet de produire une représentation latente des transcriptions d'un plus haut niveau d'abstraction. Cette représentation permet au système de compréhension de la parole d'être plus robuste aux erreurs de transcriptions automatiques. Dans un second temps, nous proposons deux approches pour générer des représentations robustes en combinant plusieurs vues d'un même dialogue dans le but d'améliorer les performances du système la compréhension. La seconde approche propose d'introduire une forme d'information de supervision dans les processus de débruitages par autoencodeur.
À l'aide d'une méthode qualitative et inductive, partant de jugements singuliers, ce travail laisse la parole au locuteur ordinaire et essaie de relever les différents sens ainsi que les diverses fonctions que peuvent avoir les termes vague/vage dans le langage courant.
Les algorithmes de machine learning sont construits pour apprendre, à partir de données, des modèles statistiques de décision ou de prédiction, sur un large panel de tâches. Quand la distribution des donnée est complexe (e.g. grande dimension avec des interactions non-linéaires entre les variables observées), les hypothèses simplificatrices peuvent être contre-productives. Il est alors nécessaire de trouver une représentation alternatives des données avant d'apprendre le modèle de décision. Jusqu'à récemment, beaucoup de représentations standards étaient construites à la main par des experts. Dans cette thèse, nous nous sommes intéressés à l'apprentissage de représentation de séries temporelles multivariées (STM) et de graphes. STM et graphes sont des objets complexes qui ont des caractéristiques les rendant difficilement traitables par des algorithmes standards de machine learning. Par exemple, ils peuvent avoir des tailles variables et ont des alignements non-triviaux, qui empêchent l'utilisation de métriques standards pour les comparer entre eux. Il est alors nécessaire de trouver pour les échantillons observés (STM ou graphes) une représentation alternatives qui les rend comparables. Les contributions de ma thèses sont un ensemble d'analyses, d'approches pratiques et de résultats théoriques présentant des nouvelles manières d'apprendre une représentation de STM et de graphes. Deux méthodes de représentation de STM ont dédiées au suivi d'état caché de systèmes mécaniques. La première propose une représentation basée "model-based" appelée Sequence-to-graph (Seq2Graph). Une preuve d'identifiabilité et une extension à des problèmes d'analyse de survie rendent cette approche puissante pour le suivi d'état de système mécaniques. Deux méthodes de représentation de graphes pour la classification sont aussi proposées. Une second méthode propose une analyse théorique et pratique du spectre du Laplacien pour la classification de graphes.
Le cadre general de cette etude consiste en une description du syntagme nominal en portugais a partir d'un corpus de textes scientifiques et techniques (documents medicaux en portugais). Il s'agit d'un travail qui, tout en etant une etude linguistique au sens classique du terme, vise a degager un ensemble de ressources linguistiques qui puissent etre utilisees par la suite dans le traitement automatique des langues (tal) notamment le traitement documentaire et la traduction par ordinateur.
Les langues des signes (LS) se sont développées naturellement au sein des communautés de Sourds. Ne disposant pas de forme écrite, ce sont des langues orales, utilisant les canaux gestuel pour l'expression et visuel pour la réception. Ces langues peu dotées ne font pas l'objet d'un large consensus au niveau de leur description linguistique. Elles intègrent des signes lexicaux, c'est-à-dire des unités conventionnalisées du langage dont la forme est supposée arbitraire, mais aussi – et à la différence des langues vocales, si on ne considère pas la gestualité co-verbale – des structures iconiques, en utilisant l'espace pour organiser le discours. Dans cette thèse, nous souhaitons montrer les limites de cette approche, en élargissant cette perspective pour envisager la reconnaissance d'éléments utilisés pour la construction du discours ou au sein de structures illustratives. Pour ce faire, nous montrons l'intérêt et les limites des corpus de linguistes : la langue y est naturelle et les annotations parfois détaillées, mais pas toujours utilisables en données d'entrée de système d'apprentissage automatique, car pas nécessairement cohérentes. Nous proposons alors la refonte d'un corpus de dialogue en langue des signes française, Dicta-Sign-LSF-v2, avec des annotations riches et cohérentes, suivant un schéma d'annotation partagé par de nombreux linguistes. Nous proposons ensuite une redéfinition du problème de la reconnaissance automatique de LS, consistant en la reconnaissance de divers descripteurs linguistiques, plutôt que de se focaliser sur les signes lexicaux uniquement. En parallèle, nous discutons de métriques de la performance adaptées. Pour réaliser une première expérience de reconnaissance de descripteurs linguistiques non uniquement lexicaux, nous développons alors une représentation compacte et généralisable des signeurs dans les vidéos. Celle-ci est en effet réalisée par un traitement parallèle des mains, du visage et du haut du corps, en utilisant des outils existants ainsi que des modèles que nous avons développés. Un prétraitement permet alors de former un vecteur de caractéristiques pertinentes. Par la suite, nous présentons une architecture adaptée et modulaire d'apprentissage automatique de descripteurs linguistiques, consistant en un réseau de neurones récurrent et convolutionnel. Nous montrons enfin via une analyse quantitative et qualitative l'effectivité du modèle proposé, testé sur Dicta-Sign-LSF-v2. L'étude des prédictions du modèle montre alors le bien-fondé de l'approche proposée, avec une performance tout à fait intéressante pour la reconnaissance continue de quatre descripteurs linguistiques, notamment au vu de l'incertitude relative aux annotations elles-mêmes. La segmentation de ces dernières est en effet subjective, et la pertinence même des catégories utilisées n'est pas démontrée de manière forte. Indirectement, le modèle proposé pourrait donc permettre de mesurer la validité de ces catégories. Avec plusieurs pistes d'amélioration envisagées, notamment sur la représentation des signeurs et l'utilisation de corpus de taille supérieure, le bilan est très encourageant et ouvre la voie à une acception plus large de la reconnaissance continue de langue des signes.
Cette thèse concerne l'analyse automatique des SMS et l'extraction des informations qui y sont contenues. Le point de départ de notre recherche est le constat que la plupart des messages courts, observés dans le corpus alpes4science, présentent des différences en comparaison avec le langage standard. Les différences sont mises en évidence, d'une part, par la morphologie particulière des mots et, d'autre part, par les règles de syntaxe et de grammaire qui ne sont pas respectées lorsque l'émetteur considère que cela ne nuit pas à l'intelligibilité du message. À cause des écarts par rapport à la langue standard, le traitement et l'analyse des messages bruités est toujours un défi pour les tâches du TAL. Le résultat produit par ce modèle a été évalué, par la suite, pour la reconnaissance d'entités nommées au travers d'une série de tests appliqués à l'aide de trois autres systèmes. Les résultats obtenus ont montré que les performances de ces systèmes de reconnaissance d'entités nommées présentent des améliorations significatives lorsqu'ils sont appliqués sur les SMS automatiquement normalisés en comparaison avec le corpus brut et manuellement transcrit. Mots-clés : communication médiée par ordinateur, langage SMS, normalisation des SMS, extraction d'
Ces travaux de thèse portent sur la détection audio-visuelle de marqueurs affectifs (rire et sourire) et attentionnels de personnes âgées en interaction sociale avec un robot. Pour comprendre efficacement et modéliser le comportement des personnes très âgées en présence d'un robot, des données pertinentes sont nécessaires. J'ai participé à la collection d'un corpus de personnes âgées notamment pour l'enregistrement des données visuelles. Le système utilisé pour contrôler le robot est un magicien d'Oz, plusieurs scénarios de conversation au quotidien ont été utilisés pour encourager les gens à coopérer avec le robot. Ces scénarios ont été élaborés dans le cadre du projet ROMEO2 avec l'association Approche. Nous avons décrit tout d'abord le corpus recueilli qui contient 27 sujets de 85 ans en moyenne pour une durée totale de 9 heures, les annotations et nous avons discuté des résultats obtenus à partir de l'analyse des annotations et de deux questionnaires. Ma recherche se focalise ensuite sur la détection de l'attention et la détection de rire et de sourire. Les motivations pour la détection de l'attention consistent à détecter quand le sujet ne s'adresse pas au robot et à adapter le comportement du robot à la situation. Après avoir considéré les difficultés liées aux personnes âgées et les résultats d'analyse obtenus par l'étude des annotations du corpus, nous nous intéressons à la rotation de la tête au niveau de l'indice visuel et à l'énergie et la qualité de voix pour la détection du destinataire de la parole. La détection de rire et sourire peut être utilisée pour l'étude sur le profil du locuteur et de ses émotions. Mes intérêts se concentrent sur la détection de rire et sourire dans la modalité visuelle et la fusion des informations audio-visuelles afin d'améliorer la performance du système automatique. Les expressions sont différentes des expressions actées ou posés à la fois en apparence et en temps de réaction. La conception d'un système qui marche sur les données réalistes des personnes âgées est encore plus difficile à cause de plusieurs difficultés à envisager telles que le manque de données pour l'entrainement du modèle statistique, l'influence de la texture faciale et de la façon de sourire pour la détection visuelle, l'influence de la qualité vocale pour la détection auditive, la variété du temps de réaction, le niveau de compréhension auditive, la perte de la vue des personnes âgées, etc. Les systèmes de détection de la rotation de la tête, de la détection de l'attention et de la détection de rire et sourire sont évalués sur le corpus ROMEO2 et partiellement évalués (détections visuelles) sur les corpus standard Pointing04 et GENKI-4K pour comparer avec les scores des méthodes de l'état de l'art. Nous avons également trouvé une corrélation négative entre la performance de détection de rire et sourire et le nombre d'évènement de rire et sourire pour le système visuel et le système audio-visuel. Ce phénomène peut être expliqué par le fait que les personnes âgées qui sont plus intéressées par l'expérimentation rient plus souvent et sont plus à l'aise donc avec des poses variées. La variété des poses et le manque de données correspondantes amènent des difficultés pour la reconnaissance de rire et de sourire pour les systèmes statistiques. Les expérimentations montrent que la rotation de la tête peut être efficacement utilisée pour détecter la perte de l'attention du sujet dans l'interaction avec le robot. Au niveau de la détection de l'attention, le potentiel d'une méthode en cascade qui utilise les modalités d'une manière complémentaire est montré.
L'identification d'appareils photos a récemment fait l'objet d'une grande attention en raison de son apport en terme sécurité et juridique. Établir l'origine d'un médias numériques, obtenus par un appareil d'imagerie est important à chaque fois que le contenu numériques est présente et utilise comme preuve devant un tribunal. L'identification d'appareils photos consiste à déterminer la marque, le modèle, ou le dispositif qui a été utilisé pour prendre une image. Notre première contribution pour l'identification du modèle d'appareil photo numérique est basée sur l'extraction de trois ensembles de caractéristiques puis l'utilisation d'apprentissage automatique. Ces caractéristiques sont la matrice de cooccurrences, des corrélations inter-canaux mesurant la trace laissée par l'interpolation CFA, et les probabilités conditionnelles calculées dans le domaine JPEG. Ces caractéristiques donnent des statistiques d'ordre élevées qui complètent et améliorent le taux d'identification. Les expériences prouvent la force de notre proposition, car la précision obtenue est supérieure à celle des méthodes basées sur la corrélation. La deuxième contribution est basée sur l'utilisation des CNNs. Contrairement aux méthodes traditionnelles, les CNNs apprennent simultanément les caractéristiques et la classification. Nous proposons d'ajouter une couche de pré-traitement (filtre passe-haut applique à l'image d'entrée) au CNN. Le CNN obtenu donne de très bonnes performances pour une faible complexité d'apprentissage. La méthode proposée donne des résultats équivalent à ceux obtenu par une approche en deux étapes (extraction de caractéristiques + SVM). Par ailleurs nous avons également examines les CNNs : AlexNet et GoogleNet.
Tout buzz a un ou plusieurs points d'origine : les sources primaires. L'information est ensuite relayée par des sources secondaires qui vont accélérer ou non la propagation en fonction de leur degré d'influence. Tout au long du cycle de vie du buzz, le contenu sémantique est amené à évoluer. La compréhension d'un buzz sur Internet passe ainsi par l'analyse de ce qui se dit et la qualification des émetteurs. Nos travaux s'axeront donc autour de deux types d'analyses complémentaires : une analyse topologique des sources (théorie des graphes et des réseaux) et une analyse du contenu textuel (linguistique de corpus).
Le partitionnement consiste à rechercher une partition d'éléments, de sorte que les éléments d'un même cluster soient plus similaires que les éléments de différents clusters. Les données proviennent de différentes sources et prennent des formes différentes. L'un des défis consiste à concevoir un système capable de tirer parti des différentes sources de données. Certaines contraintes peuvent être connues sur les données. On peut savoir qu'un objet est d'un certain type ou que deux objets partagent le même type ou sont de types différents. On peut également savoir qu'à l'échelle globale, les différents types d'objets apparaissent avec une fréquence connue. Dans cette thèse, nous nous concentrons sur le partitionnement avec trois types de contraintes : les contraintes d'étiquettes, les contraintes de paires et les contraintes de lois de puissance. Une contrainte d'étiquette spécifie dans quel cluster appartient un objet. Les contraintes par paire spécifient que les paires d'objets doivent ou ne doivent pas partager le même cluster. Enfin, la contrainte de loi de puissance est une contrainte globale qui spécifie que la distribution des tailles de cluster est soumise à une loi de puissance. Nous voulons montrer que l'introduction de la semi-supervision aux algorithmes de clustering peut modifier et améliorer les solutions retournées par des algorithmes de clustering non supervisés. Nous contribuons à cette question en proposant des algorithmes pour chaque type de contraintes. Nos expériences sur les ensembles de données UCI et les jeux de données en langage naturel montrent la bonne performance de nos algorithmes et donnent des indications pour des travaux futurs prometteurs.
Les différents dialectes de la langue arabe (DA) présentent de grandes variations phonologiques, morphologiques, lexicales et syntaxiques par rapport à la langue Arabe Standard Moderne (MSA). Jusqu'à récemment, ces dialectes n'étaient présents que sous leurs formes orales et la plupart des ressources existantes pour la langue arabe se limite à l'Arabe Standard (MSA), conduisant à une abondance d'outils pour le traitement automatique de cette variété. Étant donné les différences significatives entre le MSA et les DA, les performances de ces outils s'écroulent lors du traitement des DA. La présence de ce dernier d'une manière désordonnée dans le discours pose une sérieuse problématique pour le Traitement Automatique de Langue et fait de cet oral une langue peu dotée. Toutefois, les ressources nécessaires pour modéliser cet oral sont quasiment inexistantes. Ainsi, l'objectif de cette thèse consiste à pallier ce manque afin de construire un modèle de langage dédié à un système de reconnaissance automatique pour l'oral parlé dans les médias tunisiens. Pour ce fait, nous décrivons dans cette thèse une méthodologie de création de ressources et nous l'évaluons par rapport à une tâche de modélisation de langage. Les résultats obtenu sont encourageants.
Ce travail s'inscrit dans le domaine du contrôle performatif de la synthèse vocale, et plus particulièrement de la modification temps-réel de signaux de voix pré-enregistrés. Dans un contexte où de tels systèmes n'étaient en mesure de modifier que des paramètres de hauteur, de durée et de qualité vocale, nos travaux étaient centrés sur la question de la modification performative du rythme de la voix. Une grande partie de ce travail de thèse a été consacrée au développement de Vokinesis, un logiciel de modification performative de signaux de voix pré-enregistrés. Il a été développé selon 4 objectifs : permettre le contrôle du rythme de la voix, avoir un système modulaire, utilisable en situation de concert ainsi que pour des applications de recherche. Son développement a nécessité une réflexion sur la nature du rythme vocal et sur la façon dont il doit être contrôlé. Il est alors apparu que l'unité rythmique inter-linguistique de base pour la production du rythme vocale est de l'ordre de la syllabe, mais que les règles de syllabification sont trop variables d'un langage à l'autre pour permettre de définir un motif rythmique inter-linguistique invariant. Nous avons alors pu montrer que le séquencement précis et expressif du rythme vocal nécessite le contrôle de deux phases, qui assemblées forment un groupe rythmique : le noyau et la liaison rythmiques. Nous avons mis en place plusieurs méthodes de contrôle rythmique que nous avons testées avec différentes interfaces de contrôle. Une évaluation objective a permis de valider l'une de nos méthodes du point de vue de la précision du contrôle rythmique. De nouvelles stratégies de contrôle de la hauteur et de paramètres de qualité vocale avec une tablette graphique ont été mises en place. L'utilisation musicale de Vokinesis a été évaluée avec succès dans le cadre de représentations publiques du Chorus Digitalis, pour du chant de type variété ou musique contemporaine. Les perspectives d'application sont multiples : études scientifiques (recherches en prosodie, en parole expressive, en neurosciences...), productions sonores et musicales, pédagogie des langues, thérapies vocales.
La présente recherche s'interroge sur la présumée dichotomie entre les alternances et les généralisations de surface dans le cadre théorique de la grammaire de constructions. Plus précisément,l'objectif de cette thèse est ternaire. Par l'analyse attentive d'une grande quantité de données, nous faisons une description détaillée de l'alternance causative en anglais (The fabric stretched vs. Joan stretched the fabric), nous proposons une méthode qui permet de mesurer la force d'alternance des verbes ainsi que la quantité de sens partagée entre les deux constructions, et, enfin, nous montrons que si l'on veut rendre compte des contraintes au niveau de la construction, l'on doit alors prendre en compte les généralisations de plus bas niveau, telles que les interactions entre le verbe et ses arguments dans le cadre de chaque construction. Afin d'ajouter au débat entre alternance et généralisations de surface, nous proposons une analyse détaillée des deux constructions qui forment l'alternance causative en anglais : la construction intransitive non-causative d'une part et la construction transitive causative de l'autre. Notre but est de mesurer la quantité de sens partagée par les deux constructions mais aussi démontrer en quoi ces deux constructions diffèrent. Dans cette optique, nous prenons en compte trois éléments : construction, verbe et thème (i.e. l'entité sujette à l'évènement dénoté par le verbe). Nous utilisons la sémantique distributionnelle pour la mesure des similarités sémantiques entre les divers thèmes employés avec chaque verbe dans chaque construction dans notre corpus. Ce groupement sémantique met en lumière les différents sens verbaux employés avec chaque construction et nous permet d'établir des généralisations quant aux contraintes qui s'appliquent au thème dans chaque construction.
La quantité d'information disponible dans le domaine biomédical ne cesse d'augmenter. Pour que cette information soit facilement utilisable par les experts d'un domaine, il est nécessaire de l'extraire et de la structurer. Pour avoir des données structurées, il convient de détecter les relations existantes entre les entités dans les textes. Nos recherches se sont focalisées sur la question de l'extraction de relations complexes représentant des résultats expérimentaux, et sur la détection et la catégorisation de relations binaires entre des entités biomédicales. Nous nous sommes intéressée aux résultats expérimentaux présentés dans les articles scientifiques. Ces résultats sont importants pour les experts en biologie, par exemple pour faire de la modélisation. Dans le domaine de la physiologie rénale, une base de données a été créée pour centraliser ces résultats d'expérimentation, mais l'alimentation de la base est manuelle et de ce fait longue. Nous proposons une solution pour extraire automatiquement des articles scientifiques les connaissances pertinentes pour la base de données, c'est-à-dire des résultats expérimentaux que nous représentons par une relation n-aire. La méthode procède en deux étapes : extraction automatique des documents et proposition de celles-ci pour validation ou modification par l'expert via une interface. Nous avons également proposé une méthode à base d'apprentissage automatique pour l'extraction et la classification de relations binaires en domaine de spécialité. Nous nous sommes intéressée aux caractéristiques et variétés d'expressions des relations, et à la prise en compte de ces caractéristiques dans un système à base d'apprentissage. Nous avons étudié la prise en compte de la structure syntaxique de la phrase et la simplification de phrases dirigée pour la tâche d'extraction de relations. Nous avons en particulier développé une méthode de simplification à base d'apprentissage automatique, qui utilise en cascade plusieurs classifieurs.
Depuis le début de la décennie, les réseaux de neurones convolutifs profonds pour le traitement d'images ont démontré leur capacité à produire d'excellent résultats. Pour cela, ces modèles transforment une image en une succession de représentations latentes. Dans cette thèse, nous travaillerons à l'amélioration de la qualité de ces représentations latentes. Dans un second temps, nous proposons de structurer l'information en deux sous-espaces latents complémentaires, résolvant un conflit entre l'invariance des représentations et la reconstruction. La structuration en deux espaces permet ainsi de relâcher la contrainte posée par les architectures classiques, permettant ainsi d'obtenir de meilleurs résultats en classification semi-supervisé. Enfin, nous nous intéressons au disentangling, c'est-à-dire la séparation de facteurs sémantiques indépendants. Nous poursuivons nos travaux de structuration des espaces latent et utilisons des coûts adverses pour assurer une séparation efficace de l'information. Cela permet d'améliorer la qualité des représentations ainsi que l'édition sémantique d'images.
L'étude probabiliste de sûreté (EPS) parasismique est l'une des méthodologies les plus utilisées pour évaluer et assurer la performance des infrastructures critiques, telles que les centrales nucléaires, sous excitations sismiques. La thèse discute sur les aspects suivants :  (i) Construction de méta-modèles avec les réseaux de neurones pour construire les relations entre les intensités sismiques et les paramètres de demande des structures, afin d'accélérer l'analyse de fragilité. L'incertitude liée à la substitution des modèles des éléments finis par les réseaux de neurones est étudiée. (ii) Proposition d'une méthodologie bayésienne avec réseaux de neurones adaptatifs, afin de prendre en compte les différentes sources d'information, y compris les résultats des simulations numériques, les valeurs de référence fournies dansla littérature et les évaluations post-sismiques, dans le calcul de courbes de fragilité. (iii) Calcul des lois d'atténuation avec les réseaux de neurones. Les incertitudes épistémiques des paramètres d'entrée de lois d'atténuation, tels que la magnitude et la vitesse moyenne des ondes de cisaillement de trente mètres, sont prises en compte dans la méthodologie développée. (iv) Calcul du taux de défaillance annuel en combinant les résultats des analyses de fragilité et de l'aléa sismique. Les courbes de fragilité sont déterminées parle réseau de neurones adaptatif, tandis que les courbes d'aléa sont obtenues à partir des lois d'atténuation construites avec les réseaux de neurones. Les méthodologies proposées sont appliquées à plusieurs cas industriels, tels que le benchmark KARISMA et le modèle SMART.
Les relations paraphrastiques entre plusieurs ensembles de paraphrases peuvent se décrire en termes de suites de transformations textuelles. Pour qu'il ait paraphrase, il faut qu'une substitution lexicale noyau se mette en route entrainant d'autres modifications syntaxiques, lexicales et morphologiques. Après avoir décrit les mécanismes de paraphrasage récurrents, nous avons proposé deux formalisations. La première est théorique et explique les différentes relations paraphrastiques entretenues par les paraphrases entre-elles. La deuxième formalise les structures paraphrastiques sous-forme de prédicats-arguments. Nous considérons cette dernière adaptée au traitement automatique de la paraphrase. Nous avons à la suite implémenté un système d'extraction de structures paraphrastiques. Il s'agit d'un système opérationnel appliqué à un volume de données relevant de notre domaine d'étude, et dont le but est de donner un exemple concret d'emploi possible de notre formalisation.
Au cours des deux dernières décennies les objets connectés ont révolutionné la traçabilité des phénomènes sociaux. Les trajectoires sociales laissent aujourd'hui des traces numériques, qui peuvent être analysées pour obtenir une compréhension plus profonde des comportements collectifs. L'essor de grands réseaux sociaux (comme Facebook, Twitter et plus généralement les réseaux de communication mobile) et d'infrastructures connectées (comme les réseaux de transports publiques et les plate-formes en ligne géolocalisées) ont permis la constitution de grands jeux de données temporelles. Ces nouveaux jeux de données nous donnent l'occasion de développer de nouvelles méthodes pour analyser les dynamiques temporelles de et dans ces systèmes. De nos jours, la pluralité des données nécessite d'adapter et combiner une pluralité de méthodes déjà existantes pour élargir la vision globale que l'on a de ces systèmes complexes. Le but de cette thèse est d'explorer les dynamiques des systèmes sociaux au moyen de trois groupes d'outils : les réseaux complexes, la physique statistique et l'apprentissage automatique. Le troisième chapitre montre la valeur ajoutée de l'utilisation de jeux de données temporelles. Puis, nous analysons les résultats d'un algorithme d'apprentissage automatique non supervisé ayant pour but de classer les utilisateurs en fonction de leurs profils. Le quatrième chapitre explore les différences entre une méthode globale et une méthode locale de détection de communautés temporelles sur des réseaux scientométriques. Le dernier chapitre combine l'analyse de réseaux complexes et l'apprentissage automatique supervisé pour décrire et prédire l'impact de l'introduction de nouveaux commerces sur les commerces existants. Nous explorons l'évolution temporelle de l'impact et montrons le bénéfice de l'utilisation de mesures de topologies de réseaux avec des algorithmes d'apprentissage automatique.
Le but de cette thèse est de construire un fragment de grammaire du français rendant compte de la syntaxe de l'infinitif dans le cadre des grammaires d'arbres polychromes (GAP). La thèse se compose de deux parties. La première concerne l'étude des problèmes que pose l'infinitif. L'infinitif, comme un verbe, a des compléments et le constituant qu'il forme avec ses compléments peut être lui aussi un complément d'un verbe, d'un nom, d'un adjectif. Ce que les grammaires traditionnelles traduisent en parlant d'une double nature : il possède à la fois des propriétés nominales et verbales. D'où une difficulté spécifique pour faire entrer la syntaxe de l'infinitif dans un modèle formel qui puisse se prêter au traitement automatique. Nous appelons constituant infinitif l'unité composée d'un verbe infinitif et de ses compléments. Il est expliqué pourquoi ce terme est préféré à celui de « proposition subordonnée infinitive » . La seconde partie aborde l'analyse syntaxique des constituants infinitifs en GAP et s'organise autour des contextes dans lesquels apparaît l'infinitif (un verbe, un nom, un adjectif). Le choix d'une représentation en GAP permet de mettre à l'épreuve ce formalisme et de montrer l'intérêt qu'il y a de séparer les fonctions syntaxiques des catégories. Ainsi peut-on rendre compte des cas où un constituant d'une catégorie non nominale occupe une position en général occupée par un nom. Ce travail aura permis, en s'intéressant à la question de la syntaxe de l'infinitif, en partant d'une réinterrogation de certaines études en provenance des grammaires traditionnelles, de parvenir à l'enrichissement d'un formalisme élaboré dans la perspective du TAL.
La problématique traitée dans cette thèse vise à développer une stratégie efficace de détection et de classification des impacts en présence d'incertitudes de modélisation du robot et de son environnement et en utilisant un nombre minimal de capteurs, notamment en l'absence de capteur d'effort. La première partie de la thèse porte sur la détection d'un impact pouvant avoir lieu à n'importe quel endroit du bras robotique et à n'importe quel moment de sa trajectoire. Les méthodes de détection d'impacts sont généralement basées sur un modèle dynamique du système, ce qui les rend sujettes au compromis entre sensibilité de détection et robustesse aux incertitudes de modélisation. A cet égard, une méthodologie quantitative a d'abord été mise au point pour rendre explicite la contribution des erreurs induites par les incertitudes de modèle. Cette méthodologie a été appliquée à différentes stratégies de détection, basées soit sur une estimation directe du couple extérieur, soit sur l'utilisation d'observateurs de perturbation, dans le cas d'une modélisation parfaitement rigide ou à articulations flexibles. Une comparaison du type et de la structure des erreurs qui en découlent et de leurs conséquences sur la détection d'impacts en a été déduite. Dans une deuxième étape, de nouvelles stratégies de détection d'impacts ont été conçues : les effets dynamiques des impacts sont isolés en déterminant la marge d'erreur maximale due aux incertitudes de modèle à l'aide d'une approche stochastique. Une fois l'impact détecté et afin de déclencher la réaction post-impact du robot la plus appropriée, la deuxième partie de la thèse aborde l'étape de classification. En particulier, la distinction entre un contact intentionnel (l'opérateur interagit intentionnellement avec le robot, par exemple pour reconfigurer la tâche) et un contact non-désiré (un sujet humain heurte accidentellement le robot), ainsi que la localisation du contact sur le robot, est étudiée en utilisant des techniques d'apprentissage supervisé et plus spécifiquement des réseaux de neurones feedforward. La généralisation à plusieurs sujet humains et à différentes trajectoires du robot a été étudiée.
Cette thèse de doctorat porte sur la croissance « exclusive » et la vulnérabilité qui caractérisent le développement du Vietnam et plus largement de l'Asie aujourd'hui. Les chapitres traitent trois aspects importants de la vulnérabilité et du développement inclusif, à savoir : l'informalité (chapitre 1), le dilemme de l'éducation (chapitre 2) et l'emploi atypique (chapitre 3). Le chapitre 2 se concentre sur la variation des rendements de l'enseignement supérieur dans la population vietnamienne en recourant à différents modèles d'estimation. Le chapitre 3 est la première étude qui étudie systématiquement les écarts de salaire induits par le statut de travail temporaire dans les pays en développement d'Asie. Dans l'ensemble, toute la thèse implique que le capital humain, l'emploi et le revenu sont des facettes interdépendantes du bien-être individuel et que certains phénomènes de développement doivent être analysés dans leur hétérogénéité.
La protection de l'environnement naturel constitue un enjeu déterminant pour le futur de l'humanité. L'ESS, qui partage les principes du développement durable, est particulièrement bien placée pour mettre en œuvre des alternatives de développement plus écologiques. La thèse appréhende les organisations de l'ESS sous l'angle de l'identité organisationnelle et s'intéresse d'une part à la communication environnementale, d'autre part aux actions concrètes. L'étude de la communication environnementale a pour terrain le réseau social Twitter. Elle s'appuie sur un programme codé en Python, et sur les techniques d'exploration automatique de texte. Elle permet de mettre en évidence plusieurs stratégies rhétoriques. Une seconde étude traite de sept cas, sur la base d'entretiens semi-directifs. Elle met en lumière le rôle de l'engagement individuel mais aussi des logiques collectives dans l'action environnementale. Ce travail apporte une contribution méthodologique en développant l'approche de l'exploration automatique de texte, peu utilisée dans les Sciences de Gestion. Sur le plan théorique, la thèse introduit la dimension collective en tant qu'identité organisationnelle de l'ESS. Nous adaptons ensuite un modèle d'action environnementale en identifiant un déterminant supplémentaire spécifique à ces organisations. Finalement, la recherche invite l'ESS à remettre au centre les questions d'écologie, et donne des pistes pour soutenir les organisations dans une démarche environnementale.
Cependant, la plupart de ces travaux de recherches furent orientés sur des données statiques pour des utilisateurs experts. Dernièrement, des évolutions technologique et sociétales ont eu pour effet de rendre les données de plus en plus dynamiques et accessibles pour une population plus diverse. Par exemple des flux de données tels que les emails, les mises à jours de statuts sur les réseaux sociaux, les flux RSS, les systèmes de gestion de versions, et bien d'autres. Ces nouveaux types de données sont utilisés par une population qui n'est pas forcément entraînée ou éduquée à utiliser des visualisations de données. La plupart de ces personnes sont des utilisateurs occasionnels, d'autres utilisent très souvent ces données dans leurs travaux. Dans les deux cas, il est probable que ces personnes n'aient pas reçu de formation formelle en visualisation de données. Ces changements technologiques et sociétaux ont généré une multitude de nouveaux défis, car la plupart des techniques de visualisations sont conçues pour des experts et des bases de données statiques. Peu d'études ont été conduites pour explorer ces défis. Dans ce rapport de thèse, j'adresse la question suivante : « Peut-­on permettre à des utilisateurs non­-experts de créer leur propre visualisation et de contribuer à l'analyse de flux de données ? »  La première étape pour répondre à cette question est d'évaluer si des personnes non formées à la visualisation d'informations ou aux « data sciences » peuvent effectuer des tâches d'analyse de données dynamiques utiles, en utilisant un système de visualisation adapté pour supporter cette tâche. Dans la première partie de cette dissertation, je présente différents scénarios et systèmes, qui permettent à des utilisateurs non­-experts (de 20 à 300 ou 2000 à 700 000 personnes) d'utiliser la visualisation d'informations pour analyser des données dynamiques. Un autre problème important est le manque de principes génériques de design pour l'encodage visuel de visualisations d'informations dynamiques. Dans cette dissertation, je conçois, définis, et explore un espace de design pour représenter des donnés dynamiques pour des utilisateurs non­-experts. Cette espace de design est structuré par des jetons graphiques représentant des éléments de données qui permettent de construire dans le temps différentes visualisations, tant classiques que nouvelles. Ce paradigme est inspiré par des théories établies en psychologie du développement, tout autant que par des pratiques passées et présentes de création de visualisation à partir d'objets tangibles. Je décris tout d'abord les composants et processus de bases qui structurent ce paradigme. Ensuite, j'utiliserai cette description pour étudier "si et comment" des utilisateur non­-experts sont capables de créer, discuter, et mettre à jour leurs propres visualisations. Cette étude nous permettra de réviser notre modèle précédent et de fournir une première exploration des phénomènes relatifs à la création d'encodages visuels par des utilisateurs non­-experts sans logiciel. En résumé, cette thèse contribue à la compréhension des visualisations dynamiques pour des utilisateurs non­-experts.
Les robots sont les futurs compagnons et équipiers de demain. Cependant, nous sommes encore loin d'un vrai robot autonome, qui agirait de manière naturelle, efficace et sécurisée avec l'homme. Afin de doter le robot de la capacité d'agir naturellement avec l'homme, il est important d'étudier dans un premier temps comment les hommes agissent entre eux. Cette thèse commence donc par un état de l'art sur l'action conjointe en psychologie et philosophie avant d'aborder la mise en application des principes tirés de cette étude à l'action conjointe homme-robot. Nous décrirons ensuite le module de supervision pour l'interaction homme-robot développé durant la thèse. Une partie des travaux présentés dans cette thèse porte sur la gestion de ce que l'on appelle un plan partagé. Ici un plan partagé est une séquence d'actions partiellement ordonnées à effectuer par l'homme et/ou le robot afin d'atteindre un but donné. Dans un premier temps, nous présenterons comment le robot estime l'état des connaissances des hommes avec qui il collabore concernant le plan partagé (appelées états mentaux) et les prend en compte pendant l'exécution du plan. Cela permet au robot de communiquer de manière pertinente sur les potentielles divergences entre ses croyances et celles des hommes. Puis, dans un second temps, nous présenterons l'abstraction de ces plan partagés et le report de certaines décisions. En effet, dans les précédents travaux, le robot prenait en avance toutes les décisions concernant le plan partagé (qui va effectuer quelle action, quels objets utiliser...) ce qui pouvait être contraignant et perçu comme non naturel par l'homme lors de l'exécution car cela pouvait lui imposer une solution par rapport à une autre. Ces travaux vise à permettre au robot d'identifier quelles décisions peuvent être reportées à l'exécution et de gérer leur résolutions suivant le comportement de l'homme afin d'obtenir un comportement du robot plus fluide et naturel. Le système complet de gestions des plan partagés à été évalué en simulation et en situation réelle lors d'une étude utilisateur. Par la suite, nous présenterons nos travaux portant sur la communication non-verbale nécessaire lors de de l'action conjointe homme-robot. Ces travaux sont ici focalisés sur l'usage de la tête du robot, cette dernière permettant de transmettre des informations concernant ce que fait le robot et ce qu'il comprend de ce que fait l'homme, ainsi que des signaux de coordination. Finalement, il sera présenté comment coupler planification et apprentissage afin de permettre au robot d'être plus efficace lors de sa prise de décision. L'idée, inspirée par des études de neurosciences, est de limiter l'utilisation de la planification (adaptée au contexte de l'interaction homme-robot mais coûteuse) en laissant la main au module d'apprentissage lorsque le robot se trouve en situation "connue". Les premiers résultats obtenus démontrent sur le principe l'efficacité de la solution proposée.
Les services de microblogging (comme Twitter ou Sina Weibo) sont devenu ces dernières années des plateformes très importantes de partage d'information sur l'Internet. L'analyse de la diffusion d'information dans les microblogs nécessite la collecte de donnée des microblogs, la modélisation de la diffusion d'information et l'application des modèles résultants. Traiter les données massives issues des microblogs est un défi en soi. Concevoir des algorithmes efficaces et sans biais afin d'échantillonner les microblogs est ainsi fondamental. Ceci doit prendre en compte la complexité du phénomène de «  retweet  » qui dépend de la valeur éphémère de l'information, de la topologie du réseau de microblogging et des caractéristiques particulières des éditeurs et retweeteurs. Deux modèles ont été traditionnellement appliqués à la diffusion d'information  : les cascades indépendantes et modèle à seuil linéaire. Aucun de ces deux modèles n'est à même de décrire le processus du retweeting de façon correcte. Il devient donc nécessaire de de caractériser la diffusion d'information. De plus, une description complète de la relation entre la diffusion d'information dans les microblogs et de popularité des termes recherchés sur Internet serait utile. Ces travaux de thèse présentent une analyse complète de la diffusion d'information dans les microblogs. Les contributions ce cette thèse sont les suivantes  :  1) Il y'a deux technique d'échantillonnage sans biais pour les réseaux sociaux  : la marche aléatoire de Métropolis-Hastings (MHRW), et la méthode d'échantillonnage sans biais de graphes dirigés (USDSG). Néanmoins ces deux méthodes peuvent aboutit à un taux important d'auto-échantillonnage quand elles sont appliquées à des microblogs. Pour résoudre ce problème, j'ai modélisé l'échantillonnage d'un OSN par un processus de Markov et j'en ai déduit les conditions nécessaires et suffisantes d'un échantillonnage sans biais. L'évaluation empirique montre que la moyenne des dégrées des nœuds échantillonnés est proche de la vérité terrain alors que pour MHRW et USDSG elle est 2 à 4 fois supérieure. 2) La seconde contribution de cette thèse vise les lacunes des modèles en cascades indépendantes et de seuils linéaires. J'ai développé un modèle fondé sur les processus de Galton-Watson avec mort (GWK) qui prennent en compte tous les facteurs importants du processus de retweet. Ce nouveau modèle est validé par une application sur des données issues de Twitter et de Weibo. 3) La troisième contribution est relative au développement d'un modèle économique du marché des acteurs actifs dans le domaine du marketing sur les mots clés dans les sites de recherches. J'ai développé des méthodes de gestion de portfolios de mots clés et montrés que ces portfolios permettent d'améliorer fortement les rendements sans augmenter le niveau de risque.
Ces dernières années, avec l'apparition des sites tels que Youtube, Dailymotion ou encore Blip TV, le nombre de vidéos disponibles sur Internet aconsidérablement augmenté. Le volume des collections et leur absence de structure limite l'accès par le contenu à ces données. Le résumé automatique est un moyen de produire des synthèses qui extraient l'essentiel des contenus et les présentent de façon aussi concise que possible. Dans ce travail, nous nous intéressons aux méthodes de résumé vidéo par extraction, basées sur l'analyse du canal audio. Nous traitons les différents verrous scientifiques liés à cet objectif : l'extraction des contenus, la structuration des documents, la définition et l'estimation des fonctions d'intérêts et des algorithmes de composition des résumés. Sur chacun de ces aspects, nous faisons des propositions concrètes qui sont évaluées. Sur l'extraction des contenus, nous présentons une méthode rapide de détection de termes. La principale originalité de cette méthode est qu'elle repose sur la construction d'un détecteur en fonction des termes cherchés. Nous montrons que cette stratégie d'auto-organisation du détecteur améliore la robustesse du système, qui dépasse sensiblement celle de l'approche classique basée sur la transcription automatique de la parole. Nous présentons ensuite une méthode de filtrage qui repose sur les modèles à mixtures de Gaussiennes et l'analyse factorielle telle qu'elle a été utilisée récemment en identification du locuteur. L'originalité de notre contribution tient à l'utilisation des décompositions par analyse factorielle pour l'estimation supervisée de filtres opérants dans le domaine cepstral. Nous abordons ensuite les questions de structuration de collections de vidéos. Nous montrons que l'utilisation de différents niveaux de représentation et de différentes sources d'informations permet de caractériser le style éditorial d'une vidéo en se basant principalement sur l'analyse de la source audio, alors que la plupart des travaux précédents suggéraient que l'essentiel de l'information relative au genre était contenue dans l'image. Le troisième axe de ce travail concerne le résumé lui-même. Dans le cadre du résumé automatique vidéo, nous essayons, dans un premier temps, de définir ce qu'est une vue synthétique. S'agit-il de ce qui le caractérise globalement ou de ce qu'un utilisateur en retiendra (par exemple un moment émouvant, drôle.... Nous proposons ensuite un algorithme de recherche du résumé d'intérêt maximal qui dérive de celui introduit dans des travaux précédents, basé sur la programmation linéaire en nombres entiers.
Les ressources lexicographiques bilingues des expressions polylexicales (EPL) sont encore rares et elles le sont davantage pour le couple de langues français / arabe qui nous intéresse. Les recherches concernant ce couple de langues existent peu. Nous nous intéressons dans notre projet de recherche à extraire, à partir de corpus parallèle et comparable fr - ar spécialisés, des expressions polylexicales notamment à base verbale. Ces constructions, de notre point de vue, constituent un aspect pertinent à étudier dans les textes spécialisés. Le corpus constitué se rattachera au domaine médical, un domaine stratégique à l'heure actuelle où le monde entier fait face à une pandémie. Nous profiterons donc de la disponibilité des articles scientifiques vulgarisés qui abondent en ce moment. Dans notre étude nous adopterons une approche contrastive qui consiste à recenser et analyser les EPL et leurs équivalences traductionnelles, et ceci dans une approche corpus based ou basé sur corpus. Notre objectif principal est d'approfondir les recherches sur ce couple de langues encore trop peu étudié dans ce domaine, et de fournir des ressources pour les traducteurs et les apprenants de la traduction spécialisée. Il s'agira in fine d'exploiter les résultats dans le domaine de l'apprentissage de la traduction spécialisé assistée par ordinateur, en s'inspirant de l'approche d'Apprentissage guidée par les données, ou Data Driven Learning. Toutes les étapes seront réalisées en exploitant les outils TAL appropriés à chaque langue et à chaque procédure.
Il en découle un découpage hiérarchique des différentes tâches à réaliser afin d'analyser un énoncé. Les systèmes classiques du TAL reposent sur des analyseurs indépendants disposés en cascade au sein de chaînes de traitement (pipelines). Cette approche présente un certain nombre de limitations : la dépendance des modèles à la sélection empirique des traits, le cumul des erreurs dans le pipeline et la sensibilité au changement de domaine. Ces limitations peuvent conduire à des pertes de performances particulièrement importantes lorsqu'il existe un décalage entre les conditions d'apprentissage des modèles et celles d'utilisation. Un tel décalage existe lors de l'analyse de transcriptions automatiques de parole spontanée comme par exemple les conversations téléphoniques enregistrées dans des centres d'appels.
Le tuffeau est un matériau de construction calcaire largement utilisé dans la vallée de la Loire pour les bâtiments historiques. Le temps passant, il est confronté à des problèmes de pathologies, principalement liées à sa teneur en eau. Pour évaluer cet indicateur, le radar à sauts de fréquence (SFR) a été retenu parmi les techniques d'évaluation non destructives. Cette dernière peut être associée à une technique d'inversion « forme d'onde » dans le domaine fréquentiel (FWFI), technique performante pour obtenir la permittivité, directement en lien avec la teneur en eau. L'inversion FWFI d'un signal SFR utilise, pour le calcul direct, un modèle analytique de propagation des ondes, basé sur les fonctions de Green d'un dipôle, et sur le modèle de dispersion de Jonscher pour la caractérisation EM du milieu. Ce modèle direct est validé par une approche numérique utilisant le logiciel HFSS et par une approche expérimentale sur des blocs tests. Le problème inverse, comprenant une fonction Objectif et un algorithme de minimisation de type génétique, est ensuite étudié pour obtenir la permittivité du milieu ausculté. En parallèle, une base de données entre teneur en eau et permittivité du tuffeau est construite à l'aide de mesures EM en cellule sur des échantillons homogènes. Enfin, cette méthode est validée sur plusieurs applications pratiques, comprenant l'acquisition d'informations géométriques (estimation d'un front d'eau) et la caractérisation du matériau (estimation du gradient d'eau et cartographie hydrique).
Cette thèse est constituée de deux volets, théorique et pratique. Nous aborderons également le lien indéfectible existant entre la terminographie et la traduction spécialisée tant au niveau didactique que professionnel. Dans un second temps, à l'aide des extracteurs de termes, nous entamerons l'opération d'extraction des termes à partir du corpus délimité. Après avoir mené un tri des termes générés selon des critères linguistiques bien définis, ces termes feront l'objet d'une analyse lexicale et morphologique afin de souligner les moyens dont la langue arabe dispose pour la formation des termes ainsi que l'effet de son contact avec les autres langues. De plus, nous mènerons une enquête de terrain, par le biais d'un questionnaire destiné aux traducteurs et interprètes de conférence, dans le but d'examiner leurs besoins urgents en matière de terminologie médicale et évaluer leur usage des outils de gestion terminologique disponibles. Cette étude s'est donc fixée plusieurs finalités à savoir, répondre aux besoins de dénominations urgents dans le domaine médical et mettre à la disposition des traducteurs et interprètes de conférence un outil de gestion terminologique aisément accessible pour les assister dans leur travail exigeant la pertinence et la rapidité. Elle tentera également à ouvrir de nouveaux horizons sur les méthodes d'enseignement de la traduction spécialisée en soulignant la contribution considérable de la terminotique dans l'acquisition des compétences professionnelles dans ce domaine. L'étude se penchera également sur l'évaluation de l'utilisation des logiciels du TAL afin de dénoter leurs apports et limites dans l'extraction terminologique, ce qui pourrait guider les concepteurs pour améliorer la performance des outils développés.
Alors que l'efficacité et l'intérêt des dispositifs d'enseignement bilingue précoce ont été largement démontrés, peu d'études se sont penchées en détail sur les processus à l'oeuvre dans l'acquisition précoce d'une langue étrangère. Cette étude vise à analyser le développement de la compétence en L2 chez de jeunes enfants, dans un contexte où l'école est la seule source d'exposition à celle-ci. À partir d'un corpus recueilli dans des classes de maternelle et primaire de deux écoles franco-américaines en Californie, nous proposons d'examiner comment des enfants anglophones de 4 à 7 ans scolarisés dans un dispositif de type immersif construisent leur compétence en français. En nous appuyant sur un cadre interactionniste d'acquisition de la L2, nous montrerons comment ce contexte spécifique impacte le déroulement de l'acquisition, en termes d'opportunités d'interaction et d'input adressé aux enfants, et examinerons dans quelle mesure il fournit un socle aux premières acquisitions en français. Nous suggérons que l'idée selon laquelle commencer l'apprentissage d'une L2 le plus jeune possible serait garant d'une meilleure maîtrise de la langue n'est pas nécessairement valable dans ce contexte particulier d'acquisition, tant de multiples autres facteurs sont à prendre en compte (conscience linguistique et métalinguistique, stratégies d'interaction, influence de la L1). Des pistes pédagogiques sont également proposées à partir de ces résultats, afin de fournir quelques orientations pour l'enseignement bilingue en maternelle.
Cette thèse propose une analyse sociophonétique synchronique et diachronique de l'anglais de Tyneside à partir de deux sous-corpus du Corpus Diachronique de l'Anglais de Tyneside (DECTE) datant des années 1970 et de 1990 (Corrigan, Buchstaller, Mearns, and Moisl, 2012). Elle comporte deux grands volets : (1) une analyse de la variation inter et intra-locuteurs par le biais de transcriptions phonétiques des variantes linguistiques de FACE, GOAT, PRICE et MOUTH (Wells 1982) à l'aide d'une analyse factorielle multiple (AFM, Escofier 2008, Husson et al. 2011) (2) une étude acoustique des trajectoires formantiques de ces quatre ensembles lexicaux à l'aide de modèles mixtes additifs généralisés afin de vérifier la pertinence du codage (GAMMs, Wood 2015). Pour ce premier volet, nous proposons un profilage sociolinguistique de 44 locuteurs de Gateshead et de Newcastle, à partir de données phonétiques transcrites dans les années 1970 lors de l'Enquête Linguistique de Tyneside (TLS, Strang 1968). Bien que notre analyse porte sur la totalité des transcriptions du système phonétique des locuteurs, l'accent est davantage porté sur FACE, GOAT, PRICE and MOUTH. Selon l'AFM suivi d'une classification, FACE est l'ensemble lexical le plus déterminant dans la catégorisation sociolinguistique des locuteurs. La symétrie entre FACE et GOAT (Watt 1999), PRICE et MOUTH est plus nette chez les femmes : celles de la classe moyenne privilégient une diphtongue fermantes dans entre FACE et GOAT et une attaque de diphthongue ouverte pour PRICE et MOUTH, tandis que les femmes issues de classes plus populaires optent pour la monophtongue pan-régionale pour FACE et GOAT, avec une attaque davantage fermée et antérieure chez PRICE et MOUTH. La monophtongue centrale de GOAT la variante privilégiée par des hommes à l'accent local moins marqué, ce qui entre en cohérence avec les résultats de Watt (1998) dans le sous-corpus des années 1990 du DECTE. Le second volet analyse les trajectoires formantiques de FACE, GOAT et PRICE. Le but premier de cette analyse est de vérifier la correspondance des transcriptions avec le contour formantique. Les résultats confirment la pertinence du codage au niveau des liste de mots (TLS and PVC). Les différences entre les deux variantes principales de PRICE ([aɪ] vs. [eɪ]) se révèlent être foncièrement différentes tant sur le plan de l'attaque, de la trajectoire et de la cible.
Les progrès des technologies de l'information, comme les technologies de la communication et la technologie des bases de données, nous permettent d'accéder à de vastes quantités d'informations et provoquent une augmentation exponentielle du nombre de documents disponibles en ligne. Une extraction et une extraction efficaces deviennent de plus en plus difficiles. La représentation de document vise à représenter l'entrée de document dans un vecteur de longueur fixe pour réduire la complexité des documents et les rendre plus faciles à manipuler. Par conséquent, la représentation de documents joue un rôle important dans de nombreuses applications du monde réel, par exemple la recherche d'informations, le regroupement de textes et la classification. La moyenne pondérée simple des vecteurs de mots est un moyen efficace de représenter des phrases. Cependant, lorsque le document est long, il peut ne pas être efficace et perdre des distinctions fines. La raison en est que lorsque le document est long, il est susceptible de contenir des mots provenant de nombreux sujets différents et, par conséquent, la création d'un seul vecteur ignorerait la structure thématique. Pour surmonter la limitation mentionnée ci-dessus, nous voulons faire quelque chose d'un peu différent. C'est-à-dire qu'au lieu d'obtenir des représentations par simple moyenne, nous pouvons représenter un document par un ensemble de vecteurs et nous pouvons obtenir une nouvelle représentation. Cela s'avère plus raisonnable car l'ensemble de vecteurs peut couvrir différentes parties des documents. Par ce moyen, la représentation peut avoir la capacité de couvrir différents sujets. La partie centrale du modèle de représentation de document est l'encodeur de document hiérarchique. Nous voulons proposer une méthode pour d'abord pré-former des codeurs de transformateur bidirectionnel hiérarchique au niveau du document sur des données non étiquetées. En outre, les modèles de documents pré-entraînés peuvent être adaptés à différentes tâches (recherche d'informations, regroupement de texte et classification de texte) et collections via un réglage fin.
Dans ces travaux de thèse nous abordons l'expressivité de la parole lue avec un type de données particulier qui sont les livres audio. Les livres audio sont des enregistrements audio d'œuvres littéraires fait par des professionnels (des acteurs, des chanteurs, des narrateurs professionnels) ou par des amateurs. Ces enregistrements peuvent être destinés à un public particulier (aveugles ou personnes mal voyantes). La disponibilité de ce genre de données en grande quantité avec une assez bonne qualité a attiré l'attention de la communauté scientifique en traitement automatique du langage et de la parole en général, ainsi que des chercheurs spécialisés dans la synthèse de parole expressive. Pour explorer ce vaste champ d'investigation qui est l'expressivité, nous proposons dans cette thèse d'étudier trois entités élémentaires de l'expressivité qui sont véhiculées par les livres audio : l'émotion, les variations liées aux changements discursifs et les propriétés du locuteur. Nous traitons ces patrons d'un point de vue prosodique.
L'information occupe désormais une place centrale dans notre vie quotidienne, elle est à la fois omniprésente et facile d'accès. Pourtant, l'extraction de l'information à partir des données est un processus souvent inaccessible. En effet, même si les méthodes de fouilles de données sont maintenant accessibles à tous, les résultats de ces fouilles sont souvent complexes à obtenir et à exploiter pour l'utilisateur. La fouille de motifs combinée à l'utilisation de contraintes est une direction très prometteuse de la littérature pour à la fois améliorer l'efficience de la fouille et rendre ses résultats plus appréhendables par l'utilisateur. Cependant, la combinaison de contraintes désirée par l'utilisateur est souvent problématique car, elle n'est pas toujours adaptable aux caractéristiques des données fouillées tel que le bruit. Dans cette thèse, nous proposons deux nouvelles contraintes et un algorithme pour pallier ce problème. La contrainte de robustesse permet de fouiller des données bruitées en conservant la valeur ajoutée de la contrainte de contiguïté. La contrainte de clôture allégée améliore l'appréhendabilité de la fouille de motifs tout en étant plus résistante au bruit que la contrainte de clôture classique. L'algorithme C3Ro est un algorithme générique de fouille de motifs séquentiels intégrant de nombreuses contraintes, notamment les deux nouvelles contraintes que nous avons introduites, afin de proposer à l'utilisateur la fouille la plus efficiente possible tout en réduisant au maximum la taille de l'ensemble des motifs extraits. C3Ro rivalise avec les meilleurs algorithmes de fouille de motifs de la littérature en termes de temps d'exécution tout en consommant significativement moins de mémoire. C3Ro a été expérimenté dans le cadre de l'extraction de compétences présentes dans les offres d'emploi sur le Web
Nombre des succès de l'apprentissage profond reposent sur la disponibilité de données massivement collectées et annotées, exploités par des algorithmes supervisés. Ces annotations, cependant, peuvent s'avérer difficiles à obtenir. La conception de méthodes peu gourmandes en annotations est ainsi un enjeu important, abordé dans des approches semi-supervisées ou faiblement supervisées. Par ailleurs ont été récemment introduit les réseaux génératifs profonds, capable de manipuler des distributions complexes et à l'origine d'avancées majeures, en édition d'image et en adaptation de domaine par exemple. Dans cette thèse, nous explorons comment ces outils nouveaux peuvent être exploités pour réduire les besoins en annotations. En premier lieu, nous abordons la tâche de prédiction stochastique. Il s'agit de concevoir des systèmes de prédiction structurée tenant compte de la diversité des réponses possibles. Ensuite, nous étudions la décomposition en deux facteurs latents indépendants dans le cas où un seul facteur est annoté. Nous proposons des modèles qui visent à retrouver des représentations latentes sémantiquement cohérentes de ces facteurs explicatifs. Le premier modèle est appliqué en génération de données de capture de mouvements, le second, sur des données multi-vues. Enfin, nous nous attaquons au problème, crucial en vision par ordinateur, de la segmentation d'image. Nous proposons un modèle, inspiré des idées développées dans cette thèse, de segmentation d'objet entièrement non supervisé.
Cette thèse explore l'utilisation de données multi-échelles pour modéliser une représentation tridimensionnelle (3D) et générer un registre numérique complet d'un assemblage de fossiles contenant des hominines à partir de l'unité lithostratigraphique P à Kromdraai situé dans le " berceau de l'humanité " classé au patrimoine mondial par l'UNESCO (Province de Gauteng, Afrique du Sud). Nous avons réalisé une analyse multi-scalaire du site, avec l'application de méthodes de photogrammétrie terrestre et aérienne. Conformément aux principes et directives de la gestion du patrimoine archéologique mandatés par les agences internationales telles que l'UNESCO, nous présentons également un protocole de documentation du patrimoine. Nous avons utilisé des technologies de capture de données 3D pour numériser le site de Kromdraai et ses éléments archéologiques découverts entre 2014 et 2018 lors des fouilles. Cette recherche présente une technique originale développée pour la visualisation et la quantification des sédiments volumiques prélevés sur le site à chaque période de fouille par chaque fouilleur. Les estimations de volume calculées à l'aide de la photogrammétrie 3D fournissent un contexte temporel et spatial des sédiments prélevés lors des fouilles successives, et permettent un repositionnement virtuel et plus précis des vestiges découverts ex situ. De plus, nous avons mis en place une modélisation des métadonnées pour démontrer l'utilisation d'un système de gestion de base de données 4D pour la fusion, l'organisation et la diffusion de l'ensemble des données du site de Kromdraai et le partage de la propriété intellectuelle. Nous introduisons également l'une des premières approches statistiques de la modélisation spatiale 3D dans un site Plio-Pléistocène porteurs d'hominines en en Afrique du Sud. En mettant en œuvre des méthodes classiques de tests statistiques telles le partitionnement de données spatiales 3D, nous avons étudié les modèles de l'assemblage de fossiles dans l'unité P, ainsi qu'un échantillon de 810 spécimens catalogués entre 2014 et 2018. Le regroupement de bovidés, de carnivores, d'homininés et de primates non humains a révélé un modèle de distribution spatiale non uniforme des fossiles in situ.
La quantité d'informations sur Internet aujourd'hui accable la plupart des utilisateurs. La découverte d'informations pertinentes (p. Ex. Des nouvelles à lire ou des vidéos à regarder) prend du temps et est fastidieuse ; pourtant, elle fait partie du travail quotidien d'au moins 80% des employés en Amérique du Nord. Plusieurs systèmes de filtrage d'informations pour le Web peuvent faciliter cette tâche pour les utilisateurs. Les exemples se retrouvent dans des familles telles que les réseaux sociaux, les systèmes de notation sociale et les systèmes de bookmarking social. Tous ces systèmes exigent que l'engagement de l'utilisateur fonctionne (par exemple, la soumission ou l'évaluation du contenu). Ils fonctionnent bien dans une communauté Internet, mais souffrent dans le cas des petites communautés. En effet, dans les petites communautés, l'apport des utilisateurs est plus rare. Nous nous concentrons sur les communautés d'un endroit qui sont des communautés qui regroupent les gens qui vivent, travaillent ou étudient dans la même région. Exemples de communautés d'un lieu : (i) les étudiants d'un campus, (ii) les personnes vivant dans un quartier ou (iii) les chercheurs travaillant sur le même site. Anecdote nous savons que seulement 0,3% des travailleurs contribuent quotidiennement à leur réseau social d'entreprise. Cette information montre qu'il ya un manque d'engagement des utilisateurs dans les communautés d'un endroit.
Cette thèse est consacrée à la modélisation et l'optimisation non convexe basées sur la programmation DC et DCA pour certaines classes de problèmes issus de deux domaines importants : le Data Mining et la Cryptologie. Il s'agit des problèmes d'optimisation non convexe de très grande dimension pour lesquels la recherche des bonnes méthodes de résolution est toujours d'actualité. Notre travail s'appuie principalement sur la programmation DC et DCA. Cette démarche est motivée par la robustesse et la performance de la programmation DC et DCA, leur adaptation aux structures des problèmes traités et leur capacité de résoudre des problèmes de grande dimension. La thèse est divisée en trois parties. Dans la première partie intitulée Méthodologie nous présentons des outils théoriques servant des références aux autres. Le premier chapitre concerne la programmation DC et DCA tandis que le deuxième porte sur les algorithmes génétiques. Dans la deuxième partie nous développons la programmation DC et DCA pour la résolution de deux classes de problèmes en Data Mining. Dans le chapitre quatre, nous considérons le modèle de la classification floue FCM et développons la programmation DC et DCA pour sa résolution. Plusieurs formulations DC correspondants aux différentes décompositions DC sont proposées. Notre travail en classification hiérarchique (chapitre cinq) est motivé par une de ses applications intéressante et très importantes, à savoir la communication multicast. C'est un problème non convexe, non différentiable de très grande dimension pour lequel nous avons reformulé sous la forme des trois programmes DC différents et développé les DCA correspondants. La troisième partie porte sur la Cryptologie. Nous proposons une méthode de résolution des deux problèmes PP et PPP par DCA et une méthode de coupes dans le dernier chapitre
Cette thèse présente une réflexion sur les verbes à particule et les verbes prépositionnels en anglais. La relation qui existe entre la syntaxe et la sémantique est une des pierres angulaires de cette étude. Nous étudierons le rôle de la configuration syntaxique, de la structure argumentale et des connaissances extralinguistiques dans la construction du sens. Les aspects théoriques abordés sont la catégorisation des particules et des prépositions, l'interaction du sémantisme des particules - prépositions et le sémantisme verbal, la structure interne des verbes à particule, et les raisons pour lesquelles les particules peuvent apparaître soit avant, soit après le complément régi par le verbe. Chaque combinaison est analysée en contexte afin d'identifier les facteurs qui influencent l'interprétation sémantique finale de la combinaison. Au cours de cette étude, nous identifierons toute une gamme de facteurs qui influencent l'interprétation sémantique finale des verbes à particule et des verbes prépositionnels anglais ainsi que leur interaction.
Dans un contexte de maintien et de développement de la compétitivité de l'entreprise, les services de veille alimentent cette dernière en informations susceptibles d'être utilisées comme objets de référence pour l'analyse de l'environnement et l'aide à la prise de décisions. Le projet ALSEM vise la conception d'un système informatique assistant le veilleur dans ses travaux d'exploration et de combinaison des ressources informationnelles concernant l'environnement de l'entreprise. L'objectif de ce projet est l'intégration au sein d'un même système de techniques offrant un cadre méthodologique de gestion de l'information efficace et plus adapté à une tâche spécifique et complexe comme la veille. Notre travail se situe au confluent de trois disciplines : le traitement automatique de la langue, la sémantique et l'ingénierie des connaissances. Il vise à l'élaboration de techniques d'extraction automatique de l'information contenue dans les textes via une analyse sémantique de ces derniers et de techniques d'interprétation, de modélisation de l'information extraite pour la rendre opérationnelle.
Le Web sémantique est la vision de la prochaine génération de Web proposé par Tim Berners Les outils traditionnels d'interrogation et de raisonnement sur les données du Web sémantique sont conçus pour fonctionner dans un environnement centralisé. A ce titre, les algorithmes de calcul traditionnels vont inévitablement rencontrer des problèmes de performances et des limitations de mémoire. De gros volumes de données hétérogènes sont collectés à partir de différentes sources de données par différentes organisations. Ces sources de données présentent souvent des divergences et des incertitudes dont la détection et la résolution sont rendues encore plus difficiles dans le big data. Mes travaux de recherche présentent des approches et algorithmes pour une meilleure exploitation de données dans le contexte big data et du web sémantique. Nous avons tout d'abord développé une approche de résolution des identités (Entity Resolution) avec des algorithmes d'inférence et d'un mécanisme de liaison lorsque la même entité est fournie dans plusieurs ressources RDF décrite avec différentes sémantiques et identifiants de ressources URI. Nous avons également développé un moteur de réécriture de requêtes SPARQL basé le modèle MapReduce pour inférer les données implicites décrites intentionnellement par des règles d'inférence lors de l'évaluation de la requête. L'approche de réécriture traitent également de la fermeture transitive et règles cycliques pour la prise en compte de langages de règles plus riches comme RDFS et OWL. La deuxième contribution concerne le traitement d'incohérence dans le big data. Nous étendons l'approche présentée dans la première contribution en tenant compte des incohérences dans les données. La troisième contribution concerne le raisonnement et l'interrogation sur la grande quantité données RDF incertaines. Nous proposons une approche basée sur MapReduce pour effectuer l'inférence de nouvelles données en présence d'incertitude. Nous proposons un algorithme d'évaluation de requêtes sur de grandes quantités de données RDF probabilistes pour le calcul et l'estimation des probabilités des résultats.
Nous étudions le problème de détection de relations visuelles de la forme (sujet, prédicat, objet) dans les images, qui sont des entités intermédiaires entre les objets et les scènes visuelles complexes. Cette thèse s'attaque à deux défis majeurs : (1) le problème d'annotations coûteuses pour l'entrainement de modèles fortement supervisés, (2) la variation d'apparence visuelle des relations. Nous proposons un premier modèle de détection de relations visuelles faiblement supervisé, n'utilisant que des annotations au niveau de l'image, qui, étant donné des détecteurs d'objets pré-entrainés, atteint une précision proche de celle de modèles fortement supervisés. Nous validons expérimentalement le bénéfice apporté par chacune de ces composantes sur des bases de données réelles.
Cette thèse porte sur l'établissement de similarités de données textuelles dans le domaine de la gestion de la relation client. Elle se décline en deux parties : - l'analyse automatique de messages courts en réponse à des questionnaires de satisfaction ; - la recherche de produits à partir de l'énonciation de critères au sein d'une conversation écrite mettant en jeu un humain et un programme agent. La première partie a pour objectif la production d'informations statistiques structurées extraites des réponses aux questions. Les idées exprimées dans les réponses sont identifiées, organisées selon une taxonomie et quantifiées. La seconde partie vise à transcrire les critères de recherche de produits en requêtes compréhensibles par un système de gestion de bases de données. Les critères étudiés vont de critères relativement simples comme la matière du produit jusqu'à des critères plus complexes comme le prix ou la couleur. Les deux parties se rejoignent sur la problématique d'établissement de similarités entre données textuelles par des techniques de TAL. Les principales difficultés à surmonter sont liées aux caractéristiques des textes, rédigés en langage naturel, courts, et comportant fréquemment des fautes d'orthographe ou des négations. L'établissement de similarités sémantiques entre mots (synonymie, antonymie, etc) et l'établissement de relations syntaxiques entre syntagmes (conjonction, opposition, etc) sont également des problématiques abordées. Nous étudions également dans cette thèse des méthodes de regroupements et de classification automatique de textes afin d'analyser les réponses aux questionnaires de satisfaction.
Le sociologue Bourdieu définit le capital social comme : "L'ensemble des ressources actuelles ou potentielles qui sont liées à la possession d'un réseau durable de relations". Sur Twitter, les abonnements, mentions et retweets créent un réseau de relations pour chaque utilisateur dont les ressources sont l'obtention d'informations pertinentes, la possibilité d'être lu, d'assouvir un besoin narcissique, de diffuser efficacement des messages. Certains utilisateurs Twitter-appelés capitalistes sociaux-cherchent à maximiser leur nombre d'abonnements pour maximiser leur capital social. Nous introduisons leurs techniques, basées sur l'échange d'abonnements et l'utilisation de hashtags dédiés. Afin de mieux les étudier, nous détaillons tout d'abord une méthode pour détecter à l'échelle du réseau ces utilisateurs en se basant sur leurs abonnements et abonnés. Puis, nous montrons avec un compte Twitter automatisé que ces techniques permettent de gagner efficacement des abonnés et de se faire beaucoup retweeter. Nous établissons ensuite que ces dernières permettent également aux capitalistes sociaux d'occuper des positions qui leur accordent une bonne visibilité dans le réseau. De plus, ces méthodes rendent ces utilisateurs influents aux yeux des principaux outils de mesure. Nous mettons en place une méthode de classification supervisée pour détecter avec précision ces utilisateurs et ainsi produire un nouveau score d'influence.
Le cycle de l‟information, de la collecte à la dissémination, est central en intelligence économique. Nos travaux ont pour sujet l‟étude de l‟impact que ce fameux web 2.0 a sur le cycle en question et nous proposons des méthodes et outils afin de tirer parti de ce nouveau paradigme, et ce, pour chaque étape du cycle
La thèse est préparée dans le cadre d'une convention de cotutelle sous la direction des Professeurs Jean-Hugues Chauchat (ERIC-Lyon2) et N.V. Charonova (Université Nationale Polytechnique de Kharkov en Ukraine).1. Les résultats obtenus peuvent se résumer ainsi : Rétrospective des fondations théoriques sur la formalisation des connaissances et langue naturelle en tant que précurseurs de l'ingénierie des ontologies. Actualisation de l'état de l'art sur les approches générales dans le domaine de l'apprentissage d'ontologie, et sur les méthodes d'extraction des termes et des relations sémantiques. Une méthode d'apprentissage des patrons morphosyntaxiques et d'installation de taxonomies partielles de termes. Une méthode de formation de classes sémantiques représentant les concepts et les relations pour le domaine de la sécurité radiologique. Un cadre (famework) d'organisation des étapes de travaux menant à la construction de l'ontologie du domaine de la sécurité radiologique.3. Implémentation des trois méthodes proposées et analyse des résultats obtenus.
La surveillance de la qualité des données qui proviennent des expériences de physique des hautes énergies est une tâche exigeante mais cruciale pour assurer que les analyses physiques sont basées en données de la meilleure qualité possible. Lors de l'expérience Compact Muon Solenoid opérant au Grand collisionneur de hadrons du CERN, le paradigme actuel d'évaluation de la qualité des données est basé sur l'examen détaillé d'un grand nombre de tests statistiques. Cependant, la complexité toujours croissante des détecteurs et le volume des données de surveillance appellent un changement de paradigme. Ici, les techniques de Machine Learning promettent une percée. Cette thèse traite du problème de l'automatisation applique à la surveillance de la qualité des données avec les méthodes de détection des anomalies d' Les anomalies causées par un dysfonctionnement du détecteur sont difficiles à énumérer a priori et rares, ce qui limite la quantité de données étiquetées. Ainsi, cette thèse explore le paysage des algorithmes existants avec une attention particulière aux problèmes semi-supervisés et démontre leur validité et leur utilité sur des cas de test réels en utilisant les données de l'expérience. Dans le cadre de ce projet, l'infrastructure de surveillance a été encore optimisée et étendue, offrant des méthodes plus sensibles aux différents modes de défaillance.
Dans une démarche de co-création et d'innovation ouverte, les entreprises utilisent des plateformes de crowdsourcing d'idées pour collecter les idées des consommateurs. Ce travail doctoral porte sur un nouveau modèle de plateforme, qui s'appuie simultanément sur la compétition et la coopération pour mobiliser la foule (le modèle de « co-opétition » ). Son usage pose la question de son efficacité pour susciter des idées nombreuses et innovantes, par rapport aux modèles classiques basés uniquement sur la compétition ou la coopération. Pour répondre à cette question, nous avons mis en place deux expérimentations qui nous permettent de comparer quantitativement l'effet de la co-opétition, de la compétition et de la coopération sur la créativité des participants. Nos résultats indiquent que la co-opétition et la compétition renforcent la créativité des individus, contrairement à la coopération. Ils montrent un effet d'interaction positif entre la coopération et la compétition, qui traduit une influence plus importante de la co-opétition sur la créativité que celle de la seule compétition. Ces effets s'expliquent par le rôle médiateur de l'ambivalence motivationnelle, et non par celui de l'ambivalence émotionnelle. L'attitude envers l'indépendance modère l'effet direct des modèles d'interdépendance sur la qualité des idées, tandis que l'attitude envers la compétition et l'attitude envers la coopération n'ont
Cette thèse s'articule autour de deux thèmes : les normes sociales et les réseaux de production. Le premier chapitre porte sur une étude de cas où les normes sociales sont utilisées dans la lutte contre le discours haineux en ligne. A l'aide de méthodes de machine learning, je montre que le fait de dénoncer les opinions haineuses est un moyen de dissuader d'autres discours haineux. Cet effet s'explique par le fait que cette forme de contradiction sert de communiquer la présence d'une norme sociale ou en accentue l'importance. Le deuxième chapitre porte sur le rôle que joue le goût pour l'image sociale pour expliquer l'effet des normes sociales sur le comportement. De nombreuses études montrent que ces goûts affectent le comportement des gens en moyenne, mais nous ne savons pas encore quels individus sont les plus susceptibles d'adapter leur comportement. Je présente une expérience novatrice conçue pour combler ce vide. Elle permet de calculer une mesure individuelle de préoccupation pour l'image, montre qu'il y a une hétérogénéité substantielle et analyse sa corrélation avec d'autres préférences sociales. L'intégration verticale des entreprises peut donner lieu à des comportements anticoncurrentiels. J'aborde l'un de ces comportements, appelé verrouillage, par lequel les entreprises verticalement intégrées coupent l'approvisionnement de leurs concurrents en intrants essentiels. J'utilise de nouvelles données sur les réseaux de production pour identifier les fusions et acquisitions entre entreprises verticalement liées.
Cette thèse s'inscrit dans le cadre de travaux relatifs au Web Social Sémantique, dans la perspective de la complémentarité et de la coévolution de deux aspects du Web, l'aspect social et sémantique. Le développement du Web au cours de ces dernières années a fait émerger un énorme graphe de données structurées, sémantiques résultant en partie de l'activité des utilisateurs, le LOD. Nous nous intéressons à l'utilisation de ce graphe afin de faciliter l'accès à l'information présente sur le Web, et ce de manière utile, informative et enrichissante pour l'utilisateur. Cette problématique est notamment étudiée dans les scénarios de l'innovation sur le Web – pratiques visant à utiliser des technologies du Web pour contribuer à l'émergence de l'innovation. Une spécificité de ce contexte, assez peu abordé dans la littérature existante, est sans doute le besoin d'inciter les découvertes inattendues et fortuites. Au delà de la simple pertinence sollicitée dans toute situation de recherche et de recommandation sur le Web, le contexte d'innovation impose une certaine ouverture d'esprit pour permettre à l'utilisateur d'accéder aux informations inattendues mais néanmoins pertinentes, et permet par la même occasion de s'inspirer et de transposer des idées d'un domaine à l'autre.
La thèse s'appuie sur le concept de littératie médiatique. Après l'avoir défini, elle s'emploie à le mettre en œuvre pour les enfants non-lisants (2 à 7 ans) du Togo et/ou sourds (7 à 12 ans) de France. Partant de l'hypothèse que les contenus médiatiques peuvent contribuer au développement, aux apprentissages et à la culture de ces enfants, l'analyse infocommunicationnelle des mécanismes d'appropriation de ces contenus est effectuée pour ces deux populations cibles. L'étude mobilise ainsi des expérimentations de terrain au Togo et en France. Cette revue préliminaire des terrains d'étude montre que les politiques et les dispositifs de communication propres à notre cible restent insuffisants d'une part, et que le champ est très peu abordé dans les travaux antérieurs d'autre part. La deuxième partie se penche sur le cadre théorique de la littératie médiatique et précise sa conceptualisation dans le champ des sciences de l'information et de la communication. Le protocole VI.A.G.E est alors élaboré pour évaluer le processus d'appropriation de contenus médiatiques auprès des enfants. La troisième partie est consacrée au dépouillement des trois expérimentations de terrain menées au Togo (visionnage expérimental du film Kirikou et la sorcière) et en France (visioguide sur DVD et i-Pad, et interaction sur écran géant tactile, notamment au Musée du Quai Branly). Leurs résultats respectifs sont exposés.
A partir de l'hypothèse que la forme renseigne le sens, un corpus d'énoncés attestés a été constitué et rassemblé dans une base de données de manière à permettre l'observation de la combinatoire syntaxique, lexicale et sémantique de groupes prépositionnels compléments ("argumentaux") en dans avec un verbe. La recherche d'une part identifie et décrit l'ensemble des verbes qui sous-catégorisent cette préposition et d'autre part relève les interprétations qui ressortent de ces combinaisons de manière à avancer dans l'identité sémantique de la préposition dans et du complément dans son ensemble. Du point de vue syntaxique, le résultat de l'investigation consiste en une liste (d'emplois) de verbes définissables par leur complémentation en dans et caractérisés par un certain nombre de propriétés : l'étude aboutit ainsi à la définition d'une classe lexicale, paradigme également uni sur le plan sémantique. La préposition dans est analysable dans tous les cas comme un marqueur de "coi͏̈ncidence", quelle que soit la valeur particulière d'un complément. L'apport proprement linguistique de l'étude est défini et présenté de sorte à en permettre l'exploitation dans le domaine du traitement automatique des langues.
Dans le dialogue homme-machine, les anaphores traitees traditionnellement sont les anaphores pronominales. Puis, en exploitant ce corpus d'un point de vue documentaire (typologie des requetes, strategies de recherches....) et d'un point de vue linguistique (structure de surface des enonces, anaphores, coherences, reference,...), on remet en cause l'usage de la langue naturelle pour certaines situations et l'on emet l'hypothese d'une nouvelle phraselogie de la communication propre au dialogue homme-machine. Du point de vue traitement automatique, sont explicitees les differentes connaissances necessaires a la conduite du dialogue et a la resolution automatique des anaphores (modele de dialogue, modele de la tache, deroulement du dialogue,... Etc.), en s'appuyant d'une part sur les differentes approches linguistiques des phenomenes anaphoriques, et d'autre part sur des travaux effectues en t. A. L. En se placant dans le cadre de l'analyse automatique de langue telle qu'elle est envisagee au centre de recherche en informatique appliquee aux sciences sociales de grenoble (criss), sont degagees des regles de reconnaissance des unites anaphoriques.
Cette thèse sera consacrée à l'étude d'utilisation du traitement de langage naturel en finance. Nous étudierons d'abord les méthodes d'analyse du sentiment dans le cadre d'application sur les nouvelles financières. Avant l'ère d'intelligence artificielle, les méthodes feature engineering et statistiques étaient les outils dominant dans ce domaine, par exemple, Naive Bayse Classifier et TF-IDF. Cependant, la plupart de ces méthodes sont basées sur les embedding statiques et elles ignorent les informations contextuelles dans les corpus. Par conséquent, nous allons étudier les améliorations en précision par adopter les embeddings contextualisés. Nous étudierons aussi les méthodes qui nous permettent de générer les embeddings contextualisés, soit par définir et entraîner un modèle de zéro, soit par fine-tuner une modèle existante, par exemple, BERT ou XLNet. Nous allons ensuite focaliser sur l'analyse du sentiment sur les textes longs. En finance, il y a de différents types de documents longs, par exemple, les rapports annuel et les transcriptions de conférence. Etant donné la longueur de ces corpus, nous ne pouvons pas appliquer directement les méthodes classiques. En conséquence, nous proposons de le résumer dans un premier temps et ensuite d'entraîner un modèle de classification. Les méthodes existantes comme LDA (Latent Dirichlet Allocation) ou BERTSum fonctionnent bien sur les textes plutôt uniformes. Cependant, ils ont les performances limitées sur ces documents financiers avec les formats différents. Nous allons investiguer la méthode transfer learning pour cette tâche. Nous allons d'abord entraîner un modèle avec tous les corpus. Ensuite, pour chaque type de document, nous allons considérer un modèle plus parcimonieux avec un nombre limité de donnée. Notre résultat final sera obtenu par agréger les sorties des deux modèles. Finalement, nous sommes intéressés par la possibilité d'appliquer les méthodes NLP sur la tâche de réduction de dimension des séries temporelles. Les méthodes NLP fréquemment utilisées comme Auto-Encodeur, Generative Adversarial Network (GAN) et Variational Auto-Encoder (VAE) contiennent toutes une partie encodeur, qui nous permet de résumer les embeddings haut-dimensionnels en plusieurs vecteurs. Par conséquent, en basant sur cette idée, nous allons chercher une structure qui prend les séries temporelles haut-dimensionnelles comme l'entrée et qui donne les séries temporelles basse-dimensionnelles comme la sortie.
Les brevets d'invention titres de propriété industrielle confèrent à leurs titulaires le monopole de l'invention brevetée. On peut y trouver une sorte d'historique de l'évolution de l'artefact. Dans ce contexte le concepteur est très souvent amené à faire des recherches dans les documents de brevets afin de bénéficier des connaissances qui y sont contenues en vue de structurer le processus inventif. Développée pour assister les concepteurs dans leur démarche d'innovation, la Méthode de Conception Inventive (MCI), s'inscrit dans le modèle de la dialectique. La MCI a précisé les concepts entrant en jeu dans la description des évolutions des systèmes techniques et des artefacts. Ces items intéressent bien souvent les concepteurs et sont essentiels à la compréhension du problème sous-jacent et à la collecte de toutes les caractéristiques sur lesquelles on peut agir ; et de l'effet de leurs variations sur l'artefact. Cette thèse consiste d'abord à analyser le document de brevet d'un point de vue linguistique, afin d'en connaitre la typologie. Il s'agit, ensuite, de repérer dans le document de brevets les connaissances susceptibles d'être utiles à la MCI et à les formaliser sous forme de programme informatique. L'approche que nous proposons est issue du text-mining. Elle est à base de marqueurs linguistiques et utilise des patrons lexico-syntaxiques issus du domaine du traitement automatique des langues. Cette méthode d'extraction des concepts utiles à la MCI permet l'établissement d'une sorte de cartographie initiale des évolutions passées et possibles des caractéristiques de l'artefact. L'intérêt est en outre de faciliter grandement l'analyse préliminaire des connaissances relative au dit artefact.
Cette thèse s'intéresse à la question des changements de comportement, et notamment à la manière dont cette question s'applique au domaine informatique à travers les technologies persuasives. Dans un contexte applicatif particulier, celui de la rénovation de logements, nous nous intéressons au rôle que peuvent jouent les informations à disposition des utilisateurs sur leur façon d'élaborer leur projet de rénovation. Une façon de modifier les comportements des utilisateurs est de modifier les buts qu'ils poursuivent, soit de manière explicite, soit de manière implicite. Si l'efficacité de la première a été montrée en contexte expérimental, elle semble toutefois moins adaptée à des situations naturelles. Nous proposons donc une approche visant à modifier les buts poursuivis par les utilisateurs implicitement. Dans cette optique, nous travaillons d'abord à l'emploi de normes sociales injonctives pour inciter les utilisateurs à travailler particulièrement sur la rénovation énergétique. Au cours d'une première étude, nous comparons norme sociale injonctive et objectif arbitraire à une condition contrôle. Nous nous intéressons à la performance des participants à la tâche (améliorer la performance énergétique d'un logement) ainsi qu'à la manière dont le projet se met en place tout au long de l'étude. Les résultats montrent que norme sociale et objectif explicite ont un effet similaire sur la performance à la tâche mais différent sur l'organisation temporelle. On observe ainsi des comportements plus stables dans le cas où la norme sociale est activée, et un effet qui semble globalement moins artificiel que dans le cas où on fixe un objectif explicite à l'utilisateur. Cette première étude met également en avant la nécessité pour la norme d'être saillante, ou activée. Nous nous intéressons donc dans une deuxième étude à ce qui caractérise la saillance du message normatif. Dans la première étude, nous avions utilisé deux types d'informations différentes : le message normatif et des indices concrets relatifs au comportement désirable. Cette deuxième étude vise à distinguer ces deux informations et tester leur effet respectif. Les résultats montrent que le message normatif semble avoir un effet légèrement plus important sur la performance mais aussi plus artificiel sur les comportements des utilisateurs. Dans une troisième étude, nous nous intéressons aux caractéristiques du message, en faisant l'hypothèse qu'un message mieux perçu pourrait appuyer la saillance de la norme qu'il porte. Dans le cadre d'une collaboration avec des chercheurs en intelligence artificielle nous avons ainsi testé différents types de cadrage afin d'évaluer leur effet respectif sur la perception de l'argument auquel ils s'appliquaient. Les résultats, mitigés, montrent essentiellement que le style argumentatif (rationnel et factuel plutôt qu'émotionnel ou moral) semble avoir un poids conséquent sur la perception de l'argument. En outre, la thématique abordée par l'argument semble jouer un rôle non négligeable et devrait donc faire l'objet d'une attention particulière pour le développement d'interventions similaires. Sur le plan applicatif, nos résultats mettent d'abord en évidence la pertinence de l'utilisation des normes sociales injonctives dans un contexte de technologie persuasive. Ils montrent également que les messages portant la norme sociale doivent être conçus avec soin, en tenant compte de multiples facteurs. Sur le plan théorique, nous montrons qu'une norme sociale peut avoir un effet comparable à celui d'un objectif explicitement fixé, mais que les deux génèrent la mise en place de processus cognitifs différents. Enfin, sur le plan méthodologique, nous appliquons l'analyse de traces de l'activité au champ de l'influence sociale, ce qui, à notre connaissance, n'avait pas encore été mis en place.
L'utilisation des médicaments est souvent nécessaire au cours de la grossesse malgré un manque de connaissance de leurs effets indésirables sur la grossesse ou le fœtus. L'identification des effets indésirables des médicaments commercialisés au moyen d'outils statistiques de détection de signaux repose classiquement sur l'exploitation de grandes bases de notifications spontanées dans lesquelles le statut de grossesse est peu présent. Les bases médico-administratives sont de plus en plus utilisées en pharmacoépidémiologie y compris chez la femme enceinte. Elles sont donc potentiellement intéressantes pour une pharmacovigilance automatisée. En France, le Système National des Données de Santé couvre la quasi-totalité de la population française et les travaux de la thèse portent sur l'exploitation de son échantillon permanent, l'Echantillon Généraliste des Bénéficiaires. Le premier axe décrit la prescription des médicaments chez la femme enceinte et plus particulièrement les médicaments tératogènes ou fœtotoxiques et les supplémentations recommandées au cours de la grossesse. Nous trouvons que les femmes enceintes se voient délivrer beaucoup de médicaments au cours de la grossesse. Les médicaments à risque connu sont peu délivrés et les supplémentations recommandées sont en augmentation durant la période d'étude. Les femmes enceintes ayant de faibles revenus ont plus de délivrances de médicaments mais ont moins de délivrances de supplémentations. Le deuxième axe porte sur le développement d'une méthodologie de détection de signal basée sur l'utilisation du score de propension ou du score pronostique en grande dimension. Cette méthodologie, déclinée en 21 modalités, est appliquée pour identifier des médicaments possiblement associés à une augmentation du risque de prématurité. Les résultats montrent que l'utilisation du p-RD dérivé du score pronostique permet de mieux prendre en compte la confusion et de limiter les biais d'indication et protopathique. Pour le troisième axe, le p-RD est appliqué à vingt pathologies spécifiques ou non de la grossesse et combiné à un requêtage automatisé des mots-clés MeSH des articles de MEDLINE. Le requêtage automatisé permet une annotation facilitée des effets indésirables médicamenteux connus. En conclusion, cette thèse montre l'intérêt des données médico-administratives pour analyser l'évolution de la prescription de médicaments au cours de la grossesse et pour la détection de signaux en pharmacovigilance chez la femme enceinte.
Progressivement, les technologies numériques prennent une place plus importante dans la recherche sur les phénomènes socioculturels. Des projets d'équipement se développent dans toutes les disciplines des sciences humaines et sociales (SHS) et des mouvements prônant une révolution instrumentale se multiplient. Cette thèse en sciences de l'information et de la communication propose d'interroger l'avènement d'une recherche « numériquement équipée » en SHS à partir d'une réflexion générale sur les liens entre sciences, technique et écriture. Quels sont les enjeux épistémologiques, mais aussi politiques, sous-jacents à ces logiques d'instrumentation numérique en tant qu'elles instituent de nouvelles techniques d'écriture au cœur des pratiques de recherche ? Le mémoire présente un parcours en trois grandes parties. La première partie inscrit la recherche dans une pensée des rapports fondamentaux entre instruments techniques et connaissance scientifique. Il s'agit également de reconnaître les spécificités d'une approche « communicationnelle » de l'instrumentation scientifique, et en particulier de l'instrumentation numérique. À partir de la théorie des médias informatisés et de l'écriture numérique, et sur la base d'une démarche d'analyse techno-sémiotique, la troisième partie interroge les formes et les pouvoirs de la médiation instrumentale numérique. Sur un plan morphologique et praxéologique, en quoi consiste la conception et la mise en œuvre de tels instruments ? Sur un plan plus politique, quels sont les effets « normatifs » de ces dispositifs instrumentaux sur l'épistémologie des disciplines qui s'en saisissent ?
Le succès du mouvement Open Access ces dernières années montre la pertinence de ce modèle pour le monde de la recherche. Il s'agit de nouvelles pratiques de l'édition scientifique qui cherchent à fournir un accès libre et gratuit à l'information de la recherche et de faciliter ainsi la diffusion du savoir. Pour aller plus loin, l'Open Science désigne une approche qui vise à rendre universel, libre et gratuit l'accès non seulement aux publications scientifiques, mais aussi à leurs données, méthodologie et résultats pour permettre une meilleure reproductibilité des recherches, faciliter la collaboration entre les chercheurs, et accélérer les découvertes. La recherche scientifique d'aujourd'hui bénéficie de la société d'information et des « big data » , à travers l'exploitation de grands jeux de données, qui font partie intégrante des outils actuels pour la génération de nouvelles connaissances. Cette thèse a pour objet d'analyser et de traiter automatiquement des articles scientifiques afin d'en extraire de nouvelles méta-données concernant les jeux de données (datasets) et les résultats de la recherche liés à ces jeux de données. Nous étudierons les enjeux de l'Open Science et les différentes phénomènes concernant les articles scientifiques et leurs données, afin de proposer une typologie de jeux de données de la recherche sous forme d'ontologie. Nous mettrons en place une approche automatique pour l'identification des segments textuels se référant aux jeux de données au sein des articles scientifiques. Les nouvelles méta-données, produites suite à cette analyse automatique de corpora scientifiques, seront agrégées sous forme de Open Data afin de proposer de nouveaux outils à destination des chercheurs pour exploiter et analyser la production scientifique d'un domaine.
Les systèmes de recommandation jouent un rôle important dans l'orientation des choix des utilisateurs. La recommandation se fait généralement par une optimisation d'une mesure de précision de l'adéquation entre un utilisateur et un produit. Cependant, plusieurs travaux de recherche ont montré que l'optimisation de la précision ne produisait pas les recommandations les plus utiles pour les utilisateurs. Un système trop précis peut contribuer à confiner les utilisateurs dans leur propre bulle de choix. Ceci peut aussi produire un effet de foule qui va concentrer les usages autour de quelques articles populaires. Par conséquent, il y a un manque de diversité et de nouveauté dans les recommandations et une couverture limitée du catalogue. Même si la routine peut être sécurisante, l'être humain aime sortir des sentiers battus pour, par exemple, découvrir de nouveaux produits, tenter de nouvelles expériences. Dans cette thèse, nous présentons deux familles de modèles qui cherchent à produire des résultats qui vont au-delà des aspects de précision pour des systèmes de recommandation pour des produits culturels basés sur le contenu. Le premier modèle repose principalement sur une approche de clustering. Dans ce modèle, nous proposons de la diversité à l'utilisateur tout en restant dans le périmètre de ses goûts. Le second modèle est basé sur une fonction issue de la loi normale. Nous faisons l'hypothèse de l'existence d'une zone intermédiaire définie entre des éléments considérés comme trop similaires et d'autres considérés comme trop différents. Nos propositions sont testées sur des jeux de données standards et comparées à des algorithmes de l'état de l'art. Les résultats de nos expériences montrent que nos approches apportent de la diversité et de la nouveauté et sont compétitives par rapport aux méthodes de l'état de l'art. Nous proposons également une expérience utilisateur pour valider notre modèle basé sur la fonction issue de la loi normale. Les résultats des expériences centrées sur l'utilisateur montrent que ce modèle correspond au comportement cognitif de l'être humain ainsi qu'à sa perception de la diversité.
D'abord, nous introduisons le concept de corpus parallèle. Fratchèque est un corpus parallèle de ressources écrites dont les textes en français et en tchèque proviennent de la littérature écrite après 1945. Il ne contient pas de balises XML, le logiciel ParaConc utilisé pour le traitement du corpus n'en a pas besoin. L'élaboration du corpus est décrite d'une façon détaillée en suivant toutes les démarches et tout le paramétrage des logiciels utilisés. Elle commence avec le logiciel de reconnaissance optique de caractères FineReader et après le contrôle de la qualité des textes numérisés sous MS Word 2002 on procède à la constitution d'un corpus parallèle géré par ParaConc. La partie linguistique de la thèse s'appuie sur le corpus parallèle réalisé. Elle aborde un phénomène connu en tchèque sous le terme částice qui n'a d'équivalent univoque en français. Les termes le plus souvent liés en français à la question sont mots du discours et particules énonciatives. Selon les descriptions existantes, il y a une relation étroite entre ces mots et le discours. Cette constatation est démontrée pour deux částice – vždyt̕, přece et leurs variantes – sur les grands corpus tchèques (Analyse A) et Fratchèque (Analyse B). L'étude continue avec l'analyse systématique des types variés d'usage de vždyt̕, přece dans le but de proposer une description lexicographique pour un dictionnaire bilingue tchèque-français. Enfin, on discute quelques questions qui concernent la possibilité d'évaluer automatiquement la qualité de traductions liées à la présence de částice
Le travail de recherche exposé dans cette thèse concerne le développement d'approches d'apprentissage non-supervisé adaptés aux grands jeux de données relationnelles et dynamiques. La combinaison de ces trois caractéristiques (taille, complexité et évolution)constitue un défi majeur dans le domaine de l'exploration de données et peu de solutions satisfaisantes existent pour le moment, malgré les besoins de plus en plus manifestes des entreprises. C'est un véritable challenge, car les approches adaptées aux données relationnelle sont une complexité quadratique inadaptée à l'analyse de données dynamiques. Nous proposons ici deux approches complémentaires pour l'analyse de ce type de données. La seconde propose d'utiliser des points de support parmi les objets afin de construire un espace de représentation permettant de définir des prototypes représentatifs des clusters. Enfin, nous appliquons les approches proposées au profilage en temps réel d'utilisateurs connectés.
L'enseignement assiste par ordinateur, heritier de l'enseignement programme, a fortement marque la didactique des ces vingt dernieres annees. L'integration de l'informatique dans la pedagogie a permis de renouveler tant les reflexions que les pratiques liees a l'enseignement / apprentissage des langues en contexte educatif. Les nouvelles applications doivent tenir compte egalement de certaines potentialites des environnements informatiques comme : l'acces direct, la simulation, l'interactivite. Une analyse croisee des possibilites de traitement automatique de la parole et des parametres saillants du phenomene de comprehension de l'oral permet de proposer un modele pedagogique de l'enseignement / apprentissage de la comprehension orale s'appuyant sur quatre axes : quantitatif, qualitatif, strategique et enfin, communicatif. Pour chacun de ces axes, l'informatique interactive multimedia dispose d'atouts evidents, et ce a deux niveaux : * au niveau + macro ;  Ce dernier niveau peut servir de base a la mise en oeuvre d'un champ d'activites totalement nouvelles, irrealisables sans le support de l'informatique interactive multimedia, et dont l'implication sur l'apprentissage de la competence orale reste, bien evidemment, a preciser.
Cette étude présente une typologie des prédicats de mouvement du hongrois. Elle reflète une perception objective et simple du mouvement et de l'espace. La classification s'appuie sur des propriétés sémantiques comme la directionnalité, le mode, le lieu de destination, le but et des propriétés aspectuelles. Ces propriétés sont complétées par des propriétés morpho-syntaxiques nécessaires au traitement automatique. L'aspect contrastif de notre étude a permis de proposer une meilleure description des classes de prédicats du hongrois et de relever les différences morpho-syntaxiques et combinatoires spécifiques des deux langues dans l'expression du mouvement, comme le rôle des préfixes verbaux, des compléments locatifs ainsi que l'importance des prédicats nominaux.
Dans cette thèse, nous proposons une méthodologie basée sur les modèles pour gérer la complexité de la conception des systèmes autonomiques cognitifs intégrant des objets connectés. Cette méthodologie englobe un ensemble de patrons de conception dont nous avons défini pour modéliser la coordination dynamique des processus autonomiques pour gérer l'évolution des besoins du système, et pour enrichir les systèmes avec des propriétés cognitives qui permettent de comprendre les données et de générer des nouvelles connaissances. De plus, pour gérer les problèmes reliés à la gestion des big data et à la scalabilité du système lors du déploiement des processus, nous proposons une plate-forme sémantique supportant le traitement des grandes quantités de données afin d'intégrer des sources de données distribuées et hétérogènes déployées sur le cloud pour générer des connaissances qui seront exposées en tant que service (KaaS). Comme application de nos contributions, nous proposons un système cognitif prescriptif pour la gestion du plan de traitement du patient. Ainsi, nous élaborons des modèles ontologiques décrivant les capteurs et le contexte du patient, ainsi que la connaissance médicale pour la prise de décision. Le système proposé est évalué de point de vue clinique en collaborant avec des experts médicaux, et de point de vue performance en proposant des différentes configurations dans le KaaS.
Des agents devant prendre une décision collective sont souvent motivés par des buts individuels. Dans ces situations, deux aspects clés doivent être abordés : sélectionner une alternative gagnante à partir des voix des agents et s'assurer que les agents ne manipulent pas le résultat. Cette thèse étudie l'agrégation et la dimension stratégique des décisions collectives lorsque les agents utilisent un langage représenté de manière compacte. Nous étudions des langages de type logique : de la logique propositionnelle aux CP-nets généralisés, en passant par la logique temporelle linéaire (LTL). Notre principale contribution est l'introduction d'un cadre de vote sur les buts, dans lequel les agents soumettent des buts individuels exprimés comme des formules de la logique propositionnelle. Les fonctions d'agrégation classiques issues du vote, de l'agrégation de jugements et de la fusion de croyances sont adaptées et étudiées de manière axiomatique et computationnelle. Les propriétés axiomatiques connues dans la littérature sur la théorie du choix social sont généralisées à ce nouveau type d'entrée, ainsi que les problèmes de complexité visant à déterminer le résultat du vote. Une autre contribution importante est l'étude de l'agrégation des CP-nets généralisés, c'est-à-dire des CP-nets où la précondition de l'énoncé de préférence est une formule propositionnelle. Nous utilisons différents agrégateurs pour obtenir un classement collectif des résultats possibles. Grâce à cette thèse, deux axes de recherche sont ainsi reliés : l'agrégation des CP-nets classiques et la généralisation des CP-nets à des préconditions incomplètes. L'accent est mis sur trois règles de vote majoritaires qui se révèlent manipulables. Par conséquent, nous étudions des restrictions à la fois sur le langage des buts et sur les stratégies des agents en vue d'obtenir des résultats de votes non manipulables. Nous présentons par ailleurs une extension stratégique d'un modèle récent de diffusion d'opinion sur des réseaux d'influence. Dans les jeux d'influence définis ici, les agents ont comme but des formules en LTL et ils peuvent choisir d'utiliser leur pouvoir d'influence pour s'assurer que leur but est atteint. Des solutions classiques telles que la stratégie gagnante sont étudiées pour les jeux d'influence, en relation avec la structure du réseau et les buts des agents. Enfin, nous introduisons une nouvelle classe de concurrent game structures (CGS) dans laquelle les agents peuvent avoir un contrôle partagé sur un ensemble de variables propositionnelles. De telles structures sont utilisées pour interpréter des formules de logique temporelle en temps alternés (ATL), grâce auxquelles on peut exprimer l'existence d'une stratégie gagnante pour un agent dans un jeu itéré (comme les jeux d'influence mentionnés ci-dessus). Le résultat principal montre qu'un CGS avec contrôle partagé peut être représenté comme un CGS avec contrôle exclusif. En conclusion, cette thèse contribue au domaine de la prise de décision collective en introduisant un nouveau cadre de vote basé sur des buts propositionnels. Elle présente une étude de l'agrégation des CP-nets généralisés et une extension d'un cadre de diffusion d'opinion avec des agents rationnels qui utilisent leur pouvoir d'influence. Une réduction du contrôle partagé à un contrôle exclusif dans les CGS pour l'interprétation des logiques du raisonnement stratégique est également proposée. Par le biais de langages logiques divers, les agents peuvent ainsi exprimer buts et préférences sur la décision à prendre, et les propriétés souhaitées pour le processus de décision peuvent en être garanties.
Les avancées significatives qu'ont connu les technologies de capteurs, leur utilisation croissante L'information générée par de telles sources de données peut être qualifiée d'hétérogène sur plusieurs plans : types de mesures physiques, domaines et primitives temporelles, modèles de données etc. Dans ce contexte, l'application de méthodes de fouille de motifs constitue une opportunité pour la découverte de relations temporelles non-triviales, directement utilisables et facilement interprétables décrivant des phénomènes complexes. Nous proposons d'utiliser un ensemble d'abstraction temporelles pour construire une représentation unifiée, sous forme des flux d'intervalles (ou états), de l'information générée par un système hétérogène. A partir de cette représentation, nous nous intéressons à la découverte de dépendances temporelles quantitatives (avec information de délais) entre plusieurs flux d'intervalles. Nous introduisons le modèle de dépendances Complex Temporal Dependency (CTD) Ce modèle permets d'exprimer un ensemble riche de relations temporelles complexes. Pour ce modèle de dépendances nous proposons des algorithmes efficaces de découverte : CTD-Miner et ITLD
Cette recherche porte sur la description lexicologique des pragmatèmes. Il s'agit de phrasèmes compositionnels non-libres, contraints par la situation de communication dans laquelle ils sont énoncés. Dans ce travail, nous adoptons une approche contrastive français-espagnol. Les expressions les plus courantes de la vie quotidienne impliquent beaucoup de contraintes dont nous ne sommes pas conscients. Ainsi, saluer quelqu'un par un Bonjour !, ou finir une lettre par Cordialement, Bien à vous, n'a aucune difficulté pour un locuteur natif. Ces énoncés qui ont l'air d'une grande simplicité du point de vue de leur contenu, de leur forme et des contextes de la vie ordinaire dans lesquels ils s'emploient sont très singuliers. Ils sont rituellement émis dans des situations courantes auxquelles ils sont prototypiquement associés. Les pragmatèmes passent souvent inaperçus dans la langue, en tant qu'unités phraséologiques, et c'est lors de la traduction qu'on s'aperçoit qu'ils ne peuvent pas être traduits littéralement dans une autre langue ;  il faut trouver une expression équivalente. Il existe un lien entre les pragmatèmes et la culture. À l'intérieur d'une communauté linguistique, les locuteurs se comprennent puisqu'ils partagent une compétence linguistique et une expérience culturelle. Cependant, dans la communication dans une langue étrangère, il faut prendre en compte les éléments culturels qui conditionnent la situation dans laquelle l'échange a lieu.
Les moteurs de recherche verticaux, qui se concentrent sur des segments spécifiques du Web, deviennent aujourd'hui de plus en plus présents dans le paysage d'Internet. Les moteurs de recherche thématiques, notamment, peuvent obtenir de très bonnes performances en limitant le corpus indexé à un thème connu. Dans le cadre de cette thèse, nous nous intéressons plus précisément à la procédure de collecte de documents thématiques à partir du Web pour alimenter un moteur de recherche thématique. La procédure de collecte peut être réalisée en s'appuyant sur un moteur de recherche généraliste existant (recherche orientée) ou en parcourant les hyperliens entre les pages Web (exploration orientée).Nous étudions tout d'abord la recherche orientée. Dans ce contexte, l'approche classique consiste à combiner des mot-clés du domaine d'intérêt, à les soumettre à un moteur de recherche et à télécharger les meilleurs résultats retournés par ce dernier. Après avoir évalué empiriquement cette approche sur 340 thèmes issus de l'OpenDirectory, nous proposons de l'améliorer en deux points. En amont du moteur de recherche, nous proposons de formuler des requêtes thématiques plus pertinentes pour le thème afin d'augmenter la précision de la collecte. Nous définissons une métrique fondée sur un graphe de cooccurrences et un algorithme de marche aléatoire, dans le but de prédire la pertinence d'une requête thématique. En aval du moteur de recherche, nous proposons de filtrer les documents téléchargés afin d'améliorer la qualité du corpus produit. Dans la seconde partie de cette thèse, nous nous focalisons sur l'exploration orientée du Web. Au coeur de tout robot d'exploration orientée se trouve une stratégie de crawl qui lui permet de maximiser le rapatriement de pages pertinentes pour un thème, tout en minimisant le nombre de pages visitées qui ne sont pas en rapport avec le thème. Nous proposons d'apprendre automatiquement une fonction d'ordonnancement indépendante du thème à partir de données existantes annotées automatiquement.
Les smartphones sont devenus omniprésents dans notre vie quotidienne à cause des options qu'ils proposent. Aujourd'hui, Android est installé sur plus de 80% des smartphones. Les applications mobiles recueillent une grande quantité d'informations sur l'utilisateur. Par conséquent, Android est devenu une cible préférée des cybercriminels. Comprendre le fonctionnement des malwares et comment les détecter est devenu un défi de recherche important. Les malwares Android tentent souvent d'échapper à l'analyse statique en utilisant des techniques telles que l'obfuscation et le chargement dynamique du code. Des approches d'analyse ont été proposées pour exécuter l'application et surveiller son comportement. Néanmoins, les développeurs des malwares utilisent des bombes temporelles et logiques pour empêcher le code malveillant d'être exécuté sauf dans certaines circonstances. Par conséquent, plus d'actions sont requises pour déclencher et surveiller leurs comportements. Des approches récentes tentent de caractériser automatiquement le comportement malveillant en identifiant les endroits du code les plus suspicieux et en forçant leur exécution. Ces approches analysent seulement le code d'application et ratent les chemins d'exécution générés quand l'application appelle une méthode du framework, qui appelle à son tour une autre méthode applicative. Il fournit aussi des informations clés sur l'application analysée afin de comprendre comment le code suspicieux a été injecté dans l'application. Nous évaluons que 72,69% des échantillons ont au moins un endroit suspicieux du code qui n'est atteignable qu'à travers des appels implicites. Les approches de déclenchement actuelles utilisent principalement deux stratégies pour exécuter une partie du code applicatif. La première stratégie consiste à modifier l'application excessivement pour lancer le code ciblé sans faire attention à son contexte originel. La seconde stratégie consiste à générer des entrées pour forcer le flot de contrôle à prendre le chemin désiré sans modifier le code d'application. Cependant, il est parfois difficile de lancer un endroit spécifique du code seulement en manipulant les entrées. Nous proposons TriggerDroid, un outil qui a deux buts : forcer l'exécution du code suspicieux et garder le contexte originel de l'application. Il fournit les événements framework requis pour lancer le bon composant et satisfait les conditions nécessaires pour prendre le chemin d'exécution désiré. Pour valider notre approche, nous avons fait une expérience sur 135 malwares Android de 71 familles différentes. Les résultats montrent que notre approche nécessite plus de raffinement et d'adaptation pour traiter les cas spéciaux dus à la grande diversité des échantillons analysés. Finalement, nous fournissons un retour sur les expériences que nous avons conduites sur différentes collections, et nous expliquons notre processus expérimental. Nous présentons le dataset Kharon, une collection de malwares Android bien documentés qui peuvent être utilisés pour comprendre le panorama des malwares Android.
L'objectif de cette thèse est de développer des méthodes sémantiques de réassemblage dans le cadre compliqué des collections patrimoniales, où certains blocs sont érodés ou manquants. Le remontage de vestiges archéologiques est une tâche importante pour les sciences du patrimoine : il permet d'améliorer la compréhension et la conservation des vestiges et artefacts anciens. Certains ensembles de fragments ne peuvent être réassemblés grâce aux techniques utilisant les informations de contour et les continuités visuelles. Il est alors nécessaire d'extraire les informations sémantiques des fragments et de les interpréter. Ces tâches peuvent être accomplies automatiquement grâce aux techniques d'apprentissage profond couplées à un solveur, c'est-à-dire un algorithme de prise de décision sous contraintes. Cette thèse propose deux méthodes de réassemblage sémantique pour fragments 2D avec érosion, ainsi qu'un jeu de données et des métriques d'évaluation. La première méthode, Deepzzle, propose un réseau de neurones auquel succède un solveur. Le réseau de neurones est composé de deux réseaux convolutionnels siamois entraînés à prédire la position relative de deux fragments : il s'agit d'une classification à 9 classes. Le solveur utilise l'algorithme de Dijkstra pour maximiser la probabilité jointe. Deepzzle peut résoudre le cas de fragments manquants et surnuméraires, est capable de traiter une quinzaine de fragments par puzzle, et présente des performances supérieures à l'état de l'art de 25%. La deuxième méthode, Alphazzle, s'inspire d'AlphaZero et de recherche arborescente Monte Carlo (MCTS) à un joueur. Il s'agit d'une méthode itérative d'apprentissage profond par renforcement : à chaque étape, on place un fragment sur le réassemblage en cours. Deux réseaux de neurones guident le MCTS : un prédicteur d'action, qui utilise le fragment et le réassemblage en cours pour proposer une stratégie, et un évaluateur, qui est entraîné à prédire la qualité du résultat futur à partir du réassemblage en cours. Alphazzle prend en compte les relations entre tous les fragments et s'adapte à des puzzles de taille supérieure à ceux résolus par Deepzzle. Par ailleurs, Alphazzle se place dans le cadre patrimonial : en fin de réassemblage, le MCTS n'accède pas à la récompense, contrairement à AlphaZero. En effet, la récompense, qui indique si un puzzle est bien résolu ou non, ne peut être qu'estimée par l'algorithme, car seul un conservateur peut être certain de la qualité d'un réassemblage.
La réalisation de surface est une partie du processus global de génération de langue naturelle. Étant donné une grammaire et une représentation du sens, le réalisateur de surface produit une chaîne en langue naturelle que la grammaire associe au sens donné en entrée. Cette thèse présente trois extension de GenI, un réalisateur de surface pour une grammaire de type FB-LTAG. La première extension augmente l'efficacité du réalisateur pour le traitement de l'ambiguïté lexicale. C'est une adaptation de l'optimisation par « étiquetage électrostatique » qui existe déjà pour l'analyse. La deuxième extension concerne le nombre de sorties retournées par le réalisateur. En temps normal, l'algorithme GenI retourne toutes les phrases associées à une même forme logique. Alors qu'on peut considérer que ces entrées ont le même sens, elles présentent souvent de subtiles nuances. L'extension est permise par le fait que la grammaire FB-LTAG utilisée par le générateur a été construite à partir d'une « métagrammaire » , mettant explicitement en oeuvre les généralisations qu'elle code. La dernière extension donne la possibilité au réalisateur de servir d'environnement de débuggage de la métagrammaire. Les erreurs dans la métagrammaire peuvent avoir des conséquences importantes pour la grammaire. Comme le réalisateur donne en sortie toutes les chaînes associées à une sémantique d'entrée, il peut être utilisé pour trouver ces erreurs et les localiser dans la métagrammaire.
Nos contributions couvrent trois applications différentes, mais partagent un dénominateur commun : l'extraction des représentations d'utilisateurs concernés. Notre première application est la tâche de recommandation de produits, où les systèmes existant créent des profils utilisateurs et objets qui reflètent les préférences des premiers et les caractéristiques des derniers, en utilisant l'historique. De nos jours, un texte accompagne souvent cette note et nous proposons de l'utiliser pour enrichir les profils extraits. Notre espoir est d'en extraire une connaissance plus fine des goûts des utilisateurs. Nous pouvons, en utilisant ces modèles, prédire le texte qu'un utilisateur va écrire sur un objet. Notre deuxième application est l'analyse des sentiments et, en particulier, la classification de polarité. Notre idée est que les systèmes de recommandation peuvent être utilisés pour une telle tâche. Les systèmes de recommandation et classificateurs de polarité traditionnels fonctionnent sur différentes échelles de temps. Nous proposons deux hybridations de ces modèles : la première a de meilleures performances en classification, la seconde exhibe un vocabulaire de surprise. La troisième et dernière application que nous considérons est la mobilité urbaine. Elle a lieu au-delà des frontières d'Internet, dans le monde physique. Nous utilisons les journaux d'authentification des usagers du métro, enregistrant l'heure et la station d'origine des trajets, pour caractériser les utilisateurs par ses usages et habitudes temporelles.
Nous abordons dans cette thèse une étude sur la tâche de la désambiguïsation lexicale qui est une tâche centrale pour le traitement automatique des langues, et qui peut améliorer plusieurs applications telles que la traduction automatique ou l'extraction d'informations. Les recherches en désambiguïsation lexicale concernent principalement l'anglais, car la majorité des autres langues manque d'une référence lexicale standard pour l'annotation des corpus, et manque aussi de corpus annotés en sens pour l'évaluation, et plus important pour la construction des systèmes de désambiguïsation lexicale. En anglais, la base de données lexicale wordnet est une norme de-facto de longue date utilisée dans la plupart des corpus annotés et dans la plupart des campagnes d'évaluation. Notre contribution porte sur plusieurs axes : dans un premier temps, nous présentons une méthode pour la création automatique de corpus annotés en sens pour n'importe quelle langue, en tirant parti de la grande quantité de corpus anglais annotés en sens wordnet, et en utilisant un système de traduction automatique. Cette méthode est appliquée sur la langue arabe et est évaluée sur le seul corpus arabe, qui à notre connaissance, soit annoté manuellement en sens wordnet : l'OntoNotes 5.0 arabe que nous avons enrichi semi-automatiquement. Son évaluation est réalisée grâce à la mise en œuvre de deux systèmes supervisés (SVM, LSTM) qui sont entraînés sur les corpus produits avec notre méthode. Grâce ce travail, nous proposons ainsi une base de référence solide pour l'évaluation des futurs systèmes de désambiguïsation lexicale de l'arabe, en plus des corpus arabes annotés en sens que nous fournissons en tant que ressource librement disponible. Dans un second temps, nous proposons une évaluation in vivo de notre système de désambiguïsation de l'arabe en mesurant sa contribution à la performance de la tâche de traduction automatique.
La détection de structures par blocs dans les matrices est un enjeu important. D'abord en analyse de données, où les matrices sont classiquement utilisées pour représenter des données, par exemple via les tables de données ou les matrices d'adjacence. Dans le premier cas, la détection d'une structure par blocs de lignes et de colonnes permet de trouver un co-clustering. Dans le second cas, la détection d'une structure par blocs diagonaux dominants fournit un clustering. Dans cette thèse, nous centrons notre analyse sur la détection de blocs diagonaux dominants par permutations symétriques des lignes et des colonnes. De nombreux algorithmes pour trouver ces structures ont été créés. La première est composée d'algorithmes qui projettent les lignes de la matrice dans un espace de faible dimension composé des vecteurs propres dominants avant d'appliquer une procédure de type k-means sur les données réduites. Ces algorithmes ont le désavantage de nécessiter la connaissance du nombre de classes à découvrir. La deuxième famille est composée de procédures itératives qui, à chaque itération, cherchent la k-ième meilleure partition en deux blocs. Mais pour les matrices ayant plus de deux blocs, la partition optimale en deux blocs ne coïncide en général pas avec la véritable structure. Nous proposons donc un algorithme spectral répondant aux deux problèmes évoqués ci-dessus. Pour ce faire, nous prétraitons notre matrice via un équilibrage bi-stochastique permettant de stratifier les blocs. D'abord, nous montrons les bénéfices de cet équilibrage sur la détection de structures par blocs en l'utilisant comme prétraitement de l'algorithme de Louvain pour détecter des communautés dans des réseaux. Nous explorons aussi plusieurs mesures globales utilisées pour évaluer la cohérence d'une structure par blocs. En adaptant ces mesures à nos matrices bi-stochastiques, nous remarquons que notre équilibrage tend à unifier ces mesures. Ensuite, nous détaillons notre algorithme basé sur les éléments propres de la matrice équilibrée. Il est construit sur le principe que les vecteurs singuliers dominants d'une matrice bi-stochastique doivent présenter une structure en escalier lorsque l'on réordonne leurs coordonnées dans l'ordre croissant, à condition que la matrice ait une structure par blocs. Des outils de traitement du signal, initialement conçus pour détecter les sauts dans des signaux, sont appliqués aux vecteurs pour en détecter les paliers, et donc les séparations entre les blocs. Cependant, ces outils ne sont pas naturellement adaptés pour cette utilisation. Des procédures, mises en place pour répondre à des problèmes rencontrés, sont donc aussi détaillées. Nous proposons ensuite trois applications de la détection de structures par blocs dans les matrices. Pour ces applications, nous comparons les résultats de notre algorithme avec ceux d'algorithmes spécifiquement conçus à cet effet. Enfin, la détection des actes de dialogues dans un discours en utilisant la base de données STAC qui consiste en un chat de joueurs des "Colons de Catane" en ligne. Pour ce faire nous couplons des algorithmes de clustering non supervisés avec un réseau de neurones BiLSTM permettant de prétraiter les unités de dialogue. Enfin, nous concluons en entamant une réflexion sur la généralisation de notre méthode au cas des matrices rectangulaires.
Cette dissertation explore le sujet des modèles génératifs appliqués aux images naturelles. Cette tâche consiste a modéliser la distribution des données observées, et peut permettre de générer des données artificielles semblables aux données d'origine, où de compresser des images. Les modèles à variable latentes, qui sont au cœur de cette thèse, cherchent a résumer les principaux facteurs de variation d'une image en une variable qui peut être manipulée. Malheureusement ces modèles ont du mal à modéliser tous les modes de la distribution d'origine, ie ils ne couvrent pas les données dans toute leur variabilité. A l'inverse, les modèles basés sur le maximum de vraisemblance tels que les VAEs couvrent typiquement toute la variabilité des données, et en offrent une mesure objective. Mais ces modèles produisent des échantillons de qualité visuelle inférieure, qui sont plus facilement distingués de vrais images. Le travail présenté dans cette thèse a pour but d'obtenir le meilleur des deux mondes : des échantillons de bonne qualité tout en modélisant tout le support de la distribution. Nous proposons une procédure d'entrainement qui utilise une fonction de perte auxiliaire pour contrôler quelle information est capturée par la variable latent et quelle information est laissée à un décodeur autoregressif. Au contraire des précédentes approches pour construire des modèles hybrides de ce genre, notre modèle de nécessite pas de contraindre la capacité du décodeur autoregressif pour empêcher des modèles dégénérés qui ignorent la variable latente. La deuxième contribution est bâtie sur le modèle du GAN standard, qui utilise un discriminateur pour guider le modèle génératif. Le discriminateur évalue généralement la qualité d'échantillons individuels, ce qui rend la tache d'évaluer la variabilité des données difficile. A la place, nous proposons de fournir au discriminateur des ensembles de données, ou batches, qui mélangent des vraies images et des images générées. Dans notre troisième contribution, nous montrons que les hypothèses paramétriques habituelles faites par les VAE produisent un conflit entre les deux, menant à des performances décevantes pour les modèles hybrides. Nous proposons une solution basée sur des modèles profonds inversibles, qui entraine un espace de features dans lequel les hypothèses habituelles peuvent être faites sans poser problème. Notre approche fourni des évaluations e vraisemblance dans l'espace des images tout en étant capable de tirer profit de l'entrainement adversaire. Elle obtient des échantillons de qualité équivalente au modèle pleinement adversaires tout en améliorant les scores de maximum de vraisemblance au moment de la publication, ce qui constitue une amélioration significative.
Cette thèse se concentre sur la structuration du modèle acoustique pour améliorer la reconnaissance de la parole par modèle de Markov. La structuration repose sur l'utilisation d'une classification non supervisée des phrases du corpus d'apprentissage pour tenir compte des variabilités dues aux locuteurs et aux canaux de transmission. L'idée est de regrouper automatiquement les phrases prononcées en classes correspondant à des données acoustiquement similaires. Pour la modélisation multiple, un modèle acoustique indépendant du locuteur est adapté aux données de chaque classe. Quand le nombre de classes augmente, la quantité de données disponibles pour l'apprentissage du modèle de chaque classe diminue, et cela peut rendre la modélisation moins fiable. Une façon de pallier ce problème est de modifier le critère de classification appliqué sur les données d'apprentissage pour permettre à une phrase d'être associée à plusieurs classes. Ceci est obtenu par l'introduction d'une marge de tolérance lors de la classification ;  et cette approche est étudiée dans la première partie de la thèse. L'essentiel de la thèse est consacré à une nouvelle approche qui utilise la classification automatique des données d'apprentissage pour structurer le modèle acoustique. Ainsi, au lieu d'adapter tous les paramètres du modèle HMM-GMM pour chaque classe de données, les informations de classe sont explicitement introduites dans la structure des GMM en associant chaque composante des densités multigaussiennes avec une classe. Pour exploiter efficacement cette structuration des composantes, deux types de modélisations sont proposés. Dans la première approche on propose de compléter cette structuration des densités par des pondérations des composantes gaussiennes dépendantes des classes de locuteurs. Pour cette modélisation, les composantes gaussiennes des mélanges GMM sont structurées en fonction des classes et partagées entre toutes les classes, tandis que les pondérations des composantes des densités sont dépendantes de la classe. Lors du décodage, le jeu de pondérations des gaussiennes est sélectionné en fonction de la classe estimée. Dans une deuxième approche, les pondérations des gaussiennes sont remplacées par des matrices de transition entre les composantes gaussiennes des densités. Les approches proposées dans cette thèse sont analysées et évaluées sur différents corpus de parole qui couvrent différentes sources de variabilité (âge, sexe, accent et bruit)
La production documentaire en contexte professionnel entraîne généralement un processus de révision dans lequel les documents doivent être relus avant validation et publication. Cette tâche importante fait face à de nouvelles difficultés avec le numérique. En tant que technologie d'écriture numérique avancée, les chaînes éditoriales XML sont un cadre pertinent pour l'étude de la relecture de documents numériques. Une partie des propositions faites dans ce mémoire a mené à la réalisation de prototypes ayant été expérimentés dans des situations d'usage des chaînes éditoriales Scenari en contexte pédagogique. Ces prototypes s'appuient sur des formes linéaires de relecture permettant notamment la comparaison de deux versions du document en se basant sur un algorithme de différentiel.
Cette thèse aborde deux questions majeures du traitement automatique du langage naturel liées à l'analyse sémantique des textes : la détection des sentiments, et le résumé automatique. Nous abordons ces deux questions par des approches d'apprentissage profond, qui permettent d'exploiter au mieux les données, en particulier lorsqu'elles sont disponibles en grande quantité. Analyse des sentiments neuronale. De nombreux réseaux de neurones convolutionnels profonds ont été adaptés du domaine de la vision aux tâches d'analyse des sentiments et de classification des textes. Cependant, ces études ne permettent pas de conclure de manière satisfaisante quant à l'importance de la profondeur du réseau pour obtenir les meilleures performances en classification de textes. Nous proposons une adaptation du réseau convolutionnel profond DenseNet pour la classification de texte et étudions l'importance de la profondeur avec différents niveaux de granularité en entrée (mots ou caractères). En outre, nous proposons de modéliser conjointement sentiments et actes de dialogue, qui constituent un facteur explicatif influent pour l'analyse du sentiment. Nous avons annoté manuellement les dialogues et les sentiments sur un corpus de micro-blogs, et entraîné un réseau multi-tâches sur ce corpus. Nous montrons que l'apprentissage par transfert peut être efficacement réalisé entre les deux tâches et analysons de plus certaines corrélations spécifiques entre ces deux aspects. Résumé de texte neuronal. L'analyse de sentiments n'apporte qu'une partie de l'information sémantique contenue dans les textes et est insuffisante pour bien comprendre le texte d'origine et prendre des décisions fondées. L'utilisateur d'un tel système a également besoin des raisons sous-jacentes pour vraiment comprendre les documents. Dans cette partie, notre objectif est d'étudier une autre forme d'information sémantique fournie par les modèles de résumé automatique. Nous proposons ainsi un modèle de résumé qui présente de meilleures propriétés d'explicabilité et qui est suffisamment souple pour prendre en charge divers modules d'analyse syntaxique. Plus spécifiquement, nous linéarisons l'arbre syntaxique sous la forme de segments de texte superposés, qui sont ensuite sélectionnés par un apprentissage par renforcement (RL) et re-générés sous une forme compressée. Par conséquent, le modèle proposé est capable de gérer à la fois le résumé par extraction et par abstraction. Nous comparons ainsi de manière détaillée les modèles avec apprentissage par renforcement et les modèles exploitant une connaissance syntaxique supplémentaire des phrases ainsi que leur combinaison, selon plusieurs dimensions liées à la qualité perçue des résumés générés. Nous montrons lorsqu'il existe une contrainte de ressources (calcul et mémoire) qu'il est préférable de n'utiliser que l'apprentissage par renforcement, qui donne des résultats presque aussi satisfaisants que des modèles syntaxiques, avec moins de paramètres et une convergence plus rapide.
Depuis que les progrès en analyse automatique du langage naturel ont rendu possible la prise en charge d'un dialogue finalisé de manière entièrement automatique (ex. réservation de voyage [1]), les système ont gagné en intelligence pour être capable de répondre à des questions sur des tâches complexes et des domaines plus ouverts (par ex. le système WATSON d'IBM qui a remporté en 2011 le jeux télévisé jeopardy [2]). Récemment, les agents conversationnels autonomes (connus sous le nom de « chatbots » ) sont devenus des éléments incontournables dans la gestion de la relation client [3]. Dans ce contexte, la fonction d'agent conversationnel évolue de plus en plus vers celle de d'un conseiller virtuel duquel on attend un comportement de plus en plus intelligent allant au-delà de la collecte d'information ou fourniture de réponses simples, mais étant aussi capable de prendre des décisions comme par exemple effectuer des actions et en particulier de reconnaître ses propre limites et de savoir quand il convient de passer la main à un conseiller humain afin de préserver la qualité de service. Verrou technologique Un grand nombre de chatbots existent sur le marché, des plus simples souvent élaborés avec des règles faiblement contextuelles, aux plus complexes à base d'apprentissage automatique. Les premiers  : deviennent vite silencieux si l'on sort des limites de leur domaine de compétence, tandis que les seconds sont capable d'un peu plus de généralisation, mais cela se paie en général au prix d'une plus grande imprécision dans leurs réponses. Quel que soit le type de système considéré, tout repose sur les connaissances, c'est-à-dire la mémoire de l'agent conversationnel. Chez l'humain, la mémoire se décline selon plusieurs types [4] par exemple : épisodique (événements vécu), sémantique (connaissances), procédurale (savoir-faire), perceptive (reconnaissance des voix, des odeurs etc.) ou de travail (bloc-note à court terme). Elle peut être de nature explicite (accès et restitution conscients), implicite (subconscient, émotions) ou autobiographique (personnelle associant mémoire sémantique et épisodique). Or ces caractéristiques de la mémoire d'un agent autonome conditionnent les représentations qu'il construit [5] et donc ses actions. Outre les fonctionnalités mémorielles de l'agent conversationnel et leur impact sur la gestion du dialogue, les investigations scientifiques aborderont le problème de l'adaptation au domaine applicatif1 [8] et devront adresser dans ce cadre les avancées récentes en apprentissage automatique (approches neuronales profondes [6]), ceci à différent niveaux de granularité (analyse d'un énoncé, acquisition de connaissances de fond à partir de corpus etc.). La validation des résultat expérimentaux intermédiaires sera effectuée par le biais de l'évaluation de démonstrateurs (système de dialogue prototypes) selon une procédure d'évaluation quantitative à base de corpus avec un ensemble de mesures de performance définies à partir des protocoles d'évaluation classiques pour le domaine [7].
L'apprentissage statistique cherche à modéliser un lien fonctionnel entre deux variables X et Y à partir d'un échantillon aléatoire de réalisations de (X,Y). Lorsque la variable Y prend un nombre binaire de valeurs, l'apprentissage s'appelle la classification (ou discrimination en français) et apprendre le lien fonctionnel s'apparente à apprendre la frontière d'une variété dans l'espace de la variable X. Dans cette thèse, nous nous plaçons dans le contexte de l'apprentissage actif, i.e. nous supposons que l'échantillon d'apprentissage n'est plus aléatoire et que nous pouvons, par l'intermédiaire d'un oracle, générer les points sur lesquels l'apprentissage de la variété va s'effectuer. Dans le cas où la variable Y est continue (régression), des travaux précédents montrent que le critère de la faible discrépance pour générer les premiers points d'apprentissage est adéquat. Nous montrons, de manière surprenante, que ces résultats ne peuvent pas être transférés à la classification. Dans ce manuscrit, nous proposons alors le critère de la dispersion pour la classification. Ce critère étant difficile à mettre en pratique, nous proposons un nouvel algorithme pour générer un plan d'expérience à faible dispersion dans le carré unité. Après une première approximation de la variété, des approximations successives peuvent être réalisées afin d'affiner la connaissance de celle-ci. Deux méthodes d'échantillonnage sont alors envisageables : le « selective sampling » qui choisit les points à présenter à un oracle parmi un ensemble fini de candidats et l' « adaptative sampling » qui permet de choisir n'importe quels points de l'espace de la variable X. Le deuxième échantillonnage peut être vu comme un passage à la limite du premier. Néanmoins, en pratique, il n'est pas raisonnable d'utiliser cette méthode. Nous proposons alors un nouvel algorithme basé sur le critère de dispersion, menant de front exploitation et exploration, pour approximer une variété.
Du XVe au XVII siècle, le Portugal a occupé la première place parmi les Etats les plus avancés de son temps. Or, le contact des peuples et des cultures a toujours été source d'influences réciproques de nature diverse et multiforme. Nous nous proposons d'étudier, dans cette thèse, les empreintes lusitaniennes dans le Golfe de Guinée. La recherche a été menée dans la partie australe de la Côte d'Ivoire, du Ghana, du Togo et du Bénin et elle se fonde sur un corpus composé de quelques centaines d'entrées que nous avons répertoriées à travers une recherche bibliographique et une enquête de huit ans sur le terrain. L'analyse de ces données, se fait selon une méthode qui combine à la fois l'histoire et le structuralisme dans son approche contrastive car, il s'agit, en réalité, de comparer deux systèmes linguistiques : le portugais et des langues Niger-Congo des groupes langues kru et kwa.
L'entropie d'une distribution sur un ensemble de variables aléatoires discrètes est toujours bornée par l'entropie de la distribution factorisée correspondante. Cette propriété est due à la sous-modularité de l'entropie. Par ailleurs, les fonctions sous-modulaires sont une généralisation des fonctions de rang des matroïdes ; ainsi, les fonctions linéaires sur les polytopes associés peuvent être minimisées exactement par un algorithme glouton. Dans ce manuscrit, nous exploitons ces liens entre les structures des modèles graphiques et les fonctions sous-modulaires. Nous utilisons des algorithmes gloutons pour optimiser des fonctions linéaires sur des polytopes liés aux matroïdes graphiques et hypergraphiques pour apprendre la structure de modèles graphiques, tandis que nous utilisons des algorithmes d'inférence sur les graphes pour optimiser des fonctions sous-modulaires. La première contribution de cette thèse consiste à approcher par maximum de vraisemblance une distribution de probabilité par une distribution factorisable et de complexité algorithmique contrôlée. Comme cette complexité est exponentielle dans la largeur arborescente du graphe, notre but est d'apprendre un graphe décomposable avec une largeur arborescente bornée, ce qui est connu pour être NP-difficile. Nous posons ce problème comme un problème d'optimisation combinatoire et nous proposons une relaxation convexe basée sur les matroïdes graphiques et hypergraphiques. Ceci donne lieu à une solution approchée avec une bonne performance pratique. En troisième contribution, nous proposons et analysons des algorithmes visant à minimiser des fonctions sous-modulaires pouvant s'écrire comme somme de fonctions plus simples. Nos algorithmes n'utilisent que des oracles de ces fonctions simple basés sur minimisation sous-modulaires et de variation totale de telle fonctions.
Dans ce manuscrit, des éléments de réponse sont proposés par la formalisation et l'implémentation d'un modèle multi-échelle de description de la structure d'un segment musical : les Graphes Polytopiques à Relations Latentes (GPRL/PGLR). Dans ce travail, les segments considérés sont les sections successives qui forment une pièce musicale. En suivant le formalisme PGLR, les relations de dépendance prédominantes entre éléments musicaux d'un segment sont celles qui relient les éléments situés à des positions homologues sur la grille métrique du segment. Cette approche généralise sur le plan multi-échelle le modèle Système&amp ; Contraste qui décrit sous la forme d'une matrice 2×2 le système d'attente logique au sein d'un segment et la surprise qui découle de la réalisation de cette attente. Chaque nœud du polytope correspond à un élément musical fondamental (accord, motif, note...), chaque arête représente une relation entre deux nœuds et chaque face représente un système de relations. Le but du modèle PGLR est à la fois de décrire les dépendances temporelles entre les éléments d'un segment et de modéliser l'attente logique et la surprise qui découlent de l'observation et de la perception des similarités et des différences entre ces éléments. Cette approche a été formalisée et implémentée pour décrire la structure de séquences d'accords ainsi que de segments rythmiques et mélodiques, puis évaluée par sa capacité à prédire des segments inconnus. Les résultats obtenus donnent un large avantage à la méthode multi-échelle proposée, qui semble mieux à même de décrire efficacement la structure des segments testés.
Culturel est défini comme l'ensemble des biens matériels et immatériels d'un groupe ou d'une société hérités des générations passées. Les vases sont parmi les biens culturels les plus emblématiques d'une société. Si certaines de ces collections ont été numérisées, elles sont rarement accessibles dans un format ouvert et restent isolées. De plus, l'absence de terminologies clairement identifiées est un obstacle à la communication et au partage des connaissances. Notre travail vise à répondre à cette problématique par la mise en œuvre de pratiques relevant du web sémantique et de l'ingénierie des connaissances, et plus particulièrement par la construction sous un format du W3C d'une ontologie dédiée aux vases chinois des dynasties Ming et Qing. La construction de l'ontologie TAO CI ( « céramique » en Chinois) respecte la façon de penser des experts dans leur conceptualisation du domaine et tient compte des normes internationales en Terminologie (ISO 1087 et ISO 704). Les deux approches reposent sur la notion de caractéristique essentielle et définissent un concept comme étant comme une combinaison unique de caractéristiques. La recherche des différences entre objets, combinée à une analyse morphologique des termes chinois dont les caractères sont porteurs de sens au regard des connaissances du domaine, permet d'identifier les caractéristiques essentielles. La définition des concepts repose sur l'idée qu'un concept est un ensemble de caractéristiques essentielles suffisamment stable pour être nommé en langue. Nous avons ainsi proposé une méthode spécifique de construction d'ontologies guidée par les termes et les caractéristiques essentielles du domaine. Nous avons également été amenés à introduire de nouveaux termes (néologismes) en anglais et des concepts sans désignation en langue à des fins de structuration de l'ontologie. La construction de l'ontologie a été faite à l'aide de l'environnement Protégé, environnement le plus utilisé pour la construction d'ontologies au format du W3C (RDF/OWL). La dimension terminologique a été réduite, comme c'est souvent le cas, à des annotations (SKOS, RDFS) sur les concepts. L'ontologie TAO CI est liée à des ressources externes telles que CIDOC CRM et ATT Getty pour la partie conceptuelle, et à des musées pour les objets. Enfin, nous avons évalué l'ontologie TAO CI du point de vue du domaine (couverture) et de son implémentation. Elle est en accès libre à l'adresse : http : //www.dh.ketrc.com/otcontainer/data/OTContainer.owlLa dernière phase du projet a consisté à réaliser un site web dédié. Ce site donne accès aux différentes ressources du projet et en particulier à un dictionnaire électronique bilingue (anglais, chinois) des vases des dynasties Ming et Qing. Les entrées de ce dictionnaire correspondent aux classes OWL de l'ontologie : http : //www.dh.ketrc.com/L'ontologie TAO CI est, à notre connaissance, la première ontologie au format du web sémantique de vases en céramique chinois, ouverte et réutilisable. Elle est une illustration d'une démarche guidée par les termes et les caractéristiques essentielles du domaine qui peut être appliquée à la construction d'ontologies dans d'autres domaines du patrimoine culturel chinois.
La reconnaissance des émotions est l'un des domaines scientifiques les plus complexes. Ces dernières années, de plus en plus d'applications tentent de l'automatiser. Nous traitons dans notre recherche les expressions émotionnelles faciales en s'intéressant spécifiquement aux six émotions de base à savoir la joie, la colère, la peur, le dégoût, la tristesse et la surprise. Une étude comparative de deux méthodes de reconnaissance des émotions l'une basée sur les descripteurs géométriques et l'autre basée sur les descripteurs d'apparence est effectuée sur la base CK+, base d'émotions simulées, et la base FEEDTUM, base d'émotions spontanées. Différentes contraintes telles que le changement de résolution, le nombre limité d'images labélisées dans les bases d'émotions, la reconnaissance de nouveaux sujets non inclus dans la base d'apprentissage sont également prises en compte. Une évaluation de différents schémas de fusion est ensuite réalisée lorsque de nouveaux cas, non inclus dans l'ensemble d'apprentissage, sont considérés. Les résultats obtenus sont prometteurs pour les émotions simulées (ils dépassent 86%), mais restent insuffisant pour les émotions spontanées. Ces dernières améliorent les taux de reconnaissance des émotions spontanées.
L'analyse de donnée textuelle est facilitée par l'utilisation du text mining (TM) permettant l'automatisation de l'analyse de contenu et possède de nombreuses applications en santé. L'une d'entre elles est l'utilisation du TM pour explorer le contenu des messages échangés sur Internet. Nous avons effectué une revue de la littérature systématique afin d'identifier les applications du TM en santé mentale. De plus, le TM a permis d'explorer les préoccupations des utilisateurs du forum Doctissimo.com au sujet des antidépresseurs et anxiolytiques entre 2013 et 2015 via l'analyse des fréquences des mots, des cooccurrences, de la modélisation thématique (LDA) et de la popularité des thèmes. Les quatre applications du TM en santé mentale sont l'analyse des récits des patients (psychopathologie), le ressenti exprimé sur Internet, le contenu des dossiers médicaux, et les thèmes de la littérature médicale. Quatre grands thèmes ont été identifiés sur le forum : le sevrage (le plus fréquent), l'escitalopram, l'anxiété de l'effet du traitement et les effets secondaires. Alors que les effets indésirables des traitements est un sujet qui a tendance à décroitre, les interrogations sur les effets du sevrage et le changement de traitement sont grandissantes et associées aux antidépresseurs.
L'objectif de cette thèse est de montrer que, contrairement aux idées reçues, les mots de basses fréquences peuvent être mis à profit de façon efficace en traitement automatique des langues. Nous les mettons à contribution en alignement sous-phrastique, tâche qui constitue la première étape de la plupart des systèmes de traduction automatique fondée sur les données (traduction probabiliste ou par l'exemple). Nous montrons que les mots rares peuvent servir de fondement même dans la conception d'une méthode d'alignement sous-phrastique multilingue, à l'aide de techniques différentielles proches de celles utilisées en traduction automatique par l'exemple. Cette méthode est réellement multilingue, en ce sens qu'elle permet le traitement simultané d'un nombre quelconque de langues. Elle se veut de surcroît très simple, anytime, et permet un passage à l'échelle naturel. Nous comparons notre implémentation, Anymalign, à deux ténors statistiques du domaine sur des tâches bilingues. Bien qu'à l'heure actuelle ses résultats sont en moyenne légèrement en retrait par rapport à l'état de l'art en traduction automatique probabiliste par segments, nous montrons que la qualité propre des lexiques produits par notre méthode est en fait supérieure à celle de l'état de l'art.
Dans l'étude de l'usage des expressions référentielles, de nombreux travaux se sont intéressés à la relation entre formes linguistiques et fonctions saisies en termes informationnels, et la sensibilité précoce du jeune enfant à certaines facettes de la structuration informationnelle a été montré, notamment au statut attentionnel des référents et la dimension du topic-commentaire. La linguistique interactionnelle a adopté un point de vue complémentaire sur les expressions référentielles, et a montré comment les locuteurs signalent et accomplissent par leur choix d'une expression référentielle non seulement la référence, mais également diverses tâches liées à la gestion de l'interaction même. Nous avons souhaité, dans ce travail, apporter un éclairage multidimensionnel à la question de l'usage des expressions référentielles, et notamment au contraste entre formes faibles, fortes et disloquées, dans deux langues qui diffèrent quant aux moyens linguistiques employés pour marquer le topic. Notre analyse repose sur un corpus transversal de 12 enfants francophones et germanophones, âgés entre 2 et 3 ans. Nous avons analysé les formes et usages des expressions référentielles produites par les enfants et les adultes, en prenant en compte des facteurs morpho-syntaxiques, informationnels et interactionnels. Nos résultats montrent la complémentarité de ces différents facteurs dans l'explication des usages chez l'enfant et chez l'adulte. Nous avons pu mettre en avant les emplois spécifiques de certaines ressources linguistiques comme les dislocations, les pronoms démonstratifs der/die/das ainsi que des formes nulles et décrire leur fonctionnement dans des formats d'interaction qui facilitent l'inscription du jeune enfant dans la gestion du discours et de l'interaction.
Cette thèse présente Adetoa, système dédié au repérage et à l'annotation sémantique automatique d'expressions temporelles dans des pages Web pour une application de e-tourisme. Une étude linguistique détaillée a permis de mettre en avant les caractéristiques et la complexité de l'expression de la temporalité dans les pages Web touristiques. Ces analyses ont mené à l'élaboration d'un ensemble important de transducteurs (avec Unitex) pour les tâches de repérage et d'annotation des expressions temporelles, ce qui constitue une ressource pouvant être généralisée. Des transducteurs de liage permettent de grouper toutes les informations concernant une même offre touristique. Pour l'annotation et l'intégration d'Adetoa à la chaîne de traitement du projet Eiffel, un schéma d'annotation et des règles de transformations ont été mis au point. Il permet ainsi de rester au plus près des expressions linguistiques de manière à les caractériser finement. L'ontologie a ensuite pu être adaptée en conséquence, pour un meilleur stockage des données dans la base de connaissance qui lui correspond. L'évaluation d'Adetoa, présentée dans cette thèse, a montré des résultats satisfaisants aussi bien d'un point de vue théorique que pour cette application industrielle.
Les interfaces cerveau-ordinateur (Brain-Computer Interface ou BCI) permettent la communication entre l'utilisateur et la machine, grâce à la traduction de l'activité cérébrale en commandes qui servent à contrôler différents dispositifs. De nombreuses limitations empêchent la diffusion des systèmes BCI dans des applications réelles, telles que la phase de calibration qui résulte de la variabilité entre sessions et entre sujets. Cette phase est fondamentale car elle permet de régler les paramètres nécessaires pour le bon fonctionnement du système, mais elle est considérée beaucoup trop longue et fatigante pour le sujet. L'objectif de cette thèse est de surmonter ces limites par de nouvelles méthodes basées sur l'amélioration ou le remplacement de la phase de calibration traditionnelle, en proposant le développement d'une BCI centrée sur l'utilisateur. D'abord, nous proposons deux systèmes BCI adaptés au sujet. Le premier concerne un clavier virtuel basé sur des potentiels évoqués modulés par code-modulated Visual Evoked Potential (c-VEP) où la phase de calibration traditionnelle est remplacée par une phase dans laquelle les paramètres de stimulation sont réglés de manière adaptée au sujet. Le deuxième concerne le développement d'un système basé sur l'imagination mentale (MI-BCI) pour un sujet à déficience motrice sévère (tétraplégie), dans le cadre d'une compétition internationale BCI en direct. Les méthodes proposées ont montré des résultats prometteurs et ouvrent de nouvelles perspectives pour la diffusion des systèmes BCI plus adaptables à l'utilisateur.
Les données cliniques sont produites par différents professionnels de santé, dans divers lieux et sous diverses formes dans le cadre de la pratique de la médecine. Elles présentent par conséquent une hétérogénéité à la fois au niveau de leur nature et de leur structure mais également une volumétrie particulièrement importante et qualifiable de massive. Le travail réalisé dans le cadre de cette thèse s'attache à proposer une méthode de recherche d'information efficace au sein de ce type de données complexes et massives. L'accès aux données cliniques se heurte en premier lieu à la nécessité de modéliser l'information clinique. Ceci peut notamment être réalisé au sein du dossier patient informatisé ou, dans une plus large mesure, au sein d'entrepôts de données. Je propose dans ce mémoire une preuve de concept d'un moteur de recherche permettant d'accéder à l'information contenue au sein de l'entrepôt de données de santé sémantique du Centre Hospitalier Afin de fournir des fonctionnalités de recherche adaptées à cette représentation générique, un langage de requêtes permettant l'accès à l'information clinique par le biais des diverses entités qui la composent a été développé et implémenté dans le cadre de cette thèse. En second lieu, la massivité des données cliniques constitue un défi technique majeur entravant la mise en oeuvre d'une recherche d'information efficace. L'implémentation initiale de la preuve de concept sur un système de gestion de base de données relationnel a permis d'objectiver les limites de ces derniers en terme de performances. Une migration vers un système NoSQL orienté clé Enfin, l'apport de ce travail dans le contexte plus général de l'entrepôt de données de santé sémantique du CHU de Rouen a été évalué. La preuve de concept proposée dans ce travail a ainsi été exploitée pour accéder aux descriptions sémantiques afin de répondre à des critères d'inclusion et d'exclusion de patients dans des études cliniques. Dans cette évaluation, une réponse totale ou partielle a pu être apportée à 72,97% des critères. De plus, la généricité de l'outil a également permis de l'exploiter dans d'autres contextes tels que la recherche d'information documentaire et bibliographique en santé.
En traitement automatique de la langue, les différentes étapes d'analyse usuelles ont tour à tour amélioré la façon dont le langage peut être modélisé par les machines. Une étape d'analyse encore mal maîtrisée correspond à l'analyse sémantique. Ce type d'analyse permettrait de nombreuses avancées, telles que de meilleures interactions homme-machine ou des traductions plus fiables. Il existe plusieurs structures de représentation du sens telles que PropBank, les AMR et FrameNet. FrameNet correspond à la représentation en cadres sémantiques dont la théorie a été décrite par Charles Fillmore. Dans cette théorie, chaque situation prototypique et les différents éléments y intervenant sont représentés de telle sorte que deux situations similaires soient représentées par le même objet, appelé cadre sémantique. Le projet FrameNet est une application de cette théorie, dans laquelle plusieurs centaines de situations prototypiques sont définies. Le travail que nous décrirons ici s'inscrit dans la continuité des travaux déjà élaborés pour prédire automatiquement des cadres sémantiques. Nous verrons également que notre analyse peut être améliorée en fournissant aux modèles de prédiction des informations raffinées au préalable, avec d'un côté une analyse syntaxique dont les liens profonds sont explicités et de l'autre des représentations vectorielles du vocabulaire apprises au préalable.
L'analyse des besoins est la première étape du processus de conception. C'est aussi une source importante d'innovation en entreprise, notamment lorsqu'elle est partagée ausein de l'équipe de conception pluridisciplinaire. La prospection et l'anticipation des besoins futurs des utilisateurs porte ainsi des enjeux stratégiques majeurs pour développer des produits nouveaux adaptés aux utilisateurs finaux. L'objectif de cette thèse est de contribuer à optimiser l'anticipation des besoins dans le but de favoriser l'innovation. Nos hypothèses portent sur des facteurs méthodologiques et technologiques qui permettent d'améliorer la collaboration de l'équipe pluridisciplinaire et la performance d'anticipation des besoins. Ces hypothèses sont opérationnalisées comme trois déclinaisons de la méthode des Personas et sont testées dansle cadre de trois projets industriels. Nous montrons qu'en proposant une méthode combinant plusieurs modes de raisonnements adaptés aux profils métiers de l'équipe pluridisciplinaire, l'anticipation des besoins est améliorée en quantité et en qualité (utilité perçue par les utilisateurs). Nous montrons également que les technologies support jouent un rôle important dans l'efficacité des méthodes : une technologie collaborative et ludique comme une table interactive peut augmenter le nombre d'items stratégiques pour l'entreprise (c'est-à-dire utiles et faisables) ; une technologie immersive et ludique comme un monde virtuel permet d'orienterl'anticipation des besoins selon les objectifs du projet (techno-centrés ou centrés-utilisateurs). Ces résultats ouvrent de nombreuses perspectives pour l'évolution méthodologique et technologique de la phase d'anticipation des besoins dans les projets d'innovation.
Cette thèse porte sur la transformation de la voix d'un locuteur dans l'objectif d'indiquer la distance de celui-ci : une transformation en voix chuchotée pour indiquer une distance proche et une transformation en voix criée pour une distance plutôt éloignée. Nous effectuons dans un premier temps des analyses approfondies pour déterminer les paramètres les plus pertinentes dans une voix chuchotée et surtout dans une voix criée (beaucoup plus difficile). La contribution principale de cette partie est de montrer la pertinence des paramètres prosodiques dans la perception de l'effort vocal dans une voix criée. Nous proposons ensuite des descripteurs permettant de mieux caractériser les contours prosodiques. Pour la transformation proprement dite, nous proposons plusieurs nouvelles règles de transformation qui contrôlent de manière primordiale la qualité des voix transformées. Les résultats ont montré une très bonne qualité des voix chuchotées transformées ainsi que pour des voix criées pour des structures linguistiques relativement simples (CVC, CVCV, etc.).
La prise de décision est un domaine très étudié en sciences, que ce soit en neurosciences pour comprendre les processus sous tendant la prise de décision chez les animaux, qu'en robotique pour modéliser des processus de prise de décision efficaces et rapides dans des tâches en environnement réel. En neurosciences, ce problème est résolu online avec des modèles de prises de décision séquentiels basés sur l'apprentissage par renforcement. En robotique, l'objectif premier est l'efficacité, dans le but d'être déployés en environnement réel. Cependant en robotique ce que l'on peut appeler le budget et qui concerne les limitations inhérentes au matériel, comme les temps de calculs, les actions limitées disponibles au robot ou la durée de vie de la batterie du robot, ne sont souvent pas prises en compte à l'heure actuelle. Nous nous proposons dans ce travail de thèse d'introduire la notion de budget comme contrainte explicite dans les processus d'apprentissage robotique appliqués à une tâche de localisation en mettant en place un modèle basé sur des travaux développés en apprentissage statistique qui traitent les données sous contrainte de budget, en limitant l'apport en données ou en posant une contrainte de temps plus explicite. Dans ce cadre, l'alternance entre recherche d'information pour la localisation et la décision de se déplacer pour un robot peuvent être indirectement liés à la notion de compromis exploration
Nous apprenons très jeunes une quantité de règles nous permettant d'interagir avec d'autres personnes : des conventions sociales. Elles diffèrent des autres types d'apprentissage dans le sens où les premières personnes à les avoir utilisées n'ont fait qu'un choix arbitraire parmi plusieurs alternatives possibles : le côté de la route où conduire, la forme d'une prise électrique, ou inventer de nouveaux mots. À cause de celà, lorsqu'une nouvelle convention se crée au sein d'une population d'individus interagissant entre eux, de nombreuses alternatives peuvent apparaître et conduire à une situation complexe où plusieurs conventions équivalentes coexistent en compétition. Nous exerçons communément un contrôle actif sur nos situations d'apprentissage, en par exemple sélectionnant des activités qui ne soient ni trop simples ni trop complexes. Il a été montré que ce type de comportement, dans des cas comme l'apprentissage sensori-moteur, aide à apprendre mieux, plus vite, et avec moins d'exemples. Est-ce que de tels mécanismes pourraient aussi influencer la négociation de conventions sociales ? Le lexique est un exemple particulier de convention sociale : quels mots associer avec tel objet ou tel sens ? Une classe de modèles computationels, les Language Games, montrent qu'il est possible pour une population d'individus de construire un langage commun via une série d'interactions par paires. En particulier, le modèle appelé Naming Game met l'accent sur la formation du lexique reliant mots et sens, et montre une typique explosion de la complexité avant de commencer à écarter les conventions synonymes ou homonymes et arriver à un consensus. Différentes stratégies sont introduites, et ont des impacts différents sur à la fois le temps nécessaire pour converger vers un consensus et la quantité de mémoire nécessaire à chaque individu. Premièrement, nous limitons artificiellement la mémoire des agents pour éviter l'explosion de complexité locale. Quelques stratégies sont présentées, certaines ayant des propriétés similaires au cas standard en termes de temps de convergence. Dans un deuxième temps, nous formalisons ce que les agents doivent optimiser, en se basant sur une représentation de l'état moyen de la population. Deux stratégies inspirées de cette notion permettent de limiter les besoins en mémoire sans avoir à contraindre le système, et en prime permettent de converger plus rapidement. Nous montrons ensuite que la dynamique obtenue est proche d'un comportement théorique optimal, exprimé comme une borne inférieure au temps de convergence. Les résultats suggèrent qu'ils ont effectivement une politique active de choix de sujet de conversation, en comparaison avec un choix aléatoire. Les contributions de ce travail de thèse incluent aussi une classification des modèles de Naming Games existants, et un cadriciel open-source pour les simuler.
Dans le contexte de l'interaction humain-agent, notre objectif était d'améliorer la qualité de l'interaction en : (1) dotant l'agent de la capacité d'exprimer des attitudes sociales telles que la dominance ou l'amicalité ce qui renforcent ses compétences sociales ; (2) adaptant le comportement de l'agent selon le comportement de l'utilisateur, par conséquent l'agent et l'utilisateur s'influencent mutuellement par le biais d'une boucle interactive ; (3) prédisant le niveau d'engagement de l'utilisateur et adaptant en conséquence le comportement de l'agent, ce qui contribue à maintenir l'intérêt et la motivation de l'utilisateur. Nous nous basons sur les progrès récents dans le domaine de l'apprentissage automatique, plus particulièrement de l'extraction de séquences temporelles et des réseaux de neurones. Le premier est utilisé pour apprendre des séquences pertinentes de signaux non-verbaux qui représentent au mieux les variations d'attitude, puis les reproduire par l'agent. Le seconde est utilisé pour englober la dynamique des signaux non verbaux. Deux cas d'utilisation ont été explorés à l'aide du modèle LSTM : l'adaptation du comportement de l'agent en fonction de l'historique de comportement de l'agent et de l'utilisateur ; et la prédiction de l'engagement de l'utilisateur basée sur son propre historique de comportement. La pertinence des modèles et des algorithmes implémentés a été validée au moyen de nombreuses études approfondies et d'une évaluation quantitative rigoureuse des résultats obtenus. De plus, les travaux réalisés ont été intégrés dans une plateforme d'agents virtuels.
L'interopérabilité des applications est devenue le leitmotiv des développeurs et concepteurs en ingénierie système. La plupart des approches pour l'interopérabilité existant dans l'entreprise ont pour objectif principal l'ajustement et l'adaptation des types et structures de données nécessaire à la mise en œuvre de collaboration entre entreprises. Dans le domaine des entreprises manufacturières, le produit est une composante centrale. Des travaux scientifiques proposent des solutions pour la prise en compte des systèmes d'information issus des produits, tout au long de leur cycle de vie. Mais ces informations sont souvent non corrélées. La gestion des données de produit (PDM) est couramment mise en œuvre pour gérer toute l'information relative aux produits durant tout leur cycle de vie. Cependant, ces modèles sont généralement des "îlots" indépendants ne tenant pas compte de la problématique d'interopérabilité des applications supportant ces modèles. L'objectif de cette thèse est d'étudier cette problématique d'interopérabilité appliquée aux applications utilisées dans l'entreprise manufacturière et de définir un modèle ontologique de la connaissance des entreprises relatives aux produits qu'elles fabriquent, sur la base de leurs données techniques. Le résultat de ce travail de recherche concerne la formalisation d'une méthodologie d'identification des informations de gestion techniques des produits, sous la forme d'une ontologie, pour l'interopérabilité des applications d'entreprises manufacturières, sur la base des standards existants tels que l'ISO 10303 et l'IEC 62264.
Les essais contrôlés randomisés et les revues systématiques qui les synthétisent sont des éléments indispensables à la pratique de la médecine fondée sur les preuves. Celle-ci accorde un poids très important à la recherche biomédicale, et sera donc compromise si la base des éléments de preuve s'avère erronée. La mauvaise qualité de la recherche biomédicale est dénoncée depuis plusieurs années, mais en 2009, I.Chalmers et P.Glasziou intègrent ces critiques dans un concept plus global, le gaspillage de la recherche. Ils affirment que sans des rapports de recherche accessibles, honnêtes et utilisables, la recherche biomédicale ne pourra pas aider les patients et soignants à prendre des décisions éclairées, et pourrait ainsi être considérée comme gaspillée. Une recherche mal conçue, mal réalisée, non ou mal rapportée serait ainsi gaspillée. Ils ont estimé que près de de 85 % de la recherche biomédicale le serait. Dans le cadre de cette thèse, nous nous sommes intéressés au gaspillage, évitable, de la recherche dans les essais cliniques. Puis nous avons estimé dans quelle mesure ce gaspillage pouvait être évité de manière simple et peu coûteuse. Nos travaux suggèrent que 1) que des ajustements méthodologiques simples et peu coûteux permettraient de limiter le risque de biais dans 50 % des essais, et ainsi réduire partiellement le gaspillage de la recherche, et 2) que de nombreux essais ne mesuraient ou ne rapportaient pas complètement les critères de jugement importants, mais que ce gaspillage aurait pu être partiellement évité pour la majorité des essais.
De plus, l'intérêt dans les langages définis sur des alphabets infinis ou de grande taille est croissant au fil des années. Même si plusieurs propriétés et théories se généralisent à partir du cas fini, l'apprentissage de tels langages est une tâche difficile. En effet, dans ce contexte, l'application naïve des algorithmes d'apprentissage traditionnel n'est pas possible. Dans cette thèse, nous présentons un schéma algorithmique général pour l'apprentissage de langages définis sur des alphabets infinis ou de grande taille, comme par exemple des sous-ensembles bornés de N or R ou des vecteurs booléens de grandes dimensions. Nous nous restreignons aux classes de langages qui sont acceptés par des automates déterministes symboliques utilisant des prédicats pour définir les transitions, construisant ainsi une partition finie de l'alphabet pour chaque état. Notre algorithme d'apprentissage, qui est une adaptation du L* d'Angluin, combine l'apprentissage classique d'un automate par la caractérisation de ses états, avec l'apprentissage de prédicats statiques définissant les partitions de l'alphabet. Nous utilisons l'apprentissage incrémental avec la propriété que deux types de requêtes fournissent une information suffisante sur le langage cible. Les requêtes du premier type sont les requêtes d'adhésions, qui permettent de savoir si un mot proposé appartient ou non au langage cible. Nous étudions l'apprentissage de langages définis sur des alphabets infinis ou de grande tailles dans un cadre théorique et général, mais notre objectif est de proposer des solutions concrètes pour un certain nombre de cas particuliers. Ensuite, nous nous intéressons aux deux principaux aspects du problème. Dans un premier temps, nous supposerons que les requêtes d'équivalence renvoient toujours un contre-exemple minimal pour un ordre de longueur-lexicographique quand l'automate proposé est incorrect. Puis dans un second temps, nous relâchons cette hypothèse forte d'un oracle d'équivalence, et nous la remplaçons avec une hypothèse plus réaliste où l'équivalence est approchée par un test sur les requêtes qui utilisent un échantillonnage sur l'ensemble des mots. Dans ce dernier cas, ce type de requêtes ne garantit pas l'obtention de contre-exemples, et par conséquent de contre-exemples minimaux. Tout les algorithmes ont été implémentés, et leurs performances, en terme de construction d'automate et de taille d'alphabet, ont été évaluées empiriquement.
Dans de nombreux contextes statistiques, il est courant de s'appuyer sur un modèle linéaire (généralisé) pour effectuer des prévisions, ou sélectionner des variables. Néanmoins, il est souvent réaliste de supposer que le deuxième, voire le troisième ordre d'interactions de variables pourraient aider et améliorer les décisions prises. Le même besoin de modéliser des interactions se produit aussi fréquemment dans le contexte de la publicité en ligne, en particulier pour les enchères en temps réel et la modélisation des taux de clics (CTR). En effet, avec les contraintes de temps réel pour la prédiction, bien que de nombreux échantillons puissent être collectés, seules quelques variables sont obtenues par les acteurs dans ce champ (par exemple le type de système d'exploitation, le type de navigateur, l'heure, etc). En outre, le signal à récupérer est faible, avec seulement environ 0,1% de taux de clics (CTR) dans le meilleur des cas. Par conséquent, il pourrait être bénéfique d'intégrer des interactions pour travailler avec des contraintes aussi drastiques. Enfin et surtout, pour les données catégoriques, une telle interaction peut modéliser les raffinements hiérarchiques des catégories : cela présente un grand intérêt pour le traitement du langage naturel. Par conséquent, proposer des méthodes efficaces pour faire face à de tels scénarios pourrait avoir un impact considérable dans diverses applications de l'apprentissage automatique. Dans le scénario de haute dimension que nous visons, les méthodes de type Lasso avec interactions ont été proposées par Radchenko et James [2010], ainsi qu'une variante conçue pour gérer les interactions hiérarchiques étudiée par Bien et al. [2013]. Jusqu'à présent, nous pensons que ces méthodes sont restées sous-utilisées en raison de leurs limites computationnelles. Nous pensons que l'adaptation des développements récents proposés par les deux dernières équipes mentionnées, ainsi que les stratégies d'ensembles actifs raffinés étudiées par Massias et al. [2018] pourrait conduire des mises en œuvre plus rapides que les solutions actuellement disponibles. Enfin, la forme explicite de la régularisation utilisée est encore ouverte, notamment pour arriver à lutter contre le fléau de la dimension dans ce contexte.
Un robot d'assistance sociale (SAR) est destiné à engager les gens dans une interaction située comme la surveillance de l'exercice physique, la réadaptation neuropsychologique ou l'entraînement cognitif. Alors que les comportements interactifs de ces systèmes sont généralement scriptés, nous discutons ici du cadre d'apprentissage de comportements interactifs multimodaux qui est proposé par le projet SOMBRERO. Dans notre travail, nous avons utilisé l'apprentissage par démonstration afin de fournir au robot des compétences nécessaires pour effectuer des tâches collaboratives avec des partenaires humains. Il y a trois étapes principales d'apprentissage de l'interaction par démonstration : (1) recueillir des comportements interactifs représentatifs démontrés par des tuteurs humains ; (2) construire des modèles des comportements observés tout en tenant compte des connaissances a priori (modèle de tâche et d'utilisateur, etc.) ; et ensuite (3) fournir au robot-cible des contrôleurs de gestes appropriés pour exécuter les comportements souhaités. Les modèles multimodaux HRI (Human-Robot Interaction) sont fortement inspirés des interactions humain-humain (HHI). Le transfert des comportements HHI aux modèles HRI se heurte à plusieurs problèmes : (1) adapter les comportements humains aux capacités interactives du robot en ce qui concerne ses limitations physiques et ses capacités de perception, d'action et de raisonnement limitées ; (2) les changements drastiques des comportements des partenaires humains face aux robots ou aux agents virtuels ; (3) la modélisation des comportements interactifs conjoints ; (4) la validation des comportements robotiques par les partenaires humains jusqu'à ce qu'ils soient perçus comme adéquats et significatifs. Dans cette thèse, nous étudions et faisons des progrès sur ces quatre défis. En particulier, nous traitons les deux premiers problèmes (transfert de HHI vers HRI) en adaptant le scénario et en utilisant la téléopération immersive. A la fin de cette thèse, nous évaluons une première version de robot autonome équipé des modèles construits par apprentissage.
Cette thèse porte sur les problèmes de sécurité sur le réseau électrique français exploité par RTE, le Gestionnaire de Réseau de Transport (GRT). Les progrès en matière d'énergie durable, d'efficacité du marché de l'électricité ou de nouveaux modes de consommation poussent les GRT à exploiter le réseau plus près de ses limites de sécurité. Pour ce faire, il est essentiel de rendre le réseau plus "intelligent". Pour s'attaquer à ce problème, ce travail explore les avantages des réseaux neuronaux artificiels. Nous proposons de nouveaux algorithmes et architectures d'apprentissage profond pour aider les opérateurs humains (dispatcheurs) à prendre des décisions que nous appelons " guided dropout ". Ceci permet de prévoir les flux électriques consécutifs à une modification volontaire ou accidentelle du réseau. Pour se faire, les données continues (productions et consommations) sont introduites de manière standard, via une couche d'entrée au réseau neuronal, tandis que les données discrètes (topologies du réseau électrique) sont encodées directement dans l'architecture réseau neuronal. L'architecture est modifiée dynamiquement en fonction de la topologie du réseau électrique en activant ou désactivant des unités cachées. Le principal avantage de cette technique réside dans sa capacité à prédire les flux même pour des topologies de réseau inédites. Le "guided dropout" atteint une précision élevée (jusqu'à 99% de précision pour les prévisions de débit) tout en allant 300 fois plus vite que des simulateurs de grille physiques basés sur les lois de Kirchoff, même pour des topologies jamais vues, sans connaissance détaillée de la structure de la grille. Nous avons également montré que le "guided dropout" peut être utilisé pour classer par ordre de gravité des évènements pouvant survenir. Dans cette application, nous avons démontré que notre algorithme permet d'obtenir le même risque que les politiques actuellement mises en œuvre tout en n'exigeant que 2 % du budget informatique. Le classement reste pertinent, même pour des cas de réseau jamais vus auparavant, et peut être utilisé pour avoir une estimation globale de la sécurité globale du réseau électrique.
L'importance de la documentation du patrimoine culturel croit parallèlement aux risques auxquels il est exposé tels que les guerres, le développement urbain incontrôlé, les catastrophes naturelles, la négligence et les techniques ou stratégies de conservation inappropriées. De plus, la documentation constitue un outil fondamental pour l'évaluation, la conservation, le suivi et la gestion du patrimoine culturel. Dès lors, cet outil majeur nous permet d'estimer la valeur historique, scientifique, sociale et économique de ce patrimoine. Selon plusieurs institutions internationales dédiées à la conservation du patrimoine culturel, il y a un besoin réel de développer et d'adapter de solutions informatiques capables de faciliter et de soutenir la documentation du patrimoine culturel peu documenté surtout dans les pays en développement où il y a un manque flagrant de ressources. Parmi ces pays, la Palestine représente un cas d'étude pertinent dans cette problématique de carence en documentation de son patrimoine. Pour répondre à cette problématique, nous proposons une approche d'acquisition et d'extraction de connaissances patrimoniales dans un contexte peu documenté. Nous prenons comme cas d'étude l'église de la Nativité en Palestine et nous mettons en place notre approche théorique par le développement d'une plateforme d'acquisition et d'extraction de connaissances patrimoniales à l'aide d'un Framework pour la documentation de patrimoine culturel. Notre solution est basée sur les technologies sémantiques, ce qui nous donne la possibilité, dès le début, de fournir une description ontologique riche, une meilleure structuration de l'information, un niveau élevé d'interopérabilité et un meilleur traitement automatique (lisibilité par les machines) sans efforts additionnels. Dès lors, l'interaction entre les deux composants de notre système ainsi que les connaissances patrimoniales se développent et s'améliorent au fil de temps surtout que notre système utilise les contributions manuelles et validations des résultats automatiques (dans les deux composants) par les experts afin d'optimiser sa performance.
La théorie des graphes a longtemps été étudiée en mathématiques et en probabilité en tant qu'outil pour décrire la dépendance entre les nœuds. Cependant, ce n'est que récemment qu'elle a été mise en œuvre sur des données, donnant naissance à l'analyse statistique des réseaux réels. La topologie des réseaux économiques et financiers est remarquablement complexe : elle n'est généralement pas observée, et elle nécessite ainsi des procédures inférentielles adéquates pour son estimation, d'ailleurs non seulement les nœuds, mais la structure de la dépendance elle-même évolue dans le temps. Des outils statistiques et économétriques pour modéliser la dynamique de changement de la structure du réseau font défaut, malgré leurs besoins croissants dans plusieurs domaines de recherche. En même temps, avec le début de l'ère des “Big data”, la taille des ensembles de données disponibles devient de plus en plus élevée et leur structure interne devient de plus en plus complexe, entravant les processus inférentiels traditionnels dans plusieurs cas. Cette thèse a pour but de contribuer à ce nouveau champ littéraire qui associe probabilités, économie, physique et sociologie en proposant de nouvelles méthodologies statistiques et économétriques pour l'étude de l'évolution temporelle des structures en réseau de moyenne et haute dimension.
Les enfants « musiciens » et les jeunes musiciens professionnels surpassent les participants de contrôle dans une série d'expériences, avec une plasticité cérébrale plus rapide, et une connectivité fonctionnelle plus forte, mesurées par électroencéphalographie. Les résultats des musiciens plus âgés sont moins clairs, suggérant un impact limité de la formation musicale sur le déclin cognitif. Enfin, les jeunes musiciens ont une meilleure mémoire à long terme des nouveaux mots, ce qui contribuerait à expliquer l'avantage observé. Ces effets de transfert de la formation musicale au niveau sémantique et de la mémoire à long terme révèlent l'importance des fonctions cognitives générales et ouvrent de nouvelles perspectives pour l'éducation et la rééducation.
Le concept d'automate, central en théorie des langages, est l'outil d'appréhension naturel et efficace de nombreux problèmes concrets. L'usage intensif des automates finis dans un cadre algorithmique s 'illustre par de nombreux travaux de recherche. La correction et l 'évaluation sont les deux questions fondamentales de l'algorithmique. Une méthode classique d'évaluation s'appuie sur la génération aléatoire contrôlée d'instances d'entrée. Les travaux d´écrits dans cette thèse s'inscrivent dans ce cadre et plus particulièrement dans le domaine de la génération aléatoire uniforme d'automates finis. Cette construction s'appuie sur la méthode symbolique. Des résultats théoriques et une étude expérimentale sont exposés. Un générateur aléatoire d'automates non-déterministes illustre ensuite la souplesse d'utilisation de la méthode de Monte-Carlo par Chaînes de Markov (MCMC) ainsi que la mise en œuvre de l'algorithme de Metropolis-Hastings pour l'échantillonnage à isomorphisme près. Un résultat sur le temps de mélange est donné dans le cadre général. L 'échantillonnage par méthode MCMC pose le problème de l'évaluation du temps de mélange dans la chaîne. En s'inspirant de travaux antérieurs pour construire un générateur d'automates partiellement ordonnés, on montre comment différents outils statistiques permettent de s'attaquer à ce problème.
Les dernières tendances de l'informatique distribuées préconisent le Fog computing qui étend les capacités du Cloud en bordure du réseau, à proximité des objets terminaux etdes utilisateurs finaux localisé dans le monde physique. Le Fog est un catalyseur clé des applications de l'Internet des Objets (IoT), car il résout certains des besoins que le Cloud ne parvient à satisfaire, tels que les faibles latences, la confidentialité des données sensibles, la qualité de service ainsi que les contraintes géographiques. Pour cette raison, le Fog devient de plus en plus populaire et trouve des cas d'utilisations dans de nombreux domaines tels que la domotique, l'agriculture, la e-santé, les voitures autonomes, etc. Le Fog, cependant, est instable car il est constitué de milliards d'objets hétérogènes au sein d'un écosystème dynamique. Les objets de l'IoT tombent en pannes régulièrement parce qu'ils sont produits en masse à des couts très bas. Dans un tel écosystème, les défaillances produisent des comportements incohérents qui peuvent provoquer des situations dangereuses et coûteuses dans le monde physique. Cette Thèse propose une approche autonome de gestion de la résilience des applications IoT déployées en environnement Fog. L'approche proposée comprend quatre tâches fonctionnelles : (i) sauvegarde d'état, (ii) surveillance, (iii) notification des défaillances, et (iv) reprise sur panne. Chaque tâche est un regroupement de rôles similaires et est mise en oeuvre en tenant compte les spécificités de l'écosystème (e.g., hétérogénéité, ressources limitées). La sauvegarde d'état vise à sauvegarder les informations sur l'état de l'application. Ces informations sont constituées des données d'exécution et de la mémoire volatile, ainsi que des messages échangés et fonctions exécutées par l'application. La surveillance vise à observer et à communiquer des informations sur le cycle de vie de l'application. Lors d'une défaillance, des notifications sont propagées à la partie de l'application affectée par cette défaillance. La propagation des notifications vise à limiter la portée de l'impact de la défaillance et à fournir un service partiel ou dégradé. Pour établir une reprise sur panne, l'application est reconfigurée et les données enregistrées lors de la tâche de sauvegarde d'état sont utilisées afin de restaurer un état cohérent de l'application par rapport au monde physique. La procédure de reprise sur panne en assurant la cohérence cyber-physique évite les impacts dangereux et coûteux de la défaillance sur le monde physique. L'approche proposée a été validée à l'aide de techniques de vérification par modèle afin de vérifier que certaines propriétés importantes sont satisfaites. Cette approche de résilience a été mise en oeuvre sous la forme d'une boîte à outils, F3ARIoT, destiné aux développeurs. F3ARIoT a été évalué sur une application domotique. Les résultats montrent la faisabilité de son utilisation sur des déploiements réels d'applications Fog-IoT, ainsi que des performances satisfaisantes par rapport aux utilisateurs.
Dans le milieu aéronautique professionnel (un des secteurs professionnels les plus sûr au monde), la gestion des conséquences des erreurs humaines doit être améliorée pour garantir une sécurité maximum. Cependant, la mise en place de ces techniques est rendue difficile par les particularités des systèmes sociotechniques complexes (la certification, la complexité des systèmes conçus, le nombre de personnes impliquées…). Notre étude a pour but de développer et de valider des outils d'aide à la conception centrée sur l'utilisateur, notamment pour le traitement automatique de grande quantité de données. Les résultats de cette méthode par jugement d'expert ont été comparés à ceux obtenus à l'aide d'outils de traitement automatique. De proposer une méthodologie permettant l'extraction automatique de situations à risque pouvant donner lieu à des études plus approfondies, sur simulateur par exemple. Cette étape est primordiale dans cadre de la conception centrée utilisateur. Les liens établis avec les études des incidents/accidents laissent envisager des impacts positifs sur la sécurité aérienne.
L'estimation de la pose humaine et la reconnaissance des activités humaines sont des étapes importantes dans de nombreuses applications comme la robotique, la surveillance et la sécurité, etc. Actuellement abordées dans le domaine, ces tâches ne sont toujours pas résolues dans des environnements non-coopératifs particulièrement. Dans un premier temps, nous nous sommes concentrés sur la reconnaissance des actions complexes depuis des vidéos. Pour ceci, nous avons introduit une représentation spatio-temporelle indépendante du point de vue. Plus précisément, nous avons capturé le mouvement de la personne en utilisant un capteur de profondeur et l'avons encodé en 3D pour le représenter. Un descripteur 3D a ensuite été utilisé pour la classification des séquences avec la méthodologie bag-of-words. Pour la deuxième partie, notre objectif était l'estimation de pose articulée, qui est souvent une étape intermédiaire pour la reconnaissance de l'activité. Notre motivation était d'incorporer des informations à partir de capteurs multiples et de les fusionner pour surmonter le problème de l'auto-occlusion. Nous avons démontré que les contraintes géométriques et les paramètres de cohérence d'apparence sont efficaces pour renforcer la cohérence entre les points de vue, aussi que les paramètres classiques. Finalement, nous avons évalué ces nouvelles méthodes sur des datasets publics, qui vérifie que l'utilisation de représentations indépendantes de la vue et l'intégration d'informations à partir de points de vue multiples améliore la performance pour les tâches ciblées dans le cadre de cette manuscrit.
Les entités nommées extraites et leurs contextes textuels seront catégorisés à l'aide d'approches en Traitement Automatique des Langues afin de rendre compte de la sémantique des relations exprimées dans les textes. La description précise des documents au travers de formats d'encodage tels que le XML-EAD, Dublin Core ou XML-TEI facilite ainsi la conception d'instruments de recherche, les tâches de fouille de données mais aussi le partage en ligne ou au sein d'une communauté de chercheurs. Une ontologie sera proposée afin de répondre à ces besoins et nous permettre de formaliser cette problématique du point de vue du Web Sémantique. Ce projet reposera sur l'utilisation et l'extension d'ontologies déjà existantes telles que DBPedia ou ISA Programme Person Core Vocabulary, dont l'intérêt dans des tâches de désambiguïsation des entités nommées a déjà été largement démontré. Les documents numérisés seront traités pour la reconnaissance optique de caractères (OCR) et enfin encodés au format XML-TEI. Les données produites seront enregistrées en suivant le standard RDF, nous assurant ainsi l'interopérabilité avec tous les services appartenant à l'OpenLinkedData, afin de produire des inférences textuelles basées sur SPARQL. La méthodologie pour le peuplement de l'ontologie sera développée spécifiquement pour la tâche d'extraction et de catégorisation d'entités nommées et de données spatio-temporelles, afin de pouvoir extraire automatiquement les relations entre ces entités dans les documents.
Nous abordons, dans cette thèse, une étude sur les représentations continues de mots (en anglais word embeddings) appliquées à la détection automatique des erreurs dans les transcriptions de la parole. Notre étude se concentre sur l'utilisation d'une approche neuronale pour améliorer la détection automatique des erreurs dans les transcriptions automatiques, en exploitant les word embeddings. L'exploitation des embeddings repose sur l'idée que la détection d'erreurs consiste à trouver les possibles incongruités linguistiques ou acoustiques au sein des transcriptions automatiques. L'intérêt est donc de trouver la représentation appropriée du mot qui permet de capturer des informations pertinentes pour pouvoir détecter ces anomalies. Notre contribution dans le cadre de cette thèse porte sur plusieurs axes. D'abord, nous commençons par une étude préliminaire dans laquelle nous proposons une architecture neuronale capable d'intégrer différents types de descripteurs, y compris les embeddings. Ensuite, nous nous focalisons sur une étude approfondie des représentations continues de mots. Cette étude porte d'une part sur l'évaluation de différents types d'embeddings linguistiques puis sur leurs combinaisons. D'autre part, elle s'intéresse aux embeddings acoustiques de mots. Puis, nous présentons une étude sur l'analyse des erreurs de classifications, qui a pour objectif de percevoir les erreurs difficiles à détecter. Finalement, nous exploitons les embeddings linguistiques et acoustiques ainsi que l'information fournie par notre système de détections d'erreurs dans plusieurs cadres applicatifs.
Le principal objectif de cette étude est de situer les constructions verbales du wolof dans une perspective typologique. Il s'agit tout d'abord de proposer une description synthétique du système de prédication verbale du wolof dans une perspective typologique, en nous appuyant sur les travaux de référence concernant la conjugaison du wolof. L'analyse typologique de ces constructions périphrastiques nous sert de base empirique pour proposer une nouvelle approche de la notion d'auxiliaire. Nous considérons que, dans une perspective typologique, l'auxiliaire ne doit pas être défini comme une catégorie lexicale spécifique, ni comme une étape dans un chemin de grammaticalisation, mais plutôt comme un élément prédicatif autonome ayant une fonction spécifique. Par ailleurs, nous proposons une analyse constructionnelle de l'organisation du système de prédication verbale du wolof. Nous considérons que les constructions verbales du wolof ne forment pas un ensemble non structuré d'entités indépendantes, mais plutôt un système extrêmement structuré (un réseau de constructions). En outre, nous montrons que certaines idiosyncrasies apparentes dans le paradigme de conjugaison du wolof peuvent s'expliquer à la lumière de la diachronie. Enfin, nous proposons une analyse comparative des constructions verbales des langues atlantiques afin de déterminer ce qui, dans la conjugaison du wolof, est issu du proto-atlantique.
Cette thèse propose un modèle permettant la mise en œuvre d'un système de raisonnement à partir de cas capable d'adapter des procédures représentées sous forme de texte en langue naturelle, en réponse à des requêtes d'utilisateurs. Bien que les cas et les solutions soient sous forme textuelle, l'adaptation elle-même est d'abord appliquée à un réseau de contraintes temporelles exprimées à l'aide d'une algèbre qualitative, grâce à l'utilisation d'un opérateur de révision des croyances. Des méthodes de traitement automatique des langues sont utilisées pour acquérir les représentations algébriques des cas ainsi que pour regénérer le texte à partir du résultat de l'adaptation
Le travail de cette thèse vise à proposer un système support à la créativité selon une architecture multi-agents afin de gérer les connaissances nécessaires et produites durant un atelier de créativité. Ce travail contribue à la recherche scientifique à différents égards. Au préalable de concevoir un quelconque système, une revue des systèmes actuels supportant la créativité est réalisée pour déterminer leurs limites en termes de processus de créativité et de modes de collaboration. Pour répondre à ces limites, l'approche d'ingénierie des connaissances est adoptée. A partir de la modélisation organisationnelle d'un atelier de créativité, l'organisation des agents informatiques qui vont contribuer à la gestion des connaissances en est déduite. Par la suite, une ontologie de l'atelier de créativité est formalisée à partir de la modélisation de l'organisation afin d'apporter une représentation des connaissances et de l'environnement aux agents. Ainsi, l'architecture multi-agents proposée pour concevoir un système support à la créativité permet d'explorer de nouveaux modes de traitement des connaissances notamment concernant l'évaluation des idées. Une méthodologie d'évaluation des idées selon des méthodes d'analyse multicritère est proposée. En complément de cette méthodologie, le traitement automatique des idées a été expérimenté afin d'aider les évaluateurs dans leur tâche
Identifier les leviers de satisfaction des consommateurs est aujourd'hui capital dans un monde où la relation que tisse une entreprise avec ses clients est sa plus grande richesse. Le domaine de la fouille d'opinion, dans lequel s'inscrit cette thèse, propose des méthodes permettant de répondre à ce besoin. Celles-ci nécessitent cependant une mise à jour constante de ressources spécialisées qui sont la pierre angulaire des outils d'analyse d'opinion. Ce travail vise à développer des stratégies d'acquisition et de structuration de ces ressources, qui prennent la forme de lexiques, de patrons morpho-syntaxiques ou de textes annotés. Nous traitons ensuite la question de l'apport des différents types de ressources, dont il ressort que la meilleure stratégie est de les utiliser de concert. Enfin, nous proposons des méthodes d'acquisition pour chacune des ressources répondant non seulement aux besoins de la fouille d'opinion mais également aux contraintes du contexte industriel au sein duquel ces recherches sont menées.
La thèse vise à explorer des modèles et algorithmes d'extraction de connaissance et d'interconnexion de bases de données hétérogènes, appliquée à la gestion de contenus tels que rencontrés fréquemment dans le quotidien des journalistes. Le travail se déroulera dans le cadre du projet ANR ContentCheck (2016-2019) qui fournit le financement et dans le cadre duquel nous collaborons aussi avec l'équipe "Les Décodeurs" (journalistes spécialisés dans le fact-checking) du journal Le Monde. La démarche scientifique de la thèse se décompose comme suit :  1. Identifier les technologies et domaines de gestion de contenu (texte, données, connaissances) intervenant de façon recurrente (ou dont le besoin est ressenti comme important) dans l'activité des journalistes. Il est par exemple déjà clair que ceux-ci ont l'habitude d'utiliser "en interne" quelques bases de données construites par les journalistes eux-mêmes ;  Parmi ces problèmes, identifier ceux pour lesquels des solutions techniques (informatiques) sont connues, et le cas échéant mis en oeuvre dans des systèmes existants. 2. S'attaquer aux problèmes ouverts (sur le plan de la recherche), pour lesquels des réponses satisfaisantes manquent, liés à la modélisation et à l'algorithmique efficace pour des contenus textuels, sémantiques, et des données, dans un contexte journalistique.
Cette thèse traite dela mise en correspondance de séquences appliquée au word spotting (localisation de motsclés dans des images de documents sans en interpréter le contenu). De nombreux algorithmes existent mais très peu d'entre eux ont été évalués dans ce contexte. Nous commençons donc par une étude comparative de ces méthodes sur plusieurs bases d'images de documents historiques. Nous proposons ensuite un nouvel algorithme réunissant la plupart des possibilités offertes séparément dans les autres algorithmes. Ainsi, le FSM (Flexible Sequence Matching) permet de réaliser des correspondances multiples sans considérer des éléments bruités dans la séquence cible, qu'ils se situent au début, à la fin ou bien au coeur de la correspondance. Nous étendons ensuite ces possibilités à la séquence requête en définissant un nouvel algorithme (ESC : Examplary Sequence Cardinality). Finalement, nous proposons une méthode d'appariement alternative utilisant une mise en correspondance inexacte de chaines de codes (shape code) décrivant les mots.
Le travail présenté dans cette thèse explore les méthodes pratiques utilisées pour faciliter l'entraînement et améliorer les performances des modèles de langues munis de très grands vocabulaires. La principale limite à l'utilisation des modèles de langue neuronaux est leur coût computationnel : il dépend de la taille du vocabulaire avec laquelle il grandit linéairement. La façon la plus aisée de réduire le temps de calcul de ces modèles reste de limiter la taille du vocabulaire, ce qui est loin d'être satisfaisant pour de nombreuses tâches. La plupart des méthodes existantes pour l'entraînement de ces modèles à grand vocabulaire évitent le calcul de la fonction de partition, qui est utilisée pour forcer la distribution de sortie du modèle à être normalisée en une distribution de probabilités. Ici, nous nous concentrons sur les méthodes à base d'échantillonnage, dont le sampling par importance et l'estimation contrastive bruitée. Ces méthodes permettent de calculer facilement une approximation de cette fonction de partition. L'examen des mécanismes de l'estimation contrastive bruitée nous permet de proposer des solutions qui vont considérablement faciliter l'entraînement, ce que nous montrons expérimentalement. Enfin, nous exploitons les informations données par les unités sous-mots pour enrichir les représentations en sortie du modèle.
De nombreuses villes d'Asie du Sud-Est subissent de graves inondations liées d'une part à l'intensité croissante des précipitations et d'autre part à une urbanisation rapide souvent due à une planification urbaine non maitrisée. L'évaluation quantitative des risques d'inondation nécessite deux éléments essentiels : (1) un modèle numérique de terrain (MNT) haute définition, et (2) une chronologie de précipitations la plus longue possible. Un MNT haute définition est à la fois coûteux et long à acquérir. Les chronologies de précipitations longues sont fréquemment indisponibles dans de nombreux sites et ne présentent pas toujours une durée suffisante pour une définition pertinente des valeurs extrêmes. Cette thèse présente une approche opérationnelle pour générer des MNT haute définition et suggère une stratégie pour définir des pluies extrêmes en dehors de chronologies de précipitations longues. Des données pour la production des MNT issues de capteurs satellitaires-mission SRTM (Shuttle Radar Topography Mission) et images multi spectrales Sentinel 2-ont été utilisées et mises en œuvre. Un réseau de neurones artificiels (ANN) est utilisé afin d'améliorer la qualité du MNT. A la suite de cet apprentissage, le réseau de neurones peut être mis en œuvre pour générer, à faible coût, des MNT haute résolution dans des secteurs où les données sont partiellement indisponibles. Le MNT issu des données SRTM améliorées montre (1) une qualité nettement supérieure au MNT initial puisque le RMSE passe de 34% à 57% du RMSE ; (2) la clarté visuelle est largement améliorée ; et (3) le réseau de drainage calculé correspond davantage au réseau réel. La production de ce MNT amélioré permet une meilleure modélisation des processus d'inondation et augmente la qualité des résultats des simulations hydrauliques. Des données de précipitation issues d'un Modèle Climatologique Régional (RCM) haute résolution spatiale ainsi que des prévisions issues de données ERA-Interim (WRF / ERAI) ont été extraites, analysées et comparées avec les observations haute résolution enregistrées à Singapour. Les comparaisons ont également été effectuées avec les courbes Intensité-Durée-Fréquence (IDF) qui sont utilisées pour l'évaluation des risques d'inondation. Un modèle hydraulique détaillé a été construit avec le système de modélisation MIKE 21 pour toute la métropole de Jakarta à partir d'un MNT amélioré et des précipitations associées à des périodes de retour de 50 et 100 ans. Des cartes d'inondation ont été générées et sont utilisées par les services gestionnaires. Cet exemple démontre que les nouvelles méthodes et approches proposées dans cette thèse sont pertinentes pour produire une évaluation des risques d'inondation pertinente lorsque des données locales (MNT haute résolution et données pluviométriques sur une période longue) sont insuffisantes ou indisponibles.
Dans le traitement des langues, la représentation vectorielle des mots est une question clé, permettant l'emploi d'algorithmes basés sur des modèles mathématiques. Récemment ont émergé de nouvelles méthodes de vectorisation et leur évaluation est cruciale. Les évaluations actuelles portent surtout sur l'anglais, d'où le besoin d'évaluations multilingues. Notre travail porte sur la généralisation des évaluations, leur comparaison, l'élaboration d'évaluations nouvelles, et sur WordNet, ressource multilingue. Nous avons choisi 6 vectorisations : CBOW, SkipGram, GloVe, une plus ancienne comme base, et deux plus récentes. Comme méthode indirecte, nous prenons la catégorisation sémantique avec des algorithmes de clustering pour comparer les vectorisations sous-jacentes. Les algorithmes choisis sont : le plus utilisé (Kmeans), un neuronal (SOM) et un probabiliste (EM).Notre système applique les évaluations sur des corpus en anglais, français et arabe, et compare les vectorisations. Nous proposons 5 méthodes d'évaluation, dont 4 fondées sur WordNet, et un protocole d'évaluation par sondage. Nos résultats donnent trois classements des méthodes validés sur ces langues, s'accordant sur plusieurs points décisifs, et invalident certaines des évaluations existantes. Pour nos propres évaluations, le protocole est validé, et, de nos 5 méthodes, une a été invalidée (nous avons analysé les causes de l'échec), une a été validée pour l'anglais et le français, mais pas pour l'arabe, deux ont été validées sur les trois langues, et une reste à explorer.
En particulier, la démocratisation des dispositifs mobiles (comme les PCs, Smartphones, Tablettes, etc.) a rendu l'information accessible par le grand public partout et à tout moment, ce qui est l'origine du concept d'informatique ubiquitaire. L'approche classique des systèmes de l'informatique ubiquitaire, qui répondent aux besoins des utilisateurs indépendants les uns des autres, a été bouleversée par l'introduction de la dimension sociale. Ce rapprochement est à l'origine d'une discipline naissante « le pervasive social computing » ou l'informatique socio-pervasive. Les applications socio-pervasives connaissent une véritable expansion. Ces dernières intègrent de plus en plus la notion de communauté. Elle est, aujourd'hui, au cœur des problématiques de personnalisation et d'adaptation des applications informatiques. Dans le cadre de cette thèse, nous étudions sous différents aspects les applications informatiques centrées communautés existantes et soulignons un certain nombre de carences au niveau même de la notion de communauté, des modèles de communautés, ou encore des architectures dédiées à ces applications communautaires, etc. Pour remédier à ces défauts, nous proposons trois principales contributions :  Une architecture dynamiquement reconfigurable pour promouvoir les communautés spontanées en aidant les utilisateurs nomades à intégrer des communautés environnantes et à découvrir les services dédiés. Nous montrons la faisabilité de nos propositions pour la conception et le développement d'applications communautaires spontanées grâce au prototype Taldea. Enfin, nous testons les approches proposées de découverte de communauté et de services à travers plusieurs scénarios caractérisés par la mobilité et l'ubiquité.
Le catalogue occupe une place privilégiée dans l'offre de service des bibliothèques universitaires, pivot de l'intermédiation. Depuis 10 ans, il traverse une crise grave, voyant les usagers le délaisser à la faveur des moteurs de recherche généralistes. Le web, plus qu'un sérieux concurrent, devance aujourd'hui les systèmes d'information documentaires, et devient le point d'entrée principal pour la recherche d'information. Les bibliothèques tentent de structurer un espace documentaire qui soit habité par les usagers, au sein duquel se développe l'offre de service, mais celle-ci se présente encore comme une série de silos inertes, sans grande possibilité de navigation, malgré de considérables efforts d'ingénierie et des pistes d'évolution vers les outils de découverte. La profession, consciente de cette crise profonde, après avoir accusé les remous occasionnés par la dimension disruptive du numérique, cherche des moyens pour adapter et diversifier son offre, fluidifier la diffusion de l'information, et se réinvente un rôle d'intermédiation en cherchant à tirer profit des nouvelles pratiques des usagers, de leurs nouvelles attentes, et de nouvelles perspectives. Les bibliothèques placent leur espoir dans de nouveaux modèles de données, tentent d'y ajouter un niveau d'abstraction favorisant les liaisons avec l'univers de la connaissance. L'évolution vers le web sémantique semble une opportunité à saisir pour valoriser les collections et les rendre exploitables dans un autre contexte, au prix d'importants efforts que cette analyse tente de mesurer. Une approche constructiviste fondée sur l'observation participante et le recueil de données offre une vision issue de l'intérieur de la communauté des bibliothèques sur l'évolution des catalogues et des outils d'intermédiation, et ouvre des perspectives sur leurs enjeux.
Capturer automatiquement des équivalences sémantiques entre des unités de texte est une tâche complexe mais qui s'avère indispensable dans de nombreux contextes. Dans cette thèse, nous proposons une étude détaillée de la tâche d'acquisition de paraphrases sous-phrastiques à partir de paires d'énoncés sémantiquement liés. Nous démontrons empiriquement que les corpus parallèles monolingues, bien qu'extrêmement rares, constituent le type de ressource le plus adapté pour ce genre d'étude. Nos expériences mettent en jeu cinq techniques d'acquisition, représentatives de différentes approches et connaissances, en anglais et en français. Un résultat important de notre étude est l'identification de paraphrases qui défient actuellement les techniques étudiées, lesquelles sont classées et quantifiées en anglais et français. Nous examinons également dans cette thèse l'impact de la langue, du type du corpus et la comparabilité des paires des énoncés utilisés sur la tâche d'acquisition de paraphrases sous-phrastiques. Nous présentons le résultat d'une analyse de la performance des différentes méthodes testées en fonction des difficultés d'alignement des paires de paraphrases d'énoncés. Nous donnons, ensuite, un compte rendu descriptif et quantitatif des caractéristiques des paraphrases trouvées dans les différents types de corpus étudiés ainsi que celles qui défient les approches actuelles d'identification automatique.
La fraude par carte de crédit est devenue un problème majeur dans le secteur des paiements électroniques. Dans cette thèse, nous étudions la détection de fraude basée sur les données transactionnelles et abordons plusieurs de ces défis complexes en utilisant des méthodes d'apprentissage automatique visant à identifier les transactions frauduleuses qui ont été émises illégitimement au nom du titulaire légitime de la carte. En particulier, nous explorons plusieurs moyens d'exploiter les informations contextuelles au-delà des attributs de base d'une transaction, notamment au niveau de la transaction, au niveau de la séquence et au niveau de l'utilisateur. Au niveau des transactions, nous cherchons à identifier les transactions frauduleuses qui présentent des caractéristiques distinctes des transactions authentiques. Nous avons mené une étude empirique de l'influence du déséquilibre des classes et des horizons de prévision sur la performance d d'un classifieur de type random forest. Nous augmentons les transactions avec des attributs supplémentaires extraits de sources de connaissances externes et montrons que des informations sur les pays et les événements du calendrier améliorent les performances de classification, particulièrement pour les transactions ayant lieu sur le Web. Au niveau de la séquence, nous cherchons à détecter les fraudes qui sont difficiles à identifier en elles-mêmes, mais particulières en ce qui concerne la séquence à court terme dans laquelle elles apparaissent. Nous utilisons un réseau de neurone récurrent (LSTM) pour modéliser la séquence de transactions. Nos résultats suggèrent que la modélisation basée sur des LSTM est une stratégie prometteuse pour caractériser des séquences de transactions ayant lieu en face à face, mais elle n'est pas adéquate pour les transactions ayant lieu sur le Web. Au niveau de l'utilisateur, nous travaillons sur une stratégie existante d'agrégation d'attributs et proposons un concept flexible nous permettant de calculer de nombreux attributs au moyen d'une syntaxe simple. Nous fournissons une implémentation basée sur CUDA pour pour accélerer le temps de calcul de deux ordres de grandeur. Notre étude de sélection des attributs révèle que les agrégats extraits de séquences de transactions des utilisateurs sont plus utiles que ceux extraits des séquences de marchands. En ce qui concerne les travaux futurs, nous évoquons des méthodes d'apprentissage artificiel simples et transparentes pour la détection des fraudes par carte de crédit et nous esquissons une modélisation simple
Les études diachroniques nous permettent d'étudier les changements, d'analyser les régularités dans le changement, afin de mieux connaître la faculté de langage. Les études sont des études qualitatives avec une méthode descriptive et comparative en utilisant l'analyse de corpus des listes lexicales anciennes. La formalisation nous permet d'automatiser et de simplifier les règles de la langue sans transgresser le système grammatical de la langue pour les appliquer dans le domaine de la traduction automatique. En passant par les études de la morphonologie et de la morphologie, nous pouvons voir clairement la structure et le mécanisme de la grammaire malaise pour chercher une solution face au problème de la combinatoire affixale du lexique malais de façon systématique. L'étude d'un tel sujet a été réalisée pour comprendre clairement les structures internes de l'allomorphe, de la dérivation verbale et nominale à partir de radicaux verbaux du malais qui nous aident à formuler un modèle de traitement automatique du langage ou un outil pédagogique par le schéma et des règles. Nous avons étudié la corrélation entre les verbes et les affixes pour reconnaître, analyser et interpréter les données du corpus pour voir la relation du verbe avec le système de dérivation malais. Le résultat de la statistique du corpus nous a montré la fiabilité de notre modèle parce qu'il existe la cohérence entre les données et le modèle fabriqué
Ce sujet de recherche est centré sur la problématique de gestion des connaissances et des savoir-faire au sein d'une l'organisation humanitaire intervenant en situations d'urgence. Plus précisément, il s'agira d'expliquer pourquoi et comment les décisions ont été prises en situation d'urgence. Il s'agit donc ici de développer une méthodologie adaptée pour répondre aux problématiques spécifiques des organisations humanitaires. La méthodologie proposée sera testée et validée avec une ou plusieurs études de cas et permettra la formalisation de l'expérience, essentielle pour la rendre exploitable.
La plateforme LegalCluster propose de définir des « clusters » juridiques dans une approche écosystémique, en vue de garantir la pérennité des informations produites par les acteurs juridiques. Dans ce contexte, LegalCluster cherche à proposer un service de recommandations juridiques pour les cabinets d'expert, les directions et les clients des fonctions juridiques, localement, dans un cluster donné ou à plus large échelle. De fait, plusieurs contraintes majeures sont à relever pour permettre de délivrer un service pertinent et s'intégrant pleinement dans le milieu juridique : le secret professionnel, contexte juridique de l'utilisateur et la prise en compte de la jurisprudence. Il est alors nécessaire de proposer une solution générale pouvant être appliquée dans un cluster local, associé à des ressources et connaissances externes. Pour se faire, cette approche repose sur une combinaison entre le traitement naturel de la langue (TALN) et la recherche d'information (RI). Le traitement de la langue, reposant sur des modèles d'apprentissage, a pour but de catégoriser les questions juridiques. Du fait d'avoir des clusters locaux et différents codes juridiques à associer, notre approche s'intègre pleinement dans le cadre du Topic Modeling et de la génération de résumés abstractifs. Afin d'intégrer à la fois l'aspect métier et l'aspect protection du secret professionnel, le modèle devra être entrainer sur un corpus global, puis localement, ce qui implique la caractérisation des modèles dans ces deux composantes tout en reposant sur des données implicites comme les textes de lois et la jurisprudence. La partie Recherche d'Information a pour but de faciliter l'analyse des corpus juridiques tout en donnant de nouvelles fonctionnalités en vue de définir un moteur de recherche juridique dédié. Ainsi, l'indexation classique permet de faire la correspondance entre des mots-clés et les documents les plus pertinents associés, mais également sur les métadonnées du métier et permettra de produire une vision synthétique « locale » des données métiers. Une première étape d'intégration est de faire évoluer le modèle de structure de données pour l'adapter aux métadonnées, mais également modifier le cœur du moteur de recherche pour lui permettre de fournir des fonctions de scoring adaptés aux besoins de LegalCluster. Le moteur de recherche facilitera l'analyse et l'association de ces textes nécessaire à la brique Apprentissage.
Modéliser les préférences des utilisateurs est incontournable dans de nombreux problèmes de la vie courante, que ce soit pour la prise de décision individuelle ou collective ou le raisonnement stratégique par exemple. Comme les agents ne connaissent pas complètement leurs préférences à l'avance, nous avons seulement deux moyens de les déterminer pour pouvoir raisonner ensuite : nous pouvons les inférer soit de ce que les agents disent, soit de leurs actions non-linguistiques. Dans ce travail, nous proposons une nouvelle approche pour extraire et raisonner sur les préférences exprimées dans des dialogues de négociation. Après avoir extrait les préférences de chaque tour de dialogue, nous utilisons la structure discursive pour suivre leur évolution au fur et à mesure de la conversation. Nous utilisons les CP-nets, un modèle de représentation des préférences, pour formaliser et raisonner sur ces préférences extraites. Cette méthode est d'abord évaluée sur différents corpus de négociation pour lesquels les résultats montrent que la méthode est prometteuse. Nous l'appliquons ensuite dans sa globalité avec des raisonnements issus de la Théorie des Jeux pour prédire les échanges effectués, ou non, dans le jeu de marchandage Cette thèse présente donc une nouvelle approche à la croisée de plusieurs domaines : le Traitement Automatique des Langues (pour l'extraction automatique des préférences et le raisonnement sur leur verbalisation), l'Intelligence Artificielle (pour la modélisation et le raisonnement sur les préférences extraites) et la Théorie des Jeux (pour la prédiction des actions stratégiques dans un jeu de marchandage)
En français, plus de 8 schémas morphologiques sont disponibles pour construire des noms d'événements à partir de verbes. Les suffixations en-age,-ment,-ion,-ure,-ance,-ade,-aison,-erie, ainsi que la conversion de verbe à nom (défendre-&gt ; défense, arriver-&gt ; arrivée), partagent toutes des fonctions équivalentes sur le plan sémantique, et sélectionnent parfois les mêmes bases verbales (e.g. : gouverner-&gt ; gouvernance, gouvernement, gouverne). S'inscrivant dans le cadre théorique de la morphologie lexématique (Matthews 1974 ; Anderson 1992 ; Aronoff 1994 ; Fradin 2003 ; Booij 2005), et à la suite des travaux de Lindsay and Aronoff (2013) et Aronoff (2014, 2015, 2017, 2019), cette thèse propose d'élaborer une réflexion sur les dynamiques à l'origine de la coexistence de ces multiples schémas de nominalisation rivaux. En prenant appui sur une base de relations morphologiques construite à partir de corpus massifs, nous faisons l'utilisation de méthodes computationnelles (word embeddings, modèles statistiques, modèles analogiques ; Arndt-Lappe 2014, Lapraye 2017, Wauquier et al. 2018, Bonami and Thuilier 2019 pour d'autres cas de compétition morphologique) de sorte à faire émerger ces contraintes, et proposons de modéliser quantitativement la répartition du lexique construit par ces schémas.
Il s'agit de minimiser l'écart sémantique existant entre le document et le domaine considérés. La méthode s'appuie sur une collection d'enrichissement constituée automatiquement en lien avec le domaine d'intérêt et procède par extraction de mots-clés et détection de thèmes (topics). La méthode d'enrichissement proposé a été appliquée à des pages web. Elle est robuste au bruit indépendant du domaine considéré et facile transporter dans différentes langues. Elle est pauvre en connaissances mais elle exploite les résultats de moteurs de recherche de manière optimisée. L'approche a été testée sur différentes langues. L'évaluation a été conduite sur le français et sur 10 domaines différents. Les résultats ont été évalués par des utilisateurs dans un contexte applicatif réel et par comparaison avec des approches de références. On observe une bonne précision des résultats et une bonne cohérence sémantique au sein de chaque thème, avec une amélioration significative par rapport aux méthodes d'extraction des mots-clé et de détection de thèmes de l'état de l'art.
Cette thèse s'inscrit dans le cadre d'un travail descriptif en phonétique acoustique, avec comme objet d'étude les productions des voyelles du luxembourgeois dans la parole native et non native. L'intérêt est de concilier la variation du luxembourgeois, une langue principalement parlée, composée de nombreuses variétés régionales, évoluant dans un contexte multilingue, et son apprentissage dans le cadre de l'enseignement des langues étrangères au Grand-Duché de Luxembourg. Comme nous partons du fait que l'apprentissage d'une langue implique la connaissance des traits contrastifs des sons, nous nous intéressons aux productions de locuteurs dont la langue maternelle possède des traits différents de ceux du luxembourgeois, comme le français, afin de voir si ces traits sont reproduits dans la parole non native. Les productions vocaliques de locuteurs francophones sont étudiées en comparaison aux productions de locuteurs natifs de la région située autour de la capitale du Grand-Duché de Luxembourg, dont la variété sert de référence à l'enseignement du luxembourgeois en tant que langue étrangère. Le but de l'analyse est : -d'étendre les descriptions sur les propriétés acoustiques des voyelles produites dans une variété régionale du Grand-Duché de Luxembourg,-de relever les difficultés de productions de locuteurs francophones qui apprennent le luxembourgeois,-d'interpréter les résultats dans le cadre de l'enseignement du luxembourgeois en tant que langue étrangère. Une partie importante du travail empirique a été consacrée à la collecte des données et la création d'un corpus obtenu à travers des enregistrements de 10 locuteurs luxembourgophones et de 10 locuteurs francophones. Le corpus de compose de 12h30 de parole lue et spontanée, incluant de la parole native et non native du luxembourgeois, ainsi que de la parole native du français. Ce corpus constitue un premier corpus sur la parole native et non native du luxembourgeois et permet de faire divers analyses comparatives. Dans notre étude, nous avons fait des analyses acoustiques sur les données de la parole lue. La méthodologie utilisée a permis d'effectuer des comparaisons entre les données de la parole native et non native du luxembourgeois ainsi qu'entre les données de la L1 et la L2 des francophones. Les résultats ont apporté des informations tant sur les productions natives que sur les productions non natives des voyelles. Ces résultats, ainsi que les descriptions approfondies sur les voyelles dans la parole native, enrichissent non seulement les connaissances sur le luxembourgeois, mais aussi sur la variété servant de référence au luxembourgeois en tant que langue étrangère. En outre, ils ouvrent des perspectives d'étude sur le luxembourgeois en problématisant l'instauration de règles pour ce type d'enseignement, malgré l'absence d'un enseignement suivi de la langue dans les écoles et l'évolution des variétés régionales sur un territoire géographique concentré.
Nous montrons également que les CNNs prédisent clairement la distribution des taux d'erreurs sur une collection d'enregistrements, contrairement à l'approche état de l'art qui génère une distribution éloignée de la réalité. Ensuite, nous analysons des facteurs impactant les deux approches de prédiction. Nous évaluons également l'impact de la quantité d'apprentissage des systèmes de prédiction ainsi que la robustesse des systèmes appris avec les sorties d'un système de RAP particulier et utilisés pour prédire la performance sur une nouvelle collection de données. Nos résultats expérimentaux montrent que les deux approches de prédiction sont robustes et que la tâche de prédiction est plus difficile sur des tours de parole courts ainsi que sur les tours de parole ayant un style de parole spontané. Enfin, nous essayons de comprendre quelles informations sont capturées par notre modèle neuronal et leurs liens avec différents facteurs. Nos expériences montrent que les représentations intermédiaires dans le réseau encodent implicitement des informations sur le style de la parole, l'accent du locuteur ainsi que le type d'émission. Pour tirer profit de cette analyse, nous proposons un système multi-tâche qui se montre légèrement plus efficace sur la tâche de prédiction de performance.
Le Web d'aujourd'hui est formé, entre autres, de deux types de contenus que sont les données structurées et liées du Web sémantique et les contributions subjectives des utilisateurs du Web social. L'approche ViewpointS a été conçue comme un formalisme creuset apte à intégrer ces deux types de contenus, en préservant la subjectivité des interactions du Web Social. ViewpointS est une approche de représentation subjective des connaissances. L'approche propose aussi un second degré de subjectivité. En effet, viewpoints peuvent être interprétés différemment selon l'utilisateur grâce au mécanisme de perspective. Les ressources du Web sont représentées et liées par les viewpoints dans le Graphe de Connaissances. A partir du Graphe de Connaissances contenant les viewpoints et les ressources du Web une Carte de Connaissances composée de synapses et de ressources est créée qui est le fruit de l'interprétation et de l'agrégation des viewpoints. Chaque viewpoint contribue à la création, au renforcement ou à l'affaiblissement d'une synapse qui relie deux ressources. L'échange de viewpoints est le processus de sélection qui permet l'évolution des synapses d'une manière analogue à celles qui évoluent dans le cerveau au fil d'un sélectionnisme neuronal. Nous investiguons dans cette étude l'impact que peut avoir la représentation subjective des connaissances dans divers scénarii de construction collective des connaissances.
Nous définissons un cadre d'expression de contraintes et proposons deux approches : l'une naïve et l'autre de réécriture, permettant de filtrer dynamiquement les réponses valides obtenues à partir des sources éventuellement non-valides, ceci au moment de la requête et non pas en cherchant à les valider dans les sources des données. Ces deux approches ont été évaluées et ont montré la praticabilité de notre système. Ceci est notre principale contribution : nous étendons l'ensemble de systèmes de réécriture déjà connus(Chase, C&amp ; BC, PerfectRef, Xrewrite, etc.) avec une nouvelle solution efficace pour ce nouveau défi qu'est le filtrage des résultats en fonction d'un contexte utilisateur. Nous généralisons également les conditions de déclenchement de contraintes par rapport aux solutions existantes, en utilisant la notion de one-way MGU.
Les données de type tabulaire contiennent souvent des variables catégorielles, considérées comme des entrées non numériques avec un nombre fixe et limité d'éléments uniques, appelés catégories. De nombreux algorithmes d'apprentissage statistique nécessitent une représentation numérique des variables catégorielles. Pour cela, plusieurs stratégies existent, dont la plus courante est celle de l'encodage one-hot, qui fonctionne bien dans le cadre de l'analyse statistique classique (en termes de puissance de prédiction et d'interprétation) lorsque le nombre de catégories reste faible. Cependant, les données catégorielles non-uniformisées présentent le risque d'avoir une grande cardinalité et des redondances. En effet, les entrées peuvent partager des informations sémantiques et/ou morphologiques, et par conséquent, plusieurs entrées peuvent refléter la même entité. Sans une étape de nettoyage ou d'agrégation au préalable, les méthodes d'encodage courantes peuvent perdre en efficacité du fait d'une représentation vectorielle erronée. Même avec des données volumineuses, ces méthodes s'avèrent être performantes, et dans certains cas, elles génèrent des vecteurs facilement interprétables. Par conséquent, nos méthodes peuvent être appliquées à l'apprentissage statistique automatique (AutoML) sans aucune intervention humaine.
Dans cette thèse, nous nous intéressons au problème spécifique de l'apprentissage de structure de modèles graphiques probabilistes, c'est-à-dire trouver la structure la plus efficace pour représenter une distribution, à partir seulement d'un ensemble d'échantillons D ∼ p(v). Dans une première partie, nous passons en revue les principaux modèles graphiques probabilistes de la littérature, des plus classiques (modèles dirigés, non-dirigés) aux plus avancés (modèles mixtes, cycliques etc.). Puis nous étudions particulièrement le problème d'apprentissage de structure de modèles dirigés (réseaux Bayésiens), et proposons une nouvelle méthode hybride pour l'apprentissage de structure, H2PC (Hybrid Hybrid Parents and Children), mêlant une approche à base de contraintes (tests statistiques d'indépendance) et une approche à base de score (probabilité postérieure de la structure). Dans un second temps, nous étudions le problème de la classification multi-label, visant à prédire un ensemble de catégories (vecteur binaire y P (0, 1)m) pour un objet (vecteur x P Rd). Dans ce contexte, l'utilisation de modèles graphiques probabilistes pour représenter la distribution conditionnelle des catégories prend tout son sens, particulièrement dans le but minimiser une fonction coût complexe. Nous passons en revue les principales approches utilisant un modèle graphique probabiliste pour la classification multi-label (Probabilistic Classifier Chain, Conditional Dependency Network, Bayesian Network Classifier, Conditional Random Field, Sum-Product Network), puis nous proposons une approche générique visant à identifier une factorisation de p(y|x) en distributions marginales disjointes, en s'inspirant des méthodes d'apprentissage de structure à base de contraintes. Nous démontrons plusieurs résultats théoriques, notamment l'unicité d'une décomposition minimale, ainsi que trois procédures quadratiques sous diverses hypothèses à propos de la distribution jointe p(x, y).
L'utilisation croissante des réseaux sociaux et de capteurs génère une grande quantité de données qui peuvent être représentées sous forme de graphiques complexes. Il y a de nombreuses tâches allant de l'analyse de l'information à la prédiction et à la récupération que l'on peut imaginer sur ces données où la relation entre les noeuds de graphes devrait être informative. Tous les modèles proposés utilisent le cadre d'apprentissage de la représentation dans sa variante déterministe ou gaussienne. Dans un premier temps, nous avons proposé deux algorithmes pour la tâche de marquage de graphe hétérogène, l'un utilisant des représentations déterministes et l'autre des représentations gaussiennes. Contrairement à d'autres modèles de pointe, notre solution est capable d'apprendre les poids de bord lors de l'apprentissage simultané des représentations et des classificateurs. Deuxièmement, nous avons proposé un algorithme pour la prévision des séries chronologiques relationnelles où les observations sont non seulement corrélées à l'intérieur de chaque série, mais aussi entre les différentes séries. Nous utilisons des représentations gaussiennes dans cette contribution. C'était l'occasion de voir de quelle manière l'utilisation de représentations gaussiennes au lieu de représentations déterministes était profitable. Enfin, nous appliquons l'approche d'apprentissage de la représentation gaussienne à la tâche de filtrage collaboratif. Ceci est un travail préliminaire pour voir si les propriétés des représentations gaussiennes trouvées sur les deux tâches précédentes ont également été vérifiées pour le classement. L'objectif de ce travail était de généraliser ensuite l'approche à des données plus relationnelles et pas seulement des graphes bipartis entre les utilisateurs et les items.
Dans le domaine médical, l'informatisation des professions de santé et le développement du dossier médical personnel (DMP) entraîne une progression rapide du volume d'information médicale numérique. Le besoin de convertir et de manipuler toute ces informations sous une forme structurée constitue un enjeu majeur. C'est le point de départ de la mise au point d'outils d'interrogation appropriés pour lesquels, les méthodes issues du traitement automatique du langage naturel (TALN) semblent bien adaptées. Les travaux de cette thèse s'inscrivent dans le domaine de l'analyse de documents médicaux et traitent de la problématique de la représentation de l'information biomédicale (en particulier du domaine radiologique) et de son accès. Nous montrons l'intérêt de l'hypothèse de non séparation entre les différents types de connaissances dans le cadre d'une analyse de documents. Ce réseau combine poids et annotations sur des relations typées entre des termes et des concepts ainsi qu'un mécanisme d'inférence dont l'objet est d'améliorer la qualité et la couverture du réseau. Nous décrivons comment à partir d'informations sémantiques présentes dans le réseau, il est possible de définir une augmentation des index bruts construits pour chaque comptes rendus afin d'améliorer la recherche documentaire. Nous présentons, ensuite, une méthode d'extraction de relations sémantiques entre des termes ou concepts. Cette extraction est réalisée à l'aide de patrons linguistiques auxquels nous avons rajouté des contraintes sémantiques. Les résultats des évaluations montrent que l'hypothèse de non séparation entre les différents types de connaissances améliorent la pertinence de l'indexation. L'augmentation d'index permet une amélioration du rappel alors que les contraintes sémantiques améliorent la précision de l'extraction de relations.
L'évolution moléculaire procède par divergence depuis un ancêtre commun et en combinant des fragments d'objets évoluant d'origines différentes, par des processus introgressifs. Les transferts horizontaux de gènes sont probablement les plus connus de ces processus, mais l'introgression affecte aussi d'autres niveaux d'organisation biologique. Ainsi, la plupart des objets biologiques évoluant peuvent être composés de parties d'origines phylogénétiques différentes et décrits comme composites. Cette évolution modulaire se modélise mal par des arbres, puisque les objets composites ne sont pas seulement le résultat d'une divergence depuis un ancêtre. Les réseaux sont bien plus aptes à modéliser la modularité, et la théorie des graphes peut être utilisée pour chercher dans ces réseaux des patrons caractéristiques d'une évolution réticulée. Pendant cette thèse, j'ai développé le logiciel CompositeSearch qui détecte les gènes composites dans des jeux de données de séquences massifs, jusqu'à plusieurs millions de séquences. Cet algorithme a été utilisé pour identifier et quantifier l'abondance des gènes composites dans des environnements de sols pollués ainsi que dans les plasmides. Les résultats montrent que d'importantes adaptations et nouveautés biologiques découlent de processus œuvrant au niveau subgénique. De plus, les réseaux fournissent un cadre conceptuel dont l'utilité va bien au-delà de l'évolution moléculaire et je les ai appliqués à d'autres objets évoluant, comme les animaux (réseaux de traits morphologiques) et les langues (réseaux de mots). Dans les deux cas, la modularité se révèle être une conséquence évolutive majeure, et obéit à des règles encore à préciser.
Cette dernière décennie a donné lieu à la réémergence des méthodes d'apprentissage machine basées sur les réseaux de neurones formels sous le nom d'apprentissage profond. Bien que ces méthodes aient permis des avancées majeures dans le domaine de l'apprentissage machine, plusieurs obstacles à la possibilité d'industrialiser ces méthodes persistent, notamment la nécessité de collecter et d'étiqueter une très grande quantité de données ainsi que la puissance de calcul nécessaire pour effectuer l'apprentissage et l'inférence avec ce type de réseau neuronal. Dans cette thèse, nous proposons d'étudier l'adéquation entre des algorithmes d'inférence et d'apprentissage issus des réseaux de neurones biologiques pour des architectures matérielles massivement parallèles. Nous montrons avec trois contributions que de telles adéquations permettent d'accélérer drastiquement les temps de calculs inhérents au réseaux de neurones. Nous proposons également l'introduction d'une architecture hiérarchique basée sur des cellules complexes. Nous montrons que l'adéquation pour GPU accélère les traitements par un facteur sept, tandis que l'architecture hiérarchique atteint un facteur mille. La deuxième contribution présente trois algorithmes de propagation de décharges neuronales adaptés aux architectures parallèles. Nous réalisons une étude complète des modèles computationels de ces algorithmes, permettant de sélectionner ou de concevoir un système matériel adapté aux paramètres du réseau souhaité. Dans notre troisième axe nous présentons une méthode pour appliquer la règle Spike-Timing-Dependent-Plasticity à des données images afin d'apprendre de manière non-supervisée des représentations visuelles. Nous montrons que notre approche permet l'apprentissage d'une hiérarchie de représentations pertinente pour des problématiques de classification d'images, tout en nécessitant dix fois moins de données que les autres approches de la littérature.
L'utilisation de commandes gestuelles est une nouvelle méthode d'interaction sur interface tactile. Une bonne méthode pour faciliter la mémorisation de ces commandes gestuelles est de laisser l'utilisateur les personnaliser. Ce contexte applicatif induit une situation d'apprentissage croisé, où l'utilisateur doit mémoriser le jeu de symboles elle système doit apprendre à reconnaître les différents symboles. Cela implique un certain nombre de contraintes, à la fois sur le système de reconnaissance de symboles ct sur le système de supervision de son apprentissage. Il faut par exemple que le classifieur puisse apprendre à partir de peu de données, continuer à apprendre pendant son utilisation et suivre toute évolution des données indéfiniment. Le superviseur doit quant à lui optimiser la coopération entre l'utilisateur et le système de reconnaissance pour minimiser les interactions tout en maximisant l'apprentissage. Cette thèse présente d'une part, le système d'apprentissage évolutif Evolve oo, capable d'apprendre rapidement il partir de peu de données et de suivre les changements de concepts. L'intégration d'oubli dans le processus d'apprentissage permet de maintenir le gain de l'apprentissage indéfiniment, permettant ainsi l'ajout de classes à n'importe quel moment de l'utilisation du système ct garantissant son évolutivité « à vie » . Le superviseur actif en-ligne lntuiSup permet d'optimiser les interactions avec l'utilisateur pour entraîner un système d'apprentissage lorsque l'utilisateur est dans la boucle. Il permet de faire évoluer la proportion de données que l'utilisateur doit étiqueter en fonction de la difficulté du problème et de l'évolution de l'environnement (changements de concepts). L'utilisation d'une méthode de « dopage » de l'apprentissage permet d'optimiser la répartition de ces interactions avec l'utilisateur pour maximiser leur impact sur l'apprentissage.
Le travail présenté ici est pour une première partie à l'intersection de l'apprentissage profond et anonymisation. Un cadre de travail complet est développé dans le but d'identifier et de retirer, dans une certaine mesure et de manière automatique, les caractéristiques privées d'une identité pour des données de type image. Deux méthodes différentes de traitement des données sont étudiées. Ces deux méthodes partagent une même architecture de réseau en forme de Y et cela malgré des différences concernant les types de couches de neurones utilisés conséquemment à leur objectif d'utilisation. La première méthode de traitement des données concerne la création ex nihilo de représentations anonymisées permettant un compromis entre la conservation des caractéristiques pertinentes et l'altération des caractéristiques privées. Ce cadre de travail a abouti à une nouvelle fonction de perte. Par conséquent les représentations anonymisées sont de même nature que les données initiales (une image est transformée en une image anonymisée). Cette tâche a conduit à un autre type d'architecture (toujours en forme de Y) et a fourni des résultats fortement sensibles au type des données. La seconde partie de mon travail concerne une autre sorte d'information utile : cette partie se concentre sur la surveillance du comportement des prédicteurs. Dans le cadre de l'analyse de "modèle boîte noire", on a uniquement accès aux probabilités que le prédicteur fournit (sans aucune connaissance du type de structure/architecture qui produit ces probabilités). L'étude de ces probabilités peut servir d'indicateur d'inadéquation potentiel entre les statistiques des données et les statistiques du modèle. Deux méthodes utilisant différents outils sont présentées. La première compare la fonction de répartition des statistiques de sortie d'un ensemble connu et d'un ensemble de données à tester. La seconde fait intervenir deux outils : un outil reposant sur l'incertitude du classifieur et un autre outil reposant sur la matrice de confusion. Ces méthodes produisent des résultats concluants.
L'évolution constante des besoins des clients et des utilisateurs exige une réponse rapide de la part des équipes logicielles. Cela crée une forte demande pour un fonctionnement sans rupture des processus logiciels. L'intégration, la livraison et le déploiement continus, également connus sous le nom de DevOps, ont fait d'énormes progrès en rendant les processus logiciels réactifs au changement. Aujourd'hui, la plupart des besoins sont exprimés en langage naturel. Cette approche a un grand pouvoir expressif, mais au détriment d'autres aspects de la qualité des exigences telles que la traçabilité, la réutilisabilité, la vérifiabilité et la compréhensibilité. Le défi est ici d'améliorer ces aspects sans sacrifier l'expressivité. Cette approche a motive et inspire les travaux de la présente thèse. Alors que l'approche multiexigences se concentre sur la traçabilité et la compréhensibilité, l'approche Seamless Object-Oriented Requirements (SOOR) présentée dans cette thèse prend en compte la vérifiabilité, la réutilisabilité et la compréhensibilité. Cette thèse explore l'hypothèse de Martin Glinz selon laquelle, pour soutenir la continuité, les exigences logicielles devraient être des objets. L'exploration confirme l'hypothèse et aboutit à un ensemble de méthodes basées sur des outils pour spécifier, valider, vérifier et réutiliser les exigences orientées objets. La contribution technique réutilisable la plus importante de cette thèse est une bibliothèque Eiffel prête à l'emploi de patrons de classes, qui capturent les modèles d'exigences logicielles récurrents. Les exigences orientées objets, concrètes et sans rupture, héritent de ces patrons et deviennent des clients du logiciel spécifié. Cette thèse s'appuie sur plusieurs expériences et montre que la nouvelle approche propose favorise la vérifiabilité, la réutilisabilité et la compréhensibilité des exigences tout en maintenant l'expressivité à un niveau acceptable. Les expérimentations mettent en oeuvre plusieurs exemples, dont certains sont des standards de l'état de l'art de l'ingénierie des exigences. Chaque expérimentation illustre un problème par un exemple, propose une solution générale et montre comment la solution règle le problème. Alors que l'expérimentation s'appuie sur Eiffel et son support d'outils avancés, tels que la preuve et les tests automatisés, chaque idée présentée dans l'approche SOOR s'adapte conceptuellement à tout langage de programmation orienté objet typé statiquement, possédant un mécanisme de généricité et un support élémentaire pour les contrats.
Dans cette thèse, nous proposons d'utiliser des techniques fondées sur l'analyse factorielle pour la modélisation acoustique pour le traitement automatique de la parole, notamment pour la Reconnaissance Automatique de la parole. Nous nous sommes, dans un premier temps, intéressés à la réduction de l'empreinte mémoire des modèles acoustiques. Notre méthode à base d'analyse factorielle a démontré une capacité de mutualisation des paramètres des modèles acoustiques, tout en maintenant des performances similaires à celles des modèles de base. Nous proposons, comme alternative, une représentation vectorielle des états : les fac- teur d'états. Ces facteur d'états nous permettent de mesurer efficacement la similarité entre les états des MMC au moyen d'une distance euclidienne, par exemple. Grâce à cette représenation vectorielle, nous proposons une méthode simple et efficace pour la construction de modèles acoustiques avec des états partagés. Cette procédure s'avère encore plus efficace dans le cas de langues peu ou très peu dotées en ressouces et en connaissances linguistiques. Enfin, nos efforts se sont portés sur la robustesse des systèmes de reconnaissance de la parole face aux variabilités acoustiques, et plus particulièrement celles générées par l'environnement. Nous nous sommes intéressés, dans nos différentes expérimentations, à la variabilité locuteur, à la variabilité canal et au bruit additif. Grâce à notre approche s'appuyant sur l'analyse factorielle, nous avons démontré la possibilité de modéliser ces différents types de variabilité acoustique nuisible comme une composante additive dans le domaine cepstral. Nous soustrayons cette composante des vecteurs cepstraux pour annuler son effet pénalisant pour la reconnaissance de la parole
Les points de repère sont présentés dans les applications de différents domaines tels que le biomédical ou le biologique. C'est également l'un des types de données qui ont été utilisés dans différentes analyses, par exemple, ils ne sont pas seulement utilisés pour mesurer la forme de l'objet, mais également pour déterminer la similarité entre deux objets. En biologie, les repères sont utilisés pour analyser les variations inter-organismes. Cependant, l'offre de repères est très lourde et le plus souvent, ils sont fournis manuellement. Ces dernières années, plusieurs méthodes ont été proposées pour prédire automatiquement les points de repère, mais la dureté existe, car elles se sont concentrées sur des données spécifiques. Cette thèse porte sur la détermination automatique de points de repère sur des images biologiques, plus spécifiquement sur d'images 2D des coléoptères. Dans le cadre de nos recherches, nous avons collaboré avec des biologistes pour créer un ensemble de données comprenant les images de 293 coléoptères. Pour chaque coléoptère de cette donnée, 5 images correspondent à 5 parties prises en compte, par exemple tête, élytre, pronotum, mandibule gauche et droite. Avec chaque image, un ensemble de points de repère a été proposé manuellement par les biologistes. La première étape, nous avons apporté une méthode qui a été appliquée sur les ailes de mouche, à appliquer sur notre jeu de données dans le but de tester la pertinence des techniques de traitement d'image sur notre problème. Deuxièmement, nous avons développé une méthode en plusieurs étapes pour fournir automatiquement les points de repère sur les images. Ces deux premières étapes ont été effectuées sur les images de la mandibule qui sont considérées comme évidentes pour l'utilisation des méthodes de traitement d'images. Troisièmement, nous avons continué à considérer d'autres parties complexes restantes de coléoptères. En conséquence, nous avons utilisé l'aide de Deep Learning. Nous avons conçu un nouveau modèle de Convolutional Neural Network, nommé EB-Net, pour prédire les points de repère sur les images restantes. De plus, nous avons proposé une nouvelle procédure pour augmenter le nombre d'images dans notre jeu de données, ce qui est considéré comme notre limite à appliquer Deep Learning. Enfin, pour améliorer la qualité des coordonnées prédites, nous avons utilisé Transfer Learning, une autre technique de Deep Learning. Pour ce faire, nous avons formé EB-Net sur les points clés publics du visage. Ensuite, ils ont été transférés pour affiner les images de coléoptère. Les résultats obtenus ont été discutés avec les biologistes et ils ont confirmé que la qualité des repéres prédits est suffisamment bonne sur la plane statistique pour remplacer les repères manuels pour la plupart des analyses de morphométrie différentes.
Cette thèse aborde la problématique de l'accès à l'information scientifique et technique véhiculée par de grands ensembles documentaires. Le modèle résultant permet une immersion documentaire, et ce grâce à trois types de processus complémentaires : des processus endogènes (exploitant le corpus pour analyser le corpus), exogènes (faisant appel à des ressources externes) et anthropogènes (dans lesquels les compétences de l'utilisateur sont considérées comme ressource) sont combinés. Tous concourent à l'attribution d'une place centrale à l'utilisateur dans le système, en tant qu'agent interprétant de l'information et concepteur de ses connaissances, dès lors qu'il est placé dans un contexte industriel ou spécialisé.
Le présent travail aborde le thème de la complexité sémantique dans le langage naturel, et il propose une hypothèse basée sur certaines caractéristiques des phrases du langage naturel qui déterminent la difficulté pour l'interpretation humaine. Nous visons à introduire un cadre théorique général de la complexité sémantique de la phrase, dans lequel la difficulté d'élaboration est liée à l'interaction entre deux composants : la Mémoire, qui est responsable du rangement des représentations d'événements extraites par des corpus, et l'Unification, qui est responsable de la combinaison de ces unités dans des structures plus complexes. Nous proposons que la complexité sémantique depend de la difficulté de construire une représentation sémantique de l'événement ou de la situation exprimée par une phrase, qui peut être récupérée directement de la mémoire sémantique ou construit dynamiquement en satisfaisant les contraintes contenus dans les constructions. Pour tester nos intuitions, nous avons construit un Distributional Semantic Model pour calculer le coût de composition de l'unification des phrases. Les tests sur des bases de données psycholinguistiques ont révélé que le modèle est capable d'expliquer des phénomènes sémantiques comme la mise à jour context-sensitive des attentes sur les arguments et les métonymies logiques.
Au cours des dernières années, innombrables produits ont été conçus en utilisant des modèles numériques 3D, où les courants logiciels pour la conception et le dessin technique utilisent des modèles CAO (Conception Assistée par Ordinateur). Ces logiciels sont utilisés dans de nombreux domaines, tels que l'automobile, la marine, l'aérospatiale et plus encore. Par conséquent, il est utile de disposer de solutions technologiques capables d'évaluer les similitudes de différents produits afin que l'utilisateur puisse récupérer des modèles existants et avoir ainsi accès à des informations utiles pour la nouvelle conception. Le concept de similarité a été largement étudié dans la littérature et il est bien connu que deus objets puissent être similaire de plusieurs façons. Ces multiples possibilités rendent complexe l'évaluation de la similarité entre deux objets. À ce jour, de nombreuses méthodes ont été proposées pour l'identification de différentes similitudes entre les pièces, mais peu de travaux abordent cet problème en évoquant d'assemblages de pièces. Sur la base de ces exigences, nous proposons de définir un système qui permettant la récupération des assemblages des pièces similaires en fonction de multiple critères de similarité. Pour ce faire, il faut avoir un descripteur qui peut gérer les informations nécessaires pour caractériser les différentes similitudes entre les deux modèles. Par conséquent, l'un des points principaux de ce travail sera la définition d'un descripteur capable de coder les données nécessaires à l'évaluation des similarités. De plus, certaines des informations du descripteur peuvent être disponibles dans le modèle CAO, tandis que d'autres devront être extraites de manière appropriée. Par conséquent, des algorithmes seront proposés pour extraire les informations nécessaires pour remplir les champs du descripteur. Enfin, pour une évaluation de la similarité, plusieurs mesures entre les modèles seront définies, de sorte que chacune d'entre elles évaluent un aspect particulier de leur similarité.
La traduction automatique (TA) a connu des progrès significatifs ces dernières années et continue de s'améliorer. La TA est utilisée aujourd'hui avec succès dans de nombreux contextes, y compris les environnements professionnels de traduction et les scénarios de production. Cependant, le processus de traduction requiert souvent des connaissances plus larges qu'extraites de corpus parallèles. Étant donné qu'une injection de connaissances humaines dans la TA est nécessaire, l'un des moyens possibles d'améliorer TA est d'assurer une collaboration optimisée entre l'humain et la machine. À cette fin, de nombreuses questions sont posées pour la recherche en TA : Comment détecter les passages où une aide humaine devrait être proposée ? Comment faire pour que les machines exploitent les connaissances humaines obtenues afin d'améliorer leurs sorties ? Enfin, comment optimiser l'échange : minimiser l'effort humain impliqué et maximiser la qualité de TA ? Diverses solutions sont possibles selon les scénarios de traductions considérés. Dans cette thèse, nous avons choisi de nous concentrer sur la pré-édition, une intervention humaine en TA qui a lieu ex-ante, par opposition à la post-édition, où l'intervention humaine qui déroule ex-post. En particulier, nous étudions des scénarios de pré-édition ciblés où l'humain doit fournir des traductions pour des segments sources difficiles à traduire et choisis avec soin. Les scénarios de la pré-édition impliquant la pré-traduction restent étonnamment peu étudiés dans la communauté. De plus, dans un contexte multilingue, des difficultés communes peuvent être résolues simultanément pour de nombreuses langues. De tels scénarios s'adaptent donc parfaitement aux contextes de production standard, où l'un des principaux objectifs est de réduire le coût de l'intervention humaine et où les traductions sont généralement effectuées à partir d'une langue vers plusieurs langues à la fois. Dans ce contexte, nous nous concentrons sur la TA de revues systématiques en médecine. En considérant cet exemple, nous proposons une méthodologie indépendante du système pour la détection des difficultés de traduction. Nous définissons la notion de difficulté de traduction de la manière suivante : les segments difficiles à traduire sont des segments pour lesquels un système de TA fait des prédictions erronées. Nous formulons le problème comme un problème de classification binaire et montrons que, en utilisant cette méthodologie, les difficultés peuvent être détectées de manière fiable sans avoir accès à des informations spécifiques au système. Nous intégrons les résultats de notre procédure de détection des difficultés dans un protocole de pré-édition qui permet de résoudre ces difficultés par pré-traduction. Nous évaluons le protocole dans un cadre simulé et montrons que la pré-traduction peut être à la fois utile pour améliorer la qualité de la TA et réaliste en termes d'implication des efforts humains. En outre, les effets indirects sont significatifs. Les résultats de ces expériences pilotes confirment les résultats obtenus dans le cadre simulé et ouvrent des perspectives encourageantes pour des tests ultérieures.
Ce travail de recherche se situe dans le champ interdisciplinaire des sciences de l'information et de la communication (SIC) et a pour but d'explorer la question de l'usage du web sémantique en bibliothèques numériques. Le web oblige les bibliothèques à repenser leurs organisations, leurs activités, leurs pratiques et leurs services, afin de se repositionner en tant qu'instituts de références pour la diffusion des savoirs. Dans cette thèse, nous souhaitons comprendre les contextes d'usage du web sémantique en bibliothèques numériques françaises. Il s'agit de s'interroger sur les apports du web sémantique au sein de ces bibliothèques, ainsi que sur les défis et les obstacles qui accompagnent sa mise en place. Ensuite, nous nous intéressons aux pratiques documentaires et à leurs évolutions suite à l'introduction du web sémantique en bibliothèques numériques. La problématique s'attache au rôle que peuvent jouer les professionnels de l'information dans la mise en place du web sémantique en bibliothèques numériques. Après avoir sélectionné 98 bibliothèques numériques suite à une analyse de trois recensements, une enquête s'appuyant sur un questionnaire vise à recueillir des données sur l'usage du web sémantique dans ces bibliothèques. Ensuite, une deuxième enquête réalisée au moyen d'entretiens permet de mettre en évidence les représentations qu'ont les professionnels de l'information du web sémantique et de son usage en bibliothèque, ainsi que de l'évolution de leurs pratiques professionnelles. Les résultats montrent que la représentation des connaissances dans le cadre du web sémantique nécessite une intervention humaine permettant de fournir le cadre conceptuel pour déterminer les liens entre les données. Enfin, les professionnels de l'information peuvent devenir des acteurs du web sémantique, dans le sens où leurs rôles ne se limitent pas à l'utilisation du web sémantique mais aussi au développement de ses standards pour assurer une meilleure organisation des connaissances.
Le travail présenté dans cette thèse vise à étudier la coordination entre gestes manuels et parole lors de la production d'énoncés multimodaux. Les études menées s'intéressent plus particulièrement aux relations temporelles entre les deux modalités. Cette coordination a été étudiée plus précisément dans le cadre de la désignation qui est réalisable à la fois dans la modalité manuelle (geste de pointage) et dans la modalité parole ( « montrer avec la voix » , en utilisant la focalisation et/ou les démonstratifs par exemple). Les études présentées ont été menées dans un environnement contrôlé de laboratoire afin d'obtenir des mesures précises et reproductibles en minimisant les facteurs extérieurs de variations intra-et inter-participants. Les productions des locuteurs peuvent ainsi être comparées entre-elles en se focalisant sur les facteurs d'intérêt toutes choses maintenues le plus possible égales par ailleurs. Un travail particulier de mise en place des protocoles a néanmoins permis de maintenir une tâche assez naturelle afin de ne pas induire des productions trop artificielles. Les deux premières études se sont intéressées à la production conjointe de gestes manuels et de parole contenant de la focalisation. Plusieurs types de gestes ont été comparés (geste de pointage, geste de battement et geste d'appui sur un bouton) lors d'une tâche de désignation. Il a été montré que la production de focalisation attire le geste manuel quel que soit son type mais que l'attraction est plus « précise » et fine pour le pointage. Par ailleurs, l'apex du geste de pointage semble être cooccurent à une cible articulatoire plutôt qu'acoustique. La seconde étude manipule le lien de désignation le geste de pointage et la parole. Elle montre, en exhibant deux stratégies adoptées par les participants, la complexité des mécanismes mis en jeu dans cette coordination. Finalement, une troisième étude s'intéresse à la coordination dans une tâche interactive et collaborative plus naturelle. Les résultats montrent une cooccurrence de la partie du geste qui montre avec l'information qui lui est complémentaire en parole, i.e. avec le nom de l'objet à poser à l'endroit désigné par le geste de pointage, plutôt qu'avec la partie de la parole qui désigne, i.e. le démonstratif. Ce mémoire propose par ailleurs une exploration des procédés d'annotation multimodaux mis en place pour l'annotation de tâches semi-contrôlées mais applicables à des cas plus généraux. Le manuscrit se conclut par une mise en perspective des résultats pour l'amélioration de certains modèles de production conjointe gestes manuels/parole et fournit quelques pistes utilisables dans le domaine des agents conversationnels ainsi que pour la détection de pathologies.
Le travail est mené dans une perspective contrastive français – russe et ce, sur des corpus informatisés de données issues des deux langues. Nous avons constitué deux types de corpus : comparable (comportant les textes originaux, 60 M de mots, de la base de Frantext et Ruscorpora) et parallèle (coprus de traduction, 10 M de mots, aligné avec le logiciel Alinea d'O. Kraif). Les questions qui sous-tendent ce travail concernent les trois points suivants. Nous vérifions cela grâce à l'analyse de la combinatoire syntaxique et lexicale de ces constructions. Nous estimons que l'approche contrastive permet de mieux expliciter les similitudes et les différences aspectuelles au sein des CVN dans les deux langues, ainsi que de mettre en évidence les différences dans l'expression de l'aspect en français et en russe.
Cette thèse se centre sur les discordances dans l'ellipse périphérique (RNR) et propose une analyse basée sur l'identité de lexème entre le matériel manquant et le matériel périphérique. Dans cette thèse, nous contestons cette hypothèse. Nous avons analysé 5 types de discordance dans l'ellipse périphérique : discordances de polarité, de possessifs, de prépositions, de voix et de formes verbales. Les discordances sont assez nombreuses même dans des écrits soignés. Dans tous les cas, les discordances sont résolues par la forme qui correspond au second conjoint. Les résultats des expériences de jugements d'acceptabilité et de mouvements oculaires permettent d'intégrer les discordances dans la grammaire. Les résultats sont compatibles avec les analyses qui postulent l'identité sémantique entre le matériel manquant et l'antécédent pour l'ellipse. Nous proposons une analyse formelle en HPSG.Nous comparons les résultats obtenus avec les cas de coordination lexicale. Nous montrons que l'accord de proximité s'applique (Villavicencio et al. (2005)) et nous proposons une analyse HPSG pour la coordination de verbes et de prépositions.
Le développement de spécifications formelles correctes pour des systèmes et logiciels commence par l'analyse et la compréhension des besoins du client. Entre ces besoins décrits en langage naturel et leur spécification définie dans un langage formel précis, un écart existe et rend la tâche de développement de plus en plus difficile à accomplir. Nous sommes face à deux mondes distincts. Ce travail de thèse a pour objectif d'expliciter et d'établir des interactions entre ces deux mondes et de les faire évoluer en même temps. Par interaction, nous désignons les liens, les échanges et les activités se déroulant entre les différents documents. Parmi ces activités, nous présentons la validation comme un processus rigoureux qui démarre dès l'analyse des besoins et continue tout au long de l'élaboration de leur spécification formelle. Au fur et à mesure du développement, des choix sont effectués et les retours des outils de vérification et de validation permettent de détecter des lacunes aussi bien dans les besoins que dans la spécification. L'évolution des deux mondes est décrite via l'introduction d'un nouveau besoin dans un système existant et à travers l'application de patrons de développement. Ils facilitent la tâche de développement et aident à éviter les risques d'oublis. Quel que soit le choix, des questions se posent tout au long du développement et permettent de déceler des lacunes, oublis ou ambiguïtés dans l'existant.
Ce travail vise à permettre un accès efficace à des informations pertinentes malgré le volume croissant des données disponibles au format électronique. Ainsi, nous avons proposé une méthode mixte combinant des techniques de traitement automatique des langues pour extraire des connaissances à partir de textes et la réutilisation de ressources sémantiques existantes pour l'étape de conceptualisation. Nous avons par ailleurs développé une méthode d'alignement de termes français-anglais pour l'enrichissement terminologique de l'ontologie. L'application de notre méthodologie a permis de créer une ontologie bilingue de la maladie d'Alzheimer. Ensuite, nous avons élaboré des algorithmes pour supporter la RI sémantique guidée par une ontologie. Les concepts issus d'une ontologie ont été utilisés pour décrire automatiquement les documents mais aussi pour reformuler les requêtes. Nous nous sommes intéressés à : 1) l'identification de concepts représentatifs dans des corpus, 2) leur désambiguïsation, 3), leur pondération selon le modèle vectoriel, adapté aux concepts et 4) l'expansion de requêtes. Ces propositions ont permis de mettre en œuvre un portail de RI sémantique dédié à la maladie d'Alzheimer. Par ailleurs, le contenu des documents à indexer n'étant pas toujours accessible dans leur ensemble, nous avons exploité des informations incomplètes pour déterminer les concepts pertinents permettant malgré tout de décrire les documents. Pour cela, nous avons proposé deux méthodes de classification de documents issus d'un large corpus, l'une basée sur l'algorithme des k plus proches voisins et l'autre sur l'analyse sémantique explicite. Ces méthodes ont été évaluées sur de larges collections de documents biomédicaux fournies lors d'un challenge international.
Le traitement automatique de documents consiste en la transformation dans un format compréhensible par un système informatique de données présentes au sein de documents et compréhensibles par l'Homme. L'analyse de document et la compréhension de documents sont les deux phases du processus de traitement automatique de documents. Étant donnée une image de document constituée de mots, de lignes et d'objets graphiques tels que des logos, l'analyse de documents consiste à extraire et isoler les mots, les lignes et les objets, puis à les regrouper au sein de blocs. La compréhension de documents fait correspondre à cette structure géométrique une structure logique en considérant des liaisons logiques (à gauche, à droite, au-dessus, en-dessous) entre les objets du document. Un système de traitement de documents doit être capable de : (i) localiser une information textuelle, (ii) identifier si cette information est pertinente par rapport aux autres informations contenues dans le document, (iii) extraire cette information dans un format compréhensible par un programme informatique. Pour la réalisation d'un tel système, les difficultés à surmonter sont liées à la variabilité des caractéristiques de documents, telles que le type (facture, formulaire, devis, rapport, etc.), la mise en page (police, style, agencement), la langue, la typographie et la qualité de numérisation du document. Dans ce mémoire, nous considérons en particulier des documents numérisés, également connus sous le nom d'images de documents. Plus précisément, nous nous intéressons à la localisation d'informations textuelles au sein d'images de factures. Les factures sont des documents très utilisés mais non standards. En effet, elles contiennent des informations obligatoires (le numéro de facture, le numéro siret de l'émetteur, les montants, etc.) qui, selon l'émetteur, peuvent être localisées à des endroits différents. Les contributions présentées dans ce mémoire s'inscrivent dans le cadre de la localisation et de l'extraction d'informations textuelles fondées sur des régions identifiées au sein d'une image de document. Tout d'abord, nous présentons une approche de décomposition d'une image de documents en sous-régions fondée sur la décomposition quadtree. La méthode fondée sur cette approche, que nous proposons, permet de déterminer efficacement les régions contenant une information d'intérêt à extraire. Dans une autre approche, incrémentale et plus flexible, nous proposons un système d'extraction d'informations textuelles qui consiste en un ensemble de régions prototypes et de chemins pour parcourir ces régions prototypes. Le cycle de vie de ce système comprend cinq étapes :  - Construction d'un jeu de données synthétiques à partir d'images de factures réelles contenant les informations d'intérêts. - Partitionnement des données produites. - Détermination des régions prototypes à partir de la partition obtenue. - Détermination des chemins pour parcourir les régions prototypes, à partir du treillis de concepts d'un contexte formel convenablement construit. - Mise à jour du système de manière incrémentale suite à l'insertion de nouvelles données
L'étude des surfaces continentales constitue un enjeu majeur à l'échelle mondiale pour le suivi et la gestion des territoires, notamment en matière de répartition entre l'expansion urbaine, terres agricoles et espaces naturels. Dans ce contexte, les cartes d'OCcupation des Sols (OCS) caractérisant la couverture biophysique des terres émergées sont un atout essentiel pour l'analyse des surfaces continentales. Les algorithmes de classification supervisée permettent, à partir de séries temporelles annuelles d'images satellites et de données de référence, de produire automatiquement la carte de la période correspondante. Cependant, les données de référence sont une information coûteuse à obtenir surtout sur de grandes étendues. En effet, les campagnes de relevés terrain requièrent un fort coût humain, et les bases de données sont associées à de longs délais de mises à jour. De plus, ces données de référence disposent d'une validité limitée à la période correspondante, en raison des changements d'OCS. Ces changements concernent essentiellement l'expansion urbaine au détriment des surfaces naturelles, et les terres agricoles soumises à la rotation des cultures. L'objectif général de la thèse vise à proposer des méthodes de production de cartes d'OCS sans exploiter les données de référence de la période correspondante. Les travaux menés s'appuient sur un historique d'OCS. Cet historique regroupe toutes les informations disponibles pour la zone concernée : cartes d'OCS, séries temporelles, données de référence, modèles de classification, etc. Une première partie des travaux considère que l'historique ne contient qu'une seule période. Ainsi, nous avons proposé un protocole de classification naïve permettant d'exploiter un classifieur déjà entraîné sur une nouvelle période. Les performances obtenues ont montré que cette approche se révèle insuffisante, requérant ainsi des méthodes plus performantes. L'adaptation de domaine permet d'aborder ce type de problématique. Nous avons considéré deux approches : la projection de données via une analyse canonique des corrélations et le transport optimal. Ces deux approches permettent de projeter les données de l'historique afin de réduire les différences avec l'année à traiter. Néanmoins ces approches offrent des résultats équivalents à la classification naïve pour des coûts de production bien plus significatifs.
Dans de nombreux domaines où il existe des risques pour l'homme, comme la médecine, le nucléaire ou l'avionique, il est nécessaire de passer par une phase de certification visant à garantir le bon fonctionnement d'un système ou d'un produit. La certification se fait en fonction de documents normatifs qui expriment les exigences de justifications auxquelles le produit et le processus de développement doivent se conformer. Un audit de certification consiste alors à produire une documentation attestant la conformité avec ce cadre réglementaire. Pour faire face à ce besoin de justifications visant à assurer la conformité avec les normes en vigueur et la complétude des justifications apportées, il faut dès lors être capable de cibler les exigences de justification à revendiquer pour un projet et produire les justifications durant le développement du projet. Dans ce contexte, éliciter les exigences de justifications à partir des normes et produire les justifications nécessaires et suffisantes sont des enjeux pour assurer le respect des normes et éviter la sur-justification. Dans ces travaux nous cherchons à structurer les exigences de justification pour ensuite aider à la production des justifications associées tout en restant attentif à la confiance que l'on peut placer en elles. Pour relever ces défis, nous avons défini une sémantique formelle pour une modélisation existante des justifications : les Diagrammes de Justification. A partir de cette sémantique, nous avons pu définir un ensemble d'opérations permettant de contrôler le cycle de vie des justifications pour assurer la conformité des justifications au regard des exigences de justification. Par ce formalisme, nous avons également pu guider, voire automatiser dans certains cas, la production des justifications et la vérification de la conformité. Ces contributions ont été appliquées dans le contexte des technologies médicales pour l'entreprise AXONIC, porteuse de ces travaux.
Cette thèse, prenant place en Traitement Automatique des Langues, a pour objectif d'aider les utilisateurs dans de telles situations. Les systèmes traditionnellement proposés (tels les moteurs de recherche) ne donnent pas toujours satisfaction aux utilisateurs pour des tâches répétées, prenant peu en considération leur point de vue et leurs interactions avec le matériau textuel. Nous proposons dans cette thèse que la personnalisation et l'interaction soient au centre de nouveaux outils d'aide pour l'accès au contenu d'ensembles de textes. Ainsi, nous représentons le point de vue de l'utilisateur sur ses domaines d'intérêt par des ensembles de termes décrits et organisés selon un modèle de sémantique lexicale différentielle. Nous exploitons de telles représentations pour construire des supports cartographiques d'interactions entre l'utilisateur et l'ensemble de textes, supports lui permettant de visualiser des regroupements, des liens et des différences entre textes de l'ensemble, et ainsi d'appréhender son contenu. Afin d'opérationnaliser de telles propositions, nous avons mis au point la plate-forme ProxiDocs.
 « Qu'ai-je besoin de connaître minimalement d'une chose pour la connaître ? »  Le fait que cette question aux allures de devinette s'avère cognitivement difficile à appréhender de par son degré de généralité explique sans peine la raison pour laquelle son élucidation demeura plusieurs millénaires durant l'apanage d'une discipline unique : la Philosophie. Dans ce contexte, énoncer des critères à même de distinguer les composants primitifs de la réalité – ou le "mobilier du monde" – ainsi que leurs relations revient à produire une Ontologie. Cet ouvrage s'attelle à la tâche d'élucider le tournant historique curieux, en apparence anodin, que constitue l'émergence de ce type de questionnement dans le champ de deux disciplines connexes que constituent l'Intelligence Artificielle et l'Ingénierie des Connaissances. Nous montrons plus particulièrement ici que leur import d'une forme de méthodologie ontologique appliquée à la cognition ou à la représentation des connaissances ne relève pas de la simple analogie mais soulève un ensemble de questions et d'enjeux pertinents tant sur un plan appliqué que spéculatif. Plus spécifiquement, nous montrons ici que certaines des solutions techniques au problème de la data-masse (Big Data) – i.e. la multiplication et la diversification des données en ligne – constitue un point d'entrée aussi nouveau qu'inattendu dans de nombreuses problématiques traditionnellement philosophiques relatives à la place du langage et des raisonnements de sens commun dans la pensée ou encore l'existence d'une structuration de la réalité indépendante de l'esprit humain.
Le développement d'outils de traitement automatique pour les dialectes de l'arabe se heurte à l'absence de ressources pour ces derniers. Dans ce travail, nous nous intéressons particulièrement au traitement du dialecte tunisien. Nous proposons un système de conversion du tunisien vers une forme approximative de l'arabe standard pour laquelle l'application des outils conçus pour ce dernier permet d'obtenir de bons résultats. Ce dernier permet d'assigner des étiquettes morphosyntaxiques à la sortie de notre système de conversion. Ces étiquettes sont finalement projetées sur le tunisien. Notre système atteint une précision de 89% suite à la conversion qui représente une augmentation absolue de ∼20% par rapport à l'étiquetage d'avant la conversion.
Les enfants souffrant de troubles du langage, comme la dyslexie, rencontrent de grandes difficultés dans l'apprentissage de la lecture et dans toute tâche de lecture, par la suite. Depuis une quinzaine d'années, des outils développés dans le domaine du Traitement Automatique des Langues sont détournés pour être utilisés comme stratégie d'aide et de compensation pour les élèves en difficultés. Parallèlement, l'usage de cartes conceptuelles ou de cartes heuristiques pour aider les enfants dyslexiques à formuler leurs pensées, ou à retenir certaines connaissances, s'est développé. Ce projet a abouti, premièrement, à la réalisation d'une expérimentation exploratoire, sur l'aide à la compréhension de texte grâce aux cartes heuristiques, qui permet de définir de nouveaux axes de recherche ; 
Une collection documentaire est généralement représentée comme un ensemble de documents mais cette modélisation ne permet pas de rendre compte des relations intertextuelles et du contexte d'interprétation d'un document. Le modèle documentaire classique trouve ses limites dans les domaines spécialisés où les besoins d'accès à l'information correspondent à des usages spécifiques et où les documents sont liés par de nombreux types de relations. Le premier modèle est basée sur l'analyse formelle et relationnelle de concepts, le deuxième est basée sur les technologies du web sémantique.
Le travail présenté dans cette thèse fait partie d'un programme de recherche qui vise à comprendre comment le cerveau traite et représente les structures symboliques dans des domaines comme le langage ou les mathématiques. L'existence de structures composées de sous-éléments, tel que les morphèmes, les mots ou les phrases est très fortement suggérée par les analyses linguistiques et les données expérimentales de la psycholinguistique. En revanche, l'implémentation neuronale des opérations et des représentations qui permettent la nature combinatoire du langage reste encore essentiellement inconnue. Certaines opérations de composition élémentaires permettant une représentation interne stable des objets dans le cortex sensoriel, tel que la reconnaissance hiérarchique des formes, sont aujourd'hui mieux comprises [5]. En revanche, les modèles concernant les opérations de liaisons(binding) nécessaires à la construction de structures symboliques complexes et possiblement hiérarchiques, pour lesquelles des manipulations précises des composants doit être possible, sont encore peu testés de façon expérimentale et incapables de prédire les signaux en neuroimagerie. Combler le fossé entre les données de neuroimagerie expérimentale et les modèles proposés pour résoudre le problème de binding est une étape cruciale pour mieux comprendre les processus de traitements et de représentation des structures symboliques. Au regard de ce problème, l'objectif de ce travail était d'identifier et de tester expérimentalement les théories basées sur des réseaux neuronaux, capables de traiter des structures symboliques pour lesquelles nous avons pu établir des prédictions testables, contre des mesures existantes de neuroimagerie fMRI et ECoG dérivées de tâches de traitement du langage. Nous avons identifié deux approches de modélisation pertinentes. La première approche s'inscrit dans le contexte des architectures symboliques vectorielles (VSA), qui propose une modélisation mathématique précise des opérations nécessaires pour représenter les structures dans des réseaux neuronaux artificiels. C'est le formalisme de Paul Smolensky[10], utilisant des produit tensoriel (TPR) qui englobe la plupart des architectures VSA précédemment proposées comme, par exemple, les modèles d'Activation synchrones[9], les représentations réduites holographique[8], et les mémoires auto-associatives récursives[1]. L'architecture du Blackboard repose sur des changements de connectivité transitoires des circuits d'assemblages neuronaux, de sorte que le potentiel de l'activité neurale permise par les mécanismes de mémoire de travail après un processus de liaison, représente implicitement les structures symboliques. Dans la première partie de cette thèse, nous détaillons la théorie derrière chacun de ces modèles et les comparons, du point de vue du problème de binding. Les deux modèles sont capables de répondre à la plupart des défis théoriques posés par la modélisation neuronale des structures symboliques, notamment ceux présentées par Jackendo[3]. La NBA en revanche, considère les dynamiques temporelles de décharge de neurones artificiels, avec des représentations spatialement instables implémentées par des assemblages neuronaux.
L'assistance informatique est devenue une partie indispensable pour la réalisation de procédures chirurgicales modernes. Le désir de créer une nouvelle génération de blocs opératoires intelligents a incité les chercheurs à explorer les problèmes de perception et de compréhension automatique de la situation chirurgicale. De grands progrès ont été réalisés pour la reconnaissance des phases et des gestes chirurgicaux. Pourtant, il existe encore un vide entre ces deux niveaux de granularité dans la hiérarchie du processus chirurgical. Très peu de recherche se concentre sur les activités chirurgicales portant des informations sémantiques vitales pour la compréhension de la situation. Deux facteurs importants entravent la progression. Tout d'abord, la reconnaissance et la prédiction automatique des activités chirurgicales sont des tâches très difficiles en raison de la courte durée d'une activité, de leur grand nombre et d'un flux de travail très complexe et une large variabilité. Deuxièmement, une quantité très limitée de données cliniques ne fournit pas suffisamment d'informations pour un apprentissage réussi et une reconnaissance précise. À notre avis, avant de reconnaître les activités chirurgicales, une analyse soigneuse des éléments qui composent l'activité est nécessaire pour choisir les bons signaux et les capteurs qui faciliteront la reconnaissance. Nous avons utilisé une approche d'apprentissage profond pour évaluer l'impact de différents éléments sémantiques de l'activité sur sa reconnaissance. Grâce à une étude approfondie, nous avons déterminé un ensemble minimum d'éléments suffisants pour une reconnaissance précise. Les informations sur la structure anatomique et l'instrument chirurgical sont de première importance. Nous avons également abordé le problème de la carence en matière de données en proposant des méthodes de transfert de connaissances à partir d'autres domaines ou chirurgies. Les méthodes de ''word embedding''et d'apprentissage par transfert ont été proposées. Ils ont démontré leur efficacité sur la tâche de prédiction d'activité suivante offrant une augmentation de précision de 22%. De plus, des observations pertinentes
Malgré l'importance d'une nomenclature internationale, le domaine de la chimie souffre encore de quelques problèmes linguistiques, liés notamment à ses unités terminologiques simples et complexes, pouvant gêner la communication scientifique. A cela s'ajoute l'emploi récurrent d'emprunts. La question est de savoir comment représenter les unités terminologiques simples et complexes de cette langue spécialisée. En d'autres termes, formaliser les caractéristiques terminologiques en étudiant les mécanismes de la construction morphosyntaxique des termes de la chimie en arabe. Cette étude devrait aboutir à la mise en place d'un outil de désambigüisation sémantique qui vise à constituer un outil d'extraction des termes de la chimie en arabe et de leurs relations. La construction de cette grammaire d'identification nécessite la modélisation des patrons morphosyntaxiques à partir de leur observation en corpus etdébouche sur la définition de règles de grammaire et de contraintes.
Le développement technologique a ses avantages et ses inconvénients. Nous pouvons facilement partager et télécharger du contenu numérique en utilisant l'Internet. En outre, les utilisateurs malveillants peuvent aussi modifier, dupliquer et diffuser illégalement tout type d'informations, comme des images et des documents. Par conséquent, nous devons protéger ces contenus et arrêter les pirates. Le but de cette thèse est de protéger les documents PDF et les images en utilisant la technique de tatouage numérique Spread Transform Dither Modulation (STDM), tout en tenant compte des exigences principales de transparence, de robustesse et de sécurité. La méthode de tatouage STDM a un bon niveau de transparence et de robustesse contre les attaques de bruit. La clé principale dans cette méthode de tatouage est le vecteur de projection qui vise à diffuser le message sur un ensemble d'éléments. Cependant, un tel vecteur clé peut être estimée par des utilisateurs non autorisés en utilisant les techniques de séparation BSS (Blind Source Separation). Dans notre première contribution, nous présentons notre méthode de tatouage proposé CAR-STDM (Component Analysis Resistant-STDM), qui garantit la sécurité tout en préservant la transparence et la robustesse contre les attaques de bruit. STDM est également affecté par l'attaque FGA (Fixed Gain Attack). Dans la deuxième contribution, nous présentons notre méthode de tatouage proposé N-STDM qui résiste l'attaque FGA et améliore la robustesse contre l'attaque Additive White Gaussian Noise (AWGN), l'attaque de compression JPEG, et diversité d'attaques de filtrage et géométriques. Les expérimentations ont été menées sur des documents PDF et des images dans le domaine spatial et le domaine fréquentiel. Récemment, l'Apprentissage Profond et les Réseaux de Neurones atteints du développement et d'amélioration notable, en particulier dans le traitement d'image, la segmentation et la classification. Des modèles tels que CNN (Convolutional Neural Network) sont utilisés pour la dé-bruitage des images. CNN a une performance adéquate de dé-bruitage, et il pourrait être nocif pour les images tatouées. Dans la troisième contribution, nous présentons l'effet du FCNN (Fully Convolutional Neural Network), comme une attaque de dé-bruitage, sur les images tatouées. Les méthodes de tatouage STDM et SS (Spread Spectrum) sont utilisés durant les expérimentations pour intégrer les messages dans les images en appliquant plusieurs scénarios. Cette évaluation montre qu'un tel type d'attaque de dé-bruitage préserve la qualité de l'image tout en brisant la robustesse des méthodes de tatouages évalués.
La microscopie confocale de réflectance in-vivo (RCM) est un outil puissant pour visualiser les couches cutanées à une résolution cellulaire. Des descripteurs du vieillissement cutané ont été mis en évidence à partir d'images confocales. Cependant, leur évaluation nécessite une analyse visuelle des images par des dermatologues expérimentés. L'objectif de cette thèse est le développement d'une technologie innovante pour quantifier automatiquement le phénomène du vieillissement cutané en utilisant la microscopie confocale de réflectance in vivo. Premièrement, la quantification de l'état de l'épiderme est abordée. Les mesures proposées mettent en évidence une différence significative entre les groupes d'âge et l'exposition au soleil. Enfin, les méthodes proposées sont validées par des études cliniques et d'efficacité de produits cosmétiques
De nos jours, l'utilisation des techniques de gestion de processus au sein des entreprises permet un gain significatif d'efficacité du système opérationnel. Ces techniques assistent les experts métier lors de la modélisation des processus de l'entreprise, de leur mise en oeuvre, de leurs analyses et de leurs améliorations. Le contexte d'exécution d'un processus métier contient des informations permettant d'identifier et de comprendre les interactions entre lui-même et d'autres processus. Le premier processus est déclenché suite à un besoin client (processus opérationnel) et les autres, suite à un besoin du processus opérationnel (processus supports). La satisfaction d'un besoin client dépend d'une interaction efficace entre le processus opérationnel et les processus supports qui lui sont liés. Ces interactions se définissent au travers des données opérationnelles, manipulées par le processus opérationnel, et des données supports, manipulées par les processus supports. Nos travaux se fondent sur l'Ingénierie dirigée par les Modèles. L'approche proposée dans cette thèse est fondée sur l'annotation des modèles de processus opérationnels ou supports. Cette annotation est réalisée avec l'aide d'une ontologie définissant le domaine métier décrit par ces processus. Ces annotations sont ensuite exploitées afin de constituer un ensemble de données, dites données contextuelles. L'analyse des traces de l'exécution du processus et de ces données contextuelles permet de sélectionner les meilleurs d'entre elles, au sens métier. Ainsi, un processus opérationnel pourra être associé à un ensemble de processus supports via les données contextuelles.
À l'échelle mondiale, les chiffres relatifs à la migration des personnes ont évolué juste après la période de la Guerre froide. Les populations de réfugiés ont augmenté, passant de 2,4 millions en 1975 à 12,1 millions en 2000 (UNHCR 1995 ; UNHCR 2000). Aujourd'hui, les taux de migration en Europe font apparaître un total de plus de 4 millions d'immigrants dans différents pays européens. En outre, la France et le Royaume-Uni sont considérés comme deux des principales destinations de migration en Europe, attirant à la fois les migrants et les demandeurs de l'asile. Selon l'Office statistique de l'Union européenne situé à Luxembourg, « l'Allemagne a enregistré le plus grand nombre d'immigrants (917,1 milliers) en 2017, suivie du Royaume-Uni (644,2 milliers), de l'Espagne (532,1 milliers), de la France (370 milliers et plus), de l'Italie (343,4 milliers) » . En général, la majorité des migrants et des réfugiés vivent dans des conditions de vie difficiles. Cependant, la situation des femmes s'avère encore plus précaire en raison des inégalités socio-économiques, les femmes migrantes représentant environ 52% de l'ensemble des migrants en Europe. D'autre part, certaines femmes immigrées et réfugiées se sont chargées d'exprimer leur trouble et d'attirer l'attention du public sur leur situation difficile. L'objectif de cette recherche est, d'une part, d'approfondir les mécanismes et appareils internes qui façonnent la culture et le patrimoine européen grâce aux contributions culturelles des migrants, et d'autre part, d'en étudier et d'analyser les moyens utilisés pour canaliser leurs pensées et partager leurs histoires avec le reste du monde entre le 20ème et 21ème siècle. Cette recherche couvre plusieurs disciplines artistiques et littéraires, allant des arts visuels aux performances théâtrales, en passant par la production cinématographique, la filmographie et la photographie ; elle s'intéressera aussi aux œuvres littéraires, telles que les romans, ateliers d'écritures et bandes dessinées, qui pourraient être utilisées à des fins diverses pour restaurer l'identité et l'appartenance, ou comme un moyen de thérapie. Sur le théâtre, l'étude couvre les textes des pionniers et les représentations qui ont façonné l'histoire de la migration dans les deux régions européennes au cours des XXème et XXIème siècles, ainsi que les troupes et les groupes qui ciblent et tendent plus particulièrement à inclure les réfugiés. Nous pourrons nous référer à la dramaturge franco-algérienne Salika Amara qui a travaillé au sein de la troupe de théâtre française nommée Kahina, fondée en 1976 par des femmes d'Aubervilliers et principalement composée de femmes-actrices, et leurs homologues au Royaume-Uni, Winsomme Pinnock et Bola Agbaje, une dramaturge contemporaine anglo-nigérienne. Il va sans dire que le travail théâtral ne se limite pas aux scènes des salles de théâtre, mais couvre tous les types de représentations, formels et informels. On pense par exemple à l'association Ornina qui a été fondée par une franco-syrienne, cette compagnie travaillant avec des récits de réfugiés venant de l'Est de la France ; nous verrons aussi le travail du Bureau d'Accueil et d'Accompagnement des Migrants (BAAM), de la compagnie de théâtre CK. Point, The Good Chance Theatre au Royaume-Uni et même les pièces représentées dans des festivals internationaux (migrant'scène, Avignon, Edinburgh). En ce qui concerne le cinéma et la filmographie, nous essayerons de couvrir le cinéma minoritaire qui reflète les conditions d'exil et d'aliénation et qui est principalement produit dans les marges. Ceci est bien décrit par Naficy comme des films comprenant notamment un « Forme ouverte et style visuel de forme fermée ; structure narrative fragmentée, multilingue, épistolaire, auto-réflexive et juxtaposée de manière critique ; caractères amphiboliques, doublés, croisés et perdus ; sujets et thèmes qui impliquent le voyage, l'historicité, l'identité et le déplacement ; structures dysphoriques, euphoriques, nostalgiques, synesthésiques, liminales et politisées des modes de production du sentiment, des modes de production interstitiel et collectif ; et inscription de la (dé) localisation biographique, sociale et cinématographique des cinéastes. » (Naficy 2001 : 4) Toutes ces caractéristiques sont généralement présentes dans les films de migration ; de plus, ces productions représentent l'état d'esprit psychologique de l'immigrant ou du réfugié, en particulier si les producteurs ou les acteurs principaux sont eux-mêmes migrants ou réfugiés, comme dans le court métrage Unbroken Paradise interprété par un véritable réfugié syrien. D'autres films sur les migrations, par contre, tendent à sensibiliser l'opinion publique à certains problèmes et à défendre les droits de la femme ou les droits civils, une sorte déconstruction du 'regard masculin' et l'adoption d'une approche autoréflexive défensive. Toujours dans le domaine des arts visuels, le rôle de la photographie est indéniable. Pendant des siècles, les photos ont été un moyen d'expression à la fois fort et silencieux. Dans le cas des migrations, la tâche et le fardeau apparaissent plus lourds que d'habitude. Il suffit d'un clic pour rediriger une vision, une idée et changer la manière de voir d'un pays, qu'elle soit positive ou négative. Dans cette étude, nous pensons que le rôle des images/photos est très important et pertinent pour le parcours des migrants. Tout comme les œuvres d'art susmentionnées, la photographie sera un moyen de ré-identifier et de raconter à nouveau des histoires de migrants. Omar Imam par exemple, un photographe syrien travaillant sur la crise de la migration syrienne, avec son projet Live, Love, Refugee dans les camps des réfugiés, raconte des histoires de réfugiés avec des clichés créatifs et surréalistes et des images fixes de différentes actions visant à refléter certaines émotions et pensées. De nombreux autres projets photographiques seront abordés dans le cadre de la recherche dans le but de réécrire les identités des femmes migrantes, comme le projet Refugees Learning and Storytelling through Participatory Photography lancé par trois femmes leaders dans le domaine de la migration et de la photographie et visant à enseigner aux femmes migrantes les bases de la photographie pour les responsabiliser et les aider à raconter leurs propres histoires (Brigham, 2018). L'analyse de la littérature sur la migration comprendra l'analyse de la présence des femmes dans les textes littéraires ; cela signifiera de travailler sur des romans, des ateliers d'écriture et des oeuvres d'écrits créatifs tels que des poèmes, des journaux, des récits de voyage, etc. Selon Franz Fanon, 'l'Europe est littéralement la création du tiers monde' (Fanon, 1961, p. 99), qui fait référence à l'écriture continue de la périphérie en tant que moyen de réécrire et de réinventer l'identité "l'expérience de la migration agit comme un catalyseur et un canal pour les sentiments naissants, une re-conception de notre sens de soi et de nos relations avec les autres" (Jacobs, 2011 : 142). Cependant, l'écriture créative a une part importante dans cette recherche, principalement à cause de l'évolution de son utilisation dans les camps de réfugiés et par différents organismes afin de combler le fossé entre locaux et migrants en tant que thérapie sociologique et psychologique. Les bandes dessinées sont également une nouvelle tendance à la croissance rapide utilisée par les journalistes et les artistes afin de décrire les expériences réelles de migrants de différents endroits du monde. Grâce à un texte minimisé mais à des dessins expressifs, elles attirent l'attention des lecteurs appartenant à des origines et de statuts sociétaux différents aux conditions, épreuves et les horreurs rencontrées par les individus, les familles et les enfants à la recherche d'un abri et d'un refuge sur des sols étrangers, tels que l'excellente bande dessinée numérique Over Under Sideways Down par Karrie Fransman, et Fuyant vers l'inconnu créé par l'organisation PositiveNegative. L'analyse couvre toutes les œuvres créées par des migrants hommes et femmes, ainsi que d'autres concernant des artistes / écrivains locaux ou internationaux, avec une référence aux femmes migrantes et à la migration. Suivant une approche historique et pluridisciplinaire, l'analyse comparative vise à identifier les nouvelles tendances et méthodes utilisées pour représenter les femmes migrantes et à répondre aux questions suivantes : existe-t-il une différence entre les discours utilisés par les migrants dans les deux pays (France et Royaume-Uni) ? Quelle est la différence entre le discours habituel employé lors de la représentation artistique féminine et celui de la représentation des migrantes ? Comment est-il utilisé et à quelles fins ? Existe-t-il une différence entre le discours masculin en référence aux femmes et le discours féminin en référence aux femmes (autoréférence) ? Toutes ces disciplines ont-elles le même objectif vis-à-vis de la situation et de l'image des migrants ?
Au cours des dernières années, le Web a connu une énorme croissance en matière de contenus et d'utilisateurs. Ce phénomène a entraîné des problèmes liés à la surcharge d'information face à laquelle les utilisateurs ont des difficultés à trouver les bonnes informations. Des systèmes de recommandation ont été développés pour résoudre ce problème afin de guider les utilisateurs dans ce flux d'informations. Les approches de recommandation se sont multipliées et ont été mises en œuvre avec succès, notamment au travers d'approches telles que le filtrage collaboratif. Cependant, il existe encore des défis et des limites qui offrent des opportunités pour de nouvelles recherches. Parmi ces défis, la conception de systèmes de recommandation de lectures est devenue un axe de recherche en pleine expansion suite à l'apparition des bibliothèques numériques. Traditionnellement, les bibliothèques jouent un rôle passif dans l'interaction avec les lecteurs et ce, faute d'outils efficaces de recherche et de recommandation. Dans ce manuscrit, nous nous sommes penchée sur la création d'un système de recommandation de lectures.
Cette thèse traite de l'apprentissage en profondeur appliqu'e aux tâches de classification des images. La principale motivation du travail est de rendre les techniques d'apprentissage en profondeur actuelles plus efficaces et de faire face aux changements dans la distribution des données. Nous travaillons dans le cadre élargi de l'apprentissage continu, dans le but d'avoir à l'avenir des modèles d'apprentissage automatique pouvant être améliorés en permanence. Nous examinons d'abord la modification de l'espace étiquette d'un ensemble de données, les échantillons de données restant les mêmes. Nous considérons une hiérarchie d'étiquettes sémantiques à laquelle appartiennent les étiquettes. Nous étudions comment nous pouvons utiliser cette hiérarchie pour obtenir des améliorations dans les modèles formés à différents niveaux de cette hiérarchie. Les deuxième et troisième contributions impliquent un apprentissage continu utilisant un modèle génératif. Nous analysons la facilité d'utilisation des échantillons d'un modèle génératif dans le cas de la formation de bons classificateurs discriminants. Nous proposons des techniques pour améliorer la sélection et la génération d'échantillons à partir d'un modèle génératif. Ensuite, nous observons que les algorithmes d'apprentissage continu subissent certaines pertes de performances lorsqu'ils sont entraînés séquentiellement à plusieurs tâches. Nous analysons la dynamique de la formation dans ce scénario et comparons avec la formation sur plusieurs tâches simultanément. Nous faisons des observations qui indiquent des difficultés potentielles dans l'apprentissage de modèles dans un scénario d'apprentissage continu. Enfin, nous proposons un nouveau modèle de conception pour les réseaux de convolution. Cette architecture permet de former des modèles plus petits sans compromettre les performances. De plus, la conception se prête facilement à la parallélisation, ce qui permet une formation distribuée efficace. En conclusion, nous examinons deux types de scénarios d'apprentissage continu. Nous proposons des méthodes qui conduisent à des améliorations. Notre analyse met également en évidence des problèmes plus importants, dont nous aurions peut-être besoin de changements dans notre procédure actuelle de formation de réseau neuronal.
L'ère des grandes masses de données (big data) nous a mis face à de nouvelles problématiques de gestion et de traitement des données. La théorie des AMAS (Adaptive Multi-Agent Systems) propose de résoudre par autoorganisation des problèmes complexes pour lesquels aucune solution algorithmique n'est connue. Le comportement coopératif des agents permet au système de s'adapter à un environnement dynamique pour maintenir le système dans un état de fonctionnement adéquat. Les systèmes ambiants présentent un exemple typique de système complexe nécessitant ce genre d'approche, et ont donc été choisis comme domaine d'application pour notre travail. Cette thèse vise à explorer et décrire comment la théorie des Systèmes Multi-Agents Adaptatifs peut être appliquée aux grandes masses de données en fournissant des capacités d'analyse dynamique, en utilisant un nouvel outil analytique qui mesure en temps réel la similarité des évolutions des données. Cette recherche présente des résultats prometteurs et est actuellement appliquée dans l'opération neOCampus, le campus ambiant de l'Université Toulouse III.
Le contenu visuel se concentre souvent sur les humains. L'analyse automatique des humains à partir de données visuelles revêt donc une grande importance pour de nombreuses applications. Le but de cette thèse est d'apprendre des représentations visuelles pour l'analyse des humains. Un accent particulier est mis sur deux domaines étroitement liés de la vision artificielle : l'analyse du corps humain et la reconnaissance des actions.
L'augmentation considérable de la quantité des données textuelles rend aujourd'hui difficile leur analyse sans l'assistance d'outils. Or, un texte rédigé en langue naturelle est une donnée non-structurée, c'est-à-dire qu'elle n'est pas interprétable par un programme informatique spécialisé, sans lequel les informations des textes restent largement sous-exploitées. Pour réaliser cette tâche, nous proposons une nouvelle approche par alignement de deux types de représentations vectorielles d'entités capturant une partie de leur sens : les plongements lexicaux pour les mentions textuelles et des “plongements ontologiques” pour les concepts, conçus spécifiquement pour ce travail. L'alignement entre les deux se fait par apprentissage supervisé. Les méthodes développées ont été évaluées avec un jeu de données de référence du domaine biologique et elles représentent aujourd'hui l'état de l'art pour ce jeu de données. Ces méthodes sont intégrées dans une suite logicielle de traitement automatique des langues et les codes sont partagés librement.
Le processus de découverte de médicaments a un succès limité malgré tous les progrès réalisés. En effet, on estime actuellement que le développement d'un médicament nécessite environ 1,8 milliard de dollars américains sur environ 13 ans. Leurs applications sont polyvalentes : elles permettent d'identifier des candidats médicaments pour des cibles thérapeutiques connues, d'anticiper des effets secondaires potentiels, ou de proposer de nouvelles indications thérapeutiques pour des médicaments connus. Cette thèse est conçue selon deux cadres d'approches de criblage virtuel : les approches dans lesquelles les données sont décrites numériquement sur la base des connaissances des experts, et les approches basées sur l'apprentissage automatique de la représentation numérique à partir du graphe moléculaire et de la séquence protéique. Nous discutons ces approches et les appliquons pour guider la découverte de médicaments.
Dans chaque organisation, les processus métier sont aujourd'hui incontournables. Cette thèse vise à développer une méthode pour les améliorer. Dans le domaine de la santé, les organisations hospitalières déploient beaucoup d'efforts pour mettre leurs processus sous contrôle, notamment à cause de la très faible marge d'erreur admise. Les parcours des patients au sein des structures de santé constituent l'application qui a été choisie pour démontrer les apports de cette méthode. Cette thèse propose donc les contributions suivantes : la méthode DIAG elle-même qui, grâce à quatre différents états, extrait les informations des données de géolocalisation ; le méta-modèle DIAG qui a deux utilités : d'une part, interpréter les données de géolocalisation et donc passer des données brutes aux informations utilisables, et, d'autre part contribuer à vérifier l'alignement des données avec le domaine grâce à deux méthodes de diagnostic décrites plus bas ; deux algorithmes de découverte de processus qui utilisent la stabilité statistique des logs d'évènements ; une nouvelle approche de process mining utilisant SPC (Statistical Process Control) pour l'amélioration ; l'algorithme proDIST qui mesure les distances entre les modèles de processus ; deux méthodes de diagnostic automatique de processus pour détecter les causes des déviations structurelles dans des cas individuels et pour des processus communs. Le contexte de cette thèse confirme la nécessité de proposer de telles solutions. Une étude de cas dans le cadre de ce travail de recherche illustre l'applicabilité de la méthodologie DIAG et des fonctions et méthodes mentionnées.
Cette thèse pour but de développer des méthodes de segmentation pour des scènes fortement structurées (ex. bâtiments et environnements urbains) ou faiblement structurées (ex. paysages ou objets naturels). En particulier, les images de bâtiments peuvent être décrites en termes d'une grammaire de formes, et une dérivation de cette grammaire peut être inférée pour obtenir une segmentation d'une image. Cependant, il est difficile et long d'écrire de telles grammaires. Pour répondre à ce problème, nous avons développé une nouvelle méthode qui permet d'apprendre automatiquement une grammaire à partir d'un ensemble d'images et de leur segmentation associée. Des expériences montrent que des grammaires ainsi apprises permettent une inférence plus rapide et produisent de meilleures segmentations. De manière surprenante, même sans connaissance spécifique sur le type de scène particulier observé, nous obtenons des gains significatifs en qualité de segmentation sur plusieurs jeux de données. Enfin, nous avons développé une technique basée sur les réseaux de neurones convolutifs (CNN) pour segmenter des images de scènes faiblement structurées. Un filtrage adaptatif est effectué à l'intérieur même du réseau pour permettre des dépendances entre zones d'images distantes. Des expériences sur plusieurs jeux de données à grande échelle montrent là aussi un gain important sur la qualité de segmentation
Le critère de l'information k-means étend le critère des k-means en utilisant la divergence de Kullback comme fonction de perte. La fragmentation est une généralisation supplémentaire permettant l'approximation de chaque signal par une combinaison de fragments. Nous proposons un nouvel algorithme de fragmentation pour les signaux numériques se présentant comme un algorithme de compression avec perte. Nous avons testé la méthode sur des images en niveaux de gris sur lesquelles il a été possible de détecter des configurations translatées ou transformées par une rotation. Ceci donne l'espoir d'apporter une réponse à la reconnaissance invariante par transformations fondée sur un critère de compression très général. D'un point de vue mathématique, nous avons prouvé deux types de bornes. Ce résultat contribue à expliquer la pertinence de nos arbres syntaxiques. Ensuite, nous établissons des bornes de généralisation non asymptotiques et indépendantes de la dimension pour les différents critères des k-means et critères de fragmentation que nous avons introduits. Grâce à une nouvelle méthode de chaînage PAC-Bayésien, nous prouvons aussi une borne en O(log(n/k) sqrt{k log(k)/n}).
Ainsi, les lacs de données sont devenus une solution attractive par rapport aux entrepôts de données classiques coûteux et fastidieux (nécessitant une démarche ETL), pour les entreprises qui souhaitent stocker leurs données. Malgré leurs volumes, les données stockées dans les lacs de données des entreprises sont souvent incomplètes voire non mises à jour vis-à-vis des besoins (requêtes) des utilisateurs. Les sources de données locales ont donc besoin d'être enrichies. Ainsi, afin de permettre d'accéder et de récupérer l'information de manière simple et interopérable, les sources de données sont de plus en plus intégrées dans les services Web. Il s'agit plus précisément des services de données, y compris les services DaaS du Cloud Computing. L'enrichissement manuel des sources locales implique plusieurs tâches fastidieuses telles que l'identification des services pertinents, l'extraction et l'intégration de données hétérogènes, la définition des mappings service Cela permettrait de satisfaire les requêtes des utilisateurs tout en respectant leurs préférences en terme de coût d'exécution et de temps de réponse et en garantissant la qualité des résultats obtenus.
Les travaux de la thèse portent sur l'estimation de la saillance du mouvement dans des séquences d'images. Dans une première partie, nous avons traité un sujet très peu abordé : la détection des images présentant un mouvement saillant. Pour cela, nous nous appuyons sur un réseau de neurones convolutif et sur la compensation du mouvement de la caméra. Dans une seconde partie, nous avons conçu une méthode originale d'estimation de cartes de saillance du mouvement. Cette méthode ne requiert pas d'apprentissage. L'indice de saillance est obtenu par une étape d'inpainting du flot optique, suivie d'une comparaison avec le flot initial. Dans un troisième temps, nous nous sommes intéressés à l'estimation de la saillance de trajectoires pour appréhender une saillance progressive. Nous construisons une méthode faiblement supervisée s'appuyant sur un réseau auto-encodeur récurrent, qui représente chaque trajectoire avec un code latent. Toutes ces méthodes ont été validées sur des données de vidéo réelles.
La gestion efficace de grandes quantités d'informations est devenue un défi de plus en plus important pour les systèmes d'information. Tous les jours, de nouvelles sources d'informations émergent sur le web. Un humain peut assez facilement retrouver ce qu'il cherche, lorsqu'il s'agit d'un article,d'une vidéo, d'un artiste précis. En revanche, il devient assez difficile, voire impossible, d'avoir une démarche exploratoire pour découvrir de nouveaux contenus. Les systèmes de recommandation sont des outils logiciels ayant pour objectif d'assister l'humain afin de répondre au problème de surcharge d'informations. Les travaux présentés dans ce document proposent une architecture pour la recommandation efficace d'articles d'actualité. Contenu dans une ontologie, ce vocabulaire constitue une modélisation formelle de la vue métier sur le domaine traité. Réalisés en collaboration avec la société Actualis SARL, ces travaux ont permis la commercialisation d'un nouveau produit hautement compétitif, FristECO Pro'fil.
Face à la complexité significative du domaine mammographique ainsi que l'évolution massive de ses données, le besoin de contextualiser les connaissances au sein d'une modélisation formelle et exhaustive devient de plus en plus impératif pour les experts. C'est dans ce cadre que s'inscrivent nos travaux de recherche qui s'intéressent à unifier différentes sources de connaissances liées au domaine au sein d'une modélisation ontologique cible. D'une part, plusieurs modélisations ontologiques mammographiques ont été proposées dans la littérature, où chaque ressource présente une perspective distincte du domaine d'intérêt. D'autre part, l'implémentation des systèmes d'acquisition des mammographies rend disponible un grand volume d'informations issues des faits passés, dont la réutilisation devient un enjeu majeur. Toutefois, ces fragments de connaissances, présentant de différentes évidences utiles à la compréhension de domaine, ne sont pas interopérables et nécessitent des méthodologies de gestion de connaissances afin de les unifier. C'est dans ce cadre que se situe notre travail de thèse qui s'intéresse à l'enrichissement d'une ontologie de domaine existante à travers l'extraction et la gestion de nouvelles connaissances (concepts et relations) provenant de deux courants scientifiques à savoir : des ressources ontologiques et des bases de données comportant des expériences passées. Notre approche présente un processus de couplage entre l'enrichissement conceptuel et l'enrichissement relationnel d'une ontologie mammographique existante. Le but étant de réduire l'étape d'alignement de deux ontologies entières en un alignement de deux groupements de concepts de tailles réduits. La deuxième étape consiste à aligner les deux structures des clusters relatives aux ontologies cible et source. Les alignements validés permettent d'enrichir l'ontologie de référence par de nouveaux concepts permettant d'augmenter le niveau de granularité de la base de connaissances. Le deuxième processus s'intéresse à l'enrichissement relationnel de l'ontologie mammographique cible par des relations déduites de la base de données de domaine. Cette dernière comporte des données textuelles des mammographies recueillies dans les services de radiologies. Cette dernière consiste à filtrer et classer les règles afin de faciliter leur interprétation et validation par l'expert vi) L'enrichissement de l'ontologie par de nouvelles associations entre les concepts. Cette approche a été mise en 'uvre et validée sur des ontologies mammographiques réelles et des données des patients fournies par les hôpitaux Taher Sfar et Ben Arous.
Dans cette thèse nous avons exploré la capacité des modèles magnétiques de la physique statistique à extraire l'information essentielle contenue dans les textes. Les documents ont été représentés comme des ensembles d'unités en interaction magnétique, l'intensité de telles interactions a été mesurée et utilisée pour calculer de quantités qui sont des indices de l'importance de l'information portée. Nous proposons deux nouvelles méthodes. Cette quantité a été utilisée comme indicatrice de pertinence et appliquée à une vaste palette de tâches telles que le résumé automatique, la recherche d'information, la classification de documents et la segmentation thématique. Par ailleurs, et de façon encore exploratoire, nous proposons un deuxième algorithme qui définie un couplage grammatical pour conserver les termes importants et produire des contractions. De cette façon, la compression d'une phrase est l'état fondamental de la chaîne de termes. Comme cette compression n'est pas forcement bonne, il a été intéressant de produire des variantes en permettant des fluctuations thermiques. Nous avons fait des simulations Métropolis Monte-Carlo avec le but de trouver l'état fondamental de ce système qui est analogue au verre de spin.
Les mécanismes sous-jacents en compétition déterminent l'évolution du gaz dans le milieu interstellaire. La relaxation électronique de l'état excité le plus brillant a été simulée pour des polyacènes neutres constitués de 2 à 7 cycles aromatiques. Les résultats montrent une alternance marquée dans les temps de dépopulation de l'état initial pour les polyacènes contenant jusqu'à 6 cycles aromatiques, ce qui est corrélé avec une alternance des écarts d'énergie entre l'état initial et l'état situé juste dessous. Les résultats montrent que la population électronique excitée du chrysène décroît un ordre de grandeur plus rapidement que celle du tétracène. Ceci est aussi corrélé à une différence significative des écarts d'énergie entre l'état initial et l'état situé juste dessous. Un dernier développement majeur concerne l'utilisation d'algorithmes "Machine Learning" (ML) proposés comme un moyen d'éviter la plupart des calculs de structure électronique, très coûteux en temps calcul. Les performances d'algorithmes de réseaux de neurones appliqués à la dynamique des états excités ont été évaluées. Le cas de la relaxation électronique dans le phénanthrène neutre a été choisi comme test en raison de divers résultats expérimentaux disponibles. L'apprentissage de plusieurs réseaux de neurones a été effectué et leurs précision et efficacité analysés. De plus, des approximations de trajectoires à sauts de surface ont été interfacées à l'approche ML, résultant en un coût négligeable des simulations de dynamique non-adiabatique. L'efficacité des diverses approches simplifiées a été comparée à FSSH. Dans l'ensemble, ML se révèle un outil très prometteur pour la dynamique dans les états excités à l'échelle de la nanoseconde. Ce travail de thèse ouvre de nouvelles voies pour étudier la photophysique théorique de complexes moléculaires de grande taille. Enfin, les outils développés et implémentés dans deMon-Nano, de manière modulaire, peuvent être combinés avec d'autres approches DFTB sophistiquées (tel que "Configuration Interaction") plus adaptées aux états à transfert de charge.
En effet, pour être performants, ces modèles ont besoin de corpus annotés de taille importante. Par conséquent, uniquement les langues bien dotées peuvent bénéficier directement de l'avancée apportée par les RNs, comme par exemple les formes formelles des langues. Dans le cadre de cette thèse, nous proposons des méthodes d'apprentissage par transfert neuronal pour la construction d'outils de TAL pour les langues et domaines peu dotés en exploitant leurs similarités avec des langues et des domaines bien dotés. Précisément, nous expérimentons nos approches pour le transfert à partir du domaine source des textes formels vers le domaine cible des textes informels (langue utilisée dans les réseaux sociaux). Tout au long de cette thèse nous présentons différentes contributions. Tout d'abord, nous proposons deux approches pour le transfert des connaissances encodées dans les représentations neuronales d'un modèle source, pré-entraîné sur les données annotées du domaine source, vers un modèle cible, adapté par la suite sur quelques exemples annotés du domaine cible. Ensuite, nous effectuons une série d'analyses pour repérer les limites des méthodes proposées. Nous constatons que, même si l'approche d'apprentissage par transfert proposée améliore les résultats du domaine cible, un transfert négatif « dissimulé » peut atténuer le gain final apporté par l'apprentissage par transfert. De plus, une analyse interprétative du modèle pré-entraîné montre que les neurones pré-entraînés peuvent être biaisés par ce qu'ils ont appris du domaine source, et donc peuvent avoir des difficultés à apprendre des « ~patterns~ » spécifiques au domaine cible. Suite à cette analyse, nous proposons un nouveau schéma d'adaptation qui augmente le modèle cible avec des neurones normalisés, pondérés et initialisés aléatoirement permettant une meilleure adaptation au domaine cible tout en conservant les connaissances apprises du domaine source. Enfin, nous proposons une approche d'apprentissage par transfert qui permet de tirer profit des similarités entre différentes tâches, en plus des connaissances pré-apprises du domaine source.
On s'attend à ce qu'elle soit diagnostiquée à son stade précoce, Mild Cognitive Impairment (MCI), pour pouvoir intervenir et retarder son apparition. La tomographie par émission de positons au fluorodésoxyglucose (TEP-FDG) est considérée comme une modalité efficace pour diagnostiquer la MA et la phase précoce correspondante, car elle peut capturer les changements métaboliques dans le cerveau, indiquant ainsi des régions anormales. A cette fin, trois nouvelles méthodes indépendantes sont proposées. Ces connectivités sont représentées par des similarités ou des mesures graphiques entre régions. Combinées ensuite aux propriétés de chaque région, ces caractéristiques sont intégrées dans un cadre de classification d' Le gradient spatial est quantifié par un histogramme 2D d'orientation et exprimé sous forme multi-échelle. Les résultats sont obtenus en intégrant différentes échelles de gradients spatiaux dans différentes régions. Une telle architecture peut faciliter les opérations de convolution, de la 3D à la 2D, tout en tenant compte des relations spatiales, qui bénéficient d'une nouvelle couche de cartographie. Les expériences menées sur des ensembles de données publics montrent que les trois méthodes proposées peuvent atteindre des performances significatives et, de surcroît, dépasser les approches les plus avancées.
Cette thèse aborde les problèmes de variabilité et confusabilité phonémique du point de vue des modèles de prononciation pour un système de reconnaissance automatique de la parole. En particulier, plusieurs directions de recherche sont étudiées. Cependant, ajouter plusieurs prononciations par mot au vocabulaire peut introduire des homophones (ou quasi-homophones) et provoquer une augmentation de la confusabilité du système. Une nouvelle mesure de cette confusabilité est proposée pour analyser et étudier sa relation avec la performance d'un système de reconnaissance de la parole. Cette “confusabilité de prononciation” est plus élevée si des probabilités pour les prononciations ne sont pas fournies et elle peut potentiellement dégrader sérieusement la performance d'un système de reconnaissance de la parole. Il convient, par conséquent, qu'elle soit prise en compte lors de la génération de prononciations. On étudie donc des approches d'entraînement discriminant pour entraîner les poids d'un modèle de confusion phonémique qui autorise différentes facons de prononcer un mot tout en contrôlant le problème de confusabilité phonémique. La fonction objectif à optimiser est choisie afin de correspondre à la mesure de performance de chaque tâche particulière. Dans cette thèse, deux tâches sont étudiées : la tâche de reconnaissance automatique de la parole et la tâche de détection de mots-clés. Pour les expériences menées sur la détection de mots-clés, le “Figure of Merit” (FOM), une mesure de performance de la détection de mots-clés, est directement optimisée.
Le present travail est une etude terminologique sur le domaine de la finance, en vue d'un traitement automatique. On y analyse les formes derivees et composees d'un corpus, mettant evidence sur la relation syntactico-semantique qu'entretiennent leurs constituants, dans le but de determiner quels formants sont productifs dans la langue economique. Les expressions idiomatiques ou recurrentes du corpus sont egalement traitees. L'etude met egalement en contraste, dans le dernier temps, les termes economiques du coreen et ceux du francais en vue de proposer une aide a la redaction et a la traduction.
La spectroscopie vibrationnelle englobe les techniques optiques spécifiques de la spectroscopie infrarouge à transformée de Fourier (IRTF) et Raman (RS). Ces techniques sondent les vibrations moléculaires de l'échantillon lorsque la lumière interagit avec celui-ci, ce qui représente des 'empreintes moléculaires' de la composition chimique globale. Les deux techniques sont très prometteuses pour le diagnostic en santé, notamment dans le cadre des 'biopsies liquides', en particulier les biofluides. Cette étude a porté sur le développement des méthodologies bio-spectroscopiques pour l'analyse biochimique du sérum à visée diagnostic rapide et détection de pathologies. Au-delà de la preuve–de-concept et des études sur les variations préanalytiques (qui n'ont montré aucun effet sur le profil spectral sérique) par la congélation/décongélation du sérum et le séchage en milieu ambiant, trois études diagnostiques ont été menées sur des sérums provenant de patients avec différentes pathologies : cirrhotiques avec ou sans un carcinome hépatocellulaire, différents stades de fibrose et différents stades de tumeurs cérébrales. Tout au long de cette thèse, une série de techniques spectroscopiques IRTF et Raman ont été développées/utilisées, telles que l'ATR-IRTF, HTS-IRTF (IRTF à haut débit), spectroscopie Raman sur sérums humains séchés et liquides. Des approches chimiométriques avancées ont été utilisées telles que PCA, HCA, PLS-DA, LDA avancé, SVM-LOOCV avec fonction de base radiale et classifieurs Random Forest, avec pour but de développer un classificateur robuste de diagnostique d'une pathologie. Dans toutes les études de diagnostic, les résultats ont montré une capacité diagnostique modérée à bonne. Ces travaux démontrent que la spectroscopie vibrationnelle associée à des méthodes chimiométriques avancées peut constituer une approche complémentaire pour le diagnostic clinique, tels que les zones de soins.
Cette thèse s'intéresse au rôle de la cohésion lexicale dans différentes approches de l'analyse du discours. Nous y explorons deux hypothèses principales :  - l'analyse distributionnelle, qui permet de rapprocher des unités lexicales sur la base des contextes syntaxiques qu'elles partagent, met au jour des relations sémantiques variées pouvant être exploitées pour la détection de la cohésion lexicale des textes ;  - les indices lexicaux constituent des éléments de signalisation de l'organisation du discours pouvant être exploités aussi bien à un niveau local (identification de relations rhétoriques entre constituants élémentaires du discours) qu'à un niveau global (repérage ou caractérisation de segments de niveau supérieur dotés d'une fonction rhétorique et garantissant la cohérence et la lisibilité du texte, par exemple passages à unité thématique). Concernant le premier point, nous montrons la pertinence d'une ressource distributionnelle pour l'appréhension d'une large gamme de relations impliquées dans la cohésion lexicale des textes. Nous présentons les méthodes de projection et de filtrage que nous avons mises en œuvre pour la production de sorties exploitables. Concernant le second point, nous fournissons une série d'éclairages qui montrent l'apport d'une prise en compte réfléchie de la cohésion lexicale pour une grande variété de problématiques liées à l'étude et au repérage automatique de l'organisation textuelle : segmentation thématique de textes, caractérisation des structures énumératives, étude de la corrélation entre lexique et structure rhétorique du discours et enfin détection de réalisations d'une relation de discours particulière, la relation d'élaboration.
Malgré le développement de la surveillance syndromique et des méthodes modernes d'aide à la décision, la surveillance des épidémies dans le milieu hospitalier reste généralement une tâche manuelle. Pourtant, l'utilisation d'algorithmes de détection permettrait d'améliorer l'efficacité de la surveillance, d'en étendre les contours et de réduire le temps dévolu à cette tâche. La qualité méthodologique des évaluations publiées à ce jour est cependant trop faible pour conclure sur l'efficacité réelle de ces outils. Nous proposons donc dans cette thèse quelques éléments clés d'un cadre méthodologique suffisant pour la détection précoce, ainsi qu'un premier jeu de données comportant des événements à risque épidémique étiquetés. Ce jeu de données permettra à tous les chercheurs de développer, évaluer et comparer des algorithmes de détection. Cependant, comme l'ont montré les recherches sur les systèmes de surveillance et les outils d'intelligence artificiel appliquée aux soins, la mise en œuvre de ces nouvelles technologies peut s'avérer difficile, et les performances observées en vie réelle peuvent être différentes de celles recueillies lors du développement. Il est donc nécessaire d'inclure d'autres indicateurs et d'autres types de méthodologie d'évaluation pour mesurer la réelle utilité de ces outils, en prenant notamment en compte leur acceptabilité, leur facilité d'utilisation, leurs coûts et leurs impacts sur les pratiques.
Dans le contexte du vieillissement de la population, le but de cette thèse est d'inclure au domicile des personnes âgées un système de reconnaissance automatique de la parole (RAP) capable de reconnaître des appels de détresse pour alerter les secours. Les modèles acoustiques des systèmes de RAP sont généralement appris avec de la parole non âgée, prononcé de façon neutre et lue. Or, dans notre contexte, nous sommes loin de ces conditions idéales (voix âgée et émue), et le système doit donc être adapté à la tâche. A partir de ces corpus, une étude sur les différences entre voix jeunes/âgées d'une part, et entre voix neutre/émue d'autre part nous ont permis de développer un système de RAP adapté à la tâche. Celui-ci a ensuite été évalué sur des données issues d'une expérimentation en situation réaliste incluant des chutes jouées.
L'inversion acoustique-articulatoire de la parole consiste à récupérer la forme du conduit vocal à partir d'un signal de parole. Ce problème est abordé à l'aide d'une méthode d'analyse par synthèse reposant sur un modèle physique de production de la parole contrôlé par un petit nombre de paramètres décrivant la forme du conduit vocal : l'ouverture de la mâchoire, la forme et la position de la langue et la position des lèvres et du larynx. Afin de s'approcher de la géométrie de notre locuteur, le modèle articulatoire est construit à l'aide de contours articulatoires issus d'images cinéradiographiques présentant une vue sagittale du conduit vocal. Ce synthétiseur articulatoire nous permet de créer une table formée de couples associant un vecteur articulatoire au vecteur acoustique correspondant. Nous n'utiliserons pas les formants (fréquences de résonance du conduit vocal) comme vecteur acoustique car leur extraction n'est pas toujours fiable provoquant des erreurs lors de l'inversion. Les coefficients cepstraux sont utilisés comme vecteur acoustique. De plus, l'effet de la source et les disparités entre le conduit vocal du locuteur et le modèle articulatoire sont pris en compte explicitement en comparant les spectres naturels à ceux produits par le synthétiseur car nous disposons des deux signaux
Cette thèse propose une méthode pour coordonner flexiblement des Systèmes Multi-Agents (SMA). Plus en détails, nous étudions comment influencer des agents artificiels afin que, collectivement, ils atteignent des objectifs complexes et/ou dynamiques dans des environnements eux-aussi complexes et dynamiques (ex : un groupe de robots pour secourir les victimes lors d'un désastre, qui peut s'adapter à une grande variété de dangers, conditions climatiques, état des victimes). Dans ce but, nous avons d'abord étudié pourquoi, dans les sociétés humaines, les humains parviennent à coordonner relativement flexiblement mais pas leurs contreparties artificielles (agents des SMA). Cette opposition peut être grandement expliquée à l'aide d'un facteur clef : la culture. Les humains qui partagent un même bagage culturel se coordonnent flexiblement plus facilement, car ils ont une idée commune de ce que "travailler ensemble" veut dire. A contrario, les agents n'ont pas ce bagage et leurs échecs pour travailler ensemble s'apparente souvent à des chocs culturels. Ainsi, notre objectif consiste à répondre à la question suivante : peut-on utiliser une culture semblable à celle des humains comme un outil coordonner les SMA (et si oui, comment) ? Pour répondre à cette question, il nous faut d'abord expliquer : comment intégrer une culture semblable à celle des humains dans un SMA ? Cette seconde question en soulève une troisième à étudier en premier : comment est-ce que la culture influence la manière dont la coordination se passe dans les sociétés humaines ? 1. Nous montrons que de manière générale, la culture influence les décisions individuelles prises en situation d'interaction (ex : au travers d'attentes, de manière d'agir et de raisonner). Cette influence mène à l'occurrence de schémas d'interaction abstraits, récurrent et cohérents, qui, généralement, améliorent la performance collective. Ensuite, nous spécifions comment les principaux mécanismes l'influence connue de la culture (ex : importance culturelle accordée au pouvoir, aux règles) appliquent spécifiquement en situation de coordination (ex : la culture influence si les dirigeants donnent des ordres vs. des propositions à leurs subordonnés). 2. Nous montrons comment répliquer les mécanismes l'influence de la culture sur la coordination dans les SMA. Tout d'abord, puisque la culture est fondée dans les décisions individuelles, nous mettons en avant un mécanisme de décision humain clef qui, à la fois, est sensible à la culture et influence la coordination. Ce mécanisme se trouve dans les valeurs, ce que les gens considèrent comme "bien" ou "important" (ex : honnêteté, discipline, autonomie). Ensuite, nous intégrons ces valeurs dans une architecture agent capable de prendre des décisions en situation de coordination. Enfin, nous illustrons que notre architecture peut en effet reproduire l'influence de la culture sur la coordination à travers de deux simulations qui répliquent des phénomènes culturels en situation de coordination connus. 3. Nous étudions comment ces valeurs, inspirées des valeurs humaines, peuvent être utilisées coordonner des SMA. Tout d'abord, nous étudions pour quels problèmes les valeurs offrent un moyen opérationnel pour soutenir la coordination. A l'instar des sociétés humaines, les valeurs sont particulièrement offrent un haut niveau de flexibilité, quand les agents doivent raisonner eux-même pour établir une coordination. Puis, nous étudions les détails techniques à considérer pour utiliser en pratique des valeurs pour coordonner flexiblement des SMA (ex : quelles valeurs choisir ? Comment les représenter ?).
La psychiatrie est une spécialité médicale qui vise à fournir un diagnostic et à traiter des troubles mentaux. Malgré des classifications internationalement reconnues, la catégorisation des patients selon des critères diagnostiques reste problématique. Les catégories actuelles peinent à prendre en compte l'hétérogénéité interindividuelle, les difficultés de délimitations des syndromes et l'influence sur les symptômes de nombreux facteurs dans l'histoire individuelle ou dans l'environnement. La recherche en psychiatrie nécessite une amélioration de la description des comportements, des syndromes ou des dysfonctionnements associés aux troubles psychiatriques. À cette fin, nous proposons OntoPsychia, une ontologie pour la psychiatrie, divisée en deux modules : les facteurs sociaux et environnementaux des troubles mentaux, et les troubles mentaux. L'utilisation d'OntoPsychia associée à des outils dédiés permettra la prise en compte des facteurs sociaux et environnementaux, la représentation de la comorbidité et une proposition de consensus autour des catégories descriptives des troubles psychiatriques. Dans un premier temps, nous avons développé les deux modules ontologiques selon deux méthodes différentes. La première propose une analyse de comptes rendus d'hospitalisation, tandis que la deuxième propose un alignement de différentes classifications psychiatriques, pour répondre au besoin de consensus. Dans un deuxième temps, nous avons développé un cadre méthodologique pour valider la structure et la sémantique des ontologies.
La communication multimodale, qui est primordiale dans les relations interpersonnelles, reste encore très limitée dans les interfaces homme-machine actuelles. Parmi les différentes modalités qui ont été adoptées par les recherches en interaction homme-machine, la modalité posturale a été moins explorée que d'autres modalités comme la parole ou les expressions faciales. Les postures corporelles sont pourtant indispensables pour interpréter et situer l'interaction entre deux personnes, que ce soit en termes de contexte spatial ou de contexte social. Il manque cependant des modèles informatiques reliant ces médias et modalités aux fonctions de communication pertinentes dans les interactions interpersonnelles comme celles liées à l'espace ou aux émotions. L'objectif de cette thèse est de concevoir un premier modèle informatique permettant d'exploiter les postures dans les interactions homme-machine. Comment spécifier les comportements posturaux de personnages virtuels ? L'approche proposée consiste dans un premier temps à prendre comme point de départ des corpus vidéo filmés dans différentes situations. Nous avons défini un schéma de codage pour annoter manuellement les informations posturales à différents niveaux d'abstraction et pour les différentes parties du corps. Ces représentations symboliques ont été exploitées pour effectuer des analyses des relations spatiales et temporelles entre les postures exprimées par deux interlocuteurs. Ces représentations symboliques de postures ont été utilisées dans un deuxième temps pour simuler des expressions corporelles de personnages virtuels. Nous nous sommes intéressés à un composant des émotions particulièrement pertinent pour les études sur les postures : les tendances à l'action. Enfin, dans un troisième temps, les expressions corporelles d'un personnage virtuel ont été conçues dans une application mixte faisant intervenir un personnage virtuel et un utilisateur dans un cadre d'interaction ambiante. Les postures et gestes du personnage virtuel ont été utilisées pour aider l'utilisateur à localiser des objets du monde réel.
L'objectif de cette thèse est de montrer les différentes facettes de l'annotation de corpus dans la langue arabe. Nous présentons nos travaux scientifiques sur l'annotation de corpus et sur la création de ressources lexicales dans la langue arabe. D'abord, nous discutons des méthodes, des difficultés linguistiques, des guides d'annotation, de l'optimisation de l'effort d'annotation, ainsi que de l'adaptation à la langue arabe de procédures d'annotation existantes. Ensuite, nous montrons la complémentarité entre les différentes couches d'annotation. Enfin, nous illustrons l'importance de ces travaux pour le traitement automatique des langues en illustrant quelques exemples de ressources et d'applications.
Depuis l'invention du PageRank par Google pour les requêtes Web à la fin des années 1990, les algorithmes de graphe font partie de notre quotidien. Au milieu des années 2000, l'arrivée des réseaux sociaux a amplifié ce phénomène, élargissant toujours plus les cas d'usage de ces algorithmes. Les relations entre entités peuvent être de multiples sortes : relations symétriques utilisateur-utilisateur pour Facebook ou LinkedIn, relations asymétriques follower-followee pour Twitter, ou encore, relations bipartites utilisateur-contenu pour Netflix ou Amazon. Toutes soulèvent des problèmes spécifiques et les applications sont nombreuses : calcul de centralité pour la mesure d'influence, le partitionnement de nœuds pour la fouille de données, la classification de nœuds pour les recommandations ou l'embedding pour la prédiction de liens en sont quelques exemples. En parallèle, les conditions d'utilisation des algorithmes de graphe sont devenues plus contraignantes. D'une part, les jeux de données toujours plus gros avec des millions d'entités et parfois des milliards de relations limite la complexité asymptotique des algorithmes pour les applications industrielles. D'autre part, dans la mesure où ces algorithmes influencent nos vies, les exigences d'explicabilité et d'équité dans le domaine de l'intelligence artificielle augmentent. L'Union européenne a par exemple publié un guide de conduite pour une IA fiable. Ceci implique de pousser encore plus loin l'analyse des modèles actuels, voire d'en proposer de nouveaux. Cette thèse propose des réponses ciblées via l'analyse d'algorithmes classiques, mais aussi de leurs extensions et variantes, voire d'algorithmes originaux. La capacité à passer à l'échelle restant un critère clé. Dans le sillage de ce que le projet Scikit-learn propose pour l'apprentissage automatique sur données vectorielles, nous estimons qu'il est important de rendre ces algorithmes accessibles au plus grand nombre et de démocratiser la manipulation de graphes. Nous avons donc développé un logiciel libre, Scikit-network, qui implémente et documente ces algorithmes de façon simple et efficace. Grâce à cet outil, nous pouvons explorer plusieurs tâches classiques telles que l'embedding de graphe, le partitionnement, ou encore la classification semi-supervisée.
L'une des capacités humaines fondamentales est la capacité d'interpréter des symboles. Malgré plusieurs décennies de travaux en neuropsychologique et neuroimagerie sur le substrat cognitif et neuronal des représentations sémantiques, de nombreuses questions restent sans réponse. Dans la première partie, nous passons en revue les différentes positions théoriques concernant les corrélats cognitifs et neuraux des représentations sémantiques. De plus, nous proposons une distinction opérationnelle entre les dimensions moto-perceptives (c'est-à-dire les attributs des objets auxquels les mots se réfèrent perçus par les sens) et conceptuelles (c'est-à-dire l'information construite par l'intégration des multiples caractéristiques perceptives). Dans la deuxième partie, nous présentons les résultats des études menées afin d'étudier l'automaticité de la récupération, l'organisation topographique et la dynamique temporelle des dimensions moto-perceptives et conceptuelles de la signification des mots. En particulier, ils soulignent l'importance d'une intégration fructueuse entre les théories cognitives et les méthodes statistiques avancées afin d'éclairer les mystères entourant les représentations sémantiques.
Les représentations des mots sont à la base du plupart des systèmes modernes pour le traitement automatique du langage, fournissant des résultats compétitifs. Cependant, d'importantes questions se posent concernant les défis auxquels ils sont confrontés pour faire face aux phénomènes complexes du langage naturel et leur capacité à saisir la variabilité du langage naturel. Pour mieux gérer les phénomènes complexes du langage, de nombreux travaux ont été menées pour affiner les représentations génériques de mots ou pour créer des représentations spécialisées. Bien que cela puisse aider à distinguer la similarité sémantique des autres types de relations sémantiques, il peut ne pas suffire de modéliser certains types de relations, telles que les relations logiques d'implication ou de contradiction. La première partie de la thèse étudie l'encodage de la notion d'implication textuelle dans un espace vectoriel en imposant l'inclusion d'information. Des opérateurs d'implication sont ensuite développées et le cadre proposé peut être utilisé pour réinterpréter un modèle existant de la sémantique distributionnelle. Des évaluations sont fournies sur la détection d'hyponymie en tant que une instance d'implication lexicale. Un autre défi concerne la variabilité du langage naturel et la nécessité de désambiguïser les unités lexicales en fonction du contexte dans lequel elles apparaissent. Les représentations génériques de mots ne réussissent pas à elles seules, des architectures différentes étant généralement utilisées pour aider à la désambiguïsation. Étant donné que les représentations de mots sont construites à partir de statistiques de cooccurrence sur de grands corpus et qu'elles reflètent ces statistiques, elles fournissent une seule représentation pour un mot donné, malgré ses multiples significations. Même dans le cas de mots monosémiques, cela ne fait pas la distinction entre les différentes utilisations d'un mot en fonction de son contexte. Dans ce sens, on pourrait se demander s'il est possible d'exploiter directement les informations linguistiques fournies par le contexte d'un mot pour en ajuster la représentation. Ces informations seraient-elles utiles pour créer une représentation enrichie du mot dans son contexte ? Et si oui, des informations de nature syntaxique peuvent-elles aider au processus ou le contexte local suffit ? Dans la deuxième partie de la thèse, nous étudions une façon d'incorporer la connaissance contextuelle dans les représentations de mots eux-mêmes, en exploitant les informations provenant de l'analyse de dépendance de phrase ainsi que les informations de voisinage local. Nous proposons des représentations de mots contextualisées sensibles à la syntaxe (SATokE) qui capturent des informations linguistiques spécifiques et encodent la structure de la phrase dans leurs représentations. Cela permet de passer des représentations de type générique (invariant du contexte) à des représentations spécifiques (tenant compte du contexte). Alors que la syntaxe était précédemment considérée pour les représentations de mots, ses avantages n'ont peut-être pas été entièrement évalués au-delà des modèles qui exploitent ces informations à partir de grands corpus. Les représentations obtenues sont évaluées sur des tâches de compréhension du langage naturel : classification des sentiments, détection de paraphrases, implication textuelle et analyse du discours. Nous démontrons empiriquement la supériorité de ces représentations par rapport aux représentations génériques et contextualisées des mots existantes. Le travail proposé dans la présente thèse contribue à la recherche dans le domaine de la modélisation de phénomènes complexes tels que l'implication textuelle, ainsi que de la variabilité du langage par le biais de la proposition de représentations contextualisés.
Il s'agit dans le présent projet d'initier la langue kabyle au domaine du traitement automatique des langues naturelles (TALN) en la dotant d'une base de données, sur le logiciel NooJ, permettant la reconnaissance des unités linguistiques d'un corpus écrit. Le travail est devisé en quatre parties. Dans la première nous avons donné un aperçu historique de la linguistique formelle et présenté le domaine du TALN, le logiciel NooJ et les unités linguistiques traitées. La deuxième est consacrée à la description de processus suivi dans le traitement et l'intégration des verbes dans NooJ. Nous avons construit un dictionnaire contenant 4508 entrées et 8762 dérivés et des modèles de flexion pour chaque type d'entrée. Dans la troisième nous avons expliqué le traitement des noms et des autres unités. Nous avons, pour les noms, construit un dictionnaire (3508 entrées et 501 dérivés) que nous avons reliés à leurs modèles de flexion et pour les autres unités (870 unités dont, adverbes, prépositions, conjonctions, interrogatifs, pronoms personnels, etc.), il s'agit seulement de listes (sans flexion). Chacune de ces deux parties (deuxième et troisième) est complétée par des exemples d'applications sur un texte, chose qui nous a permis de voir, à l'aide des annotations, les différents types d'ambiguïtés. Dans la dernière partie, après avoir dégagé une liste de différents types d'amalgame, nous avons essayé de décrire, à l'aide de quelques exemples de grammaire syntaxiques, l'étape de la désambiguïsation.
Le traitement des entités nommées fait aujourd'hui figure d'incontournable en Traitement Automatique des Langues. Fort de ce succès, le traitement des entités nommées s'oriente désormais vers de nouvelles perspectives, avec la désambiguïsation et une annotation enrichie de ces unités. Ces nouveaux défis rendent cependant d'autant plus cruciale la question du statut théorique des entités nommées, lequel n'a guère été discuté jusqu'à aujourd'hui. Deux axes de recherche ont été investis durant ce travail de thèse avec, d'une part, la proposition d'une définition des entités nommées et, d'autre part, des méthodes de désambiguïsation. A la suite d'un état des lieux de la tâche de reconnaissance de ces unités, il fut nécessaire d'examiner, d'un point de vue méthodologique, comment aborder la question de la définition les entités nommées. La démarche adoptée invita à se tourner du côté de la linguistique (noms propres et descriptions définies) puis du côté du traitement automatique, ce parcours visant au final à proposer une définition tenant compte tant des aspects du langage que des exigences des systèmes informatiques. La suite du mémoire rend compte d'un travail davantage expérimental, avec l'exposé d'une méthode d'annotation fine tout d'abord, de résolution de métonymie enfin.
Une recherche consacrée à l'étymologie et à l'histoire de la terminologie linguistique permettra un gain important en connaissances historico-étymologiques. Ce gain dans la description de termes scientifiques particuliers a des répercussions plus générales : une meilleure connaissance des processus de formation du vocabulaire de la linguistique, et des indications sur les conséquences pour la sous-discipline étymologique elle-même d'un progrès massif dans la description d'une part significative du lexique (les internationalismes savants). La thèse contribue aussi à un progrès méthodologique dans un secteur faible de la science étymologique, progrès d'autant plus significatif que celui-ci est crucial pour notre connaissance du fonctionnement actuel de la création lexicale en français. Cette thèse présente la particularité de se situer sur plusieurs axes de travail, couvrant la lexicologie (et la lexicographie, car elle contient une partie consacrée à des articles lexicographiques), la terminologie (le sujet de ces articles étant les termes de la linguistique) et l'étymologie. En outre, la part philologique de ce travail est importante, car l'analyse des débuts de l'existence des termes étudiés nous a amenée à analyser des textes de diverses natures et de périodes différentes. Nous exploitons ensuite les nouvelles données offertes par nos articles lexicographiques dans le but de porter un regard synthétique sur la constitution de la terminologie linguistique française.
L'objectif principal de cette thèse est le développement d'algorithmes événementiels pour la détection et le suivi d'objets. Ces algorithmes sont spécifiquement conçus pour travailler avec une sortie produite par des caméras neuromorphiques. Ce type de caméras sont un nouveau type de capteurs bio inspirés, dont le principe de fonctionnement s'inspire de la rétine : chaque pixel est indépendant et génère des événements de manière asynchrone lorsqu'un changement de luminosité suffisamment important est détecté à la position correspondante du plan focal. Cette nouvelle façon d'encoder l'information visuelle requiert de nouvelles méthodes pour la traiter. Le système mécanique virtuel résultant est mis à jour pour chaque événement. Le chapitre suivant présente un algorithme de détection de lignes et de segments, pouvant constituer une caractéristique (feature) événementielle de bas niveau. Ensuite, deux méthodes événementielles pour l'estimation de la pose 3D sont présentées. Le premier de ces algorithmes 3D est basé sur l'hypothèse que l'estimation de la pose est toujours proche de la position réelle, et requiert donc une initialisation manuelle. Le deuxième de ces algorithmes 3D est conçu pour surmonter cette limitation. Toutes les méthodes présentées mettent à jour l'estimation de la position (2D ou 3D) pour chaque événement.
Cette thèse porte sur l'inversion acoustique-articulatoire, c'est-à-dire la récupération des mouvements des articulateurs de la parole à partir du signal sonore. Nous présentons dans ce mémoire une évolution importante des méthodes de tabulation à codebooks utilisant une table de correspondants acoustique-articulatoire précalculée à l'aide d'un modèle de synthèse acoustique. En dehors de la méthode d'inversion proprement dite, nous proposons également l'introduction de deux types de contraintes liées au contexte d'élocution : des contraintes phonétiques génériques, issues de l'analyse par des experts humains de l'invariance articulatoire des voyelles, et des contraintes visuelles, en l'occurrence des contraintes obtenues automatiquement à partir de l'enregistrement et l'analyse d'images en stéréovision du locuteur.
Ces dernières années ont connu un regain d'intérêt pour l'utilisation des graphes comme moyen fiable de représentation et de modélisation des données, et ce, dans divers domaines de l'informatique. En particulier, pour les grandes masses de données, les graphes apparaissent comme une alternative prometteuse aux bases de données relationnelles. Plus particulièrement, le recherche de sous-graphes s'avère être une tâche cruciale pour explorer ces grands jeux de données. Dans cette thèse, nous étudions deux problématiques principales. Ce problème vise à rechercher les k-meilleures correspondances (top-k) d'un graphe motif dans un graphe de données. Pour cette problématique, nous introduisons un nouveau modèle de détection de motifs de graphe nommé la Simulation Relaxée de Graphe (RGS), qui permet d'identifier des correspondances de graphes avec un certain écart (structurel) et ainsi éviter le problème de réponse vide. Ensuite, nous formalisons et étudions le problème de la recherche des k-meilleures réponses suivant deux critères, la pertinence (la meilleure similarité entre le motif et les réponses) et la diversité (la dissimilarité entre les réponses). Nous considérons également le problème des k-meilleures correspondances diversifiées et nous proposons une fonction de diversification pour équilibrer la pertinence et la diversité. En outre, nous développons des algorithmes efficaces basés sur des stratégies d'optimisation en respectant le modèle proposé. Notre approche est efficiente en terme de temps d'exécution et flexible en terme d'applicabilité. L'analyse de la complexité des algorithmes et les expérimentations menées sur des jeux de données réelles montrent l'efficacité des approches proposées. Dans un second temps, nous abordons le problème de recherche agrégative dans des documents XML. Dans un premier temps nous présentons la motivation derrière ce paradigme de recherche agrégative et nous expliquons les gains potentiels par rapport aux méthodes classiques de requêtage. Ensuite nous proposons une nouvelle approche qui a pour but de construire, dans la mesure du possible, une réponse cohérente et plus complète en agrégeant plusieurs résultats provenant de plusieurs sources de données. Les expérimentations réalisées sur plusieurs ensembles de données réelles montrent l'efficacité de cette approche en termes de pertinence et de qualité de résultat.
Les problèmes multi-objectifs se posent dans plusieurs scénarios réels dans le monde où on doit trouver une solution optimale qui soit un compromis entre les différents objectifs en compétition. On étudie deux méthodes d'apprentissage multi-objectif en détail. Dans la première méthode, on étudie le problème de trouver le classifieur optimal pour réaliser des mesures de performances multivariées. Dans la seconde méthode, on étudie le problème de classer des informations diverses dans les missions de recherche des informations.
Ensuite, nous avons constitué un corpus à partir de traductions d'experts métier et nous l'avons passé en revue pour renforcer l'analyse des différences. La différence la plus flagrante est l'utilisation de la traduction automatique (TA) ainsi que le contexte de production des traductions. En étudiant les technologies de TA actuelles, nous constatons qu'elles permettent soit une post-édition en langue cible après le processus de traduction, soit une pré-édition en langue source avant le processus de traduction. Nous suggérons de tirer profit de la situation inédite de rédacteur traduisant, pour utiliser l'expertise du rédacteur pendant le processus de traduction et de développer une fonctionnalité de TA permettant une édition en cours de processus.
La présente thèse traite de l'analyse de scènes extraites d'environnements sonores, résultat auditif du mélange de sources émettrices distinctes et concomitantes. Ouvrant le champ des sources et des recherches possibles au-delà des domaines plus spécifiques que sont la parole ou la musique, l'environnement sonore est un objet complexe. Son analyse, le processus par lequel le sujet lui donne sens, porte à la fois sur les données perçues et sur le contexte de perception de ces données. Tant dans le domaine de la perception que de l'apprentissage machine, toute expérience suppose un contrôle fin de l'expérimentateur sur les stimuli proposés. Néanmoins, la nature de l'environnement sonore nécessite de se placer dans un cadre écologique, c'est à dire de recourir à des données réelles, enregistrées, plutôt qu'à des stimuli de synthèse. Conscient de cette problématique, nous proposons un modèle permettant de simuler, à partir d'enregistrements de sons isolés, des scènes sonores dont nous maîtrisons les propriétés structurelles -- intensité, densité et diversité des sources. Appuyé sur les connaissances disponibles sur le système auditif humain, le modèle envisage la scène sonore comme un objet composite, une somme de sons sources. Le premier concerne la perception, et la notion d'agrément perçu dans des environnements urbains. L'usage de données simulées nous permet d'apprécier finement l'impact de chaque source sonore sur celui-ci. Le deuxième concerne la détection automatique d'événements sonores et propose une méthodologie d'évaluation des algorithmes mettant à l'épreuve leurs capacités de généralisation.
Ce travail de recherche vise à étudier les usages linguistiques non standard de certains scripteurs peu lettrées pendant la Grande Guerre, à partir de leurs correspondances privées.
Les applications d'analyse de données apportent des améliorations fondamentales dans de nombreux domaines tels que les sciences, la santé et la sécurité. Cela a stimulé la croissance des volumes de données (le déluge du Big Data). Pour extraire des informations utiles à partir de cette quantité énorme d'informations, différents modèles de traitement des données ont émergé tels que MapReduce, Hadoop, et Spark. Les traitements Big Data sont traditionnellement exécutés à grande échelle (les systèmes HPC et les Clouds) pour tirer parti de leur puissance de calcul et de stockage. Habituellement, ces plateformes à grande échelle sont utilisées simultanément par plusieurs utilisateurs et de multiples applications afin d'optimiser l'utilisation des ressources. Bien qu'il y ait beaucoup d'avantages à partager de ces plateformes, plusieurs problèmes sont soulevés dès lors qu'un nombre important d'utilisateurs et d'applications les utilisent en même temps, parmi lesquels la gestion des E / S et des défaillances sont les principales qui peuvent avoir un impact sur le traitement efficace des données. Nous nous concentrons tout d'abord sur les goulots d'étranglement liés aux performances des E/S pour les applications Big Data sur les systèmes HPC. Nous commençons par caractériser les performances des applications Big Data sur ces systèmes. Nous identifions les interférences et la latence des E/S comme les principaux facteurs limitant les performances. Ensuite, nous nous intéressons de manière plus détaillée aux interférences des E/S afin de mieux comprendre les causes principales de ce phénomène. De plus, nous proposons un système de gestion des E/S pour réduire les dégradations de performance que les applications Big Data peuvent subir sur les systèmes HPC. Deuxièmement, nous nous concentrons sur l'impact des défaillances sur la performance des applications Big Data en étudiant la gestion des pannes dans les clusters MapReduce partagés. Nous présentons un ordonnanceur qui permet un recouvrement rapide des pannes, améliorant ainsi les performances des applications Big Data.
Le contexte de la thèse se situe dans la perspective de travaux sur l'interprétation automatisée de manifestations complexes de faits de discours non saisissables par les méthodes actuelles de l'analyse de discours (AD) dans des données issues de transcription d'interviews politiques. Il se focalise sur le mécanisme linguistique de la « nomination » , en lien avec les concepts de dénomination, désignation, référenciation. En particulier, les sorties des reconnaissances d'entités et de coréférences pourront être exploitées, afin de déterminer leur apport pour un système expérimental de focalisé sur les nominations. Un retour sera fait à chaque traitement TAL afin d'évaluer son apport dans la reconnaissance des nominations, dans une optique d'intégration aux outils traditionnels de l'AD. Un des enjeux de cette thèse est aussi de proposer un système de classification pour l'entreprise Reticular afin de qualifier différents acteurs de la vie politique.
Ce sujet de thèse porte sur la traduction automatique de contenus générés par les utilisateur (par exemple sur les réseaux sociaux). Il remet en cause les méthodes de traduction de l'état de l'art en soulevant deux défis scientifiques : 1) la nécessité de dépasser les limites d'une traduction phrase par phrase en prenant en compte le contexte de l'énoncé et 2) le caractère très bruité des énoncés qui contrairement aux textes journalistiques généralement considérés en TAL sont extrêmement diversifiés, truffés d'abréviations, de fautes d'orthographe ou typographiques et d'erreurs grammaticales.
Cette thèse s'inscrit dans le cadre de la recherche d'informations sur le Web à l'aide de méthodes inspirées des recherches en informatique linguistique du laboratoire LaLICC. Notre travail avait pour but de développer un outil qui permet d'assister de manière interactive, lors d'une session de recherche, un utilisateur souhaitant collecter des informations disponibles sur le Web sur une notion ou un sujet donné. L'idée fondamentale mise en œuvre dans l'outil réalisé, appelé RAP, a consisté à orienter la recherche selon un ou plusieurs points de vue prédéfinis qui permettent de satisfaire d'une manière graduelle les besoins informationnels de l'utilisateur. Conceptuellement, une partie importante de notre travail a consisté à étudier la manière de caractériser la notion de besoin d'un utilisateur qui constitue le fondement intuitif sur lequel repose la notion de points de vue. Pour cela, les connaissances linguistiques sur lesquelles nous nous sommes appuyée nous ont permis de ne plus voir la notion de besoin comme étant nécessairement liée à une communauté d'utilisateurs particulière. Nos réflexions nous ont alors amené à poser les notions de besoin informationnel élémentaire ou complexe comme cadre théorique de notre recherche. A ces besoins correspondent les points de vue que l'utilisateur peut sélectionner pour orienter la recherche d'informations. Techniquement, orienter la recherche selon un point de vue revient à reformuler la requête utilisateur en y intégrant les marqueurs linguistiques relatifs au point de vue choisi, par exemple celui de la Causalité ou celui de la Citation. La reformulation a alors pour but d'une part, de réduire de façon notable le bruit, et d'autre part, de cibler des pages Web possédant un contenu sémantique riche. La réalisation des points de vue par cette technique de reformulation implique l'utilisation de marqueurs linguistiques issus des travaux de l'équipe LaLICC sur le filtrage sémantique des textes. Chaque classe de marqueurs relative au point de vue choisi intervient dans le processus de reformulation des requêtes de l'utilisateur à travers la technique de reformulation que nous avons développée, ensuite dans l'extraction des parties, paragraphes ou segments textuels du document où la manifestation textuelle de ce point de vue est détectée, aidant ainsi l'utilisateur à mieux sélectionner les pages Web intéressantes parmi les pages résultats du moteur de recherche consulté. L'ensemble de la démarche a été concrétisé par la construction de l'outil RAP écrit en Java et comprenant une Interface Homme-Machine conviviale, dans lequel 27 points de vue ont été implémentés découlant des différentes approches de six points de vue principaux : Causalité, Relations descriptives, Citation, Thème/Position, Problème/Solution, Acteurs.
Analyse probabiliste est l'un des domaines de recherche les plus attractives en langage naturel En traitement. Analyseurs probabilistes succès actuels nécessitent de grandes treebanks qui Il est difficile, prend du temps et coûteux à produire. Par conséquent, nous avons concentré notre l'attention sur des approches moins supervisés. Nous avons proposé deux catégories de solution : l'apprentissage actif et l'algorithme semi-supervisé. Stratégies d'apprentissage actives permettent de sélectionner les échantillons les plus informatives pour annotation. La plupart des stratégies d'apprentissage actives existantes pour l'analyse reposent sur la sélection phrases incertaines pour l'annotation. Nous montrons dans notre recherche, sur quatre différents langues (français, anglais, persan, arabe), que la sélection des phrases complètes ne sont pas une solution optimale et de proposer un moyen de sélectionner uniquement les sous-parties de phrases. Comme nos expériences ont montré, certaines parties des phrases ne contiennent aucune utiles information pour la formation d'un analyseur, et en se concentrant sur les sous-parties incertains des phrases est une solution plus efficace dans l'apprentissage actif.
Nous nous intéressons dans cette thèse aux systèmes de communication homme-machine multimodale qui utilisent les modes suivants : la parole, le geste et le visuel. L'usager communique avec le système par un énoncé oral en langue naturelle et/ou un geste. Dans sa requête, encodée sur les différentes modalités, l'usager exprime son but et désigne des objets (référents) nécessaires à la réalisation de ce but. Le système doit identifier de manière précise et non ambiguë ces objets désignés. Les principaux aspects de la réalisation consistent en les modélisations du traitement de la langue naturelle dans le contexte de la parole, du traitement du geste et du contexte visuel (utilisation de la saillance visuelle) en prenant en compte les difficultés inhérentes en contexte de la communication multimodale : erreur de reconnaissance de la parole, ambiguïté de la langue naturelle, imprécision du geste due à la performance de l'usager, ambiguïté dans la désignation due à la perception des objets affichés ou à la topologie de l'affichage. Pour l'interprétation complète de la requête nous proposons une méthode de fusion/vérification des résultats des traitements de chaque modalité pour trouver les objets désignés par l'usager.
La reconnaissance automatique des émotions dans la parole est un sujet de recherche relativement récent dans le domaine du traitement de la parole, puisqu'il est abordé depuis une dizaine d'années environs. Ce sujet fait de nos jours l'objet d'une grande attention, non seulement dans le monde académique mais aussi dans l'industrie, grâce à l'augmentation des performances et de la fiabilité des systèmes. Les premiers travaux étaient fondés sur des donnés jouées par des acteurs, et donc non spontanées. Même aujourd'hui, la plupart des études exploitent des séquences pré-segmentées d'un locuteur unique et non une communication spontanée entre plusieurs locuteurs. Cette méthodologie rend les travaux effectués difficilement généralisables pour des informations collectées de manière naturelle. Les travaux entrepris dans cette thèse se basent sur des conversations de centre d'appels, enregistrés en grande quantité et mettant en jeu au minimum 2 locuteurs humains (un client et un agent commercial) lors de chaque dialogue. Notre but est la détection, via l'expression émotionnelle, de la satisfaction client. Dans une première partie nous présentons les scores pouvant être obtenus sur nos données à partir de modèles se basant uniquement sur des indices acoustiques ou lexicaux. Nous montrons que pour obtenir des résultats satisfaisants une approche ne prenant en compte qu'un seul de ces types d'indices ne suffit pas. Nous proposons pour palier ce problème une étude sur la fusion d'indices de types acoustiques, lexicaux et syntaxico-sémantiques. Nous montrons que l'emploi de cette combinaison d'indices nous permet d'obtenir des gains par rapport aux modèles acoustiques même dans les cas ou nous nous basons sur une approche sans pré-traitements manuels (segmentation automatique des conversations, utilisation de transcriptions fournies par un système de reconnaissance de la parole). Dans une seconde partie nous remarquons que même si les modèles hybrides acoustiques/linguistiques nous permettent d'obtenir des gains intéressants la quantité de données utilisées dans nos modèles de détection est un problème lorsque nous testons nos méthodes sur des données nouvelles et très variées (49h issus de la base de données de conversations). Pour remédier à ce problème nous proposons une méthode d'enrichissement de notre corpus d'apprentissage. Ces ajouts nous permettent de doubler la taille de notre ensemble d'apprentissage et d'obtenir des gains par rapport aux modèles de départ. Enfin, dans une dernière partie nous choisissons d'évaluées nos méthodes non plus sur des portions de dialogues comme cela est le cas dans la plupart des études, mais sur des conversations complètes. Nous utilisons pour cela les modèles issus des études précédentes (modèles issus de la fusion d'indices, des méthodes d'enrichissement automatique) et ajoutons 2 groupes d'indices supplémentaires :  i) Des indices « structurels » prenant en compte des informations comme la durée de la conversation, le temps de parole de chaque type de locuteurs. ii) des indices « dialogiques » comprenant des informations comme le thème de la conversation ainsi qu'un nouveau concept que nous nommons « implication affective » . Celui-ci a pour but de modéliser l'impact de la production émotionnelle du locuteur courant sur le ou les autres participants de la conversation. Nous montrons que lorsque nous combinons l'ensemble de ces informations nous arrivons à obtenir des résultats proches de ceux d'un humain lorsqu'il s'agit de déterminer le caractère positif ou négatif d'une conversation.
La vision de l'informatique ubiquitaire permettant de construire des espaces intelligents interactifs dans l'environnement physique passe, peu à peu, du domaine de la recherche à la réalité. La capacité de calcul ne se limite plus à l'ordinateur personnel mais s'intègre dans de multiples appareils du quotidien, et ces appareils deviennent, grâce à plusieurs interfaces, capables de communiquer directement les uns avec les autres ou bien de se connecter à Internet. Dans cette thèse, nous nous sommes intéressés à un type d'environnement cible de l'informatique ubiquitaire qui forme ce que nous appelons un réseau hybride à connexions intermittentes (ICHN). Un ICHN est un réseau composé de deux parties : une partie fixe et une partie mobile. La partie fixe est constituée de plusieurs infostations fixes (potentiellement reliées entre elles avec une infrastructure fixe, typiquement l'Internet). La partie mobile, quant à elle, est constituée de smartphones portés par des personnes nomades. Tandis que la partie fixe est principalement stable, la partie mobile pose un certain nombre de défis propres aux réseaux opportunistes. En effet, l'utilisation de moyens de communication à courte portée couplée à des déplacements de personnes non contraints et à des interférences radio induit des déconnexions fréquentes. Le concept du "store, carry and forward" est alors habituellement appliqué pour permettre la communication sur l'ensemble du réseau. Avec cette approche, un message peut être stocké temporairement sur un appareil avant d'être transféré plus tard quand les circonstances sont plus favorables. Ainsi, n'importe quel appareil devient un relai de transmission opportuniste qui permet de faciliter la propagation d'un message dans le réseau. Dans ce contexte, la fourniture de services est particulièrement problématique, et exige de revisiter les composants principaux du processus de fourniture, tels que la découverte et l'invocation de service, en présence de ruptures de connectivité et en l'absence de chemins de bout en bout. Cette thèse aborde les problèmes de fourniture de service sur l'ensemble d'un ICHN et propose des solutions pour la découverte de services, l'invocation et la continuité d'accès. En ce qui concerne le défi de la découverte de services, nous proposons TAO-DIS, un protocole qui met en œuvre un mécanisme automatique et rapide de découverte de services. TAO-DIS tient compte de la nature hybride d'un ICHN et du fait que la majorité des services sont fournis par des infostations. Il permet aux utilisateurs mobiles de découvrir tous les services dans l'environnement afin d'identifier et de choisir les plus intéressants. Pour permettre aux utilisateurs d'interagir avec les services découverts, nous introduisons TAO-INV. TAO-INV est un protocole d'invocation de service spécialement conçu pour les ICHN. Il se fonde sur un ensemble d'heuristiques et de mécanismes qui assurent un acheminement efficace des messages (des requêtes et des réponses de services) entre les infostations fixes et les clients mobiles tout en conservant un surcoût et des temps de réponses réduits. Puisque certaines infostations dans le réseau peuvent être reliées entre elles, nous proposons un mécanisme de continuité d'accès (handover) qui modifie le processus d'invocation pour réduire les délais de délivrance. Dans sa définition, il est tenu compte de la nature opportuniste de la partie mobile de l'ICHN. Nous avons mené diverses expérimentations pour évaluer nos solutions et les comparer à d'autres protocoles conçus pour des réseaux ad hoc et des réseaux opportunistes. Les résultats obtenus tendent à montrer que nos solutions surpassent ces autres protocoles, notamment grâce aux optimisations que nous avons développées pour les ICHN. À notre avis, construire des protocoles spécialisés qui tirent parti des techniques spécifiquement conçues pour les ICHN est une approche à poursuivre en complément des recherches sur des protocoles de communication polyvalents
Cette thèse présente des résultats appartenant aux trois thèmes fondamentaux de la cryptographie à clé publique : l'intégrité, l'authentification et la confidentialité. Au sein de chaque thème nous concevons des nouvelles primitives et améliorons des primitives existantes. Le premier chapitre, dédié à l'intégrité, introduit une preuve non-interactive de génération appropriée de clés publiques RSA et un protocole de co-signature dans lequel tout irrespect de l'équité laisse automatiquement la partie lésée en possession d'une preuve de culpabilité incriminant la partie tricheuse. Le second chapitre, ayant pour sujet l'authentification, montre comme une mesure de temps permet de raccourcir les engagements dans des preuves à divulgation nulle et comment des biais, introduits à dessin dans le défi, permettent d'accroitre l'efficacité de protocoles. Ce chapitre généralise également le protocole de Fiat-Shamir à plusieurs prouveurs et décrit une fraude très sophistiquée de cartes-à-puce illustrant les dangers de protocoles d'authentification mal-conçus. Au troisième chapitre nous nous intéressons à la confidentialité. Nous y proposons un cryptosystème à clé publique où les hypothèses de complexité traditionnelles sont remplacées par un raffinement du concept de CAPTCHA et nous explorons l'application du chiffrement-pot-de-miel au langage naturel.
Ce travail de recherche se positionne dans la continuité de la thèse [21] effectuée au laboratoire LRI. Dans le cadre de cette thèse, nous mettrons en place des méthodes d'extraction et d'analyse des données internes aux réseaux sociaux mais également issues d'autres sources de données en vue de leur réutilisation dans la plateforme Octopeek. Néanmoins, la réalisation d'un tel système demeure un défi scientifique qui devra prendre en compte le volume de données, le temps d'exécution de la recherche et la sémantique des termes afin de fournir les meilleures réponses. En effet, pour chaque information recherchée, il faudra trouver la méthode, l'algorithme optimisé qui prend en compte la rapidité de calcul, et le scoring associé. Nous serons amenés à implémenter des algorithmes pour une utilisation à la volée et sur plusieurs personnes en parallèle.
La méthodologie de recherche mise en œuvre est une démarche empirique qui s'appuie sur l'analyse de situations de conception. Au cours du doctorat, un protocole expérimental mis en œuvre a été dupliqué trois fois dans des laboratoires partenaires. Cette recherche aboutit à un mémoire de thèse qui présente plusieurs contributions :  La première contribution se situe autour de la méthodologie de recherche proposée. Enfin la troisième contribution porte sur l'analyse des interactions dans les activités de conception étudiées. Les analyses identifient et qualifient les impacts des méthodes étudiées sur le contenu des interactions dans les phases amont de la conception.
Cette thèse est une contribution à la branche professionnelle de l'anglais de spécialité et au domaine de l'anglais comme lingua franca. Le contexte de la recherche est le milieu de l'entreprise où les employés échangent des courriels dans le cadre de la réalisation d'actions professionnelles routinières. Dans ce contexte, l'anglais est considéré comme une langue internationale et, dans la situation où les employés sont natifs d'autres langues que l'anglais, la lingua franca. La première partie traite des quatre concepts fondamentaux de cette recherche : l'anglais comme langue internationale, le registre, la phraséologie et les discours professionnels. La seconde partie présente la démarche méthodologique dont l'objectif estla constitution d'un corpus de 500 courriels professionnels à partir d'une base de données plus large que nous avons constituée lors de notre enquête de terrain dans le monde de l'entreprise. Le corpus est tout d'abord défini selon quatre situations linguistiques que nous présentons ci-dessous : 1. scripteurs natifs et destinataires natifs 2. scripteurs natifs et destinataires non natifs 3. scripteurs non natifs et destinataires natifs 4. scripteurs non natifs et destinataires non natifs Il est ensuite défini selon les quatre situations professionnelles suivantes : 1. achats et ventes de produits 2. management d'équipes distantes 3. administration des ressources humaines 4. résolution de problèmes techniques A partir de ce corpus, nous menons une étude de la variation sur trois ensembles de traits linguistico-discursifs et paralinguistiques qui nous permettent d'évaluer le degré de minimalisme dans les courriels, le degré d'imbrication du texte dans le contexte ainsi que de mesurer le caractère interpersonnel et intime de ce type d'échange. Notre étude nous mène tout d'abord à confirmer que l'analyse de registre est une approche efficace pour la caractérisation des discours ordinaires et routiniers dans les entreprises. Elle interroge ensuite la solidité des normes et du concept de communauté de discours en présentant l'anglais en circulation sur les réseaux professionnels, éphémères et mondiaux, comme une variété fluide.
Ce travail de thèse porte sur un nouveau paradigme pour la synthèse de la parole à partir du texte, à savoir la synthèse incrémentale. L'objectif est de délivrer la parole de synthèse au fur et à mesure de la saisie du texte par l'utilisateur, contrairement aux systèmes classiques pour lesquels la synthèse est déclenchée après la saisie d'une ou plusieurs phrases. L'application principale visée est l'aide aux personnes présentant un trouble sévère de la communication orale, et communiquant principalement à l'aide d'un synthétiseur vocal. Un synthétiseur vocal incrémental permettrait de fluidifier une conversation en limitant le temps que passe l'interlocuteur à attendre la fin de la saisie de la phrase à synthétiser. Un des défi que pose ce paradigme est la synthèse d'un mot ou d'un groupe de mot avec une qualité segmentale et prosodique acceptable alors que la phrase qui le contient n'est que partiellement connue au moment de la synthèse. Pour ce faire, nous proposons différentes adaptations des deux principaux modules d'un système de synthèse de parole à partir du texte : le module de traitement automatique de la langue naturelle (TAL) et le module de synthèse sonore. Pour le TAL en synthèse incrémentale, nous nous sommes intéressé à l'analyse morpho-syntaxique, qui est une étape décisive pour la phonétisation et la détermination de la prosodie cible. Nous décrivons un algorithme d'analyse morpho-syntaxique dit "à latence adaptative". Ce dernier estime en ligne si une classe lexicale (estimée à l'aide d'un analyseur morpho-syntaxique standard basé sur l'approche n-gram), est susceptible de changer après l'ajout par l'utilisateur d'un ou plusieurs mots. Si la classe est jugée instable, alors la synthèse sonore est retardée, dans le cas contraire, elle peut s'effectuer sans risque a priori de dégrader de la qualité segmentale et suprasegmentale. Cet algorithme exploite une ensemble d'arbre de décisions binaires dont les paramètres sont estimés par apprentissage automatique sur un large corpus de texte. Les résultats des évaluations objectives et perceptives montrent l'intérêt de la méthode proposée pour la langue française. Enfin, nous décrivons un prototype complet qui combine les deux méthodes proposées pour le TAL et la synthèse par HMM incrémentale. Une évaluation perceptive de la pertinence et de la qualité des groupes de mots synthétisés au fur et à mesure de la saisie montre que notre système réalise un compromis acceptable entre réactivité (minimisation du temps entre la saisie d'un mot et sa synthèse) et qualité (segmentale et prosodique) de la parole de synthèse.
S'il est indéniable que de nos jours la traduction automatique (TA) facilite la communication entre langues, et plus encore depuis les récents progrès des systèmes de TA statistiques, ses résultats sont encore loin du niveau de qualité des traductions obtenues avec des traducteurs humains. Ce constat résulte en partie du mode de fonctionnement d'un système de TA statistique, très contraint sur la nature des modèles qu'il peut utiliser pour construire et évaluer de nombreuses hypothèses de traduction partielles avant de parvenir à une hypothèse de traduction complète. En conséquence, de tels modèles complexes sont typiquement uniquement utilisés en TA pour effectuer le reclassement de listes de meilleures hypothèses complètes. Bien que ceci permette dans les faits de tirer profit d'une meilleure modélisation de certains aspects des traductions, cette approche reste par nature limitée : en effet, les listes d'hypothèses reclassées ne représentent qu'une infime partie de l'espace de recherche du décodeur, contiennent des hypothèses peu diversifiées, et ont été obtenues à l'aide de modèles dont la nature peut être très différente des modèles complexes utilisés en reclassement. Nous formulons donc l'hypothèse que de telles listes d'hypothèses de traduction sont mal adaptées afin de faire s'exprimer au mieux les modèles complexes utilisés. Les travaux que nous présentons dans cette thèse ont pour objectif de permettre une meilleure exploitation d'informations riches pour l'amélioration des traductions obtenues à l'aide de systèmes de TA statistique. Notre première contribution s'articule autour d'un système de réécriture guidé par des informations riches. Des réécritures successives, appliquées aux meilleures hypothèses de traduction obtenues avec un système de reclassement ayant accès aux mêmes informations riches, permettent à notre système d'améliorer la qualité de la traduction. L'originalité de notre seconde contribution consiste à faire une construction de listes d'hypothèses par passes multiples qui exploitent des informations dérivées de l'évaluation des hypothèses de traduction produites antérieurement à l'aide de notre ensemble d'informations riches. Notre système produit ainsi des listes d'hypothèses plus diversifiées et de meilleure qualité, qui s'avèrent donc plus intéressantes pour un reclassement fondé sur des informations riches. De surcroît, notre système de réécriture précédent permet d'améliorer les hypothèses produites par cette deuxième approche à passes multiples. Notre troisième contribution repose sur la simulation d'un type d'information idéalisé parfait qui permet de déterminer quelles parties d'une hypothèse de traduction sont correctes. Cette idéalisation nous permet d'apporter une indication de la meilleure performance atteignable avec les approches introduites précédemment si les informations riches disponibles décrivaient parfaitement ce qui constitue une bonne traduction. Cette approche est en outre présentée sous la forme d'une traduction interactive, baptisée « pré-post-édition » , qui serait réduite à sa forme la plus simple : un système de TA statistique produit sa meilleure hypothèse de traduction, puis un humain apporte la connaissance des parties qui sont correctes, et cette information est exploitée au cours d'une nouvelle recherche pour identifier une meilleure traduction.
La RAP non native souffre encore d'une chute significative de précision. Cette dégradation est due aux erreurs d'accent et de prononciation que produisent les locuteurs non natifs. Les recherches que nous avons entreprises ont pour but d'atténuer l'impact des accents non natifs sur les performances des systèmes de RAP. Nous avons proposé une nouvelle approche pour la modélisation de prononciation non native permettant de prendre en compte plusieurs accents étrangers. Cette approche automatique utilise un corpus de parole non native et deux ensembles de modèles acoustiques : le premier ensemble représente l'accent canonique de la langue cible et le deuxième représente l'accent étranger. Les modèles acoustiques du premier ensemble sont modifiés par l'ajout de nouveaux chemins d'états HMM. Nous avons proposé une nouvelle approche pour la détection de la langue maternelle basée sur la détection de séquences discriminantes de phonèmes. Par ailleurs, nous avons proposé une approche de modélisation de prononciation non native multi-accent permettant de prendre en compte plusieurs accents étrangers simultanément. Nous avons conçu une approche automatique pour la detection des contraintes graphémiques et leur prise en compte pour l'approche de RAP non native. En outre, Nous avons proposé trois nouvelles approches efficaces dont le but est l'accélération du calcul de vraisemblance sans dégradation de la précision.
Les systèmes de recommandation actuels ont besoin de recommander des objets pertinents aux utilisateurs (exploitation), mais pour cela ils doivent pouvoir également obtenir continuellement de nouvelles informations sur les objets et les utilisateurs encore peu connus (exploration). Il s'agit du dilemme exploration/exploitation. Un tel environnement s'inscrit dans le cadre de ce que l'on appelle " apprentissage par renforcement ". Dans la littérature statistique, les stratégies de bandit sont connues pour offrir des solutions à ce dilemme. Les contributions de cette thèse multidisciplinaire adaptent ces stratégies pour appréhender certaines problématiques des systèmes de recommandation, telles que la recommandation de plusieurs objets simultanément, la prise en compte du vieillissement de la popularité d'un objet ou encore la recommandation en temps réel.
Dans de nombreux domaines tels que l'apprentissage statistique, la recherche opérationnelle ou encore la conception de circuits, une tâche est modélisée par un jeu de paramètres que l'on cherche à optimiser pour prendre la meilleure décision possible. Mathématiquement, le problème revient à minimiser une fonction de l'objectif recherché par des algorithmes itératifs. Le développement de ces derniers dépend alors de la géométrie de la fonction ou de la structure du problème. Dans une première partie, cette thèse étudie comment l'acuité d'une fonction autour de ses minima peut être exploitée par le redémarrage d'algorithmes classiques. Les schémas optimaux sont présentés pour des problèmes convexes généraux. Ils nécessitent cependant une description complète de la fonction, ce qui est rarement disponible. Des stratégies adaptatives sont donc développées et prouvées être quasi-optimales. Une analyse spécifique est ensuite conduite pour les problèmes parcimonieux qui cherchent des représentations compressées des variables du problème. Leur géométrie conique sous-jacente, qui décrit l'acuité de la fonction de l'objectif, se révèle contrôler à la fois la performance statistique du problème et l'efficacité des procédures d'optimisation par une seule quantité. Une seconde partie est dédiée aux problèmes d' Ceux-ci effectuent une analyse prédictive de données à l'aide d'un large nombre d' Des méthodes algorithmiques systématiques sont développées en analysant la géométrie induite par une partition des données. Une analyse théorique est finalement conduite lorsque les variables sont groupées par analogie avec les méthodes parcimonieuses.
Ce mémoire traite des modèles génératifs profonds appliqués à la génération automatique de musique symbolique. Nous nous attacherons tout particulièrement à concevoir des modèles génératifs interactifs, c'est-à-dire des modèles instaurant un dialogue entre un compositeur humain et la machine au cours du processus créatif. En effet, les récentes avancées en intelligence artificielle permettent maintenant de concevoir de puissants modèles génératifs capables de générer du contenu musical sans intervention humaine. En revanche, la conception d'assistants puissants, flexibles et expressifs destinés aux créateurs de contenus musicaux me semble pleine de sens. Que ce soit dans un but pédagogique ou afin de stimuler la créativité artistique, le développement et le potentiel de ces nouveaux outils de composition assistée par ordinateur sont prometteurs. Dans ce manuscrit, je propose plusieurs nouvelles architectures remettant l'humain au centre de la création musicale. Les modèles proposés ont en commun la nécessité de permettre à un opérateur de contrôler les contenus générés. Afin de rendre cette interaction aisée, des interfaces utilisateurs ont été développées ;  les possibilités de contrôle se manifestent sous des aspects variés et laissent entrevoir de nouveaux paradigmes compositionnels.
Le Web est une source proliférante d'objets multimédia, décrits dans différentes langues naturelles. Afin d'utiliser les techniques du Web sémantique pour la recherche de tels objets (images, vidéos, etc.), nous proposons une méthode d'extraction de contenu dans des collections de textes multilingues, paramétrée par une ou plusieurs ontologies. Le processus d'extraction est utilisé pour indexer les objets multimédia à partir de leur contenu textuel, ainsi que pour construire des requêtes formelles à partir d'énoncés spontanés. Il est basé sur une annotation interlingue des textes, conservant les ambiguïtés de segmentation et la polysémie dans des graphes. Cette première étape permet l'utilisation de processus de désambiguïsation “factorisés” au niveau d'un lexique pivot (de lexèmes interlingues). Le passage d'une ontologie en paramètre du système se fait en l'alignant de façon automatique avec le lexique interlingue. Il est ainsi possible d'utiliser des ontologies qui n'ont pas été conçues pour une utilisation multilingue, et aussi d'ajouter ou d'étendre l'ensemble des langues et leurs couvertures lexicales sans modifier les ontologies. Un démonstrateur pour la recherche multilingue d'images, développé pour le projet ANR OMNIA, a permis de concrétiser les approches proposées. Le passage à l'échelle et la qualité des annotations produites ont ainsi pu être évalués.
De par leur grand nombre et leur sévérité, les maladies rares (MR) constituent un enjeu de santé majeur. Des bases de données de référence, comme Orphanet et Orphadata, répertorient les informations disponibles à propos de ces maladies. Cependant, il est difficile pour ces bases de données de proposer un contenu complet et à jour par rapport à ce qui est disponible dans la littérature. En effet, des millions de publications scientifiques sur ces maladies sont disponibles et leur nombre augmente de façon continue. Cette thèse s'intéresse à l'extraction de connaissances à partir de textes et propose d'utiliser les résultats de l'extraction pour enrichir une ontologie de domaine. Nous avons étudié trois directions de recherche : (1) l'extraction de connaissances à partir de textes, et en particulier l'extraction de relations maladie-phénotype (M-P) ; (2) l'identification d'entité nommées complexes, en particulier de phénotypes de MR ; et (3) l'enrichissement d'une ontologie en considérant les connaissances extraites à partir de texte. Tout d'abord, nous avons fouillé une collection de résumés d'articles scientifiques représentés sous la forme graphes pour un extraire des connaissances sur les MR. Nous nous sommes concentrés sur la complétion de la description des MR, en extrayant les relations M-P. Cette trouve des applications dans la mise à jour des bases de données de MR telles que Orphanet. Pour cela, nous avons développé un système appelé SPARE* qui extrait les relations M-P à partir des résumés PubMed, où les phénotypes et les MR sont annotés au préalable par un système de reconnaissance des entités nommées. SPARE* suit une approche hybride qui combine une méthode basée sur des patrons syntaxique, appelée SPARE, et une méthode d'apprentissage automatique (les machines à vecteurs de support ou SVM). SPARE* bénéficié à la fois de la précision relativement bonne de SPARE et du bon rappel des SVM. Ensuite, SPARE* a été utilisé pour identifier des phénotypes candidats à partir de textes. Pour cela, nous avons sélectionné des patrons syntaxiques qui sont spécifiques aux relations M-P uniquement. Ensuite, ces patrons sont relaxés au niveau de leur contrainte sur le phénotype pour permettre l'identification de phénotypes candidats qui peuvent ne pas être références dans les bases de données ou les ontologies. Ces candidats sont vérifiés et validés par une comparaison avec les classes de phénotypes définies dans une ontologie de domaine comme HPO. Cette comparaison repose sur une modèle sémantique et un ensemble de règles de mises en correspondance définies manuellement pour cartographier un phénotype candidate extrait de texte avec une classe de l'ontologie. Nos expériences illustrent la capacité de SPARE* à des phénotypes de MR déjà répertoriés ou complètement inédits. Nous avons appliqué SPARE* à un ensemble de résumés PubMed pour extraire les phénotypes associés à des MR, puis avons mis ces phénotypes en correspondance avec ceux déjà répertoriés dans l'encyclopédie Orphanet et dans Orphadata ;  Enfin, nous avons appliqué les structures de patrons pour classer les MR et enrichir une ontologie préexistante. Tout d'abord, nous avons utilisé SPARE* pour compléter les descriptions en terme de phénotypes de MR disponibles dans Orphadata. Ensuite, nous proposons de compter et grouper les MR au regard de leur description phénotypique, et ce en utilisant les structures de patron.
Cette thèse présente une nouvelle approche de la détection automatique des frontières prosodiques et de la structure prosodique en français, basée sur une représentation théorique hiérarchique de cette structure. Nous avons utilisé une théorie descriptive du système prosodique du français pour créer un modèle prosodique linguistique adapté au traitement automatique de la parole spontanée. Ce modèle permet de détecter de façon automatique les frontières des groupes prosodiques et de les regrouper dans une structure hiérarchique. La structure prosodique de chaque énoncé est ainsi représentée sous forme d'un arbre prosodique. Nous avons démontré que ce modèle représentation était adapté pour le traitement automatique de la parole spontanée en français. La segmentation prosodique ainsi obtenue a été comparée à la segmentation prosodique manuelle. La pertinence de la structure prosodique a été également vérifiée manuellement. Nous avons appliqué notre modèle à différents types de données de parole continue spontanée avec différents types de segmentations phonétiques et lexicales : segmentation manuelle ainsi que différentes segmentations automatiques, et notamment aux données segmentées par le système de reconnaissance automatique de la parole. L'utilisation de cette segmentation a fourni une performance satisfaisante. Nous avons également établi une corrélation entre le niveau du noeud dominant dans l'arbre prosodique et la fiabilité de la détection de la frontière correspondante. Ainsi, il est envisageable d'enrichir la détection de frontières prosodiques en attribuant une mesure de confiance à la frontière en fonction de son niveau dans l'arbre prosodique.
Avec le développement des dispositifs de capture et d'Internet, les gens accèdent à un nombre croissant d'images. L'évaluation de l'esthétique visuelle a des applications importantes dans plusieurs domaines, de la récupération d'image et de la recommandation à l'amélioration. L'évaluation de la qualité esthétique de l'image vise à déterminer la beauté d'une image pour les observateurs humains. De nombreux problèmes dans ce domaine ne sont pas bien étudiés, y compris la subjectivité de l'évaluation de la qualité esthétique, l'explication de l'esthétique et la collecte de données annotées par l'homme. La prédiction conventionnelle de la qualité esthétique des images vise à prédire le score moyen ou la classe esthétique d'une image. Cependant, la prédiction esthétique est intrinsèquement subjective, et des images avec des scores / classe esthétiques moyens similaires peuvent afficher des niveaux de consensus très différents par les évaluateurs humains. Des travaux récents ont traité de la subjectivité esthétique en prédisant la distribution des scores humains, mais la prédiction de la distribution n'est pas directement interprétable en termes de subjectivité et pourrait être sous-optimale par rapport à l'estimation directe des descripteurs de subjectivité calculés à partir des scores de vérité terrain. De plus, les étiquettes des ensembles de données existants sont souvent bruyantes, incomplètes ou ne permettent pas des tâches plus sophistiquées telles que comprendre pourquoi une image est belle ou non pour un observateur humain. Dans cette thèse, nous proposons tout d'abord plusieurs mesures de la subjectivité, allant de simples mesures statistiques telles que l'écart type des scores, aux descripteurs nouvellement proposés inspirés de la théorie de l'information. Nous évaluons les performances de prédiction de ces mesures lorsqu'elles sont calculées à partir de distributions de scores prédites et lorsqu'elles sont directement apprises à partir de données de vérité terrain. Nous constatons que cette dernière stratégie donne en général de meilleurs résultats. Nous utilisons également la subjectivité pour améliorer la prédiction des scores esthétiques, montrant que les mesures de subjectivité inspirées de la théorie de l'information fonctionnent mieux que les mesures statistiques. Ensuite, nous proposons un ensemble de données EVA (Explainable Visual Aesthetics), qui contient 4070 images avec au moins 30 votes par image. EVA a été collecté en utilisant une approche plus disciplinée inspirée des meilleures pratiques d'évaluation de la qualité. Il offre également des caractéristiques supplémentaires, telles que le degré de difficulté à évaluer le score esthétique, l'évaluation de 4 attributs esthétiques complémentaires, ainsi que l'importance relative de chaque attribut pour se forger une opinion esthétique. L'ensemble de données accessible au public devrait contribuer aux recherches futures sur la compréhension et la prédiction de l'esthétique de la qualité visuelle. De plus, nous avons étudié l'explicabilité de l'évaluation de la qualité esthétique de l'image. Une analyse statistique sur EVA démontre que les attributs collectés et l'importance relative peuvent être combinés linéairement pour expliquer efficacement les scores d'opinion moyenne esthétique globale. Nous avons trouvé que la subjectivité a une corrélation limitée avec la difficulté personnelle moyenne dans l'évaluation esthétique, et la région du sujet, le niveau photographique et l'âge affectent de manière significative l'évaluation esthétique de l'utilisateur.
Ce travail vise à améliorer la compréhension des signaux sismiques dérivés des fonctions de corrélation inter-récepteur du bruit sismique, ce qui est critique pour une imagerie fiable de la Terre profonde basée sur le bruit. La thèse comprend sept chapitres. Le chapitre 1 introduit les connaissances de base sur le bruit sismique, de la terminologie à ses origines diverses. Le chapitre 2 fournit une vue d'ensemble de la littérature sur l'historique et le développement de la méthode récente de corrélation de bruit, et passe en revue diverses techniques pour le prétraitement des données de bruit sismique et le post-traitement des fonctions de corrélation de bruit. Des méthodes de traitement du bruit basées sur les statistiques et un schéma modifié pour calculer la fonction de corrélation sont développés dans ce chapitre. Le chapitre 3 propose plusieurs techniques basées sur la transformée de Radon pour mesurer les lenteurs des champs d'ondes corrélés et analyser en termes de phases sismiques les signaux dérivés du bruit. Le chapitre 6 discute des conditions dans lesquelles apparaissent des phases sans correspondance dans la réponse physique de la Terre qui peuvent fausser les analyses des structures profondes basées sur le bruit.. Le dernier chapitre fournit un résumé sur les contributions de cette thèse et une perspective de plusieurs travaux soit en cours soit envisagés pour le futur.
La théorie du grounding (ancrage) de Clark \&amp ;  Schaefer (1989) suggère que les participants à un dialogue cherchent à atteindre la compréhension mutuelle en produisant des preuves de leur compréhension et peut alors permettre d'améliorer la robustesse des systèmes. Les modélisations informatiques de l'ancrage qui visent à résoudre ce problème font l'objet de plusieurs simplifications ou sont trop complexes à mettre en oeuvre. Cette modélisation a été implémentée et adjointe à un système d'interprétation symbolique classique. L'évaluation du système a été réalisée par simulation sur corpus en générant des dialogues d'ancrage de manière artificielle entre deux instances du système. Les résultats obtenus à l'issue de l'évaluation montrent un gain significatif de compréhension et valident en cela l'approche générale.
Dans cette thèse, nous présentons une étude sur la visualisation des résultats Web d'images sur les dispositifs nomades. Nos principales conclusions ont été inspirées par les avancées récentes dans deux principaux domaines de recherche – la recherche d'information et le traitement automatique du langage naturel. Tout d'abord, nous avons examiné différents sujets tels que le regroupement des résultats Web, les interfaces mobiles, la fouille des intentions sur une requête, pour n'en nommer que quelques-uns. Ensuite, nous nous sommes concentré sur les mesures d'association lexical, les métriques de similarité d'ordre élevé, etc. Notamment afin de valider notre hypothèse, nous avons réalisé différentes expériences avec des jeux de données spécifiques de la tâche. De nombreuses caractéristiques sont évaluées dans les solutions proposées. Premièrement, la qualité de regroupement en utilisant à la fois des métriques d'évaluation classiques, mais aussi des métriques plus récentes. Deuxièmement, la qualité de l'étiquetage de chaque groupe de documents est évaluée pour s'assurer au maximum que toutes les intentions des requêtes sont couvertes. Finalement, nous évaluons l'effort de l'utilisateur à explorer les images dans une interface basée sur l'utilisation des galeries présentées sur des dispositifs nomades. Un chapitre entier est consacré à chacun de ces trois aspects dans lesquels les jeux de données-certains d'entre eux construits pour évaluer des caractéristiques spécifiques-sont présentés. Comme résultats de cette thèse, nous sommes développés : deux algorithmes adaptés aux caractéristiques du problème, deux jeux de données pour les tâches respectives et un outil d'évaluation pour le regroupement des résultats d'une requête (SRC pour les sigles en anglais). Concernant les algorithmes, Dual C-means est notre principal contribution. Il peut être vu comme une généralisation de notre algorithme développé précédemment, l'AGK-means. Les deux sont basés sur des mesures d'association lexical à partir des résultats Web. Un nouveau jeu de données pour l'évaluation complète d'algorithmes SRC est élaboré et présenté. De même, un nouvel ensemble de données sur les images Web est développé et utilisé avec une nouvelle métrique à fin d'évaluer l'effort fait pour les utilisateurs lors qu'ils explorent un ensemble d'images. Enfin, nous avons développé un outil d'évaluation pour le problème SRC, dans lequel nous avons mis en place plusieurs mesures classiques et récentes utilisées en SRC. Nos conclusions sont tirées compte tenu des nombreux facteurs qui ont été discutés dans cette thèse. Cependant, motivés par nos conclusions, des études supplémentaires pourraient être développés. Celles-ci sont discutées à la fin de ce manuscrit et notre résultats préliminaires suggère que l'association de plusieurs sources d'information améliore déjà la qualité du regroupement.
Dans l'expression de la relation concrète, ils sont généralement classés dans la catégorie des locatifs topologiques qui expriment une relation d'inclusion ou de contact entre deux entités. Dans les emplois métaphoriques, ils peuvent tous exprimer un cadre, bien que chaque locatif ait des emplois spécifiques propres. Ces quatre locatifs sont parfois interchangeables aussi bien pour les emplois concrets que métaphoriques. Notre étude comparative en chinois contemporain est basée sur un corpus de textes de différents styles : écrit, oral et écrit avec des caractéristiques de l'oral. Les statistiques d'emploi des locatifs recueillies montrent l'importance des styles de textes dans le choix des locatifs. La pragmatique et des contextes sémantiques sont aussi des facteurs déterminants. L'analyse diachronique réalisée sur un corpus de 26 documents représentatifs des périodes archaïque, médiévale et pré-moderne indique l'évolution de chaque locatif et la tendance d'
Cette thèse étudie le rôle des anticipations dans les cycles économiques en analysant trois types d'anticipations différentes. Dans un premier temps, je me concentre sur une explication théorique des cycles économiques générée par des changements d'anticipations qui se révèlent auto-réalisatrices. Ce chapitre contribue à améliorer un puzzle provenant de la littérature sunspot, soutenant ainsi une interprétation des cycles économiques basée sur les prophéties auto-réalisatrices. Dans un deuxième temps, j'analyse empiriquement comment les annonces de la banque centrale se propagent à l'économie via la modification des croyances des acteurs du marché. Ce chapitre montre que des annonces crédibles sur les futures politiques monétaires non conventionnelles peuvent être utilisées comme un instrument de coordination des anticipations dans un contexte de crise de la dette souveraine. Dans un troisième temps, je m'intéresse à un concept plus large d'anticipations et étudie le pouvoir prédictif du climat politique sur la tarification du risque souverain. Ce chapitre montre que le climat politique apporte un pouvoir prédictif supplémentaire aux spreads des obligations d'Etat, au-delà des déterminants traditionnels. Différentes méthodologies sont utilisées dans cette thèse, notamment des analyses théoriques et empiriques, du web scraping ainsi que des méthodes d'apprentissage automatique et d'analyse textuelle. Par ailleurs, j'exploite dans cette thèse des données innovantes provenant du réseau social Twitter. Tous mes résultats transmettent le même message : les anticipations comptent, tant pour la recherche en économie que pour l'élaboration de politiques économiques.
Dans ces travaux de thèse nous proposons de tirer parti de la théorie des ensembles flous afin d'améliorer les interactions entre les systèmes de bases de données et les utilisateurs. Les mécanismes coopératifs visent à aider les utilisateurs à mieux interagir avec les SGBD. Ces mécanismes doivent faire preuve de robustesse : ils doivent toujours pouvoir proposer des réponses à l'utilisateur. Empty set (0,00 sec) est un exemple typique de réponse qu'il serait désirable d'éradiquer. Le caractère informatif des explications de réponses est parfois plus important que les réponses elles-mêmes : ce peut être le cas avec les réponses vides et pléthoriques par exemple, d'où l'intérêt de mécanismes coopératifs robustes, capables à la fois de contribuer à l'explication ainsi qu'à l'amélioration des résultats. Par ailleurs, l'utilisation de termes de la langue naturelle pour décrire les données permet de garantir l'interprétabilité des explications fournies. Permettre à l'utilisateur d'utiliser des mots de son propre vocabulaire contribue à la personnalisation des explications et améliore l'interprétabilité. Ces axes définissent des approches coopératives où l'intérêt des explications est de permettre à l'utilisateur de comprendre comment sont calculés les résultats proposés dans un effort de transparence. Le caractère informatif des explications apporte une valeur ajoutée aux résultats bruts, et forme une réponse coopérative.
Les procédés de traduction constituent un sujet important pour les traductologues et les linguistes. Face à un certain mot ou segment difficile à traduire, les traducteurs humains doivent appliquer les solutions particulières au lieu de la traduction littérale, telles que l'équivalence idiomatique, la généralisation, la particularisation, la modulation syntaxique ou sémantique, etc. En revanche, ce sujet a reçu peu d'attention dans le domaine du Traitement Automatique des Langues (TAL). Notre problématique de recherche se décline en deux questions : est-il possible de reconnaître automatiquement les procédés de traduction ? Pour vérifier notre hypothèse, nous avons annoté un corpus parallèle anglais-français en procédés de traduction, tout en établissant un guide d'annotation. Notre typologie de procédés est proposée en nous appuyant sur des typologies précédentes, et est adaptée à notre corpus. L'accord inter-annotateur (0,67) est significatif mais dépasse peu le seuil d'un accord fort (0,61), ce qui reflète la difficulté de la tâche d'annotation. En nous fondant sur des exemples annotés, nous avons ensuite travaillé sur la classification automatique des procédés de traduction. Même si le jeu de données est limité, les résultats expérimentaux valident notre hypothèse de travail concernant la possibilité de reconnaître les différents procédés de traduction. Nous avons aussi montré que l'ajout des traits sensibles au contexte est pertinent pour améliorer la classification automatique. En vue de tester la généricité de notre typologie de procédés de traduction et du guide d'annotation, nos études sur l'annotation manuelle ont été étendues au couple de langues anglais-chinois. Ce couple de langues partagent beaucoup moins de points communs par rapport au couple anglais Le guide d'annotation a été adapté et enrichi. Dans le but de valider l'intérêt de ces études, nous avons conçu un outil d'aide à la compréhension écrite pour les apprenants de français langue étrangère. Une expérience sur la compréhension écrite avec des étudiants chinois confirme notre hypothèse de travail et permet de modéliser l'outil. D'autres perspectives de recherche incluent l'aide à la construction de ressource de paraphrases, l'évaluation de l'alignement automatique de mots et l'évaluation de la qualité de la traduction automatique.
De nos jours, l'IA repose en grande partie sur l'utilisation de données de grande taille et sur des méthodes d'apprentissage machine améliorées qui consistent à développer des algorithmes de classification et d'inférence en tirant parti de grands ensembles de données de grande taille. Ces grandes dimensions induisent de nombreux phénomènes contre-intuitifs, conduisant généralement à une mauvaise compréhension du comportement de nombreux algorithmes d'apprentissage machine souvent conçus avec des intuitions de petites dimensions de données. En tirant parti du cadre multidimensionnel (plutôt que d'en souffrir), la théorie des matrices aléatoires (RMT) est capable de prédire les performances de nombreux algorithmes non linéaires aussi complexes que certains réseaux de neurones aléatoires, ainsi que de nombreuses méthodes du noyau telles que les SVM, la classification semi-supervisée, l'analyse en composantes principales ou le regroupement spectral. Pour caractériser théoriquement les performances de ces algorithmes, le modèle de données sous-jacent est souvent un modèle de mélange gaussien (MMG) qui semble être une hypothèse forte étant donné la structure complexe des données réelles (par exemple, des images). En outre, la performance des algorithmes d'apprentissage automatique dépend du choix de la représentation des données (ou des caractéristiques) sur lesquelles ils sont appliqués. Encore une fois, considérer les représentations de données comme des vecteurs gaussiens semble être une hypothèse assez restrictive. S'appuyant sur la théorie des matrices aléatoires, cette thèse vise à aller au-delà de la simple hypothèse du MMG, en étudiant les outils classiques d'apprentissage machine sous l'hypothèse de vecteurs aléatoires concentrés qui généralisent les vecteurs Gaussiens. Cette hypothèse est particulièrement motivée par l'observation que l'on peut utiliser des modèles génératifs (par exemple, les GAN) pour concevoir des structures de données complexes et réalistes telles que des images, grâce à des transformations Lipschitzienne de vecteurs gaussiens. Cela suggère notamment que l'hypothèse de concentration sur les données mentionnée ci-dessus est un modèle approprié pour les données réelles et qui est tout aussi mathématiquement accessible que les MMG. Par conséquent, nous démontrons à travers cette thèse, en nous appuyant sur les GANs, l'intérêt de considérer le cadre des vecteurs concentrés comme un modèle pour les données réelles. En particulier, nous étudions le comportement des matrices de Gram aléatoires qui apparaissent au cœur de divers modèles linéaires, des matrices à noyau qui apparaissent dans les méthodes à noyau et également des méthodes de classification qui reposent sur une solution implicite (par exemple, la couche de Softmax dans les réseaux de neurones), avec des données aléatoires supposées concentrées. L'analyse de ces méthodes pour des données concentrées donne le résultat surprenant qu'elles ont asymptotiquement le même comportement que pour les données de MMG. Ce résultat suggère fortement l'aspect d'universalité des grands classificateurs d'apprentissage machine par rapport à la distribution sous-jacente des données.
La reconnaissance des entités nommées (EN) reste un problème pour de nombreuses applications de Traitement Automatique des Langues Naturelles. Dans cette version minimale, Nemesis atteint environ 90% en précision et 80% en rappel. Pour augmenter le rappel, nous proposons différents modules optionnels (examen d'un contexte encore plus large et utilisation du Web comme source de nouveaux contextes) et une étude pour la réalisation d'un module de désambiguïsation et d'apprentissage de règles.
La lecture est un des savoirs fondamentaux acquis à l'école primaire. D'abord centré sur le décodage dans les premières années, l'enseignement se focalise ensuite essentiellement sur la compréhension et l'automatisation de la lecture. Mais a lecture fluente du lecteur expert ne se résume pas seulement à une vitesse de lecture élevée, elle se caractérise également par une prosodie adaptée au texte, notamment en termes de phrasé et d'expressivité. En omettant l'aspect prosodique de la fluence, on tend à entretenir une confusion entre fluence et vitesse de lecture. Les dimensions prosodiques de la fluence ont longtemps été négligées dans l'étude du développement de la lecture. Seules quelques études récentes se sont intéressées à leur développement dans diverses langues, mais il n'en existe aucune en français. La dimension prosodique de la fluence mérite d'être plus largement étudiée, notamment chez l'apprenti lecteur, et c'est l'objectif de cette thèse. Nous abordons ces questions en utilisant trois types de mesures complémentaires de la prosodie : une mesure subjective à l'aide d'une échelle multidimensionnelle et deux mesures objectives que sont les marqueurs acoustiques de phrasé et d'expressivité et une méthode d'évaluation automatique basé sur l'analyse des signaux de parole. Dans un premier temps, nous avons abordé le développement des compétences prosodiques en lecture d'un point de vue subjectif, en adaptant une échelle anglophone d'évaluation de la prosodie au français. L'étude des corrélations entre scores subjectifs et marqueurs acoustiques a permis de mettre en évidence les marqueurs affectant le jugement de l'auditeur. Les données acoustiques ont ensuite été utilisées pour mieux comprendre le lien entre prosodie et compréhension. Ces données ont permis de proposer un modèle de croissance pour chaque dimension de la fluence et étudier les liens de causalité entre automaticité, prosodie et compréhension. Les connaissances acquises dans cette thèse sur le développement de la prosodie en lecture et son lien avec la compréhension écrite chez l'enfant français nous permettent de proposer de nouveaux outils d'évaluation de la fluence incluant la prosodie, et d'envisager le développement d'outils d'entrainement à la lecture prosodique. Ces outils offrent de nouvelles perspectives pour l'enseignement de la lecture ainsi que pour le diagnostic etla prise en charge des enfants en difficulté d'apprentissage de la lecture.
Les problématiques abordées dans ma thèse sont de définir une adaptation unifiée entre la sélection des documents et les stratégies de recherche de la réponse à partir du type des documents et de celui des questions, intégrer la solution au système de Questions Nous développons et étudions une méthode basée sur une approche de Recherche d'Information pour la sélection de documents en QR. Celle-ci s'appuie sur un modèle de langue et un modèle de classification binaire de texte en catégorie pertinent ou non pertinent d'un point de vue QR. Cette méthode permet de filtrer les documents sélectionnés pour l'extraction de réponses par un système QR. Nous présentons la méthode et ses modèles, et la testons dans le cadre QR à l'aide de RITEL. L'évaluation est faite en français en contexte web sur un corpus de 500 000 pages web et de questions factuelles fournis par le programme Quaero. Celle-ci est menée soit sur des documents complets, soit sur des segments de documents. L'hypothèse suivie est que le contenu informationnel des segments est plus cohérent et facilite l'extraction de réponses. Dans le premier cas, les gains obtenus sont faibles comparés aux résultats de référence (sans filtrage). Dans le second cas, les gains sont plus élevés et confortent l'hypothèse, sans pour autant être significatifs. Une étude approfondie des liens existant entre les performances de RITEL et les paramètres de filtrage complète ces évaluations. Le système de segmentation créé pour travailler sur des segments est détaillé et évalué. Son évaluation nous sert à mesurer l'impact de la variabilité naturelle des pages web (en taille et en contenu) sur la tâche QR, en lien avec l'hypothèse précédente. En général, les résultats expérimentaux obtenus suggèrent que notre méthode aide un système QR dans sa tâche. Cependant, de nouvelles évaluations sont à mener pour rendre ces résultats significatifs, et notamment en utilisant des corpus de questions plus importants.
Les projets en humanités numériques utilisent de plus en plus des méthodes de collaboration axées sur le public, telles que le crowdsourcing pour atteindre les objectifs de recherche, de conservation et d'édition scientifique en sciences humaines et sociales. Par exemple, le crowdsourcing représente une opportunité pour accélérer les projets de transcription pour des communautés de chercheurs qui travaillent traditionnellement dans des circuits En outre, l'efficacité du crowdsourcing pour les humanités numériques n'est pas documenté dans la littérature. Se pose ainsi la question de savoir si le public peut produire du matériel pouvant être par la suite utilisé pour des éditions scientifiques, auxquels cas, pour quel type de projet et combien de post-traitement ou corrections seront nécessaires. Cette thèse de doctorat examinera le potentiel apport du crowdsourcing des transcriptions pour les projets d'édition scientifique en humanités numériques. Pour cela, nous allons premièrement explorer les technologies et les techniques disponibles pour produire les transcriptions sous format XML en ligne. Deuxièmement, ayant développé et testé une plateforme internet de transcription que nous présenterons, nous pourrons examiner les besoins des utilisateurs vis-à-vis des environnements de travail collaboratifs fondées sur les retours des utilisateurs et les environments de crowdsourcing industriels existants. Troisièmement, les données récoltées seront soumises à une analyse numérique qui permettra de comparer les productions des experts et celle des non-experts en s'appuyant sur les mesures de distances entre documents. Les résultats obtenus permettront de déterminer le potentiel apport du crowdsourcing pour les projets d'édition numérique scientifique. Enfin, le travail se terminera avec une discussion sur les implications des travaux actuels et présentera des opportunités pour des recherches futures sur le terrain.
Cette thèse aborde les problématiques liées à la construction de Topic Maps et à leur utilisation pour la recherche d'information dans le cadre défini par le Web sémantique (WS). Un contenu à organiser étant très souvent volumineux et sujet à enrichissement perpétuel, il est pratiquement impossible d'envisager une création et gestion d'une Topic Map, le décrivant, de façon manuelle. Plusieurs travaux de recherche ont concerné la construction de Topic Maps à partir de documents textuels [Ellouze et al. 2008a]. Cependant, aucune d'elles ne permet de traiter un contenu multilingue. De plus, bien que les Topic Maps soient, par définition, orientées utilisation (recherche d'information), peu d'entre elles prennent en compte les requêtes des utilisateurs. Notre approche est incrémentale et évolutive, elle est basée sur un processus automatisé, qui prend en compte des documents multilingues et l'évolution de la Topic Map selon le changement du contenu en entrée et l'usage de la Topic Map. Pour enrichir la Topic Map, nous nous basons sur deux ontologies générales et nous explorons toutes les questions potentielles relatives aux documents sources. Dans ACTOM, en plus des liens d'occurrences reliant un Topic à ses ressources, nous catégorisons les liens en deux catégories : (a) les liens ontologiques et (b) les liens d'usage. Nous proposons également d'étendre le modèle des Topic Maps défini par l'ISO en rajoutant aux caractéristiques d'un Topic des méta-propriétés servant à mesurer la pertinence des Topics plus précisément pour l'évaluation de la qualité et l'élagage dynamique de la Topic Map.
Cette thèse s'intéresse à la description didactique de la variation linguistique dans une approche à contraintes. Cette approche nous permet d'envisager la variation dans une perspective fonctionnelle plutôt que normative et de décrire les variantes « non-standard » comme plus ou moins appropriées à une tâche plutôt que comme des déviations de la norme. Pour illustrer notre approche, nous l'appliquons à la description de la dislocation clitique à gauche en français. Nous suggérons que ces contraintes sont toutes de nature pragmatique et que leur interaction influe sur l'emploi de la dislocation clitique à gauche en français. Ces hypothèses sont testées empiriquement via une étude de corpus, une série de test de jugements d'acceptabilité et un test de Matched Guise. De plus, nous postulons que l'apprentissage des contraintes pragmatiques en langue étrangère dépend de leur enseignement explicite et l'exposition répétée à la construction dans des contextes acceptables. Suivant l'hypothèse de l'interface dynamique (Ellis, 2005), nous suggérons que l'apprentissage explicite des contraintes de la dislocation clitique à gauche dans le contexte de la classe de langue facilite leur apprentissage implicite lorsque les apprenant se retrouve dans une situation de communication avec des locuteurs natifs du français. Le rôle de l'exposition est exploré empiriquement en répliquant un test de jugements d'acceptabilité et le test de Matched Guise avec des participants non-natifs. Les contraintes stylistiques sont décrites via la compétence de savoir-être et les registres sociolinguistiques (CECR, 2001).
À partir d'une conception dynamique et plurielle des émotions, cette thèse propose une réflexion sur l'inscription discursive des affects dans les interactions numériques de type WhatsApp. Elle s'inscrit dans le domaine de l'analyse du discours en interaction (Kerbrat-Orecchioni, 2005) en faisant dialoguer le cadre du discours numérique (Paveau, 2017) avec les propositions théoriques des sciences de l'information et la communication (Allard, 2017). Si les linguistes ont longtemps souligné l'infinité des marques langagières des émotions (Kerbrat-Orecchioni, 2000) voire même leurs hétérogénéités (Plantin, 2011 ; Micheli, 2014), ce postulat est encore plus attesté dans les écosystèmes numériques où l'expression des affects se trouve distribuée dans toute l'interface numérique intégrant indifféremment les mots et les gestes (Jeanneret et Souchier, 1999). Suite à une réflexion méthodologique autour de la constitution du corpus numérique WhatsApp, ce travail porte sur les ressources sémiotiques et discursives dans l'expression des émotions. Ces dernières se voient matérialisées dans de nouvelles formes verbales comme les émotimots lol et mdr, entendus comme un sociolecte de l'affecte. Mais dans le cadre des messageries numériques, l'expression des émotions dépassent le verbal pour s'incarner dans des formes l'iconique marquées les photo discours, où la capture photographique se tisse avec le verbale pour co-construire le sens. À partir d'analyses qualitatives des observables prélevés, cette thèse montre comment les locuteurs et locutrices renouvellent sans cesses les formes d'expression des émotions et comment ils et elles composent avec les affordances numériques du système, pour explorer des versions inédites dans la gestion interactionnelle des affects.
Les humains et les robots travaillant en toute sécurité et en parfaite harmonie dans un environnement est l'un des objectifs futurs de la communauté robotique. Quand les humains et les robots peuvent travailler ensemble dans le même espace, toute une catégorie de tâches devient prête à l'automatisation, allant de la collaboration pour l'assemblage de pièces, à la manutention de pièces et de materiels ainsi qu'à leur livraison. Des normes existent sur la collaboration entre robots et humains, cependant elles se focalisent à limiter les distances d'approche et les forces de contact entre l'humain et le robot. Un outil clé pour la sécurité entre des robots et des humains travaillant dans un environnement inclut la reconnaissance de l'intention dans lequel le robot tente de comprendre l'intention d'un agent (l'humain) en reconnaissant tout ou partie des actions de l'agent pour l'aider à prévoir les actions futures de cet agent. Dans cette thèse, nous présentons une approche qui est capable de déduire l'intention d'un agent grâce à la reconnaissance et à la représentation des informations de l'état. Cette approche est différente des nombreuses approches présentes dans la littérature qui se concentrent principalement sur la reconnaissance de l'activité (par opposition à la reconnaissance de l'état) et qui « devinent » des raisons pour expliquer les observations.
Dans les domaines de spécialité, les applications telles que la recherche d'information ou la traduction automatique, s'appuient sur des ressources terminologiques pour prendre en compte les termes, les relations sémantiques ou les regroupements de termes. Pour faire face au coût de la constitution de ces ressources, des méthodes automatiques ont été proposées. Parmi celles-ci, l'analyse distributionnelle s'appuie sur la redondance d'informations se trouvant dans le contexte des termes pour établir une relation. Alors que cette hypothèse est habituellement mise en oeuvre grâce à des modèles vectoriels, ceux-ci souffrent du nombre de dimensions considérable et de la dispersion des données dans la matrice des vecteurs de contexte. En corpus de spécialité, ces informations contextuelles redondantes sont d'autant plus dispersées et plus rares que les corpus ont des tailles beaucoup plus petites. De même, les termes complexes sont généralement ignorés étant donné leur faible nombre d'occurrence. Des relations sémantiques acquises en corpus sont utilisées pour généraliser et normaliser ces contextes. Nous avons évalué la robustesse de notre méthode sur quatre corpus de tailles, de langues et de domaines différents. L'analyse des résultats montre que, tout en permettant de prendre en compte les termes complexes dans l'analyse distributionnelle, l'abstraction des contextes distributionnels permet d'obtenir des groupements sémantiques de meilleure qualité mais aussi plus cohérents et homogènes.
La Génération Automatique de Langue Naturelle vise à produire des textes dans une langue humaine à partir d'un ensemble de données non-linguistiques. La dernière sous-tâche est connue comme la tâche de Réalisation de Surface (RS). Dans ma thèse, j'étudie la tâche de RS quand les données d'entrée sont extraites de Bases de Connaissances (BC). Je présente deux nouvelles approches pour la réalisation de surface à partir de bases de connaissances : une approche supervisée et une approche faiblement supervisée. Dans l'approche supervisée, je présente une méthode basée sur des corpus pour induire une grammaire à partir d'un corpus parallèle de textes et de données. Dans l'approche faiblement supervisée, j'explore une méthode pour la réalisation de surface à partir de données extraites d'une BC qui ne requière pas de corpus parallèle. À la place, je construis un corpus de textes liés au domaine et l'utilise pour identifier les lexicalisations possibles des symboles de la BC et leurs modes de verbalisation. J'évalue les phrases générées et analyse les questions relatives à l'apprentissage à partir de corpus non-alignés. Dans chacune de ces approches, les méthodes proposées sont génériques et peuvent être facilement adaptées pour une entrée à partir d'autres ontologies
La présente recherche s'intéresse à la syntaxe des constructions exceptives (CE) et sa correspondance avec la sémantique au sein de deux langues : le français et l'arabe. Nous situerons notre analyse des marqueurs sauf, excepté, hormis, etc. dans le cadre des listes/entassements paradigmatiques, constructions dans lesquelles deux éléments occupent la même position syntaxique et dont le cas le plus connu est la coordination. Cette analyse s'éloigne de celle généralement associée à ces marqueurs dans les grammaires et les dictionnaires français qui les traitent comme des prépositions. Nos analyses nous amènent à considérer les marqueurs ʾillā, ġayr et siwā en arabe comme des conjonctions de coordination. Ces items, comme leurs homologues français, mettent en relation deux éléments X et Y où X à droite du marqueur et Y à gauche forment des listes/entassements paradigmatiques au sens où ils partagent la même fonction syntaxique dans l'énoncé. Nous analysons les unités ʿadā (mā-ʿadā), ẖalā (mā-ẖalā), ḥāšā (mā-ḥāšā) comme des verbes. Ces verbes introduisent une proposition qui entretient une relation de parataxe avec la proposition précédente. Nous considérons, enfin, les unités bistiṯnāʾi et biẖilāfi comme des locutions prépositives introduisant une séquence qui entretient une relation de subordination avec la proposition principale.
Les informations échangées dans les textes des courriels sont généralement concernées par des événements complexes ou des processus métier dans lesquels les entités qui échangent des courriels collaborent pour atteindre les objectifs finaux des processus. Ainsi, le flux d'informations dans les courriels envoyés et reçus constitue une partie essentielle, les activités métier de l'entreprise. L'extraction d'informations sur les processus métier à partir des courriels peut aider à améliorer la gestion des courriels pour les utilisateurs. Il peut également être utilisé pour trouver des réponses riches à plusieurs questions analytiques sur les employés et les organisations. Aucun des travaux précédents n'a résolu le problème de la transformation automatique des journaux de courriels en journaux d'événements pour éventuellement en déduire les processus métier non documentés. Dans ce but, nous travaillons dans cette thèse sur un framework qui induit des informations de processus métier à partir d'emails. Nous introduisons des approches qui contribuent à ce qui suit : (1) découvrir pour chaque courriel le sujet de processus qui le concerne, (2) découvrir l'instance de processus métier à laquelle appartient chaque courriel, (3) extraire les activités de processus métier des courriels et associer ces activités aux métadonnées qui les décrivent, (4) améliorer la performance de la découverte des instances de processus métier et des activités métier en utilisant la relation entre ces deux problèmes, et enfin (5) estimer au préalable la date/heure réelle d'un activité métier. En utilisant les résultats des approches mentionnées, un journal d'événements est généré qui peut être utilisé pour déduire les modèles de processus métier d'un journal de courriels. L'efficacité de toutes les approches ci-dessus est prouvée par l'application de plusieurs expériences sur l'ensemble de données de courriel ouvert d'
Cette thèse s'inscrit dans le cadre de l'émergence de l'apprentissage profond et aborde la compréhension de la parole assimilée à l'extraction et à la représentation automatique du sens contenu dans les mots d'une phrase parlée. Nous étudions une tâche d'étiquetage en concepts sémantiques dans un contexte de dialogue oral évaluée sur le corpus français MEDIA. Depuis une dizaine d'années, les modèles neuronaux prennent l'ascendant dans de nombreuses tâches de traitement du langage naturel grâce à des avancées algorithmiques ou à la mise à disposition d'outils de calcul puissants comme les processeurs graphiques. De nombreux obstacles rendent la compréhension complexe, comme l'interprétation difficile des transcriptions automatiques de la parole étant donné que de nombreuses erreurs sont introduites par le processus de reconnaissance automatique en amont du module de compréhension. Nous présentons un état de l'art décrivant la compréhension de la parole puis les méthodes d'apprentissage automatique supervisé pour la résoudre en commençant par des systèmes classiques pour finir avec des techniques d'apprentissage profond. Les contributions sont ensuite exposées suivant trois axes. Puis nous abordons la gestion des erreurs de reconnaissance automatique et des solutions pour limiter leur impact sur nos performances. Enfin, nous envisageons une désambiguïsation de la tâche de compréhension permettant de rendre notre système plus performant.
Des items ciblant les CP ont été créés pour élaborer une tâche de réception sur la base d'énoncés en LSF filmés et une tâche de production à partir d'images. Une seconde tâche de production a également été proposée pour évaluer les compétences narratives et les constructions prédicatives en situation de récit.
En séparant le matériel sur lequel fonctionnent les fonctions/services réseau et le logiciel qui réalise et contrôle de ces fonctions/services, les réseaux logiciels (ou SDN, Software Les paradigmes SDN et NFV introduisent plus de flexibilité et permettent un meilleur contrôle, ainsi les technologies associées devraient dominer une grande partie du marché du réseautage dans les prochaines années (estimé à 3,68 milliards USD en 2017 et prévu par certains pour atteindre 54 milliards USD d'ici 2022 à un taux de croissance annuel composé (TCAC) de 71,4%). Cependant, l'un des soucis majeurs des opérateurs à propos de Network Softwarization est la sécurité. Dans cette thèse, nous avons d'abord conçu et implémenté un framework de test de pénétration (pentesting) pour les contrôleurs SDN. Nous avons proposé un ensemble d'algorithmes pour l'empreinte digitale d'un contrôleur SDN distant, i.e. En utilisant notre framework, les opérateurs réseau peuvent évaluer la sécurité de leurs déploiements SDN (y compris Opendaylight, Floodlight et Cisco Open SDN Controller) avant de les mettre en production. En outre, sOFTDP surpasse OFDP de plusieurs ordres de grandeur en terme de performance, ce que nous avons confirmé par des tests approfondis. Le deuxième axe de notre recherche dans cette thèse est la gestion intelligente et automatique des réseaux logiciels. Inspirés par les avancées récentes dans les techniques d'apprentissage automatique, notamment les réseaux de neurones profonds (DNN, Deep Neural Networks), nous avons créé un moteur d'ingénierie de trafic pour le SDN appelé NeuRoute, entièrement basé sur les DNN. Les contrôleurs SDN/OpenFlow actuels utilisent par défaut un routage basé sur l'algorithme de Dijkstra pour les chemins les plus courts mais fournissent des API pour développer des applications de routage personnalisées.
Après Septembre 2008, du fait du gel du marché interbancaire, d'un manque de liquidité, d'une perte de confiance et des difficultés des institutions financières, la transmission de la politique monétaire au sein de la zone euro a été sévèrement altérée. La Banque Centrale Européenne (BCE) a donc dû avoir recours à des politiques monétaires non-conventionnelles. En considérant, au sein de la zone euro, les contraintes imposées à la banque centrale et la fragmentation des marchés financiers, l'objectif de cette thèse empirique est d'évaluer les canaux de transmission des politiques monétaires conventionnelles et non-conventionnelles de la BCE. Les comportements de prêts des banques étant liés à leurs coûts de financement, le premier essai se focalise sur le canal de transmission des prêts bancaires. Il étudie l'évolution des activités de prêts syndiqués d'institutions financières européennes et leur réaction aux politiques de la BCE. La communication de la banque centrale revêt une importance toute particulière dans une union monétaire. Les deuxième et troisième essais se concentrent sur le canal des signaux. Le deuxième essai étudie sur la communication durant les conférences de presse mensuelles ainsi que ses effets sur la prévisibilité des décisions de politique monétaire et sur les rendements et la volatilité des marchés financiers. Le dernier essai se focalise sur l'utilisation du guidage des taux d'intérêt futurs, une communication non-conventionnelle informant les marchés du niveau futur des taux d'intérêt de court-terme. Il étudie l'efficacité de cette annonce et sa capacité à influencer les prévisions de taux d'intérêt faites par les acteurs de marché.
La tendance actuelle de l'intelligence artificielle (IA) est de s'appuyer fortement sur des systèmes capables d'apprendre à partir d'exemples, tels que les modèles d'apprentissage profond ou 'Deep Learning'(DL), une incarnation moderne des réseaux de neurones artificiels. Bien que de nombreuses applications aient été mises sur le marché ces dernières années (y compris les voitures autonomes, les assistants automatisés, les services de réservation et les chatbots, les améliorations des moteurs de recherche, les recommandations et la publicité, et les applications de soins de santé, pour n'en nommer que quelques-uns), les modèles DL sont notoirement difficiles à déployer dans de nouvelles applications. En particulier, ils nécessitent un grand nombre d'exemples d'apprentissage, des heures d'entrainement sur GPU et des ingénieurs hautement qualifiés pour ajuster manuellement leurs architectures. Cette thèse contribuera à réduire la barrière d'entrée dans l'utilisation des modèles DL pour de nouvelles applications, une étape vers la 'démocratisation de l'IA'. L'angle pris sera de développer de nouvelles approches d'apprentissage par transfer ou 'Transfert Learning'(TL), basées sur des architectures DL modulaires. L'apprentissage par transfert englobe toutes les techniques pour accélérer l'apprentissage en capitalisant sur l'exposition à des tâches similaires précédentes. Par exemple, l'utilisation de réseaux pré-entrainés est une tactique clé de TL utilisée par les gagnants du récent défi AutoDL https : //autodl.chalearn.org/. Le doctorant fera avancer la notion de réutilisation des réseaux pré-formés en tout ou en partie (modularité). Plusieurs questions importantes se posent dans ce contexte. D'un point de vue technique, les limites actuelles du pré-apprentissage sont les suivantes :  (T1) Dans de nombreux domaines, il n'y a pas de réseaux pré-entrainés disponibles, en raison du manque d'ensembles de données massifs dans les domaines connexes ;  (T2) Les nouvelles architectures de réseaux telles que les « réseaux de neurones graphiques » (GNN) ne se prêtent pas facilement au pré-apprentissage ;  (T3) Outre le simple recyclage de la dernière couche et le réglage fin des couches internes, les moyens de réutiliser les réseaux pré-entrainés dans de nouveaux contextes sont sous-développés. Ces trois problèmes offrent des opportunités de recherche stimulantes pour utiliser efficacement les connaissances antérieures, des simulateurs de données et / ou l'augmentation des données et développer de nouveaux algorithmes et architectures qui apprennent de manière modulaire et réutilisable. Du point de vue de la recherche fondamentale, la modularité et l'héritage des modules d'apprentissage pré-entrainés dans les systèmes d'apprentissage d'inspiration biologique est un sujet brûlant en IA. Les questions sans réponse comprennent :  (F1) La modularité du cerveau augmente-t-elle son efficacité ou s'agit-il d'un héritage d'évolution qui ne joue aucun rôle particulier ;  (F2) De même, dans quel contexte et comment la modularité aide-t-elle dans les systèmes artificiels (par exemple, pour mettre en œuvre des invariances, pour aider à transférer l'apprentissage, etc.) ;  (F3) La spécialisation du module gêne-t-elle ou aide-t-elle à généraliser de nouvelles modalités de données (par exemple, de nouvelles données de capteur), et si oui, comment ? Dans ce contexte, le doctorant étudiera une nouvelle approche de transfert d'apprentissage que nous appelons « Apprentissage Modulaire Profond » . Le candidat abordera le problème de l'apprentissage de grands réseaux de neurones artificiels dont les architectures sont modulaires et dont les modules sont éventuellement réutilisables. Une méthode possible pour aborder le problème sera d'utiliser des algorithmes d'optimisation à plusieurs niveaux, abordant l'optimisation du système global (atteindre un objectif de niveau supérieur) sous la contrainte que les modules atteignent un objectif de niveau inférieur (réutilisabilité). Un objectif scientifique sera de remettre en cause l'hypothèse selon laquelle la modularité est essentielle pour les systèmes d'apprentissage, en ce qu'elle accélère l'apprentissage en rendant possible une forme efficace d'apprentissage par transfert, une fonctionnalité centrale de l'IA. Plusieurs principes / conjectures / hypothèses peuvent guider cette recherche, notamment :  (P1) Le principe de la parcimonie ou du 'rasoir d'Ockham'incarné dans la théorie de l'apprentissage moderne sous le nom de 'régularisation', qui, selon les mots du profane, déclare que 'de deux théories équivalentes puissantes pour reproduire observations, il faut préférer la plus simple '' ; en effet, les architectures modulaires partageant des sous-modules identiques ont moins de paramètres ajustables et peuvent donc être considérées comme moins complexes que par exemple les réseaux entièrement connectés. (P2) L'hypothèse de l'innéité : les capacités de résolution de tâches sont une combinaison de compétences innées et acquises. Est-ce une caractéristique des systèmes intelligents de s'appuyer davantage sur des compétences acquises telles que la langue plutôt que d'en hériter ? Est-il vrai que la langue peut être complètement apprise 'à partir de zéro' ? (P3) Induction, déduction, conceptualisation et causalité : les systèmes d'apprentissage intelligents reposent-ils sur la modularité de la conceptualisation, de l'acquisition du langage et de l'inférence causale ? Pour mettre un cadre en pratique à cette recherche, l'étudiant choisira des applications dans des domaines tels que la biomédecine (par exemple, toxicité moléculaire ou efficacité), l'écologie, l'économétrie, la reconnaissance vocale, le traitement du langage naturel, le traitement d'images ou de vidéos, etc. Voir par exemple http : //snap.stanford.edu/data/.
Le résumé automatique de document repose généralement sur des méthodes par extraction qui sélectionnent dans le texte des passages pertinents et les juxtaposent pour former un résumé. Ces méthodes sont peu adaptées à la problématique du résumé de conversations orales de part la nature spontanée de celles-ci et l'importance de l'interaction entre les locuteurs. En ne sélectionnant que certains passages, les résumés par extraction ne contiennent qu'un verbatim de ce qui a été dit, et non pas une description synthétique de ce qui s'est passé lors de la conversation. C'est pourquoi des approche abstractives basées sur la détection de concepts permettrait de palier ces difficultés. Puis nous étudions l'intérêt de l'utilisation de modèles sémantiques dans la tâche de résumé automatique. Enfin nous proposons une méthode de résumé à base de patrons. Les méthodes de résumé par remplissage de patrons ont montré leur intérêt dans des domaines spécifiques pour le résumé automatique de texte. Dans notre cas, elles permettent de traiter du problème de différence de genre entre les données source (transcriptions de conversations) et la forme des résumés à générer (narration synthétique). Toutefois, elles nécessitent l'écriture manuelle de patrons de résumés et l'annotation manuelle de quantités de données source en concepts à détecter pour remplir ces patrons.
Ce mémoire concerne la reconnaissance automatique des Actes de Dialogues (ADs) en tchéque et en français. Les ADs sont des unités au niveau de la phrase qui représentent des différents états d'un dialogue, comme par exemple les questions, les affirmations, les hésitations, etc. Les résultats expérimentaux confirment que chaque type d'attributs apporte des informations pertinentes et complémentaires. Les méthodes proposées qui exploitent la position des mots sont particulièrement intéresantes, parce qu'elles utilisent une information globale sur la structure de la phrase. Une autre contribution conséquente, relative au manque de corpus étiquettés dans le domaine de la reconnaissance automatique des ADs, concerne le développement et l'étude de méthodes d' Les résultats expérimentaux démontrent que la méthode proposée est une approche intéressante pour la création de nouveaux corpus d'actes de dialogues à moindre coût.
La désambiguïsation lexicale (DL) et la traduction automatique (TA) sont deux tâches centrales parmi les plus anciennes du traitement automatique des langues (TAL). Bien qu'ayant une origine commune, la DL ayant été conçue initialement comme un problème fondamental à résoudre pour la TA, les deux tâches ont par la suite évolué très indépendamment. En effet, d'un côté, la TA a su s'affranchir d'une désambiguïsation explicite des termes grâce à des modèles statistiques et neuronaux entraînés sur de grandes quantités de corpus parallèles, et de l'autre, la DL, qui est confrontée à certaines limitations comme le manque de ressources unifiées et un champs d'application encore restreint, reste un défi majeur pour permettre une meilleure compréhension de la langue en général. Aujourd'hui, dans un contexte où les méthodes à base de réseaux de neurones et les représentations vectorielles des mots prennent de plus en plus d'ampleur dans la recherche en TAL, les nouvelles architectures neuronales et les nouveaux modèles de langue pré-entraînés offrent non seulement de nouvelles possibilités pour développer des systèmes de DL et de TA plus performants, mais aussi une opportunité de réunir les deux tâches à travers des modèles neuronaux joints, permettant de faciliter l'étude de leurs interactions. Dans cette thèse, nos contributions porteront dans un premier temps sur l'amélioration des systèmes de DL, par l'unification des données nécessaires à leur mise en oeuvre, la conception de nouvelles architectures neuronales et le développement d'approches originales pour l'amélioration de la couverture et des performances de ces systèmes. Ensuite, nous développerons et comparerons différentes approches pour l'intégration de nos systèmes de DL état de l'art et des modèles de langue, dans des systèmes de TA, pour l'amélioration générale de leur performance. Enfin, nous présenterons une nouvelle architecture pour l'apprentissage d'un modèle neuronal joint pour la DL et la TA, s'appuyant sur nos meilleurs systèmes neuronaux pour l'une et l'autre tâche.
Cette thèse se propose d'étudier la grammaticalisation, processus d'évolution linguistique par lequel les éléments fonctionnels de la langue se trouvent remplacés au cours du temps par des mots ou des constructions de contenu, c'est-à-dire servant à désigner des entités plus concrètes. La grammaticalisation est donc un cas particulier de remplacement sémantique. Or, la langue faisant l'objet d'un consensus social bien établi, il semble que le changement sémantique s'effectue à contre-courant de la bonne efficacité de la communication ;  pourtant, il est attesté dans toutes les langues, toutes les époques et, comme le montre la grammaticalisation, toutes les catégories linguistiques. Ces profils de fréquence sont extraits de la base de données de Frantext, qui permet de couvrir une période de sept siècles. Les distributions statistiques des observables décrivant ces deux phénomènes sont obtenues et quantifiées. Un modèle de marche aléatoire est ensuite proposé reproduisant ces deux phénomènes. La latence s'y trouve expliquée comme un phénomène critique, au voisinage d'une bifurcation point-col. Une extension de ce modèle articulant l'organisation du réseau sémantique et les formes possibles de l'évolution est ensuite discutée.
L'objectif de cette thèse est d'étudier la fonction argumentale afin d'élaborer une méthode pour l'acquisition automatique des termes d'une manière pertinente et efficace. Nous avons d'abord discuté du profilage du corpus et de la constitution du corpus web pour le traitement automatique des langues. Ensuite, trois méthodes ont été développées en nous fondant sur les caractéristiques morphologiques des unités lexicales et la relation d'appropriation entre les prédicats appropriés et leurs arguments. La méthode distributionnelle a pour objet d'exploiter les structures prédicat-argument pour repérer les arguments de la classe sémantique donnée. La méthode morphosémantique est développée en se fondant sur les structures internes des unités lexicales en vue d'étendre la liste de termes. La méthode combinatoire qui associe les deux premières approches permet d'améliorer la pertinence du résultat. Finalement, nous avons développé une réflexion sur la particularité de la langue, la classe sémantique, la langue de spécialité et la récursivité de la langue dans la perspective du traitement automatique des langues.
En revanche, lorsqu'il est question de traiter les langues peu dotées, on est souvent confronté au manque d'outils et de données. Dans cette thèse, on s'intéresse à certaines formes vernaculaires de l'arabe utilisées au Maghreb. Ces formes sont connues sous le terme de dialecte que l'on peut classer dans la catégorie des langues peu dotées. Exceptés des textes brutes extraits généralement des réseaux sociaux, il existe très peu de ressources permettant de traiter les dialectes arabes. Ces derniers, comparativement aux autres langues peu dotées possèdent plusieurs spécificités qui les rendent plus difficile à traiter. Nous pouvons citer notamment l'absence de règles d'écriture de ces dialectes, ce qui conduit les usagers à écrire le dialecte sans suivre des règles précises, par conséquent un même mot peut avoir plusieurs graphies. Les mots en arabe dialectal peuvent s'écrire en utilisant le script arabe et/ou le script latin (écriture dite arabizi). Pour les dialectes arabes du Maghreb, ils sont particulièrement influencés par des langues étrangères comme le français et l'anglais. En plus de l'emprunt de mots de ces langues, un autre phénomène est à prendre en compte en traitement automatique des dialectes. Il s'agit du problème connu sous le terme de code-switching. Ce phénomène est connu en linguistique sous le terme de diglossie. Cela a pour conséquence de laisser libre cours à l'utilisateur qui peut écrire en plusieurs langues dans une même phrase. Il peut ainsi commencer en dialecte arabe et au milieu de la phrase, il peut "switcher" vers le français, l'anglais ou l'arabe standard. En plus de cela, il existe plusieurs dialectes dans un même pays et a fortiori plusieurs dialectes différents dans le monde arabe. Il est donc clair que les outils NLP classiques développés pour l'arabe standard ne peuvent être utilisés directement pour traiter les dialectes. L'objectif principal de ce travail consiste à proposer des méthodes permettant la construction automatique de ressources pour les dialectes arabes en général et les dialectes du Maghreb en particulier. Cela représente notre contribution à l'effort fourni par la communauté travaillant sur le traitement automatique des dialectes arabes. Nous avons ainsi produit des méthodes permettant de construire des corpus comparables, des ressources lexicales contenant les différentes formes d'une entrée et leur polarité. Par ailleurs, nous avons développé des méthodes pour le traitement de l'arabe standard sur des données de Twitter et également sur les transcriptions provenant d'un système de reconnaissance automatique de la parole opérant sur des vidéos en arabe extraites de chaînes de télévisions arabes telles que Al Jazeera, France24, Euronews, etc. Nous avons ainsi comparé les opinions des transcriptions automatiques provenant de sources vidéos multilingues différentes et portant sur le même sujet en développant une méthode fondée sur la théorie linguistique dite Appraisal.
Le raisonnement spatial et temporel qualitatif est un domaine principal d'études de l'intelligence artificielle et, en particulier, du domaine de la représentation des connaissances, qui traite des concepts cognitifs fondamentaux de l'espace et du temps de manière abstraite. Dans notre thèse, nous nous focalisons sur les formalismes du domaine du raisonnement spatial et temporel qualitatif représentant les informations par des contraintes et apportons des contributions sur plusieurs aspects. En particulier, étant donnée des bases de connaissances d'informations qualitatives sur l'espace ou le temps, nous définissons des nouvelles conditions de consistance locale et des techniques associées afin de résoudre efficacement les problèmes fondamentaux se posant. En outre, nous enrichissons le domaine des formalismes spatio-temporels par des contributions concernant une logique spatio-temporelle combinant la logique temporelle propositionnelle (PTL) avec un langage de contraintes qualitatives spatiales et une étude de la problématique consistant à gérer une séquence temporelle de configurations spatiales qualitatives devant satisfaire des contraintes de transition.
Nous employons une méthode différentielle, non-compositionnelle et endogène. Notre but est de maximiser la factorisation afin de permettre le traitement de nouvelles langues avec un coût marginal minimal. Pour ce faire nous exploitons les propriétés du genre journalistique et tout particulièrement la répétition de certains éléments à des positions clés. Notre grain d'analyse est le grain caractère de façon à être indépendant des contraintes posées par le concept de mot graphique dans un grand nombre de langues. Nous aboutissons à l'implantation du système DAnIEL (Data Analysis for Information Extraction in any Language). DAnIEL opère une classification des documents selon qu'ils décrivent ou non des faits épidémiologiques et les regroupe par faits épidémio-logiques sous la forme de paires maladie-lieu. DAnIEL est rapide et efficace en comparaison des systèmes existants. Il nécessite des ressources légères pour fonctionner, facilitant ainsi le traitement de nouvelles langues.
L'attention est le processus qui consiste à filtrer les informations utiles à l'activité, de celles qui lui sont inutiles. La réalité augmentée (RA) de surlignage guide ce processus de sélection de l'information, en mettant en valeur certains éléments par rapport à d'autres. Telle qu'envisagée actuellement pour la conduite automobile, la RA de surlignage met en évidence des éléments liés à l'activité générale de conduite (e.g. panneaux en cas de mauvaise visibilité, direction à emprunter), mais indépendamment des manœuvres. Or, la littérature sur l'attention visuelle en activité nous montre que les parcours oculaires sont très spécifiques aux buts et sous-buts immédiats. Une RA qui ne respecte pas cette priorisation « naturelle » du traitement de l'information risque donc de perturber la prise d'informations. Le premier objectif de cette recherche est de déterminer dans quelle mesure l'allocation de l'attention visuelle en conduite automobile se concentre sur les informations liées à la manœuvre. Le second objectif est d'étudier l'impact de la RA sur cette allocation de l'attention. Nous avons mis en place trois expérimentations dans lesquelles les participants visualisaient des scènes statiques et dynamique de conduite automobile, et devaient décider s'ils pouvaient réaliser une manœuvre. Nous avons analysé les variations d'allocation de l'attention visuelle selon les manœuvres notifiées et les conditions de RA à l'aide d'enregistrements oculométriques. Nos résultats montrent que l'attention visuelle est fortement allouée aux indices permettant la prise de décision, mais qu'elle ne néglige pas les indices permettant la compréhension générale de la scène. La RA optimise l'attention visuelle lorsqu'elle met en évidence des indices liés à la manœuvre, mais elle perturbe l'attention visuelle dans les autres conditions. Ces résultats permettent d'identifier et de caractériser différents risques inhérents à la RA de surlignage, et de discuter des pistes de conception pour les prendre en compte.
Les Maladies Neurodégénératives Rares à Expression Motrice (MNDREM) sont un groupe très hétérogène de pathologies neurodégénératives de physiopathologie acquise ou innée, pouvant se déclarer à tout âge. Elles impactent de façon ciblée, les structures neurologiques centrales ou périphériques impliquées dans l'activité motrice. Ces structures sont fonctionnellement et topographiquement proches des centres de cognition. C'est ainsi que dans certaines conditions, la neurodégénérescence des structures motrices impacte les fonctions cognitives. Elles se traduisent par des troubles de la marche et /ou de l'équilibre, une diminution ou une absence de mouvement, une atteinte cognitive variée pouvant aller jusqu'à la démence. Les MNDREM sont sporadiques ou familiales. La littérature scientifique indique un nombre important de variants génétiques pathogènes responsables des MNDREM, ce qui aide la classification mais la complexifie également puisqu'on constate qu'un même gène peut être impliqué dans plusieurs pathologies et une pathologie peut être causée par différents gènes. Par conséquent, dans les MNDREM, on observe des symptomatologies atypiques, des chevauchements ou « overlap » cliniques des maladies alléliques. Aux Antilles françaises, les données de la littérature en matière de MNDREM sont limitées mais riches de certaines observations confortant les atypies comme par exemple le syndrome parkinsonien atypique des Caraïbes. Sur le plan génétique, les investigations diagnostiques sont restreintes aux filières médicales basées en métropole et sur la comparaison de données génomiques de populations le plus souvent caucasiennes. L'état des lieux des MNDREM en Martinique, et le travail expérimental d'analyse de données de NGS, sont les premiers travaux de ce type dans le domaine et dans notre région. Elle n'avait presque jamais été utilisée dans l'analyse des MNDREM. Nous l'avons employée car c'est une méthode de choix de recherche à la fois de variants simples et de variants de structures nouveaux. Les résultats montrent une rentabilité diagnostic de 58% puisque nous avons identifié un variant probablement pathogène chez 7 patients sur 12 testés. Nous avons trouvé ces variants dans des gènes connus de MNDREM parfois décrits dans des populations asiatiques, mais aussi, d'ascendance africaine ou caucasienne. Certains de ces gènes peuvent être impliqués dans plusieurs symptomatologies, confortant le constat de chevauchement clinique dans ces maladies. Ce travail jette les bases épidémiologiques des MNDREM aux Antilles et propose un socle de registre en la matière. Sur le plan expérimental, il a permis de proposer une étiologie moleculaire impliquant des variants préalablement décrit ou nouveaux. Il est également une preuve de concept quant aux moyens bioinformatiques d'analyse de données de NGS en Martinique. Il ouvre la voie vers d'autres travaux du même genre, susceptibles de dresser les spécificités géniques des MNDREM de notre région et étoffer les données de la littérature scientifiques et médicales.
La thèse consiste en l'apprentissage d'une tâche complexe de robotique de manipulation en utilisant très peu d'aprioris. Plus précisément, la tâche apprise consiste à atteindre un objet avec un robot série. L'objectif est de réaliser cet apprentissage sans paramètres de calibrage des caméras, modèles géométriques directs, descripteurs faits à la main ou des démonstrations d'expert. L'apprentissage par renforcement profond est une classe d'algorithmes particulièrement intéressante dans cette optique. En effet, l'apprentissage par renforcement permet d'apprendre une compétence sensori-motrice en se passant de modèles dynamiques. Par ailleurs, l'apprentissage profond permet de se passer de descripteurs faits à la main pour la représentation d'état. Cependant, spécifier les objectifs sans supervision humaine est un défi important. Certaines solutions consistent à utiliser des signaux de récompense informatifs ou des démonstrations d'experts pour guider le robot vers les solutions. D'autres consistent à décomposer l'apprentissage. Par exemple, l'apprentissage "petit à petit" ou "du simple au compliqué" peut être utilisé. Cependant, cette stratégie nécessite la connaissance de l'objectif en termes d'état. D'autres approches utilisant plusieurs robots en parallèle peuvent également être utilisés mais nécessite du matériel coûteux. Ainsi, nous décomposons la tâche d'atteinte en 3 sous tâches. La première tâche consiste à apprendre à fixer un objet avec un système de deux caméras pour le localiser dans l'espace. Cette tâche est apprise avec de l'apprentissage par renforcement profond et un signal de récompense faiblement supervisé. Pour la tâche suivante, deux compétences sont apprises en parallèle : la fixation d'effecteur et une fonction de coordination main-oeil. Le but de cette tâche est d'être capable de localiser l'effecteur du robot à partir des coordonnées articulaires. En plus de la tâche d'atteinte, un predicteur d'atteignabilité d'objet est appris. La principale contribution de ces travaux est l'apprentissage d'une tâche de robotique complexe en n'utilisant que très peu de supervision.
Cette thèse traite de l'apprentissage automatique pour la classification de données. Nous proposons une nouvelle mesure d'incertitude qui permet de caractériser l'importance des données et qui améliore les performances de l'apprentissage actif par rapport aux mesures existantes. Cette mesure détermine le plus petit poids nécessaire à associer à une nouvelle donnée Les méthodes existantes d'apprentissage actif à partir de flux de données, sont initialisées avec quelques données étiquetées qui couvrent toutes les classes possibles. Cependant, dans de nombreuses applications, la nature évolutive du flux fait que de nouvelles classes peuvent apparaître à tout moment. Nous proposons une méthode efficace de détection active de nouvelles classes dans un flux de données multi-classes. Cette méthode détermine de façon incrémentale une zone couverte par les classes connues, et détecte les données qui sont extérieures à cette zone et proches entre elles, comme étant de nouvelles classes. Enfin, il est souvent difficile d'obtenir un étiquetage totalement fiable car l'opérateur humain est sujet à des erreurs d'étiquetage qui réduisent les performances du classifieur appris. Cette problématique a été résolue par l'introduction d'une mesure qui reflète le degré de désaccord entre la classe donnée manuellement et la classe prédite et une nouvelle mesure d'"informativité" permettant d'exprimer la nécessité pour une donnée mal étiquetée d'être réétiquetée par un opérateur alternatif
Les systèmes de recommandation se sont imposés comme étant des outils indispensables face à une quantité de données qui ne cesse chaque jour de croître depuis l'avènement d'Internet. Leur objectif est de proposer aux utilisateurs des items susceptibles de les intéresser sans que ces derniers n'aient besoin d'agir pour les obtenir. Après s'être majoritairement focalisés sur la précision de la prédiction d'intérêt, ces systèmes ont évolué pour prendre en compte d'autres critères dans leur processus de recommandation, tels que les facteurs humains inhérents à la prise de décision, afin d'améliorer la qualité et l'utilité des recommandations. Cependant, la prise en compte de certains facteurs humains tels que la diversité et le contexte demeure critiquable. Alors que le contexte des utilisateurs est inféré sur la base d'informations collectées à l'insu de leur vie privée, la prise en compte de la diversité est quant à elle réduite à une dimension qu'un système se doit de maximiser. Partant du postulat inverse selon lequel l'analyse de l'évolution de la diversité au cours du temps permet de définir le contexte de l'utilisateur, nous proposons dans ce manuscrit une nouvelle approche de modélisation contextuelle basée sur la diversité. En effet, nous soutenons qu'une variation de diversité remarquable peut être la conséquence d'un changement de contexte et qu'il faut alors adapter la stratégie de recommandation en conséquence. Nous présentons la première approche de la littérature permettant de modéliser en temps réel l'évolution de la diversité, ainsi qu'une nouvelle famille de contextes dits implicites n'exploitant aucune donnée sensible. La possibilité de remplacer les contextes traditionnels (explicites) par les contextes implicites est confirmée de plusieurs manières. Premièrement, nous démontrons sur deux corpus issus d'applications réelles qu'il existe un fort recouvrement entre les changements de contextes explicites et les changements de contextes implicites. Deuxièmement, une étude utilisateur impliquant de nombreux participants nous permet de démontrer l'existence de liens entre les contextes explicites et les caractéristiques des items consultés dans ces derniers. Fort de ces constats et du potentiel offert par nos modèles, nous présentons également plusieurs approches de recommandation et de prise en compte des besoins des utilisateurs
Ces travaux de recherche visent à développer des méthodes d'apprentissage automatique pour l'analyse de l'électroencéphalogramme (EEG) continu. L'EEG continu est une modalité avantageuse pour l'évaluation fonctionnelle des états cérébraux en réanimation ou pour d'autres applications. Les sous-parties de ce travail s'articulent autour de l'évaluation pronostique du coma post-anoxique, choisie comme application pilote. Un petit nombre d'enregistrement longue durée a été réalisé, et des enregistrements existants ont été récupérés au CHU Grenoble. Nous commençons par valider l'efficacité des réseaux de neurones profonds pour l'analyse EEG d'échantillons bruts. Nous choisissons à cet effet de travailler sur la classification de stades de sommeil. Nous utilisons un réseau de neurones convolutionnel adapté pour l'EEG que nous entrainons et évaluons sur le jeu de données SHHS (Sleep Heart Health Study). Cela constitue le premier system neuronal à cette échelle (5000 patients) pour l'analyse du sommeil. Les performances de classification atteignent ou dépassent l'état de l'art. En utilisation réelle, pour la plupart des applications cliniques le défi principal est le manque d'annotations adéquates sur les patterns EEG ou sur de court segments de données (et la difficulté d'en établir). Les annotations disponibles sont généralement haut niveau (par exemple, le devenir clinique) est sont donc peu nombreuses. Nous recherchons comment apprendre des représentations compactes de séquences EEG de façon non-supervisée/semi-supervisée. Le domaine de l'apprentissage non supervisé est encore jeune. Pour se comparer aux travaux existants nous commençons avec des données de type image, et investiguons l'utilisation de réseaux adversaires génératifs (GANs) pour l'apprentissage adversaire non-supervisé de représentations. La qualité et la stabilité de différentes variantes sont évaluées. Nous appliquons ensuite un GAN de Wasserstein avec pénalité sur les gradients à la génération de séquences EEG. Le système, entrainé sur des séquences mono-piste de patients en coma post anoxique, est capable de générer des séquences réalistes. Nous développons et discutons aussi des idées originales pour l'apprentissage de représentations en alignant des distributions dans l'espace de sortie du réseau représentatif. Pour finir, les signaux EEG multipistes ont des spécificités qu'il est souhaitable de prendre en compte dans les architectures de caractérisation. Chaque échantillon d'EEG est un mélange instantané des activités d'un certain nombre de sources. Partant de ce constat nous proposons un système d'analyse composé d'un sous-système d'analyse spatiale suivi d'un sous-système d'analyse temporelle. Le sous-système d'analyse spatiale est une extension de méthodes de séparation de sources construite à l'aide de couches neuronales avec des poids adaptatifs pour la recombinaison des pistes, c'est à dire que ces poids ne sont pas appris mais dépendent de caractéristiques du signal d'entrée. Nous montrons que cette architecture peut apprendre à réaliser une analyse en composantes indépendantes, si elle est entrainée sur une mesure de non-gaussianité. Pour l'analyse temporelle, des réseaux convolutionnels classiques utilisés séparément sur les pistes recombinées peuvent être utilisés.
Les concepts de découverte et d'extraction de connaissances ainsi que d'inférencesont abordés sous différents angles au sein de la littérature scientifique. En effet, de nombreux domaines s'y intéressent allant de la recherche d'information, à l'implication textuelle en passant par les modèles d'enrichissement automatique des bases de connaissances. Ces concepts suscitent de plus en plus d'intérêt à la fois dans le monde académique et industriel favorisant le développement de nouvelles méthodes. Cette thèse propose une approche automatisée pour l'inférence et l'évaluation de connaissances basée sur l'analyse de relations extraites automatiquement à partir de textes. L'originalité de cette approche repose sur la définition d'un cadre tenant compte (i) de l'incertitude linguistique et de sa détection dans le langage naturel réalisée au travers d'une méthode d'apprentissage tenant compte d'une représentation vectorielle spécifique des phrases, (ii) d'une structuration des objets étudiés (e.g. syntagmes nominaux) sous la forme d'un ordre partiel tenant compte à la fois des implications syntaxiques et d'une connaissance a priori formalisée dans un modèle de connaissances de type taxonomique (iii) d'une évaluation des relations extraites et inférées grâce à des modèles de sélection exploitant une organisation hiérarchique des relations considérées. Cette organisation hiérarchique permet de distinguer différents critères en mettant en oeuvre des règles de propagation de l'information permettant ainsi d'évaluer la croyance qu'on peut accorder à une relation en tenant compte de l'incertitude linguistique véhiculée. Bien qu'a portée plus large, notre approche est ici illustrée et évaluée au travers de la définition d'un système de réponse à un questionnaire, généré de manière automatique, exploitant des textes issus du Web. Nous montrons notamment le gain informationnel apporté par la connaissance a priori, l'impact des modèles de sélection établis et le rôle joué par l'incertitude linguistique au sein d'une telle chaîne de traitement. Les travaux sur la détection de l'incertitude linguistique et la mise en place de la chaîne de traitement ont été validés par plusieurs publications et communications nationales et internationales.
Ce travail se situe dans le contexte de la recherche d'information (RI) utilisant des techniques d'intelligence artificielle (IA) telles que l'apprentissage profond (DL). Il s'intéresse à des tâches nécessitant l'appariement de textes, telles que la recherche ad-hoc, le domaine du questions-réponses et l'identification des paraphrases. L'objectif de cette thèse est de proposer de nouveaux modèles, utilisant les méthodes de DL, pour construire des modèles d'appariement basés sur la sémantique de textes, et permettant de pallier les problèmes de l'inadéquation du vocabulaire relatifs aux représentations par sac de mots, ou bag of words (BoW), utilisées dans les modèles classiques de RI. En effet, les méthodes classiques de comparaison de textes sont basées sur la représentation BoW qui considère un texte donné comme un ensemble de mots indépendants. Le processus d'appariement de deux séquences de texte repose sur l'appariement exact entre les mots. La principale limite de cette approche est l'inadéquation du vocabulaire. Ce problème apparaît lorsque les séquences de texte à apparier n'utilisent pas le même vocabulaire, même si leurs sujets sont liés. Par exemple, la requête peut contenir plusieurs mots qui ne sont pas nécessairement utilisés dans les documents de la collection, notamment dans les documents pertinents. Les représentations BoW ignorent plusieurs aspects, tels que la structure du texte et le contexte des mots. Ces caractéristiques sont très importantes et permettent de différencier deux textes utilisant les mêmes mots et dont les informations exprimées sont différentes. Un autre problème dans l'appariement de texte est lié à la longueur des documents. Les parties pertinentes peuvent être réparties de manières différentes dans les documents d'une collection. Ceci est d'autant vrai dans les documents volumineux qui ont tendance à couvrir un grand nombre de sujets et à inclure un vocabulaire variable. Un document long pourrait ainsi comporter plusieurs passages pertinents qu'un modèle d'appariement doit capturer. Contrairement aux documents longs, les documents courts sont susceptibles de concerner un sujet spécifique et ont tendance à contenir un vocabulaire plus restreint. L'évaluation de leur pertinence est en principe plus simple que celle des documents plus longs. Dans cette thèse, nous avons proposé différentes contributions répondant chacune à l'un des problèmes susmentionnés. Tout d'abord, afin de résoudre le problème d'inadéquation du vocabulaire, nous avons utilisé des représentations distribuées des mots (plongement lexical) pour permettre un appariement basé sur la sémantique entre les différents mots. Ces représentations ont été utilisées dans des applications de RI où la similarité document-requête est calculée en comparant tous les vecteurs de termes de la requête avec tous les vecteurs de termes du document, indifféremment. Contrairement aux modèles proposés dans l'état-de-l'art, nous avons étudié l'impact des termes de la requête concernant leur présence/absence dans un document. Nous avons adopté différentes stratégies d'appariement document/requête. L'intuition est que l'absence des termes de la requête dans les documents pertinents est en soi un aspect utile à prendre en compte dans le processus de comparaison. En effet, ces termes n'apparaissent pas dans les documents de la collection pour deux raisons possibles : soit leurs synonymes ont été utilisés ; soit ils ne font pas partie du contexte des documents en questions.
Cette thèse s'inscrit dans le domaine de la synthèse de la parole à partir du texte et traite, plus précisément, de la synthèse par corpus. Nous présentons une méthode alternative de sélection de corpus : une méthode basée sur un algorithme glouton avec la divergence de Kullback-Leibler comme critère de sélection de phrases. Cette approche vise à construire un corpus dont la distribution des unités tend vers une distribution cible fixée a priori. Nous proposons également une mise à jour efficace du critère ce qui permet de diminuer significativement le temps de sélection du corpus. C'est pourquoi la seconde partie de notre travail porte sur l'utilisation de la méthode proposée dans le cadre de l'adaptation de la base acoustique réduite pour une application précise. Nous montrons que l'adaptation de la base réduite permet d'améliorer la qualité de la synthèse par rapport à celle obtenue avec des bases réduites mais non adaptées.
Un défi pour les systèmes de recherche basée sur le contenu réside dans la nécessité d'avoir une base annotée. Cette thèse propose un système d'annotation d'images interactif par le regard afin d'alléger la tâche d'annotation. Le but est de classer un petit ensemble d'images en fonction d'une catégorie cible (classification binaire) pour classer un grand ensemble d'images. Parmi les caractéristiques du regard pointées comme informatives sur l'intention des utilisateurs, nous avons élaboré un estimateur d'intention par le regard, calculable en temps réel, indépendant de l'utilisateur et de la catégorie cible. Cette annotation implicite est meilleure qu'une annotation aléatoire mais reste incertaine. Dans une deuxième partie, les images ainsi annotées sont utilisées pour classifier un plus grand ensemble d'images avec un algorithme prenant en compte l'incertitude des labels : P-SVM combinant classification et régression. Nous avons déterminé parmi différentes stratégies un critère de pertinence pour discriminer les labels les plus fiables, utilisés pour la classification, des labels les plus incertains, utilisés pour la régression. La précision du P-SVM est évaluée dans différents contextes et peut atteindre les performances d'un algorithme de classification standard entraîné avec les labels certains. Ces évaluations ont tout d'abord été menées sur un benchmark standard pour se comparer à l'état de l'art, et dans un second temps, sur une base d'images de nourriture.
La France possède une large base de données nationale regroupant les données de liquidation de l'Assurance Maladie, de mortalité et des données hospitalières : le Système National des Données de Santé (SNDS). Celui-ci couvre actuellement la quasi-totalité de la population française de la naissance (ou immigration), au décès (ou émigration), en incluant tous les remboursements de frais médicaux ou paramédicaux. En recueillant de manière systématique et prospective les dispensations médicamenteuses, les événements hospitaliers et les décès, le SNDS est doté d'un fort potentiel pour l'évaluation du médicament en vie réelle. Suite au retrait mondial du rofecoxib en 2004, de nombreuses initiatives visant au développement et à l'évaluation de méthodologies adaptées aux bases de données populationnelles pour la surveillance des risques liés à l'usage du médicament ont vu le jour, en particulier le réseau EU-ADR en Europe (Exploring and Understanding Adverse Drug Reactions by integrative mining of clinical records and biomedical knowledge) et OMOP (Observational Outcomes Partnership) aux États-Unis. Ces travaux ont démontré l'utilité des approches pharmaco-épidémiologiques pour la détection de signaux de pharmacovigilance. Cependant, le SNDS n'a jamais été testé dans cette optique. Sur les 3 approches étudiées, c'est la série de cas autocontrôlés qui a montré les meilleures performances dans UGIB et ALI avec des AUC respectifs de 0,80 et 0,94 et des MSE de 0,07 et 0,12. Pour UGIB, les performances optimales ont été observées lorsque l'ajustement tenait compte des traitements concomitants et lorsque les 30 premiers jours d'exposition au médicament d'intérêt étaient utilisés comme fenêtre de risque. Pour ALI, les performances optimales ont été également obtenues lors de l'ajustement en fonction des traitements concomitants, mais en utilisant une fenêtre de risque correspondant à l'ensemble de la période couverte par les dispensations de médicament d'intérêt. L'utilisation de médicaments témoins négatifs a montré que l'erreur systématique résultant de l'application de l'approche et des paramètres optimaux dans le SNDS semblait faible, mais que les biais protopathiques et de confusion restaient présents. Au total, ces travaux ont montré que les séries de cas autocontrôlées sont à considérer comme une approche adaptée à la détection d'alertes de pharmacovigilance associées à ALI et à UGIB dans le SNDS. Un point de vue clinique demeure toutefois nécessaire pour écarter tout risque de faux positif résultant de potentiels biais résiduels. L'application d'une telle approche à d'autres événements d'intérêt et son utilisation en routine constitueraient des progrès substantiels en matière de pharmacovigilance en France.
Les Machines de Boltzmann restreintes (RBM) sont des modèles graphiques capables d'apprendre simultanément une distribution de probabilité et une représentation des données. Malgré leur architecture relativement simple, les RBM peuvent reproduire très fidèlement des données complexes telles que la base de données de chiffres écrits à la main MNIST. Cependant, toutes les variantes de ce modèle ne sont pas aussi performantes les unes que les autres, et il n'y a pas d'explication théorique justifiant ces observations empiriques. Dans la première partie de ma thèse, nous avons cherché à comprendre comment un modèle si simple peut produire des distributions de probabilité si complexes. Nous avons pu caractériser théoriquement un régime compositionnel pour les RBM, et montré sous quelles conditions (statistique des poids, choix de la fonction de transfert) ce régime peut ou ne peut pas émerger. Les prédictions qualitatives et quantitatives de cette analyse théorique sont en accord avec les observations réalisées sur des RBM entraînées sur des données réelles. Nous avons ensuite appliqué les RBM à l'analyse et à la conception de séquences de protéines. De part leur grande taille, il est en effet très difficile de simuler physiquement les protéines, et donc de prédire leur structure et leur fonction. Il est cependant possible d'obtenir des informations sur la structure d'une protéine en étudiant la façon dont sa séquence varie selon les organismes. Par exemple, deux sites présentant des corrélations de mutations importantes sont souvent physiquement proches sur la structure. Dans le même esprit, nous avons montré sur plusieurs familles de protéines que les RBM peuvent aller au-delà de la structure, et extraire des motifs étendus d'acides aminés en coévolution qui reflètent les contraintes phylogénétiques, structurelles et fonctionnelles des protéines. De plus, on peut utiliser les RBM pour concevoir de nouvelles séquences avec des propriétés fonctionnelles putatives par recombinaison de ces motifs.
La veille en santé animale, notamment la détection précoce de l'émergence d'agents pathogènes exotiques et émergents à l'échelle mondiale, est l'un des moyens de lutte contre l'introduction de ces agents pathogènes en France. Récemment, il y a eu une réelle prise de conscience par les autorités sanitaires de l'utilité de l'information non-structurée concernant les maladies infectieuses publiée sur le Web. Toutefois, pour l'élaborer, nous l'appliquons à cinq maladies animales infectieuses exotiques : la peste porcine africaine, la fièvre aphteuse, la fièvre catarrhale ovine, la maladie du virus Schmallenberg et l'influenza aviaire. Nous démontrons que des techniques de fouille de textes, complétées par les connaissances d'experts du domaine, sont la fondation d'une veille sanitaire du Web à la fois efficace et réactive pour détecter des émergences de maladies exotiques au niveau international. Notre outil sera utilisé par le dispositif de veille sanitaire internationale en France, et facilitera la détection précoce de signaux de dangers sanitaires émergents dans les articles médias du Web.
Les résultats obtenus montrent que (1) la perception et la production des voyelles sont fortement influencées par la L1 des apprenants aussi bien au début qu'après 9 mois d'apprentissage ; (2) les performances en perception et en production des néo-apprenants dépendent plus des voyelles elles-mêmes que de leur statut en L2 par rapport à la L1 (voyelles nouvelles, similaires, ou identiques) ; (3) les entraînements phonétiques que nous avons administrés n'apportent pas de bénéfice sur la perception et la production des voyelles orales françaises par les néo-apprenants tunisien.
Aujourd'hui, un utilisateur peut interagir avec des assistants virtuels, comme Alexa, Siri ou Cortana, pour accomplir des tâches dans un environnement numérique. Dans ces systèmes, les liens entre des ordres exprimés en langage naturel et leurs réalisations concrètes sont précisées lors de la phase de conception. Une approche plus adaptative consisterait à laisser l'utilisateur donner des instructions en langage naturel ou des démonstrations lorsqu'une tâche est inconnue de l'assistant. Une solution adaptative devrait ainsi permettre à l'assistant d'agir sur un environnement numérique plus vaste composé de multiples domaines d'application et de mieux répondre aux besoins des utilisateurs. Des systèmes robotiques, inspirés par des études portant sur le développement du langage chez l'humain, ont déjà été développés pour fournir de telles capacités d'adaptation. Ici, nous étendons cette approche à l'interaction humaine avec un assistant virtuel qui peut, premièrement, apprendre le lien entre des commandes verbales et la réalisation d'actions basiques d'un domaine applicatif spécifique. Ensuite, il peut apprendre des liens plus complexes en combinant ses connaissances procédurales précédemment acquises en interaction avec l'utilisateur. La flexibilité du système est démontrée par sa forte adaptabilité au langage naturel, sa capacité à apprendre des actions dans de nouveaux domaines (Email, Wikipedia,...), et à former des connaissances procédurales hybrides en utilisant plusieurs services numériques, par exemple, en combinant une recherche Wikipédia avec un service de courrier électronique
Les systèmes actuels d'improvisation musicales sont capables de générer des séquences musicales unidimensionnelles par recombinaison du matériel musical. Cependant, la prise en compte de plusieurs dimensions (mélodie, harmonie...) et la modélisation de plusieurs niveaux temporels sont des problèmes difficiles. Dans cette thèse, nous proposons de combiner des approches probabilistes et des méthodes issues de la théorie des langages formels afin de mieux apprécier la complexité du discours musical à la fois d'un point de vue multidimensionnel et multi-niveaux dans le cadre de l'improvisation où la quantité de données est limitée. Dans un premier temps, nous présentons un système capable de suivre la logique contextuelle d'une improvisation représentée par un oracle des facteurs tout en enrichissant son discours musical à l'aide de connaissances multidimensionnelles représentées par des modèles probabilistes interpolés. Ensuite, ces travaux sont étendus pour modéliser l'interaction entre plusieurs musiciens ou entre plusieurs dimensions par un algorithme de propagation de croyance afin de générer des improvisations multidimensionnelles. Enfin, nous proposons un système capable d'improviser sur un scénario temporel avec des informations multi-niveaux représenté par une grammaire hiérarchique. Nous proposons également une méthode d'apprentissage pour l'analyse automatique de structures temporelles hiérarchiques. Tous les systèmes sont évalués par des musiciens et improvisateurs experts lors de sessions d'
L'augmentation du nombre de documents multimédias rend nécessaire la mise en place de méthodes de structuration automatique capables de faciliter l'accès à l'information contenue dans tout type de documents. Dans ce cadre, nous proposons deux types de structuration, linéaire et hiérarchique, s'appuyant sur les transcriptions automatiques de la parole prononcée dans les documents. Les transcriptions sont exploitées par le biais de méthodes issues du traitement automatiques des langues adaptées aux spécificités des transcriptions automatiques – erreurs de transcription, faible nombre de répétitions de vocabulaire – grâce à la prise en compte de connaissances linguistiques et d'informations issues de la reconnaissance automatique de la parole et du signal. Les expérimentations menées sur trois corpora composés de journaux télévisés et d'émissions de reportages montrent que ces approches conduisent à une amélioration des performances des méthodes de structuration développées.
Nos travaux de thèse ont pour but d'étudier la manifestation de la néologie dans un dictionnaire de langue française. Nous avons choisi de travailler à partir de la version électronique du "Nouveau Petit Robert" parce qu'il incarne un dictionnaire de référence. Nous avons étudié les mots nouveaux attestés entre 1990 et 2012 dans le "Nouveau Petit Robert Électronique" 2012. Le corpus dont nous disposons se compose de 477 mots nouveaux attestés entre 1990 et 2012. Nous avons aussi consacré une partie à la lexicographie où nous avons mis en lumière les particularités lexicographiques du "Nouveau Petit Robert Électronique" 2012 en comparant quelques extraits des mots du corpus avec deux autres dictionnaires : "Le Petit Larousse Illustré" 2016 et le "Wiktionnaire". Les modes de formation les plus performants des mots du corpus sont ceux relevant de la matrice interne avec un total de 266 mots et ceux relevant de la matrice externe qui regroupe 186 mots. Les mots du corpus appartenant à la matrice interne sont les plus nombreux, cela montre que le lexique se renouvelle par lui-même. Nous avons observé qu'il y avait un déclin des sciences humaines au profit de sciences techniques.
Cette thèse est une étude de l'interlangue prosodique des apprenants d'anglais dont la langue maternelle est le français ou l'espagnol. Elle est organisée en deux parties principales. La première est une étude sur les méthodes de conception et de représentation de la prosodie pour l'analyse de l'interlangue, un système linguistique hybride présentant des caractéristiques de la langue maternelle de l'apprenant, des caractéristiques de la langue cible et des caractéristiques intermédiaires ou développementales. Cette partie débouche sur un cadre méthodologique pour l'analyse phonétique et pour l'interprétation phonologique de ce type de systèmes prosodiques. Les résultats montrent des traces de l'influence de leurs langues maternelles respectives à un niveau phonétique et phonologique, ainsi que des caractéristiques développementales communes aux deux groupes de locuteurs. Les résultats servent de base pour une réflexion sur les niveaux d'abstraction de l'étude de la prosodie et sur les priorités didactiques pour l'enseignement de l'anglais oral au niveau universitaire.
Les études sur le figement ont été remises à l'honneur par le traitement automatique des langues naturelles. En effet, la fréquence des expressions figées dans les textes rend leur description indispensable. Pour cette raison, nous avons entrepris l'analyse des adjectivaux prédicatifs. Ainsi sur la base du modèle des classes d'objets, nous avons élaboré une typologie sémantico-syntaxique de ces suites. Notre recherche a pour objectif final la création d'un dictionnaire électronique des adjectivaux prédicatifs. Ce travail s'articule autour de trois points essentiels : le figement des adjectivaux, leurs propriétés définitionnelles et enfin leur classification fondée sur des propriétés distributionnelles communes.
Ces travaux de recherche portent sur l'aide à l'exploration de bases de données. La particularité de l'approche proposée repose sur un principe de co-évolution de l'utilisateur et d'une interface intelligente. Cette dernière devant permettre d'apporter une aide à la compréhension du domaine représenté par les données. Pour cela, une métaphore de musée virtuel vivant a été adoptée. Ce musée évolue de façon incrémentale au fil des interactions de l'utilisateur. Il incarne non seulement les données mais également des informations sémantiques explicitées par un modèle de connaissances spécifique au domaine exploré. A travers l'organisation topologique et l'évolution incrémentale, le musée personnalise en ligne le parcours de l'utilisateur. L'approche est assurée par trois mécanismes principaux : l'évaluation du profil de l'utilisateur modélisé par une pondération dynamique d'informations sémantiques, l'utilisation de ce profil dynamique pour établir une recommandation ainsi que l'incarnation des données dans le musée. L'approche est appliquée au domaine du patrimoine dans le cadre du projet ANTIMOINE, financé par l'Agence Nationale de la Recherche (ANR). La généricité de cette dernière a été démontrée à travers son application à une base de données de publications mais également à travers l'utilisation de types d'interfaces variés (site web, réalité virtuelle).Des expérimentations ont permis de valider l'hypothèse que notre système s'adapte aux évolutions des comportements de l'utilisateur et qu'il est capable, en retour, d'influencer ce dernier. Elles ont également permis de comparer une interface 2D avec une interface 3D en termes de qualité de perception, de guidage, de préférence et d'efficacité.
Cette hypothèse permet de développer directement des garanties théoriques sur la précision de l'apprentissage. Cependant, elle n'est pas réaliste dans un grand nombre de domaines applicatifs qui ont émergé au cours des dernières années. Dans cette thèse, nous nous intéressons à quatre problèmes différents en intelligence artificielle, unis par un point commun : tous impliquent un transfer de connaissance d'un domaine vers un autre. Le premier problème est le raisonnement par analogie et s'intéresse à des assertions de la forme "A est à B ce que C est à D". Le second est l'apprentissage par transfert et se concentre sur des problèmes de classification dans des contextes où les données d'entraînement et de test ne sont pas de même distribution (ou n'appartiennent même pas au même espace). Le troisième est l'apprentissage sur flux de données, qui prend en compte des données apparaissant continument une à une à haute fréquence, avec des changements de distribution. Le dernier est le clustering collaboratif et consiste à faire échanger de l'information entre algorithmes de clusterings pour améliorer la qualité de leurs prédictions. Ce cadre s'appuie sur la notion de complexité de Kolmogorov, qui mesure l'information continue dans un objet. Cet outil est particulièrement adapté au problème de transfert, du fait qu'il ne repose pas sur la notion de probabilité tout en étant capable de modéliser les changements de distributions. En plus de cet effort de modélisation, nous proposons dans cette thèse diverses discussions sur d'autres aspects ou applications de ces problèmes. Ces discussions s'articulent autour de la possibilité de transfert dans différents domaines et peuvent s'appuyer sur d'autres outils que la complexité.
L'anticipation des besoins des clients est cruciale pour toute entreprise — c'est particulièrement vrai des banques d'investissement telles que BNP Paribas Corporate and Institutional Banking au vu de leur rôle dans les marchés financiers. Cette thèse s'intéresse au problème de la prédiction des intérêts futurs des clients sur les marchés financiers, et met plus particulièrement l'accent sur le développement d'algorithmes ad hoc conçus pour résoudre des problématiques spécifiques au monde financier. Ce manuscrit se compose de cinq chapitres, répartis comme suit :  - Le chapitre 1 expose le problème de la prédiction des intérêts futurs des clients sur les marchés financiers. Le but de ce chapitre est de fournir aux lecteurs toutes les clés nécessaires à la bonne compréhension du reste de cette thèse. Ces clés sont divisées en trois parties : une mise en lumière des jeux de données à notre disposition pour la résolution du problème de prédiction des intérêts futurs et de leurs caractéristiques, une vue d'ensemble, non exhaustive, des algorithmes pouvant être utilisés pour la résolution de ce problème, et la mise au point de métriques permettant d'évaluer la performance de ces algorithmes sur nos jeux de données. Ce chapitre se clôt sur les défis que l'on peut rencontrer lors de la conception d'algorithmes permettant de résoudre le problème de la prédiction des intérêts futurs en finance, défis qui seront, en partie, résolus dans les chapitres suivants ;  - Le chapitre 2 compare une partie des algorithmes introduits dans le chapitre 1 sur un jeu de données provenant de BNP Paribas CIB, et met en avant les difficultés rencontrées pour la comparaison d'algorithmes de nature différente sur un même jeu de données, ainsi que quelques pistes permettant de surmonter ces difficultés. Ce comparatif met en pratique des algorithmes de recommandation classiques uniquement envisagés d'un point de vue théorique au chapitre précédent, et permet d'acquérir une compréhension plus fine des différentes métriques introduites au chapitre 1 au travers de l'analyse des résultats de ces algorithmes ;  -Le chapitre 3 introduit un nouvel algorithme, Experts Network, i.e., réseau d'experts, conçu pour résoudre le problème de l'hétérogénéité de comportement des investisseurs d'un marché donné au travers d'une architecture de réseau de neurones originale, inspirée de la recherche sur les mélanges d'experts. Dans ce chapitre, cette nouvelle méthodologie est utilisée sur trois jeux de données distincts : un jeu de données synthétique, un jeu de données en libre accès, et un jeu de données provenant de BNP Paribas CIB. Ce chapitre présente aussi en plus grand détail la genèse de l'algorithme et fournit des pistes pour l'améliorer ;  - Le chapitre 4 introduit lui aussi un nouvel algorithme, appelé History-augmented collaborative filtering, i.e., filtrage collaboratif augmenté par historiques, qui proposes d'augmenter les approches de factorisation matricielle classiques à l'aide des historiques d'interaction des clients et produits considérés. Ce chapitre poursuit l'étude du jeu de données étudié au chapitre 2 et étend l'algorithme introduit avec de nombreuses idées. Plus précisément, ce chapitre adapte l'algorithme de façon à permettre de résoudre le problème du cold start, i.e., l'incapacité d'un système de recommandation à fournir des prédictions pour de nouveaux utilisateurs, ainsi qu'un nouveau cas d'application sur lequel cette adaptation est essayée ;  - Le chapitre 5 met en lumière une collection d'idées et d'algorithmes, fructueux ou non, qui ont été essayés au cours de cette thèse. Ce chapitre se clôt sur un nouvel algorithme mariant les idées des algorithmes introduits aux chapitres 3 et 4.
Les diagrammes de décision Multi-valués (MDD) sont des structures de données efficaces et largement utilisées dans les domaines tels que la vérification, l'optimisation et la programmation dynamique. Dans cette thèse, nous commençons par améliorer les principaux algorithmes tels que la réduction de MDD, permettant aux MDD de potentiellement compresser exponentiellement des ensembles de tuples, ou la combinaison de MDD, tels que l'intersection ou l'union. Ensuite, nous proposons des versions parallèles de ces algorithmes ainsi que des versions permettant de travailler avec la version non déterministe des MDD. De plus, dans le domaine des MDD relâchés, un domaine de plus en plus étudié, nous définissons les notions de réduction et combinaison relâchés, ainsi que leurs algorithmes associés. Nous résolvons le problème de l'échantillonnage des solutions d'un MDD avec respect de loi de probabilité tels que des fonctions de probabilité de masse ou des chaines de Markov. Grâce à eux, nous montrons que nous pouvons reformuler plusieurs contraintes et en définir de nouvelles tout en étant basés sur des MDD. Finalement nous appliquons nos algorithmes à des problèmes industriels réels de génération de texte et musique, et de modélisation de réservoir de pétrole.
Un des principaux pans du traitement automatique des langues (TAL) est l'extraction sous forme structurée des informations contenues dans un document. Cette extraction est généralement constituée de trois étapes : l'extraction d'entités nommées, des relations les liant au sein du texte et enfin celle des événements. La notion d'événement recouvre différents phénomènes caractérisés par un nombre variable d'actants. L'extraction d'événements consiste alors à identifier la présence d'un événement puis à en déterminer les arguments, c'est-à-dire les différentes entités y remplissant des rôles spécifiques. Les meilleures approches actuelles, reposant sur différents modèles neuronaux, se focalisent sur le voisinage direct du mot dans la phrase. Les informations présentes dans le reste du document sont alors généralement ignorées. Cette thèse présente donc différentes approches visant à exploiter ce contexte distant au sein du document. Nous reproduisons en premier lieu un modèle convolutif obtenant des performances à l'état de l'art et en analysons plusieurs paramètres. Nous réalisons ensuite une expérience permettant d'illustrer le fait que ce modèle, malgré ses bonnes performances, n'exploite effectivement qu'un contexte très restreint au niveau phrastique. Dans un deuxième temps, nous présentons deux méthodes de production et d'intégration d'une représentation du contexte distant à un modèle neuronal opérant au niveau intra-phrastique. Nous montrons par ailleurs la supériorité de cette approche sur une représentation générique du document. Une seconde contribution, répondant aux limitations de la première méthode, permet d'exploiter dynamiquement, pour chaque cible de prédiction, une représentation des phrases les plus pertinentes au sein du contexte grâce à un modèle de convolution de graphe. Cette méthode permet d'obtenir les meilleures performances pour un modèle simple sur différents jeux de données. Enfin, dans un troisième temps, nous considérons une autre approche de la prise en compte du contexte inter-phrastique. Nous cherchons à modéliser plus directement les interdépendances entre les différentes instances d'événements au sein d'un document afin de réaliser une prédiction jointe. Nous utilisons pour cela le cadre d'apprentissage PSL (Probabilistic Soft Logic) qui permet de modéliser de telles interdépendances sous forme de règles logiques.
La résistance aux antimicrobiens (AMR) est une préoccupation majeure de santé publique mondiale. Le Plan d'Action National du Viet Nam sur la résistance aux antimicrobiens a reconnu la surveillance comme l'un des éléments essentiels du contrôle. Cependant, le système actuel de surveillance de l'AMR (AMRSS) au Vietnam est susceptible de sur-représenter les infections graves et les infections nosocomiales (HAI), ce qui pourrait entraîner une surestimation de la résistance des infections acquises dans la communauté (CAI). Cette thèse vise à évaluer l'AMRSS au Viet Nam et à faire des suggestions pour optimiser l'efficacité de l'AMRSS en fournissant des données AMR précises et représentatives pour les patients CAI dans ce contexte. Une revue systématique de la littérature a été menée pour produire un aperçu des AMRSS qui ont été mis en œuvre à l'échelle mondiale et des évaluations de ces systèmes. Il n'y a pas de cadre normalisé ni de lignes directrices pour mener une évaluation de l'AMRSS. Moins de 10% des systèmes ont signalé une évaluation du système, en se concentrant sur quelques attributs tels que la représentativité, l'opportunité, le biais, le coût, la couverture et la sensibilité. La sensibilité de l'AMRSS était de l'ordre de 2 à 5% et est restée similaire entre les deux périodes. Il y a eu un retard dans la soumission des données des hôpitaux, ce qui a affecté la rapidité de la surveillance. Aucune évaluation du système de surveillance n'a été effectuée pour identifier les problèmes et mettre en œuvre des solutions rapides. Dans l'ensemble, les résultats ont montré que la précision des données sur l'AMR est améliorée lorsque le nombre d'hôpitaux augmente (diminution de 0,6% de l'erreur quadratique moyenne pour un hôpital). Pour un budget donné, le nombre optimal d'hôpitaux par type peut être déterminé en utilisant cette approche de modélisation pour identifier un système avec les meilleures valeurs pour chaque attribut de performance. Les résultats indiquent que l'AMRSS actuel peut augmenter les proportions d'hôpitaux spécialisés et provinciaux pour accroître la précision des données et la représentativité du système. Les résultats sont valables pour un AMRSS avec des structures organisationnelles et des protocoles de collecte de données similaires de VINARES. Le budget que le gouvernement et les partenaires de développement étrangers sont prêts à consacrer à la surveillance de l'AMR est également un facteur important pour identifier la combinaison optimale des hôpitaux pour l'AMRSS.
Nous nous intéressons à la fois à la modélisation des fréquences des mots dans les collections textuelles et aux modèles probabilistes de recherche d'information (RI). Concernant les modèles statistiques de fréquences de mots, nous portons notre attention sur l'étude du phénomène de rafale (burstiness). Nous établissons une propriété sur les distributions de probabilité caractérisant leur capacité à modéliser ce phénomène et nous étudions ensuite les distributions Beta Negative Binomial et Log-Logistique pour la modélisation des fréquences de mots. Nous portons ensuite notre attention sur les modèles probabilistes de RI et leur propriétés fondamentales. Nous pouvons montrer que les modèles classiques ne reposent pas sur des lois de probabilité en rafale, même si des propriétés fondamentales comme la concavité des modèles permettent implicitement de le prendre en compte. Enfin, nous étudions empiriquement et théoriquement les modèles de rétro-pertinence. Nous proposons un cadre théorique qui permet ainsi d'expliquer leurs caractéristiques empiriques et leur performances. Ceci permet entre autres de mettre en avant les propriétés importantes des modèles de retro-pertinence et de montrer que certains modèles de référence sont déficients.
Cette thèse traite de la détection de fraude par carte de crédit. Selon la Banque Centrale Européenne, la valeur des fraudes utilisant des cartes en 2016 s'élevait à 1,8 milliard d'euros. Le défis pour les institutions est de réduire ces fraudes. En règle générale, les systèmes de détection de la fraude sont consistués d'un système automatique construit à base de règles "si-alors" qui contrôlent toutes les transactions en entrée et déclenchent une alerte si la transaction est considérée suspecte. Un groupe expert vérifie l'alerte et décide si cette dernière est vrai ou pas. Les critères utilisés dans la sélection des règles maintenues opérationnelles sont principalement basés sur la performance individuelle des règles. Cette approche ignore en effet la non-additivité des règles. Nous proposons une nouvelle approche utilisant des indices de puissance. Cette approche attribue aux règles un score normalisé qui quantifie l'influence de la règle sur les performances globales du groupe de règles. Les indice utilisés sont le "Shapley Value" et le "Banzhaf Value". Leurs applications sont : 1) Aide à la décision de conserver ou supprimer une règle ; 2) Sélection du nombre k de règles les mieux classées, afin de travailler avec un ensemble plus compact. En utilisant des données réelles de fraude par carte de crédit, nous montrons que :  1) Cette approche permet de mieux évaluer les performances du groupe plutot que de les évaluer isolément. 2) La performance de l'ensemble des règles peut être atteinte en conservant le dixième des règles. Nous observons que cette application peut être comsidérée comme une tâche de sélection de caractéristiques : ainsi nous montrons que notre approche est comparable aux algorithmes courants de sélection des caractéristiques. Il présente un avantage dans la gestion des règles, car attribue un score normalisé à chaque règle. Ce qui n'est pas le cas pour la plupart des algorithmes, qui se concentrent uniquement sur une solution d'ensemble. Nous proposons une nouvelle version du Banzhaf Value, à savoir le k-Banzhaf ; qui surclasse la précedente en terme de temps de calcul et possède des performances comparables. Enfin, nous mettons en œuvre un processus d'auto-apprentissage afin de renforcer l'apprentissage dans un algorithme. Nous comparons ces derniers avec nos trois indices de puissance pour effectuer une classification sur les données de fraude par carte de crédit. En conclusion, nous observons que la sélection de caractéristiques basée sur les indices de puissance a des résultats comparables avec les autres algorithmes dans le processus d'auto-apprentissage.
Les systèmes de dialogue incrémentaux sont capables d'entamer le traitement des paroles de l'utilisateur au moment même où il les prononce (sans attendre de signal de fin de phrase tel un long silence par exemple). Ils peuvent ainsi prendre la parole à n'importe quel moment et l'utilisateur peut faire de même (et interrompre le système). De ce fait, ces systèmes permettent d'effectuer une plus large palette de comportements de prise de parole en comparaison avec les systèmes de dialogue traditionnels. Cette thèse s'articule autour de la problématique suivante : est-il possible pour un système de dialogue incrémental d'apprendre une stratégie optimale de prise de parole de façon autonome ? Tout d'abord, une analyse des mécanismes sous-jacents à la dynamique de prise de parole dans une conversation homme-homme a permis d'établir une taxonomie de ces phénomènes. Ensuite, une nouvelle architecture permettant de doter les systèmes de dialogues conventionnels de capacités de traitement incrémentales de la parole, à moindre coût, a été proposée. Dans un premier temps, un simulateur de dialogue destiné à répliquer les comportements incrémentaux de l'utilisateur et de la reconnaissance vocale a été développé puis utilisé pour effectuer les premier tests de stratégies de dialogue incrémentales. Ces dernières ont été développées à base de règles issues de l'analyse effectuée lors de l'établissement de la taxonomie des phénomènes de prise de parole. Les résultats de la simulation montrent que le caractère incrémental permet d'obtenir des interactions plus efficaces. La meilleure stratégie à base de règles a été retenue comme référence pour la suite. Dans un second temps, une stratégie basée sur l'apprentissage par renforcement a été implémentée. Elle est capable d'apprendre à optimiser ses décisions de prise de parole de façon totalement autonome étant donnée une fonction de récompense. Une première comparaison, en simulation, a montré que cette stratégie engendre des résultats encore meilleurs par rapport à la stratégie à base de règles. En guise de validation, une expérience avec des utilisateurs réels a été menée (interactions avec une maison intelligente).
Pouvoir manipuler et de comparer de mesures de probabilité est essentiel pour de nombreuses applications en apprentissage automatique. Le transport optimal (TO) définit des divergences entre distributions fondées sur la géométrie des espaces sous-jacents : partant d'une fonction de coût définie sur l'espace dans lequel elles sont supportées, le TO consiste à trouver un couplage entre les deux mesures qui soit optimal par rapport à ce coût. En dépit de ces avantages, l'emploi du TO pour les sciences des données a longtemps été limité par les difficultés mathématiques et computationnelles liées au problème d'optimisation sous-jacent. Pour contourner ce problème, une approche consiste à se concentrer sur des cas particuliers admettant des solutions en forme close, ou pouvant se résoudre efficacement. En particulier, le TO entre mesures elliptiques constitue l'un des rares cas pour lesquels le TO admet une forme close, définissant la géométrie de Bures-Wasserstein (BW). Cette thèse s'appuie tout particulièrement sur la géométrie de BW, dans le but de l'utiliser comme outil de base pour des applications en sciences des données. Pour ce faire, nous considérons des situations dans lesquelles la géométrie de BW est tantôt utilisée comme un outil pour l'apprentissage de représentations, étendue à partir de projections sur des sous-espaces, ou régularisée par un terme entropique. Dans une première contribution, la géométrie de BW est utilisée pour définir des plongements sous la forme de distributions elliptiques, étendant la représentation classique sous forme de vecteurs de R^d. Dans une deuxième contribution, nous prouvons l'existence de transports qui extrapolent des applications restreintes à des projections en faible dimension, et montrons que ces plans "sous-espace optimaux" admettent des formes closes dans le cas de mesures gaussiennes. La troisième contribution de cette thèse consiste à obtenir des formes closes pour le transport entropique entre des mesures gaussiennes non-normalisées, qui constituent les premières expressions non triviales pour le transport entropique. Finalement, dans une dernière contribution nous utilisons le transport entropique pour imputer des données manquantes de manière non-paramétrique, tout en préservant les distributions sous-jacentes.
Les assistants vocaux font partie de notre vie quotidienne. Leurs performances sont mises à l'épreuve en présence de distorsions du signal, telles que le bruit, la réverbération et les locuteurs simultanés. Cette thèse aborde le problème de l'extraction du signal d'intérêt dans de telles conditions acoustiques difficiles en localisant d'abord le locuteur cible puis en utilisant la position spatiale pour extraire le signal de parole correspondant. Dans un premier temps, nous considérons la situation courante où le locuteur cible prononce un mot ou une phrase connue, comme le mot de réveil d'un système de commande vocale mains-libres. Nous proposons une méthode afin d'exploiter cette information textuelle pour améliorer la localisation du locuteur en présence de locuteurs simultanés. La solution proposée utilise un système de reconnaissance vocale pour aligner le mot de réveil au signal vocal corrompu. Un spectre de référence représentant les phones alignés est utilisé pour calculer un identifiant qui est ensuite utilisé par un réseau de neurones profond pour localiser le locuteur cible. Les résultats sur des données simulées montrent que la méthode proposée réduit le taux d'erreur de localisation par rapport à la méthode classique GCC-PHAT. Des améliorations similaires sont constatées sur des données réelles. Étant donnée la position spatiale estimée du locuteur cible, la séparation de la parole est effectuée en trois étapes. Dans la première étape, une simple formation de voie delay-and-sum (DS) est utilisée pour rehausser le signal provenant de cette direction, qui est utilisé dans la deuxième étape par un réseau de neurones pour estimer un masque temps-fréquence. Ce masque est utilisé pour calculer les statistiques du second ordre et pour effectuer une formation de voie adaptative dans la troisième étape. Un ensemble de données réverbéré, bruité avec plusieurs canaux et plusieurs locuteurs---inspiré du célèbre corpus WSJ0-2mix---a été généré et la performance de la méthode proposée a été étudiée en terme du taux d'erreur sur les mots (WER). Pour rendre le système plus robuste aux erreurs de localisation, une approche par déflation guidée par la localisation (SLOGD) qui estime les sources de manière itérative est proposée. À chaque itération, la position spatiale d'un locuteur est estimée puis utilisée pour estimer un masque correspondant à ce même locuteur. La source estimée est retirée du mélange avant d'estimer la position et le masque de la source suivante. La méthode proposée surpasse Conv-TasNet. Enfin, le problème d'expliquer la robustesse des réseaux de neurones utilisés pour calculer les masques temps-fréquence à des conditions de bruit différentes. Nous utilisons la méthode dite SHAP pour quantifier la contribution de chaque point temps-fréquence du signal d'entrée au masque temps-fréquence estimé. Nous définissons une métrique qui résume les valeurs SHAP et montrons qu'elle est corrélée au WER obtenu sur la parole séparée. À notre connaissance, il s'agit de la première étude sur l'explicabilité des réseaux de neurones dans le contexte de la séparation de la parole.
Cette thèse propose plusieurs contributions sur le thème de la détection d'implications textuelles (DIT). La DIT est la capacité humaine, étant donné deux textes, à pouvoir dire si le sens du second texte peut être déduit à partir de celui du premier. Une des contributions apportée au domaine est un système de DIT hybride prenant les analyses d'un analyseur syntaxique stochastique existant afin de les étiqueter avec des rôles sémantiques, puis transformant les structures obtenues en formules logiques grâce à des règles de réécriture pour tester finalement l'implication à l'aide d'outils de preuve. L'autre contribution de cette thèse est la génération de suites de tests finement annotés avec une distribution uniforme des phénomènes couplée avec une nouvelle méthode d'évaluation des systèmes utilisant les techniques de fouille d'erreurs développées par la communauté de l'analyse syntaxique permettant une meilleure identification des limites des systèmes. Pour cela nous créons un ensemble de formules sémantiques puis nous générons les réalisations syntaxiques annotées correspondantes à l'aide d'un système de génération existant. Nous testons ensuite s'il y a implication ou non entre chaque couple de réalisations syntaxiques possible. Enfin nous sélectionnons un sous-ensemble de cet ensemble de problèmes d'une taille donnée et satisfaisant un certain nombre de contraintes à l'aide d'un algorithme que nous avons développé.
Cette thèse est consacrée à l'étude des systèmes de recommandation basés sur des réseaux de neurones artificiels appris pour faire de l'ordonnancement de produits avec des retours implicites (sous forme de clics). Dans ce sens, nous proposons un nouveau modèle neuronal qui apprend conjointement la représentation des utilisateurs et des produits dans un espace latent, ainsi que la relation de préférence des utilisateurs sur les produits. Nous montrons que le modèle proposé est apprenable au sens du principe de la minimisation du risque empirique et performant par rapport aux autres modèles de l'état de l'art sur plusieurs collections. En outre, nous contribuons à la création de deux nouvelles collections, produites grâce aux enregistrements des comportements de clients de Kelkoo (https : //www.kelkoo.com/) ;  Les deux jeux de données recueillent des retours implicites des utilisateurs sur des produits, ainsi qu'un grand nombre d'informations contextuelles concernant à la fois les clients et les produits. La collections de données de Purch contient en plus une information sur la popularité des produits ainsi que des commentaires textuelles associés. Mots clés. Systèmes de recommandation, apprentissage d'ordonnancement, réseaux de neurones, recommandations avec des retours implicites, Modèles probabilistes latents temporels
En tant que syntagmes sémantiquement non-compositionnels, les locutions sont des unités lexicales à part entière, qui doivent avoir leur propre entrée dans un modèle du lexique. De plus, en vertu de leur signifiant syntagmatique, les locutions témoignent – à des degrés divers – d'une flexibilité formelle (passivation, insertion de modificateurs, substitution de certains constituants, etc.). Notre thèse défend l'idée selon laquelle une description des locutions combinant à la fois l'identification des unités lexicales qui les composent et l'identification des relations de dépendance syntaxique qui unissent les unités constituantes, permettra de prédire leurs différents emplois possibles dans la phrase. Une telle description n'est possible que dans un modèle du lexique décrivant précisément la combinatoire des lexies. La thèse a deux principaux apports. Le premier est le développement d'un modèle de description lexico-syntaxique relativement fine des locutions du français. Le second est l'identification et l'étude de différentes variations structurales, syntaxiques et lexicales liées à la flexibilité formelle des locutions. Les variations des locutions sont mises en corrélation avec leurs structures lexico-syntaxiques, mais également avec leurs définitions lexicographiques. Ceci nous conduit à introduire la notion de projection structurale, centrale dans le continuum de la flexibilité formelle des locutions
L'objectif de cette thèse est de faciliter l'utilisation de l'apprentissage supervisé dans les systèmes de détection pour renforcer la détection. Dans ce but, nous considérons toute la chaîne de traitement de l'apprentissage supervisé (annotation, extraction d'attributs, apprentissage, et évaluation) en impliquant les experts en sécurité. Tout d'abord, nous donnons des conseils méthodologiques pour les aider à construire des modèles de détection supervisés qui répondent à leurs contraintes opérationnelles. De plus, nous concevons et nous implémentons DIADEM, un outil de visualisation interactif qui aide les experts en sécurité à appliquer la méthodologie présentée. DIADEM s'occupe des rouages de l'apprentissage supervisé pour laisser les experts en sécurité se concentrer principalement sur la détection. Par ailleurs, nous proposons une solution pour réduire le coût des projets d'annotations en sécurité informatique. Nous concevons et implémentons un système d'apprentissage actif complet, ILAB, adapté aux besoins des experts en sécurité. Nos expériences utilisateur montrent qu'ils peuvent annoter un jeu de données avec une charge de travail réduite grâce à ILAB. Enfin, nous considérons la génération automatique d'attributs pour faciliter l'utilisation de l'apprentissage supervisé dans les systèmes de détection. Nous définissons les contraintes que de telles méthodes doivent remplir pour être utilisées dans le cadre de la détection de menaces. Nous comparons trois méthodes de l'état de l'art en suivant ces critères, et nous mettons en avant des pistes de recherche pour mieux adapter ces techniques aux besoins des experts en sécurité.
Apprendre une nouvelle langue signifie exploiter, plus ou moins consciemment,l'espace de la proximité du système linguistique cible avec d'autres langues. Nous nous intéressons à ces transferts et à l'influence que ceux-ci exercent sur l'interlangue, dans le cas des apprenants roumains de FLE, étudiant aussi l'italien et l'espagnol. L'analyse des données nous permet de rendre compte du rapport qui s'établit entre la conformité avec la norme et la complexité de l'interlangue et aussi de déterminer le rôle des transferts linguistiques positifs et négatifs par rapport à ces deux dimensions de l'interlangue (la complexité et la conformité)
L'enjeu majeur de cette thèse réside dans l'amélioration de l'adéquation entre l'information retournée et les attentes des utilisateurs à l'aide de profils riches et efficaces. Il s'agit donc d'exploiter au maximum les retours utilisateur (qu'ils soient donnés sous la forme de clics, de notes ou encore d'avis écrits) et le contexte. Durant ces travaux de thèse, nous avons choisi d'exploiter les textes écrit par les utilisateurs pour affiner leurs profils et contextualiser la recommandation. À cette fin, nous avons utilisé les avis postés sur les sites spécialisés (IMDb, RateBeer, BeerAdvocate) et les boutiques en ligne (Amazon) ainsi que les messages postés sur Twitter. En plus d'aider à l'amélioration des performances du système, elle permet d'apporter une forme d'explication quant aux items proposés. Ainsi, nous proposons d'accompagner l'utilisateur dans son accès à l'information au lieu de le contraindre à un ensemble d'items que le système juge pertinents.
Elles ont donné lieu à de nombreuses études en Traitement Automatique du Langage Naturel. En effet, leur étude et leur identification précise sont primordiales, sur les plans théorique et applicatif. Cependant, la majorité des travaux de recherche sur le sujet portent sur des usages de langage quotidien : dialogues " à bâtons rompus ", demandes d'horaire, discours, etc. Mais qu'en est-il des productions orales spontanées produites dans un cadre contraint ? Aucune étude n'a à notre connaissance été menée dans ce contexte. Or, on sait que l'utilisation d'une " langue de spécialité " dans le cadre d'une tâche donnée entraîne des comportements spécifiques. Notre travail de thèse est consacré à l'étude linguistique et informatique des disfluences dans un tel cadre. Il s'agit de dialogues de contrôle de trafic aérien, aux contraintes pragmatiques et linguistiques. Nous effectuons une étude exhaustive des phénomènes de disfluences dans ce contexte. Dans un premier temps nous procédons à l'analyse fine de ces phénomènes. Ensuite, nous les modélisons à un niveau de représentation abstrait, ce qui nous permet d'obtenir les patrons correspondant aux différentes configurations observées. Enfin nous proposons une méthodologie de traitement automatique. Celle-ci consiste en plusieurs algorithmes pour identifier les différents phénomènes, même en l'absence de marqueurs explicites. Elle est intégrée dans un système de traitement automatique de la parole. Enfin, la méthodologie est validée sur un corpus de 400 énoncés.
Prendre en compte l'aspect sémantique des données textuelles lors de la tâche de classification s'est imposé comme un réel défi ces dix dernières années. Cette difficulté vient s'ajouter au fait que la plupart des données disponibles sur les réseaux sociaux sont des textes courts, ce qui a notamment pour conséquence de rendre les méthodes basées sur la représentation "bag of words" peu efficientes. L'approche proposée dans ce projet de recherche est différente des approches proposées dans les travaux antérieurs sur l'enrichissement des messages courts et ce pour trois raisons. Tout d'abord, nous n'utilisons pas des bases de connaissances externes comme Wikipedia parce que généralement les messages courts qui sont traités par l'entreprise proveniennent des domaines spécifiques. Deuxièment, les données à traiter ne sont pas utilisées pour la constitution de ressources à cause du fonctionnement de l'outil. Dans cette thèse, nous proposons la création de ressources permettant d'enrichir les messages courts afin d'améliorer la performance de l'outil du regroupement sémantique de l'entreprise Succeed Together. Ce dernier implémente des méthodes de classification supervisée et non supervisée. Pour constituer ces ressources, nous utilisons des techniques de fouille de données séquentielles.
Le but de cette thèse était de conduire des recherches sur la synthèse et transformation expressive de voix chantée, en vue de pouvoir développer un synthétiseur de haute qualité capable de générer automatiquement un chant naturel et expressif à partir d'une partition et d'un texte donnés. Cette thèse apporte diverses contributions dans chacune de ces 3 directions. Tout d'abord, un système de synthèse complet a été développé, basé sur la concaténation de diphones. L'architecture modulaire de ce système permet d'intégrer et de comparer différent modèles de signaux. Ensuite, la question du contrôle est abordée, comprenant la génération automatique de la f0, de l'intensité, et des durées des phonèmes. La modélisation de styles de chant spécifiques a également été abordée par l'apprentissage des variations expressives des paramètres de contrôle modélisés à partir d'enregistrements commerciaux de chanteurs célèbres. Enfin, des investigations sur des transformations expressives du timbre liées à l'intensité et à la raucité vocale ont été menées, en vue d'une intégration future dans notre synthétiseur.
La recherche d'information sémantique (RIS), cherche à proposer des modèles qui permettent de s'appuyer, au delà des calculs statistiques, sur la signification et la sémantique des mots du vocabulaire, afin de mieux caractériser les documents pertinents au regard du besoin de l'utilisateur et de les retrouver. Le but est ainsi de dépasser les approches classiques purement statistiques (de « sac de mots » ), fondées sur des appariements de chaînes de caractères sur la base des fréquences des mots et de l'analyse de leurs distributions dans le texte. Pour ce faire, les approches existantes de RIS, à travers l'exploitation de ressources sémantiques externes (thésaurus ou ontologies), procèdent en injectant des connaissances dans les modèles classiques de RI de manière à désambiguïser le vocabulaire ou à enrichir la représentation des documents et des requêtes. Les ressources sémantiques, ainsi exploitées, sont « aplaties » , les calculs se cantonnent, généralement, à des calculs de similarité sémantique. Afin de permettre une meilleure exploitation de la sémantique en RI, nous mettons en place un nouveau modèle, qui permet d'unifier de manière cohérente et homogène les informations numériques (distributionnelles) et symboliques (sémantiques) sans sacrifier la puissance des analyses. Le réseau sémantico-documentaire ainsi modélisé est traduit en graphe pondéré. Le mécanisme d'appariement est assuré par une propagation d'activation dans le graphe. Ce nouveau modèle permet à la fois de répondre à des requêtes exprimées sous forme de mots clés, de concepts oumême de documents exemples. L'algorithme de propagation a le mérite de préserver les caractéristiques largement éprouvéesdes modèles classiques de recherche d'information tout en permettant une meilleure prise en compte des modèles sémantiques et de leurs richesse. Selon que l'on introduit ou pas de la sémantique dans ce graphe, ce modèle permet de reproduire une RI classique ou d'assurer en sus certaines fonctionnalités sémantiques. La co-occurrence dans le graphe permet alors de révélerune sémantique implicite qui améliore la précision en résolvant certaines ambiguïtés sémantiques. L'exploitation explicite des concepts ainsi que des liens du graphe, permettent la résolution des problèmes de synonymie, de term mismatch et de couverture sémantique. Ces fonctionnalités sémantiques, ainsi que le passage à l'échelle du modèle présenté, sont validés expérimentalement sur un corpus dans le domaine médical.
La thèse a deux objectifs : le premier est de développer un analyseur qui permet d'analyser automatiquement des sources textuelles en chinois simplifié afin de segmenter les textes en mots et de les étiqueter par catégories grammaticales, ainsi que de construire les relations syntaxiques entre les mots. Le deuxième est d'extraire des informations autour des entités et des actions qui nous intéressent à partir des textes analysés. Afin d'atteindre ces deux objectifs, nous avons traité principalement les problématiques suivantes : les ambiguïtés de segmentation, la catégorisation ; le traitement des mots inconnus dans les textes chinois ; l'ambiguïté de l'analyse syntaxique ; la reconnaissance et le typage des entités nommées. Le texte d'entrée est traité phrase par phrase. L'analyseur commence par un traitement typographique au sein des phrases afin d'identifier les écritures latines et les chiffres. Ensuite, nous segmentons la phrase en mots à l'aide de dictionnaires. Un modèle de langue n-gramme élaboré à partir d'un corpus d'apprentissage permet de sélectionner le meilleur résultat de segmentation et de catégorisation. Une analyse en dépendance est utilisée pour marquer les relations entre les mots. Nous effectuons une première identification d'entités nommées à la fin de l'analyse syntaxique. Ceci permet d'identifier les entités nommées en unité ou en groupe nominal et également de leur attribuer un type. Ces entités nommées sont ensuite utilisées dans l'extraction. Les règles d'extraction permettent de valider ou de changer les types des entités nommées. L'extraction des connaissances est composée des deux étapes : extraire et annoter automatiquement des contenus à partir des textes analysés ; vérifier les contenus extraits et résoudre la cohérence à travers une ontologie.
Les travaux présentés dans cette thèse se situent dans le cadre de la synthèse de la parole à partir du texte et, plus précisément, dans le cadre de la synthèse paramétrique utilisant des règles statistiques. Nous nous intéressons à l'influence des descripteurs linguistiques utilisés pour caractériser un signal de parole sur la modélisation effectuée dans le système de synthèse statistique HTS. Pour cela, deux méthodologies d'évaluation objective sont présentées. La première repose sur une modélisation de l'espace acoustique, généré par HTS par des mélanges gaussiens (GMM). En utilisant ensuite un ensemble de signaux de parole de référence, il est possible de comparer les GMM entre eux et ainsi les espaces acoustiques générés par les différentes configurations de HTS. La seconde méthodologie proposée repose sur le calcul de distances entre trames acoustiques appariées pour pouvoir évaluer la modélisation effectuée par HTS de manière plus locale. Cette seconde méthodologie permet de compléter les diverses analyses en contrôlant notamment les ensembles de données générées et évaluées.
Cette thèse présente une méthodologie pour résoudre des problèmes de classification, en particulier ceux concernant le classement séquentiel pour les tâches de traitement du langage naturel. Elle propose l'utilisation d'une méthode itérative, basée sur l'analyse des erreurs, pour améliorer la performance de classification. Ce sont des experts du domaine qui suggèrent l'intégration des connaissances spécifiques du domaine dans le processus d'apprentissage automatique. De plus, cette thèse propose un schéma de classes pour représenter des analyse des phrases dans une structure de donnés unique, y compris les résultats de divers analyses linguistiques. Cela nous permet de mieux gérer le processus itératif d'amélioration du classificateur, où des ensembles d'attributs différents pour l'apprentissage sont utilisés à chaque itération. Nous proposons également de stocker des attributs dans un modèle relationnel, plutôt que des structures traditionnelles à base de texte, pour faciliter l'analyse et la manipulation de données nécessaires pour l'apprentissage.
La nécessité d'estimer la taille d'un logiciel pour pouvoir en estimer le coût et l'effort nécessaire à son développement est une conséquence de l'utilisation croissante des logiciels dans presque toutes les activités humaines. De plus, la nature compétitive de l'industrie du développement logiciel rend courante l'utilisation d'estimations précises de leur taille, au plus tôt dans le processus de développement. Traditionnellement, l'estimation de la taille des logiciels était accomplie a posteriori à partir de diverses mesures appliquées au code source. Cependant, avec la prise de conscience, par la communauté de l'ingénierie logicielle, que l'estimation de la taille du code est une donnée cruciale pour la maîtrise du développement et des coûts, l'estimation anticipée de la taille des logiciels est devenue une préoccupation répandue. Une fois le code écrit, l'estimation de sa taille et de son coût permettent d'effectuer des études contrastives et éventuellement de contrôler la productivité. D'autre part, les bénéfices apportés par l'estimation de la taille sont d'autant plus grands que cette estimation est effectuée tôt pendant le développement. Notre recherche se positionne autour des mesures d'estimation de la taille fonctionnelle, couramment appelées Analyse des Points de Fonctions, qui permettent d'estimer la taille d'un logiciel à partir des fonctionnalités qu'il doit fournir à l'utilisateur final, exprimées uniquement selon son point de vue, en excluant en particulier toute considération propre au développement. Un problème significatif de l'utilisation des points de fonction est le besoin d'avoir recours à des experts humains pour effectuer la quotation selon un ensemble de règles de comptage. Le processus d'estimation représente donc une charge de travail conséquente et un coût important. D'autre part, le fait que les règles de comptage des points de fonction impliquent nécessairement une part d'interprétation humaine introduit un facteur d'imprécision dans les estimations et rend plus difficile la reproductibilité des mesures. Nous proposons de fournir aux experts humains une aide automatique dans le processus d'estimation, en identifiant dans le texte des spécifications, les endroits les plus à même de contenir des points de fonction. Enfin, l'identification non ambiguë des points de fonction permettra de faciliter et d'améliorer la reproductibilité des mesures. À notre connaissance, les travaux présentés dans cette thèse sont les premiers à se baser uniquement sur l'analyse du contenu textuel des spécifications, applicable dès la mise à disposition des spécifications préliminaires et en se basant sur une approche générique reposant sur des pratiques établies d'analyse automatique du langage naturel.
Les applications futures de la robotique, en particulier pour des robots de service à la personne, exigeront des capacités d'adaptation continue à l'environnement, et notamment la capacité à reconnaître des nouveaux objets et apprendre des nouveaux mots via l'interaction avec les humains. Bien qu'ayant fait d'énormes progrès en utilisant l'apprentissage automatique, les méthodes actuelles de vision par ordinateur pour la détection et la représentation des objets reposent fortement sur de très bonnes bases de données d'entrainement et des supervisions d'apprentissage idéales. En revanche, les enfants de deux ans ont une capacité impressionnante à apprendre à reconnaître des nouveaux objets et en même temps d'apprendre les noms des objets lors de l'interaction avec les adultes et sans supervision précise. Par conséquent, suivant l'approche de le robotique développementale, nous développons dans la thèse des approches d'apprentissage pour les objets, en associant leurs noms et leurs caractéristiques correspondantes, inspirées par les capacités des enfants, en particulier l'interaction ambiguë avec l'homme en s'inspirant de l'interaction qui a lieu entre les enfants et les parents. L'idée générale est d'utiliser l'apprentissage cross-situationnel (cherchant les points communs entre différentes présentations d'un objet ou d'une caractéristique) et la découverte de concepts multi-modaux basée sur deux approches de découverte de thèmes latents : la Factorisation en Natrices Non-Négatives (NMF) et l'Allocation de Dirichlet latente (LDA). Cette thèse souligne les solutions algorithmiques requises pour pouvoir effectuer un apprentissage efficace de ces associations de mot-référent à partir de données acquises dans une configuration d'acquisition simplifiée mais réaliste qui a permis d'effectuer des simulations étendues et des expériences préliminaires dans des vraies interactions homme-robot. Nous avons également apporté des solutions pour l'estimation automatique du nombre de thèmes pour les NMF et LDA.Nous avons finalement proposé deux stratégies d'apprentissage actives : la Sélection par l'Erreur de Reconstruction Maximale (MRES) et l'Exploration Basée sur la Confiance (CBE), afin d'améliorer la qualité et la vitesse de l'apprentissage incrémental en laissant les algorithmes choisir les échantillons d'apprentissage suivants. Nous avons comparé les comportements produits par ces algorithmes et montré leurs points communs et leurs différences avec ceux des humains dans des situations d'apprentissage similaires.
D'une part, nous considérons le problème de l'apprentissage de mesure de similarité pour deux tâches : (i) la détection de rupture dans des signaux multivariés et (ii) le problème de déformation temporelle entre paires de signaux. Les méthodes généralement utilisées pour résoudre ces deux problèmes dépendent fortement d'une mesure de similarité. Nous présentons des algorithmes usuels de prédiction structuré, efficaces pour effectuer l'apprentissage. Nous considérons la partition comme une représentation symbolique donnant (i) une information complète sur l'ordre des symboles et (ii) une information approximative sur la forme de l'alignement attendu. Nous apprenons un classifieur pour chaque symbole avec ces informations. Nous développons une méthode d'apprentissage fondée sur l'optimisation d'une fonction convexe. Nous démontrons la validité de l'approche sur des données musicales.
L'augmentation rapide de la population combinée à la mobilité croissante des individus a engendré le besoin de systèmes de gestion d'identités sophistiqués. À cet effet, le terme biométrie se réfère généralement aux méthodes permettant d'identifier les individus en utilisant des caractéristiques biologiques ou comportementales. Les méthodes les plus populaires, c'est-à-dire la reconnaissance d'empreintes digitales, d'iris ou de visages, se basent toutes sur des méthodes de vision par ordinateur. L'adoption de réseaux convolutifs profonds, rendue possible par le calcul générique sur processeur graphique, ont porté les récentes avancées en vision par ordinateur. Ces avancées ont permis une amélioration drastique des performances des méthodes conventionnelles en biométrie, ce qui a accéléré leur adoption pour des usages concrets, et a provoqué un débat public sur l'utilisation de ces techniques. Dans ce contexte, les concepteurs de systèmes biométriques sont confrontés à un grand nombre de challenges dans l'apprentissage de ces réseaux. Dans cette thèse, nous considérons ces challenges du point de vue de l'apprentissage statistique théorique, ce qui nous amène à proposer ou esquisser des solutions concrètes. Premièrement, nous répondons à une prolifération de travaux sur l'apprentissage de similarité pour les réseaux profonds, qui optimisent des fonctions objectif détachées du but naturel d'ordonnancement recherché en biométrie. Précisément, nous introduisons la notion d'ordonnancement par similarité, en mettant en évidence la relation entre l'ordonnancement bipartite et la recherche d'une similarité adaptée à l'identification biométrique. Nous étendons ensuite la théorie sur l'ordonnancement bipartite à ce nouveau problème, tout en l'adaptant aux spécificités de l'apprentissage sur paires, notamment concernant son coût computationnel. La thèse aborde ces trois exemples, en propose une étude statistique minutieuse, ainsi que des méthodes pratiques qui donnent les outils nécessaires aux concepteurs de systèmes biométriques pour adresser ces problématiques, sans compromettre la performance de leurs algorithmes.
Notre travail vise a confronter certains paradigmes informatiques aux problemes que pose le traitement automatique de la construction du sens. Il s'agit d'une part de determiner ces comportements et d'autre part de les representer. Il en va de meme pour les comportements lexicaux : les savoirs associes aux mots ne sont pas donnes une fois pour toutes. On utilise donc le prototype (dans le cadre de la programmation a prototypes) comme un outil de representation de certains faits linguistiques dans la mesure ou nous pensons qu'il peut repoemes que posent ces faits de langue. Cet outil de representation conduit a construire des structures de representation simples et ajustables pour rendre compte justement des problemes d'ajustements qui sont a l'oeuvre dans la construction du sens dans le langage naturel. La pap encourage une approche de representation faite de petits sauts successifs qui ameliore la qualite de la representation produite. Cette demarche s'accorde aussi avec la necessaire approche artisanale que constitue le travail du linguiste dans sa volonte de decrire les comportements des faits de langue.
Cette étude compare le fonctionnement de l'énoncé averbal (EAV) en kabyle et en allemand, en prenant comme cadre théorique la triade sémantico-logique établie par Zemb (1978), i. e. le thème (ce dont on parle), le rhème (ce qu'on en dit) et le phème (lieu d'articulation de la modalisation et de la négation) appliquée par Behr et Quintin (1996) et Behr (2013) à la catégorisation des EAV de l'allemand. Nous postulons que chaque langue dispose de moyens morphosyntaxiques, contextuelles et situationnelles contribuant à la réalisation d'EAV et que ces moyens sont plus étendus en kabyle. Nous supposons qu'il existe des structures sémantico-logiques uniques qui pourraient s'exprimer à travers des structures morphosyntaxiques variées. Nous supposons enfin que les EAV réalisent toutes les modalités, disposent de moyens morphologiques et/ ou contextuels permettant de les localiser dans le cadre temporel. Parmi les résultats, nous avons constaté que les EAV sont plus fréquents en kabyle grâce aux structures prédicatives grammaticalisées, sauf l'EAV représentant une continuité syntaxique avec le segment de gauche dont la fréquence en allemand est due au scrambling. Au niveau syntaxique, la pré-/postposition du thème par rapport au rhème obéit à des contraintes liées à la langue, i. e. l'état du nom en kabyle et la définitude du GN en allemand ; des contraintes propres à l'EAV se manifestent dans la prédilection pour l'ordre rhème-thème en allemand. Les EAV expriment toutes les modalités, ils sont situés dans le temps par les circonstants, certains démonstratifs ou le contexte, et les nominalisations en tant que rhème existentiel expriment l'aspectualité télique et atélique.
Les réunions de concertations pluridisciplinaires en oncologie permettent aux experts des différentes spécialités de choisir les meilleures options thérapeutiques pour les patients. Les données nécessaires à ces réunions sont souvent collectées manuellement, avec un risque d'erreur lors de l'extraction et un coût important pour les professionnels de santé. Le projet ASIMOV vise à utiliser le traitement des langues naturelles et l'intégration de données pour identifier et extraire dans les entrepôts de données et les textes cliniques les informations cruciales à la prise de décision, en particulier sur le plan temporel. Le projet se penche sur la question de l'apprentissage automatique dans un contexte pauvre en annotations. Nous nous appuierons sur des méthodes mixtes et développerons des outils pour généraliser nos approches.
L'énergie est fondamentale pour maintenir le confort et façonne notre vie moderne. Avec la demande excédentaire en énergie, les systèmes de gestion de l'énergie résidentielle apparaissent avec le temps. Ils visent à réduire ou moduler la consommation d'énergie tout en maintenant un niveau de confort acceptable. Des systèmes efficaces de gestion de l'énergie domestique devraient intégrer une représentation comportementale d'un système domestique, y compris les habitants. Il établit des relations entre différentes variables environnementales et des phénomènes hétérogènes présents dans une maison. Par conséquent, ces systèmes sont complexes à construire et à comprendre pour les habitants. Cela était justifié car il était presque impossible d'impliquer les occupants et de créer une relation entre les occupants et les systèmes énergétiques. Ce concept crée différents problèmes car les occupants sont détachés du système énergétique et ne comprennent pas ses fonctionnalités ni son fonctionnement. Pour surmonter cette difficulté, ce travail met en avant le concept de "faire avec" en essayant d'impliquer l'occupant dans la boucle avec son système de gestion de l'énergie. C'est là que l'explication est nécessaire pour permettre aux occupants de découvrir les connaissances du système énergétique et de développer leur capacité à comprendre comment le système fonctionne et pourquoi il recommande différentes actions. L'explication est le moyen de découvrir de nouvelles connaissances et, par conséquent, d'impliquer les occupants. Pour les humains, l'explication joue un rôle important dans la vie. C'est l'un des principaux outils d'apprentissage et de compréhension. Il est même utilisé dans la communication et les aspects sociaux. Les gens ont tendance à l'utiliser en plus d'apprendre à montrer leurs connaissances sur un sujet pour gagner la confiance des autres ou pour clarifier une situation. Mais générer des explications n'est pas une tâche facile. C'est l'un des problèmes scientifiques récurrents de plusieurs décennies. Les explications ont de nombreuses formes, types et niveaux de clarté. Cette étude se concentre sur les explications causales. Comme il s'agit de la forme d'explication la plus intuitive à comprendre par les occupants, elle est conçue pour transférer les connaissances issues de systèmes complexes tels que les modèles énergétiques. Le défi scientifique est de savoir comment construire des explications de causalité pour les habitants à partir d'un flux de données de capteurs observées.
Des études récentes montrent une part croissante de requêtes sur les moteurs de recherche du Web comportant des critères géographiques. Cette part est encore plus conséquente sur des corpus plus spécifiques tels que des documents patrimoniaux (récits de voyages par exemple). On admet que l'information géographique est composée de trois facettes : le spatial, le temporel et le thématique. L'objet de ce travail de thèse est de combiner les trois facettes pour effectuer des recherches multicritère. Ce travail s'intègre au croisement de plusieurs disciplines : Traitement Automatique des Langages Naturels (TALN), Systèmes d'Information Géographique (SIG), Recherche d'Information classique (RI) et Recherche d'Information Géographique (RIG). Notre première contribution porte sur une méthode originale de combinaison des index spécifiques. Pour pouvoir effectuer cette combinaison, nous proposons d'imiter les approches d'homogénéisation utilisées dans les stratégies de RI classiques portant sur des termes et les lemmes correspondants. Notre deuxième contribution porte sur une approche d'uniformisation générique mise en oeuvre sur l'information spatiale et l'information temporelle. La dernière contribution consiste en un cadre d'évaluation d'un système de recherche géographique. Grâce à ce cadre nous avons pu vérifier et quantifier l'apport de la combinaison de critères géographiques ainsi que comparer différentes approches de combinaisons.
La veille anticipative stratégique et intelligence collective (VASIC) proposée par Lesca est une méthode aidant les entreprises à se mettre à l'écoute de leur environnement pour anticiper des opportunités ou des risques. Cette méthode nécessite la collecte d'informations. Or, avec le développement des technologies de l'information, les salariés font face à une surabondance d'informations. Afin d'aider à pérenniser le dispositif de veille stratégique, il est nécessaire de mettre en place des outils pour gérer la surinformation. Dans cette thèse, nous proposons une mesure de voisinage pour estimer si deux informations sont proches ; nous avons créé un prototype, nommé Alhena, basé sur cette mesure. Nous démontrons les propriétés de notre mesure ainsi que sa pertinence dans le cadre de la veille stratégique. Nous montrons également que le prototype peut servir dans d'autres domaines tels que la littérature, l'informatique et la psychologie. Ce travail est pluridisciplinaire : il aborde des aspects de veille stratégique (en sciences de gestion), de la recherche d'informations, d'informatique linguistique et de mathématiques. Nous nous sommes attachés à partir d'un problème concret en sciences de gestion à proposer un outil qui opérationnalise des techniques informatiques et mathématiques en vue d'une aide à la décision (gain de temps, aide à la lecture,...).
Cette thèse se situe à l'intersection de trois domaines de recherche : le raisonnement à partir de cas, l'extraction de connaissances et la représentation des connaissances. Raisonner à partir de cas consiste à résoudre un nouveau problème en utilisant un ensemble de problèmes déjà résolus, appelés cas. Dans cette thèse, un langage de représentation des variations entre cas est introduit. Nous montrons comment ce langage peut être utilisé pour représenter les connaissances d'adaptation et pour modéliser la phase d'adaptation en raisonnement à partir de cas. Un processus d'extraction de connaissances, appelé CabamakA, est mis au point. Ce processus permet d'apprendre des connaissances d'adaptation par généralisation à partir d'une représentation des variations entre cas. Une discussion est ensuite menée sur les conditions d'opérationnalisation de CabamakA au sein d'un processus d'acquisition de connaissances. L'étude aboutit à la proposition d'un nouveau type d'approche pour l'acquisition de connaissances d'adaptation dans lequel le processus d'extraction de connaissances est déclenché de manière opportuniste au cours d'une session particulière de résolution de problèmes. Les différents concepts introduits dans la thèse sont illustrés dans le domaine culinaire à travers leur application au système de raisonnement à partir de cas Taaable, qui constitue le contexte applicatif de l'étude.
Cette thèse s'inscrit dans une série d'études récemment entamées qui cherchent à caractériser les lectes d'apprenants avancés de l'anglais. Nous présentons une analyse de quelques facteurs sémantiques, discursifs et inter-linguistiques qui sous-tendent l'emploi des formes verbales en anglais langue étrangère par des apprenants avancés francophones et catalanophones en milieu guidé. À partir d'un corpus de narrations orales élicitées à partir d'un livre en images, nous examinons la distribution de la morphologie temporo-aspectuelle par rapport à l'aspect sémantique des prédicats (l'hypothèse de l'aspect) et le type d'information temporelle que ces prédicats encodent dans la narration (l'hypothèse du discours). L'emploi de la morphologie verbale est considéré également du point de vue du style rhétorique de l'apprenant, c'est-à-dire des choix systématiques faits dans une tâche communicative spécifique à partir d'un répertoire appris de formes cibles, mais aussi à travers le filtre inconscient du mode de sélection et d'organisation de l'information en langue maternelle. Même si l'anglais, le français et le catalan grammaticalisent des distinctions aspectuelles, ceci ne permet pas aux apprenants étudiés de faire un emploi de la morphologie verbale tout à fait semblable à celui des locuteurs natifs. Des coalitions prototypiques entre la sémantique des prédicats et celle de la forme verbale, qui caractérisent l'emploi de la morphologie verbale aux stades moins avancés, persistent dans l'emploi des prédicats duratifs (a)téliques et débouchent sur un emploi généralisé du progressif en anglais, souvent dans des contextes où la présence de ce marqueur génère une tension avec le type d'information temporelle encodée. Les moyens d'encoder le déroulement dans la langue maternelle des apprenants semble brouiller leurs hypothèses relatives à l'emploi du progressif en discours dans la langue cible. Seul un sous-ensemble d'apprenants très avancés utilise la morphologie verbale d'une façon véritablement libérée du sémantisme de la construction verbale, de façon similaire aux locuteurs natifs. Pour ces apprenants, le progressif acquiert une fonction discursive et sa présence n'est plus systématique dans les contextes où l'information sur le caractère non-borné d'une situation peut être récupérée à partir d'autres éléments, sémantiques ou syntaxiques. Il existe en effet des légères différences entre les productions des apprenants et des locuteurs natifs en ce qui concerne la palette de fonctions discursives que les formes verbales présentent dans la narration. Notre étude ouvre des pistes de recherche sur l'étanchéité des oppositions grammaticales dans le domaine de la morphologie verbale, sur les coalitions atypiques qui peuvent surgir en discours et la façon dont ces usages périphériques peuvent s'apprendre (et s'enseigner) dans un milieu guidé. Il en résulte aussi que la production orale chez des apprenants avancés se construit à travers le filtre d'une façon de penser le monde qui reste, de façon irréductible, celui de la langue maternelle.
Les environnements côtiers en Normandie sont propices à de multiples aléas (érosion, submersion marine, inondation par débordement de cours d'eau ou remontée de nappe, crue turbide par ruissellement, mouvement de versant côtier ou continental). Des interactions entre aléas vont se produire au sein des versants et des vallées où les populations côtières et leurs activités tendent à se densifier depuis le XIXe siècle. Dans le cadre de cette thèse, ainsi que dans le programme ANR RICOCHET, trois sites d'étude ont été sélectionnés à l'embouchure des rivières littorales : de Auberville à Pennedepie, de Quiberville à Dieppe et de Criel-sur-Mer à Ault en raison d'importants enjeux socio-économiques et de fortes interactions entre les phénomènes hydrologiques et gravitaires.
Les méthodologies classiques utilisées pour la détection de piétons Nous considérons un système à plusieurs classifieurs (Multiple Classifier System, MCS), composé de deux ensembles différents, le premier basé sur les classifieurs SVM (SVM-ensemble) et le deuxième basé sur les CNN (CNN-ensemble), combinés dans le cadre de la Théorie des Fonctions de Croyance (TFC). La TFC nous permet de prendre en compte une valeur d'imprécision supposée correspondre soit à une imprécision dans la procédure de calibration, soit à une imprécision spatiale. Cet algorithme s'appuie sur des mesures évidentielles déduites des fonctions de croyance. Pour le second ensemble, pour exploiter les avancées de l'apprentissage profond, nous avons reformulé notre problème comme une tâche de segmentation en soft labels. Une architecture entièrement convolutionelle a été conçue pour détecter les petits objets grâce à des convolutions dilatées. Pour conclure, nous montrons que la sortie du MCS peut être utile aussi pour le comptage de personnes.
Cette thèse traite de la reconnaissance d'entités dans les documents océrisés guidée par une base de données. Une entité peut être, par exemple, une entreprise décrite par son nom, son adresse, son numéro de téléphone, son numéro TVA, etc. ou des méta-données d'un article scientifique tels que son titre, ses auteurs et leurs affiliations, le nom de son journal, etc. Disposant d'un ensemble d'entités structurées sous forme d'enregistrements dans une base de données et d'un document contenant une ou plusieurs de ces entités, nous cherchons à identifier les entités contenues dans le document en utilisant la base de données. Ce travail est motivé par une application industrielle qui vise l'automatisation du traitement des images de documents administratifs arrivant en flux continu. Nous avons abordé ce problème comme un problème de rapprochement entre le contenu du document et celui de la base de données. Les difficultés de cette tâche sont dues à la variabilité de la représentation d'attributs d'entités dans la base et le document et à la présence d'attributs similaires dans des entités différentes. À cela s'ajoutent les redondances d'enregistrements et les erreurs de saisie dans la base de données et l'altération de la structure et du contenu du document, causée par l'OCR. Devant ces problèmes, nous avons opté pour une démarche en deux étapes : la résolution d'entités et la reconnaissance d'entités. La première étape consiste à coupler les enregistrements se référant à une même entité et à les synthétiser dans un modèle entité. Pour ce faire, nous avons proposé une approche supervisée basée sur la combinaison de plusieurs mesures de similarité entre attributs. Ces mesures permettent de tolérer quelques erreurs sur les caractères et de tenir compte des permutations entre termes. La deuxième étape vise à rapprocher les entités mentionnées dans un document avec le modèle entité obtenu. Nous avons procédé par deux manières différentes, l'une utilise le rapprochement par le contenu et l'autre intègre le rapprochement par la structure. Pour le rapprochement par le contenu, nous avons proposé deux méthodes : M-EROCS et ERBL. M-EROCS, une amélioration/adaptation d'une méthode de l'état de l'art, consiste à faire correspondre les blocs de l'OCR avec le modèle entité en se basant sur un score qui tolère les erreurs d'OCR et les variabilités d'attributs. ERBL consiste à étiqueter le document par les attributs d'entités et à regrouper ces labels en entités. Pour le rapprochement par les structures, il s'agit d'exploiter les relations structurelles entre les labels d'une entité pour corriger les erreurs d'étiquetage. La méthode proposée, nommée G-ELSE, consiste à utiliser le rapprochement inexact de graphes attribués modélisant des structures locales, avec un modèle structurel appris pour cet objectif. Cette thèse étant effectuée en collaboration avec la société ITESOFT-Yooz, nous avons expérimenté toutes les étapes proposées sur deux corpus administratifs et un troisième corpus extrait du Web
Cette thèse étudie les rapports entre l'expression d'attitudes agressives contrôlées et la perception de la dominance, à partir d'extraits de séances télévisées du conseil Municipal de Montreuil (93100) durant l'année 2013, période marquée par un climat politique vif et hostile. Un corpus a été constitué à partir d'extraits de parole spontanée de la Maire, Dominique Voynet, et de quatre de ses opposants. Les cinq locuteurs ont participé à l'enregistrement d'une relecture neutre du corpus de leurs propres extraits de parole (25 stimuli par locuteur) ainsi qu'à l'auto-évaluation perceptive de leurs stimuli (profils émotionnels), dont les résultats ont été comparés à l'évaluation perceptive des extraits par des auditeurs naïfs. Les extraits originaux et relus ont été comparés au niveau de leur structuration prosodico-syntaxique et de leurs caractéristiques temporelles et mélodiques. Les résultats montrent que 1) certains locuteurs semblent plus s'appuyer sur des paramètres mélodiques et d'autres sur des paramètres temporels ; 2) on peut néanmoins dégager les tendances générales concernant les corrélats dans la parole de l'hostilité et de la dominance dans notre corpus : a) des écarts entre structuration syntaxique et prosodique des extraits, b) la réduction ou l'absence d'allongements syllabiques finaux pré-pausaux, c) de fortes variations de plage de variation de F0 de part et d'autre des pauses silencieuses.
Aujourd'hui de plus en plus de données de différents types sont accessibles. L'Analyse Formelle de Concepts (AFC) et les pattern structures sont des systèmes formels qui permettent de traiter les données ayant une structure complexe. Mais le nombre de concepts trouvé par l'AFC est fréquemment très grand. Pour faire face à ce problème, on peut simplifier la représentation des données, soit par projection de pattern structures, soit par introduction de contraintes pour sélectionner les concepts les plus pertinents. Le manuscrit commence avec l'application de l'AFC à l'exploration de structures moléculaires et la recherche de structures particulières. Avec l'augmentation de la taille des ensembles de données, de bonnes contraintes deviennent essentielles. Pour cela on explore la stabilité d'un concept et on l'applique à l'exploration d'un ensemble de données de substances chimiques mutagènes. La recherche de concepts stables dans cet ensemble de données nous a permis de trouver de nouveaux candidats mutagènes potentiels qui peuvent être interprétés par les chimistes. Cependant, pour les cas plus complexes, la représentation simple par des attributs binaires ne suffit pas. En conséquence, on se tourne vers des pattern structures qui peuvent traiter différents types de données complexes. On étend le formalisme original des projections pour avoir plus de liberté dans la manipulation de données. On montre que cette extension est essentielle pour analyser les trajectoires de patients décrivant l'historique de l'hospitalisation des patients. Finalement, le manuscrit se termine par une approche originale et très efficace qui permet de trouver directement des motifs stables.
Dans le Web des données, des graphes de connaissances de plus en plus nombreux sont simultanément publiés, édités, et utilisés par des agents humains et logiciels. Cette large adoption rend essentielles les tâches d'appariement et de fouille. L'appariement identifie des unités de connaissances équivalentes, plus spécifiques ou similaires au sein et entre graphes de connaissances. Cette tâche est cruciale car la publication et l'édition parallèles peuvent mener à des graphes de connaissances co-existants et complémentaires. Cependant, l'hétérogénéité inhérente aux graphes de connaissances (e.g., granularité, vocabulaires, ou complétude) rend cette tâche difficile. Motivés par une application en pharmacogénomique, nous proposons deux approches pour apparier des relations n-aires représentées au sein de graphes de connaissances : une méthode symbolique à base de règles et une méthode numérique basée sur le plongement de graphe. Nous les expérimentons sur PGxLOD, un graphe de connaissances que nous avons construit de manière semi-automatique en intégrant des relations pharmacogénomiques de trois sources du domaine. La tâche de fouille permet quant à elle de découvrir de nouvelles unités de connaissances à partir des graphes de connaissances. Leur taille croissante et leur nature combinatoire entraînent des problèmes de passage à l'échelle que nous étudions dans le cadre de la fouille de patrons de chemins. Nous proposons également l'annotation de concepts, une méthode d'amélioration des graphes de connaissances qui étend l'Analyse Formelle de Concepts, un cadre mathématique groupant des entités en fonction de leurs attributs communs. Au cours de tous nos travaux, nous nous sommes particulièrement intéressés à tirer parti des connaissances de domaines formalisées au sein d'ontologies qui peuvent être associées aux graphes de connaissances. Nous montrons notamment que, lorsqu'elles sont prises en compte, ces connaissances permettent de réduire l'impact des problèmes d'hétérogénéité et de passage à l'échelle dans les tâches d'appariement et de fouille.
Ce travail de thèse vise à combler le fossé entre les domaines de la sémantique du Web et de la physique des particules expérimentales. En prenant comme cas d'utilisation un type spécifique d'expérience de physique, les expériences d'irradiation utilisées pour tester la résistance des composants au rayonnement, un modèle de domaine, ce qui, dans le domaine de la sémantique du Web, est appelé ontologie, a été créé pour décrire les principaux concepts de la gestion des données des expériences d'irradiation.
Cette thèse débute par l'étude d'architectures profondes à noyaux pour les données complexes. L'une des clefs du succès des algorithmes d'apprentissage profond est la capacité des réseaux de neurones à extraire des représentations pertinentes. Cependant, les raisons théoriques de ce succès nous sont encore largement inconnues, et ces approches sont presque exclusivement réservées aux données vectorielles. L'architecture proposée consiste à remplacer les blocs élémentaires des réseaux usuels par des fonctions appartenant à des vv-RKHSs. Bien que très différents à première vue, les espaces fonctionnels ainsi définis sont en réalité très similaires, ne différant que par l'ordre dans lequel les fonctions linéaires/non-linéaires sont appliquées. En plus du contrôle théorique sur les couches, considérer des fonctions à noyau permet de traiter des données structurées, en entrée comme en sortie, étendant le champ d'application des réseaux aux données complexes. Nous conclurons cette partie en montrant que ces architectures admettent la plupart du temps une paramétrisation finie-dimensionnelle, ouvrant la voie à des méthodes d'optimisation efficaces pour une large gamme de fonctions de perte. La seconde partie de cette thèse étudie des alternatives à la moyenne empirique comme substitut de l'espérance dans le cadre de la Minimisation du Risque Empirique (Empirical Risk Minimization, ERM). En effet, l'ERM suppose de manière implicite que la moyenne empirique est un bon estimateur. Cependant, dans de nombreux cas pratiques (e.g. données à queue lourde, présence d'anomalies, biais de sélection), ce n'est pas le cas. La Médiane-des-Moyennes (Median-of-Means, MoM) est un estimateur robuste de l'espérance construit comme suit : des moyennes empiriques sont calculées sur des sous-échantillons disjoints de l'échantillon initial, puis est choisie la médiane de ces moyennes. Nous proposons et analysons deux extensions de MoM, via des sous-échantillons aléatoires et/ou pour les U-statistiques. Il est ainsi prouvé que la minimisation d'un estimateur MoM (aléatoire) est robuste aux anomalies, tandis que les méthodes de tournoi MoM sont étendues au cas de l'apprentissage sur les paires. Enfin, nous proposons une méthode d'apprentissage permettant de résister au biais de sélection.
De nos jours, les réseaux sociaux ont considérablement changé la façon dont les personnes prennent des photos qu'importe le lieu, le moment, le contexte. Plus que 500 millions de photos sont partagées chaque jour sur les réseaux sociaux, auxquelles on peut ajouter les 200 millions de vidéos échangées en ligne chaque minute. Plus particulièrement, avec la démocratisation des smartphones, les utilisateurs de réseaux sociaux partagent instantanément les photos qu'ils prennent lors des divers événements de leur vie, leurs voyages, leurs aventures, etc. Partager ce type de données présente un danger pour la vie privée des utilisateurs et les expose ensuite à une surveillance grandissante. Ajouté à cela, aujourd'hui de nouvelles techniques permettent de combiner les données provenant de plusieurs sources entre elles de façon jamais possible auparavant. Cependant, la plupart des utilisateurs des réseaux sociaux ne se rendent même pas compte de la quantité incroyable de données très personnelles que les photos peuvent renfermer sur eux et sur leurs activités (par exemple, le cas du cyberharcèlement). Premièrement, nous fournissons un framework capable de mesurer le risque éventuel de ré-identification des personnes et d'assainir les documents multimédias destinés à être publiés et partagés. Deuxièmement, nous proposons une nouvelle approche pour enrichir le profil de l'utilisateur dont on souhaite préserver l'anonymat. Pour cela, nous exploitons les évènements personnels à partir des publications des utilisateurs et celles partagées par leurs contacts sur leur réseau social. Nous décrivons les expérimentations que nous avons menées sur des jeux de données réelles et synthétiques. Les résultats montrent l'efficacité de nos différentes contributions.
L'utilisation massive de l'Internet et les ordinateurs ont changé plusieurs aspects de notre vie quotidienne et la façon que nous postulons pour un travail n'y fait pas exception. Depuis les 15 dernières années, les chercheurs du Traitement de la Langue Naturelle ont étudié comment améliorer les performances des recruteurs avec l'aide du recrutement électronique. Beaucoup de systèmes ont été développés dans ce domaine, depuis les moteurs de recherche de candidats ou de postes jusqu'au classement automatique de candidats. Dans ce dernier cas, les systèmes développés font, pour la plupart, la comparaison entre les CV des candidats et les offres d'emploi. Seul un système utilise les CV de processus de sélection relevant du passé pour classer les candidats à un nouveau poste. Dans le cadre de cette thèse, nous avons étudié la possibilité et la façon d'utiliser les CV, sans avoir à exploiter aucun processus de sélection précédent, pour développer nouvelles méthodes applicables aux systèmes de recrutement électronique. Plus spécifiquement, nous commençons par le traitement automatique d'un grand ensemble de CV utilisés pendant des processus réels de recrutement et sélection. Ensuite, nous analysons et appliquons différentes mesures de proximité pour savoir lesquelles sont les plus appropriées pour étudier les CV des candidats. Après, nous introduisons une méthode innovante qui repose sur le Relevance Feedback et l'utilisation de mesures de proximité seulement sur les CV pour pouvoir classer les candidats d'un poste. Dans cette thèse, nous montrons que les CV contiennent assez d'information sur le processus de sélection pour pouvoir classer les candidats. Néanmoins, il est important de choisir correctement les mesures de proximité à utiliser. D'ailleurs, nous présentons des résultats intéressants de la triple comparaison entre les CV et les offres d'emploi. Les résultats obtenus dans cette thèse forment une base pour la conception de nouveaux prototypes de systèmes de recrutement électronique et possiblement le début d'une nouvelle façon pour les développer.
Les premiers documents attestant l'utilisation d'une chaise à roues utilisée pour transporter une personne avec un handicap datent du 6ème siècle en Chine. À l'exception des fauteuils roulants pliables X-frame inventés en 1933, 1400 ans d'évolution de la science humaine n'ont pas changé radicalement la conception initiale des fauteuils roulants. Pendant ce temps, les progrès de l'informatique et le développement de l'intelligence artificielle depuis le milieu des années 1980 ont conduit inévitablement à la conduite de recherches sur des fauteuils roulants intelligents. Plutôt que de se concentrer sur l'amélioration de la conception sous-jacente, l'objectif principal de faire un fauteuil roulant intelligent est de le rendre le plus accessible. Même si l'invention des fauteuils roulants motorisés ont partiellement atténué la dépendance d'un utilisateur à d'autres personnes pour la réalisation de leurs actes quotidiens, certains handicaps qui affectent les mouvements des membres, le moteur ou la coordination visuelle, rendent impossible l'utilisation d'un fauteuil roulant électrique classique. L'accessibilité peut donc être interprétée comme l'idée d'un fauteuil roulant adaptée à la pathologie de l'utilisateur de telle sorte que il / elle soit capable d'utiliser les outils d'assistance. S'il est certain que les robots intelligents sont prêts à répondre à un nombre croissant de problèmes dans les industries de services et de santé, il est important de comprendre la façon dont les humains et les utilisateurs interagissent avec des robots afin d'atteindre des objectifs communs. En particulier dans le domaine des fauteuils roulants intelligents d'assistance, la préservation du sentiment d'autonomie de l'utilisateur est nécessaire, dans la mesure où la liberté individuelle est essentielle pour le bien-être physique et social. De façon globale, ce travail vise donc à caractériser l'idée d'une assistance par contrôle partagé, et se concentre tout particulièrement sur deux problématiques relatives au domaine de la robotique d'assistance appliquée au fauteuil roulant intelligent, à savoir une assistance basée sur la vision et la navigation en présence d'humains. En ciblant les tâches fondamentales qu'un utilisateur de fauteuil roulant peut avoir à exécuter lors d'une navigation en intérieur, une solution d'assistance à bas coût, basée vision, est conçue pour la navigation dans un couloir. Le système fournit une assistance progressive pour les tâches de suivi de couloir et de passage de porte en toute sécurité. L'évaluation du système est réalisée à partir d'un fauteuil roulant électrique de série et robotisé. A partir de la solution plug and play imaginée, une formulation adaptative pour le contrôle partagé entre l'utilisateur et le robot est déduite. De plus, dans la mesure où les fauteuils roulants sont des dispositifs fonctionnels qui opèrent en présence d'humains, il est important de considérer la question des environnements peuplés d'humains pour répondre de façon complète à la problématique de la mobilité en fauteuil roulant. En s'appuyant sur les concepts issus de l'anthropologie, et notamment sur les conventions sociales spatiales, une modélisation de la navigation en fauteuil roulant en présence d'humains est donc proposée. De plus, une stratégie de navigation, qui peut être intégrée sur un robot social (comme un fauteuil roulant intelligent), permet d'aborder un groupe d'humains en interaction de façon équitable et de se joindre à eux de façon socialement acceptable. Enfin, à partir des enseignements tirés des solutions proposées d'aide à la mobilité en fauteuil roulant, nous pouvons formaliser mathématiquement un contrôle adaptatif partagé pour la planification de mouvement relatif à l'assistance à la navigation. La validation de ce formalisme permet de proposer une structure générale pour les solutions de navigation assistée en fauteuil roulant et en présence d'humains.
La reconnaissance des entités nommées et une discipline cruciale du domaine du TAL. Elle sert à l'extraction de relations entre entités nommées, ce qui permet la construction d'une base de connaissance (Surdeanu and Ji, 2014), le résumé automatique (Nobata et al., 2002), etc... Nous nous intéressons ici aux phénomènes de structurations qui les entourent. Nous distinguons ici deux types d'éléments structurels dans une entité nommée. Les premiers sont des sous-chaînes récurrentes, que nous appelerons les affixes caractéristiques d'une entité nommée. Le second type d'éléments est les tokens ayant un fort pouvoir discriminant, appelés des tokens déclencheurs. Nous détaillerons l'algorithme que nous avons mis en place pour extraire les affixes caractéristiques, que nous comparerons à Morfessor (Creutz and Lagus, 2005b). Nous appliquerons ensuite notre méthode pour extraire les tokens déclencheurs, utilisés pour l'extraction d'entités nommées du Français et d'adresses postales. Nous proposons un type de cascade d'étiqueteurs linéaires qui n'avait jusqu'à présent jamais été utilisé pour la reconnaissance d'entités nommées, généralisant les approches précédentes qui ne sont capables de reconnaître des entités de profondeur finie ou ne pouvant modéliser certaines particularités des entités nommées structurées.
Didactique du discours : le français langue d'écrit universitaire en Algérie. Étude contrastive entre filières scientifiques et sciences humaines » aborde la question de l'enseignement et/ou l'apprentissage des langues étrangères en Algérie à travers les caractéristiques du genre scientifique. Il s'est agi de savoir si ce genre conserve sa stabilité dans l'écrit universitaire quand il est question de pratique d'une langue étrangère, en l'occurrence, le français. La langue française n'existe pas pour elle-même. Elle est la langue de scolarité à l'université algérienne et pose entre autres causes un obstacle à la réussite. Se basant sur un corpus hétérogène constitué de douze mémoires d'études soutenus en Algérie, nous voudrions contraster, parmi ces rédactions, l'écrit des sciences humaines et sociales avec l'écrit des sciences dures et de la nature. L'application d'une démarche assistée par Hyperbase, un logiciel de traitement automatique des langues, est nécessaire vu la taille volumineuse du corpus. De plus, elle continue à développer de nouvelles techniques pour la recherche scientifique. Afin de nous familiariser au contexte de l'usage de la langue français en Algérie, nous avons effectué une enquête par questionnaire auprès des apprenants et des enseignants universitaires algériens. Le résultat principal obtenu de cette recherche montre que le genre est toujours dominant même dans un cadre précis de l'usage d'une langue étrangère.
Cette thèse a pris la forme d'un partenariat entre l'équipe VORTEX du laboratoire de recherche en informatique IRIT et l'entreprise Andil, spécialisée dans l'informatique pour l'e-learning. Ce partenariat est conclu autour d'une thèse CIFRE, dispositif soutenu par l'État via l'ANRT. La doctorante, Angela Bovo, a travaillé au sein de l'Université Toulouse 1 Capitole. Un partenariat a également été noué avec l'institut de formation Juriscampus, qui nous a fourni des données issues de formations réelles pour nos expérimentations. Notre objectif principal avec ce projet était d'améliorer les possibilités de suivi des étudiants en cours de formation en ligne pour éviter leur décrochage ou leur échec. Nous avons proposé des possibilités de suivi par apprentissage automatique classique en utilisant comme données les traces d'activité des élèves. Nous avons également proposé, à partir de nos données, des indicateurs de comportement des apprenants. Avec Andil, nous avons conçu et réalisé une application web du nom de GIGA, déjà commercialisée et appréciée par les responsables de formation, qui implémente ces propositions et qui a servi de base à de premières expériences de partitionnement de données qui semblent permettre d'identifier les étudiants en difficulté ou en voie d'abandon. Les implémentations ne sont toutefois pas encore parvenues à produire des résultats probants.
Notre étude s'intéresse à un aspect phonologique : le comportement du phonème /R/ du français parlé à Niamey, capitale du Niger, pays de l'Afrique subsaharienne. Nos enquêtes de terrain ont été effectuées dans une ville où le français demeure la langue officielle, et où l'on retrouve aussi d'autres langues nationales et/ou locales (haousa, songhaï-zarma, touareg, peul, kanuri et arabes). Une présentation des rhotiques du point de vue phonétique et phonologique s'est avéré nécessaire, avant la classification et l'analyse de nos données. D'une part nous avons analysé les allophones de /R/ réalisés par les enquêtés. Ces analyses montrent que la réalisation largement majoritaire est la vibrante alvéolaire [r], suivie de loin par la fricative uvulaire [ʁ], puis par les réalisations [ɰ], [χ], [ɻ] et la non-réalisation de /R/ [ø]. Tous ces résultats ont été comparés ensuite à ceux d'autres points d'enquêtes PFC dans le monde. D'autre part, nous nous sommes intéressés à la chute de /R/ dans les groupes consonantiques et en finales, pour aboutir à la conclusion que ce phénomène dépend du lexique et, plus exactement, concerne généralement la prononciation des chiffres (par exemple quatre [katR]&gt ; 
La microcirculation désigne le sous-ensemble du système circulatoire où s'effectuent les échanges gazeux et liquidiens extracellulaires. Elle est composée des artérioles, des capillaires et des veinules. Les objectifs de ce travail sont d'étudier, de comprendre et d'identifier de nouvelles étiologies iatrogènes à ces pathologies microvasculaires, ainsi que d'évaluer et de comparer l'efficacité et la sécurité des traitements utilisés dans ces pathologies. Nous avons, à cette fin, réalisé plusieurs études à partir des bases de données de pharmacovigilances, de données d'essais cliniques et de la littérature. Ce travail de thèse nous a permis d'explorer le rôle des médicaments dans ces pathologies microvasculaires, champs qui restait encore peu étudié dans la littérature. Ces travaux nous ont permis d'identifier de nombreuses classes pharmacologiques dont le rôle était encore non décrit dans ces pathologies. L'étude des mécanismes pharmacologiques à l'origine de ces effets indésirables permet également d'émettre de nouvelles hypothèses physiopathologiques à l'origine de ces maladies. Les traitements utilisés dans ces différentes pathologies microcirculatoires sont à l'heure actuelle encore peu spécifiques et des travaux de recherche important doivent encore être réalisés afin de personnaliser la prise en charge des patients.
Notre thèse s'inscrit dans le cadre des langages contrôlés pour le génie logiciel. Elle a pour but de faciliter l'adoption de l'approche par règles métier (ARM) par les entreprises en créant un langage contrôlé en vue d'aider à la spécification des règles métier par les experts métier. Notre solution va permettre de réduire la distance sémantique entre les experts métier et les experts système afin de répondre non seulement au besoin d'intercompréhension entre ces derniers mais aussi pour réaliser un transfert automatique de la description des règles métier vers les systèmes d'information (SI). Ce langage contrôlé que nous avons créé permettra d'assurer en plus la consistance et la traçabilité de ces règles avec leur implantation
Les facteurs humains figurent parmi les causes originelles de trop nombreux accidents, dans les transports, l'industrie ou encore dans les parcours de soins. La formation des équipes interprofessionnelles à la gestion des risques dans un environnement reproduisant fidèlement le contexte professionnel est un enjeu majeur. La motivation de cette thèse est de proposer un environnement virtuel multi-joueurs destiné à la formation à la gestion des risques liés à des défauts de communication ou de prises de décision. Pour cela, une méthode de création de scénarios interactifs destinés à la formation à la gestion des risques a été présentée. L'environnement multi-joueurs interactif s'appuie sur cet ensemble cohérent. Ils permettent aussi le contrôle de la situation pédagogique dans son ensemble. Une méthode à forte valeur d'innovation a aussi été proposée pour structurer le débriefing d'une formation à la gestion des risques.
Le nombre d'objets connectés ne cesse de croître à tel point que des milliards d'objets sont attendus dans un futur proche. L'approche de cette thèse met en place un système de gestion autonomique pour des systèmes à base d'objets connectés, en les combinant avec d'autres services comme par exemple des services météo accessibles sur internet. Des paramètres comme le temps d'exécution ou l'énergie consommée sont aussi considérés afin d'optimiser les choix d'actions à effectuer et de services utilisés. Un prototype concret a été réalisé dans un scénario de ville intelligente et de bus connectés dans le projet investissement d'avenir S2C2.
Les fausses informations se multiplient et se propagent rapidement sur les réseaux sociaux. Dans cette thèse, nous analysons les publications d'un point de vue multimodal entre le texte et l'image associée. Plusieurs études ont été menées durant cette thèse. La première compare plusieurs types de médias présents sur les réseaux sociaux et vise à les discriminer de manière automatique. La second permet la détection et la localisation de modifications dans une image grâce à la comparaison avec une ancienne version de l'image. Enfin, nous nous sommes intéressés à des fusions de connaissances basées sur les prédictions d'autres équipes de recherche afin de créer un système unique.
Une présentation des grands principes de l'intercompréhension multilingue introduit la description des projets de recherche dans l'Union européenne (et en Amérique latine) et de leurs applications, classés selon leurs positionnements réceptif (compréhension) ou réciproque (communication interactive) : ils se caractérisent surtout par l'exploitation de la linguistique de surface ou par celle de l'extralinguistique suivant que les langues sont parentes ou plus éloignées. Un large emprunt est fait aux résultats des travaux sur le traitement automatique des langues (TAL : P. Pognan et D. Lemay.) Une liste des institutions officielles permettra de mieux les approcher. En conclusion, nous proposons parmi les stratégies, les méthodes et les outils explorés, une sélection de ceux que l'on pourrait développer en premier lieu.
Cette thèse s'intéresse à la notion de budget pour étudier des problèmes de complexité (complexité en calculs, tâche complexe pour un agent, ou complexité due à une faible quantité de données). En effet, l'objectif principal des techniques actuelles en apprentissage statistique est généralement d'obtenir les meilleures performances possibles, sans se soucier du coût de la tâche. La notion de budget permet de prendre en compte ce paramètre tout en conservant de bonnes performances. Nous nous concentrons d'abord sur des problèmes de classification en grand nombre de classes : la complexité en calcul des algorithmes peut être réduite grâce à l'utilisation d'arbres de décision (ici appris grâce à des techniques d'apprentissage par renforcement budgétisées) ou à l'association de chaque classe à un code (binaire). Nous nous intéressons ensuite aux problèmes d'apprentissage par renforcement et à la découverte d'une hiérarchie qui décompose une tâche en plusieurs tâches plus simples, afin de faciliter l'apprentissage et la généralisation. Cette découverte se fait ici en réduisant l'effort cognitif de l'agent (considéré dans ce travail comme équivalent à la récupération et à l'utilisation d'une observation supplémentaire).
Nous analyserons en premier lieu l'ensemble des contraintes concourant à l'exercice de l'interprétation en langue des signes pouvant se distinguer de celles généralement observées en interprétation entre langues vocales (nous incluons les langues vocales syntaxiquement très éloignées) telles que les contraintes socio-économiques, les contraintes linguistiques et enfin les contraintes d'espace. Nous procéderons ensuite à une analyse cognitive du processus de l'interprétation en nous référant au modèle d'Efforts de l'interprétation simultanée de Gile (Effort d'Écoute et d'Analyse, Effort de Mémorisation à court terme, Effort de Production, Effort de Coordination de ces trois activités simultanées), et nous chercherons à envisager sa transposition aux langues des signes. Pour mieux comprendre les mécanismes constitutifs du processus, nous observerons particulièrement le concept de scénarisation (Séro-Guillaume, 2008) pour une première analyse de la charge cognitive de l'interprète en action. Cette capacité de représentation synthétique visuelle est-elle plus ou moins grande si on prend en compte le degré d'abstraction du discours, la technicité de l'énoncé, le manque de correspondances lexicales, le contexte de l'interprétation (pédagogique, conférence, etc.), la préparation ? Notre analyse du processus se base sur un corpus constitué de plusieurs études empiriques d'interprétations vers la langue des signes : une étude semi-expérimentale, une étude de cas naturaliste et une étude expérimentale, ainsi que sur des interviews d'interprètes et un focus group. Les observations faites sur l'ensemble de ces études nous ont permis de croiser nos données et de dégager les éléments pertinents de nos résultats pour une avancée dans la compréhension du processus cognitif de l'interprétation en langue des signes.
De nombreuses applications en traitement du langage naturel (TALN) reposent sur les représentations de mots, ou “word embeddings”. Ces représentations doivent capturer à la fois de l'information syntaxique et sémantique pour donner des bonnes performances dans les tâches en aval qui les utilisent. Cependant, les méthodes courantes pour les apprendre utilisent des textes génériques comme Wikipédia qui ne contiennent pas d'information sémantique précise. De plus, un espace mémoire important est requis pour pouvoir les sauvegarder car le nombre de représentations de mots à apprendre peut être de l'ordre du million. J'ai développé dict2vec, un modèle qui utilise l'information des dictionnaires linguistiques lors de l'apprentissage des word embeddings. Les word embeddings appris par dict2vec obtiennent des scores supérieurs d'environ 15% par rapport à ceux appris avec d'autres méthodes sur des tâches de similarités sémantiques de mots. La seconde partie de mes travaux consiste à réduire la taille mémoire des word embeddings. J'ai développé une architecture basée sur un auto-encodeur pour transformer des word embeddings à valeurs réelles en vecteurs binaires, réduisant leur taille mémoire de 97% avec seulement une baisse de précision d'environ 2% dans des tâches de TALN en aval.
Notre objectif est de décrire les propriétés formelles des phrases à verbes supports en français et en malais et de les comparer du point de vue syntaxique et sémantique. Nous avons appliqué quatre tests de reconnaissance du verbe support afin de déterminer le statut de ces trois verbes dans les constructions étudiées. Notre étude est structurée en six chapitres. Le deuxième chapitre donne une présentation générale du malais. Le troisième chapitre présente le verbe support buat/membuat (faire) en malais. Le quatrième chapitre présente l'étude du verbe support beri/memberi (donner) en malais. Le cinquième chapitre présente l'étude du verbe support ambil/mengambil (prendre) en malais. Le sixième chapitre présente l'étude comparative des verbes supports faire (membuat), donner (memberi) et prendre (mengambil) en français et en malais. Les résultas obtenus nous ont montré que le français et le malais partagent les mêmes caractéristiques générales des verbes supports. Ces résultats nous ont permis également de montrer que malgré l'universalité du phénomène, chaque langue possède ses propres mécanismes concernant le fonctionnement des verbes supports et le système de la détermination.
Notre travail, vise à construire, à partir d'une méthode d'analyse automatique profonde ou neuronale, des représentations sémantiques de valeurs aspecto-temporelles (associées aux indices linguistiques) du passé composé français.
La tâche d'alignement d'un texte dans une langue source avec sa traduction en langue cible est souvent nommée alignement de bi-textes. Elle a pour but de faire émerger les relations de traduction qui peuvent s'exprimer à différents niveaux de granularité entre les deux faces du bi-texte. De nombreuses applications de traitement automatique des langues naturelles s'appuient sur cette étape afin d'accéder à des connaissances linguistiques de plus haut niveau. Parmi ces applications, nous pouvons citer bien sûr la traduction automatique, mais également l'extraction de lexiques et de terminologies bilingues, la désambigüisation sémantique ou l'apprentissage des langues assisté par ordinateur. La complexité de la tâche d'alignement de bi-textes s'explique par les différences linguistiques entre les langues. Dans le cadre des approches probabilistes, l'alignement de bi-textes est modélisé par un ensemble de variables aléatoires cachés. Afin de réduire la complexité du problème, le processus aléatoire sous-jacent fait l'hypothèse simplificatrice qu'un mot en langue source est lié à au plus un mot en langue cible, ce qui induit une relation de traduction asymétrique. Néanmoins, cette hypothèse est simpliste, puisque les alignements peuvent de manière générale impliquer des groupes de mots dans chacune des langues. Afin de rétablir cette symétrie, chaque langue est considérée tour à tour comme la langue source et les deux alignements asymétriques résultants sont combinés à l'aide d'une heuristique. Cette étape de symétrisation revêt une importance particulière dans l'approche standard en traduction automatique, puisqu'elle précède l'extraction des unités de traduction, à savoir les paires de segments. La spécificité de notre approche consiste à remplacer les heuristiques utilisées par des modèles d'apprentissage discriminant. L'interaction entre les liens d'alignement est alors prise en compte par l'empilement ("stacking'') d'un second modèle prenant en compte la structure à prédire sans pour autant augmenter la complexité globale. Cette formulation peut être vue comme une manière d'apprendre la combinaison de différentes méthodes d'alignement : le modèle considère ainsi l'union des alignements d'entrées pour en sélectionner les liens jugés fiables. Ces améliorations sont mesurées en terme de taux d'erreur sur les alignements et aussi en terme de qualité de traduction via la métrique automatique BLEU.Nous proposons également un modèle permettant à la fois de sélectionner et d'évaluer les unités de traduction extraites d'un bi texte aligné. Ce cadre permet l'utilisation de caractéristiques riches et nombreuses favorisant ainsi une décision robuste. Nous proposons une méthode simple et efficace pour annoter les paires de segments utiles pour la traduction. Le problème d'apprentissage automatique qui se pose alors est particulier, puisque nous disposons que d'exemples positifs. Nous proposons donc d'utiliser l'approche SVM à une classe afin de modéliser la sélection des unités de traduction. Grâce à cette approche, nous obtenons des améliorations significatives en terme de score BLEU pour un système entrainé avec un petit ensemble de données.
Le développement d'un système de reconnaissance de la parole exige la disponibilité d'une grande quantité de ressources à savoir, grands corpus de texte et de parole, un dictionnaire de prononciation. Néanmoins, ces ressources ne sont pas disponibles directement pour des dialectes arabes. De ce fait, le développement d'un SRAP pour les dialectes arabes se heurte à de multiples difficultés à savoir, l''abence de grandes quantités de ressources et l'absence d''une orthographe standard vu que ces dialectes sont parlés et non écrit. Dans cette perspective, les travaux de cette thèse s'intègrent dans le cadre du développement d'un SRAP pour le dialecte tunisien. Une première partie des contributions consiste à développer une variante de CODA (Conventional Orthography for Arabic Dialectal) pour le dialecte tunisien. En fait, cette convention est conçue dans le but de fournir une description détaillée des directives appliquées au dialecte tunisien. Compte tenu des lignes directives de CODA, nous avons constitué notre corpus nommé TARIC : Corpus de l'interaction des chemins de fer de l'arabe tunisien dans le domaine de la SNCFT. Outre ces ressources, le dictionnaire de prononciation s'impose d'une manière indispensable pour le développement d'un SRAP. À ce propos, dans la deuxième partie des contributions, nous visons la création d'un système nommé conversion (Graphème-Phonème) G2P qui permet de générer automatiquement ce dictionnaire phonétique. Toutes ces ressources décrites avant sont utilisées pour adapter un SRAP pour le MSA du laboratoire LIUM au dialecte tunisien dans le domaine de la SNCFT. L'évaluation de notre système donné lieu WER de 22,6% sur l'ensemble de test.
Cet accident est le résultat d'un décalage inédit entre l'état de l'art des heuristiques des ingénieurs de forage et celui des ingénieurs antipollution. Deepwater Horizon est en ce sens un cas d'ingénierie en situation extrême, tel que défini par Guarnieri et Travadel. Nous proposons d'abord de revenir sur le concept général d'accident au moyen d'une analyse linguistique poussée présentant les espaces sémantiques dans lesquels se situe l'accident. Cela permet d'enrichir son « noyau de sens » et l'élargissement de l'acception commune de sa définition. Puis, nous amenons que la revue de littérature doit être systématiquement appuyée par une assistance algorithmique pour traiter les données compte tenu du volume disponible, de l'hétérogénéité des sources et des impératifs d'exigences de qualité et de pertinence. En effet, plus de huit cent articles scientifiques mentionnant cet accident ont été publiés à ce jour et une vingtaine de rapports d'enquêtes, constituant notre matériau de recherche, ont été produits. Notre méthode montre les limites des modèles d'accidents face à un cas comme Deepwater Horizon et l'impérieuse nécessité de rechercher un moyen de formalisation adéquat de la connaissance. De ce constat, l'utilisation des ontologies de haut niveau doit être encouragée. L'ontologie DOLCE a montré son grand intérêt dans la formalisation des connaissances à propos de cet accident et a permis notamment d'élucider très précisément une prise de décision à un moment critique de l'intervention. La population, la création d'instances, est le coeur de l'exploitation de l'ontologie et son principal intérêt mais le processus est encore très largement manuel et non exempts d'erreurs. Cette thèse propose une réponse partielle à ce problème par un algorithme NER original de population automatique d'une ontologie. Enfin, l'étude des accidents n'échappe pas à la détermination des causes et à la réflexion sur les « faits socialement construits » . Cette thèse propose les plans originaux d'un « pipeline sémantique » construit à l'aide d'une série d'algorithmes qui permet d'extraire la causalité exprimée dans un document et de produire un graphe représentant ainsi le « cheminement causal » sous-jacent au document. On comprend l'intérêt pour la recherche scientifique ou industrielle de la mise en lumière ainsi créée du raisonnement afférent de l'équipe d'enquête. Cette thèse est un travail d'assembleur, d'architecte, qui amène à la fois un regard premier sur le cas Deepwater Horizon et propose le forage des données, une méthode et des moyens originaux pour aborder un évènement, afin de faire émerger du matériau de recherche des réponses à des questionnements qui échappaient jusqu'alors à la compréhension.
Notre thèse s'inscrit dans la théorie de l'énonciation à la suite des travaux d'Antoine Culioli et de Jean-Pierre Desclés. Tout énoncé nécessite un énonciateur. Selon que l'énonciateur se réfère ou non à la situation énonciative plusieurs types d'énoncés apparaissent. Nous avons comparé deux types d'énoncés décrits par Aristote avec les représentations établies par Jean-Pierre Desclés dans la Grammaire Applicative et Cognitive. Dans les deux cas, un changement est représenté entre deux situations stables. Dans les deux cas, un mouvement en train de se déployer est décrit dans son inaccomplissement. Nous avons proposé de comprendre la notion d'entéléchie utilisée par Aristote lorsqu'il définit le mouvement, à partir de la notion de processus inaccompli décrite par Jean-Pierre Desclés. Alors que l'énoncé d'un changement nécessite de valider deux états stables pour une unique référence temporelle, l'énoncé d'un mouvement en train de se déployer nécessite que l'énonciateur constate effectivement un mouvement qu'il savait possible. Un paramètre temporel reste commun à ces deux types d'énoncés. L'écriture d'un programme informatique a permis d'extraire automatiquement les valeurs de ce paramètre temporel et de distinguer les différents énoncés d'un corpus textuel numérisé. L'application informatique met en œuvre la description théorique dans un composant logiciel utilisable par d'autres programmes pour organiser temporellement un texte en vue du traitement automatique des langues naturelles.
Le traitement automatique de la langue quechua (TALQ) ne dispose pas actuellement d'un dictionnaire électronique des verbes, du français-quechua. Pourtant, un projet visant la traduction automatique nécessite au préalable, entre autres, cette importante ressource. La réalisation d'un tel dictionnaire peut ouvrir également de nouvelles perspectives dans l'enseignement à distance, dans les domaines de l'accès multilingue aux informations, l'annotation/l'indexation des documents, la correction orthographique et pour le TAL en général. La première difficulté consiste à sélectionner un dictionnaire français comme base de travail. Parmi les nombreux dictionnaires français, il en existe très peu en format électronique, et moins encore ceux dont les sources soient en libre accès au public. Parmi ces derniers, l'ouvrage Les verbes français (LVF), contenant 25 610 sens verbaux, que Jean Dubois et Françoise Dubois-Charlier ont publié chez Larousse en 1997, est un dictionnaire particulièrement complet ; de plus il a l 'avantage d'avoir une licence « open source » et un format compatible avec la plateforme NooJ. En tenant en compte ces considérations nous avons choisi traduire ce dictionnaire en quechua. Cependant, cette tâche se heurte à un obstacle considérable : le lexique quechua de verbes simples compte moins de l 500 entrées. Comment faire correspondre 25 610 sens verbaux français avec seulement 1 500 verbes quechua ? Sommes-nous condamnés à utiliser beaucoup de polysémies ? Par exemple, dans LVF il y a 27 sens verbaux du verbe « tourner » ; doit-on tous les traduire par muyuy ? Ou bien, pouvons-nous utiliser une stratégie particulière et remarquable de la langue pour répondre à ce défi : la génération de nouveaux verbes par dérivation suffixale ? Nous avons inventorié tous les suffixes du quechua qui permettent d'obtenir une forme dérivée possédant le comportement d'un verbe simple. Cet ensemble de suffixes que nous appelons SIP_DRV, contient 27 éléments. Ainsi chaque verbe quechua transitif ou intransitif donne naissance à au moins 27 verbes dérivés. Il reste cependant à formaliser les paradigmes et grammaires qui vont nous permettre d'obtenir les dérivations compatibles avec la morphosyntaxe de la langue. Cela a été réalisé avec NooJ. L'application de ces grammaires nous a permis d'obtenir 40 500 unités linguistiques conjugables (ULAV) à partir de 1 500 verbes simples quechua. Ce résultat encourageant nous permet d'envisager une solution favorable à notre projet de traduction des 25 000 sens verbaux du français en quechua. Afin d'obtenir la traduction de ces ULAV, nous avons besoin d'abord de connaître la modalité d'énonciation qu'apporte chaque SIP quand il s'agglutine au radical verbal pour le transformer. Chaque suffixe peut avoir plusieurs modalités d'énonciation. Nous les avons obtenus à partir du corpus, de notre propre expérience et quelques enregistrements dans le terrain. Nous avons ainsi construit un tableau indexé contenant toutes ces modalités. Ensuite, nous utilisons des opérateurs de NooJ pour programmer les grammaires qui présentent la traduction automatique en une forme glosés de modalités d'énonciation. Finalement, nous avons développé un algorithme qui nous a permis d'obtenir la traduction réciproque du français vers le quechua de plus de 8 500 sens verbaux de niveau 3 et un certain nombre de sens verbaux de niveau 4 et 5.
Les réseaux sociaux, et Twitter en particulier, sont devenus une source d'information privilégiée pour les journalistes ces dernières années. Beaucoup effectuent une veille sur Twitter, à la recherche de sujets qui puissent être repris dans les médias. Cette thèse vise à étudier et à quantifier l'effet de ce changement technologique sur les décisions prises par les rédactions. Par la suite, nous étudions différents types d'algorithmes pour découvrir automatiquement les tweets qui se rapportent aux mêmes événements. Nous testons différentes représentation vectorielles de tweets, en nous intéressants aux représentations vectorielles de texte, et aux représentations texte-image. Enfin, nous concevons un instrument économétrique pour identifier un effet causal de la popularité d'un événement sur Twitter sur sa couverture par les médias traditionnels. Nous montrons que la popularité d'un événement sur Twitter a un effet sur le nombre d'articles qui lui sont consacrés dans les médias traditionnels, avec une augmentation d'environ 1 article pour 1000 tweets supplémentaires.
La synthèse de la parole par corpus (sélection d'unités) est le sujet principal de cette thèse. Tout d'abord, une analyse approfondie et un diagnostic de l'algorithme de sélection d'unités (algorithme de recherche dans le treillis d'unités) sont présentés. L'importance de l'optimalité de la solution est discutée et une nouvelle mise en œuvre de la sélection basée sur un algorithme A* est présenté. Trois améliorations de la fonction de coût sont également présentées. La première est une nouvelle façon – dans le coût cible – de minimiser les différences spectrales en sélectionnant des séquences d'unités minimisant un coût moyen au lieu d'unités minimisant chacune un coût cible de manière absolue. Ce coût est testé pour une distance sur la durée phonémique mais peut être appliqué à d'autres distances. Notre deuxième proposition est une fonction de coût cible visant à améliorer l'intonation en se basant sur des coefficients extraits à travers une version généralisée du modèle de Fujisaki. Les paramètres de ces fonctions sont utilisés au sein d'un coût cible. Enfin, notre troisième contribution concerne un système de pénalités visant à améliorer le coût de concaténation. Il pénalise les unités en fonction de classes reposant sur une hiérarchie du degré de risque qu'un artefact de concaténation se produise lors de la concaténation sur un phone de cette classe. Ce système est différent des autres dans la littérature en cela qu'il est tempéré par une fonction floue capable d'adoucir le système de pénalités pour les unités présentant des coûts de concaténation parmi les plus bas de leur distribution.
La quantité d'informations, de produits et de relations potentielles dans les réseaux sociaux a rendu indispensable la mise à disposition de recommandations personnalisées. L'activité d'un utilisateur est enregistrée et utilisée par des systèmes de recommandation pour apprendre ses centres d'intérêt. Les recommandations sont également utiles lorsqu'estimer la pertinence d'un objet est complexe et repose sur l'expérience. L'apprentissage automatique offre d'excellents moyens de simuler l'expérience par l'emploi de grandes quantités de données. Cette thèse examine le démarrage à froid en recommandation, situation dans laquelle soit un tout nouvel utilisateur désire des recommandations, soit un tout nouvel objet est proposé à la recommandation. En l'absence de données d'intéraction, les recommandations reposent sur des descriptions externes. En optimisation, il est possible d'aborder le choix d'algorithme dans un portfolio d'algorithmes comme un problème de recommandation. Notre première contribution concerne un système à deux composants, un sélecteur et un ordonnanceur d'algorithmes, qui vise à réduire le coût de l'optimisation d'une nouvelle instance d'optimisation tout en limitant le risque d'un échec de l'optimisation. Les deux composants sont entrainés sur les données du passé afin de simuler l'expérience, et sont alternativement optimisés afin de les faire coopérer. Ce système a remporté l'Open Algorithm Selection Challenge 2017.L'appariement automatique de chercheurs d'emploi et d'offres est un problème de recommandation très suivi par les plateformes de recrutement en ligne. Une seconde contribution concerne le développement de techniques spécifiques pour la modélisation du langage naturel et leur combinaison avec des techniques de recommandation classiques afin de tirer profit à la fois des intéractions passées des utilisateurs et des descriptions textuelles des annonces. Une discussion sur la pertinence des différents systèmes de recommandations pour des applications similaires est proposée.
Dans cette thèse, nous étudions deux problèmes d'apprentissage automatique : (I) la détection des communautés et (II) l'appariement adaptatif. I) Il est bien connu que beaucoup de réseaux ont une structure en communautés. La détection de ces communautés nous aide à comprendre et exploiter des réseaux de tout genre. Cette thèse considère principalement la détection des communautés par des méthodes spectrales utilisant des vecteurs propres associés à des matrices choisiesavec soin. Nous faisons une analyse de leur performance sur des graphes artificiels. Au lieu du modèle classique connu sous le nom de « Stochastic Block Model » (dans lequel les degrés sont homogènes) nous considérons un modèle où les degrés sont plus variables : le « Degree-Corrected Stochastic Block Model » (DC-SBM). Nous étudions ce modèle dans deux régimes : le régime dense et le régime « épars » , ou « dilué » . Dans le régime dense, nous prouvons qu'un algorithme basé sur une matrice d'adjacence normalisée réussit à classifier correctement tous les nœuds sauf une fraction négligeable. Dans le régime épars il existe un seuil en termes de paramètres du modèle en-dessous lequel n'importe quel algorithme échoue par manque d'information. En revanche, nous prouvons qu'un algorithme utilisant la matrice « non-backtracking » réussit jusqu'au seuil - cette méthode est donc très robuste. Pour montrer cela nous caractérisons le spectre des graphes qui sont générés selon un DC-SBM dans son régime épars. Nous concluons cette partie par des tests sur des réseaux sociaux. II) Les marchés d'intermédiation en ligne tels que des plateformes de Question-Réponse et des plateformes de recrutement nécessitent un appariement basé sur une information incomplète des deux parties. Nous développons un modèle de système d'appariement entre tâches et serveurs représentant le comportement de telles plateformes. Pour ce modèle nous donnons une condition nécessaire et suffisante pour que le système puisse gérer un certain flux de tâches. Nous introduisons également une politique de « back-pressure » sous lequel le débit gérable par le système est maximal. Nous prouvons que cette politique atteint un débit strictement plus grand qu'une politique naturelle « gloutonne » . Nous concluons en validant nos résultats théoriques avec des simulations entrainées par des données de la plateforme Stack-Overflow.
Nos travaux portent sur les Expressions Numériques Approximatives (ENA), définies comme des expressions linguistiques impliquant des valeurs numériques et un adverbe d'approximation, telles que "environ 100". Nous nous intéressons d'abord à l'interprétation d'ENA non contextualisées, dans ses aspects humain et computationnel. Nous avons ensuite proposé deux modèles d'interprétation, basés sur un même principe de compromis entre la saillance cognitive des bornes des intervalles et leur distance à la valeur de référence de l'ENA, formalisé par un front de Pareto. Leur validation expérimentale à partir de données réelles montre qu'ils offrent de meilleures performances que les modèles existants. Nous avons également montrél'intérêt du modèle flou en l'implémentant dans le cadre des requêtes flexibles de bases de données. Nous avons ensuite montré, par une étude empirique, que le contexte et les interprétations, implicite vs explicite, ont peu d'effet sur les intervalles. Nous nous intéressons enfin à l'addition et à la multiplication d'ENA, par exemple pour évaluer la surface d'une pièce d'"environ 10" par "environ 20 mètres". Nous avons mené une étude dont les résultats indiquent que les imprécisions liées aux opérandes ne sont pas prises en compte lors des calculs.
Ainsi, les contributions principales de cette série de travaux sont centrées autour de 1) l'étude des dépendances spatiales, temporelles, linguistique et du réseau liées aux inégalités et 2) l'inférence du statut socioéconomique à partir de ces signaux multimodaux. De l'autre nous cherchons nous même à fournir des éléments de réponse aux questions posées par les sciences sociales qui se sont avérées trop intractable pour être abordées sans le volume et la qualité de données nécessaires.
Le but essentiel de la thèse est de développer une ontologie juridique bien fondée pour l'utiliser dans le raisonnement à base des règles.
Cette thèse en informatique porte sur la problématique de la capitalisation des processus d'analyse de traces d'apprentissage au sein de la communauté des Learning Analytics (LA). Il s'agit de permettre de partager, adapter et réutiliser ces processus d'analyse de traces. Cela empêche de les partager, mais aussi de les ré-exploiter simplement en dehors de leurs contextes initiaux, quand bien même les nouveaux contextes seraient similaires. L'objectif de cette thèse est de fournir des modélisations et des méthodes permettant la capitalisation des processus d'analyse de traces d'apprentissage, ainsi que d'assister les différents acteurs de l'analyse, notamment durant la phase de réutilisation. Notre deuxième contribution répond au premier verrou lié à la dépendance technique des processus d'analyse actuels, et à leur partage. Nous proposons un méta-modèle qui permet de décrire les processus d'analyse indépendamment des outils d'analyse. Ce méta-modèle formalise la description des opérations utilisées dans les processus d'analyse, des processus eux-mêmes et des traces utilisées, afin de s'affranchir des contraintes techniques occasionnées par ces outils. Ce formalisme commun aux processus d'analyse permet aussi d'envisager leur partage. Il a été mis en œuvre et évalué dans un de nos prototypes. Notre troisième contribution traite le deuxième verrou sur la ré-exploitation des processus d'analyse. Nous proposons un framework ontologique pour les processus d'analyse, qui permet d'introduire de manière structurée des éléments sémantiques dans la description des processus d'analyse. Cette approche narrative enrichit ainsi le formalisme précédent et permet de satisfaire les propriétés de compréhension, d'adaptation et de réutilisation nécessaires à la capitalisation. Cette approche ontologique a été mise en œuvre et évaluée dans un autre de nos prototypes. Enfin, notre dernière contribution répond au dernier verrou identifié et concerne de nouvelles pistes d'assistances aux acteurs, notamment une nouvelle méthode de recherche des processus d'analyse, Nous utilisons également le réseau sémantique sous-jacent à cette modélisation ontologique pour renforcer l'assistance aux acteurs en leur fournissant des outils d'inspection et de compréhension lors de la recherche. Cette assistance a été mise en œuvre dans un de nos prototypes, et évaluée empiriquement.
Située au cœur de l'océan Indien et ayant une superficie de 2 040 km2, la République de Maurice est un pays insulaire qui regroupe quatre îles : Maurice (l'île principale), Rodrigues, Agaléga et Saint-Brandon. Près de 1,3 million de personnes y habitent et constituent ensemble une communauté linguistique plurilingue au sein de laquelle plus de 10 langues sont pratiquées. En dépit de cette richesse linguistique, façonnée par son histoire propice au contact de langues, et des compétences linguistiques plurilingues des Mauriciens, la langue la plus couramment parlée, par 86,50 % de la population mauricienne au sein du domicile familial (selon le recensement ministériel de 2011), est le mauricien : une langue créole à base française. Les études sur le créole mauricien ont débuté pendant la période de colonisation. Au XIXe siècle, Baissac (1880) a proposé une « étude sur le patois créole mauricien » et au XXe siècle, après l'indépendance en mars 1968, Baker (1972) a publié un ouvrage sur sa description linguistique. La seule grammaire contemporaine disponible est celle de Police-Michel et al. (2012). Diverses thèses ont aussi été soutenues sur les catégories syntaxiques du créole mauricien notamment sur le nom (Alleesaib, 2012), le verbe (Henri, 2010) et l'adverbe (Hassamal, 2017). Toutefois, parmi tous les travaux recensés, aucun ne s'est purement intéressé au domaine et à la thématique du Traitement Automatique des Langues (TAL). David (2019) a réalisé un travail de recherche sur l'étiquetage morphosyntaxique du créole mauricien, mais globalement les travaux en traitement automatique demeurent lacunaires pour cette langue peu dotée informatiquement. À partir de ces travaux en syntaxe ainsi que des méthodes et des outils de TAL développés par David (2019), cette thèse vise à construire et à exploiter un corpus arboré (un corpus écrit annoté syntaxiquement) pour le créole mauricien, à l'exemple du French Treebank pour le français (Abeillé et al., 2003) et du Penn Treebank pour l'anglais (Taylor et al., 2003). La démarche méthodologique envisagée dans le cadre de ce travail se regroupera autour de 5 principales phases. D'abord, la première phase s'attèlera à la constitution, à la normalisation, et à la structuration d'un corpus électronique écrit. Les outils informatiques indispensables à ce travail seront développés ou réadaptés au cours de la deuxième phase. La troisième phase se consacrera à l'annotation du corpus à partir d'un schéma d'annotation cohérent et structuré. Les expérimentations seront menées durant la quatrième phase. Enfin, la cinquième phase s'occupera d'évaluer la qualité des annotations réalisées et la performance des outils développés. Finalement, l'objectif ultime de cette thèse est de parvenir à la réalisation d'analyses syntaxiques automatiques en créole mauricien, tout en dotant cette langue de tous les éléments nécessaires à son traitement automatique, en particulier dans une perspective d'analyse syntaxique.
L'objectif de cette thèse a été de proposer une approche robuste pour traiter le problème de la recherche de la réponse précise à une question. Notre première contribution a été la conception et la mise en oeuvre d'un modèle de représentation robuste de l'information et son implémentation. Son objectif est d'apporter aux phrases des documents et aux questions de l'information structurelle, composée de groupes de mots typés (segments typés) et de relations entre ces groupes. Ce modèle a été évalué sur différents corpus (écrits, oraux, web) et a donné de bons résultats, prouvant sa robustesse. Notre seconde contribution a consisté en la conception d'une méthode de réordonnancement des candidats réponses retournés par un système de questions-réponses. Cette méthode a aussi été conçue pour des besoins de robustesse, et s'appuie sur notre première contribution. L'idée est de comparer une question et le passage d'où a été extraite une réponse candidate, et de calculer un score de similarité, en s'appuyant notamment sur une distance d'édition. Le réordonnanceur a été évalué sur les données de différentes campagnes d'évaluation. Les résultats obtenus sont particulièrement positifs sur des questions longues et complexes. Ces résultats prouvent l'intérêt de notre méthode, notre approche étant particulièrement adaptée pour traiter les questions longues, et ce quel que soit le type de données. Le réordonnanceur a ainsi été évalué sur l'édition 2010 de la campagne d'évaluation Quaero, où les résultats sont positifs.
Nous sommes intéressé par la reconstitution et la prédiction des séries temporelles multivariées à partir des données partiellement observées et/ou agrégées. Après investiguer le krigeage, qui est une méthode de la litérature de la statistique spatio-temporelle, et une méthode hybride basée sur le clustering des individus, nous proposons un cadre général de reconstitution et de prédiction basé sur la factorisation de matrice nonnégative. Ce cadre prend en compte de manière intrinsèque la corrélation entre les séries temporelles pour réduire drastiquement la dimension de l'espace de paramètres. Une fois que le problématique est formalisé dans ce cadre, nous proposons deux extensions par rapport à l'approche standard. La première extension prend en compte l'autocorrélation temporelle des individus. Cette information supplémentaire permet d'améliorer la précision de la reconstitution. La deuxième extension ajoute une composante de régression dans la factorisation de matrice nonnégative. Celle-ci nous permet d'utiliser dans l'estimation du modèle des variables exogènes liées avec la consommation électrique, ainsi de produire des facteurs plus interprétatbles, et aussi améliorer la reconstitution. Sur le côté théorique, nous nous intéressons à l'identifiabilité du modèle, ainsi qu'à la propriété de la convergence des algorithmes que nous proposons. La performance des méthodes proposées en reconstitution et en prédiction est testé sur plusieurs jeux de données de consommation électrique à niveaux d'agrégation différents.
Cette thèse pluridisciplinaire, à l'interface entre informatique et linguistique historique, a pour objet la modélisation computationnelle des changements phonétiques, en s'appuyant sur l'identification et la prédiction de cognats. Les divergences phonologiques entre cognats capturent en effet pour une part les divergences entre l'évolution phonétique de langues apparentées. Le travail se structure en trois étapes principales. La première consiste en la création d'une base de données lexicales étymologiques de taille suffisamment importante pour permettre l'entraînement de modèles neuronaux modélisant les correspondances phonétiques entre langues par le biais de la tâche de prédiction de cognats. La conception et l'entraînement de tels réseaux constitue la deuxième étape du travail. Les réseaux de neurones étudiés sont des réseaux de séquence à séquence, similaires à ceux qui constituent aujourd'hui l'état de l'art en traduction automatique, mais adaptés aux spécificités de ce travail, et notamment au faible volume de données. Le troisième étape consiste en un travail de validation et d'analyse des résultats produits par nos modèles neuronaux, en collaboration avec des linguistes historiques.
Les entités nommées ont été l'objet de nombreuses études durant les années 1990. Il est par exemple intéressant de savoir qu'un texte contient des occurrences des mots « Google » et « Youtube » ; mais l'analyse devient plus intéressante si le système est capable de détecter une relation entre ces deux éléments, voire de les typer comme étant une relation d'achat (Google ayant racheté Youtube en 2006). Notre contribution s'articule autour de deux grands axes : tracer un contour plus précis autour de la définition de la relation entre entités nommées, notamment au regard de la linguistique, et explorer des techniques pour l'élaboration de systèmes d'extraction automatique qui sollicitent des linguistes.
Leur rôle est fondamentale dans de nombreux cadres d'application comme la reconnaissance automatique de la parole, la traduction automatique, l'extraction et la recherche d'information. Ce type de modèle prédit un mot uniquement en fonction des n-1 mots précédents. Pourtant, cette approche est loin d'être satisfaisante puisque chaque mot est traité comme un symbole discret qui n'a pas de relation avec les autres. Ainsi les spécificités du langage ne sont pas prises en compte explicitement et les propriétés morphologiques, sémantiques et syntaxiques des mots sont ignorées. Sa construction repose sur le dénombrement de successions de mots, effectué sur des données d'entrainement. Ce sont donc uniquement les textes d'apprentissage qui conditionnent la pertinence de la modélisation n-gramme, par leur quantité (plusieurs milliards de mots sont utilisés) et leur représentativité du contenu en terme de thématique, époque ou de genre. Cette représentation continue confère aux modèles neuronaux une meilleure capacité de généralisation et leur utilisation a donné lieu à des améliorations significative en reconnaissance automatique de la parole et en traduction automatique. Ainsi par le passé, les modèles neuronaux ont été utilisés soit pour des tâches avec peu de données d'apprentissage, soit avec un vocabulaire de mots à prédire limités en taille. La première contribution de cette thèse est donc de proposer une solution qui s'appuie sur la structuration de la couche de sortie sous forme d'un arbre de classification pour résoudre ce problème de complexité. Le modèle se nomme Structure OUtput Layer (SOUL) et allie une architecture neuronale avec les modèles de classes. La deuxième contribution de cette thèse est d'analyser les représentations continues induites et de comparer ces modèles avec d'autres architectures comme les modèles récurrents. Enfin, la troisième contribution est d'explorer la capacité de la structure SOUL à modéliser le processus de traduction. Les résultats obtenus montrent que les modèles continus comme SOUL ouvrent des perspectives importantes de recherche en traduction automatique.
Cette thèse propose une étude des constructions Verbe causatif + Nom d'émotion (susciter l'étonnement, déclencher l'enthousiasme, attiser la jalousie) selon la méthodologie établie dans le cadre du projet ANR
Avec l'émergence et la prolifération des applications du Web sémantique, de nombreuses et récentes larges bases de connaissances (BC) sont disponibles sur le Web. Ces BC contiennent des entités (nommées) et des faits sur ces entités. Elles contiennent également les classes sémantiques de ces entités et leurs liens mutuels. De plus, plusieurs BC peuvent être interconnectées au niveau entités, formant ainsi le noyau du Web des données liées (ou ouvertes). Une caractérisation essentielle de ces BC est qu'elles contiennent des millions à des billions de triplets RDF incertains. Les causes de cette incertitude sont diverses et multiples. Elle peut résulter de l'intégration de sources de données de différents niveaux de fiabilité ou elle peut être causée par des considérations de préservation de la confidentialité. Aussi, elle peut être due à des facteurs li´es au manque d'informations, à la limitation des équipements de mesures ou à l'évolution d'informations. L'objectif de ce travail de thèse est d'améliorer l'ergonomie et la convivialité des systèmes modernes visant à exploiter des BC entachées d'incertitude. En particulier, ce travail propose des techniques coopératives et intelligentes aidant l'utilisateur dans ses prises de décisions quand ses recherches retournent des résultats insatisfaisants en termes de quantité ou de fiabilité. L'approche proposée pour le traitement de ce problème est guidée par la requête initiale et offre un double avantage : (i) elle permet de fournir une explication sur l'échec de la requête en identifiant les MFS (Minimal Failing Sub-queries) et, (ii) elle permet de calculer des requêtes alternatives appelées XSS (maXimal Succeeding Subqueries),sémantiquement proches de la requête initiale et dont les réponses sont non-vides. Par ailleurs, d'un point de vue utilisateur, cette solution présente un niveau élevé de flexibilité dans le sens o`u plusieurs degrés d'incertitude peuvent être simultanément considérés. L'ensemble de nos propositions ont été validées par une série d'expérimentations portant sur différentes larges bases de connaissances en présence d'incertitude (WatDiv et LUBM). Nous avons aussi utilisé plusieurs Triplestores pour mener nos tests.
Les tables du Lexique-Grammaire, dont le développement a été initié par Gross (1975), constituent un lexique syntaxique très riche pour le français. Elles couvrent diverses catégories lexicales telles que les verbes, les noms, les adjectifs et les adverbes. Cette base de données linguistiques n'est cependant pas directement exploitable informatiquement car elle est incomplète et manque de cohérence. Pour rendre ces tables exploitables, il faut expliciter les propriétés intervenant dans chacune d'entre elles. De plus, un grand nombre de ces propriétés doivent être renommées dans un souci de cohérence. Notre objectif est d'adapter les tables pour les rendre utilisables dans diverses applications de Traitement Automatique des Langues (TAL), notamment l'analyse syntaxique. Nous expliquons les problèmes rencontrés et les méthodes adoptées pour permettre leur intégration dans un analyseur syntaxique. Nous proposons LGExtract, un outil générique pour générer un lexique syntaxique pour le TAL à partir des tables du Lexique-Grammaire. Il est relié à une table globale dans laquelle nous avons ajouté les propriétés manquantes et un unique script d'extraction incluant toutes les opérations liées à chaque propriété devant être effectuées pour toutes les tables. Nous présentons également LGLex, le nouveau lexique syntaxique généré des verbes, des noms prédicatifs, des expressions figées et des adverbes. Ensuite, nous montrons comment nous avons converti les verbes et les noms prédicatifs de ce lexique au format Alexina, qui est celui du lexique Lefff (Lexique des Formes Fléchies du Français) (Sagot, 2010), un lexique morphologique et syntaxique à large couverture et librement disponible pour le français. Ceci permet son intégration dans l'analyseur syntaxique FRMG (French MetaGrammar) (Thomasset et de La Clergerie, 2005), un analyseur profond à large couverture pour le français, basé sur les grammaires d'arbres adjoints (TAG), reposant habituellement sur le Lefff. Cette étape de conversion consiste à extraire l'information syntaxique codée dans les tables du Lexique-Grammaire. Nous présentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous évaluons l'analyseur syntaxique FRMG sur le corpus de référence de la campagne d'évaluation d'analyseurs du français Passage (Produire des Annotations Syntaxiques à Grande Échelle) (Hamon et al., 2008), en comparant sa version basée sur le Lefff avec notre version reposant sur les tables du Lexique-Grammaire converties.
Les systèmes de vidéosurveillance sont des outils importants pour les agences chargées de l'application de la loi dans la lutte contre la criminalité. Les chambres de contrôle de la vidéosurveillance ont deux fonctions principales : surveiller en direct les zones de surveillance et résoudre les infractions en enquêtant les archives. Pour soutenir ces tâches difficiles, plusieurs solutions significatives issues des domaines de la recherche et du marché ont été proposées. Cependant, le manque de modèles génériques et précis pour la représentation du contenu vidéo fait de la construction d'un système intelligent et automatisé capable d'analyser et de décrire des vidéos une tâche ardue. De plus, le domaine d'application montre toujours un écart important entre le domaine de la recherche et les besoins réels, ainsi qu'un manque entre ces besoins réels et les outils d'analyse vidéo dans le marché. Par conséquence, jusqu'à présent dans les systèmes de surveillance conventionnels, la surveillance en direct et la recherche dans des archives reposent principalement sur des opérateurs humains. Cette thèse propose une nouvelle approche pour la description textuelle de contenus importants dans des scènes de vidéosurveillance, basée sur une nouvelle "ontologie VSSD" générique, sans contexte, centrée sur les interactions entre deux objets. L'ontologie proposée est générique, flexible et extensible, dédiée à la description de scènes de vidéosurveillance. Tout en analysant les différentes scènes vidéo, notre approche introduit de nombreux nouveaux concepts et méthodes concernant la médiation et l'action distante, la description synthétique, ainsi qu'une nouvelle façon de segmenter la vidéo et de classer les scènes. Nous introduisons une nouvelle méthode heuristique de distinction entre les objets déformables et non déformables dans les scènes.
Les systèmes de dialogue homme-machine ont pour objectif de permettre un échange oral efficace et convivial entre un utilisateur humain et un ordinateur. Leurs domaines d'applications sont variés, depuis la gestion d'échanges commerciaux jusqu'au tutorat ou l'aide à la personne. Cependant, les capacités de communication de ces systèmes sont actuellement limités par leur aptitude à comprendre la parole spontanée. Nos travaux s'intéressent au module de compréhension de la parole et présentent une proposition entièrement basée sur des approches stochastiques, permettant l'élaboration d'une hypothèse sémantique complète. Nous avons eu recours au formalisme FrameNet pour assurer une généricité maximale à notre représentation sémantique. Le développement d'un système à base de règles et d'inférences logiques nous a ensuite permis d'annoter automatiquement le corpus. Pour parvenir à la représentation sémantique globale complète, nous proposons et évaluons un algorithme de composition d'arbres décliné selon deux variantes. Le module de compréhension construit au cours de ce travail peut être adapté au traitement de tout type de dialogue. Il repose sur une représentation sémantique riche et les modèles utilisés permettent de fournir des listes d'hypothèses sémantiques scorées.
Le volume et la complexité des données générées par les systèmes d'information croissent de façon singulière dans les entrepôts de données. Le domaine de l'informatique décisionnelle (aussi appelé BI) a pour objectif d'apporter des méthodes et des outils pour assister les utilisateurs dans leur tâche de recherche d'information. est alors une tâche ardue, alors que les employés d'une entreprise cherchent généralement à réduire leur charge de travail. Pour faire face à ce constat, le domaine « Enterprise Search » s'est développé récemment, et prend en compte les différentes sources de données appartenant aussi bien au réseau privé d'entreprise qu'au domaine public (telles que les pages Internet). Pourtant, les utilisateurs de moteurs de recherche actuels souffrent toujours de du volume trop important d'information à disposition. Nous pensons que de tels systèmes pourraient tirer parti des méthodes du traitement naturel des langues associées à celles des systèmes de questions/réponses. En effet, les interfaces en langue naturelle permettent aux utilisateurs de rechercher de l'information en utilisant leurs propres termes, et d'obtenir des réponses concises et non une liste de documents dans laquelle l'éventuelle bonne réponse doit être identifiée. Un challenge lors de la construction d'un tel système consiste à interagir avec les différentes applications, et donc avec les langages utilisés par ces applications d'une part, et d'être en mesure de s'adapter facilement à de nouveaux domaines d'application d'autre part. Notre rapport détaille un système de questions/réponses configurable pour des cas d'utilisation d'entreprise, et le décrit dans son intégralité. Dans les systèmes traditionnels de l'informatique décisionnelle, les préférences utilisateurs ne sont généralement pas prises en compte, ni d'ailleurs leurs situations ou leur contexte. Les systèmes état-de-l'art du domaine tels que Soda ou Safe ne génèrent pas de résultats calculés à partir de l'analyse de la situation des utilisateurs. Ce rapport introduit une approche plus personnalisée, qui convient mieux aux utilisateurs finaux. Notre expérimentation principale se traduit par une interface de type search qui affiche les résultats dans un dashboard sous la forme de graphes, de tables de faits ou encore de miniatures de pages Internet. En fonction des requêtes initiales des utilisateurs, des recommandations de requêtes sont aussi affichées en sus, et ce dans le but de réduire le temps de réponse global du système. En ce sens, ces recommandations sont comparables à des prédictions. Notre travail se traduit par les contributions suivantes : tout d'abord, une architecture implémentée via des algorithmes parallélisés et qui prend en compte la diversité des sources de données, à savoir des données structurées ou non structurées dans le cadre d'un framework de questions/réponses qui peut être facilement configuré dans des environnements différents. De plus, une approche de traduction basée sur la résolution de contrainte, qui remplace le traditionnel langage-pivot par un modèle conceptuel et qui conduit à des requêtes multidimensionnelles mieux personnalisées. En outre, en ensemble de patrons linguistiques utilisés pour traduire des questions BI en des requêtes pour bases de données, qui peuvent être facilement adaptés dans le cas de configurations différentes.
Cette thèse propose de s'intéresser au problème de la prédiction en apprentissage statistique sous contrainte de coût, notamment du coût de l'information utilisée par le système de prédiction. Les approches classiques d'apprentissage statistique utilisent généralement le seul aspect de la performance en prédiction pour évaluer la qualité d'un modèle, ignorant le coût potentiel du modèle, par exemple en quantité de données utilisées en apprentissage (nombre d'exemples, nombre d'étiquette, mémoire) ou en inférence (quantité de features -ou caractéristiques-). Nous développons trois modèles qui intègrent pendant l'apprentissage une notion du coût de l'information utilisée pour la prédiction, avec pour objectif de contraindre le coût de la prédiction en inférence. Nous utilisons des méthodes d'apprentissage de représentations avec des architectures type réseau de neurones récurrents et des algorithmes par descente de gradient pour l'apprentissage. La dernière partie du manuscrit s'intéresse au coût lié aux étiquettes, usuellement dénommé apprentissage actif dans la littérature. Nous présentons nos travaux pour une approche nouvelle de ce problème en utilisant le méta-apprentissage ainsi qu'une première instanciation basée sur des réseaux récurrents bi-directionnels.
La vidéosurveillance représente l'un des domaines de recherche privilégiés en vision par ordinateur. Le défi scientifique dans ce domaine comprend la mise en œuvre de systèmes automatiques pour obtenir des informations détaillées sur le comportement des individus et des groupes. En particulier, la détection de mouvements anormaux de groupes d'individus nécessite une analyse fine des frames du flux vidéo. Dans le cadre de cette thèse, la détection de mouvements anormaux est basée sur la conception d'un descripteur d'image efficace ainsi que des méthodes de classification non linéaires. Nous proposons trois caractéristiques pour construire le descripteur de mouvement : (i) le flux optique global, (ii) les histogrammes de l'orientation du flux optique (HOFO) et (iii) le descripteur de covariance (COV) fusionnant le flux optique et d'autres caractéristiques spatiales de l'image. Sur la base de ces descripteurs, des algorithmes de machine learning (machines à vecteurs de support (SVM)) mono-classe sont utilisés pour détecter des événements anormaux. Deux stratégies en ligne de SVM mono-classe sont proposées : la première est basée sur le SVDD (online SVDD) et la deuxième est basée sur une version « moindres carrés » des algorithmes SVM (online LS-OC-SVM)
La notion de métrique joue un rôle clef dans les problèmes d'apprentissage automatique tels que la classification, le clustering et le ranking. L'apprentissage à partir de données de métriques adaptées à une tâche spécifique a suscité un intérêt croissant ces dernières années. Ce domaine vise généralement à trouver les meilleurs paramètres pour une métrique donnée sous certaines contraintes imposées par les données. La métrique apprise est utilisée dans un algorithme d'apprentissage automatique dans le but d'améliorer sa performance. La plupart des méthodes d'apprentissage de métriques optimisent les paramètres d'une distance de Mahalanobis pour des vecteurs de features. Les méthodes actuelles de l'état de l'art arrivent à traiter des jeux de données de tailles significatives. En revanche, le sujet plus complexe des séries temporelles multivariées n'a reçu qu'une attention limitée, malgré l'omniprésence de ce type de données dans les applications réelles. Une importante partie de la recherche sur les séries temporelles est basée sur la dynamic time warping (DTW), qui détermine l'alignement optimal entre deux séries temporelles. L'état actuel de l'apprentissage de métriques souffre de certaines limitations. La plus importante est probablement le manque de garanties théoriques concernant la métrique apprise et sa performance pour la classification. La théorie des fonctions de similarité (ℰ, ϓ, T)-bonnes a été l'un des premiers résultats liant les propriétés d'une similarité à celles du classifieur qui l'utilise. Une deuxième limitation vient du fait que la plupart des méthodes imposent des propriétés de distance, qui sont coûteuses en terme de calcul et souvent non justifiées. Dans cette thèse, nous abordons les limitations précédentes à travers deux contributions principales. La première est un nouveau cadre général pour l'apprentissage conjoint d'une fonction de similarité et d'un classifieur linéaire. Cette formulation est inspirée de la théorie de similarités (ℰ, ϓ, τ)-bonnes, fournissant un lien entre la similarité et le classifieur linéaire. Elle est convexe pour une large gamme de fonctions de similarité et de régulariseurs. Nous dérivons deux bornes de généralisation équivalentes à travers les cadres de robustesse algorithmique et de convergence uniforme basée sur la complexité de Rademacher, prouvant les propriétés théoriques de notre formulation. Notre deuxième contribution est une méthode d'apprentissage de similarités basée sur DTW pour la classification de séries temporelles multivariées. Le problème est convexe et utilise la théorie des fonctions (ℰ, ϓ, T)-bonnes liant la performance de la métrique à celle du classifieur linéaire associé. A l'aide de la stabilité uniforme, nous prouvons la consistance de la similarité apprise conduisant à la dérivation d'une borne de généralisation.
L'objectif de cette recherche est d'étudier un phénomène linguistique qui s'est développé en Tunisie dans le cyber-territoire des réseaux sociaux. Cette étude a deux objectifs principaux :  (1) examiner la langue de la jeunesse tunisienne de la capitale, et en particulier sa réalisation en arabish/arabizi (caractères latines). (2) Construire un corpus d'arabish tunisien annoté avec plusieurs niveaux d'annotation (Translittération, Partie du discours), en utilisant des techniques TAL.
La grammaire électronique est une des ressources les plus importantes pour le traitement automatique des langues naturelles. Parce que le développement manuel d'une grammaire est une tâche coûteuse, beaucoup d'efforts pour le développement automatique de grammaires ont été fournis pendant la décennie dernière. Le développement automatique d'une grammaire signifie qu'un système extrait une grammaire à partir d'un corpus arboré. Les étiquettes syntaxiques et morphologiques du corpus nous permettent d'extraire les traits syntaxiques automatiquement. Pendant les expériences d'extraction, nous modifions le corpus pour améliorer les grammaires extraites et extrayons cinq types de grammaires, donc quatre grammaires lexicalisées et une grammaire lexicalisée avec traits. Les grammaires extraites sont évaluées par la taille, la couverture et l'ambiguïté moyenne. La croissance du nombre de schémas d'arbres n'est pas stabilisée à l'issue de l'extraction, ce qui semble indiquer que la taille du corpus n'es pas suffisante pour atteindre la convergence des grammaires. Cependant le nombre de schémas apparaissant au moins deux fois dans le corpus est quasiment stabilisé à l'issue de l'extraction et le nombre de schémas des grammaires supérieures (celles qui sont extraites après la modification du corpus) est aussi plus stabilisé que les grammaires inférieurs. Nous évaluons notre programme d'extraction en l'appliquant à un autre corpus arboré. Enfin, nous comparons nos grammaires avec celle de Han et al. (2001) écrite à la main.
Depuis quelques décennies, de nombreux scientifiques alertent au sujet de la disparition des langues qui ne cesse de s'accélérer. Face au déclin alarmant du patrimoine linguistique mondial, il est urgent d'agir afin de permettre aux linguistes de terrain, a minima, de documenter les langues en leur fournissant des outils de collecte innovants et, si possible, de leur permettre de décrire ces langues grâce au traitement des données assisté par ordinateur. C'est ce que propose ce travail, en se concentrant sur trois axes majeurs du métier de linguiste de terrain : la collecte, la transcription et l'analyse. Les enregistrements audio sont primordiaux, puisqu'ils constituent le matériau source, le point de départ du travail de description. De plus, tel un instantané, ils représentent un objet précieux pour la documentation de la langue. Les fonctionnalités implémentées permettent d'enregistrer différents types de discours (parole spontanée, parole élicitée, parole lue) et de partager les enregistrements avec les locuteurs. L'application permet, en outre, la construction de corpus alignés " parole source (peu dotée) - parole cible (bien dotée) ", " parole-image", " parole-vidéo " qui présentent un intérêt fort pour les technologies de la parole, notamment pour l'apprentissage non supervisé. Afin de compléter l'aide apportée aux linguistes, nous proposons d'utiliser des techniques de traitement automatique de la langue pour lui permettre de tirer partie de la totalité de ses données collectées. Parmi celles-ci, la RAP peut être utilisée pour produire des transcriptions, d'une qualité satisfaisante, de ses enregistrements. Une fois les transcriptions obtenues, le linguiste peut s'adonner à l'analyse de ses données. Afin qu'il puisse procéder à l'étude de l'ensemble de ses corpus, nous considérons l'usage des méthodes d'alignement forcé. Nous démontrons que de telles techniques peuvent conduire à des analyses linguistiques fines. En retour, nous montrons que la modélisation de ces observations peut mener à des améliorations des systèmes de RAP.
L'asthme résulte de multiples facteurs génétiques et environnementaux et des interactions entre ces facteurs. L'objectif de cette thèse a été de proposer des stratégies d'analyses d'interactions gène-gène et gène-environnement pour identifier de nouveaux gènes associés à l'asthme et l'atopie. Les tests d'interactions entre variants génétiques sont appliqués aux paires de gènes sélectionnées. Ces analyses, menées dans trois études familiales (n=3244), ont permis d'identifier une interaction entre deux gènes (ADGRV1 et DNAH5) impliqués dans la mobilité ciliaire, mécanisme émergent dans l'asthme. Une méta-analyse de GWAS du délai de survenue de l'asthme, menée dans neuf études (n=19348), a permis d'identifier un nouveau locus (16q12) et d'en confirmer quatre autres. Cinq de ces études comportaient des données sur l'exposition au tabagisme passif pendant la petite enfance (ELTS) (n=8273).
Face à la quantité d'information textuelle disponible sur le web en langue arabe, le développement des Systèmes de Recherche d'Information (SRI) efficaces est devenu incontournable pour retrouver l'information pertinente. La plupart des SRIs actuels de la langue arabe reposent sur la représentation par sac de mots et l'indexation des documents et des requêtes est effectuée souvent par des mots bruts ou des racines. Nous apportons quatre contributions au niveau de processus de représentation, d'indexation et de recherche d'information en langue arabe. La première contribution consiste à représenter les documents à la fois par des termes simples et des termes complexes. Cela est justifié par le fait que les termes simples seuls et isolés de leur contexte sont ambigus et moins précis pour représenter le contenu des documents. Ainsi, nous avons proposé une méthode hybride pour l'extraction de termes complexes en langue arabe, en combinant des propriétés linguistiques et des modèles statistiques. Le filtre linguistique repose à la fois sur l'étiquetage morphosyntaxique et la prise en compte des variations pour sélectionner les termes candidats. Pour sectionner les termes candidats pertinents, nous avons introduit une mesure d'association permettant de combiner l'information contextuelle avec les degrés de spécificité et d'unité. Par conséquent, nous étudions plusieurs extensions des modèles existants de RI pour l'intégration des termes complexes. En outre, nous explorons une panoplie de modèles de proximité. Pour la prise en compte des dépendances de termes dans les modèles de RI, nous introduisons une condition caractérisant de tels modèle et leur validation théorique. La troisième contribution permet de pallier le problème de disparité des termes en proposant une méthode pour intégrer la similarité entre les termes dans les modèles de RI en s'appuyant sur les représentations distribuées des mots (RDMs). L'idée sous-jacente consiste à permettre aux termes similaires à ceux de la requête de contribuer aux scores des documents. La dernière contribution concerne l'amélioration des modèles de rétro-pertinence (Pseudo Relevance Feedback PRF). Étant basée également sur les RDM, notre méthode permet d'intégrer la similarité entre les termes d'expansions et ceux de la requête dans les modèles standards PRF. La validation expérimentale de l'ensemble des contributions apportées dans le cadre de cette thèse est effectuée en utilisant la collection standard TREC 2002/2001 de la langue arabe.
Les modèles graphiques probabilistes codent les dépendances cachées entre les variables aléatoires pour la modélisation des données. L'estimation des paramètres est une partie cruciale et nécessaire du traitement de ces modèles probabilistes. Ces modèles très généraux ont été utilisés dans de nombreux domaines tels que la vision par ordinateur, le traitement du signal, le traitement du langage naturel et bien d'autres. Nous nous sommes surtout concentrés sur les modèles log-supermodulaires, qui constituent une partie spécifique des distributions familiales exponentielles, où la fonction potentielle est supposée être négative d'une fonction sous-modulaire. Cette propriété sera très pratique pour le maximum d'estimations a posteriori et d'apprentissage des paramètres. Malgré la restriction apparente des modèles d'intérêt, ils couvrent une grande partie des familles exponentielles, puisqu'il y a beaucoup de fonctions qui sont sous-modulaires, par exemple, les coupes graphiques, l'entropie et autres. Il est bien connu que le traitement probabiliste est un défi pour la plupart des modèles, mais nous avons été en mesure de relever certains des défis au moins approximativement. Dans ce manuscrit, nous exploitons les idées perturb-and-MAP pour l'approximation de la fonction de partition et donc un apprentissage efficace des paramètres. De plus, le problème peut également être interprété comme une tâche d'apprentissage de structure, où chaque paramètre ou poids estimé représente l'importance du terme correspondant. Nous proposons une méthode d'estimation et d'inférence approximative des paramètres pour les modèles où l'apprentissage et l'inférence exacts sont insolubles dans le cas général en raison de la complexité du calcul des fonctions de partition. La première partie de la thèse est consacrée aux garanties théoriques. Étant donné les modèles log-supermodulaires, nous tirons parti de la propriété de minimisation efficace liée à la sous-modularité. En introduisant et en comparant deux limites supérieures existantes de la fonction de partition, nous sommes en mesure de démontrer leur relation en prouvant un résultat théorique. Nous introduisons une approche pour les données manquantes comme sous-routine naturelle de la modélisation probabiliste. Il semble que nous puissions appliquer une technique stochastique à l'approche d'approximation par perturbation et carte proposée tout en maintenant la convergence tout en la rendant plus rapide dans la pratique. La deuxième contribution principale de cette thèse est une généralisation efficace et évolutive de l'approche de l'apprentissage paramétrique. Dans cette section, nous développons de nouveaux algorithmes pour effectuer l'estimation des paramètres pour diverses fonctions de perte, différents niveaux de supervision et nous travaillons également sur l'évolutivité. En particulier, en travaillant principalement avec des coupes graphiques, nous avons pu intégrer différentes techniques d'accélération. Nous traitons d'un problème général d'apprentissage des signaux continus. Dans cette partie, nous nous concentrons sur les représentations de modèles graphiques clairsemés. Nous utilisons des régularisateurs à faible densité commune comme potentiels basés sur les antécédents. Les techniques de débruitage proposées ne nécessitent pas le choix d'un redresseur précis à l'avance. Pour effectuer l'apprentissage de la représentation clairsemée, la communauté utilise souvent les pertes symétriques comme l1, mais nous proposons de paramétrer la perte et d'apprendre le poids de chaque composante de perte à partir des données. C'est possible grâce à l'approche que nous avons proposée dans les sections précédentes. Pour tous les aspects de l'estimation des paramètres mentionnés ci-dessus, nous avons effectué les calculs suivants des expériences nationales visant à approuver l'idée ou à la comparer à des repères existants, et à démontrer sa performance dans la pratique.
Les trois principaux résultats sont les suivants. Tout d'abord, nous étudions les propriétés structurelles des graphes obtenus à partir de bases de connaissances exprimées avec des règles existentielles et nous donnons plusieurs indications sur la manière dont leur génération peut être améliorée. Deuxièmement, nous proposons une technique pour générer un graphe d'argumentation où plusieurs arguments peuvent attaquer collectivement, remplaçant ainsi la relation d'attaque binaire classique et montrons expérimentalement les avantages de cette technique. Troisièmement, nous nous intéressons aux approches fondées sur les classements pour le raisonnement en logique et en argumentation.
L'expansion de la radio et le développement de nouveaux standards enrichissent la diversité et la quantité de données contenues sur les ondes de radiodiffusion. Il devient alors judicieux de développer un moteur de recherches qui aurait la capacité de rendre toutes ces données accessibles comme le font les moteurs de recherche sur internet à l'image de Google. Les possibilités offertes par un tel moteur s'il existe sont nombreuses. Ainsi, le projet SurfOnHertz, qui a été lancé en 2010 et s'est terminé en 2013, avait pour but de mettre au point un navigateur qui serait capable d'indexer les flux audios de toutes les stations radios. Cette indexation se traduirait, entre autres, par de la détection de mots clés dans les flux audios, la détection de publicités, la classification de genres musicaux. Le navigateur une fois mis au point deviendrait le premier moteur de recherches de genre à traiter les contenus radiodiffusés. Relever un tel challenge nécessite d'avoir un dispositif pour capter toutes les stations en cours de diffusion dans la zone géographique concernée, les démoduler et transmettre les contenus audios à un moteur d'indexation. Ainsi, les travaux de cette thèse visent à proposer des architectures numériques portées sur une plateforme SDR pour extraire, démoduler, et mettre à disposition le contenu audio de chacune des stations diffusées dans la zone géographique du récepteur. Vu le grand nombre de standards radio existants aujourd'hui, la thèse porte principalement les standards FM et DRM30. Cependant les méthodologies proposées sont extensibles à d'autres standards. Le choix de ce type de comcomposant est justifié de par les grandes possibilités qu'il offre en termes de parallélisme de traitements, de maitrise de ressources disponibles, et d'embarquabilité. Le développement des algorithmes a été fait dans un souci de minimisation de la quantité de blocs de calculs utilisés. D'ailleurs, bon nombre d'implémentations ont été réalisées sur un Stratix II, technologie aux ressources limitées par rapport aux FPGAs d'aujourd'hui disponibles sur le marché. Cela atteste la viabilité des algorithmes présentés. Les algorithmes proposés opèrent ainsi l'extraction simultanée de tous les canaux radios lorsque les stations ne peuvent occuper que des emplacements uniformément espacés comme la FM en Europe occidentale, et aussi, pour des standards dont la répartition des stations dans le spectre semble plutôt aléatoire comme le DRM30. Une autre partie des discussions porte sur le moyen de les démoduler simultanément.
L'étudiant pourra s'inspirer des 'grounds'de Prawitz (2009), la logique dialogique de Lorenzen (1961) ou la ludique de Girard (2001) et de la 'proof theoretical semantics'de Francez (2015). Cela permet de distinguer des expression de sens voisin comme 'un peu'/ 'peu'ou 'chaque'/ 'tout'. La méthode d'analyse devra s'intégrer à la plateforme Grail d'analyse logique et grammaticale du français à large échelle. Cette approche et ces outils est particulièrement pertinente pour l'analyse automatique d'une argumentation, d'un dialogue argumentatif ou d'un débats.
Dans cette thèse, nous proposons une nouvelle approche WCUM (Web Content and Usage Mining based approach) permettant de relier l'analyse du contenu à l'analyse de l'usage d'un site Web afin de mieux comprendre le comportement général des visiteurs du site. Afin de pallier le problème de détermination du nombre de classes sur les lignes et les colonnes, nous proposons de généraliser certains indices proposés initialement pour évaluer les partitions obtenues par des algorithmes de classification simple, aux algorithmes de classification simultanée. Pour évaluer la performance de ces indices nous proposons un algorithme de génération de biclasses artificielles pour effectuer des simulations et valider les résultats. Des expérimentations sur des données artificielles ainsi qu'une application sur des données réelles ont été réalisées pour évaluer l'efficacité de l'approche proposée.
L'extraction de sous-structures significatives a toujours été un élément clé de l'étude des graphes. Dans le cadre de l'apprentissage automatique, supervisé ou non, ainsi que dans l'analyse théorique des graphes, trouver des décompositions spécifiques et des sous-graphes denses est primordial dans de nombreuses applications comme entre autres la biologie ou les réseaux sociaux. Dans cette thèse, nous cherchons à étudier la dégénérescence de graphe, en partant d'un point de vue théorique, et en nous appuyant sur nos résultats pour trouver les décompositions les plus adaptées aux tâches à accomplir. C'est pourquoi, dans la première partie de la thèse, nous travaillons sur des résultats structurels des graphes à arête-admissibilité bornée, prouvant que de tels graphes peuvent être reconstruits en agrégeant des graphes à degré d'arête quasi-borné. Nous fournissons également des garanties de complexité de calcul pour les différentes décompositions de la dégénérescence, c'est-à-dire si elles sont NP-complètes ou polynomiales, selon la longueur des chemins sur lesquels la dégénérescence donnée est définie. Dans la deuxième partie, nous unifions les cadres de dégénérescence et d'admissibilité en fonction du degré et de la connectivité. Dans ces cadres, nous choisissons les plus expressifs, d'une part, et les plus efficaces en termes de calcul d'autre part, à savoir la dégénérescence 1-arête-connectivité pour expérimenter des tâches de dégénérescence standard, telle que la recherche d'influenceurs. Suite aux résultats précédents qui se sont avérés peu performants, nous revenons à l'utilisation du k-core mais en l'intégrant dans un cadre supervisé, i.e. les noyaux de graphes. Ainsi, en fournissant un cadre général appelé core-kernel, nous utilisons la décomposition k-core comme étape de prétraitement pour le noyau et appliquons ce dernier sur chaque sous-graphe obtenu par la décomposition pour comparaison. Nous sommes en mesure d'obtenir des performances à l'état de l'art sur la classification des graphes au prix d'une légère augmentation du coût de calcul. Enfin, nous concevons un nouveau cadre de dégénérescence de degré s'appliquant simultanément pour les hypergraphes et les graphes biparties, dans la mesure où ces derniers sont les graphes d'incidence des hypergraphes. Cette décomposition est ensuite appliquée directement à des architectures de réseaux de neurones pré-entrainés étant donné qu'elles induisent des graphes biparties et utilisent le core d'appartenance des neurones pour réinitialiser les poids du réseaux. Cette méthode est non seulement plus performant que les techniques d'initialisation de l'état de l'art, mais il est également applicable à toute paire de couches de convolution et linéaires, et donc adaptable à tout type d'architecture.
Le corpus est la matière première de la linguistique informatique et du traitement automatique du langage. Or ces ressources contiennent beaucoup de bruit (menus, publicités, etc.). Le filtrage des données parasites et des répétitions nécessite un nettoyage à grand échelle que les chercheurs font en général à la main. Cette thèse propose un système automatique de constitution de corpus web nettoyés de leur bruit. Le système est évalué sous l'angle de l'efficacité de la suppression du bruit et du temps d'exécution. Nos expérimentations, faites sur quatre langues, sont évaluées à l'aide de notre propre corpus de référence. Nous comparons notre méthode avec trois méthodes traitant du même problème que la nôtre, Nutch, BootCat et JusText. Les performances de notre système sont meilleures pour la qualité d'extraction, même si pour le temps de calcul, Nutch et BootCat dominent.
La modélisation informatique et la simulation sont des activités de plus en plus répandues lors de la conception de systèmes complexes et critiques tels que ceux embarqués dans les avions. Une proposition pour la conception et réalisation d'abstractions compatibles avec les objectifs de simulation est présentée basés sur la théorie de l'informatique, le contrôle et le système des concepts d'ingénierie. Il adresse deux problèmes fondamentaux de fidélité dans la simulation, c'est-à-dire, pour une spécification du système et quelques propriétés d'intérêt, comment extraire des abstractions pour définir une architecture de produit de simulation et jusqu'où quel point le comportement du modèle de simulation représente la spécification du système. Une notion générale de cette fidélité de la simulation, tant architecturale et comportementale, est expliquée dans les notions du cadre expérimental et discuté dans le contexte des abstractions de modélisation et des relations d'inclusion. Une approche semi-formelle basée sur l'ontologie pour construire et définir l'architecture de produit de simulation est proposée et démontrée sur une étude d'échelle industrielle. Une approche formelle basée sur le jeu théorique et méthode formelle est proposée pour différentes classes de modèles des systèmes et des simulations avec un développement d'outils de prototype et cas des études. Les problèmes dans la recherche et implémentation de ce cadre de fidélité sont discutées particulièrement dans un contexte industriel.
Un système informatique ne peut traiter un texte sans que certaines informations, comme les mots ou les phrases, ne soient annotées. Or, à ce jour, aucun système ne réalise automatiquement une annotation parfaite d'un texte. Ce constat fait, une question s'impose : quel système de traitement automatique des langues obtient les meilleures performances, un système qui intégre l'imperfection des annotations dans son processus de raisonnement ou un système prevu pour raisonner à partir d'annotations parfaites mais travaillant avec des annotations imparfaites ? Pour y répondre nous avons proposé un modèle d'inférence probabiliste reposant sur les réseaux bayésiens (RB), un formalisme adapté pour travailler sur des données imparfaites. Nous avons travaillé sur le problème de la résolution du pronom "it" anaphorique dans les textes anglais et validé notre modèle en évaluant deux RB sur des corpus différents : un RB pour la reconnaissances des pronoms impersonnels et un RB pour le choix de l'antécédant.
Stimulée par l'usage intensif des téléphones mobiles, l'exploitation conjointe des données textuelles et des données spatiales présentes dans les objets géotextuels (p. ex. tweets, photos Flickr, critiques de points d'intérêt) Cependant, ces approches traditionnelles se sont révélées peu efficaces face aux textes issus des réseaux sociaux. En effet, ces derniers sont généralement de courte longueur, utilisent des mots non conventionnels ou ambiguës et peuvent difficilement être mis en correspondance avec d'autres documents, notamment à cause de l'inadéquation du vocabulaire. De fait, les approches proposées jusqu'à présent conduisent généralement à de faibles taux de rappel et de précision. Nous proposons ainsi de tirer parti des contextes géographiques et de la sémantique distributionnelle pour résoudre la tâche de prédiction sémantique de l'emplacement. Concernant l'amélioration des représentations de textes, nous proposons une approche de régularisation a posteriori qui intègre l'information spatiale dans l'apprentissage des plongements lexicaux. L'objectif sous-jacent est de révéler d'éventuelles relations sémantiques locales entre les mots, ainsi que la multiplicité des sens d'un même mot.
Cette thèse en informatique s'intéresse à la structuration et à l'exploration de collections journalistiques. Elle fait appel à plusieurs domaines de recherche : sciences sociales, à travers l'étude de la production journalistique ; ergonomie ;  L'hyperliage consiste à construire automatiquement des liens entre documents multimédias. Nous étendons ce concept en l'appliquant à l'entièreté d'une collection afin d'obtenir un hypergraphe, et nous intéressons notamment à ses caractéristiques topologiques et à leurs conséquences sur l'explorabilité de la structure construite. Nous proposons dans cette thèse des améliorations de l'état de l'art selon trois axes principaux : une structuration de collections d'actualités à l'aide de graphes mutli-sources et multimodaux fondée sur la création de liens inter-documents, son association à une diversité importante des liens permettant de représenter la grande variété des intérêts que peuvent avoir différents utilisateurs, et enfin l'ajout d'un typage des liens créés permettant d'expliciter la relation existant entre deux documents. Ces différents apports sont renforcés par des études utilisateurs démontrant leurs intérêts respectifs.
Le Web est en croissance continue, et une quantité énorme de données est générée par les réseaux sociaux, permettant aux utilisateurs d'échanger une grande diversité d'informations. En outre, les textes au sein des réseaux sociaux sont souvent subjectifs. L'exploitation de cette subjectivité présente au sein des textes peut être un facteur important lors d'une recherche d'information. En particulier, cette thèse est réalisée pour répondre aux besoins de la plate-forme Books de Open Edition en matière d'amélioration de la recherche et la recommandation de livres, en plusieurs langues. La plateforme offre des informations générées par des utilisateurs, riches en sentiments. Par conséquent, l'analyse précédente, concernant l'exploitation de sentiment en recherche d'information, joue un rôle important dans cette thèse et peut servir l'objectif d'une amélioration de qualité de la recherche de livres en utilisant les informations générées par les utilisateurs. Par conséquent, nous avons choisi de suivre une voie principale dans cette thèse consistant à combiner les domaines analyse de sentiment (AS) et recherche d'information (RI), dans le but d'améliorer les suggestions de la recherche de livres. Nos objectifs peuvent être résumés en plusieurs points :  • Une approche d'analyse de sentiment, facilement applicable sur différentes langues, peu coûteuse en temps et en données annotées. • De nouvelles approches pour l'amélioration de la qualité lors de la recherche de livres, basées sur l'utilisation de l'analyse de sentiment dans le filtrage, l'extraction et la classification des informations
Les langues en Malaisie meurent à un rythme alarmant. A l'heure actuelle, 15 langues sont en danger alors que deux langues se sont éteintes récemment. Une des méthodes pour sauvegarder les langues est de les documenter, mais c'est une tâche fastidieuse lorsque celle-ci est effectuée manuellement. Un système de reconnaissance automatique de la parole (RAP) serait utile pour accélérer le processus de documentation de ressources orales. Cependant, la construction des systèmes de RAP pour une langue cible nécessite une grande quantité de données d'apprentissage comme le suggèrent les techniques actuelles de l'état de l'art, fondées sur des approches empiriques. L'objectif principal de cette thèse est d'étudier les effets de l'utilisation de données de langues étroitement liées, pour construire un système de RAP pour les langues à faibles ressources en Malaisie. Des études antérieures ont montré que les méthodes inter-lingues et multilingues pourraient améliorer les performances des systèmes de RAP à faibles ressources. Dans cette thèse, nous essayons de répondre à plusieurs questions concernant ces approches : comment savons-nous si une langue est utile ou non dans un processus d'apprentissage trans-lingue ? Comment la relation entre la langue source et la langue cible influence les performances de la reconnaissance de la parole ? Nous étudions les effets de l'utilisation des données du malais, une langue locale dominante qui est proche de l'iban, pour développer un système de RAP pour l'iban, sous différentes contraintes de ressources. Nous proposons plusieurs approches pour adapter les données du malais afin obtenir des modèles de prononciation et des modèles acoustiques pour l'iban. Celui-ci est fondé sur des techniques d'amorçage, pour améliorer la correspondance entre les données du malais et de l'iban. Pour augmenter la performance des modèles acoustiques à faibles ressources, nous avons exploré deux techniques de modélisation : les modèles de mélanges gaussiens à sous-espaces (SGMM) et les réseaux de neurones profonds (DNN). Les résultats montrent que l'utilisation de données du malais est bénéfique pour augmenter les performances des systèmes de RAP de l'iban. Par ailleurs, nous avons également adapté les modèles SGMM et DNN au cas spécifique de la transcription automatique de la parole non native (très présente en Malaisie). Nous avons proposé une approche fine de fusion pour obtenir un SGMM multi-accent optimal. En outre, nous avons développé un modèle DNN spécifique pour la parole accentuée. Les deux approches permettent des améliorations significatives de la précision du système de RAP. De notre étude, nous observons que les modèles SGMM et, de façon plus surprenante, les modèles DNN sont très performants sur des jeux de données d'apprentissage en quantité limités.
Ce travail de thèse prend pour objet d'étude les concepts de référence et de trouble de la référence au regard de l'aspect dynamique, interlocutoire, du processus de co-construction dialogique de la référence. Partant d'une revue critique de la littérature concernant l'abord théorico-méthodologique du processus de co-construction dialogique de la référence, notre première étude se propose de réinterroger l'abord actuel de ce phénomène au regard des aspects structurels, actionnels et représentationnels des interactions verbales. Nos résultats aboutissent à l'établissement d'un nouveau modèle, qualitativement plus sensible. Son potentiel heuristique est d'ailleurs éprouvé au sein de travaux complémentaires, prenant désormais pour objet d'étude le trouble de la référence tel qu'il est susceptible de s'exprimer en interaction verbale schizophrénique. Les résultats obtenus aux travers de deux études fournissent des critères permettant la caractérisation dynamique des contextes d'émergence et d'expression du trouble de la référence. Ils sont par ailleurs en amont d'une interprétation des processus cognitifs impliqués au regard de la théorie de la modularité massive. Par ailleurs, les résultats fournis par une étude supplémentaire font état d'une relative indépendance du trouble quant à l'intensité symptomatologique de la pathologie schizophrénique. Pointant l'intérêt d'un abord dynamique des troubles, la contribution de nos résultats est essentiellement à concevoir au regard des objectifs de l'approche pragmatique en psychopathologie, offrant des pistes de recherche si bien dans le cadre du renouvellement des classifications que dans celui d'une clinique de l'efficience cognitive.
En 2015, le nombre de nouveaux cas de cancer du sein en France s'élève à 54 000. Le taux de survie 5 ans après le diagnostic est de 89 %. Si les traitements modernes permettent de sauver des vies, certains sont difficiles à supporter. De nombreux projets de recherche clinique se sont donc focalisés sur la qualité de vie (QdV) qui fait référence à la perception que les patients ont de leurs maladies et de leurs traitements. La QdV est un critère d'évaluation clinique pertinent pour évaluer les avantages et les inconvénients des traitements que ce soit pour le patient ou pour le système de santé. Dans cette thèse, nous nous intéresserons aux histoires racontées par les patients dans les médias sociaux à propos de leur santé, pour mieux comprendre leur perception de la QdV. Ce nouveau mode de communication est très prisé des patients car associé à une grande liberté du discours due notamment à l'anonymat fourni par ces sites. L'originalité de cette thèse est d'utiliser et d'étendre des méthodes de fouille de données issues des médias sociaux pour la langue Française. Les contributions de ce travail sont les suivantes : (1) construction d'un vocabulaire patient/médecin ; (2) détection des thèmes discutés par les patients ; (3) analyse des sentiments des messages postés par les patients et (4) mise en relation des différentes contributions citées. Dans un premier temps, nous avons utilisé les textes des patients pour construire un vocabulaire patient/médecin spécifique au domaine du cancer du sein, en recueillant divers types d'expressions non-expertes liées à la maladie, puis en les liant à des termes biomédicaux utilisés par les professionnels de la santé. Nous avons combiné plusieurs méthodes de la littérature basées sur des approches linguistiques et statistiques. Pour évaluer les relations obtenues, nous utilisons des validations automatiques et manuelles. Nous avons ensuite transformé la ressource construite dans un format lisible par l'être humain et par l'ordinateur en créant une ontologie SKOS, laquelle a été intégrée dans la plateforme BioPortal. Dans un deuxième temps, nous avons utilisé et étendu des méthodes de la littérature afin de détecter les différents thèmes discutés par les patients dans les médias sociaux et de les relier aux dimensions fonctionnelles et symptomatiques des auto-questionnaires de QdV (EORTC QLQ-C30 et EORTC QLQ-BR23). Afin de détecter les thèmes, nous avons appliqué le modèle d'apprentissage non supervisé LDA avec des prétraitements pertinents. Ensuite, nous avons proposé une méthode permettant de calculer automatiquement la similarité entre les thèmes détectés et les items des auto-questionnaires de QdV. Nous avons ainsi déterminé de nouveaux thèmes complémentaires à ceux déjà présents dans les questionnaires. Ce travail a ainsi mis en évidence que les données provenant des forums de santé sont susceptibles d'être utilisées pour mener une étude complémentaire de la QdV. Dans un troisième temps, nous nous sommes focalisés sur l'extraction de sentiments (polarité et émotions). Pour cela, nous avons évalué différentes méthodes et ressources pour la classification de sentiments en Français. Ces expérimentations ont permis de déterminer les caractéristiques utiles dans la classification de sentiments pour différents types de textes, y compris les textes provenant des forums de santé. Finalement, nous avons utilisé les différentes méthodes proposées dans cette thèse pour quantifier les thèmes et les sentiments identifiés dans les médias sociaux de santé. De manière générale, ces travaux ont ouvert des perspectives prometteuses sur diverses tâches d'analyse des médias sociaux pour la langue française et en particulier pour étudier la QdV des patients à partir des forums de santé.
Le projet de thèse porte sur l'application des approches neuronales pour la représentation de textes et l'appariement de textes en recherche d'information en vue de lever le verrou du fossé sémantique. Notre première contribution comprend des modèles neuronaux pour l'apprentissage en ligne et apprentissage hors ligne des représentations de texte à plusieurs niveaux (mot, sens, document). Ces modèles intègrent les contraintes relationnelles issues des ressources externes par régularisation de la fonction objectif ou par enrichissement sémantique des instances d'apprentissage. Ce réseau apprend à mesurer un score de pertinence entre un document et une requête à partir des vecteurs de représentation en entrée modélisant des objets (concepts, entités) identifiés dans la requêtes et documents et leurs relations issues des ressources externes.
Le développement et la multiplication de dispositifs connectés, en particulier avec les \textit{smartphones}, nécessitent la mise en place de moyens d'authentification. Afin d'améliorer les performances et la fiabilité des authentifications, différentes sources biométriques sont susceptibles d'être utilisées dans un processus de fusion. La biométrie multimodale réalise, en particulier, la fusion des informations extraites de différentes modalités biométriques.
Cette thèse s'inscrit dans le cadre de l'apprentissage profond appliqué à la compréhension de la parole. Jusqu'à présent, cette tâche était réalisée par l'intermédiaire d'une chaîne de composants mettant en oeuvre, par exemple, un système de reconnaissance de la parole, puis différents traitements du langage naturel, avant d'impliquer un système de compréhension du langage sur les transcriptions automatiques enrichies. Récemment, des travaux dans le domaine de la reconnaissance de la parole ont montré qu'il était possible de produire une séquence de mots directement à partir du signal acoustique. Tout d'abord, nous présentons un état de l'art décrivant les principes de l'apprentissage neuronal profond, de la reconnaissance de la parole, et de la compréhension de la parole. Nous décrivons ensuite les contributions réalisées selon trois axes principaux. Nous proposons un premier système répondant à la problématique posée et l'appliquons à une tâche de reconnaissance des entités nommées. Puis, nous proposons une stratégie de transfert d'apprentissage guidée par une approche de type curriculum learning. Cette stratégie s'appuie sur les connaissances génériques apprises afin d'améliorer les performances d'un système neuronal sur une tâche d'extraction de concepts sémantiques. Ensuite, nous effectuons une analyse des erreurs produites par notre approche, tout en étudiant le fonctionnement de l'architecture neuronale proposée. Enfin, nous mettons en place une mesure de confiance permettant d'évaluer la fiabilité d'une hypothèse produite par notre système.
Cette thèse explore l'usage des Grammaires Categorielles Abstraites (CGA) pour la Génération Automatique de Texte (GAT) dans un contexte industriel. Les systèmes GAT basés sur des théories linguistiques ont un long historique, cependant ils sont relativement peu utilisés en industrie, qui préfère les approches plus "pragmatiques", le plus souvent pour des raisons de simplicité et de performance. Cette étude montre que les avancées récentes en linguistique computationnelle permettent de concilier le besoin de rigueur théorique avec le besoin de performance, en utilisant CGA pour construire les principaux modules d'un système GAT de qualité industrielle ayant des performances comparables aux méthodes habituellement utilisées en industrie.
Le travail de recherche consistera à mieux comprendre l'évaluation des actifs verts ainsi que les risques extrêmes qu'ils font porter aux investisseurs par rapport à d'autres actifs financiers. Pour cela, j'utiliserai des techniques d'apprentissage automatique, en particulier d'analyse textuelle pour la labélisation de ces actifs, ou d'apprentissage supervisé pour l'évaluation des primes de risques de ces actifs.
L'évaluation des capacités motrices est une activité essentielle de l'analyse de mouvement. Cette activité permet de quantifier la performance d'un patient et ainsi d'être capable de suivre et de contrôler son évolution pour assurer un traitement adapté. Les kinésithérapeutes ont donc besoin d'outils précis leurs permettant de mesurer cette performance. Pour cela, ils ont développé leurs propres outils basés sur l'observation et des exercices normés. Pourtant, cette activité pourrait être supportée et augmentée par l'utilisation de technologies avancées. Il existe une catégorie d'outils technologiques permettant le suivi et la capture de ces mouvements. Leur utilisation dans des systèmes d'aide à l'évaluation pourrait affiner l'évaluation des thérapeutes et également augmenter sa reproductibilité. Pour assurer l'utilisation dans la durée de ce type d'outils, il est nécessaire de répondre à la question suivante : « quels sont les enjeux et les critères de développement spécifiques aux systèmes d'évaluation des capacités motrices ? » . Dans ce travail de thèse, cette question a été restructurée suivant 3 axes : « comment mesurer les capacités motrices ? » , « comment analyser et communiquer le résultat ? » et enfin « comment intégrer ce système dans la pratique médicale ? » . Pour chacun de ces axes, les critères clés de développement ont été investigués et des contributions sont présentées. Afin d'illustrer ces critères, une étude de cas a été menée : l'instrumentation, à l'aide de nouvelles technologies de capture de mouvements, d'un protocole de mesure de capacités motrices (aussi appelé MFM ou Mesure de la Fonction Motrice).
Le but du Registre National du Cancer (RNC) du Luxembourg est de collecter des données sur le cancer et la qualité des traitements au Luxembourg. Afin d'obtenir des données de haute qualité et comparables avec celles d'autres registres ou pays, le RNC suit les règles et standards internationaux de codification comme la Classification International des Maladies pour l'Oncologie (COM-O). Ces standards sont complexes et considérables, compliquant fortement le processus de collecte des données. Les encodeurs en charge de la collecte des données sont souvent confrontés à des situations dans lesquelles des données sont manquantes ou contradictoires, les empêchant d'appliquer les règles fournies. Pour les aider dans leur tâche, les exports de codification du RNC répondent aux questions de codage des encodeurs. Cependant, ces réponses requièrent beaucoup de temps des experts. Le but de ce projet est de réduire le temps d'expert nécessaire et de faciliter le travail des encodeurs. D'un point de vue scientifique, cette thèse s'intéresse au problème de synthèse d'informations à partir d'un ensemble de données provenant de différentes sources avec des contraintes et recommandations à respecter. Le raisonnement à partir de cas est utilisé pour résoudre ce problème car cette méthodologie ressemble à cette employée par les experts. La méthode de résolution conçue utilise des arguments fournis par les experts de codification dans le cadre de questions posées précédemment par les encodeurs. Ce document décrit comment ces arguments servent à identifier des questions similaires et à expliquer la réponse aux encodeurs et aux experts. Une évaluation préliminaire a été réalisée pour évaluer la performance de la méthode et identifier des pistes d'améliorations. Dans un premier temps, le travail produit porte sur les registres du cancers et la codification médicale, cependant l'approche est généralisable à d'autres domaines.
Le secteur médical est un domaine dynamique en constante évolution, nécessitant des améliorations continues de ses processus métier et une assistance intelligente aux acteurs impliqués. Ce travail de thèse se focalise sur le processus de soins nécessitant l'implantation d'une prothèse. La particularité de ce processus est qu'il met en interaction deux cycles de vie appartenant respectivement au domaine médical et celui de l'ingénierie. Ceci implique plusieurs actions de collaboration entre des acteurs métier très variés. Cependant, des problèmes de communication et de partage de connaissances peuvent exister en raison de l'hétérogénéité de la sémantique utilisée et des pratiques métiers propres à chaque domaine. Pour se faire, un cadre conceptuel est proposé pour analyser les connexions entre les cycles de vie de maladie (domaine Médical)et de la prothèse (domaine d'ingénierie). Sur la base de cette analyse, un modèle sémantique sous forme d'une ontologie pour le domaine médical est définit dans le cadre de la construction d'une approche PLM à base de connaissances. L'application de cette proposition est démontrée à travers l'implémentation de quelques fonctions utiles dans un outil PLM du marché nommé AUDROS.
Notre thèse vise à illustrer la réalisation et l'utilisation d'un formalisme (architecture à plusieurs modules) de traitement de la référence à des fins de recherches en traitement automatique de l'oral. Nous ne prétendons pas avoir trouvé une solution miracle pour la résolution de la référence. Nous supposons seulement que le traitement des structures coréférentielles orales nécessite la mise en pratique de formalismes standards aussi complexe que la théorie du gouvernement et du liage. La langue orale à sa propre logique mais les formes standards de la langue y apparaissent largement
Le traitement automatique des langues et plus particulièrement la compréhension automatique de documents a pour objectif de proposer des méthodes permettant d'en extraire les informations pertinentes. Les approches les plus efficaces aujourd'hui font appel à des méthodes d'apprentissage artificel supervisé et à de très grandes quantités d'exemple annotés manuellement. Ce sujet de thèse propose de répondre à deux problématiques : 1) comment générer des données synthétiques ?
L'objectif de cette thèse est de proposer un système de recommandations permettant aux grands distributeurs d'améliorer leurs assortiments de produits distribués à travers de nombreux points de vente. Dans ce contexte, la problématique adressée est celle de la planification d'assortiment qui consiste à éliciter les meilleurs produits, e.g., ceux faisant le plus de chiffre d'affaires. Pour ce faire, nous proposons dans un premier temps une comparaison des méthodes pragmatiques mises en place dans l'industrie avec l'état de l'art associé à la planification d'assortiment. Cette comparaison permet de mettre en lumière le problème de transversalité des connaissances utilisées aujourd'hui pour améliorer l'assortiment. Pour pallier ce problème, nous proposons des structures de connaissances propres à la grande distribution. Grâce à ces structures, une méthode Agile d'optimisation de l'assortiment pouvant être intégrée dans un processus d'amélioration continue est formalisée. Cette méthode permet d'intégrer l'expertise humaine, que nous considérons comme indispensable, aux différents leviers actuellement adoptés. Pour souligner la modularité de notre approche, nous proposons ensuite une analyse sémantique des magasins qui, en plus d'améliorer la précision de nos simulations, permet de définir un nouvel axe d'amélioration de l'assortiment. Cette analyse se base sur les structures de connaissances propres à chaque enseigne et sur les mesures de similarités sémantiques. Enfin, pour perfectionner notre méthode et aller plus loin dans l'exploitation ces structures, nous proposons une analyse sémantique des consommateurs qui sont les cibles finales de l'assortiment. Cette seconde analyse sémantique permet d'apporter de nouvelles connaissances pour les distributeurs et d'apporter de nouvelles contraintes sur les assortiments. En parallèle de ces contributions scientifiques, différentes applications ont été développées pour souligner l'interopérabilité de nos contributions avec des notions propres à différents types de distributeurs (e.g. Alimentaire, Bricolage...). Ces applications sont présentées dans le manuscrit dans la limite du respect de la confidentialité et de la propriété intellectuelle.
La résolution des pronoms est le processus par lequel un pronom anaphorique est mis en relation avec son antécédent. Les humains en sont capables sans efforts notables en situation normale. En revanche, les systèmes automatiques ont une performance qui reste loin derrière, malgré des algorithmes de plus en plus sophistiqués, développés par la communauté du Traitement Automatique des Langues. La recherche en psycholinguistique a montré à travers des expériences qu'au cours de la résolution de nombreux facteurs sont pris en compte par les locuteurs. Une question importante se pose : comment les facteurs interagissent et quel poids faut-il attribuer à chacun d'entre eux ? Une deuxième question qui se pose alors est comment les théories linguistiques de la résolution des pronoms incorporent tous les facteurs. Nous proposons une nouvelle approche à ces problématiques : la simulation computationnelle de la charge cognitive de la résolution des pronoms. De cette façon, les modèles computationnels représentent une alternative aux expériences classiques avec des items expérimentaux construits manuellement. D'abord, nous avons simulé la charge cognitive des pronoms en utilisant des poids de facteurs de résolution appris sur corpus. Ensuite, nous avons testé si les concepts de la Théorie de l'Information sont pertinents pour prédire la charge cognitive des pronoms. Finalement, nous avons procédé à l'évaluation d'un modèle psycholinguistique sur des données issues d'un corpus enrichi de mouvements oculaires. Les résultats de nos expériences montrent que la résolution des pronoms est en effet multi-factorielle et que l'influence des facteurs peut être estimée sur corpus. Nos résultats montrent aussi que des concepts de la Théorie de l'Information sont pertinents pour la modélisation des pronoms. Nous concluons que l'évaluation des théories sur des données de corpus peut jouer un rôle important dans le développement de ces théories et ainsi amener dans le futur à une meilleure prise en compte du contexte discursif.
Les travaux de cette thèse portent sur les conséquences du développement du numérique sur la pratique de recherche en SHS au sens large et en histoire en particulier. L'introduction du numérique bouleverse les pratiques de recherche en histoire en mettant à disposition du chercheur un grand volume de sources numérisées ainsi que de nombreux outils d'analyse et d'écriture. Si ces nouveaux moyens de recherche permettent à la discipline d'adopter de nouvelles approches et de renouveler certains points de vue, ils posent également des questions sur les plans méthodologique et épistémologique. Devant ce constat, nous avons choisi d'étudier plus en détail l'impact des outils de recherche d'information, bibliothèques numériques et moteurs de recherche de sources sur l'activité de recherche en histoire. Ces systèmes offrent un accès à un grand volume de documents historiques mais leur fonctionnement repose sur des traitements informatiques pour la plupart invisibles aux yeux des utilisateurs, qui peuvent ainsi s'apparenter à des boîtes noires. L'objectif principal de cette thèse est donc de donner les moyens aux utilisateurs d'observer et de comprendre ces processus dans l'optique de leur permettre d'en intégrer les effets de bord à leur méthodologie. Afin de mieux positionner notre objet d'étude, nous proposons un cadre conceptuel reposant sur la notion de ressource numérique. Sur la base de ce cadre conceptuel, nous proposons une analyse des bibliothèques numériques et moteurs de recherche de sources en fonction de chacun des contextes. Ces indicateurs sont ensuite croisés avec le fonctionnement du système, dans ces contextes de production et d'exécution, pour en révéler les biais méthodologiques. À l'issue de ces analyses, nous proposons un réinvestissement de ces résultats sous la forme d'un outil logiciel dédié à l'enseignement d'une approche critique de la recherche d'information en ligne pour les apprentis historiens. Ces travaux sont évalués par une démarche expérimentale. Ce prototype a fait l'objet de plusieurs phases d'expérimentation liées à son développement, l'évaluation de ces fonctionnalités et de son impact sur la pratique dans un contexte de formation.
Cette thèse explore la reconnaissance de gestes à partir de capteurs inertiels pour Smartphone. Ces gestes consistent en la réalisation d'un tracé dans l'espace présentant une valeur sémantique, avec l'appareil en main. Notre étude porte en particulier sur l'apprentissage de métrique entre signatures gestuelles grâce à l'architecture "Siamoise" (réseau de neurones siamois, SNN), qui a pour but de modéliser les relations sémantiques entre classes afin d'extraire des caractéristiques discriminantes. Les stratégies classiques de formation d'ensembles d'apprentissage sont essentiellement basées sur des paires similaires et dissimilaires, ou des triplets formés d'une référence et de deux échantillons respectivement similaires et dissimilaires à cette référence. Ainsi, nous proposons une généralisation de ces approches dans un cadre de classification, où chaque ensemble d'apprentissage est composé d'une référence, un exemple positif, et un exemple négatif pour chaque classe dissimilaire. Par ailleurs, nous appliquons une régularisation sur les sorties du réseau au cours de l'apprentissage afin de limiter les variations de la norme moyenne des vecteurs caractéristiques obtenus. A l'aide de deux bases de données inertielles, la base MHAD (Multimodal Human Activity Dataset) ainsi que la base Orange, composée de gestes symboliques inertiels réalisés avec un Smartphone, les performances de chaque contribution sont caractérisées. Ainsi, des protocoles modélisant un monde ouvert, qui comprend des gestes inconnus par le système, mettent en évidence les meilleures capacités de détection et rejet de nouveauté du SNN. En résumé, le SNN proposé permet de réaliser un apprentissage supervisé de métrique de similarité non-linéaire, qui extrait des vecteurs caractéristiques discriminants, améliorant conjointement la classification et le rejet de gestes inertiels.
Dans le cadre de cette thèse, nous nous sommes concentrés sur la reconnaissance et la traduction automatique de la parole de vidéos arabes et dialectales. Les approches statistiques proposées dans la littérature pour la reconnaissance automatique de la parole (RAP) sont indépendantes de la langue et elles sont applicables à l'arabe standard. Cependant, cette dernière présente quelques caractéristiques que nous devons prendre en considération afin de booster les performances du système de RAP. Parmi ces caractéristiques on peut citer l'absence de l'indication des voyelles dans le texte ce qui rend difficile leur apprentissage par le modèle acoustique. Nous avons proposé plusieurs approches de modélisation acoustique et/ou de langage afin de mieux reconnaître la parole arabe. L'arabe standard n'est pas la langue maternelle, c'est pourquoi dans les conversations quotidiennes, on utilise le dialecte, un arabe inspiré de l'arabe standard, mais pas seulement. Nous avons travaillé sur l'adaptation du système développé pour l'arabe standard au dialecte algérien qui est l'une des variantes de la langue arabe les plus difficiles à reconnaître par les systèmes de RAP. Cela est dû aux mots empruntés d'autres langues, au code-switching et au manque de ressources. Notre proposition pour remédier à ces problèmes est de tirer profit des données orales et textuelles d'autres langues impactant le dialecte. Le texte résultant de la RAP arabe a été utilisé pour la traduction automatique (TA). Nous avons réalisé dans un premier temps une étude comparative entre l'approche statistique à base de segments et l'approche neuronale utilisées dans le cadre de la TA. Ensuite, nous nous sommes intéressés à l'adaptation de ces deux approches pour traduire le texte code-switché. Notre étude portait sur le mélange de l'arabe et de l'anglais dans des documents officiels des nations unies. Pour pallier les différents problèmes dus à la propagation des erreurs dans le système séquentiel, nous avons travaillé sur l'adaptation du vocabulaire du système de RAP et sur la proposition d'une nouvelle modélisation permettant la traduction directe de la parole.
Donner du sens aux données textuelles est une besoin essentielle pour faire les ordinateurs comprendre notre langage. Pour extraire des informations exploitables du texte, nous devons les représenter avec des descripteurs avant d'utiliser des techniques d'apprentissage. Dans ce sens, le but de cette thèse est de faire la lumière sur les représentations hétérogènes des mots et sur la façon de les exploiter tout en abordant leur nature implicitement éparse. Dans un premier temps, nous proposons un modèle de réseau basé sur des hypergraphes qui contient des données linguistiques hétérogènes dans un seul modèle unifié. En d'autres termes, nous introduisons un modèle qui représente les mots au moyen de différentes propriétés linguistiques et les relie ensemble en fonction desdites propriétés. Notre proposition diffère des autres types de réseaux linguistiques parce que nous visons à fournir une structure générale pouvant contenir plusieurstypes de caractéristiques descriptives du texte, au lieu d'une seule comme dans la plupart des représentations existantes. Cette représentation peut être utilisée pour analyser les propriétés inhérentes du langage à partir de différents points de vue, oupour être le point de départ d'un pipeline de tâches du traitement automatique de langage. Deuxièmement, nous utilisons des techniques de fusion de caractéristiques pour fournir une représentation enrichie unique qui exploite la nature hétérogènedu modèle et atténue l'eparsité de chaque représentation. Ces types de techniques sont régulièrement utilisés exclusivement pour combiner des données multimédia. Dans notre approche, nous considérons différentes représentations de texte comme des sources d'information distinctes qui peuvent être enrichies par elles-mêmes. Cette approche n'a pas été explorée auparavant, à notre connaissance. Troisièmement, nous proposons un algorithme qui exploite les caractéristiques du réseau pour identifier et grouper des mots liés sémantiquement en exploitant les propriétés des réseaux. Contrairement aux méthodes similaires qui sont également basées sur la structure du réseau, notre algorithme réduit le nombre de paramètres requis et surtout, permet l'utilisation de réseaux lexicaux ou syntaxiques pour découvrir les groupes de mots, au lieu d'un type unique des caractéristiques comme elles sont habituellement employées. Nous nous concentrons sur deux tâches différentes de traitement du langage naturel : l'induction et la désambiguïsation des sens des mots (en anglais, Word Sense, Induction and Disambiguation, ou WSI/WSD) et la reconnaissance d'entité nommées(en anglais, Named Entity Recognition, ou NER). Au total, nous testons nos propositions sur quatre ensembles de données différents. Les résultats obtenus nous permettent de montrer la pertinence de nos contributions et nous donnent également un aperçu des propriétés des caractéristiques hétérogènes et de leurs combinaisons avec les méthodes de fusion. Aussi, nous analysons les opérateurs de fusion utilisés afin de mieux comprendre la raison de ces améliorations. Et deuxièmement, nous abordons encore une fois la tâche WSI/WSD, cette fois-ci avec la méthode à base de graphes proposée afin de démontrer sa pertinence par rapport à la tâche. Nous discutons les différents résultats obtenus avec des caractéristiques lexicales ou syntaxiques.
Ces dernières années, les techniques d'apprentissage profond ont fondamentalement transformé l'état de l'art de nombreuses applications de l'apprentissage automatique, devenant la nouvelle approche standard pour plusieurs d'entre elles. Les architectures provenant de ces techniques ont été utilisées pour l'apprentissage par transfert, ce qui a élargi la puissance des modèles profonds à des tâches qui ne disposaient pas de suffisamment de données pour les entraîner à partir de zéro. Le sujet d'étude de cette thèse couvre les espaces de représentation créés par les architectures profondes. Dans un premier temps, nous étudions les propriétés de leurs espaces, en prêtant un intérêt particulier à la redondance des dimensions et la précision numérique de leurs représentations. Nos résultats démontrent un fort degré de robustesse, pointant vers des schémas de compression simples et puissants. Ensuite, nous nous concentrons sur le l'affinement de ces représentations. Nous choisissons d'adopter un problème multi-tâches intermodal et de concevoir une fonction de coût capable de tirer parti des données de plusieurs modalités, tout en tenant compte des différentes tâches associées au même ensemble de données. Afin d'équilibrer correctement ces coûts, nous développons également un nouveau processus d'échantillonnage qui ne prend en compte que des exemples contribuant à la phase d'apprentissage, c'est-à-dire ceux ayant un coût positif. Enfin, nous testons notre approche sur un ensemble de données à grande échelle de recettes de cuisine et d'images associées. Notre méthode améliore de 5 fois l'état de l'art sur cette tâche, et nous montrons que l'aspect multitâche de notre approche favorise l'organisation sémantique de l'espace de représentation, lui permettant d'effectuer des sous-tâches jamais vues pendant l'entraînement, comme l'exclusion et la sélection d'ingrédients. Les résultats que nous présentons dans cette thèse ouvrent de nombreuses possibilités, y compris la compression de caractéristiques pour les applications distantes, l'apprentissage multi-modal et multitâche robuste et l'affinement de l'espace des caractéristiques. Pour l'application dans le contexte de la cuisine, beaucoup de nos résultats sont directement applicables dans une situation réelle, en particulier pour la détection d'allergènes, la recherche de recettes alternatives en raison de restrictions alimentaires et la planification de menus.
Les essaims de robots sont des systèmes composés d'un grand nombre de robots relativement simples. Du fait du grand nombre d'unités, ces systèmes ont de bonnes propriétés de robustesse et de passage à l'échelle. Néanmoins, il reste en général difficile de concevoir manuellement des contrôleurs pour les essaims de robots, à cause de la grande complexité des interactions inter-robot. Par conséquent, les approches automatisées pour l'apprentissage de comportements d'essaims de robots constituent une alternative attrayante. Dans cette thèse, nous étudions l'adaptation de comportements d'essaim de robots avec des méthodes de Embodied Evolutionary Robotics (EER) distribuée. Ainsi, nous fournissons trois contributions principales :  (1) Nous étudions l'influence de la pression à la sélection dirigée vers une tâche dans un essaim d'agents robotiques qui utilisent une approche d'EER distribuée. Nous évaluons l'impact de différents opérateurs de sélection dans un algorithme d'EER distribuée pour un essaim de robots. Nos résultats montrent que le plus forte la pression à la sélection est, les meilleures performances sont atteintes lorsque les robots doivent s'adapter à des tâches particulières. (2) Nous étudions l'évolution de comportements collaboratifs pour une tâche de récolte d'objets dans un essaim d'agents robotiques qui utilisent une approche d'EER distribuée. Nous réalisons un ensemble d'expériences où un essaim de robots s'adapte à une tâche collaborative avec un algorithme d'EER distribuée. Nos résultats montrent que l'essaim s'adapte à résoudre la tâche, et nous identifions des limitations concernant le choix d'action. (3) Nous proposons et validons expérimentalement un mécanisme complètement distribué pour adapter la structure des neurocontrôleurs des robots dans un essaim qui utilise une approche d'EER distribuée, ce qui permettrait aux neurocontrôleurs d'augmenter leur expressivité. Nos expériences montrent que notre mécanisme, qui est complètement décentralisé, fournit des résultats similaires à un mécanisme qui dépend d'une information globale
Les smartphones et les tablettes sont devenus des outils presque incontournables dans la vie de tout les jours. Cependant, la diversité des applications mobiles, de leurs sources et de leur simplicité d'installation et d'utilisation rendent complexes la gestion de la sécurité de ce type de terminaux. Pour faire face à ces menaces, il existe plusieurs types d'approche. On peut en citer : l'analyse statique, l'analyse dynamique ou encore l'analyse comportementale. Pradeo offre à ses clients une solution d'audit de sécurité automatique des applications mobiles. Ce produit a été développé principalement dans le cadre de plusieurs thèses CIFRE en collaboration avec le laboratoire LIRMM. Cependant, ces travaux n'incluent pas de méthode d'apprentissage automatique permettant d'exploiter des données collectées à partir des terminaux mobiles afin d'anticiper des situations à risque. Dans cette thèse, nous nous intéresserons donc à l'application de méthodes d'analyse de données et d'intelligence artificielle afin de corréler des données provenant de plusieurs sources et ainsi implémenter une analyse comportementale efficace. Nous jugeons que les méthodes existantes ne s'appliquent pas directement à la problématique posée par Pradeo en raison de la vélocité des données, de leur caractère hétérogène et des spécificités de l'environnement mobile, comme développé dans les verrous ci-après :  - La non-modification de l'OS ; l'agent de collecte de données sur un terminal mobile doit pouvoir fonctionner sans qu'il soit nécessaire de jailbreaker le terminal - La consommation d'énergie, de mémoire de stockage et de données réseau de l'agent mobile doit être minimale - L'aspect temps réel "souple" doit être pris en considération lors du traitement de gros volumes de données (données provenant de +1 million de terminaux) - La marge d'erreur du modèle de prédiction doit être minimale afin de réduire au maximum toute vérification manuelle - La prise en compte d'un large panel de terminaux mobiles, OS, constructeurs Le but de cette thèse est d'enrichir la technologie Pradeo afin d'identifier des situations à risque sur un terminal mobile. L'objectif sera d'ajouter une brique d'analyse dynamique et intelligente du contexte global du terminal mobile qui corrélera plusieurs sources de données :  - Des événements (système, réseaux et applicatifs) remontés en temps réel par les terminaux - Les rapports d'analyse statique des applications mobiles-Des sources de menaces / vulnérabilités externes
Notre recherche repose sur l'analyse des politiques publiques françaises et marocaines en matière d'adoption des technologies du Cloud Computing et du Big Data. Enjeux pour lesquels l'Etat doit apporter aujourd'hui des réponses politiques et techniques. — Développer des outils numériques du type Big Data articulés à des solutions « Cloud Computing » afin d'améliorer des services publics. — Développer et assurer la présence de l'Etat et de ses administrations dans le cyberespace ;  — Mettre les outils du type Coud Computing au service de la sécurité nationale pour faire face aux dispositifs de cyber-renseignement étrangers. Dans un contexte de transformations profondes de la société induites par le numérique, l'Etat doit réaffirmer ses droits sur son propre territoire. Pour faire face aux risques sécuritaires, l'Etat français comme l'Etat marocain se sont dotés des instruments juridiques et techniques qui s'appuient précisément sur les technologies du Cloud Computing et du Big Data. L'arsenal juridique français s'est vu renforcé dernièrement par l'adoption successive et accélérée — sans débat national — de la Loi de programmation militaire (2014-2019) puis sur les lois anti-terroriste (2014) et sur le Renseignement (2015). Ces différents textes ont agité le débat politique en instillant une inquiétude grandissante relative au déploiement de dispositifs numériques de surveillance. Nous avons ainsi pu traiter des notions comme celles de cyber-sécurité, de cyber-souveraineté et de cyber-surveillance au sein du Cyber-Etat. 3. Les enjeux liés au traitement des données personnelles au sein du Cyber-Etat et produites par les activités quotidiennes du cyber-citoyen.
Le modèle de la forme, deux polygones décrivant les contours interne et externe des lèvres, est appris par ACP. Le modèle d'apparence est un réseau de neurones qui classifie les points de l'image. Son entraînement nécessite de repérer les lèvres sur des images naturelles et, plutôt que de recourir à un étiquetage manuel, nous proposons une nouvelle méthode automatique utilisant deux répétitions d'une même phrase, avec et sans maquillage bleu. Le maquillage permet d'extraire le contour des lèvres et l'alignement par DTW des canaux acoustiques des deux séquences permet d'estimer la forme des lèvres sur les images naturelles, grâce aux formes extraites avec le maquillage.
L'essor des travaux en informatique affective voit la naissance de diverses questions de recherches pour étudier les interactions agents /humains. Parmi elles, se pose la question de l'impact des relations interpersonnelles sur les stratégies de communications. Les interactions entre un agent conversation et un utilisateur humain prennent généralement place dans des environnements collaboratifs où les interlocuteurs partagent des buts communs. La relation interpersonnelle que les individus créent durant leurs interactions affecte leurs stratégies de communications. Par ailleurs, des individus qui collaborent pour atteindre un but commun sont généralement amenés à négocier. Ce type de négociation permet aux négociateurs d'échanger des informations afin de mieux collaborer. L'objectif cette thèse est d'étudier l'impact de la relation interpersonnelle de dominance sur les stratégies de négociation collaborative entre un agent et un humain. Ce travail se base sur des études en psychologie sociale qui ont défini les comportements liés à la manifestation de la dominance dans une négociation. Nous proposons un modèle de négociation collaborative dont le modèle décisionnel est régi par la relation de dominance. En effet, en fonction de sa position dans le spectre de dominance, l'agent est capable d'exprimer une stratégie de négociation spécifique. En parallèle, l'agent simule une relation interpersonnelle de dominance avec son interlocuteur. Pour ce faire, nous avons doté l'agent d'un modèle de théorie de l'esprit qui permet à l'agent de raisonner sur les comportements de son interlocuteur afin de prédire sa position dans le spectre de dominance. Ensuite, il adapte sa stratégie de négociation vers une stratégie complémentaire à celle détectée chez son interlocuteur. Nos résultats ont montré que les comportements de dominance exprimés par notre agent sont correctement perçus. Par ailleurs, le modèle de la théorie de l'esprit est capable de faire de bonnes prédictions avec seulement une représentation partielle de l'état mental de l'interlocuteur. Enfin, la simulation de la relation interpersonnelle de dominance a un impact positif sur la négociation : les négociateurs atteignent de bon taux de gains communs.
Des études précédentes démontrent qu'avoir accès à la structure syntaxique des phrases aide les enfants à découvrir le sens des mots nouveaux. Cela implique que les enfants doivent avoir accès à certains aspects de la structure syntaxique avant même de connaître beaucoup de mots. Étant donné que dans toutes les langues du monde la structure prosodique d'une phrase corrèle avec sa structure syntaxique, et que par ailleurs les mots et morphèmes grammaticaux sont utiles pour déterminer la catégorie syntaxique des mots, il se pourrait que les enfants utilisent la prosodie et les mots grammaticaux pour initialiser leur acquisition lexicale et syntaxique. Dans cette thèse, j'ai étudié le rôle de la prosodie phrasale et des mots grammaticaux pour guider l'analyse syntaxique chez les enfants (PARTIE 1) et la possibilité que les jeunes enfants exploitent cette information pour apprendre le sens des mots nouveaux (PARTIE 2). Dans la partie 1, j'ai construit des paires minimales de phrases en français et en anglais afin de tester si les enfants exploitent la relation entre les structures prosodique et syntaxique pour guider leur interprétation des homophones noms-verbes. J'ai démontré que les enfants d'âge préscolaire utilisent la prosodie phrasale en temps réel pour guider leur analyse syntaxique. En écoutant des phrases telles que [La petite ferme] [..., les enfants interprètent ferme comme un nom, mais pour les phrases telles que [La petite][ferme...], ils interprètent ferme comme un verbe (Chapitre 3). Cette capacité a également été observée chez les enfants américains : en écoutant des phrases telles que « The baby flies … » , ils utilisent la prosodie des phrases pour décider si flies est un nom ou un verbe (Chapitre 4). Par la suite, j'ai démontré que même les enfants d'environ 20 mois utilisent la prosodie des phrases pour récupérer leur structure syntaxique et pour en déduire la catégorie syntaxique des mots (Chapitre 5), une capacité qui serait extrêmement utile pour découvrir le sens des mots inconnus. C'est cette hypothèse que j'ai testé dans la partie 2, à savoir si l'information syntaxique obtenue à partir de la prosodie phrasale et des mots grammaticaux permet aux enfants d'apprendre le sens des mots. Une première série d'études s'appuie sur des phrases disloquées à droite contenant un verbe nouveau en français : [ili dase], [le bébéi] qui est minimalement différente de la phrase transitive [il dase le bébé]. Mes résultats montrent que les enfants de 28 mois exploitent les informations prosodiques de ces phrases pour contraindre leur interprétation du sens du nouveau verbe (Chapitre 6). Dans une deuxième série d'études, j'ai étudié si la prosodie et les mots grammaticaux guident l'acquisition de noms et de verbes. J'ai utilisé des phrases comme « Regarde la petite bamoule » qui peuvent être produites soit comme [Regarde la petite bamoule !], où « bamoule » est un nom, ou [Regarde], [la petite] [bamoule !], où bamoule est un verbe. Les enfants de 18 mois ont correctement analysé ces phrases et ont attribué une interprétation de nom ou de verbe au mot bamoule selon sa position dans la structure prosodique-syntaxique des phrases (Chapitre 7). Ensemble, ces études montrent que les jeunes enfants exploitent les mots grammaticaux et la structure prosodique des phrases pour inférer la structure syntaxique et contraindre ainsi l'interprétation possible du sens des mots. Ce mécanisme peut permettre aux enfants de construire une représentation initiale de la structure syntaxique des phrases, avant même de connaître la signification des mots. Bien que les informations prosodiques et les mots grammaticaux puissent prendre des formes différentes selon les langues, nos études suggèrent que cette information pourrait représenter un outil universel et qui permettrait aux enfants d'accéder à certaines informations syntaxiques des phrasesqu'ils entendent, et d'initialiser l'acquisition du langage.
Même si le capital immatériel représente une part de plus en plus importante de la valeur de nos organisations, il n'est pas toujours possible de stocker, tracer ou capturer les connaissances et les expertises, par exemple dans des projets de taille moyenne. Le courrier électronique est encore largement utilisé dans les projets d'entreprise en particulier entre les équipes géographiquement dispersées. Dans cette étude, nous présentons une nouvelle approche pour détecter les zones à l'intérieur de courriels professionnels où des éléments de connaissances sont susceptibles de se trouver. Nous définissons un contexte étendu en tenant compte non seulement du contenu du courrier électronique et de ses métadonnées, mais également des compétences et des rôles des utilisateurs. Également l'analyse pragmatique linguistique est mêlée aux techniques usuelles du traitement de langage naturel. Après avoir décrit notre méthode KTR et notre modèle, nous l'appliquons à un corpus réel d'entreprise et évaluons les résultats en fonction des algorithmes d'apprentissage, de filtrage et de recherche
L'analyse automatique des expressions faciales représente à l'heure actuelle une problématique importante associée à de multiples applications telles que la reconnaissance de visages ou encore les interactions homme machine. Dans cette thèse, nous nous attaquons au problème de la reconnaissance d'expressions faciales à partir d'une image ou d'une séquence d'images. Nous abordons le problème sous trois angles. Tout d'abord, nous étudions les macro-expressions faciales et nous proposons de comparer l'efficacité de trois descripteurs différents. Nous examinons aussi l'apport de descripteurs spatio-temporels capables de prendre en compte des informations dynamiques utiles pour séparer les classes ambigües. La grosse limitation des méthodes de classification supervisée est qu'elles sont très coûteuses en termes de labélisation de données. Ainsi nous nous sommes intéressés à l'adaptation de domaine et à l'apprentissage avec peu ou pas de données labélisées. La méthode proposée nous permet de traiter des données non labélisées provenant de distributions différentes de celles du domaine source de l'apprentissage ou encore des données qui ne concernent pas les mêmes labels mais qui partagent le même contexte. Le transfert de connaissance s'appuie sur un apprentissage euclidien et des réseaux de neurones convolutifs de manière à définir une fonction de mise en correspondance entre les informations visuelles provenant des expressions faciales et un espace sémantique issu d'un modèle de langage naturel. Dans un troisième temps, nous nous sommes intéressés à la reconnaissance des micro-expressions faciales. Ainsi nous proposons un réseau de neurones auto-encodeur récurrent destiné à capturer les changements spatiaux et temporels associés à toutes les déformations du visage autres que celles dues aux micro-expressions. Ensuite, nous apprenons un modèle statistique basé sur un mélange de gaussiennes afin d'estimer la densité de probabilité de ces déformations autres que celles dues aux micro-expressions. Tous nos algorithmes sont testés et évalués sur des bases d'expressions faciales actées et/ou spontanées.
L'objectif de ces travaux de recherche est d'étudier les méthodes d'évaluation de la qualité des images biométriques multimodales sur des échantillons acquis de manière non contrainte. De nombreuses s études ont noté l'importance de la qualité de l'échantillon pour un système de reconnaissance ou un algorithme de comparaison,puisque la performance du système biométrique est intrinsèquement dépendant de la qualité des images de l'échantillon. Dès lors, la nécessité d'évaluer la qualité des échantillons biométriques pour plusieurs modalités (empreintes digitales, iris,visage, etc.) est devenue primordiale notamment avec l'apparition de systèmes biométriques multimodaux de haute précision. Après une introduction présentant un historique de la biométrie et des préceptes liés à la qualité des échantillons biométriques, nous présentons le concept d'évaluation de la qualité des échantillons pour plusieurs modalités. Les normes de qualité ISO / CEI récemment établies pour les empreintes digitales, l'iris et le visage sont présentées. De plus, des approches d'évaluation de la qualité des échantillons conçues spécifiquement pour les empreintes digitales avec et sans contact, pour l'iris(dont une image est capturée en proche infrarouge et dans le domaine visible),ainsi que le visage sont étudiées. Finalement, des techniques d'évaluation des performances des mesures de qualité des échantillons biométriques sont également étudiées. Sur la base des conclusions formulées suite à l'étude des solutions algorithmiques portant sur l'évaluation de la qualité des échantillons biométriques, nous proposons un cadre commun pour l'évaluation de la qualité d'image biométrique pour plusieurs modalité. Après avoir étudié les attributs de qualité basés sur l'image par modalité biométrique, nous examinons quelle intersection existe pour l'ensemble des modalités. Ensuite, nous sélectionnons et redéfinissons les attributs de qualité basés sur l'image qui sont les plus importants afin de définir un cadre commun. Afin de relier ces attributs de qualité aux vrais échantillons biométriques,nous développons une nouvelle base de données de qualité d'image biométrique multi-modalité qui contient des images échantillons de haute qualité et des images dégradées pour l'empreinte digitale acquise sans contact, l'iris (dont l'acquisition est réalisée dans le spectre visible) et le visage. Les types de dégradation appliqués sont liés aux attributs de qualité qui sont communs aux diverses modalités et qui sont basés sur l'image. Un autre aspect important du cadre commun proposé est la qualité de l'image et ses applications en biométrie. Nous avons d'abord introduit et classifié les métriques de qualité d'image existantes, puis effectué un bref aperçu des métriques de qualité d'image sans référence, qui peuvent être appliquées pour l'évaluation de la qualité des échantillons biométriques. De plus, nous étudions comment les mesures de qualité d'image sans référence ont été utilisées pour l'évaluation de la qualité des empreintes digitales, de l'iris et des modalités biométriques du visage. Des expériences pour l'évaluation de la performance des métriques de qualité d'image sans référence sur les images de visage et de l'iris sont effectuées. Les résultats expérimentaux indiquent qu'il existe plusieurs métriques qui peuvent évaluer la qualité des échantillons biométriques de l'iris et du visage avec un fort coefficient de correlation. À travers le travail réalisé dans cette thèse, nous avons démontré l'applicabilité des métriques de qualité d'image sans référence pour l'évaluation d'échantillons biométriques multi-modalité non contraints.
Ce travail de thèse, qui se situe à l'interface entre traitement du signal, informatique et statistiques, vise à l'élaboration de méthodes d'apprentissage automatique à grande échelle et de garanties théoriques associées. Le schéma de compression utilisé permet de tirer profit d'une architecture distribuée ou de traiter des données en flux, et a déjà été utilisé avec succès sur plusieurs tâches d'apprentissage non-supervisé  : partitionnement type k-moyennes, modélisation de densité avec modèle de mélange gaussien, analyse en composantes principales. Les contributions de la thèse s'intègrent dans ce cadre de plusieurs manières. D'une part, il est montré qu'en bruitant le sketch, des garanties de confidentialité (différentielle) peuvent être obtenues ;  des bornes exactes sur le niveau de bruit requis sont données, et une comparaison expérimentale permet d'établir que l'approche proposée est compétitive vis-à-vis d'autres méthodes récentes. Ensuite, le schéma de compression est adapté pour utiliser des matrices aléatoires structurées, qui permettent de réduire significativement les coûts de calcul et rendent possible l'utilisation de méthodes compressives sur des données de grande dimension. Enfin, un nouvel algorithme basé sur la propagation de convictions est proposé pour résoudre la phase d'apprentissage (à partir du sketch) pour le problème de partitionnement type k-moyennes.
Elles nous permettent par exemple d'effectuer des paiements ou encore de signer des documents numériques. Parce que les cartes à puces contiennent des informations personnelles et sensibles relatives à leur propriétaire légitime, elles sont convoitées par les attaquants. En particulier, ces attaquants peuvent utiliser le fuzzing. Cette attaque consiste à tester le plus de messages de communication possible avec un programme afin de pouvoir détecter des vulnérabilités. Cette thèse vise à protéger les cartes à puces face aux attaques par fuzzing. Deux approches de détection automatique d'erreurs d'implémentation sont proposées. La première, est l'adaptation pour Java et son amélioration d'un outil issu de l'état de l'art. Il repose sur une technique de fouille de code sources automatique. La seconde approche est également basée sur la fouille de code sources, en prenant en compte les limites de la première. En particulier, la précision et la réduction des dimensions est améliorée par l'utilisation de techniques issues du Traitement Automatique du Langage Naturel. De plus, une étude des techniques de plagiat augmente la robustesse de l'analyse face aux différences d'implémentation des applets. Cette évaluation repose sur trois oracles construits manuellement à partir de programmes AES et d'applets OpenPGP. Les résultats montrent que la seconde approche permet de détecter des vulnérabilités avec plus de précision, de rappel, et en moins de temps que la première approche. Aussi, son implémentation nommée Confiance
Cette thèse étudie la coarticulation C-à-V en français et son interaction avec d'autres sources de variations dans le but de mieux comprendre ce qui la module et ce qui gouverne la variation dans la parole. Pour cela, à partir de grands corpus de parole, nous avons testé comment la coarticulation C-à-V était fonction : 1) des caractéristiques articulatoires des consonnes et voyelles impliquées à partir de 18.5k voyelles /i, e, ɛ, a, x, u, o, ɔ/ (/x/=/ø, œ, ə/) en contexte ALVéolaire, UVulaire et VÉLaire ;  Cependant, certains résultats suggèrent que la modulation de la coarticulation par la position prosodique et le style de parole, ont des fonctions linguistiques différentes dont les implications sur la variation dans la parole seront discutées. Enfin, une réflexion sur les changements de sons en lien avec la préférence universelle pour l'antériorisation des voyelles postérieures fermées sera proposée à partir des différences observées entre les voyelles.
Les principaux enjeux technico-scientifiques du 21ème siècle sont caractérisés par une interdisciplinarité et une convergence des technologies de plus en plus importantes. L'évolution des produits et services basés sur la bio- et la nanotechnologie sont parmi les exemples les plus connus. Il manque cependant des processus et des méthodes permettant d'organiser et de structurer la résolution de problème dans des environnements interdisciplinaires – ce terme faisant ici référence à la collaboration entre ingénieurs et chercheurs scientifiques. Ainsi, la question de recherche de ce travail de doctorat est la suivante : Comment soutenir et faciliter la résolution créative de problème interdisciplinaire et l'intégration des technologies dans des domaines basés sur la connaissance ? Pour répondre à cette question, trois hypothèses ont été formulées :  La deuxième hypothèse suggère un impact du support méthodologique sur le processus de résolution de problème en groupe ainsi que sur les résultats de ce processus. La troisième hypothèse suggère quant à elle que les concepts et notions clés des méthodes analytiques comme TRIZ et USIT peuvent être utilisés dans un processus d'intégration de technologie et peuvent soutenir ce processus. Alors que la composition de groupe impacte principalement les aspects quantitatifs et qualitatifs des solutions proposées, le support méthodologique influence quant à lui le processus de résolution de problème ainsi que les aspects qualitatifs des solutions. Plus important, l'impact des méthodes semble être dépendant de la composition des groupes. Pour tester la troisième hypothèse, les résultats de la première expérimentation ont été utilisés pour générer un modèle permettant de structurer la recherche et l'intégration d'une ou plusieurs technologies dans le cadre du développement de nouveaux produits. Ce modèle, qui intègre des méthodes et outils provenant de différentes méthodologies, a été testé par des ingénieurs lors d'une étude de cas industrielle dans le secteur des roulements à bille. L'évaluation du modèle montre qu'il semble faciliter le transfert de connaissance et améliorer la créativité des concepts développés comparé aux approches déjà existantes. En ce qui concerne l'effort nécessaire pour l'apprentissage et la mise en œuvre du modèle développé, les performances sont comparables à celles obtenues avec les méthodes préexistantes. Les résultats de ce travail sont particulièrement intéressants pour les équipes de la R et D et leur management dans les secteurs de la haute technologie ainsi que dans des domaines à l'interface entre l'ingénierie et les sciences naturelles.
Le but de cette thèse est de proposer des méthodes pour récupérer les noms propres manquants dans un système de reconnaissance. Nous proposons de modéliser le contexte sémantique et d'utiliser des informations thématiques contenus dans les documents audio à transcrire. Des modèles probabilistes de thème et des projections dans un espace continu obtenues à l'aide de réseaux de neurones sont explorés pour la tâche de récupération des noms propres pertinents. Une évaluation approfondie de ces représentations contextuelles a été réalisée. En s'appuyant sur ce modèle, nous proposons un nouveau modèle (Neural Bag-of-Weighted-Words, NBOW2) qui permet d'estimer un degré d'importance pour chacun des mots du document et a la capacité de capturer des mots spécifiques à ce document. Des expériences de reconnaissance automatique de bulletins d'information télévisés montrent l'efficacité du modèle proposé. L'évaluation de NBOW2 sur d'autres tâches telles que la classification de textes montre des bonnes performances
Le développement des technologies de l'information et de la communication à modifié en profondeur la manière dont nous avons accès aux connaissances. Face à l'afflux de données et à leur diversité, il est nécessaire de meure su point des technologies performantes et robustes pour y rechercher des informations. Après une caractérisation de leur nature linguistique, nous proposons une approche par instructions, fondée sur les marqueurs (balises) d'annotation, qui considère ces éléments isolément (début ou fin d'une annotation). En seconde partie, nous faisons état des travaux en fouille de données et présentons un cadre formel pour explorer les données. Nous y proposons une formulation alternative par segments, qui limite la combinatoire lors de l'exploration. Les motifs corrélés à un ou plusieurs marqueurs d'annotation sont extraits comme règles d'annotation. La dernière partie décrit le cadre expérimental, quelques spécificités de l'implémentation du système (mXS) et les résultats obtenus. Nous montrons l'intérêt d'extraire largement les règles d'annotation et expérimentons les motifs de segments. Nous fournissons des résultats chiffrés relatifs aux performances du système à divers point de vue et dans diverses configurations. Ils montrent que l'approche que nous proposons est compétitive et qu'elle ouvre des perspectives dans le cadre de l'observation des langues naturelles et de l'annotation automatique.
Cette thèse propose de développer des nouvelles méthodes pour apprendre des stratégies de dialogue par apprentissage par renforcement afin de résoudre le double défi des systèmes de dialogue permettant d'accomplir une tache précise : créer des agents conversationnels capables de résoudre efficacement la tache tout en générant un procédé de communication de niveau égal à celui d'un humain.
À mesure que la production de textes numériques croît exponentiellement, un besoin grandissant d'analyser des corpus de textes se manifeste dans beaucoup de domaines d'application, tant ces corpus constituent des sources inépuisables d'information et de connaissance partagées. Ainsi proposons-nous dans cette thèse une nouvelle approche de visualisation analytique pour l'analyse de corpus textuels, mise en œuvre pour les besoins spécifiques du journalisme d'investigation. Motivées par les problèmes et les tâches identifiés avec une journaliste d'investigation professionnelle, les visualisations et les interactions ont été conçues suivant une méthodologie centrée utilisateur, impliquant l'utilisateur durant tout le processus de développement. En l'occurrence, les journalistes d'investigation formulent des hypothèses, explorent leur sujet d'investigation sous tous ses angles, à la recherche de sources multiples étayant leurs hypothèses de travail. La réalisation de ces tâches, très fastidieuse lorsque les corpus sont volumineux, requiert l'usage de logiciels de visualisation analytique se confrontant aux problématiques de recherche abordées dans cette thèse. D'abord, la difficulté de donner du sens à un corpus textuel vient de sa nature non structurée. Nous avons donc recours au modèle vectoriel et son lien étroit avec l'hypothèse distributionnelle, ainsi qu'aux algorithmes qui l'exploitent pour révéler la structure sémantique latente du corpus. Bien que l'exploration des sujets de haut niveau aide à localiser des sujets d'intérêt ainsi que leur voisinage, l'identification de faits précis, de points de vue ou d'angles d'analyse, en lien avec un événement ou une histoire, nécessite un niveau de structuration plus fin pour représenter des variantes de sujet. Cette structure imbriquée révélée par Bimax, une méthode de biclustering basée sur des motifs avec chevauchement, capture au sein des biclusters les co-occurrences de termes partagés par des sous-ensembles de documents pouvant dévoiler des faits, des points de vue ou des angles associés à des événements ou des histoires communes. Cette thèse aborde les problèmes de visualisation de biclusters avec chevauchement en organisant les biclusters terme-document en une hiérarchie qui limite la redondance des termes et met en exergue les parties communes et distinctives des biclusters. Nous avons évalué l'utilité de notre logiciel d'abord par un scénario d'utilisation doublé d'une évaluation qualitative avec une journaliste d'investigation. En outre, les motifs de co-occurrence des variantes de sujet révélées par Bima. sont déterminés par la structure de sujet englobante fournie par une méthode d'extraction de sujet. Cependant, la communauté a peu de recul quant au choix de la méthode et son impact sur l'exploration et l'interprétation des sujets et de ses variantes. Ainsi nous avons conduit une expérience computationnelle et une expérience utilisateur contrôlée afin de comparer deux méthodes d'extraction de sujet.
Depuis ces dernières décennies, des millions d'internautes produisent et échangent des données sur le Web. Actuellement, une grande quantité de descriptions RDF est disponible en ligne, notamment grâce à des projets de recherche qui traitent du Web de données liées, comme par exemple DBpedia et LinkedGeoData. De plus, de nombreux fournisseurs de données ont adopté les technologies issues de cette communauté du Web de données en partageant, connectant, enrichissant et publiant leurs informations à l'aide du standard RDF, comme les gouvernements (France, Canada, Grande-Bretagne, etc.), les universités (par exemple Open University) ainsi que les entreprises (BBC, CNN, etc.). Il en résulte que de nombreux acteurs actuels (particuliers ou organisations) produisent des quantités gigantesques de descriptions RDF qui sont échangées selon différents formats (RDF/XML, Turtle, N-Triple, etc.). Pour ce faire, nous proposons une approche intitulée R2NR qui à partir de différentes descriptions relatives à une même information produise une et une seule description normalisée qui est optimisée en fonction de multiples paramètres liés à une application cible. Notre approche est illustrée en décrivant plusieurs cas d'étude (simple pour la compréhension mais aussi plus réaliste pour montrer le passage à l'échelle) nécessitant l'étape de normalisation. La contribution de cette thèse peut être synthétisée selon les points suivants :  i. Produire une description RDF normalisée (en sortie) qui préserve les informations d'une description source (en entrée), ii. Éliminer les redondances et optimiser l'encodage d'une description normalisée, iii. Engendrer une description RDF optimisée en fonction d'une application cible (chargement rapide, stockage optimisée...), iv. Définir de manière complète et formelle le processus de normalisation à l'aide de fonctions, d'opérateurs, de règles et de propriétés bien fondées, etc. v. Fournir un prototype RDF2NormRDF (avec deux versions : en ligne et hors ligne) permettant de tester et de valider l'efficacité de notre approche. Afin de valider notre proposition, le prototype RDF2NormRDF a été utilisé avec une batterie de tests. Nos résultats expérimentaux ont montré des mesures très encourageantes par rapport aux approches existantes, notamment vis-à-vis du temps de chargement ou bien du stockage d'une description normalisée, tout en préservant le maximum d'informations.
Certaines applications du traitement automatique des langues sont amenées à traiter des flux de données textuelles caractérisés par l'emploi d'un vocabulaire en perpétuelle évolution, que ce soit au niveau de la création des mots que des sens de ceux existant déjà. En partant de ce constat, nous avons mis au point un algorithme incrémental pour construire automatiquement et faire évoluer une base lexicale qui répertorie des unités lexicales non étiquetées sémantiquement observées dans des flux. Cette représentation est complétée par une modélisation vectorielle visualisable qui tient compte des aspects continus du sens et de la proximité sémantique entre concepts. Ce modèle est alors exploité pour propager l'étiquetage manuel d'un petit nombre d'entités nommées (EN : unités lexicales qui se référent habituellement à des personnes, des lieux, des organisations...) à d'autres EN non étiquetées observées dans un flux pendant la construction incrémentale du treillis. Les concepts de ce treillis sont enrichis avec les étiquettes d'EN observées dans un corpus d'apprentissage. Ces concepts et leurs étiquettes attachées sont respectivement employés pour l'annotation non supervisée et la classification supervisée des EN d'un corpus de test.
La modélisation de données séquentielles est utile à de nombreux domaines : reconnaissance de parole, de gestes, d'écriture, ou encore la synthèse d'animations pour des avatars virtuels. Notre modélisation part du constat qu'une part importante de la variabilité entre les séquences d'observations peut être la conséquence de quelques variables contextuellesfixes le long de la séquence ou qui varient en fonction du temps. Une phrase peut être exprimée différemment en fonction de l'humeur du locuteur, un geste peut être plus ample en fonction de la taille de l'acteur etc... Cette méthode réalise du partage d'information entre les classes la ou les approches génératives apprennent des modèles de classes indépendants.
La structure d'un réseau de neurones détermine dans une large mesure son coût d'entraînement et d'utilisation, ainsi que sa capacité à apprendre. Ces deux aspects sont habituellement en compétition : plus un réseau de neurones est grand, mieux il remplira la tâche qui lui a été assignée, mais plus son entraînement nécessitera des ressources en mémoire et en temps de calcul. Dans ce contexte, des réseaux de neurones aux structures variées doivent être entraînés, ce qui nécessite un nouveau jeu d'hyperparamètres d'entraînement à chaque nouvelle structure testée. L'objectif de la thèse est de traiter différents aspects de ce problème. La première contribution est une méthode d'entraînement de réseau qui fonctionne dans un vaste périmètre de structures de réseaux et de tâches à accomplir, sans nécessité de régler le taux d'apprentissage. La deuxième contribution est une technique d'entraînement et d'élagage de réseau, conçue pour être insensible à la largeur initiale de celui-ci. La dernière contribution est principalement un théorème qui permet de traduire une pénalité d'entraînement empirique en a priori bayésien, théoriquement bien fondé. Ce travail résulte d'une recherche des propriétés que doivent théoriquement vérifier les algorithmes d'entraînement et d'élagage pour être valables sur un vaste ensemble de réseaux de neurones et d'objectifs.
Les services géolocalisés (LBS) sont destinés à délivrer de l'information adéquate aux utilisateurs quelque soit le temps et l'endroit et ceci en se basant sur leur profil, contexte et position géographique. En général, ces données sont stockées de plusieurs bases de données géographiques (BDG) dans le monde entier. D'autre part, le nombre croissant des différentes BDG couvrant la même zone géographique et la récupération des données/métadonnées non erronées pour un service quelconque, impliquent de nombreux raisonnements et de contrôles d'accès aux BDG afin de résoudre les ambiguïtés dues à la présence des objets homologues dupliqués sur l'écran mobile. Donc, mon but ultime sera de générer automatiquement une carte unique intégrant plusieurs interfaces des fournisseurs sur laquelle les objets homologues seront intégrés avant de les visualiser sur l'écran mobile. Nos nouveaux concepts, basés sur certains algorithmes de fusion, sur l'ontologie pour assurer l'intégration au niveau sémantique et cartographique, sur l'orchestration des géo web services, sont implémentés dans des prototypes modulaires et évalués.
L'accès aux informations pertinentes, adaptées aux besoins et au profil de l'utilisateur est un enjeu majeur dans le cadre actuel caractérisé par une prolifération massive des ressources d'information hétérogènes. En effet, nous proposons une approche de recommandation contextuelle et proactive dans un environnement mobile qui permet de recommander des informations pertinentes à l'utilisateur sans attendre à ce que ce dernier initie une interaction. Cela permettra de réduire les efforts, le temps et l'interaction de l'utilisateur avec son appareil mobile et de présenter les informations pertinentes au bon moment et au bon endroit. Cette approche prend aussi en considération les situations où la recommandation pourrait déranger l'utilisateur. Il s'agit d'équilibrer le processus de recommandation contre les interruptions intrusives. En effet, il existe différents facteurs et situations qui rendent l'utilisateur moins ouvert aux recommandations. Comme nous travaillons dans le contexte des appareils mobiles, nous considérons que les applications mobiles telles que la caméra, le clavier, l'agenda, etc., sont de bons représentants de l'interaction de l'utilisateur avec son appareil puisqu'ils représentent en quelque sorte la plupart des activités qu'un utilisateur pourrait entreprendre avec son appareil mobile au quotidien, comme envoyer des messages, converser, tweeter, naviguer ou prendre des photos.
La motivation principale de cette thèse est de proposer un système de recommandation personnalisé pour les plateformes d'informations. Pour cela, nous avons démontré que les opinions peuvent constituer un descripteur efficace pour améliorer la qualité de la recommandation. Au cours de cette thèse, nous avons abordé ce problème en proposant trois contributions principales. Le modèle de profil proposé repose sur trois éléments : les entités nommées, les aspects et les sentiments. Deuxièmement, nous avons proposé une approche de classement des opinions permettant de filtrer et sélectionner seulement les opinions pertinentes. Les résultats montrent que notre approche surpasse deux approches récemment proposées pour le classement des opinions. Les résultats montrent que l'enrichissement des contenus des articles de presse
La structure discursive d'un texte est un élément essentiel à la compréhension du contenu véhiculé par ce texte. Elle affecte, par exemple, la structure temporelle du texte, ou encore l'interprétation des expressions anaphoriques. Dans cette thèse, nous aborderons les effets de la structure discursive sur l'analyse de sentiments. L'analyse des sentiments est un domaine de recherche extrêmement actif en traitement automatique des langues. Devant l'abondance de données subjectives disponibles, l'automatisation de la synthèse des multiples avis devient cruciale pour obtenir efficacement une vue d'ensemble des opinions sur un sujet donné. La plupart des travaux actuels proposent une analyse des opinions au niveau du document ou au niveau de la phrase en ignorant la structure discursive. Dans cette thèse, nous nous plaçons dans le contexte de la théorie de la SDRT (Segmented Discourse Representation Theory) et proposons de répondre aux questions suivantes :  - Existe-t-il un lien entre la structure discursive d'un document et les opinions émises dans ce même document ? - Quel est le rôle des relations de discours dans la détermination du caractère objectif ou subjectif d'un segment textuel ? - Quel est l'impact de la structure discursive lors de la détermination de l'opinion globale véhiculée dans un document ? - Est-ce qu'une approche basée sur le discours apporte une réelle valeur ajoutée comparée à une approche classique basée sur la notion de 'sacs de mots' ?
Le traitement d'événements complexes (Complex Event Processing – CEP) consiste en l'analyse de flux de données afin d'en extraire des motifs et comportements particuliers décrits, en général, dans un formalisme logique. Dans l'approche classique, les données d'un flux – ou événements – sont supposées être l'observation complète et parfaite du système produisant ces événements. Cependant, dans de nombreux cas, les moyens permettant la collecte de ces données, tels que des capteurs, ne sont pas pour autant infaillibles et peuvent manquer la détection d'un événement particulier ou au contraire en produire. Dans cette thèse, nous nous sommes employé à étudier les modèles possibles de représentation de l'incertain et, ainsi, offrir au CEP une robustesse vis-à-vis de cette incertitude ainsi que les outils nécessaires pour permettre la reconnaissance de comportement complexe de façon pertinente les flux d'événements en se basant sur le formalisme des chroniques. Dans cette optique, trois approches ont été considérées. La première se base sur les réseaux logiques de Markov pour représenter la structure des chroniques sous un ensemble de formules logiques adjointe dune valeur de confiance. Nous montrons que ce modèle, bien que largement appliqué dans la littérature, est inapplicable pour une application concrète au regard des dimensions d'un tel problème. La seconde approche se basent sur des techniques issues de la communauté SAT pour énumérer l'ensemble des solutions possibles d'un problème donné et ainsi produire une valeur de confiance pour la reconnaissance dune chronique exprimée, encore une fois, sous une requête logique. Finalement, nous proposons une dernière approche basée sur les chaînes de Markov pour produire un ensemble d'échantillons expliquant l'évolution du modèle en accord avec les données observées. Ces échantillons sont ensuite analysés par en système de reconnaissance pour compter les occurrences dune chronique particulière.
Le Transport Optimal régularisé par l'Entropie (TOE) permet de définir les Divergences de Sinkhorn (DS), une nouvelle classe de distance entre mesures de probabilités basées sur le TOE. Celles-ci permettent d'interpoler entre deux autres distances connues : le Transport Optimal (TO) et l'Ecart Moyen Maximal (EMM). Les DS peuvent être utilisées pour apprendre des modèles probabilistes avec de meilleures performances que les algorithmes existants pour une régularisation adéquate.
Rendre les analyseurs sémantiques robustes aux variations lexicales et stylistiques est un véritable défi pour de nombreuses applications industrielles. De nos jours, l'analyse sémantique nécessite de corpus annotés spécifiques à chaque domaine afin de garantir des performances acceptables. Les techniques d'apprenti-ssage par transfert sont largement étudiées et adoptées pour résoudre ce problème de manque de robustesse et la stratégie la plus courante consiste à utiliser des représentations de mots pré-formés. Cependant, les meilleurs analyseurs montrent toujours une dégradation significative des performances lors d'un changement de domaine, mettant en évidence la nécessité de stratégies d'apprentissage par transfert supplémentaires pour atteindre la robustesse. Ce travail propose une nouvelle référence pour étudier le problème de dépendance de domaine dans l'analyse sémantique. Nous utilisons un nouveau corpus annoté pour évaluer les techniques classiques d'apprentissage par transfert et pour proposer et évaluer de nouvelles techniques basées sur les réseaux antagonistes. Toutes ces techniques sont testées sur des analyseurs sémantiques de pointe. Nous affirmons que les approches basées sur les réseaux antagonistes peuvent améliorer les capacités de généralisation des modèles. Nous testons cette hypothèse sur différents schémas de représentation sémantique, langages et corpus, en fournissant des résultats expérimentaux à l'appui de notre hypothèse.
Cette thèse a pour but de contribuer à améliorer les interfaces Homme-machine. En particulier, nos appareils devraient répliquer notre capacité à traiter continûment des flux d'information. Cependant, le domaine de l'apprentissage statistique dédié à la reconnaissance de séries temporelles pose de multiples défis. Nos travaux utilisent la reconnaissance de gestes comme exemple applicatif, ces données offrent un mélange complexe de poses corporelles et de mouvements, encodées sous des modalités très variées. Pour ce faire, nous avons implémenté un environnement de test partagé qui est plus favorable à une étude comparative équitable. Nous proposons des ajustements sur les fonctions de coût utilisées pour entraîner les réseaux de neurones et sur les expressions du modèle hybride afin de gérer un large déséquilibre des classes de notre base d'apprentissage. Bien que les publications récentes semblent privilégier l'architecture BD Dans un second temps, nous présentons une étude de l'apprentissage dit « en un coup » appliqué aux gestes. Ce paradigme d'apprentissage gagne en attention mais demeure peu abordé dans le cas de séries temporelles. Nous proposons une architecture construite autour d'un réseau de neurones bidirectionnel. Son efficacité est démontrée par la reconnaissance de gestes isolés issus d'un dictionnaire de langage des signes. À partir de ce modèle de référence, nous proposons de multiples améliorations inspirées par des travaux dans des domaines connexes, et nous étudions les avantages ou inconvénients de chacun
Les développeurs sont impatients de créer diverses applications Web pour répondre à la demande croissante des gens. Pour construire une application Web, les développeurs doivent connaître quelques technologies de programmation de base. De plus, ils préfèrent utiliser certains composants tiers (tels que les bibliothèques côté serveur, côté client, services REST) dans les applications web. En incluant ces composants, ils pourraient bénéficier de la maintenabilité, de la réutilisabilité, de la lisibilité et de l'efficacité. Dans cette thèse, nous proposons d'aider les développeurs à utiliser des composants tiers lorsqu'ils créent des applications web. Nous présentons trois obstacles lorsque les développeurs utilisent les composants tiers : Quelles sont les meilleures bibliothèques JavaScript à utiliser ? Comment obtenir les spécifications standard des services REST ? Comment s'adapter aux changements de données des services REST ? C'est pourquoi nous présentons trois approches pour résoudre ces problèmes. Ces approches ont été validées par plusieurs études de cas et données industrielles. Nous décrivons certains travaux futurs visant à améliorer nos solutions et certains problèmes de recherche que nos approches peuvent cibler.
L'Optimisation Combinatoire (OC) est un domaine de recherche qui est en perpétuel changement. Résoudre un problème d'optimisation combinatoire (POC) consiste essentiellement à trouver la ou les meilleures solutions dans un ensemble des solutions réalisables appelé espace de recherche qui est généralement de cardinalité exponentielle en la taille du problème. Pour résoudre des POC, plusieurs méthodes ont été proposées dans la littérature. On distingue principalement les méthodes exactes et les méthodes d'approximation. Ne pouvant pas viser une résolution exacte de problèmes NP-Complets lorsque la taille du problème dépasse une certain seuil, les chercheurs on eu de plus en plus recours, depuis quelques décennies, aux algorithmes dits hybrides (AH) ou encore à au calcul parallèle. Dans cette thèse, nous considérons la classe POC des problèmes de conception d'un réseau fiable. Nous présentons un algorithme hybride parallèle d'approximation basé sur un algorithme glouton, un algorithme de relaxation Lagrangienne et un algorithme génétique, qui produit des bornes inférieure et supérieure pour les formulations à base de flows. Afin de valider l'approche proposée, une série d'expérimentations est menée sur plusieurs applications : le Problème de conception d'un réseau k-arête-connexe avec contrainte de borne (kHNDP) avec L = 2,3, le problème de conception d'un réseau fiable Steiner k-arête-connexe (SkESNDP) et ensuite deux problèmes plus généraux, à savoir le kHNDP avec L >= 2 et le problème de conception d'un réseau fiable k-arête-connexe (kESNDP). L'étude expérimentale de la parallélisation est présentée après cela. Dans la dernière partie de ce travail, nous présentons deux algorithmes parallèles exactes : un Branch-and-Bound distribué et un Branch-and-Cut distribué. Une série d'expérimentation a été menée sur une grappe de 128 processeurs, et des accélération intéressantes ont été atteintes pour la résolution du problèmes kHNDP avec k=3 et L=3.
De nombreuses substances chimiques sont utilisées par l'industrie cosmétique pour entrer dans la composition de formules. En dehors de la nécessité d'évaluer leur efficacité, l'industrie cosmétique se doit surtout d'évaluer la sécurité de leurs substances pour l'humain. L'évaluation toxicologique des substances chimiques est réalisée dans le but de révéler un effet toxique potentiel de la substance testée. Parmi les effets potentiels que l'on souhaite détecter, la toxicité du développement (tératogénicité), c'est-à-dire la capacité d'une substance à provoquer l'apparition d'anomalies lors du développement embryonnaire, est fondamentale. En accord avec les législations internationales qui interdisent à l'industrie cosmétique d'avoir recours à des tests sur animaux de laboratoire pour l'évaluation de leurs substances, l'évaluation toxicologique de ces substances se base sur les résultats de tests in silico, in vitro et de tests faits sur des modèles alternatifs aux animaux de laboratoire. Pour le moment cependant, peu de méthodes alternatives existent et ont été validées pour la toxicologie du développement. Le développement de nouvelles méthodes alternatives est donc requis. D'autre part, en plus de l'évaluation de la sécurité des substances chez l'humain, l'évaluation de la toxicité pour l'environnement est nécessaire. L'usage de la plupart des produits cosmétiques et d'hygiène corporelle conduit, après lavage et rinçage, à un rejet à l'égout et donc dans les cours d'eau. Il en résulte que les environnements aquatiques (eaux de surface et milieux marins côtiers) sont parfois exposés aux substances chimiques incluses dans les formules cosmétiques. Ainsi, l'évaluation toxicologique environnementale des cosmétiques et de leurs ingrédients nécessite de connaître leur toxicité sur des organismes représentatifs de chaînes alimentaires aquatiques. Dans ce contexte, le modèle embryon de poisson présente un double avantage pour l'industrie cosmétique. Il est donc pertinent pour évaluer la toxicité environnementale des substances chimiques. D'autre part, ce modèle apparaît prometteur pour évaluer l'effet tératogène de substances chimiques chez l'humain. Pour ces raisons, un test d'analyse de la tératogénicité des substances chimiques est actuellement développé. Ce test se base sur l'analyse d'embryons de medaka (Oryzias Latipes) à 9 jours post fertilisation, après exposition des embryons par balnéation à des substances à concentrations déterminées. L'analyse de paramètres fonctionnels et morphologiques conduit au calcul d'un indice tératogène, qui permet de tirer une conclusion quant à l'effet tératogène de la substance testée. Cet indice est calculé à partir des mesures du taux de mortalité et du taux de malformations chez les embryons. L'objectif de ce projet est d'automatiser le test d'analyse de la tératogénicité, par classification automatique des embryons faite à partir d'image et de vidéo. Nous nous sommes ensuite concentrés sur deux types de malformations courantes qui sont les malformations axiales, et l'absence de vessie natatoire, en utilisant une méthode d'apprentissage automatique. Cette analyse doit être complétée par l'analyse d'autres malformations et conduire à un calcul du taux de malformations et de l'indice tératogène pour la substance testée
Notre travail de recherche vise deux objectifs étroitement liés. Le premier consiste à proposer une aide à l'évaluation des écrits scientifique et pour cause : le nombre de publication augmente, les limites entre les domaines deviennent floues, et il devient difficile de trouver des publications pertinentes si bien qu'un besoin pratique d'évaluation surgit. Il s'agit aussi de trouver les moyens d'une aide à l'expertise, appuyée sur des indices discursifs permettant d'aider le lecteur à repérer les points clés d'une publication (phase préalable à une évaluation) notamment à travers l'identification du problème de recherche. Ces indices qui annoncent la formulation du problème de recherche dans les articles scientifiques sont repérables sous forme de « formules de discours » . Notre recherche ne s'étend pas sur la formulation de la problématique scientifique au vu de la complexité de cette notion et de la difficulté de la définir d'un point de vue de l'extraction d'information. Nous proposons une modélisation de ces formules de discours que nous avons intégrée dans l'analyseur syntaxique Xerox Incremental Parser (XIP) sous forme de règles de reconnaissance. Nous avons utilisé un corpus d'articles de recherche en sciences de l'éducation extraits du corpus Scientext pour y détecter ces formules de discours. Le choix du domaine est motivé par ma participation au projet européen EERQI dont le but est de renforcer et d'améliorer la visibilité mondiale et la compétitivité de la recherche européenne en éducation. Différentes approches méthodologiques ont été adoptées afin de procéder à une étude linguistique fine de ces formules dîtes de discours entre autres : l'analyse de discours (M. Pecman, 2004, K. Hyland, 2005, Á. Sándor, A. Kaplan, G. Randeau, 2006, D. Siepman, 2007, A. Tutin, 2007-2010), robust parsing (S. Aït-Mokhtar, J. Chanod, R. Roux, 2002). Il s'agit donc de mettre en œuvre une approche applicative en vue de l'aide à la lecture experte à travers l'identification, la typologie et le fonctionnement des associations lexicales véhiculant le problème de recherche.
Cette thèse dont la problématique est partie de la polémique au niveau du nombre des affixes dans la langue fon, elle nous a conduits à nous demander si l'on peut procéder à la dérivation affixale sur les noms propres fon ? Nous avons résolu cette problématique à travers les différentes parties qui constituent cette thèse. Nous avons d'entrée, abordé la théorie de la "Commutation" dont l'application commence par l'identification des unités phonologiques, leur définition et leur classement en fonction de leurs traits oppositionnels, et contrastifs, et aussi comment ces unités se combinent entre elles. Nous réalisons que dans langue fon, la composition et la dérivation sont aussi des facteurs de formation des noms et des verbes. Notre démarche dans ce domaine a été de suivre un ordre logique en partant des unités les plus petites en allant vers les plus grandes : phonèmes, syllabes, mots phonologiques. Un état de l'art de la lexicologie et de la lexicographie est fait. Il reste néanmoins encore embrionnaire, malgré le fait qu'il ait été débuté avec la pénétration européenne, les œuvres des missionnaires, et la mise en œuvre de nouveaux chantiers à l'avènement de l'indépendance. Une partie, traite de la morphologie dérivationnelle, c'est une des parties les plus importantes de la thèse. Dans une approche onomastique, elle met en œuvre la dérivation de la nomenclature du nom des rois d'Abomey, la structure organisationnelle du pouvoir à la cour royale à laquelle s'ajoute la méthode matricielle de création de nouveau noms. Dans le même orde d'idée, une étude syntaxico-morphologique du système de numération a été faite afin de facilter le comptage en langue fon. A travers une analyse ethnolinguistique, nous avons traité à travers une typologie variée, les anthroponymes événementiels : choix des noms de personnes ayant trait à la vie, au sort, à la destinée, à la mort, à la famille, à la fécondité, à l'amitié et à la réussite, du nom des jours, des mois et les noms de personnes qui sont nées tel ou tel jour de la semaine, ces noms qui tiennent compte des réalités locales et ethnologiques. Dans une perspective dynamique, après avoir fait le bilan de la lexicologie et de la lexicographie depuis la période mécanographique jusqu'au début d'informatisation, c'est-à-dire la lexicologie et l'automatisation, la dictionnairique pour aboutir à la création d'un dictionnaire étymologique bilingue dans les langues fon et français. Les apports sont à la fois quantitatifs et qualitatifs, car, notre problématique ayant été résolue, nous avons ouvert une perspective vers l'informatisation des langues d'une part et d'autre part sur le problème d'émergence des langues nationales en tant que facteurs de développement pour répondre aux Objectifs du Millénaire pour le Développement (OMD).
L'expansion du média Internet pour le recrutement a entraîné ces dernières années la multiplication des canaux dédiés à la diffusion des offres d'emploi. Dans un contexte économique où le contrôle des coûts est primordial, évaluer et comparer les performances des différents canaux de recrutement est devenu un besoin pour les entreprises. Cette thèse a pour objectif le développement d'un outil d'aide à la décision destiné à accompagner les recruteurs durant le processus de diffusion d'une annonce. Il fournit au recruteur la performance attendue sur les sites d'emploi pour un poste à pourvoir donné. Nous proposons dans un second temps un algorithme prédictif de la performance des offres d'emploi, basé sur un système hybride de recommandation, adapté à la problématique de démarrage à froid. Ce système, basé sur une mesure de similarité supervisée, montre des résultats supérieurs à ceux obtenus avec des approches classiques de modélisation multivariée.
La dystrophie myotonique (DM) est considérée comme l'une des maladies neuromusculaires les plus complexes. Bien que les travaux de recherche de ces 30 dernières années aient permis de mieux comprendre les mécanismes moléculaires sous-jacents, la nature de l'anomalie génétique hors norme, son expression multisystémique et son large spectre clinique ne permettent pas, à l'heure actuelle, une prise en charge optimale des patients. Mon travail a eu pour but d'approfondir les connaissances et de préciser l'histoire naturelle de cette maladie rare. La première partie du manuscrit est consacrée à la présentation de l'observatoire DM-Scope, sur lequel s'appuie tout mon travail de thèse. Après la description du concept, du fonctionnement et de la plateforme de recueil, les caractéristiques de la cohorte DM1, à partir de laquelle les analyses ont été réalisées, sont présentées : spectre clinique couvert, atteinte multisystémique, corrélations génotype/phénotype, interrelations entre les symptômes et comparaison à la dystrophie myotonique de type II (DM2). Ensuite, dans une deuxième partie, nous abordons les avancées majeures obtenues dans les DM grâce à DM-Scope et aux analyses réalisées pendant ma thèse :  (i) précision de l'histoire naturelle de la maladie, notamment avec la proposition d'une nouvelle classification ;  (ii) mise en exergue de facteurs déterminants du phénotype comme le genre, la taille de la mutation ou les interrelations entre les symptômes. Ces travaux ont conduit à des recommandations de soins, notamment pour la transition enfants-adultes mais aussi la validation de critères d'inclusion importants pour les essais cliniques comme le genre. DM-Scope permet d'accéder à des échantillons biologiques pour des études de recherche fondamentale et valider de nouvelles approches thérapeutiques. Il est aujourd'hui un leader à l'international et un outil incontournable dans la recherche translationnelle dans la DM. Ce concept transférable à n'importe quelle autre population, peut être utilisé pour la prise en charge d'autres maladies rares. Enfin, le développement d'un modèle de survie construit à partir de la cohorte DM de l'observatoire est présenté. Ce modèle a trois spécificités :  (i) il est applicable en grande dimension, à des cas comme DM-Scope, où l'on a un nombre important de variables ;  (ii) il prend en compte les risques compétitifs, lorsque les patients sont exposés simultanément à plusieurs évènements. Dans notre observatoire, l'étude des décès de cause respiratoire est biaisée sans la prise en compte des évènements concurrents tels que le décès de cause cardiaque ;  (iii) il modélise l'hétérogénéité entre les groupes de patients (effets centres), potentiellement due à une prise en charge différente. L'analyse des données de DM-Scope nécessite cette spécificité issue des modèles à fragilité car l'observatoire est multicentrique (55 centres). Le modèle est transférable et applicable à d'autres données car de plus en plus de bases sont de grandes dimensions, la majorité des analyses de survie ont une censure liée à la survenue de l'événement d'intérêt et les études multicentriques sont de plus en plus communes.
Le travail envisagé dans le cadre de la thèse vise à proposer une méthode et un outil pour mesurer et améliorer la qualité des modèles de processus métier. L'originalité de l'approche est qu'elle vise non seulement la qualité syntaxique mais aussi la qualité sémantique et pragmatique en s'appuyant notamment sur les connaissances du domaine.
La classification se base sur un jeu de données étiquetées par un expert. Plus le jeu de données est grand, meilleure est la performance de classification. Pourtant, la requête à un expert peut parfois être coûteuse. Le but de l'apprentissage actif est alors de minimiser le nombre de requêtes à l'expert. Améliorer la précision de l'estimation nécessite d'annoter de nouvelles données. Il y a donc un dilemme entre utiliser le budget d'annotations disponible pour améliorer la performance du classifieur selon l'estimation actuelle du critère ou pour améliorer la précision sur le critère. Ce dilemme est bien connu dans le cadre de l'optimisation en budget fini sous le nom de dilemme entre exploration et exploitation. Les solutions usuelles pour résoudre ce dilemme dans ce contexte font usage du principe d'Optimisme Face à l'Incertitude. Dans cette thèse, nous montrons donc qu'il est possible d'adapter ce principe au problème d'apprentissage actif pour la classification. Pour cela, plusieurs algorithmes ont été être développés pour des classifieurs de complexité croissante, chacun utilisant le principe de l'Optimisme Face à l'Incertitude, et leurs résultats ont été évalués empiriquement
Les lexiques bilingues sont des ressources particulièrement utiles pour la Traduction Automatique et la Recherche d'Information Translingue. Leur construction manuelle nécessite une expertise forte dans les deux langues concernées et est un processus coûteux. Plusieurs méthodes automatiques ont été proposées comme une alternative, mais elles qui ne sont disponibles que dans un nombre limité de langues et leurs performances sont encore loin derrière la qualité des traductions manuelles. Notre travail porte sur l'extraction de ces lexiques bilingues à partir de corpus de textes parallèles et comparables, c'est à dire la reconnaissance et l'alignement d'un vocabulaire commun multilingue présent dans ces corpus.
L'avènement et la disponibilité de l'apprentissage automatique et de l'intelligence artificielle ont fait la une des journaux ces dernières années, ouvrant les portes à leur application dans divers secteurs tels que la banque, la finance, le médical, etc. L'apprentissage automatique peut aider à prévenir la fraude et à améliorer la gestion des risques, les prévisions d'investissement et la prise de décision. Les modèles d'apprentissage sont également devenus plus importants dans le secteur de la santé avec de nombreuses applications potentielles offrant des soins de haute qualité et rentables aux patients. Les évolutions rapides des modèles d'apprentissage facilitent le développement d'outils de traitement du langage naturel (PNL) qui peuvent exploiter et manipuler d'énormes quantités de données. Portée par les motivations susmentionnées, l'objectif principal de cette thèse de doctorat est de contribuer à de nouvelles méthodes d'apprentissage pour des systèmes complexes dynamiques dans le secteur de la santé. Le candidat qui est employé par un centre médical en Arabie saoudite a accès à une grande quantité de données cliniques anonymes. Il doit explorer une gamme d'outils, de techniques et de cadres d'IA et de les appliquer aux données cliniques afin d'extraire des connaissances précieuses et de générer des résultats prédictifs précis. Ce processus comprend de nombreuses étapes :  Anonymisation des données Nettoyage des données : suppression du bruit et détection des valeurs aberrantes Création d'ensembles de données d'apprentissage, de validation et de test (équilibré ou déséquilibré) Proposer et comparer différents modèles de langage tels que Bert, doc2vec, etc. Ajout de nouvelles règles pour affiner les modèles de langage existants Application d'algorithmes d'apprentissage automatique pour entraîner les modèles Comparaison de la précision et des performances des modèles formés Les modèles formés peuvent être utilisés pour résoudre de nombreux problèmes de classification ou de prédiction. L'un des principaux résultats de l'étude est un outil d'autodiagnostic explicable en clinique qui peut aider les médecins.
Cette thèse a pour objet l'analyse des dénominations monoréférentielles (DM) dénommant des référents singuliers ─ lieu, personne, événement, institution ou produit de l'activité humaine ─ dans un corpus établi
La théorie de l'information a influencé un grand nombre de domaines scientifiques depuis son introduction par Claude Shannon en 1948. A part la loi de Fitts et la loi de Hick, qui sont apparus lorsque les psychologues expérimentaux étaient encore enthousiastes à l'idée d'appliquer la théorie de l'information aux différents domaines de la psychologie, les liens entre la théorie de l'information et l'interaction humain Cette thèse démontre ainsi que la théorie de l'information peut être utilisée comme un outil unifié pour comprendre et concevoir la communication et l'interaction humain-machine.
Le présent travail se propose d'examiner, par une analyse contrastive, les positionnements qui ressortent d'une terminologie relative aux politiques publiques sécuritaires. Cette recherche s'appuie sur une sélection de textes juridiques non contraignants, en français et en espagnol, publiés entre 2001 et 2018 par la Commission européenne et deux États membres : la France et l'Espagne. Le choix d'analyser les actes émis par les autorités européennes et nationales découle de certaines spécificités qui caractérisent la production du discours institutionnel. Les études récentes dans ce champ de recherche ont démontré que celles-ci tendent à favoriser une rhétorique consensuelle qui soit à même de désamorcer le débat politique. Or, ces stratégies discursives comportent en elles-mêmes la trace de positionnements idéologiques précis. L'observation de la circulation des termes, dans un contexte pluriel comme celui de l'UE, permet alors de détecter les discordances qui caractérisent les productions discursives plurielles concernant la sécurité commune. Pour ce faire, nous avons adopté une approche théorique qui articule la terminologie à certaines notions de l'analyse du discours « à la française » (ADF). La terminologie place le terme, à savoir l'unité lexicale utilisée dans un domaine spécialisé de la connaissance, au centre de sa réflexion. Nous focalisons donc notre étude sur la valeur que le lexique acquiert lorsqu'il est prononcé par une autorité légitime – l'institution – dans le secteur spécifique des politiques sécuritaires. La recherche terminologique a progressivement montré que les termes, comme les unités lexicales, sont liés au contexte d'utilisation et aux conditions de production du discours dans lequel ils s'insèrent. Les variantes dénominatives, qui émergent des sous-corpus, dépendent donc du contexte linguistique et extralinguistique qui entoure l'utilisation du terme. À partir de cet arrière-plan, nous nous sommes demandé si les variantes pouvaient être le symptôme de positions idéologiques discordantes. En ce sens, l'ADF, qui s'intéresse traditionnellement aux idéologies sous-jacentes au langage, nous a fourni les notions nécessaires pour comprendre les raisons pouvant expliquer la variation d'un terme. L'approche méthodologique nous a permis de combiner une analyse lexicométrique du corpus à une observation détaillée du terme dans son contexte, dans les reprises intertextuelles et dans les sources terminographiques. Nos résultats sont présentés après un parcours d'analyse qui commence, selon une méthode déductive, par le choix de certains termes : « prévention » , « détection » , « répression » , « combattant terroriste étranger » et « criminalité transfrontalière » . Ces derniers ont été sélectionnés sur la base de recherches menées en amont dans la littérature des relations internationales et sont représentatifs de certaines tensions qui alimentent le débat académique, politique et juridique. Il s'agit, d'une part, d'observer les termes concernant les actions stratégiques ( « prévention » , « détection » , « répression » ) et, d'autre part, de réfléchir à la conceptualisation de la menace et de l'ennemi ( « combattant terroriste étranger » et « criminalité transfrontalière » ). En conclusion, notre travail vise à observer les décalages et les ouvertures interprétatives qui se créent lorsque des termes circulent et sont utilisés pour légitimer des pratiques discursives. La thèse montre que le discours institutionnel sur la sécurité finit par occulter les débats qui pourtant sont bien présents et qui devraient donc être explicitement inclus dans l'espace public
L'augmentation massive du volume de données générées chaque jour par les individus sur Internet offre aux chercheurs la possibilité d'aborder la question de la prédictibilité des marchés financiers sous un nouvel angle. Sans prétendre apporter une réponse définitive au débat entre les partisans de l'efficience des marchés et les chercheurs en finance comportementale, cette thèse vise à améliorer notre compréhension du processus de formation des prix sur les marchés financiers grâce à une approche Big Data. En examinant les caractéristiques propres à chaque utilisateur (niveau d'expérience, approche d'investissement, période de détention), cet essai fournit des preuves empiriques montrant que le comportement des investisseurs naïfs, sujets à des périodes d'excès d'optimisme ou de pessimisme Le deuxième essai propose une méthodologie permettant de mesurer l'attention des investisseurs aux informations en temps réel, en combinant les données des médias traditionnels avec le contenu des messages envoyés par une liste d'experts sur la plateforme Twitter. Cet essai démontre que lorsqu'une information attire l'attention des investisseurs, les mouvements de marchés sont caractérisés par une forte hausse des volumes échangés, une hausse de la volatilité et des sauts de prix. Le troisième essai étudie le risque de manipulation informationnelle en examinant un nouveau jeu de données de messages publiés sur Twitter à propos des entreprises de petite capitalisation. Cet essai propose une nouvelle méthodologie permettant d'identifier les comportements anormaux de manière automatisée en analysant les interactions entre les utilisateurs.
Ce travail s'inscrit dans la terminologie conceptuelle et vise à modéliser les connaissances (concepts et termes dénotant ceux-ci) dans la balance des paiements et la position extérieure. Pour ce faire, des outils de traitement automatique de la langue (TAL) et de développement d'ontoterminologies seront mis en œuvre ainsi que des échanges avec les experts du domaine. Les ontoterminologies élaborées seront exportées dans des formats d'échanges compréhensibles par un ordinateur, en particulier ceux du W3C. Enfin, cette thèse vise à constituer un dictionnaire électronique facilitant la transmission des connaissances du domaine à des non-experts.
Avec l'augmentation du nombre de capteurs et d'actuateurs dans les avions et le développement de liaisons de données fiables entre les avions et le sol, il est devenu possible d'améliorer la sécurité et la fiabilité des systèmes à bord en appliquant des techniques d'analyse en temps réel. Cependant, étant donné la disponibilité limité des ressources de calcul embarquées et le coût élevé des liaisons de données, les solutions architecturelles actuelles ne peuvent pas exploiter pleinement toutes les ressources disponibles, limitant leur précision. Notre but est de proposer un algorithme distribué de prédiction de panne qui pourrait être exécuté à la fois à bord de l'avion et dans une station au sol tout en respectant un budget de communication. Dans cette approche, la station au sol disposerait de ressources de calcul rapides et de données historiques et l'avion disposerait de ressources de calcul limitées et des données de vol actuelles. Dans cette thèse, nous étudierons les spécificités des données aéronautiques et les méthodes déjà existantes pour produire des prédictions de pannes à partir de ces dernières et nous proposerons une solution au problème posé. Notre contribution sera détaillé en trois parties. Premièrement, nous étudierons le problème de prédiction d'événements rares créé par la haute fiabilité des systèmes aéronautiques. Beaucoup de méthodes d'apprentissage en classification reposent sur des jeux de données équilibrés. Plusieurs approches existent pour corriger le déséquilibre d'un jeu de donnée et nous étudierons leur efficacité sur des jeux de données extrêmement déséquilibrés. Deuxièmement, nous étudierons le problème d'analyse textuelle de journaux car de nombreux systèmes aéronautiques ne produisent pas d'étiquettes ou de valeurs numériques faciles à interpréter mais des messages de journaux textuels. Nous étudierons les méthodes existantes basées sur une approche statistique et sur l'apprentissage profond pour convertir des messages de journaux textuels en une forme utilisable en entrée d'algorithmes d'apprentissage pour classification. Nous proposerons notre propre méthode basée sur le traitement du langage naturel et montrerons comment ses performances dépassent celles des autres méthodes sur un jeu de donnée public standard. Enfin, nous offrirons une solution au problème posé en proposant un nouvel algorithme d'apprentissage distribué s'appuyant sur deux paradigmes d'apprentissage existant, l'apprentissage actif et l'apprentissage fédéré. Nous détaillerons notre algorithme, son implémentation et fournirons une comparaison de ses performances avec les méthodes existantes
Le neuroblastome est le cancer solide extra-cranial le plus fréquent chez l'enfant. Il est caractérisé par une très grande hétérogénéité tant au niveau clinique que moléculaire. Pour répondre à cette question, il convient d'identifier chez les patients ayant rechuté, les différentes populations clonales coexistant au diagnostic et/ou à la rechute. Cela permet, entre autre, d'étudier les voies différemment altérées entre ces deux temps. Sur ces données, l'application de notre méthode a permis d'identifier des différences dans le ratio de variants prédits fonctionnels par rapport à ceux prédits passagers entre les populations ancestrales, enrichies à la rechute ou appauvries à la rechute.
L'objectif de cette thèse est de formaliser une méthodologie visant à valider l'architecture d'un système d'aide à l'installation d'outils d'assistance pour le pilotage d'un fauteuil électrique. Nous voulons redonner un début de mobilité suffisant à la personne embarquée, en utilisant des expériences de navigations dans des environnements virtuels. Pour cela il faut concevoir un système capable de trouver les améliorations pertinentes à apporter au fauteuil électrique. Nous avons élaboré une architecture basée sur des cycles "Expérience ? Analyse ? Modification du fauteuil roulant". Cette architecture se décompose en trois modules : un simulateur, un évaluateur, un configurateur. Ce travail s'inscrit dans le projet VAHM (développé au LASC) qui utilise des fauteuils électriques du commerce sur lesquels ont été ajoutés une série de capteurs et un calculateur. Notre simulateur remplace la partie physique du VHAM. A partir de l'analyse des données issues de l'étape d'expérience, nous avons calculé des critères représentatifs de comportements ou/et de situations particulières survenues lors de la navigation. Nous avons répertorié, en qualité de roboticiens, l'ensemble des situations problématiques selon différents aspects. Nous avons identifié et caractérisé chaque aspect par un ensemble de critères. Le vecteur contenant les valeurs des critères constitue l'entrée d'un système d'aide à la décision qui indiquera en sortie la ou les fonctionnalités à installer pour améliorer la mobilité du patient en fauteuil électrique. La base de connaissance du système d'aide à la décision s'appuie sur des réflexions, concernant les comportements et situations particulières qu'une personne handicapée va rencontrer pendant la navigation. L'originalité de notre travail provient de l'application orientée handicap permettant d'évaluer et de mettre en évidence les difficultés d'un groupe de personnes très physiquement handicapées, ne pouvant pas se déplacer en fauteuil électrique dans des mondes réels, en utilisant des mises en situation dans des environnements virtuels. L'architecture du système a permis de fournir des solutions d'assistance en passant par un système d'aide à la décision évolutif basé sur la logique floue.
De nos jours, les médias sociaux en ligne ont transformé notre façon de créer, de partager et d'accéder à l'information. Ces plateformes reposent sur de gigantesques réseaux favorisent le libre échange d'informations entre des centaines de millions de personnes à travers le monde entier, et cela de manière instantanée. Qu'ils soient en lien avec un évènement global ou en lien avec un évènement local, ces messages peuvent influencer une société et peuvent contenir des informations utiles pour la détection ou la prédiction de phénomènes du monde réel. Cependant, certains messages diffusés peuvent avoir un impact très négatif dans la vie réelle. Ces messages contenant une « infox » peuvent avoir des conséquences désastreuses. Pour éviter et anticiper ces situations dramatiques, suivre les rumeurs, éviter les mauvaises réputations, il est nécessaire d'étudier puis de modéliser la propagation de l'information. Or, la plupart des modèles de diffusion introduits reposent sur des hypothèses axiomatiques représentées par des modèles mathématiques. Par conséquent, ces modèles sont éloignés des comportements de diffusion des utilisateurs dans la mesure où ils n'intègrent pas les observations faites sur des cas concrets de diffusion. Dans nos travaux, nous étudions le phénomène de diffusion de l'information à deux échelles. À une échelle microscopique, nous avons observé les comportements de diffusion selon des traits de personnalité des utilisateurs en analysant les messages qu'ils publient en termes de sentiments et d'émotions. À une échelle macroscopique, nous avons analysé l'évolution du phénomène de diffusion en prenant en compte la dimension géographique des utilisateurs.
Les réseaux sociaux en ligne (OSNs) recueillent une masse de données à caractère privé. Notre thèse propose certaines réponses. Dans le premier chapitre nous analysons l'impact du partage des données personnelles de l'utilisateur sur sa vie privée. Tout d'abord, nous montrons comment les intérêts d'un utilisateur--à titre d'exemple ses préférences musicales--peuvent être à l'origine de fuite d'informations sensibles. Pour ce faire, nous inférons les attributs non divulgués du profil de l'utilisateur en exploitant d'autres profils partageant les même ''goûts musicaux''. Notre approche extrait la sémantique des intérêts en utilisant Wikipedia, les partitionne sémantiquement et enfin regroupe les utilisateurs ayant des intérêts semblables. Nos expérimentations réalisées sur plus de 104 milles profils publics collectés sur Facebook et plus de 2000 profils privés de bénévoles, montrent que notre technique d'inférence prédit efficacement les attributs qui sont très souvent cachés par les utilisateurs. Dans un deuxième temps, nous exposons les conséquences désastreuses du partage des données privées sur la sécurité. Nous nous focalisons sur les informations recueillies à partir de profils publics et comment celles-ci peuvent être exploitées pour accélérer le craquage des mots de passe. Premièrement, nous proposons un nouveau « craqueur » de mot de passe basé sur les chaînes de Markov permettant le cassage de plus de 80% des mots de passe, dépassant ainsi toutes les autres méthodes de l'état de l'art. Notre travail se base sur la plate-forme publicitaire d'estimationd'utilisateurs de Facebook pour calculer l'entropie de chaque attribut public. Ce calcul permet d'évoluer l'impact du partage de ces informations publiquement. Nos résultats, basées sur un échantillon de plus de 400 mille profils publics Facebook, montrent que la combinaison de sexe, ville de résidence et age permet d'identifier d'une manière unique environ 18% des utilisateurs. Dans la deuxième section de notre thèse nous analysons les interactions entre la plate-forme du réseau social et des tiers et son impact sur à la vie privée des utilisateurs. Nos résultats indiquent que le « tracking » utilisé par les OSNs couvre la quasi-totalité des catégories Web, indépendamment du contenu et de l'auditoire. Finalement, nous développons une plate-forme de mesure pour étudier l'interaction entre les plates-formes OSNs, les applications sociales et les « tierces parties » (e.g., fournisseurs de publicité). Nous démontrons que plusieurs applications tierces laissent filtrer des informations relatives aux utilisateurs à des tiers non autorisés. Ce comportement affecte à la fois Facebook et RenRen avec une sévérité variable : 22 % des applications Facebook testées transmettent au moins un attribut à une entité externe.
L'apprentissage machine (Machine Learning) est actuellement utilisé et testé dans de nombreux secteurs. L'utilisation de telles méthodes présente un intérêt évident pour la conception et la fabrication. Des applications intéressantes de ces approches lors du processus de conception et de fabrication concernent la prise de décision pour un choix de technologie de fabrication, une optimisation des paramètres de conception et de fabrication ou encore un suivi dynamique du processus de fabrication. Malheureusement, plusieurs barrières rendent difficiles leurs utilisations dans ces domaines spécifiques. En effet, le Machine Learning dans les approches actuellement médiatisées (apprentissage profond, réseaux convolutifs) nécessite l'utilisation d'un grand nombre de données pour entraîner les modèles. En conception et fabrication, la collecte d'une quantité suffisante de données est une opération parfois impossible, surtout lors des phases de préconception où la pièce et le produit n'existent pas encore. Par ailleurs, si la collecte de données est possible pour certaines phases du processus, les modèles entrainés sont souvent spécifiques à certaines technologies ou géométries. Par conséquent, ils sont difficilement généralisables. Plus spécifiquement, comment se rapprocher des performances cognitives humaines pour apprendre à partir d'un nombre de cas réduits dans le domaine de la fabrication ? Répondre à ces questions est essentielle pour permettre une application du Machine Learning aux secteurs de la conception et de la fabrication. Résoudre ces problèmes permettrait de disposer de modèles d'apprentissage pour la conception et la fabrication capables de généraliser. La prise de décisions concernant la conception, la technologie utilisée et la méthode de fabrication ainsi que l'optimisation dynamique des paramètres de fabrication sont des applications innovantes envisagées. Au cours de cette thèse, plusieurs pistes de recherche seront explorées. Toutes ces pistes considèrent le méta-apprentissage qui vise à se rapprocher des performances de l'apprentissage humain. La combinaison d'un travail de développement de métriques, de modèles et de méthodes d'optimisations constitue la base de la stratégie de recherche. Des métriques métiers capables de synthétiser la connaissance en conception et fabrication seront utilisées en s'appuyant sur la méthode DACM, une méthode de métamodélisation développée par l'université de Tampere. Cette méthode permet de créer un graphe causal qui représente un processus via une représentation graphique des liens de causes à effets entre les variables du problème considéré. La méthode DACM a été développée au cours d'une thèse en cotutelle précédente entre l'université Grenoble Alpes et l'université de Tampere. L'exploitation de ces précédents travaux est possible au travers des réseaux Bayésiens qui peuvent être vus comme l'étape qui suit la génération de graphes causaux. Ces réseaux Bayésiens permettent d'intégrer et découvrir de nouvelles corrélations et apportent une vision statistique du modèle causal. Ces approches sont utilisées en traitement du signal, mais aussi dans les systèmes de recommandation ou dans les systèmes d'analyse du langage. Ces méthodes ont démontré leurs performances dans les concours annuels organisés par Netflix. Une première preuve de concept a été déjà réalisée dans le cadre de la collaboration UGA / Tampere et vient d'être soumise en journal. Durant la thèse en cotutelle, l'objectif est d'explorer ces différentes approches pour des applications de prise de décision, de choix de paramètres optimaux de fabrication et conception et éventuellement si les progrès le permettent de contrôle en phase de fabrication. Un plan ambitieux de publications en journal et conférences dans les domaines de la fabrication et de la conception mais aussi de l'informatique est prévu. La thèse en cotutelle devrait être réalisée par compilation d'articles de journaux (4 minimum).
La modélisation des relations spatiales est essentielle dans une grande variété d'applications de réalité virtuelle, tel que des environnements d'apprentissage humain, des musées virtuels, des systèmes d'aides à la navigation. Cependant, les relations spatiales ont été considérées comme des informations abstraites et donc, difficiles à spécifier. Afin d'aborder cette question, cette thèse propose une approche pour modéliser les relations spatiales entre les objets virtuels dans des environnements de réalité virtuelle. Nous formalisons un modèle formel des relations spatiales dédié aux environnements de réalité virtuelle. Ensuite, nous proposons un langage et un cadre pour spécifier les relations spatiales à un niveau conceptuel. Nous montrons que le langage proposé est une base pertinente pour spécifier des contraintes spatiales liées aux activités des agents et des utilisateurs dans les environnements de réalité virtuelle.
Cette thèse se situe à l'intersection de deux domaines de recherche scientifique la Reconnaissance d' Alors que le second se focalise sur la résolution de problèmes d' Parmi les problèmes difficiles existants en ROS, le problème de la distance d'édition entre graphes (DEG) a été sélectionné comme le cœur de ce travail. Les contributions portent sur la conception de méthodes adoptées du domaine RO pour la résolution du problème de DEG. Explicitement, des nouveaux modèles linéaires en nombre entiers et des matheuristiques ont été développé à cet effet et de très bons résultats ont été obtenus par rapport à des approches existantes.
L'une des difficultés d'une langue peu dotée est l'inexistence des services liés aux technologies du traitement de l'écrit et de l'oral. Dans cette thèse, nous avons affronté la problématique de l'étude acoustique de la parole isolée et de la parole continue en Fongbe dans le cadre de la reconnaissance automatique de la parole. La complexité tonale de l'oral et la récente convention de l'écriture du Fongbe nous ont conduit à étudier le Fongbe sur toute la chaîne de la reconnaissance automatique de la parole. En plus des ressources linguistiques collectées (vocabulaires, grands corpus de texte, grands corpus de parole, dictionnaires de prononciation) pour permettre la construction des algorithmes, nous avons proposé une recette complète d'algorithmes (incluant des algorithmes de classification et de reconnaissance de phonèmes isolés et de segmentation de la parole continue en syllabe), basés sur une étude acoustique des différents sons, pour le traitement automatique du Fongbe. Dans ce manuscrit, nous avons aussi présenté une méthodologie de développement de modèles accoustiques et de modèles du langage pour faciliter la reconnaissance automatique de la parole en Fongbe. Dans cette étude, il a été proposé et évalué une modélisation acoustique à base de graphèmes (vu que le Fongbe ne dispose pas encore de dictionnaire phonétique) et aussi l'impact de la prononciation tonale sur la performance d'un système RAP en Fongbe. Enfin, les ressources écrites et orales collectées pour le Fongbe ainsi que les résultats expérimentaux obtenus pour chaque aspect de la chaîne de RAP en Fongbe valident le potentiel des méthodes et algorithmes que nous avons proposés.
On constate actuellement un engouement général pour la recherche d'informations dans les textes, dû sans doute à la disponibilité d'une masse considérable d'informations sur le réseau internet. D'où la nécessité des dictionnaires reliant étroitement sémantique et syntaxe. Ils'agit des verbes, des noms et des adjectifs qui impliquent un "dire" et dont nous avons fait le recensement le plus exhaustif possible. Les structures syntaxiques sont determinées par le contenu sémantique des prédicats, de sorte qu'il est patent que ces deux niveaux sont étroitement imbriqués. Ces classes cependant déterminent une structure argumentale qui leur est propre.
Pour pouvoir restituer des informations qui correspondent aux besoins de l'utilisateur, les mécanismes d'adaptation doivent disposer de métadonnées sur celui-ci telles que ses caractéristiques personnelles, ses préférences générales, ses centres d'intérêt. De ce fait, le profil utilisateur construit à partir de celles-ci devient central dans tout système basé sur la personnalisation. Nous appelons les techniques ou processus associés à cette approche " profilage social ". Le terme " profil social " désigne un profil construit à l'aide du réseau social de l'utilisateur. Un profil social contient les métadonnées traduisant les intérêts de l'utilisateur extraits à partir des informations partagées par les individus de son réseau social. Les intérêts de l'utilisateur évoluant au fil du temps dans la vie réelle, il en est de même pour ceux extraits depuis son réseau social : pertinents à un moment donné, ils peuvent ne plus être significatifs ultérieurement. Pour prendre en compte l'évolution des intérêts dans le profil social, nous avons proposé d'améliorer l'efficacité des processus de construction du profil social existants en intégrant la prise en compte de l'évolution du réseau social de l'utilisateur. Nous proposons d'intégrer un facteur temporel dans ces processus (approche basée sur des individus et approche basée sur les communautés). La solution permet de privilégier les intérêts provenant d'informations significatives et à jour. Il s'agit donc d'intégrer une mesure temporelle dans l'étape d'extraction et pondération des intérêts. Cette mesure est calculée d'une part, à partir de la pertinence temporelle des informations utilisées pour extraire cet intérêt et d'autre part, à partir de la pertinence temporelle de l'individu qui partage ces informations. Nous mettons en œuvre la méthode proposée au travers d'expérimentations dans deux réseaux sociaux différents : DBLP, un réseau de publications scientifiques et Twitter, un réseau de micro-blogs. Les résultats de ces expérimentations nous ont permis de montrer l'efficacité de la méthode temporelle proposée par rapport aux processus de construction du profil social qui ne prennent pas en compte des critères temporels. En étudiant les résultats en fonction des techniques de pondération des intérêts ou fonctions temporelles utilisées, nous constatons que la fonction temporelle et la technique utilisées donnant les meilleurs résultats varient selon l'approche de construction du profil social choisie, selon la taille et la densité du réseau étudié mais aussi selon sur le type de réseau.
Cette thèse porte sur la construction automatique d'outils et de ressources pour l'analyse linguistique de textes des langues peu dotées. Nous proposons une approche utilisant des réseaux de neurones récurrents (RNN-Recurrent Neural Networks) et n'ayant besoin que d'un corpus parallèle ou mutli-parallele entre une langue source bien dotée et une ou plusieurs langues cibles moins bien ou peu dotées. Ce corpus parallèle ou mutli-parallele est utilisé pour la construction d'une représentation multilingue des mots des langues source et cible. Nous avons utilisé cette représentation multilingue pour l'apprentissage de nos modèles neuronaux et nous avons exploré deux architectures neuronales : les RNN simples et les RNN bidirectionnels. Nous avons aussi proposé plusieurs variantes des RNN pour la prise en compte d'informations linguistiques de bas niveau (informations morpho-syntaxiques) durant le processus de construction d'annotateurs linguistiques de niveau supérieur (SuperSenses et dépendances syntaxiques). Nous avons démontré la généricité de notre approche sur plusieurs langues ainsi que sur plusieurs tâches d'annotation linguistique. Notre approche a les avantages suivants : (a) elle n'utilise aucune information d'alignement des mots, (b) aucune connaissance concernant les langues cibles traitées n'est requise au préalable (notre seule supposition est que, les langues source et cible n'ont pas une grande divergence syntaxique), ce qui rend notre approche applicable pour le traitement d'un très grand éventail de langues peu dotées, (c) elle permet la construction d'annotateurs multilingues authentiques (un annotateur pour N langages).
Ma thèse a pour but l'étude des désignations nominales des événements pour l'extraction automatique. Mes travaux s'inscrivent en traitement automatique des langues, soit dans une démarche pluridisciplinaire qui fait intervenir linguistique et informatique. L'extraction d'information a pour but d'analyser des documents en langage naturel et d'en extraire les informations utiles à une application particulière. Dans ce but général, de nombreuses campagnes d'extraction d'information ont été menées~ : pour chaque événement considéré, il s'agit d'extraire certaines informations relatives (participants, dates, nombres, etc.). Pourtant, ces travaux ne s'intéressent que peu aux mots utilisés pour décrire l'événement (particulièrement lorsqu'il s'agit d'un nom). L'événement est vu comme un tout englobant, comme la quantité et la qualité des informations qui le composent. Contrairement aux travaux en extraction d'informations générale, notre intérêt principal est porté uniquement sur la manière dont sont nommés les événements qui se produisent et particulièrement à la désignation nominale utilisée. Pour nous, l'événement est ce qui arrive, ce qui vaut la peine qu'on en parle. Les événements plus importants font l'objet d'articles de presse ou apparaissent dans les manuels d'Histoire. Un événement peut être évoqué par une description verbale ou nominale. Dans cette thèse, nous avons réfléchi à la notion d'événement. Nous avons observé et comparé les différents aspects présentés dans l'état de l'art jusqu'à construire une définition de l'événement et une typologie des événements en général, et qui conviennent dans le cadre de nos travaux et pour les désignations nominales des événements. Nous avons aussi dégagé de nos études sur corpus différents types de formation de ces noms d'événements, dont nous montrons que chacun peut être ambigu à des titres divers. Pour toutes ces études, la composition d'un corpus annoté est une étape indispensable, nous en avons donc profité pour élaborer un guide d'annotation dédié aux désignations nominales d'événements. Nous avons étudié l'importance et la qualité des lexiques existants pour une application dans notre tâche d'extraction automatique. Nous avons aussi, par des règles d'extraction, porté intérêt au cotexte d'apparition des noms pour en déterminer l'événementialité. À la suite de ces études, nous avons extrait un lexique pondéré en événementialité (dont la particularité est d'être dédié à l'extraction des événements nominaux), qui rend compte du fait que certains noms sont plus susceptibles que d'autres de représenter des événements. Utilisée comme indice pour l'extraction des noms d'événements, cette pondération permet d'extraire des noms qui ne sont pas présents dans les lexiques standards existants. Enfin, au moyen de l'apprentissage automatique, nous avons travaillé sur des traits d'apprentissage contextuels en partie fondés sur la syntaxe pour extraire de noms d'événements.
Dans le but de faciliter la tâche d'évaluation du niveau de sécurité incendie aux ingénieurs et permettre aux spécialistes impliqués dans le domaine d'utiliser leurs langages et outils préférés, nous proposons de créer un langage dédié au domaine de la sécurité incendie générant automatiquement une simulation en prenant en considération les langages métiers utilisés par les spécialistes intervenants dans le domaine. Ce DSL nécessite la définition, la formalisation, la composition et l'intégration de plusieurs modèles, par rapport aux langages spécifiques utilisés par les spécialistes impliqués dans le domaine. Le langage spécifique dédié au domaine de la sécurité incendie est conçu par composition et intégration de plusieurs autres DSLs décrits par des langages techniques et naturels (ainsi que des langages naturels faisant référence à des langages techniques). Ces derniers sont modélisés de manière à ce que leurs composants soient précis et fondés sur des bases mathématiques permettant de vérifier la cohérence du système (personnes et matériaux sont en sécurité) avant sa mise en œuvre. Dans ce contexte, nous proposons d'adopter une approche formelle, basée sur des spécifications algébriques, pour formaliser les langages utilisés par les spécialistes impliqués dans le système de génération, en se concentrant à la fois sur les syntaxes et les sémantiques des langages dédiés. Dans l'approche algébrique, les concepts du domaine sont abstraits par des types de données et les relations entre eux. La sémantique des langages spécifiques est décrite par les relations, le mapping (correspondances) entre les types de données définis et leurs propriétés. Le langage de simulation est basé sur un langage conçu par la composition de plusieurs DSL spécifiques précédemment décrits et formalisés. Les différents DSLs sont implémentés en se basant sur les concepts de la programmation fonctionnelle et le langage fonctionnel Haskell bien adapté à cette approche. Le résultat de ce travail est un outil informatique dédié à la génération automatique de simulation, dans le but de faciliter la tâche d'évaluation du niveau de sécurité incendie aux ingénieurs.
Nous proposons un modèle de vérification grammaticale automatique gauche-droite issu de l'analyse d'un corpus d'erreurs tapuscrites. Les travaux menés en psychologie cognitive ont montré que le processus de révision procède au travers de la confrontation d'une attente à un résultat. Ainsi, la détection d'une erreur grammaticale reposerait, chez l'humain, sur une attente du réviseur non comblée. Ce principe est à la base du modèle que nous avons élaboré. Pour faciliter la gestion des attentes du point de vue traitement numérique, nous convions deux concepts courants en TAL : le principe d'unification et la segmentation en chunks. Le premier est particulièrement adapté à la vérification des accords et le second constitue une unité de calcul intermédiaire permettant de définir des bornes simplifiant la recherche d'incohérences grammaticales. Enfin, l'originalité de ce modèle réside dans une analyse gauche-droite construite au fur et à mesure de la lecture/écriture.
Cette thèse se situe dans le contexte des modèles logique de Recherche d'Information (RI). Cependant, en étudiant les modèles actuels de RI basés sur la logique, nous montrons que ces modèles ont généralement des lacunes. Premièrement, les modèles de RI logiques proposent normalement des représentations complexes de document et des requête et difficile à obtenir automatiquement. Deuxièmement, la décision de pertinence d-&gt ; q, qui représente la correspondance entre un document d et une requête q, pourrait être difficile à vérifier. Enfin, la mesure de l'incertitude U(d-&gt ; q) est soit ad-hoc ou difficile à mettre en oeuvre. Dans cette thèse, nous proposons un nouveau modèle de RI logique afin de surmonter la plupart des limites mentionnées ci Nous représentons les documents et les requêtes comme des phrases logiques écrites en Forme Normale Disjonctive. Nous argumentons également que la décision de pertinence d-&gt ; q pourrait être remplacée par la validité de l'implication matérielle. Pour vérifier si d-&gt ; q est valide ou non, nous exploitons la relation potentielle entre PL et la théorie des treillis. Nous proposons d'abord une représentation intermédiaire des phrases logiques, où elles deviennent des noeuds dans un treillis ayant une relation d'ordre partiel équivalent à la validité de l'implication matérielle. En conséquence, nous transformons la vérification de validité de d-&gt ; q, ce qui est un calcul intensif, en une série de vérifications simples d'inclusion d'ensembles. Afin de mesurer l'incertitude de la décision de pertinence U(d-&gt ; q), nous utilisons la fonction du degré d'inclusion Z, qui est capable de quantifier les relations d'ordre partielles définies sur des treillis. Enfin, notre modèle est capable de travailler efficacement sur toutes les phrases logiques sans aucune restriction, et est applicable aux données à grande échelle. Notre modèle apporte également quelques conclusions théoriques comme : la formalisation de l'hypothèse de van Rijsbergen sur l'estimation de l'incertitude logique U(d-&gt ; q) en utilisant la probabilité conditionnelle P(q|d), la redéfinition des deux notions Exhaustivité et Spécificité, et finalement ce modèle a également la possibilité de reproduire les modèles les plus classiques de RI. De manière pratique, nous construisons trois instances opérationnelles de notre modèle. Une instance pour étudier l'importance de Exhaustivité et Spécificité, et deux autres pour montrer l'insuffisance de l'hypothèse sur l'indépendance des termes. Nos résultats expérimentaux montrent un gain de performance lors de l'intégration Exhaustivité et Spécificité. Le travail présenté dans cette thèse doit être poursuivit par plus d'expérimentations, en particulier sur l'utilisation de relations, et par des études théoriques en profondeur, en particulier sur les propriétés de la fonction Z.
Ce travail sur la réduction segmentale (i.e. délétion ou réduction temporelle) en français spontané nous a permis non seulement de proposer deux méthodes de recherche pour les études en linguistique, mais également de nous interroger sur l'influence de différents facteurs de variation sur divers phénomènes de réduction et d'apporter des connaissances sur la propension à la réduction des segments. Nous avons appliqué la méthode descendante qui utilise l'alignement forcé avec variantes lorsqu'il s'agissait de phénomènes de réduction spécifiques. Lorsque ce n'était pas le cas, nous avons utilisé la méthode ascendante qui examine des segments absents et courts. Trois phénomènes de réduction ont été choisis : l'élision du schwa, la chute du /ʁ/ et la propension à la réduction des segments. La méthode descendante a été utilisée pour les deux premiers. Les facteurs en commun étudiés sont le contexte post-lexical, le style, le sexe et la profession. L'élision du schwa en syllabe initiale de mots polysyllabiques et la chute du /ʁ/ post-consonantique en finale de mots ne sont pas toujours influencées par les mêmes facteurs. De même, l'élision du schwa lexical et celle du schwa épenthétique ne sont pas conditionnées par les mêmes facteurs. L'étude sur la propension à la réduction des segments nous a permis d'appliquer la méthode ascendante et d'étudier la réduction des segments de manière générale. Les résultats suggèrent que les liquides et les glides résistent moins à la réduction que les autres consonnes et que les voyelles nasales résistent mieux à la réduction que les voyelles orales. Parmi les voyelles orales, les voyelles hautes arrondies ont tendance à être plus souvent réduites que les autres voyelles orales.
L'identification automatique d'expressions polylexicales (EP) est un pré-requis pour de nombreuses applications de traitement automatique des langues. Cette tâche représente un défi car les EP, et en particulier les verbales (EPV) telles que 'casser sa pipe'(signifiant 'mourir'), ont des formes de surface très variables ('cassera-t-il un jour sa pipe ? On se penche ici sur un sous-problème de l'identification d'EPV, à savoir l'identification d'occurrences d'EPV vues dans d'autres contextes, quelque soit leur forme de surface, ce qui nécessite de prendre en compte l'ambiguïté pour éviter des lectures littérales ('casser sa vieille pipe') ou des co-occurrences fortuites ('casser le tuyau de sa pipe'). On considère pour cela deux approches : la première se fonde sur une mesure de la variabilité des EPV indépendante de la langue. La seconde consiste à modéliser le problème comme une tâche de classification d'après des traits pertinents pour la variabilité morpho-syntaxique des EPV, ce qui nous a conduit à développer un système (VarIDE), qui a participé à la compétition PARSEME d'identification automatique d'EPV en 2018.
Cette thèse se place dans le contexte d'Accordys, un projet d'ingénierie des connaissances qui vise à fournir un système de rapprochement de cas en fœtopathologie, qui est le domaine de l'étude des maladies rares et dysmorphies du fœtus. Ce projet se base sur un corpus de comptes rendus d'examens fœtaux. Ce matériel consiste en des comptes rendus en texte brut présentant un vocabulaire très spécifique (qui n'est que partiellement formalisé dans des terminologies médicales en français), des économies linguistiques (un style "prise de notes" très prononcé rendant difficile l'utilisation d'outils analysant la grammaire du texte) et une mise en forme matérielle exhibant une structuration commune latente (un découpage en sections, sous-sections, observations). La mise en correspondance entre cas et modèle (instanciation du modèle) est réalisée via un mapping d'arbres ayant pour base une méthode de Monte Carlo. Nous comparerons ceci avec des mesures de similarités obtenues en représentant nos comptes rendus (soit tels quels, soit enrichis sémantiquement grâce à un annotateur sémantique) dans un modèle vectoriel.
Cette thèse aborde le problème de la détection automatique des comparaisons figuratives dans des textes littéraires en prose écrits en français ou en anglais et propose un canevas pour décrire ces comparaisons d'un point de vue stylistique. Une comparaison figurative correspond ici à toute structure syntaxique qui met en parallèle au moins deux entités, déroge au principe de compositionnalité et crée une image mentale dans l'esprit de ceux à qui elle est destinée. Trois éléments principaux distinguent notre approche des travaux précédents : son ancrage dans les théories linguistiques et cognitives sur les comparaisons littérales et figuratives, sa capacité à gérer des marqueurs appartenant à différentes catégories grammaticales et sa flexibilité qui lui permet d'envisager différents scénarios syntaxiques. De fait, nous proposons une méthode comprenant trois modules complémentaires : - un module syntaxique qui utilise des dépendances syntaxiques et des règles manuelles pour identifier les comparaisons potentielles ainsi que leurs composantes ; - un module sémantique qui mesure la saillance des motifs détectés et la similarité sémantique des termes comparés en se basant sur une base de données préétablie ; - et un module d'annotation qui fournit entre autres des informations sur le type de comparaison (idiomatique, sensorielle…) et sur les catégories sémantiques employées. Pour finir, au vu des données recueillies au cours des deux campagnes d'annotation que nous avons menées, il paraît clair que la détection automatique des comparaisons figuratives doit tenir compte de plusieurs facteurs parmi lesquels la saillance, la catégorisation et la syntaxe de la phrase.
Notre travail a pour objectif une analyse syntaxique des adverbes de temps coréens dont l'interprétation correspond à une durée ou à une date (e. G. 3sigan dongan (pendant 3 heures), 5uel 6il (le 6 mai)). Pour la linguistique formelle comme pour le traitement informatique des langues, une description aussi exhaustive et explicite que possible est indispensable. La méthodologie du lexique-grammaire (M. Gross 1975, 1986) nous a fourni un modèle de description formelle et systématique de la langue naturelle. Nous avons choisi de décrire les combinaisons lexicales concernées par des graphes d'automates finis, qui constituent autant de "grammaires locales" représentant les différents types de séquences adverbiales possibles. Nos graphes peuvent être intégrés directement à un analyseur syntaxique automatique pour localiser les adverbes de durée et de date en coréen dans des textes quelconques. Nous consacrons le chapitre 3 à l'analyse des formes interprétables comme des durées et le chapitre 4 à celle des formes interprétables comme des dates. Nous analysons comment les groupes nominaux de temps étudiés dans le deuxième chapitre peuvent entrer dans des phrases qui donnent lieu à des interprétations de durée ou de date
Un système SHM par ondes guidées a pour but d'évaluer l'intégrité d'une grande variété de structures fines, telles que les fuselages d'avions, les tuyaux, les réservoirs, etc. Un tel système est basé sur un réseau de capteurs piézoélectriques pour l'excitation et la mesure des ondes guidées. Cette thèse présente les travaux menés dans le but de développer un système de SHM par ondes guidées capable de détecter, localiser et dimensionner efficacement les défauts dans des structures aéronautiques assimilables à des plaques, en matériaux composites ou en aluminium. Ce travail comprend également une étude approfondie des algorithmes d'imagerie DAS, MV et Excitelet, les plus prometteurs parmi ceux de la littérature, une évaluation de leurs performances par analyse statistique sur une grande base de données de résultats de simulations d'imagerie par ondes guidées et propose une méthode d'imagerie parcimonieuse. Alors que la détection et la localisation des défauts à partir de l'analyse des images est aisée, le dimensionnement du défaut est un problème plus complexe en raison de sa forte dimensionnalité et de sa non-linéarité. Il est démontré que ce problème peut être résolu par des méthodes d'apprentissage automatique sur une grande base de données de résultats de simulations d'imagerie par ondes guidées. Elles sont efficaces dans des conditions opérationnelles stationnaires mais sont sensibles aux variations de l'environnement et notamment aux fluctuations de température. Ce travail présente donc l'étude de la robustesse face aux effets thermiques des méthodes d'imagerie par ondes guidées et propose un modèle de détection de défauts capable d'analyser des résultats d'imagerie détériorés. Plusieurs techniques de compensation des effets thermiques sont étudiées et des améliorations sont proposées. Leur efficacité est validée pour les plaques d'aluminium mais des améliorations supplémentaires sont nécessaires pour les étendre aux plaques de composites.
La présente thèse étudie la modélisation conjointe des contenus visuels et textuels extraits à partir des documents multimédias pour résoudre les problèmes intermodaux. Ces tâches exigent la capacité de ``traduire''l'information d'une modalité vers une autre. Un espace de représentation commun, par exemple obtenu par l'Analyse Canonique des Corrélation ou son extension kernelisée est une solution généralement adoptée. Sur cet espace, images et texte peuvent être représentés par des vecteurs de même type sur lesquels la comparaison intermodale peut se faire directement. Néanmoins, un tel espace commun souffre de plusieurs déficiences qui peuvent diminuer la performance des ces tâches. Le premier défaut concerne des informations qui sont mal représentées sur cet espace pourtant très importantes dans le contexte de la recherche intermodale. Le deuxième défaut porte sur la séparation entre les modalités sur l'espace commun, ce qui conduit à une limite de qualité de traduction entre modalités. Pour faire face au premier défaut concernant les données mal représentées, nous avons proposé un modèle qui identifie tout d'abord ces informations et puis les combine avec des données relativement bien représentées sur l'espace commun. Les évaluations sur la tâche d'illustration de texte montrent que la prise en compte de ces information fortement améliore les résultats de la recherche intermodale. La contribution majeure de la thèse se concentre sur la séparation entre les modalités sur l'espace commun pour améliorer la performance des tâches intermodales. Nous proposons deux méthodes de représentation pour les documents bi-modaux ou uni-modaux qui regroupent à la fois des informations visuelles et textuelles projetées sur l'espace commun. Nos approches permettent d'obtenir des résultats de l'état de l'art pour la recherche intermodale ou la classification bi-modale et intermodale.
Les objectifs de cette thèse s'inscrivent dans la large problématique de recherche d'information dans les données issues du Dossier Patient Informatisé (DPI). Les aspects abordés dans cette problématique sont multiples : d'une part la mise en oeuvre d'une recherche d'information clinomique au sein du DPI et d'autre part la recherche d'information au sein de données non structurées issues du DPI. Dans un premier temps, l'un des objectifs de cette thèse est d'intégrer au sein du DPI des informations dépassant le cadre de la médecine pour intégrer des données, informations et connaissances provenant de la biologie moléculaire ;  L'intégration de ce type de données permet d'améliorer les systèmes d'information en santé, leur interopérabilité ainsi que le traitement et l'exploitation des données à des fins cliniques. Un enjeu important est d'assurer l'intégration de données hétérogènes, grâce à des recherches sur les modèles conceptuels de données, sur les ontologies et serveurs terminologiques et sur les entrepôts sémantiques. L'intégration de ces données et leur interprétation selon un même modèle de données conceptuel sont un verrou important. Enfin, il est important d'intégrer recherche clinique et recherche fondamentale afin d'assurer une continuité des connaissances entre recherche et pratique clinique et afin d'appréhender la problématique de personnalisation des soins. Cette thèse aboutit ainsi à la conception et au développement d'un modèle générique des données omiques exploité dans une application prototype de recherche et visualisation dans les données omiques et cliniques d'un échantillon de 2 000 patients. Le second objectif de ma thèse est l'indexation multi terminologique de documents médicaux à travers le développement de l'outil Extracteur de Concepts Multi-Terminologique (ECMT). Il exploite les terminologies intégrées au portail terminologique Health Terminology/Ontology Portal (HeTOP) pour identifier des concepts dans des documents non structurés. Ainsi, à partir d'un document rédigé par un humain, et donc porteur potentiellement d'erreurs de frappe, d'orthographe ou de grammaire,l'enjeu est d'identifier des concepts et ainsi structurer l'information contenue dans le document. Pour la recherche d'information médicale, l'indexation présente un intérêt incontournable pour la recherche dans les documents non structurés, comme lescomptes
Les réseaux ad hoc mobiles sont des réseaux qui se forment spontanément grâce à la présence de terminaux mobiles. Ces réseaux sans fil sont de faible capacité. Les nœuds se déplacent librement et de manière imprévisible et ils se déchargent très rapidement. En conséquence, un réseau MANET est très enclin à subir des partitionnements fréquents. La réplication des données constitue un mécanisme prometteur pour pallier ce problème. Cependant, la mise en œuvre d'un tel mécanisme dans un environnement aussi contraint en ressources constitue un réel défi. L'objectif principal est donc de réaliser un mécanisme peu consommateur en ressources. Le second objectif de la réplication est de permettre le rééquilibrage de la charge induite par les requêtes de données. Dans cette thèse, nous proposons CReaM (Community-Centric and Resource-Aware Replication Model”) un modèle de réplication adapté à un réseau MANET. CReaM fonctionne en mode autonomique : les prises de décisions se basent sur des informations collectées dans le voisinage du nœud plutôt que sur des données globalement impliquant tous les nœuds, ce qui permet de réduire le trafic réseau lié à la réplication. Pour réduire l'usage des ressources induit par la réplication sur un nœud, les niveaux de consommation des ressources sont contrôlés par un moniteur. Toute consommation excédant un seuil prédéfini lié à cette ressource déclenche le processus de réplication. Pour permettre le choix de la donnée à répliquer, une classification multi critères a été proposée (rareté de la donnée, sémantique, niveau de demande) ; et un moteur d'inférence qui prend en compte l'état de consommation des ressources du nœud pour désigner la catégorie la plus adaptée pour choisir la donnée à répliquer. Pour permettre de placer les réplicas au plus près des nœuds intéressés, CReaM propose un mécanisme pour l'identification et le maintien à jour des centres d'intérêt des nœuds. Les utilisateurs intéressés par un même sujet constituent une communauté. Par ailleurs, chaque donnée à répliquer est estampillée par le ou les sujets au(x)quel(s) elle s'apparente. Un nœud désirant placer un réplica apparenté à un sujet choisira le nœud ayant la plus grande communauté sur ce sujet. Les résultats d'expérimentations confirment la capacité de CReaM à améliorer la disponibilité des données au même niveau que les solutions concurrentes, tout en réduisant la charge liée à la réplication.
Cette thèse cherche à établir si les différents processus de catégorisation influençant les évaluations des audiences sur les marchés conduisent à une stabilisation ou à une plus grande variabilité de leurs évaluations. Bien que les travaux de recherche fondateurs portant sur la catégorisation aient insisté sur le rôle stabilisateur des catégories sur les marchés, la recherche récente suggère que les évaluations des audiences peuvent varier substantiellement, même sur des marchés dotés de catégories pré-existantes bien établies. Cette variabilité résulte notamment des préférences hétérogènes des audiences pour les offres typiques, de changements dans les significations associées aux catégories ou de l'utilisation par les audiences de plusieurs modes d'évaluation. En se basant sur ces nouveaux résultats, cette thèse cherche pourquoi les évaluations des audiences sont si variables et explore en détail le rôle joué par les catégories de marché dans cette variabilité. Cette thèse propose que i) les catégories ambigües, ii) l'influence d'attractions temporaires parmis les audiences aux côtés des catégories plus stables et iii) la co-existence de plusieurs types d'évaluateurs contribuent à produire de la variabilité dans les évaluations des audiences. Les deux premiers essais empiriques utilisent des données sur des entreprises cotées en bourse aux Etats-Unis. Dans ces essais, la similarité des entreprises aux prototypes des catégories existantes ou l'attraction temporaire des audiences vers certains attributs sont mesurés à l'aide de contenus sémantiques extraits d'un large corpus de rapports annuels et de prospectus d'entrée en bourse. Le troisième essai est un modèle théorique. Cette thèse contribue à la littérature sur le rôle des catégories sur les marchés, à la recherche émergente sur le niveau de distinction optimal et aux approches computationelles de l'étude des organisations.
Les indices du genre sont construits à un double niveau : morphosyntaxique, à partir d'un système de descripteurs original et adapté, et textuel, en fonction des composantes et des thématiques. Cette partition est méthodologique, l'interaction étant constante dans la structuration du texte. Après une description d'ensemble du corpus, une recherche contrastive met en évidence des principes de variation extrinsèques : incidence du style d'auteur, confrontation de l'article à d'autres formes de genres scientifiques, variations d'un domaine à l'autre et incidence de la langue choisie (103 articles de linguistique en anglais) ;  Cette étude quantitative et qualitative a permis de saisir les différents aspects d'un objet normatif complexe et multidimensionnel en le caractérisant sur différents plans de sa réalisation et de ses régulations linguistiques.
La difficulté de lecture de Una meditación a été soulignée tant par la critique que par Juan Benet lui-même. Ce travail essaie de caractériser cette difficulté et, par ce biais, la spécificité de l'expérience de lecture du roman de Benet. Notre étude s'appuie sur la psycholinguistique de la compréhension des textes, qui nous permet de définir la norme de lisibilité implicite par rapport à laquelle Una meditación est jugé « difficile » . Nous étudions les deux aspects qui, par rapport à cette norme, constituent les principales sources de difficulté du texte bénétien : la disposition de la matière romanesque (au niveau du récit et de la phrase) et le système de référenciation des personnages. Sur le plan de la disposition, le récit et – à son échelle – la phrase se caractérisent par une forte discontinuité pourtant dissimulée, par un développement temporel de forme spirale, et par le brouillage des relations hiérarchiques entre les événements. Sur le plan de la référenciation, la notion de nom du personnage perd son sens traditionnel, car les noms sont peu employés, ambigus, multiples, ou inexistants ;  mais c'est surtout l'omniprésence de la référenciation pronominale qui déroute le lecteur en lui exigeant implicitement de ne pas oublier un seul détail du texte. Nous analysons aussi la figure du narrateur et nuançons une lecture courante selon laquelle le texte serait le produit d'une remémoration. Nous concluons que la « difficulté » de Una meditación semble être au service d'une écriture qui, à travers l'indifférenciation des personnages et des histoires, dépasse la fiction et vise un portait générique de la nature humaine.
En été 2013, le terme de "Big Data" fait son apparition et suscite un fort intérêt auprès des entreprises. Cette thèse étudie ainsi l'apport de ces méthodes aux sciences actuarielles. Elle aborde aussi bien les enjeux théoriques que pratiques sur des thématiques à fort potentiel comme l'textit{Optical Character Recognition} (OCR), l'analyse de texte, l'anonymisation des données ou encore l'interprétabilité des modèles. Commençant par l'application des méthodes du machine learning dans le calcul du capital économique, nous tentons ensuite de mieux illustrer la frontrière qui peut exister entre l'apprentissage automatique et la statistique. Mettant ainsi en avant certains avantages et différentes techniques, nous étudions alors l'application des réseaux de neurones profonds dans l'analyse optique de documents et de texte, une fois extrait. L'utilisation de méthodes complexes et la mise en application du Réglement Général sur la Protection des Données (RGPD) en 2018 nous a amené à étudier les potentiels impacts sur les modèles tarifaires. En appliquant ainsi des méthodes d'anonymisation sur des modèles de calcul de prime pure en assurance non-vie, nous avons exploré différentes approches de généralisation basées sur l'apprentissage non-supervisé. Enfin, la réglementation imposant également des critères en terme d'explication des modèles, nous concluons par une étude générale des méthodes qui permettent aujourd'hui de mieux comprendre les méthodes complexes telles que les réseaux de neurones
La thèse vise à évaluer de nouvelles formes d'interaction Homme-Machine. Ainsi, comportements et réactions d'utilisateurs sont recueillis à l'aide de méthodes ergonomiques et techniques de suivi du regard (eye-tracking). Une approche expérimentale a été adoptée afin d'évaluer l'apport de chaque axe. Pour cela, des participants (étudiants de niveau Licence) ont manipulé un logiciel de création d'animation qui leur était inconnu (Flash) afin de réaliser trois scénarios. Tout au long de leur découverte du logiciel, les participants étaient accompagnés d'un dispositif d'aide intégrant, suivant l'expérimentation, un ACA (fourni par FT R &amp ;  D) ou une technique d'adaptation (détection d'intention et évolution suivant les connaissances). Les différentes études réalisées montrent que les deux sources d'innovation employées ont été perçues positivement par la majorité des participants. Elles ont montré d'autre part qu'un ACA a un effet rassurant et qu'il peut vraisemblablement être utilisé lors de la prise en main d'un logiciel. Pour le système adaptatif, le fait que le système évolue de manière autonome n'a pas perturbé les participants, mais n'améliore guère les performances.
Cette thèse concerne l'étude des débats argumentatifs entre plusieurs agents artificiels. Notre travail est motivé par les difficultés qui surgissent quand un nombre important d'utilisateurs interagissent et débattent sur le Web, en échangeant des arguments sur différents sujets. Ces difficultés se situent au niveau de la représentation des connaissances des agents impliqués dans le débat, de la représentation du débat, de la façon de tirer les conclusions du débat, de l'évaluation de la qualité du débat, de la définition des protocoles spécifiques d'interaction, et de l'étude des stratégies des agents qui souhaitent atteindre un but précis via le débat. La contribution de cette thèse consiste donc en :  a) la modélisation d'un débat argumentatif entre plusieurs agents, la modélisation des expertises de ces derniers, et l'agrégation des opinions des différents experts sur différentes parties d'un débat ;  b) l'apport d'une aide à l'agent responsable de la gestion d'un débat donné, la proposition d'une méthode permettant d'évaluer la qualité des débats argumentatifs en fonction de la confiance que l'on peut avoir en leurs conclusions, ainsi que la proposition de solutions permettant d'améliorer la qualité des débats dont les conclusions ne sont pas clairement établies ;  c) l'apport d'une aide permettant aux agents qui participent à un débat argumentatif de déterminer quels arguments placer dans le débat, l'étude des systèmes argumentatifs munis d'une structure dynamique, l'étude des moyens disponibles permettant à un agent d'influencer un système dynamique afin d'atteindre son but, l'étude des modifications minimales permettant à un agent d'atteindre un objectif donné, l'étude des stratégies argumentatives basées sur ce changement minimal ;  d) la définition, l'étude et l'évaluation des protocoles argumentatifs multi-agents, ainsi que la définition de protocoles de différents types (1) basés sur une évaluation numérique d'arguments et (2) basés sur des extensions d'arguments, l'utilisation des différentes techniques pour assurer la cohérence d'un débat tout en laissant une liberté d'expression aux agents, et enfin un grand nombre d'expérimentations (sur des débats) permettant de tester différentes stratégies et de les évaluer en fonction de différents critères.
La prise de son et le traitement multicanale sont des objets de recherche à Orange Labs. Une thèse en cours traite du filtrage multicanal par apprentissage et montre la valeur ajoutée des réseaux de neurones pour estimer un filtre de rehaussement spatial robuste. La thèse proposée vise à utiliser de nouvelles méthodes d'apprentissage pour fournir des informations nécessaires au filtrage : les début et fin de phrase et la position au cours du temps des sources sonores. L'idée est de coupler une antenne ambisonique avec des réseaux de neurones profonds. Ainsi, la thèse s'intéresse à 3 aspects de la localisation : l'estimation du nombre de sources dans le mélange, l'estimation des directions d'arrivées des sources et le suivi des sources au cours du temps (tracking).
La linguistique informatique est un domaine de recherche qui se concentre sur les méthodes et les perspectives de la modélisation formelle (statistique ou symbolique) de la langue naturelle. La première partie de la recherche présentée ici se concentre sur la création d'un analyseur syntaxique de surface (ou analyseur en chunks) pour le hongrois. La sortie de l'analyseur de surface est conçue pour servir d'entrée pour un traitement ultérieur visant à annoter les relations de dépendance entre le prédicat et ses compléments essentiels et circonstanciels. L'analyseur profond est mis en œuvre dans NooJ (Silberztein, 2004) en tant qu'une cascade de grammaires. Le deuxième objectif de recherche était de proposer une représentation lexicale pour la structure argumentale en hongrois. Cette représentation doit pouvoir gérer la vaste gamme de phénomènes qui échappent à la dichotomie traditionnelle entre un complément essentiel et un circonstanciel (p. ex. Nous avons eu recours à des résultats de la recherche récente sur la réalisation d'arguments et choisi un cadre qui répond à nos critères et qui est adaptable à une langue non-configurationnelle. La première étape consistait à définir les règles de codage et de construire un vaste base de données lexicale pour les verbes et leurs compléments. Et, plus généralement, quelle est la nature des alternances spécifiques aux classes verbales en hongrois ? Dans la phase finale de la recherche, nous avons étudié le potentiel de l'acquisition automatique pour extraire des classes de verbes à partir de corpus.
Ma recherche prend la forme d'une enquête au sein des milieux de production des savoirs français contemporains qui vise à comprendre les différentes significations du terme open en sciences. J'ai considéré le qualificatif open comme une formule, dont l'analyse de ses traductions en français (ouvert, libre, gratuit) tout autant que des noms qui lui sont associés (science, data, access) constitue le fil directeur de son étude. Cette enquête, qui a débuté en 2013, s'est surtout centrée sur un évènement particulier, la consultation sur le projet de loi pour une République numérique (septembre-octobre 2015), en particulier l'article 9 sur « le libre accès aux publications scientifiques de la recherche publique » . Cette consultation en ligne a donné une envergure nationale et publique aux problématiques d'accès aux savoirs. En tant qu'épreuve de réalité « équipée » d'un dispositif numérique participatif, elle a été l'occasion d'observer presque « en direct » la défense de différentes conceptions de « ce que devrait être » le régime contemporain des savoirs en France. M'inscrivant dans une démarche par théorisation ancrée, j'ai constitué progressivement, à propos de ce moment particulier de cristallisation des débats sur l'open en sciences, un corpus de documents reflétant le déploiement des échanges sur des espaces/dispositifs numériques distincts : site web de la consultation, blogs scientifiques, revues académiques, médias « grand public » , rapports. Les mouvements itératifs de cette enquête, alliant méthodes numériques (réalisation d'une cartographie de similarité des votes) et analyse qualitative du corpus, tout autant que les concepts théoriques mobilisés à la croisée entre sciences de l'information et de la communication et sociologie pragmatique de la critique, ont donné lieu à une modélisation. Par la suite, en passant de la modélisation à une théorisation transposable à d'autres terrains de recherche, je montre comment, derrière les discours sur l'open, la distinction entre deux logiques (technoindustrielle ou processuelle) peut être pertinente pour analyser les reconfigurations actuelles d'autres agencements sociétaux. Les stratégies dans l'épreuve employées lors de la consultation illustrent dans ce sens la coexistence de deux conceptions « numériques » de la démocratie (représentative étendue ou contributive), présentes dans le design même de la plateforme consultative. Dans la dernière partie, je propose d'expliquer les dynamiques de reconfiguration d'un esprit et d'un agencement sociétal dans une interprétation énactive en considérant les couplages permanents entre cognition, actions médiées par les technologies et environnement sociotechnique. L'expérience même du doctorat narrée tout au long de ce récit constitue aussi l'exemple d'un processus d'énaction sur mes propres conceptions de l'open. En ce sens, elle ouvre une piste de réflexion sur la nature située et incarnée de toute production de savoirs, qui n'échappe pas aux limites tout autant qu'aux potentialités de la métacognition.
L'étude des usages et pratiques informationnels des usagers en démarche de recherche d'information est un axe majeur de la recherche en sciences de l'information et de la communication, à en juger par les travaux consacrés à cette problématique et disponibles sur Internet. Si les pays du Nord ont une tradition bien ancrée en la matière, au Sud, par contre, et plus précisément dans les pays francophones d'Afrique au sud du Sahara, peu d'études ont été consacrées à cette thématique. La présente étude se place dans ce contexte thématique et géographique, et vise comme public cible les doctorants de l'Université de Bamako. Il s'agira d'abord de définir une typologie de cette communauté d'utilisateurs de l'information (spécialisation, écoles doctorales et laboratoires d'attache, thématique de recherche, etc.), puis d'évaluer ses besoins et pratiques informationnels et enfin d'identifier l'existence de difficultés éventuelles d'accès à l'information et de proposer des pistes de solutions. Pour la collecte de données une enquête réalisée auprès de doctorants et l'observation de leurs comportements en situation de recherche d'information seront privilégiées. Les résultats de l'étude permettront une meilleure connaissance des besoins et pratiques informationnels de la communauté des doctorants de l'université de Bamako et pourraient être utilisés par les unités d'information en vue d'améliorer l'offre de services d'information à leur intention.
Notre étude a pour objet les représentations discursives de l'animation japonaise. En France, le dessin animé japonais, l'anime, fait aujourd'hui partie des habitudes culturelles de plusieurs générations, mais a connu des débuts tumultueux à son arrivée dans le pays au milieu des années 70. Au Japon d'où il est originaire, l'anime fait depuis longtemps partie de la culture populaire du pays. Nous avons cherché à savoir comment l'anime, décrit comme étant bien implanté au Japon mais aussi en France, était perçu par les jeunes habitants de ces deux pays. Nous avons également souhaité savoir si la culture de ces deux pays pouvait exercer une influence sur la façon dont il est dépeint. Nous avons effectué une enquête auprès d'étudiants français et japonais à qui nous avons soumis un questionnaire dans leur langue respective. L'analyse discursive et sémantique des réponses des participants français et japonais a permis d'extraire leurs représentations en matière d'anime, mais aussi en ce qui concerne les cultures française et japonaise. Nous avons ensuite procédé à l'analyse comparative de nos résultats qui a permis de confronter le regard des étudiants français à celui de leurs homologues japonais.
La construction de ressources linguistiques arabes riches en informations syntaxiques constitue un enjeu important pour le développement de nouveaux outils de traitement automatique. Cette thèse propose une approche pour la création d'un treebank de l'arabe intégrant des informations d'un type nouveau reposant sur le formalisme des Grammaires de Propriétés. Une propriété syntaxique caractérise une relation pouvant exister entre deux unités d'une certaine structure syntaxique. Nous avons pu ainsi construire, à l'aide de cette grammaire, d'autres ressources linguistiques arabes. Une grammaire de propriétés lexicalisée probabiliste fait partie de son modèle d'apprentissage pour pouvoir affecter positivement le résultat d'analyse et caractériser ses structures syntaxiques avec les propriétés de ce modèle. Nous avons enfin évalué les résultats obtenus en les comparant à celles du Stanford Parser.
Ce travail de thèse se focalise sur une classe de méthodes d'apprentissage profond, probabilistes et non-supervisées qui utilisent l'inférence variationnelle pour créer des modèles évolutifs de grande capacité pour ce type de données. Nous présentons deux classes d'apprentissage variationnel profond, puis nous les appliquons à deux problèmes spécifiques liés au domaine maritime. La première application est l'identification de systèmes dynamiques à partir de données bruitées et partiellement observées. Nous introduisons un cadre qui fusionne l'assimilation de données classique et l'apprentissage profond moderne pour retrouver les équations différentielles qui contrôlent la dynamique du système. En utilisant une formulation d'espace d'états, le cadre proposé intègre des composantes stochastiques pour tenir compte des variabilités stochastiques, des erreurs de modèle et des incertitudes de reconstruction. La deuxième application est la surveillance du trafic maritime à l'aide des données AIS. Nous proposons une architecture d'apprentissage profond probabiliste multitâche pouvant atteindre des performances très prometteuses dans différentes tâches liées à la surveillance du trafic maritime, telles que la reconstruction de trajectoire, l'identification du type de navire et la détection d'anomalie, tout en réduisant considérablement la quantité de données à stocker et le temps de calcul. Pour la tâche la plus importante-la détection d'anomalie, nous introduisons un détecteur géospatialisé qui utilise l'apprentissage profond variationnel pour construire une représentation probabiliste des trajectoires AIS, puis détecter les anomalies en jugeant la probabilité de cette trajectoire.
The Chronicles of Narnia (1950-1956) est un célèbre recueil de sept romans, traditionnellement reconnus comme des œuvres de littérature de jeunesse et à l'origine du genre fantasy. L'un des intérêts majeurs de ces livres réside dans leur substrat symbolique, exprimé dans le texte par un double niveau de lecture qui évoque la tradition chrétienne. Notre thèse consiste en l'analyse d'un corpus incluant les originaux en anglais et les traductions françaises, publiées sous le titre de Le Monde de Narnia (2005) Parmi ces marqueurs sont notamment analysés les déictiques, la modalité, la transitivité, les choix lexicaux et la prosodie sémantique. Les parties du discours correspondant à ces marqueurs sont analysées notamment en lien avec l'instance du narrateur, de par son rôle clé pour l'idéologie dans le texte, et de régie dans la focalisation. Notre analyse porte une attention particulière à la dimension du sacré et aux thèmes de la violence, de la mort et du genre en traduction de littérature de jeunesse. La littérature de jeunesse, toujours plus ou moins caractérisée par un but éducatif, tout comme les livres qui constituent notre corpus, s'avère un véhicule axiologique puissant, qui reflète les valeurs qu'une société défend et transmet à un moment donné. Notre travail a montré notamment que les traductions françaises ont tendance à affaiblir le message religieux, éloignant le regard du lecteur ou rendant flous les contours de l'espace. Dans l'ensemble, l'idéologie du texte cible est caractérisée par un certain nombre d'écarts par rapport au texte source et met en avant d'autres valeurs, pourtant présentes, elles aussi, dans l'original. Ce travail de recherche montre une méthode permettant d'aborder le texte dans le but d'une meilleure compréhension des enjeux qui sous-tendent la traduction, et en particulier la traduction de l'idéologie et du point de vue dans les livres pour enfants.
Au Mexique, l'un des problèmes technologiques prioritaires est la préservation du patrimoine culturel sous sa forme numérique. Dans cette recherche, l'intérêt principal est la commande, la gestion et l'identification du patrimoine culturel immatériel en images. En vision par ordinateur, l'intégration du système visuel humain dans les méthodes d'apprentissage automatique et les classificateurs est devenue un domaine de recherche intensif pour la reconnaissance d'objets et l'extraction de contenu. Les cartes dites de saillance, sont définies comme une représentation topographique de l'attention visuelle sur une scène, modélisant l'attention instantanément et attribuant un degré d'intérêt à chaque valeur de pixel de l'image. Les cartes des points saillants se sont avérées très efficaces pour mettre en évidence les régions d'intérêt dans plusieurs tâches de contenu visuel et de sa compréhension. Dans ce contexte, nous nous concentrons sur l'intégration des modèles d'attention visuelle dans le pipeline de formation des réseaux neuronaux profonds pour la reconnaissance des structures architecturales mexicaines. Nous considérons que les principales contributions de cette recherche se situent dans les domaines d'intérêt suivants :  i) Ensemble de données à usage spécifique : la collecte de données relatives au sujet est une tâche essentielle pour résoudre le problème de la classification architecturale. ii) Sélection des données : nous utilisons des méthodes de prédiction des points saillants pour sélectionner et recadrer les régions pertinentes pour le contexte sur les images. iii) Modélisation de l'attention visuelle : nous annotons les images par une tâche réelle d'observation des images, nous enregistrons les fixations des yeux avec un système de suivi des yeux pour construire des cartes de saillance subjective. iv) Intégration de l'attention visuelle : nous intégrons l'attention visuelle dans les réseaux neuronaux profonds de deux manières : a) pour filtrer les caractéristiques dans une couche de regroupement basée sur les points saillants et b) avec des mécanismes d'attention. Dans cette recherche, différentes composantes essentielles à la formation d'un réseau neuronal sont abordées dans le but de reconnaître le contenu culturel mexicain et d'extrapoler ces résultats à des bases de données à grande échelle dans des tâches de classification similaires, comme dans ImageNet. Enfin, nous montrons que l'intégration de modèles d'attention visuelle « générés par une expérience psycho-visuelle » permet de réduire le temps de formation et d'améliorer les performances en termes de précision.
La pharmacovigilance est une discipline fondamentale pour la sécurité et la confiance dans le médicament. Cette discipline a évolué au fil du temps et s'est renforcée, mais souffre encore d'imperfection. Nous nous sommes proposés dans ce travail d'apporter une solution originale d'amélioration. Dans une première partie, nous décrivons et analysons l'évolution de la pharmacovigilance et son fonctionnement actuel tant du point de vue juridique que du point de vue scientifique, et ce, tant au niveau national qu'au niveau de l'Union Européenne. Nous avons analysé les insuffisances juridiques et pratiques puis avons fait des propositions pour les combler. Nous avons donc formulé un certain nombre de possibilités, avant de développer dans la deuxième partie une approche originale : la pédagogie. En partant du constat que la pharmacovigilance de terrain repose sur les professionnels de santé, nous avons étudié l'offre de formation de ces derniers et proposer d'apporter une formation universitaire plus qualitative et suffisamment quantitative, en pharmacovigilance et iatrogénie, le tout en s'appuyant sur des méthodes pédagogiques et des outils adaptés aux étudiants actuels. Le paradigme pédagogique proposé s'appuie sur une pédagogie d'explication, qui se base sur la recherche en droit pharmaceutique et sur une pédagogie innovante hybride, alliant cours en présentiel et ressources en e-learning. Les outils utilisés sont notamment des exposés magistraux complétés par des évaluations formatives sous forme de tests par boîtiers de vote électroniques et des cas cliniques en e-learning disponible sur une plateforme. Cette pédagogie doit mener les étudiants vers une meilleure compréhension de la pharmacovigilance et une pratique de la gestion des évènements iatrogènes médicamenteux. En conséquence le travail mené permet de mieux former les professionnels de santé à une gestion du risque médicamenteux avec un objectif final de confortation du système de pharmacovigilance. Ce dispositif sera complété dans le futur par de la simulation et de jeux de rôle.
L'instrumentation massive des systèmes industriels permet aujourd'hui de générer de gros volumes de données de surveillance et de maintenance qui sont encore actuellement sous-exploitées dans le cadre de la maintenance prévisionnelle. Un enjeu important pour la valorisation de ces données concerne donc le développement d'algorithmes permettant l'analyse en continu de ces données, complexes et hétérogènes, afin de prédire les défaillances et de planifier les opérations de maintenance. Les approches actuelles d'apprentissage automatique et notamment l'apprentissage profond et leurs succès et résultats impressionnants dans le domaine de la reconnaissance visuelle et du traitement du langage naturel sont donc à fort potentiel pour la maintenance prévisionnelle. Des premières approches ont été proposées sur des cas d'application particulier de maintenance.
Cette thèse présente un cadre formel pour l'interaction Homme-robot (HRI), qui reconnaître un important lexique de gestes statiques et dynamiques mesurés par des capteurs portatifs. Gestes statiques et dynamiques sont classés séparément grâce à un processus de segmentation. Les tests expérimentaux sur la base de données de gestes UC2017 ont montré une haute précision de classification. La classification en ligne des gestes permet une classification prédictive avec réussit. Le réseau propose a atteint une haute précision de rejet de les gestes non entraînés de la base de données UC2018 DualMyo.
Nous avons defini un modele de representation des connaissances contenues dans le discours, que ce soit un texte ou un dialogue homme-machine. Cette representation est fondee sur des bases linguistiques et notre modele s'appuie sur des elements du fonctionnement cognitif. Nous proposons un formalisme oriente objet, dont les fondements theoriques sont les systemes logiques de lesniewski : l'ontologie et la mereologie. Le premier repose sur un foncteur primitif appele "epsilon" interprete comme est un, le second sur la relation partie de appelee " l'ingredience ". Ces systemes logiques constituent une base theorique plus adaptee que la logique classique des predicats.
Le but de ce travail est d'examiner les propriétés sémantiques et morphosyntaxiques des noms abstraits apparentés à des prédicats verbaux ou adjectivaux. En nous fondant sur l'hypothèse que le caractère statif commun à ces noms permet une analyse unifiée, nous proposons une étude de leurs différents emplois et montrons notamment qu'outre une acception stative, ces noms peuvent avoir une seconde lecture et dénotent alors des occurrences. Dans la seconde partie, nous nous intéressons au comportement syntaxique des noms statifs, i.e. le nombre et la détermination, mais aussi la modification adjectivale. Ceci nous permet de dégager deux comportements morphosyntaxiques distincts, corrélés à la distinction entre les deux lectures mise en évidence dans la première partie. Dans leur lecture stative, ces noms ont un comportement proche de celui des noms massifs concrets et fonctionnent comme des noms relationnels : ils nécessitent un argument avec lequel ils entrent dans une relation syntaxique de prédication. Inversement, dans leur lecture d'occurrence, ces noms se comportent comme des noms comptables concrets et ne sont pas intrinsèquement relationnels. L'analyse des noms statifs que nous proposons tend à montrer que ceux-ci partagent leurs propriétés sémantiques avec certains types de prédicats verbaux et adjectivaux, et leurs propriétés syntaxiques avec diverses classes de noms concrets.
Les travaux de cette thèse explorent les propriétés de procédures d'estimation par agrégation appliquées aux problèmes de régressions en grande dimension. Cependant, le comportement théorique de l'agrégat avec extit{prior} de Laplace Le Chapitre 2 explicite une borne du risque de prédiction de cet estimateur. Le Chapitre 4 introduit des variantes du Lasso pour améliorer les performances de prédiction dans des contextes partiellement labélisés.
Ces dernières années, le besoin de données géographiques de référence a significativement augmenté. Pour y répondre, il est nécessaire de mettre jour continuellement les données de référence existantes. Cette tâche est coûteuse tant financièrement que techniquement. Pour ce qui concerne les réseaux routiers, trois types de voies sont particulièrement complexes à mettre à jour en continu : les chemins piétonniers, les chemins agricoles et les pistes cyclables. Cette complexité est due à leur nature intermittente (elles disparaissent et réapparaissent régulièrement) et à l'hétérogénéité des terrains sur lesquels elles se situent (forêts, haute montagne, littoral, etc.).En parallèle, le volume de données GPS produites par crowdsourcing et disponibles librement augmente fortement. Le nombre de gens enregistrant leurs positions, notamment leurs traces GPS, est en augmentation, particulièrement dans le contexte d'activités sportives. Ces traces sont rendues accessibles sur les réseaux sociaux, les blogs ou les sites d'associations touristiques. Cependant, leur usage actuel est limité à des mesures et analyses simples telles que la durée totale d'une trace, la vitesse ou l'élévation moyenne, etc. Une attention particulière est portée aux voies existantes mais absentes du référentiel. L'approche proposée se compose de trois étapes :  La première consiste à évaluer et augmenter la qualité des traces GPS acquises par la communauté. Cette qualité a été augmentée en filtrant (1) les points extrêmes à l'aide d'un approche d'apprentissage automatique et (2) les points GPS qui résultent d'une activité humaine secondaire (en dehors de l'itinéraire principal). Les points restants sont ensuite évalués en termes de précision planimétrique par classification automatique. La seconde étape permet de détecter de potentielles mises à jour. Pour cela, nous proposons une solution d'appariement par distance tampon croissante. Cette distance est adaptée à la précision planimétrique des points GPS classifiés pour prendre en compte la forte hétérogénéité de la précision des traces GPS. Nous obtenons ainsi les parties des traces n'ayant pas été appariées au réseau de voies des données de référence. Ces parties sont alors considérées comme de potentielles voies manquantes dans les données de référence. Finalement nous proposons dans la troisième étape une méthode de décision multicritère visant à accepter ou rejeter ces mises à jour possibles. Les voies manquantes dans les données de références IGN BDTOPO
Cette thèse s'inscrit dans le cadre du projet PERDIDO dont les objectifs sont l'extraction et la reconstruction d'itinéraires à partir de documents textuels. Les objectifs de cette thèse sont de concevoir un système automatique permettant d'extraire, dans des récits de voyages ou des descriptions d'itinéraires, des déplacements, puis de les représenter sur une carte. Nous proposons une approche automatique pour la représentation d'un itinéraire décrit en langage naturel. Notre approche est composée de deux tâches principales. La première tâche a pour rôle d'identifier et d'extraire les informations qui décrivent l'itinéraire dans le texte, comme par exemple les entités nommées de lieux et les expressions de déplacement ou de perception. La seconde tâche a pour objectif la reconstruction de l'itinéraire. Notre proposition combine l'utilisation d'information extraites grâce au traitement automatique du langage ainsi que des données extraites de ressources géographiques externes (comme des gazetiers). L'étape d'annotation d'informations spatiales est réalisée par une approche qui combine l'étiquetage morpho-syntaxique et des patrons lexico-syntaxiques (cascade de transducteurs) afin d'annoter des entités nommées spatiales et des expressions de déplacement ou de perception. Une première contribution au sein de la première tâche est la désambiguïsation des toponymes, qui est un problème encore mal résolu en NER et essentiel en recherche d'information géographique. Nous proposons un algorithme non-supervisé de géo-référencement basé sur une technique de clustering capable de proposer une solution pour désambiguïser les toponymes trouvés dans les ressources géographiques externes, et dans le même temps proposer une estimation de la localisation des toponymes non référencés. Nous proposons un modèle de graphe générique pour la reconstruction automatique d'itinéraires, où chaque noeud représente un lieu et chaque segment représente un chemin reliant deux lieux. Un calcul d'arbre de recouvrement minimal à partir d'un graphe pondéré est utilisé pour obtenir automatiquement un itinéraire sous la forme d'un graphe. Chaque segment du graphe initial est pondéré en utilisant une méthode d'analyse multi-critère combinant des critères qualitatifs et des critères quantitatifs. La valeur des critères est déterminée à partir d'informations extraites du texte et d'informations provenant de ressources géographique externes. Par exemple, nous combinons les informations issues du traitement automatique de la langue comme les relations spatiales décrivant une orientation (ex : se diriger vers le sud) avec les coordonnées géographiques des lieux trouvés dans les ressources pour déterminer la valeur du critère "relation spatiale". De plus, à partir de la définition du concept d'itinéraire et des informations utilisées dans la langue pour décrire un itinéraire, nous avons modélisé un langage d'annotation d'information spatiale adapté à la description de déplacements, s'appuyant sur les recommendations du consortium TEI (Text Encoding and Interchange). Enfin, nous avons implémenté et évalué les différentes étapes de notre approche sur un corpus multilingue de descriptions de randonnées (Français, Espagnol et Italien).
La Gestion de Connaissance et de l'innovation sont des thèmes à forte importance dans l'actualité, surtout parce que ces deux sujets sont liés et ils influencent la performance des entreprises. L'objectif de cette recherche est d'analyser le lien entre gestion de la connaissance et de l'innovation à partir de trois industries de produit simple dans le pôle industriel de Barcarena, état du Pará, au Brésil. On veut évaluer à la fois la relation de la Gestion de Connaissance dans ces entreprises et leur capacité d'innovation, pour comprendre l'influence des connaissances pour la capacité innovatrice, surtout l'innovation incrémentale. Pour cela, on se servira de modèles d'analyse qui prennent en compte des facteurs tels quels la culture, le leadership, la technologie, les ressources humaines et les processus. Notre approche méthodologique est qualitative. On prit comme base théorique les concepts et la littérature autour de la Gestion de Connaissance et de l'innovation. L'axe de l'industrie fut choisi par son importance économique dans la région où la recherche a été développée. De plus, selon les résultats les entreprises qui mieux gèrent les connaissances ont plus de possibiliter d'innover.
La production et la diffusion de musique numérisée ont explosé ces dernières années. Une telle quantité de données à traiter nécessite des méthodes efficaces et rapides pour l'analyse et la recherche automatique de musique. Cette thèse s'attache donc à proposer des contributions pour l'analyse sémantique de la musique, et en particulier pour la reconnaissance du genre musical et de l'émotion induite (ressentie par l'auditoire), à l'aide de descripteurs de bas-niveau sémantique mais également de niveau intermédiaire. Afin d'accéder aux propriétés sémantiques à partir des descripteurs bas-niveau, des modélisations basées sur des algorithmes de types K-means et GMM utilisant des BoW et Gaussian super vectors ont été envisagées pour générer des dictionnaires. Compte-tenu de la très importante quantité de données à traiter, l'efficacité temporelle ainsi que la précision de la reconnaissance sont des points critiques pour la modélisation des descripteurs de bas-niveau. Ainsi, notre première contribution concerne l'accélération des méthodes K-means, GMM et UMB-MAP, non seulement sur des machines indépendantes, mais également sur des clusters de machines. Afin d'atteindre une vitesse d'exécution la plus importante possible sur une machine unique, nous avons montré que les procédures d'apprentissage des dictionnaires peuvent être réécrites sous forme matricielle pouvant être accélérée efficacement grâce à des infrastructures de calcul parallèle hautement performantes telle que les multi-core CPU ou GPU. En particulier, en s'appuyant sur GPU et un paramétrage adapté, nous avons obtenu une accélération de facteur deux par rapport à une implémentation single thread. Concernant le problème lié au fait que les données ne peuvent pas être stockées dans la mémoire d'une seul ordinateur, nous avons montré que les procédures d'apprentissage des K-means et GMM pouvaient être divisées par un schéma Map-Reduce pouvant être exécuté sur des clusters Hadoop et Spark. En utilisant notre format matriciel sur ce type de clusters, une accélération de 5 à 10 fois a pu être obtenue par rapport aux librairies d'accélération de l'état de l'art. En complément des descripteurs audio bas-niveau, des descripteurs de niveau sémantique intermédiaire tels que l'harmonie de la musique sont également très importants puisqu'ils intègrent des informations d'un niveau d'abstraction supérieur à celles obtenues à partir de la simple forme d'onde. Ainsi, notre seconde contribution consiste en la modélisation de l'information liée aux notes détectées au sein du signal musical, en utilisant des connaissances sur les propriétés de la musique. Cette contribution s'appuie sur deux niveaux de connaissance musicale : le son des notes des instruments ainsi que les statistiques de co-occurrence et de transitions entre notes. Pour le premier niveau, un dictionnaire musical constitué de notes d'instruments a été élaboré à partir du synthétiseur Midi de Logic Pro 9. Basé sur ce dictionnaire, nous avons proposé un algorithme « Positive Constraint Matching Pursuit » (PCMP) pour réaliser la décomposition de la musique.
L'objectif de cette thèse est d'évaluer l'intérêt d'une nouvelle interface de commande pour fauteuil roulant électrique, un joystick à retour d'effort, destinée à des personnes handicapés moteurs ayant des difficultés à contrôler classiquement leur fauteuil. Ce joystick devra être implémenté sur un fauteuil «  intelligent  » muni de capteurs télémétriques. Le retour d'effort est calculé en fonction de la proximité des obstacles et aide l'utilisateur sans le contraindre à se diriger vers la direction libre. Le premier chapitre du mémoire est une étude bibliographique portant sur les fauteuils «  intelligents  » , sur les modes de commande en téléopération, sur les interfaces haptiques en robotique et sur la modélisation des tâches de pilotage. Le second chapitre décrit la conception d'un simulateur de pilotage de fauteuil destiné à tester des fonctionnalités nouvelles. Le troisième et dernier chapitre porte sur un ensemble de résultats expérimentaux visant à conclure sur l'intérêt du retour d'effort pour le pilotage de fauteuils électriques et sur le choix de son algorithme de calcul. Les paramètres testés sont notamment la configuration de l'environnement (couloir, passage de porte, espace libre, …) et la cinématique du fauteuil (traction avant, traction arrière)
Comment les Travellers anglais, gallois, écossais, irlandais et des Etats-Unis sont-ils représentés à travers quelques vieux mythes gadji qui ont la vie dure.
Ces dernières années, l'apprentissage profond a complètement changé le domaine de vision par ordinateur. Plus rapide, donnant de meilleurs résultats, et nécessitant une expertise moindre pour être utilisé que les méthodes classiques de vision par ordinateur, l'apprentissage profond est devenu omniprésent dans tous les problèmes d'imagerie, y compris l'imagerie médicale. Afin de trouver automatiquement des réseaux de neurones adaptés à des tâches spécifiques, nous avons ainsi apporté des contributions à l'optimisation d'hyper-paramètres de réseaux de neurones. Cette thèse propose une comparaison de certaines méthodes d'optimisation, une amélioration en performance d'une de ces méthodes, l'optimisation bayésienne, et une nouvelle méthode d'optimisation d'hyper-paramètres basé sur la combinaison de deux méthodes existantes : l'optimisation bayésienne et hyperband. Une fois équipés de ces outils, nous les avons utilisés pour des problèmes d'imagerie médicale : la classification de champs de vue en IRM, et la segmentation du rein en échographie 3D pour deux groupes de patients. Cette dernière tâche a nécessité le développement d'une nouvelle méthode d'apprentissage par transfert reposant sur la modification du réseau de neurones source par l'ajout de nouvelles couches de transformations géométrique et d'intensité. En dernière partie, cette thèse revient vers les méthodes classiques de vision par ordinateur, et nous proposons un nouvel algorithme de segmentation qui combine les méthodes de déformations de modèles et l'apprentissage profond. Cette méthode est validé sur la tâche de la segmentation du rein en échographie 3D.
Dans de nombreux domaines, les graphes constituent une représentation naturelle efficace pour différents types de données. Des exemples notables existent pour réaliser des analyses comportementales dans les domaines de la cybersécurité ou de l'analyse des réseaux sociaux. Dans le premier cas, le comportement des utilisateurs sur Internet peut être observé par leurs requêtes DNS, interprétées comme les étapes successives d'un marcheur aléatoire sur un graphe dans lequel les noms de domaine sont les sommets et les arêtes représentent le comportement moyen au niveau de la population. Il est alors possible d'étudier le comportement des utilisateurs en analysant le sous-graphe induit par des mouvements d'un unique utilisateur. Dans le cas de l'analyse des réseaux sociaux, les représentations graphiques résultent naturellement des interactions de l'utilisateur. Les noeuds symbolisent ainsi les utilisateurs, et leurs interactions peuvent être interprétées comme des arêtes (partage d'intérêts ou de messages). Comprendre et analyser les structures des graphes est donc un outil clé dans de nombreux domaines d'applications réelles. Il est donc essentiel de trouver des méthodes efficaces et robustes pour la classification ou le regroupement de noeuds ou de graphes. Dans ce contexte, les réseaux de neurones sur graphes apparaissent comme une technologie clé, mais soulèvent des questions cruciales quant à leur robustesse face aux attaques contradictoires (adversarial attacks) et à la confidentialité des données qu'elles manipulent. L'objectif de cette thèse est d'explorer la robustesse et la confidentialité des approches basées sur les réseaux de neurones sur graphes, en examinant des solutions combinant des algorithmes à réponses aléatoires et le chiffrement homomorphe afin d'assurer un compromis satisfaisant entre performance, robustesse et confidentialité des données.
L'objectif de cette recherche est de batir un répertoire descriptif generalise des structures interrogatives directes du francais. Le travail est pluridisciplinaire : linguistique, informatique, documentation. Dans la partie linguistique nous présentons la définition formelle de la transformation et des types de transformation permis pour le traitement des interrogatives. Apres avoir explicite les conventions de notation valables pour l'ensemble du travail linguistique, nous proposons les transformations particulières portant sur les dix éléments interrogatifs du francais (qui, quoi, que, pourquoi, combien, ou, quand, comment, quel, lequel) et les inversions (est-il venu ? que fait pierre ? qui est-ce qui est venu ?...). L'implémentation sous forme d'un répertoire vise ensuite l'incorporation des contraintes apportées par la description linguistique. Nous expliquons en quoi consiste notre repertoire informatise des structures interrogatives du francais (risif), programme a l'aide du langage fx, et de quelle maniere nous proposons des codages et des diagnostics de séquences interrogatives. Enfin, nous proposons une consultation de bases de donnees (cbd) a visee linguistique. La cbd peut etre distincte du repertoire risif ou directement liee a celui-ci. La visualisation et la comparaison de structures extraites des bases de donnees et celles analysees precedemment (dans risif) sont possibles. On peut également modifier et creer des BD au sein de l'environnement fx lisp.
Cependant, la construction de telles ressources est coûteuse. Elles ne sont donc disponibles que pour un nombre restreint de langues et de domaines. De plus, les choix de formalisation de la sémantique peuvent différer selon les besoins (requêtes SQL, formules logiques,...). Il est dans ce contexte nécessaire de développer des méthodes qui soient moins dépendantes de ces données de supervision, par exemple en exploitant des ressources linguistiques. Dans le cadre de cette thèse, nous nous intéresserons à développer des approches jointes pour l'analyse sémantique et la génération contrôlée de textes fondées sur des architectures neuronales de type 'auto-encodeurs' : une phrase est encodée en une représentation latente (par analyse sémantique) puis regénérée à partir de cette dernière (génération contrôlée). Nous nous focaliserons sur des approches faiblement supervisées en utilisant des connaissances sur la langue, le domaine ainsi que le formalisme visé plutôt que sur l'accumulation de données annotées.
La traduction automatique vise à traduire des documents d'une langue à une autre sans l'intervention humaine. Avec l'apparition des réseaux de neurones profonds (DNN), la traduction automatique neuronale(NMT) a commencé à dominer le domaine, atteignant l'état de l'art pour de nombreuses langues. Combiné avec la flexibilité architecturale des DNN, ce cadre a aussi ouvert une piste de recherche sur la multimodalité, ayant pour but d'enrichir les représentations latentes avec d'autres modalités telles que la vision ou la parole, par exemple. Cette thèse se concentre sur la traduction automatique multimodale(MMT) en intégrant la vision comme une modalité secondaire afin d'obtenir une meilleure compréhension du langage, ancrée de façon visuelle. J'ai travaillé spécifiquement avec un ensemble de données contenant des images et leurs descriptions traduites, où le contexte visuel peut être utile pour désambiguïser le sens des mots polysémiques, imputer des mots manquants ou déterminer le genre lors de la traduction vers une langue ayant du genre grammatical comme avec l'anglais vers le français. Je propose deux approches principales pour intégrer la modalité visuelle : (i) un mécanisme d'attention multimodal qui apprend à prendre en compte les représentations latentes des phrases sources ainsi que les caractéristiques visuelles convolutives, (ii) une méthode qui utilise des caractéristiques visuelles globales pour amorcer les encodeurs et les décodeurs récurrents. Grâce à une évaluation automatique et humaine réalisée sur plusieurs paires de langues, les approches proposées se sont montrées bénéfiques. Enfin,je montre qu'en supprimant certaines informations linguistiques à travers la dégradation systématique des phrases sources, la véritable force des deux méthodes émerge en imputant avec succès les noms et les couleurs manquants.
Cette thèse est consacrée à la problématique des stratégies d'apprentissage et d'intégration, quelles que soient leurs modalités (visuelles, textuelles), destinées à réaliser efficacement des opérations de détection et d'annotation de concepts visuels, problématique qui est devenue un sujet de recherche très populaire et important ces dernières années en raison de sa large gamme d'applications : indexation et récupération d'images ou de vidéos, systèmes de contrôle d'accès, vidéo-surveillance, etc.
Le cerveau humain est composé d'un grand nombre de réseaux neuraux interconnectés, dont les neurones et les synapses en sont les briques constitutives. Caractérisé par une faible consommation de puissance, de quelques Watts seulement, le cerveau humain est capable d'accomplir des tâches qui sont inaccessibles aux systèmes de calcul actuels, basés sur une architecture de type Von Neumann. La conception de systèmes neuromorphiques vise à réaliser une nouvelle génération de systèmes de calcul qui ne soit pas de type Von Neumann. L'utilisation de mémoire non-volatile innovantes en tant que synapses artificielles, pour application aux systèmes neuromorphiques, est donc étudiée dans cette thèse. L'utilisation des dispositifs PCM en tant que synapses de type binaire et probabiliste est étudiée pour l'extraction de motifs visuels complexes, en évaluant l'impact des conditions de programmation sur la consommation de puissance au niveau du système. Une nouvelle stratégie de programmation, qui permet de réduire l'impact du problème de la dérive de la résistance des dispositifs PCM est ensuite proposée. Il est démontré qu'en utilisant des dispositifs de tailles réduites, il est possible de diminuer la consommation énergétique du système. La variabilité des dispositifs OxRAM est ensuite évaluée expérimentalement par caractérisation électrique, en utilisant des méthodes statistiques, à la fois sur des dispositifs isolés et dans une matrice complète de mémoire. Un modèle qui permets de reproduire la variabilité depuis le niveau faiblement résistif jusqu'au niveau hautement résistif est ainsi développé. Une architecture de réseau de neurones de type convolutionnel est ensuite proposée sur la base de ces travaux éxperimentaux.
Ces travaux de thèse portent sur la reconnaissance automatique du stress chez des humains en interaction dans des situations anxiogènes : prise de parole en public, entretiens et jeux sérieux à partir d'indices Une partie des travaux portent sur la fusion des informations apportées par les différentes modalités. L'expression et la gestion du stress sont influencées à la fois par des différences interpersonnelles (traits de personnalité, expériences passées, milieu culturel) et contextuelles (type de stresseur, enjeux de la situation). Les comparaisons inter-individus, et inter-corpus révèlent la diversité de l'expression du stress. Une application de ces travaux pourrait être la conception d'outils thérapeutiques pour la maitrise du stress, notamment à destination des populations phobiques.
Le projet GenderedNews vise à proposer de nouvelles méthodes pour mesurer et expliquer le niveau de biais de genre dans les médias en France. Ces biais peuvent être définis comme le fait que les médias d'information tendent d'une part à surpondérer les hommes par rapport aux femmes en termes de mentions et de citations, et d'autre part à attribuer aux femmes un rôle social spécifique impliquant souvent, entre autres, l'anonymat, une capacité d'action réduite dans la société et la confusion entre cette action et leur état matrimonial ou familial. De nombreuses études empiriques ont prouvé l'existence de ces biais et ont permis de mieux les comprendre à l'échelle internationale. Cependant, la recherche sur cette question est souvent basée sur des données limitées en volume et produites par des ONG, des administrations ou des organismes de réglementation des médias. Ces données sont généralement traitées par des analyses de contenu manuelles qui ne permettent pas de rendre compte systématiquement des évolutions des biais sexistes dans les médias à long terme et sur un grand nombre de sources, ni d'expliquer ces biais en termes de variables telles que le financement des médias, la taille des salles de rédaction ou d'autres variables organisationnelles. Le projet GenderedNews vise à fournir et à analyser des sources de données importantes et stables dans le temps ainsi qu'à explorer de nouvelles méthodes pour documenter les préjugés sexistes dans les médias. Il est basé sur un programme de recherche collaboratif entre un sociologue des médias et un informaticien ayant des compétences en études des médias, études de genre, traitement du langage naturel et collecte de données numériques. Il a également une dimension de partenariat importante dans la mesure où plusieurs médias importants sont associés et fournissent un accès à leurs données. GenderedNews se concentre sur deux types de biais et deux mesures différentes de ces biais. Les biais d'échantillonnage se produisent par la sélection d'un échantillon biaisé de personnes mentionnées dans les médias. Ils peuvent être étudiés en comptant simplement combien d'hommes et de femmes ont accès à la visibilité publique d'un côté et en étudiant les modèles de cadrage des hommes et des femmes représentés de l'autre côté. Les biais de citation proviennent de la sélection d'un échantillon biaisé de personnes qui, en plus d'être visibles, sont autorisées à exprimer leurs opinions dans les médias. Ils peuvent également être étudiés en utilisant les deux approches : compter combien d'un côté et analyser comment de l'autre. Dans le cadre du projet, le/la doctorante contribuera plus spécifiquement à l'étude des biais de citation.
Tout professionnel de la santé est sujet devant un patient à une incertitude inhérente à la pratique médicale. Dans le cas d'incident médical lors d'un trajet aérien, cette incertitude comporte trois sources additionnelles : (1) variabilité des conditions aéronautiques, (2) variabilité individuelle des conditions du patient, (3) variabilité individuelle des compétences de l'intervenant. Aujourd'hui les incidents médicaux dans l'avion sont estimés à 350 par jour dans le monde et lorsqu'ils surviennent, ils sont pris en charge dans 95% des cas par des professionnels de la santé passagers qui se portent volontaires. C'est souvent pour eux une première expérience. A part l'assistance à distance par télémédecine l'intervenant, souvent seul face à ses doutes et son incertitude, ne dispose d'aucune autre aide à bord. Par ailleurs l'aviation civile dispose de systèmes de retour d'expérience (RETEX) pour gérer la complexité de tels processus. Des politiques de recueil et d'analyse des événements sont mises en place à l'échelle internationale, par exemple ECCAIRS (European Co-ordination Centre for Accident and Incident Reporting Systems) et ASRS (Aviation Safety Reporting System).Dans ce travail de thèse, nous proposons tout d'abord une formalisation sémantique basée sur les ontologies pour préciser conceptuellement le vocabulaire des incidents médicaux se produisant durant les vols commerciaux. Enfin, nous proposons une architecture de Système d'Aide à la Décision Médicale (SADM) qui intègre la gestion des incertitudes présentes tant sur les données récoltées que les niveaux de compétences des professionnels médicaux intervenants.
Afin d'atteindre cet objectif, un système de recherche d'information doit représenter, stocker et organiser l'information, puis fournir à l'utilisateur les éléments correspondant au besoin d'information exprimé par sa requête. La plupart des systèmes de recherche d'information (SRI) utilisent des termes simples pour indexer et retrouver des documents. Cependant, cette représentation n'est pas assez précise pour représenter le contenu des documents et des requêtes, du fait de l'ambiguïté des termes isolés de leur contexte. Une solution à ce problème consiste à utiliser des termes complexes à la place de termes simples isolés. Cette approche se fonde sur l'hypothèse qu'un terme complexe est moins ambigu qu'un terme simple isolé. Notre thèse s'inscrit dans le cadre de la recherche d'information dans un domaine de spécialité en langue arabe. L'objectif de notre travail a été d'une part, d'identifier les termes complexes présents dans les requêtes et les documents. D'autre part, d'exploiter pleinement la richesse de la langue en combinant plusieurs connaissances linguistiques appartenant aux niveaux morphologique et syntaxique, et de montrer comment l'apport de connaissances morphologiques et syntaxiques permet d'améliorer l'accès à l'information. En outre, nous avons avons défini linguistiquement les termes complexes en langue arabe et nous avons développé un système d'identification de termes complexes sur corpus qui produit des résultats de bonne qualité en terme de précision, en s'appuyant sur une approche mixte qui combine modèle statistique et données linguistiques
La duplication de code source a de nombreuses origines : copie et adaptation inter-projets ou clonage au sein d'un même projet. Rechercher des correspondances de code copié permet de le factoriser dans un projet ou de mettre en évidence des situations de plagiat. Nous étudions des méthodes statiques de recherche de similarité sur du code ayant potentiellement subi des opérations d'édition telle que l'insertion, la suppression, la transposition ainsi que la factorisation et le développement de fonctions. Des techniques d'identification de similarité génomique sont examinées et adaptées au contexte de la recherche de clones de code source sous forme lexemisée. Après une discussion sur des procédés d'alignement de lexèmes et de recherche par empreintes de n-grams, est présentée une méthode de factorisation fusionnant les graphes d'appels de fonctions de projets au sein d'un graphe unique avec introduction de fonctions synthétiques exprimant les correspondances imbriquées. Elle utilise des structures d'indexation de suffixes pour la détermination de facteurs répétés. Une autre voie d'exploration permettant de manipuler de grandes bases indexées de code par arbre de syntaxe est abordée avec la recherche de sous-arbres similaires par leur hachage et leur indexation selon des profils d'abstraction variables. En amont et en aval de la recherche de correspondances, des métriques de similarité sont définies afin de préselectionner les zones d'examen, affiner la recherche ou mieux représenter les résultats
Le but de ce projet est d'explorer et d'exploiter les techniques de classification des documents et traitement du langage naturel en mettant en œuvre les systèmes multi-agents afin de répondre à un besoin portant sur l'analyse et de classification des sites web chez Olfeo.
La représentation du temps et de l'espace est une tâche importante dans de nombreux domaines de l'Intelligence Artificielle tels que le traitement du langage naturel, les systèmes d'informations géographiques (GIS), la conception assistée par ordinateur (CAO), la navigation de robots. De nombreux formalismes qualitatifs ont été proposés pour représenter un ensemble d'entités spatiales ou temporelles et leurs relations. La plupart de ces formalismes utilisent des réseaux de contraintes qualitatives (RCQ en abrégé) pour représenter l'ensemble des informations d'un système. Dans certaines applications, en particulier de type multi-agents, plusieurs sources d'informations peuvent chacune fournir un réseau de contraintes qualitatives pour représenter leur connaissance sur l'ensemble des positions relatives d'un ensemble d'objets. La multiplicité des sources d'informations fournissant les RCQ fait que souvent ces RCQ sont conflictuels, et il est alors utile de mettre en oeuvre une méthode de fusion de ces réseaux pour résoudre les conflits. De nombreux opérateurs de fusion ont été définis dans le cadre de la logique propositionnelle. En s'inspirant en partie de ces travaux, nous élaborons des processus de fusion spécifiques aux RCQ et nous en étudions les propriétés logiques.
Cette thèse met l'accent sur l'expression linguistique des sentiments et des émotions dans un corpus jusque là peu interrogé dans ce type d'études, en l'occurrence les formes de communication instantanée rendues possibles par les nouvelles technologies et qui semblent prédisposées à accueillir de nombreux marqueurs expressifs. Elle déplace le curseur des études sur l'expression linguistique de ces deux catégories affectives du système vers l'emploi, en interrogeant un corpus formé de quatre formes de communication : blogs, forums de discussion, réseau Facebook et plateforme de microblogging Twitter. Le travail ancre la réflexion au niveau cognitif en cherchant à montrer, dans une perspective dynamique, comment se construit ce type de discours dans l'interaction médiatisée. Il aborde ainsi les différentes manifestations linguistiques et extralinguistiques qui chargent ces écrits électroniques d'une dimension émotionnelle ouvrant sur une dimension interactive intense. Il permet une réflexion sur les frontières écrit / oral et sur la naissance d'un nouveau langage expressif propre aux écrits électroniques.
Avec le développement de l'Internet des Objets, la réalisation d'environnements composés de diverses ressources connectées (objets, capteurs, services, données, etc.) devient une réalite tangible. La création de telles applications va cependant de pair avec le design d'outils supportant les utilisateurs en mobilité, en particulier afin de réaliser la sélection la plus efficace possible des ressources de l'environnement dans lequel l'utilisateur se trouve. Tandis qu'une telle sélection requiert la définition de modèles permettant de décrire de façon précise les caractéristiques de ces ressources, elle doit également prendre en compte les profils et préférences utilisateurs. Enfin, l'augmentation du nombre de ressources connectées, potentiellement mobiles, requiert également le développement de processus de sélection qui “passent à l'échelle”. Des avancées dans ce champ de recherche restent encore à faire, notamment à cause d'une connaissance assez floue concernant les acteurs (ainsi que leurs interactions) définissant (i.e., prenant part à) l'éco-système qu'est un “espace intelligent”. En outre, la multiplicité de diverses ressources connectées implique des problèmes d'interopérabilité et de scalabilité qu'il est nécessaire d'adresser. Si le Web Sémantique apporte une réponse à des problèmes d'interopérabilité, il en soulève d'autres liés au passage à l'échelle. S'appuyant sur mes recherches conduites au sein des Bell Labs, cette dissertation identifie les interactions entre les différents acteurs de cet éco-système et propose des représentations formelles, basées sur une sémantique, permettant de décrire ces acteurs. Cette dissertation propose également des procédures de recherche, permettant à l'utilisateur (ou ses applications) de trouver des ressources connectées en se basant sur l'analyse de leur description sémantique. En particulier, ces procédures s'appuient sur une architecture distribuée, également décrite dans cette dissertation, afin de permettre un passage à l'échelle.
Cette thèse porte sur le développement d'une chaîne de traitement complète pour réaliser des tâches de reconnaissance d'écriture manuscrite non contrainte. Trois difficultés majeures sont à résoudre : l'étape du prétraitement, l'étape de la modélisation optique et l'étape de la modélisation du langage. Au stade des prétraitements il faut extraire correctement les lignes de texte à partir de l'image du document. Une méthode de segmentation itérative en lignes utilisant des filtres orientables a été développée à cette fin. La difficulté dans l'étape de la modélisation optique vient de la diversité stylistique des scripts d'écriture manuscrite. Les modèles optiques statistiques développés sont des modèles de Markov cachés (HMM-GMM) et les modèles de réseaux de neurones récurrents (BLSTM-CTC). Les réseaux récurrents permettent d'atteindre les performances de l'état de l'art sur les deux bases de référence RIMES (pour le Français) et IAM (pour l'anglais). L'étape de modélisation du langage implique l'intégration d'un lexique et d'un modèle de langage statistique afin de rechercher parmi les hypothèses proposées par le modèle optique, la séquence de mots (phrase) la plus probable du point de vue linguistique. La difficulté à ce stade est liée à l'obtention d'un modèle de couverture lexicale optimale avec un minimum de mots hors vocabulaire (OOV). Pour cela nous introduisons une modélisation en sous-unités lexicales composée soit de syllabes soit de multigrammes. Ces modèles couvrent efficacement une partie importante des mots hors vocabulaire. Elles sont équivalentes aux modèles traditionnels en présence d'un faible taux de mots hors lexique. Grâce à la taille compacte du modèle de langage reposant sur des unités sous-lexicales, un système de reconnaissance multilingue unifié a été réalisé.
Au cours des dernières années, les réseaux de type (NDN) sont devenus une des architectures réseau les plus prometteuses. Pour être adopté à l'échelle d'Internet, NDN doit résoudre les problèmes inhérents à l'Internet actuel. En supposant (i) qu'un ordinateur appartient au réseau d'une entreprise basée sur une architecture NDN, (ii) que l'ordinateur a déjà été compromis par un support malveillant, et (iii) que la société installe un pare-feu, la thèse évalue la situation dans laquelle l'ordinateur infecté tente de divulguer des données à un attaquant externe à l'entreprise. Les contributions de cette thèse sont au nombre de cinq. Tout d'abord, cette thèse propose une attaque par fuite d'informations via un paquet donné et un paquet intérêt propres à NDN. Deuxièmement, afin de remédier à l'attaque fuite d'informations, cette thèse propose un pare-feu basé sur l'utilisation d'une liste blanche et d'une liste noire afin de surveiller et traiter le trafic NDN provenant des consommateurs. Troisièmement, cette thèse propose un filtre de noms NDN pour classifier un nom dans un paquet d'intérêt comme étant légitime ou non. Pour prendre en compte le flux de trafic analysé par le pare-feu NDN, cette thèse propose comme quatrième contribution la surveillance du flux NDN à travers le pare-feu. Enfin, afin de traiter les inconvénients du filtre de noms NDN, cette thèse propose un filtre de flux NDN permettant de classer un flux comme légitime ou non. L'évaluation des performances montre que le filtre de flux complète de manière tout à fait performante le filtre de nom et réduit considérablement le débit de fuite d'informations
La thèse actuelle présente une étude ERP du traitement du stress métrique en français. En effet, l'accentuation métrique joue un rôle important dans la compréhension des langues comme l'anglais et le néérlandais, mais son rôle dans le traitement du français n'est pas bien connu. Le français est une langue traditionnellement décrite sans accent. Cette thèse remet en question cette vision traditionnelle et s'aligne sur deux modèles métriques d'accentuation français, proposant que l'accent est encodé dans des patrons cognitifs sous-jacents. Dans notre étude interdisciplinaire en français sur le traitement des contraintes métriques, nous adoptons une approche fonctionnelle. Nous utilisons la méthode des potentiels évoqués (ERP), qui nous fournit une mesure extrêmement sensible et précise nous permettant de déterminer s'il existe un accent métrique en français et dans quelle mesure l'accent métrique aide l'auditeur à comprendre la parole.
Profitant de la quantité d'information maintenant disponible, la recherche et l'industrie se sont mises en quête de moyens pour analyser automatiquement les opinions exprimées dans les textes. Pour nos travaux, nous nous plaçons dans un contexte multilingue et multi-domaine afin d'explorer la classification automatique et adaptative de polarité. Nous proposons dans un premier temps de répondre au manque de ressources lexicales par une méthode de construction automatique de lexiques affectifs multilingues à partir de microblogs. Pour une meilleure analyse des textes, nous proposons aussi de remplacer le traditionnel modèle n -gramme par une représentation à base d'arbres de dépendances syntaxiques. Dans notre modèles, les n-grammes ne sont plus construits à partir des mots mais des triplets constitutifs des dépendances syntaxiques. Cette manière de procéder permet d'éviter la perte d'information que l'on obtient avec les approches classiques à base de sacs de mots qui supposent que les mots sont indépendants. Nos propositions ont fait l'objet d'évaluations quantitatives pour différents domaines d'applications (les films, les revues de produits commerciaux, les nouvelles et les blogs) et pour plusieurs langues (anglais, français, russe, espagnol et chinois), avec en particulier une participation officielle à plusieurs campagnes d'évaluation internationales (SemEval 2010, ROMIP 2011, I2B2 2011).
Cette thèse étudie l'adverbe autrement, au travers de ses trois emplois principaux : adverbe de manière, connecteur d'hypothèse négative, et rupteur de topique. L'accent est mis sur son fonctionnement anaphorique et son rôle dans la structure du discours. Après avoir passé en revue les théories du discours et la littérature sur l'adverbe, on dégage les propriétés des trois emplois grâce à des énoncés tirés de corpus oraux et écrits, en montrant comment le contexte sert à la récupération de l'antécédent et comment l'adverbe s'appuie sur le discours et le construit en même temps. Dès l'adverbe de manière, anaphore et portée droite sont essentielles à la construction du sens. Avec le connecteur, les relations référentielles laissent place aux relations logiques de proposition à proposition, tandis que le rupteur de topique est un emploi métalinguistique portant sur des constituants abstraits du discours. Un noyau de sens [l'anaphore et la négation] est dégagé, commun aux trois emplois et permettant d'envisager des points de passage entre eux. Cette étude synchronique est ensuite mise à profit pour reconstruire la grammaticalisation de l'adverbe, le détail des observations présentes contrebalançant la rareté des données historiques. On montre que c'est à travers la notion de construction, c'est-à-dire l'emploi de l'adverbe dans certains contextes, que l'évolution a pu avoir lieu : en particulier, l'ordre des mots en ancien français a été crucial, permettant à l'adverbe de manière d'occuper la position initiale propice à la réanalyse ; 
La Chirurgie Augmentée fait appel à des dispositifs médicaux (Dispositifs de Chirurgie Augmentée ou DCA) permettant au chirurgien de mieux se repérer dans l'espace, et donc d'enrichir son environnement chirurgical en vue de faciliter la réalisation de son geste. L'essor de ces dispositifs, par leur multiplication et par leur médiatisation, a amené les pouvoirs publics à s'interroger sur la qualité associée aux interventions assistées de ces appareils. Dans ce travail, nous illustrons la problématique de la Qualité associée aux interventions assistées de DCA par une description historique du premier robot médical actif utilisé pour la pose de prothèses totales de hanche. Nous abordons ensuite la notion de la qualité en médecine en général puis de la qualité des DCA en particulier. Nous verrons qu'il n'y a pas de dispositions spécifiques pour ces dispositifs et qu'il n'apparaît pas adéquat de parler de la qualité d'un DCA sans prendre en compte l'environnement dans lequel il est utilisé. C'est pourquoi il est essentiel de structurer l'usage de ces dispositifs ainsi que l'environnement dans lequel ils sont utilisés. Une des manières de structurer cet environnement est d'utiliser les ontologies. En utilisant la fonction d'édition d'ontologies du logiciel ISIS, nous avons modélisé une intervention chirurgicale pour insuffisance ligamentaire du ligament croisé antérieur, avec et sans DCA, ainsi que l'environnement associé. Cette représentation ontologique est constituée d'un ensemble de 45 Diagrammes Ontologiques (DO) comportant au total 1072 concepts. Nous décrivons le matériel et la méthode utilisés pour construire l'ensemble de ces diagrammes. Pour parler de la qualité des DCA, un utilisateur peut créer son système d'information à partir de notre modèle ontologique afin de disposer de ses propres indicateurs. La validation de notre modèle structurel a été réalisée par un expert à travers un scénario d'une intervention chirurgicale, créé à partir du modèle ontologique. Nous abordons enfin les perspectives possibles de notre travail.
Un processus ponctuel déterminantal (DPP) génère des configurations aléatoires de points ayant tendance à se repousser. La notion de répulsion est encodée par les sous-déterminants d'une matrice à noyau, au sens des méthodes à noyau en apprentissage artificiel. Cette forme algébrique particulière confère aux DPP de nombreux avantages statistiques et computationnels. Cette thèse porte sur l'échantillonnage des DPP, c'est à dire sur la conception d'algorithmes de simulation pour ce type de processus. Les motivations pratiques sont l'intégration numérique, les systèmes de recommandation ou encore la génération de résumés de grands corpus de données. Dans le cadre fini, nous établissons la correspondance entre la simulation de DPP spécifiques, dits de projection, et la résolution d'un problème d'optimisation linéaire dont les contraintes sont randomisées. Nous en tirons une méthode efficace d'échantillonnage par chaîne de Markov. Dans le cadre continu, certains DPP classiques peuvent être simulés par le calcul des valeurs propres de matrices tridiagonales aléatoires bien choisies. Nous en fournissons une nouvelle preuve élémentaire et unificatrice, dont nous tirons également un échantillonneur approché pour des modèles plus généraux. En dimension supérieure, nous nous concentrons sur une classe de DPP utilisée en intégration numérique. Nous proposons une implémentation efficace d'un schéma d'échantillonnage exact connu, qui nous permet de comparer les propriétés d'estimateurs Monte Carlo dans de nouveaux régimes. En vue d'une recherche reproductible, nous développons une boîte à outils open-source, nommée DPPy, regroupant les différents outils d'échantillonnage sur les DPP.
Dans cette thèse, nous proposons une nouvelle methode, Triangular Similarity Metric Learning (TSML), pour spécifier une fonction métrique de données automatiquement. Le système TSML proposée repose une architecture Siamese qui se compose de deux sous-systèmes identiques partageant le même ensemble de paramètres. Chaque sous-système traite un seul échantillon de données et donc le système entier reçoit une paire de données en entrée. Le système TSML comprend une fonction de coût qui définit la relation entre chaque paire de données et une fonction de projection permettant l'apprentissage des formes de haut niveau. Pour la fonction de coût, nous proposons d'abord la similarité triangulaire (Triangular Similarity), une nouvelle similarité métrique qui équivaut à la similarité cosinus. Sur la base d'une version simplifiée de la similarité triangulaire, nous proposons la fonction triangulaire (the triangular loss) afin d'effectuer l'apprentissage de métrique, en augmentant la similarité entre deux vecteurs dans la même classe et en diminuant la similarité entre deux vecteurs de classes différentes. Par rapport aux autres distances ou similarités, la fonction triangulaire et sa fonction gradient nous offrent naturellement une interprétation géométrique intuitive et intéressante qui explicite l'objectif d'apprentissage de métrique. En ce qui concerne la fonction de projection, nous présentons trois fonctions différentes : une projection linéaire qui est réalisée par une matrice simple, une projection non-linéaire qui est réalisée par Multi-layer Perceptrons (MLP) et une projection non-linéaire profonde qui est réalisée par Convolutional Neural Networks (CNN). Avec ces fonctions de projection, nous proposons trois systèmes de TSML pour plusieurs applications : la vérification par paires, l'identification d'objet, la réduction de la dimensionnalité et la visualisation de données. Pour chaque application, nous présentons des expérimentations détaillées sur des ensembles de données de référence afin de démontrer l'efficacité de notre systèmes de TSML.
Cette thèse est consacrée à l'étude des syntagmes nominaux simples en chinois mandarin. Le deuxième chapitre vise à présenter les caractéristiques les plus saillantes des syntagmes nominaux en mandarin. Le troisième chapitre se centre sur les propriétés distributionnelles de treize quantificateurs indéfinis. Le quatrième chapitre met au jour la double fonction du quantificateur indéfini yīdiǎnr 'un peu de'.
Ce travail de recherche se donne pour objectif de formuler des éléments de réflexion indispensables à une initiation à l'écrit de recherche universitaire, pour venir en aide aux étudiants locuteurs non natifs. Plusieurs questions ont sous-tendu cette étude : quel rôle pourraient avoir les études descriptives des écrits scientifiques dans une familiarisation réussie à l'écrit de recherche ? Quel est l'intérêt d'une initiation aux fonctions rhétoriques basée sur la phraséologie transdisciplinaire et fondée sur une approche dite par genre ? Est-il possible de soumettre des éléments d'aide dont pourraient profiter tous les étudiants quelles que soient leurs disciplines ? Une étude exploratoire autour d'une fonction rhétorique particulière qu'est "le positionnement" a permis de comprendre dans quelle mesure des éléments d'ordre linguistique, en l'occurrence les collocations transdisciplinaires, pourraient aider les étudiants à moins appréhender cette exigence d'un écrit essentiellement polyphonique et argumentatif ou encore à se positionner davantage.
Le crowdsourcing est une technique qui permet de recueillir une large quantité de données d'une manière rapide et peu onéreuse. Néanmoins, La disparité comportementale et de performances des "workers" d'une part et la variété en termes de contenu et de présentation des tâches par ailleurs influent considérablement sur la qualité des contributions recueillies. Par conséquent, garder leur légitimité impose aux plateformes de crowdsourcing de se doter de mécanismes permettant l'obtention de réponses fiables et de qualité dans un délai et avec un budget optimisé. Dans cette thèse, nous proposons CAWS (Context AwareWorker Selection), une méthode de contrôle de la qualité des contributions dans le crowdsourcing visant à optimiser le délai de réponse et le coût des campagnes. CAWS se compose de deux phases, une phase d'apprentissage opérant hors-ligne et pendant laquelle les tâches de l'historique sont regroupées de manière homogène sous forme de clusters. Pour chaque cluster, un profil type optimisant la qualité des réponses aux tâches le composant, est inféré ;  la seconde phase permet à l'arrivée d'une nouvelle tâche de sélectionner les meilleurs workers connectés pour y répondre. Il s'agit des workers dont le profil présente une forte similarité avec le profil type du cluster de tâches, duquel la tâche nouvellement créée est la plus proche. En outre, CrowdED rend possible la comparaison de méthodes de contrôle de qualité quelle que soient leurs catégories, du fait du respect d'un cahier des charges lors de sa constitution. Les résultats de l'évaluation de CAWS en utilisant CrowdED comparés aux méthodes concurrentes basées sur la sélection de workers, donnent des résultats meilleurs, surtout en cas de contraintes temporelles et budgétaires fortes. Les expérimentations réalisées avec un historique structuré en catégories donnent des résultats comparables à des jeux de données où les taches sont volontairement regroupées de manière homogène. La dernière contribution de la thèse est un outil appelé CREX (CReate Enrich eXtend) dont le rôle est de permettre la création, l'extension ou l'enrichissement de jeux de données destinés à tester des méthodes de crowdsourcing. Il propose des modules extensibles de vectorisation, de clusterisation et d'échantillonnages et permet une génération automatique d'une campagne de crowdsourcing.
La tâche d'exploration dans des ressources inexploitées mais nouvellement numérisées, afin d'y trouver des informations pertinentes, est complexifiée par la quantité de ressources disponibles. Grâce au projet ANR CIRESFI, la ressource la plus importante, pour la Comédie-Italienne du XVIIIe siècle, est un ensemble de registres comptables constituée de 28 000 pages. L'extraction d'informations est un processus long et complexe qui demande une expertise à chaque étape : détection et segmentation, extraction de caractéristiques, reconnaissance d'écriture manuscrite. Les systèmes à base de réseaux de neurones profonds dominent dans l'ensemble ces approches. Le problème majeur est qu'ils nécessitent d'avoir une grande quantité de données pour réaliser leur apprentissage. Cependant, les registres de la Comédie-Italienne ne possèdent pas de vérité terrain. Pour palier ce manque de données, nous explorons des approches pouvant opérer un apprentissage par transfert de connaissance. Cela signifie utiliser un ensemble de données déjà étiquetées et disponibles, possédant un minimum de points communs avec nos données pour entraîner les systèmes, pour ensuite les appliquer sur nos données. L'ensemble de nos expérimentations nous ont montré la difficulté de réaliser cette tâche, chaque choix à chaque étape ayant un impact fort sur la suite du système. Nous convergeons vers une solution séparant le modèle optique du modèle de langage afin de réaliser un apprentissage indépendant avec différents types de ressources disponibles et se rejoignant grâce à une projection de l'ensemble des informations dans un espace commun nonlatent.
XML est devenu le format standard d'échange de données. Nous souhaitons construire un environnement multi-système où des systèmes locaux travaillent en harmonie avec un système global, qui est une évolution conservatrice des systèmes locaux. Dans cet environnement, l'échange de données se fait dans les deux sens. Nous proposons des outils pour faciliter l'évolution de base de données XML. Des expériences ont été menées sur des données synthétiques et réelles pour montrer l'efficacité de nos méthodes.
Ce travail évalue les possibilités d'automatisation d'une analyse morphologique des mots de la langue russe. Cette analyse est soumise à deux contraintes principales :  - elle ne peut s'appuyer que sur la connaissance du mot seul et non du contexte dont il a été tiré. - aucune racine n'est connue a priori, à l'exception des racines entrant dans la formation : de dérives homonymes, de dérives dont la segmentation ne peut être contrôlée par des règles simples. Une grande partie de ce travail consiste tout naturellement à définir les facteurs autorisant une réduction sensible de cet ensemble de racines. L'analyse est conduite à l'aide : 1) de trois ensembles de morphèmes (préfixes, suffixes, désinences) auxquels viennent s'ajouter les deux formes du pronom réfléchi postposé, 2) de règles de reconnaissance des mots n'appartenant pas au fond slave (mots étrangers). L'analyse doit aboutir à la production d'un grammatème (fiche signalétique) des mots analysés aussi réduit que possible. La réduction du grammatème résulte de l'intersection des ensembles d'informations liées à chacun des éléments morphémiques entrant dans la composition des mots à analyser.
Cette thèse aborde le défi de la conception des systèmes de mobilité urbaine. Elle vise à développer un modèle d'expérience-voyageur pour faciliter, dans une démarche de conception, le diagnostic des problèmes de voyage et améliorer la pertinence des modèles de transport pour les voyageurs. En combinant les points de vue de la conception de l'expérience-utilisateur et du transport, elle contribue à approfondir la compréhension de comment les voyageurs vivent leur voyage et particulièrement des problèmes qu'ils rencontrent. Le premier axe d'investigation est lié à la modélisation de l'expérience-voyageur pour alimenter un diagnostic pertinent et riche des problèmes de voyage. Dans un deuxième axe, les voyageurs sont impliqués, par une démarche de théorie ancrée, pour identifier les problèmes qu'ils rencontrent lors de l'utilisation de systèmes de mobilité urbaine au moyen de stimuli appropriés. Un troisième axe introduit des attributs subjectifs de voyage dans des modèles de transport afin d'améliorer leur précision Cette recherche utilise la recherche-action comme méthodologie. Elle combine revue de littérature dans les disciplines de conception et de transport, quatre observations terrain, quinze interviews en profondeurs aves des voyageurs et experts en transport, cinq ateliers de problématisation, et deux expérimentations, dans une amélioration cyclique des résultats. Les différentes utilisations du modèle ont permis un diagnostic approfondi de trois systèmes de mobilité urbaine (train de banlieue, bus à la demande, navette sur voie dédiée) et la mise au point d'attributs centrés sur le voyageur pour un modèle d'optimisation et une simulation multi-agents qui ont été testé par une enquête de plus de 450 participants.
Dans l'ère de l'information et de la connaissance, la traduction automatique (TA) devient progressivement un outil indispensable pour transposer la signification d'un texte d'une langue source vers une langue cible. La TA des noms propres (NP), en particulier, joue un rôle crucial dans ce processus,puisqu'elle permet une identification précise des personnes, des lieux, des organisations et des artefacts à travers les langues. Malgré un grand nombre d'études et des résultats significatifs concernant la reconnaissance d'entités nommées (dont le nom propre fait partie) dans la communauté de TAL dans le monde, il n'existe presque aucune recherche sur la traduction automatique des noms propres (TANP) pour le vietnamien. En raison des caractéristiques différentes d'écriture de NP, la translittération ou la transcription et la traduction de plusieurs de langues incluant l'anglais, le français, le russe, le chinois, etc. vers levietnamien, le TANP de ces langues vers le vietnamien est stimulant et problématique. Cette étude se concentre sur les problèmes de TANP d'anglais vers le vietnamien et de français vers le vietnamien résultant du moteurs courants de la TA et présente les solutions de prétraitement de ces problèmes pour améliorer la qualité de la TA. A travers l'analyse et la classification d'erreurs de la TANP faites sur deux corpus parallèles de textes avec PN (anglais-vietnamien et français-vietnamien), nous proposons les solutions concernant deux problématiques importantes : (1) l'annotation de corpus, afin de préparer des bases de données pour le prétraitement et (2) la création d'un programme pour prétraiter automatiquement les corpus annotés, afin de réduire les erreurs de la TANP et d'améliorer la qualité de traduction des systèmes de TA, tels que Google, Vietgle, Bing et EVTran. L'efficacité de différentes méthodes d'annotation des corpus avec des NP ainsi que les taux d'erreurs de la TANP avant et après l'application du programme de prétraitement sur les deux corpus annotés est comparés et discutés dans cette thèse. Ils prouvent que le prétraitement réduit significativement le taux d'erreurs de la TANP et, par la même, contribue à l'amélioration de traduction automatique vers la langue vietnamienne.
L'objectif général des travaux à réaliser est de proposer des méthodes flexibles et robustes pour fusionner les connaissances en domaine ouvert.
L'électronique organique est un domaine de recherche qui vise à développer de nouvelles technologies basées sur des matériaux semi-conducteurs organiques (SCOs). D'une manière générale, deux approches sont utilisées pour le design moléculaire de SCOs. La première approche consiste à assembler des fragments moléculaires, connus pour certaines propriétés, afin de synthétiser des matériaux fonctionnels pour une application visée comme les diodes électrophosphorescentes organiques (PhOLEDs). La seconde approche est plus exploratrice et consiste à développer des nouveaux fragments moléculaires pouvant posséder une ou plusieurs propriétés souhaitées pour une application donnée. Dans ces travaux de thèse les deux approches ont été développées. D'un côté, nous avons élaboré des matériaux hôtes pour des PhOLEDs en ajustant leur propriétés (première approche), et, de l'autre côté, nous nous sommes intéressés à une toute nouvelle génération de SCOs : les anneaux moléculaires (seconde approche). Ses travaux ont permis la fabrication de PhOLEDs rouge, verte et bleue présentant les performances globales les plus élevées de la littérature. Ces travaux nous ont permis d'incorporer pour la première fois des anneaux moléculaires dans des transistors organiques à effet de champs afin d'en étudier les propriétés de transport.
Cette thèse s'inscrit dans le cadre d'une étude sur le potentiel de la transcription automatique pour l'instrumentation de situations pédagogiques. Notre contribution porte sur plusieurs axes. Dans un premier temps, nous décrivons l'enrichissement et l'annotation du corpus COCo que nous avons réalisés dans le cadre du projet ANR PASTEL. Dans ce cadre multi-thématiques, nous nous sommes ensuite intéressés à la problématique de l'adaptation linguistique des systèmes de reconnaissance automatique de la parole (SRAP). La proposition d'adaptation des modèles s'appuie à la fois sur les supports de présentation de cours fournis par les enseignants et sur des données spécialisées récoltées automatiquement à partir du web. Ainsi, nous avons proposé deux protocoles d'évaluation. Le premier porte sur une évaluation intrinsèque, permettant d'estimer la performance seulement pour des mots spécialisés de chacun des cours (IWER_Average). D'autre part, nous proposons une évaluation extrinsèque, qui estime la performance pour deux tâches exploitant la transcription : la recherche d'informations et l'indexabilité. L'adaptation reposant sur une collecte de données issues du web, nous avons cherché à rendre compte de la reproductibilité des résultats sur l'adaptation de modèles de langage en comparant les performances obtenues sur une longue période temporelle. Nos résultats expérimentaux montrent que même si les données sur le web changent en partie d'une période à l'autre, la variabilité de la performance des systèmes de transcription adaptés est restée non significative à partir d'un nombre minimum de documents collectés. Enfin, nous avons proposé une approche permettant de structurer la sortie de la transcription automatique en segmentant thématiquement la transcription et en alignant la transcription avec les diapositives des supports de cours. Pour la segmentation, l'intégration de l'information de changement de diapositives dans l'algorithme TextTiling apporte un gain significatif en termes de F-mesure. Pour l'alignement, nous avons développé une technique basé sur des représentations TF-IDF en imposant une contrainte pour respecter l'ordre séquentiel des diapositives et des segments de transcription et nous avons vérifié la fiabilité de l'approche utilisée à l'aide d'une mesure de confiance.
Les méthodes d'analyse rythmique existantes se concentrent généralement sur l'un de ces aspects à la fois et n'exploitent pas la richesse de la structure musicale, ce qui compromet la cohérence musicale des estimations automatiques. Dans ce travail, nous proposons de nouvelles approches tirant parti des informations multi-échelles pour l'analyse automatique du rythme. Nos modèles prennent en compte des interdépendances intrinsèques aux signaux audio de musique, en permettant ainsi l'interaction entre différentes échelles de temps et en assurant la cohérence musicale entre elles. Ce système est conçu pour tirer parti des informations de structure musicale (c'est-à-dire des répétitions de sections musicales) dans un cadre unifié. Nous proposons également un modèle linguistique pour la détection conjointe des temps et du micro-timing dans la musique afro-latino-américaine. De plus, notre modèle d'estimation conjointe des temps et du microtiming représente une avancée vers des systèmes plus interprétables. Les méthodes présentées ici offrent des alternatives nouvelles et plus holistiques pour l'analyse numérique du rythme, ouvrant des perspectives vers une analyse automatique plus complète de la musique.
Au cours des dernières années, l'apprentissage profond est devenu l'approche privilégiée pour le développement d'une intelligence artificielle moderne (IA). L'augmentation importante de la puissance de calcul, ainsi que la quantité sans cesse croissante de données disponibles ont fait des réseaux de neurones profonds la solution la plus performante pour la resolution de problèmes complexes. Pour résoudre ce problème, les réseaux de neurones basés sur les algèbres des nombres complexes et hypercomplexes ont été développés. En particulier, les réseaux de neurones de quaternions (QNN) ont été proposés pour traiter les données tridimensionnelles et quadridimensionnelles, sur la base des quaternions représentant des rotations dans notre espace tridimensionnel. Malheureusement, et contrairement aux réseaux de neurones à valeurs complexes qui sont de nos jours acceptés comme une alternative aux réseaux de neurones réels, les QNNs souffrent de nombreuses lacunes qui sont en partie comblées par les différents travaux détaillés par ce manuscrit. Ainsi, la thèse se compose de trois parties qui introduisent progressivement les concepts manquants, afin de faire des QNNs une alternative aux réseaux neuronaux à valeurs réelles. La premiere partie présente et répertorie les précédentes découvertes relatives aux quaternions et aux réseaux de neurones de quaternions, afin de définir une base pour la construction des QNNs modernes. La deuxième partie introduit des réseaux neuronaux de quaternions état de l'art, afin de permettre une comparaison dans des contextes identiques avec les architectures modernes traditionnelles. Plus précisément, les QNNs étaient majoritairement limités par leurs architectures trop simples, souvent composées d'une seule couche cachée comportant peu de neurones. Premièrement, les paradigmes fondamentaux, tels que les autoencodeurs et les réseaux de neurones profonds sont présentés. Ensuite, les très répandus et étudiés réseaux de neurones convolutionnels et récurrents sont étendus à l'espace des quaternions. Dans un scénario traditionnel impliquant des QNNs, les caractéristiques d'entrée sont manuellement segmentées en quatre composants, afin de correspondre à la representation induite par les quaternions. Malheureusement, il est difficile d'assurer qu'une telle segmentation est optimale pour résoudre le problème considéré. De plus, une segmentation manuelle réduit fondamentalement l'application des QNNs à des tâches naturellement définies dans un espace à au plus quatre dimensions. De ce fait, la troisième partie de cette thèse introduit un modèle supervisé et un modèle non supervisé permettant l'extraction de caractéristiques d'entrée désentrelacées et significatives dans l'espace des quaternions, à partir de n'importe quel type de signal réel uni-dimentionnel, permettant l'utilisation des QNNs indépendamment de la dimensionnalité des vecteurs d'entrée et de la tâche considérée. Les expériences menées sur la reconnaissance de la parole et la classification de documents parlés montrent que les approches proposées sont plus performantes que les représentations traditionnelles de quaternions.
Le choix d'un système de gestion de bases de données (SGBD) et de plateforme d'exécution pour le déploiement est une tâche primordiale pour la satisfaction des besoins non-fonctionnels(comme la performance temporelle et la consommation d'énergie). La difficulté de ce choix explique la multitude de tests pour évaluer la qualité des bases de données (BD) développées. Cette évaluation se base essentiellement sur l'utilisation des métriques associées aux besoins non fonctionnels. En effet, une mine de tests existe couvrant toutes les phases de cycle de vie de conception d'une BD. Les tests et leurs environnements sont généralement publiés dans des articles scientifiques ou dans des sites web dédiés comme le TPC (Transaction Processing Council). Par conséquent, cette thèse contribue à la capitalisation et l'exploitation des tests effectués afin de diminuer la complexité du processus de choix. En analysant finement les tests, nous remarquons que chaque test porte sur les jeux de données utilisés, la plateforme d'exécution, les besoins non fonctionnels, les requêtes, etc. Nous proposons une démarche de conceptualisation et de persistance de toutes.ces dimensions ainsi que les résultats de tests. Cette thèse a donné lieu aux trois contributions. (1) Une conceptualisation basée sur des modélisations descriptive,prescriptive et ontologique pour expliciter les différentes dimensions. (2) Le développement d'un entrepôt de tests multidimensionnel permettant de stocker les environnements de tests et leurs résultats. (3) Le développement d'une méthodologie de prise de décision basée sur un système de recommandation de SGBD et de plateformes.
La désambiguïsation lexicale automatique fait partie des domaines du Traitement Automatique de la Langue (TAL) les plus productifs. Deux raisons expliquent cette productivité : d'une part l'intérêt considérable de la désambiguïsation lexicale automatique pour un nombre important d'applications, d'autre part l'absence de consensus sur la manière d'appréhender cette tâche. Nous proposons un modèle d'un genre nouveau fondé sur la théorie de la construction dynamique du sens. Parallèlement, nous proposons une méthode de calcul automatique de classes sémantiques pour les éléments lexicaux qui sont rattachés au verbe.
La recherche d'information ainsi que l'aide à la décision nécessitent un accès rapide et eﬃcace aux connaissances contenues dans une collection de documents de santé, ainsi qu'une bonne exploitation des connaissances médicales. L'indexation (description à l'aide de mots clés) permet de rendre ces connaissances accessibles et utilisables. Dans le domaine de la santé, le nombre de ressources électroniques disponibles augmente de manière exponentielle ainsi la nécessité de disposer de solutions automatiques pour faciliter l'accès aux connaissances ainsi que l'indexation est omniprésente. L'objectif de cette thèse a été de développer un outil d'aide à l'indexation automatique multi-terminologique, multi-document et multi-tâche nommé F-MTI (French Multi-terminology Indexer) capable de produire une proposition une indexation pour les documents de santé. Cet outil a nécessité l'élaboration de méthodes de Traitement Automatique de la Langue Naturelle. Il a été appliqué à l'indexation documentaire dans le catalogue de santé en ligne CISMeF, à l'indexation des données thérapeutiques pour les médicaments et à l'indexation des diagnostics et des actes médicaux pour les dossiers médicaux éléctroniques.
Dans cette thèse, je me concentrerai principalement sur l'inférence variationnelle et les modèles probabilistes. En particulier, je couvrirai plusieurs projets sur lesquels j'ai travaillé pendant ma thèse sur l'amélioration de l'efficacité des systèmes AI / ML avec des techniques variationnelles. La thèse comprend deux parties. Dans la première partie, l'efficacité des modèles probabilistes graphiques est étudiée. Dans la deuxième partie, plusieurs problèmes d'apprentissage des réseaux de neurones profonds sont examinés, qui sont liés à l'efficacité énergétique ou à l'efficacité des échantillons
Cette recherche s'intéresse à l'usage de deux variables du français traditionnellement décrites comme phonologiques : la liaison et l'élision du schwa. Ces variables sont étudiées au cours d'interactions naturelles entre trois enfants et leurs parents respectifs. Plus précisément, l'objectif de cette thèse est de décrire les particularités du discours adressé à l'enfant (DAE) au niveau de l'usage des variables phonologiques et de mesurer leur impact sur l'émergence de la production de ces mêmes variables chez l'enfant. Après la présentation du cadre théorique d'analyse et de la méthodologie de recueil, de structuration et d'analyse des données, le travail de recherche s'organise en trois parties. La première étude basée sur corpus, descriptive, a deux principaux objectifs. Dans un premier temps, il s'agit de mesurer à quelle variation les jeunes enfants sont exposés au domicile familial. Ensuite, le but est de confronter les résultats des études précédentes sur l'acquisition de la liaison, principalement obtenus à partir de tâches expérimentales, à des données issues de corpus denses d'interactions parent-enfant. Cette étude a notamment permis de relever l'influence de facteurs liés à l'usage, comme la fréquence, sur l'emploi des variables phonologiques. La seconde étude se focalise sur les caractéristiques du DAE. Les résultats présentés démontrent notamment que l'usage des variables phonologiques est modulé en DAE, et ce essentiellement à un stade précoce. Cette modulation s'atténue ensuite au cours du développement linguistique des jeunes sujets. La dernière étude de ce travail de recherche permet de mettre en relation les productions enfantines et parentales. Il apparaît que le développement de la variation phonologique va dans le sens des hypothèses émises par les modèles basés sur l'usage : la variation phonologique est à un stade précoce mémorisée à l'intérieur de constructions spécifiques, particulièrement fréquentes et saillantes dans le DAE.
L'évolution rapide dans les environnements métier d'aujourd'hui impose de nouveaux défis pour la gestion efficace et rentable des processus métiers. Dans un tel environnement très dynamique, la conception des processus métiers devient une tâche fastidieuse, source d'erreurs et coûteuse. Par conséquent, l'adoption d'une approche permettant la réutilisation et l'adaptabilité devient un besoin urgent pour une conception de processus prospère. Les modèles de processus configurables récemment introduits représentent l'une des solutions recherchées permettant une conception de processus par la réutilisation, tout en offrant la flexibilité. Un modèle de processus configurable est un modèle générique qui intègre de multiples variantes de procédés d'un même processus métier à travers des points de variation. Ces points de variation sont appelés éléments configurables et permettent de multiples options de conception dans le modèle de processus. Un modèle de processus configurable doit être configuré selon une exigence spécifique en sélectionnant une option de conception pour chaque élément configurable. Depuis lors, la question de la conception et de la configuration des modèles de processus configurables a été étudiée. Cependant, les approches existantes proposent de recommander des modèles de processus configurables entiers qui sont difficiles à réutiliser, nécessitent un temps complexe de calcul et peuvent confondre le concepteur du processus. D'autre part, les résultats de la recherche sur la conception des modèles de processus configurables ont mis en évidence la nécessité des moyens de soutien pour configurer le processus. Par conséquent, de nombreuses approches ont proposé de construire un système de support de configuration pour aider les utilisateurs finaux à sélectionner les choix de configuration souhaitables en fonction de leurs exigences. Notre objectif est double : (i) assister la conception des processus configurables d'une manière à ne pas confondre les concepteurs par des recommandations complexes et (i) assister la création des systèmes de soutien de configuration afin de libérer les analystes de processus de la charge de les construire manuellement. Pour atteindre le premier objectif, nous proposons d'apprendre de l'expérience acquise grâce à la modélisation des processus passés afin d'aider les concepteurs de processus avec des fragments de processus configurables. Les fragments proposés inspirent le concepteur du processus pour compléter la conception du processus en cours. Pour atteindre le deuxième objectif, nous nous rendons compte que les modèles de processus préalablement conçus et configurés contiennent des connaissances implicites et utiles pour la configuration de processus.
Cette thèse s'intéresse aux formalismes qui permettent de représenter mathématiquement non seulement le sens de phrases indépendantes mais aussi de textes entiers, en incluant les liens de sens que les différentes phrases qui les composent entretiennent les unes avec les autres. Nous ne nous posons pas seulement la question du sens et de sa représentation, mais aussi celle de la détermination algorithmique de cette représentation à partir des séquences de mots qui composent les énoncés. Nous nous situons donc à l'interface de trois traditions : l'analyse discursive, la sémantique formelle et la linguistique computationnelle. La plupart de travaux formels portant sur le discours ne prêtent que peu d'attention aux verbes de dire (affirmer, dire, etc.) et d'attitude propositionnelle (penser, croire, etc.). Tous ces verbes, que nous regroupons sous l'abréviation « VAP » , ont en commun d'exprimer l'attitude ou la position d'une personne sur une proposition donnée. Ils sont utilisés fréquemment et introduisent de nombreuses subtilités échappant de fait aux théories actuelles. Cette thèse a pour objectif principal de mettre à jour les principes d'une grammaire formelle compatible avec l'analyse du discours et prenant en compte les VAP. Nous commençons donc par présenter de nombreuses données linguistiques illustrant les interactions entre VAP et relations discursives. Il est souvent considéré que les connecteurs adverbiaux (ensuite, par exemple, etc.) sont anaphoriques. Cependant, nous pouvons nous demander si, en pratique, un système de linguistique computationnelle ne peut pas gérer cette catégorie particulière d'anaphore comme s'il s'agissait d'un type de dépendance structurelle, étendant d'une certaine manière la syntaxe au-delà de la phrase. C'est ce que nous nous proposons de faire à l'aide du formalisme D-STAG. Cela nous amène à développer une approche anaphorique, c'est-à-dire dans laquelle les arguments des relations discursives ne sont plus déterminés uniquement par la structure grammaticale des énoncés. Cela est possible avec la sémantique par continuation, que nous utilisons en combinaison à la sémantique événementielle. Nous avançons plusieurs pistes pour y répondre et étudions plus en détail le cas de la négation. Nous montrons que ces difficultés ont pour origine l'analyse standard de la négation, qui traite phrases positives et phrases négatives de manière fondamentalement différente. Rejetant cette vue, nous présentons une formalisation nouvelle de la notion d'événement négatif, adaptée à l'analyse de divers phénomènes linguistiques.
De nos jours, avec l'abondance croissante de données de très grande taille, les problèmes de classification de grande dimension ont été mis en évidence comme un challenge dans la communauté d'apprentissage automatique et ont beaucoup attiré l'attention des chercheurs dans le domaine. Au cours des dernières années, les techniques d'apprentissage avec la parcimonie et l'optimisation stochastique se sont prouvées être efficaces pour ce type de problèmes. Dans cette thèse, nous nous concentrons sur le développement des méthodes d'optimisation pour résoudre certaines classes de problèmes concernant ces deux sujets. Nos méthodes sont basées sur la programmation DC (Difference of Convex functions) et DCA (DC Algorithm) étant reconnues comme des outils puissants d'optimisation non convexe. La thèse est composée de trois parties. La première partie aborde le problème de la sélection des variables. La deuxième partie étudie le problème de la sélection de groupes de variables. La dernière partie de la thèse liée à l'apprentissage stochastique. Dans la première partie, nous commençons par la sélection des variables dans le problème discriminant de Fisher (Chapitre 2) et le problème de scoring optimal (Chapitre 3), qui sont les deux approches différentes pour la classification supervisée dans l'espace de grande dimension, dans lequel le nombre de variables est beaucoup plus grand que le nombre d'observations. Poursuivant cette étude, nous étudions la structure du problème d'estimation de matrice de covariance parcimonieuse et fournissons les quatre algorithmes appropriés basés sur la programmation DC et DCA (Chapitre 4). Deux applications en finance et en classification sont étudiées pour illustrer l'efficacité de nos méthodes. La deuxième partie étudie la L_p,0régularisation pour la sélection de groupes de variables (Chapitre 5). En utilisant une approximation DC de la L_p,0norme, nous prouvons que le problème approché, avec des paramètres appropriés, est équivalent au problème original. Considérant deux reformulations équivalentes du problème approché, nous développons différents algorithmes basés sur la programmation DC et DCA pour les résoudre. Comme applications, nous mettons en pratique nos méthodes pour la sélection de groupes de variables dans les problèmes de scoring optimal et d'estimation de multiples matrices de covariance. Dans la troisième partie de la thèse, nous introduisons un DCA stochastique pour des problèmes d'estimation des paramètres à grande échelle (Chapitre 6) dans lesquelles la fonction objectif est la somme d'une grande famille des fonctions non convexes. Comme une étude de cas, nous proposons un schéma DCA stochastique spécial pour le modèle loglinéaire incorporant des variables latentes
C'est une recherche transverse qui traite à la fois des langues contrôlées et de la traduction automatique français-arabe, deux concepts intimement liés. Dans une situation de crise où la communication doit jouer pleinement son rôle, et dans une mondialisation croissante où plusieurs langues cohabitent, notre recherche montre que l'association de ces deux concepts est plus que nécessaire. Nul ne peut contester aujourd'hui la place prépondérante qu'occupe la sécurité dans le quotidien des personnes et les enjeux qu'elle représente au sein des sociétés modernes. Toutefois, et contrairement à une idée bien ancrée qui tend à associer les risques d'une mauvaise communication à l'oral uniquement, l'usage de la langue écrite peut lui aussi comporter des risques. En effet des messages mal écrits peuvent conduire à de réelles catastrophes et à des conséquences irréversibles notamment dans des domaines jugés sensibles tels que les domaines à sécurité critique. C'est dans ce contexte que s'inscrit notre recherche. Elle apporte en effet des notions nouvelles à travers plusieurs procédés normatifs intervenant non seulement dans le processus de contrôle mais également dans le processus de traduction. Il introduit de nouveaux concepts notamment celui des macrostructures miroir contrôlées, où la syntaxe et la sémantique des langues source et cible sont représentées au même niveau.
Une interaction sociale désigne toute action réciproque entre deux ou plusieurs individus, au cours de laquelle des informations sont partagées sans "médiation technologique". Dans le contexte de tests et d'études observationnelles, de multiples mécanismes sont utilisés pour étudier ces interactions tels que les questionnaires, l'observation directe des événements et leur analyse par des opérateurs humains, ou l'observation et l'analyse à posteriori des événements enregistrés par des spécialistes (psychologues, sociologues, médecins, etc.). Pour faire face aux problèmes susmentionnés, il peut donc s'avérer utile d'automatiser le processus d'analyse de l'interaction sociale. Il s'agit donc de combler le fossé entre les processus d'analyse des interactions sociales basés sur l'homme et ceux basés sur la machine. Nous proposons donc une approche holistique qui intègre des signaux hétérogènes multimodaux et des informations contextuelles (données "exogènes" complémentaires) de manière dynamique et optionnelle en fonction de leur disponibilité ou non. Une telle approche permet l'analyse de plusieurs "signaux" en parallèle (où les humains ne peuvent se concentrer que sur un seul). Cette analyse peut être encore enrichie à partir de données liées au contexte de la scène (lieu, date, type de musique, description de l'événement, etc.) ou liées aux individus (nom, âge, sexe, données extraites de leurs réseaux sociaux, etc.) Les informations contextuelles enrichissent la modélisation des métadonnées extraites et leur donnent une dimension plus "sémantique". La gestion de cette hétérogénéité est une étape essentielle pour la mise en œuvre d'une approche holistique.
Dans cette dissertation, nous proposons des méthodes d'apprentissage automa-tique aptes à bénéficier de la récente explosion des volumes de données digitales. Premièrement nous considérons l'amélioration de l'efficacité des méthodes derécupération d'image. Nous proposons une approche d'apprentissage de métriques locales coordonnées (Coordinated Local Metric Learning, CLML) qui apprends des métriques locales de Mahalanobis, puis les intègre dans une représentation globale où la distance l2 peut être utilisée. Ceci permet de visualiser les données avec une unique représentation 2D, et l'utilisation de méthodes de récupération efficaces basées sur la distance l2. Notre approche peut être interprétée comme l'apprentissage d'une projection linéaire de descripteurs donnés par une méthode a noyaux de grande dimension définie explictement. Cette interprétation permet d'appliquer des outils existants pour l'apprentissage de métriques de Mahalanobis à l'apprentissage de métriques locales coordonnées. Nos expériences montrent que la CLML amé-liore les résultats en matière de récupération de visage obtenues par les approches classiques d'apprentissage de métriques locales et globales. Nous explorerons différentes stratégies d'apprentissage de métriques locales à partir des couches intermédiaires d'un CNN, afin de faire le rapprochement entre des images de sources différentes. Dans nos expériences, la profondeur de la couche optimale pour une tâche donnée est positivement corrélée avec le changement entre le domaine source (données d'entraînement du CNN) et le domaine cible. Les résultats montrent que nous pouvons utiliser des CNN entraînés sur des images du spectre visible pour obtenir des résultats meilleurs que l'état de l'art pour la reconnaissance faciale hétérogène (images et dessins quasi-infrarouges). Troisièmement, nous présentons les "tissus de neurones convolutionnels" (Convolutional Neural Fabrics) permettant l'exploration de l'espace discret et exponentiellement large des architectures possibles de réseaux neuronaux, de manière efficiente et systématique. Au lieu de chercher à sélectionner une seule architecture optimale, nous proposons d'utiliser un "tissu" d'architectures combinant un nombre exponentiel d'architectures en une seule. Le tissu est une représentation 3D connectant les sorties de CNNs à différentes couches, échelles et canaux avec un motif de connectivité locale, homogène et creux. Les seuls hyper-paramètres du tissu (le nombre de canaux et de couches) ne sont pas critiques pour la performance. La nature acyclique du tissu nous permet d'utiliser la rétro-propagation du gradient durant la phase d'apprentissage. De manière automatique, nous pouvons donc configurer le tissu de manière à implémenter l'ensemble de toutes les architectures possibles (un nombre exponentiel) et, plus généralement, des ensembles (combinaisons) de ces modèles. La complexité de calcul et de taille mémoire du tissu évoluent de manière linéaire alors qu'il permet d'exploiter un nombre exponentiel d'architectures en parallèle, en partageant les paramètres entre architectures. Nous présentons des résultats à l'état de l'art pour la classification d'images sur le jeu de données MNIST et CIFAR10, et pour la segmentation sémantique sur le jeu de données Part Labels.
Ce dernier est d'ailleurs en général traité en deux temps : tout d'abord, réaliser un prétraitement sur les données hétérogènes, multidimensionnelles et imprécises pour les transformer en un flux d'évènements symbolique, puis utiliser des techniques de reconnaissance de plans sur ces mêmes évènements. Ceci permet de décrire des étapes de plans symboliques de haut niveau sans avoir à se soucier des spécificités des capteurs bas niveau. Cependant, cette première étape est destructrice d'information et de ce fait génère une ambigüité supplémentaire dans le processus de reconnaissance. De plus, séparer les tâches de reconnaissance de comportements est générateur de calculs redondants et rend l'écriture de la bibliothèque de plans plus ardue. Ainsi, nous proposons d'aborder cette problématique sans séparer en deux le processus de reconnaissance. Pour y parvenir, nous proposons un nouveau modèle hiérarchique, inspiré de la théorie des langages formels, nous permettant de construire un pont au-dessus du fossé sémantique séparant les mesures des capteurs des intentions des entités. Grâce à l'aide d'un ensemble d'algorithmes manipulant ce modèle, nous sommes capables, à partir d'observations, de déduire les plausibles futures évolutions de la zone sous surveillance, tout en les justifiant des explications nécessaires.
Il est généralement facile pour les humains de distinguer rapidement différents lieux en se basant uniquement sur leur aspect visuel.. Ces catégories sémantiques peuvent être utilisées comme information contextuelle favorisant la détection et la reconnaissance d'objets. Des travaux récents en reconnaissance des lieux visent à doter les robots de capacités similaires. Contrairement aux travaux classiques, portant sur la localisation et la cartographie, cette tâche est généralement traitée comme un problème d'apprentissage supervisé. La reconnaissance de lieux sémantiques-la capacité à reconnaître la catégorie sémantique à laquelle une scène appartient – peut être considérée comme une condition essentielle en robotique autonome. Un robot autonome doit en effet pouvoir apprendre facilement l'organisation sémantique de son environnement pour pouvoir fonctionner et interagir avec succès. Si nous faisons l'hypothèse que les objets sont plus faciles à reconnaître quand la scène dans laquelle ils apparaissent est bien identifiée, la deuxième approche semble plus appropriée. Elle est cependant fortement dépendante de la nature des descripteurs d'images utilisées qui sont généralement dérivés empiriquement a partir des observations générales sur le codage d'images. En opposition avec ces propositions, une autre approche de codage des images, basée sur un point de vue plus théorique, a émergé ces dernières années. Les modèles d'extraction de caractéristiques fondés sur le principe de la minimisation d'une fonction d'énergie en relation avec un modèle statistique génératif expliquant au mieux les données, ont abouti à l'apparition des Machines de Boltzmann Restreintes (Rectricted Boltzmann Machines : RBMs) capables de coder une image comme la superposition d'un nombre limité de caractéristiques extraites à partir d'un plus grand alphabet. Il a été montré que ce processus peut être répété dans une architecture plus profonde, conduisant à une représentation parcimonieuse et efficace des données initiales dans l'espace des caractéristiques. Le problème complexe de la classification dans l'espace de début est ainsi remplacé par un problème plus simple dans l'espace des caractéristiques. Après avoir realisé un codage approprié, une régression softmax dans l'espace de projection est suffisante pour obtenir des résultats de classification prometteurs. A notre connaissance, cette approche n'a pas encore été proposée pour la reconnaissance de scène en robotique autonome. Nous avons comparé nos méthodes avec les algorithmes de l'état-de-l'art en utilisant une base de données standard de localisation de robot. Nous avons étudié l'influence des paramètres du système et comparé les différentes conditions sur la même base de données. Les expériences réalisées montrent que le modèle que nous proposons, tout en étant très simple, conduit à des résultats comparables à l'état-de-l'art sur une tâche de reconnaissance de lieux sémantiques.
Cette thèse porte sur la question de la sémantique causative. Elle propose une typologie sémantique pour les verbes causatifs analytiques CAUSE, MAKE, HAVE, GET et LET, fondée sur le modèle de la dynamique des forces. Le troisième chapitre est un tour d'horizon des idées les plus récurrentes dans la littérature au sujet de la sémantique des verbes causatifs analytiques. Dans le quatrième chapitre, nous proposons une étude de corpus portant sur les propriétés lexico-sémantiques des verbes CAUSE, MAKE, HAVE, GET et LET. Sur la base des donnés empiriques de notre étude de corpus, nous présentons, dans le dernier chapitre, une nouvelle typologie sémantique pour les verbes causatifs analytiques anglais
L'abondance de textes dans le domaine biomédical nécessite le recours à des méthodes de traitement automatique pour améliorer la recherche d'informations précises. L'extraction d'information (EI) vise précisément à extraire de l'information pertinente à partir de données non-structurées. Une grande partie des méthodes dans ce domaine se concentre sur les approches d'apprentissage automatique, en ayant recours à des traitements linguistiques profonds. L'analyse syntaxique joue notamment un rôle important, en fournissant une analyse précise des relations entre les éléments de la phrase. Elle comprend l'évaluation de différents analyseurs ainsi qu'une analyse détaillée des erreurs. Une fois l'analyseur le plus adapté sélectionné, les différentes étapes de traitement linguistique pour atteindre une EI de haute qualité, fondée sur la syntaxe, sont abordés : ces traitements incluent des étapes de pré-traitement (segmentation en mots) et des traitements linguistiques de plus haut niveau (lié à la sémantique et à l'analyse de la coréférence). Cette thèse explore également la manière dont les différents niveaux de traitement linguistique peuvent être représentés puis exploités par l'algorithme d'apprentissage. Les méthodes et les approches décrites sont explorées en utilisant deux corpus biomédicaux différents, montrant comment les résultats d'IE sont utilisés dans des tâches concrètes.
Les environnements confinés, tels que les blocs chirurgicaux ou les salles blanches, hébergent des processus complexes auxquels sont associés de nombreux risques. Leur conception, leur réalisation et leur exploitation sont complexes, de par les très nombreuses normes les encadrant. La qualification de ces « environnements normés » , afin d'en garantir la qualité de conception, requiert une expertise fine du métier et souffre du manque d'outil en permettant l'automatisation. Partant de ce constat, nous proposons une approche unifiée visant à faciliter la qualification des environnements normés. Celle-ci s'appuie sur une représentation du contexte normatif sous la forme d'un graphe unique, ainsi que sur une modélisation de l'environnement et son objet final par étapes successives permettant une vérification incrémentale de même que la production d' Cette démarche, illustrée au travers du domaine des environnements confinés médicaux, est générique et peut s'appliquer à l'ensemble des environnements normés.
Le présent manuscrit présente de nouvelles techniques d'extraction des structures : du dialogue de groupe, d'une part ;  Déceler la structure de longs textes et de conversations est une étape cruciale afin de reconstruire leur signification sous-jacente. La difficulté de cette tâche est largement reconnue, sachant que le discours est une description de haut niveau du langage, et que le dialogue de groupe inclut de nombreux phénomènes linguistiques complexes. Historiquement, la représentation du discours a fortement évolué, partant de relations locales, formant des collections non-structurées, vers des arbres, puis des graphes contraints. Nos travaux utilisent ce dernier paradigme, via la Théorie de Représentation du Discours Segmenté. Notre recherche se base sur un corpus annoté de discussions en ligne en anglais, issues du jeu de société Nous discutons de deux investigations liées à notre corpus. La première étend la définition de la contrainte de la frontière droite, une formalisation de certains principes de cohérence de la structure du discours, pour l'adapter au dialogue de groupe. La seconde fait la démonstration d'un processus d'extraction de données permettant à un joueur artificiel des Colons d'obtenir un avantage stratégique en déduisant les possessions de ses adversaires à partir de leurs négociations. Nous proposons de nouvelles méthodes d'analyse du dialogue, utilisant conjointement apprentissage automatisé, algorithmes de graphes et optimisation linéaire afin de produire des structures riches et expressives, avec une précision supérieure comparée aux efforts existants. Nous décrivons notre méthode d'analyse du discours par contraintes, d'abord sur des arbres en employant la construction d'un arbre couvrant maximal, puis sur des graphes orientés acycliques en utilisant la programmation linéaire par entiers avec une collection de contraintes originales. Nous appliquons enfin ces méthodes sur les structures de l'argumentation, avec un corpus de textes en anglais et en allemand, parallèlement annotés avec deux structures du discours et une argumentative. Nous comparons les trois couches d'annotation et expérimentons sur l'analyse de l'argumentation, obtenant de meilleurs résultats, relativement à des travaux similaires.
Dans le cadre des travaux en cours pour informatiser un grand nombre de langues «  peu dotées  » , en particulier celles de l'espace francophone, nous avons créé un système de traduction automatique français-somali dédié à un sous-langage journalistique, permettant d'obtenir des traductions de qualité, à partir d'un corpus bilingue construit par post-édition des résultats de Google Translate (GT), à destination des populations somalophones et non francophones de la Corne de l'Afrique. Ce dernier constitue'est un corpus aligné, et de très bonne qualité, car nous l'avons construit en post-éditant les pré-traductions de GT, qui combine pour cela avec une combinaison de lason système de TA français-anglais et système de TA anglais-somali. Il Ce corpus a également fait l'objet d'une évaluation de la part depar 9 annotateurs bilingues qui ont donné une note score de qualité à chaque segment du corpus, et corrigé éventuellement notre post-édition. D'autre part, nous avons mis en place une iMAG (passerelle interactive d'accès multilingue) qui permet à des internautes somaliens non francophones du continent d'accéder en somali à l'édition en ligne du journal «  La Nation de Djibouti  » .
Nous avons développé une nouvelle approche pour la morphologie traditionnelle arabe destinés aux traitements automatiques de l'arabe écrit. Cette approche permet de formaliser plus simplement la morphologie sémitique en utilisant Unitex, une suite logicielle fondée sur des ressources lexicales pour l'analyse de corpus. Pour les verbes (Neme, 2011), j'ai proposé une taxonomie flexionnelle qui accroît la lisibilité du lexique et facilite l'encodage, la correction et la mise-à-jour par les locuteurs et linguistes arabes. La grammaire traditionnelle définit les classes verbales par des schèmes et des sous-classes par la nature des lettres de la racine. Dans ma taxonomie, les classes traditionnelles sont réutilisées, et les sous-classes sont redéfinies plus simplement. La couverture lexicale de cette ressource pour les verbes dans un corpus test est de 99 %. Pour les noms et les adjectifs (Neme, 2013) et leurs pluriels brisés, nous sommes allés plus loin dans l'adaptation de la morphologie traditionnelle. Tout d'abord, bien que cette tradition soit basée sur des règles dérivationnelles Ensuite, nous avons gardé les concepts de racine et de schème, essentiels au modèle sémitique. Pourtant, notre innovation réside dans l'inversion du modèle traditionnel de racine-et-schème au modèle schème-et-racine, qui maintient concis et ordonné l'ensemble des classes de modèle et de sous-classes de racine. Ainsi, nous avons élaboré une taxonomie pour le pluriel brisé contenant 160 classes flexionnelles, ce qui simplifie dix fois l'encodage du pluriel brisé. Depuis, j'ai élaboré des ressources complètes pour l'arabe écrit. Ces ressources sont décrites dans Neme et Paumier (2019). Ainsi, nous avons complété ces taxonomies par des classes suffixées pour les pluriels réguliers, adverbes, et d'autres catégories grammaticales afin de couvrir l'ensemble du lexique. En tout, nous obtenons environ 1000 classes de flexion implémentées au moyen de transducteurs concatenatifs et non-concatenatifs. A partir de zéro, j'ai créé 76000 lemmes entièrement voyellisés, et chacun est associé à une classe flexionnelle. Ces lemmes sont fléchis en utilisant ces 1000 FST, produisant un lexique entièrement fléchi de plus 6 millions de formes. J'ai étendu cette ressource entièrement fléchie à l'aide de grammaires d'agglutination pour identifier les mots composés jusqu'à 5 segments, agglutinés autour d'un verbe, d'un nom, d'un adjectif ou d'une particule. Les grammaires d'agglutination étendent la reconnaissance à plus de 500 millions de formes de mots valides, partiellement ou entièrement voyelles. La taille de fichier texte généré est de 340 mégaoctets (UTF-16). Il est compressé en 11 mégaoctets avant d'être chargé en mémoire pour la recherche rapide (fast lookup). La génération, la compression et la minimisation du lexique prennent moins d'une minute sur un MacBook. Le taux de couverture lexical d'un corpus est supérieur à 99 %. La vitesse de tagger est de plus de 200 000 mots/s, si les ressources ont été pré-chargées en mémoire RAM. La précision et la rapidité de nos outils résultent de notre approche linguistique systématique et de l'adoption des meilleurs choix pratiques en matière de méthodes mathématiques et informatiques. La procédure de recherche est rapide parce que nous utilisons l'algorithme de minimisation d'automate déterministique acyclique (Revuz, 1992) pour comprimer le dictionnaire complet, et parce qu'il n'a que des chaînes constantes.
Le problème de représentation automatique de la signification logique des énoncés ambigus en langage naturel a suscité l'intérêt des chercheurs dans le domaine de la sémantique computationnelle et de la logique. L'ambiguïté dans le langage naturel peut se manifester au niveau lexical / syntaxique / sémantique de la construction de sens, ou elle peut être causée par d'autres facteurs tels que la grammaticalité et le manque de contexte dans lequel la phrase est effectivement prononcée. L'approche traditionnelle Montagovienne ainsi que ses extensions modernes ont tenté de capturer ce phénomène en fournissant quelques modèles qui permettent la génération automatique de formules logiques. Cependant, il existe un axe de recherche qui n'est pas encore profondément étudié : classer les interprétations d'énoncés ambigus en fonction des préférences réelles des utilisateurs de la langue. Ce manque suggère une nouvelle direction d'étude qui est partiellement explorée dans ce mémoire en modélisant des préférences de sens en alignement avec certaines des théories de performance préférentielles humaines bien étudiées disponibles dans la littérature linguistique et psycholinguistique. Afin d'atteindre cet objectif, nous suggérons d'utiliser / d'étendre les Grammaires catégorielles pour notre analyse syntaxique et les Réseaux catégoriels de preuve comme notre analyse syntaxique. Nous utilisons également le Lexique Génératif Montagovien pour dériver une formule logique multi-triée comme notre représentation de signification sémantique. Nous utilisons un cadre appelé Montagovian Generative
Durant les situations de crise, telles que les catastrophes, le besoin de recherche d'informations (RI) pertinentes partagées dans les microblogs en temps réel est inévitable. Cependant, le grand volume et la variété des flux d'informations partagées en temps réel dans de telles situations compliquent cette tâche. Contrairement aux approches existantes de RI basées sur l'analyse du contenu, nous proposons de nous attaquer à ce problème en nous basant sur les approches centrées utilisateurs tout en levant un certain nombre de verrous méthodologiques et technologiques inhérents : 1) à la collection des données partagées par les utilisateurs à évaluer, 2) à la modélisation de leurs comportements, 3) à l'analyse des comportements, et 4) à la prédiction et le suivi des utilisateurs primordiaux en temps réel. Dans ce contexte, nous détaillons les approches proposées dans cette thèse afin de prédire les utilisateurs primordiaux qui sont susceptibles de partager les informations pertinentes et exclusives ciblées et de permettre aux intervenants d'urgence d'accéder aux informations requises Nous avons tout d'abord étudié l'efficacité de différentes catégories de mesures issues de la littérature et proposées dans cette thèse pour représenter le comportement des utilisateurs. En se basant sur cette approche de modélisation, nous entraînons différents modèles de prédiction qui apprennent à différencier les comportements des utilisateurs primordiaux de ceux qui ne le sont pas durant les situations de crise. La pertinence et l'efficacité des modèles de prédiction appris ont été validées à l'aide des données collectées par notre système multi-agents MASIR durant deux inondations qui ont eu lieu en France et des vérités terrain appropriées à ces collections.
Notre étude est centrée sur les mécanismes elliptiques au sein des anaphores associatives méronymiques. Nous sommes partie de l'hypothèse que dans ce type d'anaphore, il existe deux structures : une structure profonde et une structure de surface. La première consiste en la présence des trois éléments : le tout, le prédicat partitif et la partie. La deuxième, où apparaissent les différents types d'ellipse, fait l'objet de notre travail. Nous nous sommes attardée sur trois types d'ellipse que nous considérons caractéristiques des anaphores méronymiques : l'ellipse du prédicat partitif, celle du deuxième élément de la structure N DeN et celle de l'antécédent anaphorique. Traitées séparément, les anaphores nominales, les anaphores verbales et les anaphores adverbiales ont été soumises dans un premier temps à une description syntactico-sémantique et dans un deuxième temps à la théorie des trois fonctions primaires. Cette théorie nous a permis d'expliquer la possibilité pour certains éléments, d'être élidés au sein de l'anaphore associative méronymique.
Les dispositifs mobiles tels que les smartphones, les montres connectées ou les lunettes électroniques ont révolutionné la façon dont nous interagissons. Les lunettes électroniques nous intéressent, car elles fournissent aux utilisateurs une vision simultanée des mondes physique et numérique. Cependant, l'interaction sur les lunettes électroniques n'est pas bien explorée. L'amélioration de l'interaction sur ce dispositif peut convaincre les utilisateurs à l'utiliser plus dans la vie quotidienne. Le sujet de thèse est focalisé sur l'étude et le développement de nouvelles techniques d'interaction avec les lunettes électroniques. En effet, il n'est pas possible d'interagir aussi finement en mobilité ou en situation d'urgence par rapport à une situation stable telle qu'assis devant un bureau. Notre contexte de travail se situe dans le domaine de la santé et en particulier celui du personnel médical visitant un patient dans un hôpital. Le personnel médical doit pouvoir accéder aux données du patient déjà collectées, obtenir des données physiologiques en temps réel, préparer son diagnostic et communiquer avec ses collègues. Le verrou scientifique pour la thèse ici est de trouver des solutions qui permettent au personnel médical de réaliser ces tâches de façon plus précise et moins contraignante. Le but est de rendre plus efficace le diagnostic et le partage d'informations et de faire d'un dispositif encore non maîtrisé un outil fonctionnel en milieu professionnel de la santé. Dans cette optique, les travaux de cette thèse présentent des contributions théoriques et applicatives. Nous avons tout d'abord répertorié les différents travaux effectués dans le cadre des lunettes électroniques pour le domaine de la santé, tout en indiquant le potentiel, les résultats pertinents et les limites. Nous nous sommes focalisés sur les lunettes électroniques pour afficher et manipuler les dossiers patients. D'un point de vue conceptuel, nous avons proposé un espace de conception à huit dimensions pour identifier les lacunes dans les systèmes existants et aider à la conception de nouveaux systèmes. D'un point de vue applicatif, en interaction en sortie, nous avons introduit une technique appelée K-Fisheye sur une interface à tuiles qui permet de parcourir un grand ensemble de données comme celui présent dans le dossier patient. Nous avons utilisé ensuite l'espace de conception pour porter un système existant sur les lunettes électroniques. Le prototype obtenu s'appelle mCAREglass. Ensuite, nous avons conçu une nouvelle technique d'entrée de texte appelé TEXTile qui permet de saisir du texte sur un tissu interactif communiquant avec les lunettes.
De nombreuses applications génèrent et reçoivent des données sous la forme de flux continu, illimité, et très rapide. Cela pose naturellement des problèmes de stockage, de traitement et d'analyse de données qui commencent juste à être abordés dans le domaine des flux de données. L'apprentissage de ce modèle est, contrairement à sa version de départ, guidé non seulement par la nouveauté qu'apporte une donnée d'entrée mais également par la donnée elle-même. De ce fait, le modèle ILoNDF peut acquérir constamment de nouvelles connaissances relatives aux fréquences d'occurrence des données et de leurs variables, ce qui le rend moins sensible au bruit. Dans un premier temps, notre travail se focalise sur l'étude du comportement du modèle ILoNDF dans le cadre général de la classification à partir d'une seule classe en partant de l'exploitation des données fortement multidimensionnelles et bruitées. Ce type d'étude nous a permis de mettre en évidence les capacités d'apprentissage pures du modèle ILoNDF vis-à-vis de l'ensemble des méthodes proposées jusqu'à présent. Dans un deuxième temps, nous nous intéressons plus particulièrement à l'adaptation fine du modèle au cadre précis du filtrage d'informations. Notre objectif est de mettre en place une stratégie de filtrage orientée-utilisateur plutôt qu'orientée-système, et ceci notamment en suivant deux types de directions. La première direction concerne la modélisation utilisateur à l'aide du modèle ILoNDF. Cette modélisation fournit une nouvelle manière de regarder le profil utilisateur en termes de critères de spécificité, d'exhaustivité et de contradiction. La seconde direction, complémentaire de la première, concerne le raffinement des fonctionnalités du modèle ILoNDF en le dotant d'une capacité à s'adapter à la dérive du besoin de l'utilisateur au cours du temps. Enfin, nous nous attachons à la généralisation de notre travail antérieur au cas où les données arrivant en flux peuvent être réparties en classes multiples.
Un des enjeux majeurs du marché des synthétiseurs et de la recherche en synthèse sonore aujourd'hui est de proposer une nouvelle forme de synthèse permettant de générer des sons inédits tout en offrant aux utilisateurs de nouveaux contrôles plus intuitifs afin de les aider dans leur recherche de sons. En effet, les synthétiseurs sont actuellement des outils très puissants qui offrent aux musiciens une large palette de possibilités pour la création de textures sonores, mais également souvent très complexes avec des paramètres de contrôle dont la manipulation nécessite généralement des connaissances expertes. Cette thèse s'intéresse ainsi au développement et à l'évaluation de nouvelles méthodes d'apprentissage machine pour la synthèse sonore permettant la génération de nouveaux sons de qualité tout en fournissant des paramètres de contrôle pertinents perceptivement. Le premier challenge que nous avons relevé a donc été de caractériser perceptivement le timbre musical synthétique en mettant en évidence un jeu de descripteurs verbaux utilisés fréquemment et de manière consensuelle par les musiciens. Deux études perceptives ont été menées : un test de verbalisation libre qui nous a permis de sélectionner huit termes communément utilisés pour décrire des sons de synthétiseurs, et une analyse à échelles sémantiques permettant d'évaluer quantitativement l'utilisation de ces termes pour caractériser un sous-ensemble de sons, ainsi que d'analyser leur "degré de consensualité". Dans un second temps, nous avons exploré l'utilisation d'algorithmes d'apprentissage machine pour l'extraction d'un espace de représentation haut-niveau avec des propriétés intéressantes d'interpolation et d'extrapolation à partir d'une base de données de sons, le but étant de mettre en relation cet espace avec les dimensions perceptives mises en évidence plus tôt. S'inspirant de précédentes études sur la synthèse sonore par apprentissage profond, nous nous sommes concentrés sur des modèles du type autoencodeur et avons réalisé une étude comparative approfondie de plusieurs types d'autoencodeurs sur deux jeux de données différents. Ces expériences, couplées avec une étude qualitative via un prototype non temps-réel développé durant la thèse, nous ont permis de valider les autoencodeurs, et en particulier l'autoencodeur variationnel (VAE), comme des outils bien adaptés à l'extraction d'un espace latent de haut-niveau dans lequel il est possible de se déplacer de manière continue et fluide en créant de tous nouveaux sons. Cependant, à ce niveau, aucun lien entre cet espace latent et les dimensions perceptives mises en évidence précédemment n'a pu être établi spontanément. Pour finir, nous avons donc apporté de la supervision au VAE en ajoutant une régularisation perceptive durant la phase d'apprentissage. En utilisant les échantillons sonores résultant du test perceptif avec échelles sémantiques labellisés suivant les huit dimensions perceptives, il a été possible de contraindre, dans une certaine mesure, certaines dimensions de l'espace latent extrait par le VAE afin qu'elles coïncident avec ces dimensions. Un test comparatif a été finalement réalisé afin d'évaluer l'efficacité de cette régularisation supplémentaire pour conditionner le modèle et permettre un contrôle perceptif (au moins partiel) de la synthèse sonore.
La non-adhérence médicamenteuse désigne les situations où le patient ne suit pas les directives des autorités médicales concernant la prise d'un médicament. Il peut s'agir d'une situation où le patient prend trop (sur-usage) ou pas assez (sous-usage) de médicaments, boit de l'alcool alors qu'il y a une contrindication, ou encore commet une tentative de suicide à l'aide de médicaments. Selon [HAYNES 2002] améliorer l'adhérence pourrait avoir un plus grand impact sur la santé de la population que tout autre amélioration d'un traitement médical spécifique. Cependant les données sur la non-adhérence sont difficiles à acquérir, puisque les patients en situation de non-adhérence sont peu susceptibles de rapporter leurs actions à leurs médecins. Dans un premier temps, nous collectons un corpus de messages postés sur des forums médicaux. Nous construisons des vocabulaires de noms de médicaments et de maladies utilisés par les patients. Nous utilisons ces vocabulaires pour indexer les médicaments et maladies dans les messages. Ensuite nous utilisons des méthodes d'apprentissage supervisé et de recherche d'information pour détecter les messages de forum parlant d'une situation de non-adhérence. Nous identifions 3 motivations : gérer soi-même sa santé, rechercher un effet différent de celui pour lequel le médicament est prescrit, être en situation d'addiction ou d'accoutumance. La gestion de sa santé recouvre ainsi plusieurs situations : éviter un effet secondaire, moduler l'effet du médicament, sous-utiliser un médicament perçu comme inutile, agir sans avis médical. Additionnellement, une non-adhérence peut survenir par erreur ou négligence, sans motivation particulière. À l'issue de notre étude nous produisons : un corpus annoté avec des messages de non-adhérence, un classifieur capable de détecter les messages de non-adhérence, une typologie des situations de non-adhérence et une analyse des causes de la non-adhérence.
Dans cette thèse, nous étudions la segmentation d'un flux audio en parole, musique et parole sur musique (P/M). Cette étape est fondamentale pour toute application basée sur la transcription automatique de flux radiophoniques et plus généralement multimédias. L'application visée ici est un système de détection de mots clés dans les émissions radiophoniques. Les performances de ce système dépendront de la bonne segmentation du signal fournie par le système de discrimination parole/musique. En effet, une mauvaise classification du signal peut provoquer des omissions de mots clés ou des fausses alarmes. Afin d'améliorer la discrimination parole/musique, nous proposons une nouvelle méthode de paramétrisation du signal. Nous utilisons la décomposition en ondelettes qui permet une analyse des signaux non stationnaires dont la musique est un exemple. Nous calculons différentes énergies sur les coefficients d'ondelettes pour construire nos vecteurs de paramètres. Cette architecture a été choisie car elle permet de trouver les meilleurs paramètres indépendamment pour chaque tâche P/NP et M/NM. Une fusion des sorties des classifieurs est alors effectuée pour obtenir la décision finale : parole, musique ou parole sur musique. Les résultats obtenus sur un corpus réel d'émissions de radio montrent que notre paramétrisation en ondelettes apporte une nette amélioration des performances en discrimination M/NM et P/M par rapport à la paramétrisation de référence fondée sur les coefficients cepstraux.
Cette thèse a pour objectif de contribuer à l'étude de la traduction en Égypte entre 1798 et 1873 et son rôle sur les réformes entreprises par les intellectuels sous l'égide de la politique du Wālī réformiste (Mohamed Ali Pacha), mais aussi à l'étude de l'école al-Alsun et son apport à la traduction, l'expansion lexicale et à l'acquisition de la Nahḍa. Notre sujet revêt donc une importance particulière parce qu'il traite non seulement le début des relations linguistiques et culturelles franco-égyptiennes qui favorisa l'apparition de la traduction (au sens moderne) et de l'arabe juridique moderne grâce à l'école al-Alsun au Caire en 1834. Ce n'est pas un hasard si nous avons accordé une attention particulière à cette région du Mašriq al-'arabī, car elle constitue un foyer où la majorité des activités de traduction eurent lieu par rapport aux autres régions du monde arabe. D'ailleurs, pendant la collection du corpus de notre projet de recherche, nous n'avons pas trouvé un ouvrage consacré à cette école qui, pourtant, joua un rôle primordial sur l'évolution de la société arabe en général et la société égyptienne en particulier. Toutefois, lorsqu'il s'agit de la « traduction » dans le monde arabe, la dimension historique révèle les grandes aptitudes de cette discipline telles que : la traduction du persan à l'arabe ou du grec à l'arabe à l'époque omeyyade et abbasside, mais aussi la traduction au sens contemporain qui fait l'objet de notre recherche.
L'objectif de cette thèse est d'étudier les sentiments dans les documents comparables. Premièrement, nous avons recueillis des corpus comparables en anglais, français et arabe de Wikipédia et d'Euronews, et nous avons aligné ces corpus au niveau document. Nous avons en plus collecté des documents d'informations des agences de presse locales et étrangères dans les langues anglaise et arabe. Deuxièmement, nous avons présenté une mesure de similarité cross-linguistique des documents dans le but de récupérer et aligner automatiquement les documents comparables. Ensuite, nous avons proposé une méthode d'annotation cross-linguistique en termes de sentiments, afin d'étiqueter les documents source et cible avec des sentiments. Enfin, nous avons utilisé des mesures statistiques pour comparer l'accord des sentiments entre les documents comparables source et cible. Les méthodes présentées dans cette thèse ne dépendent pas d'une paire de langue bien déterminée, elles peuvent être appliquées sur toute autre couple de langue
Ce travail porte sur la synthèse de la parole audio-visuelle. Dans la littérature disponible dans ce domaine, la plupart des approches traite le problème en le divisant en deux problèmes de synthèse. Le premier est la synthèse de la parole acoustique et l'autre étant la génération d'animation faciale correspondante. Mais, cela ne garantit pas une parfaite synchronisation et cohérence de la parole audio-visuelle. Pour pallier implicitement l'inconvénient ci-dessus, nous avons proposé une approche de synthèse de la parole acoustique-visuelle par la sélection naturelle des unités synchrones bimodales. La synthèse est basée sur le modèle de sélection d'unité classique. L'idée principale derrière cette technique de synthèse est de garder l'association naturelle entre la modalité acoustique et visuelle intacte. Nous décrivons la technique d'acquisition de corpus audio-visuelle et la préparation de la base de données pour notre système. Nous présentons une vue d'ensemble de notre système et nous détaillons les différents aspects de la sélection d'unités bimodales qui ont besoin d'être optimisées pour une bonne synthèse. L'objectif principal de ce travail est de synthétiser la dynamique de la parole plutôt qu'une tête parlante complète. Nous décrivons les caractéristiques visuelles cibles que nous avons conçues. Nous avons ensuite présenté un algorithme de pondération de la fonction cible. Cet algorithme que nous avons développé effectue une pondération de la fonction cible et l'élimination de fonctionnalités redondantes de manière itérative. Elle est basée sur la comparaison des classements de coûts cible et en se basant sur une distance calculée à partir des signaux de parole acoustiques et visuels dans le corpus. Enfin, nous présentons l'évaluation perceptive et subjective du système de synthèse final. Les résultats montrent que nous avons atteint l'objectif de synthétiser la dynamique de la parole raisonnablement bien
Les systèmes de Traitement Automatique des Langues Naturelles (TALN) sont de manière récurrente confrontés au problème de la génération et de la propagation d'hypothèses concurrentes et erronées. Afin d'écarter ces erreurs d'interprétation du processus d'analyse, il apparaît indispensable d'avoir recours à des stratégies spécifiques de contrôle dont l'objectif est de différencier les hypothèses concurrentes selon leur degré de pertinence. Sur la plupart des cas d'indétermination observés, on constate que cette évaluation de la pertinence relative des hypothèses repose sur l'exploitation de plusieurs sources de connaissances hétérogènes, qui doivent être combinées pour garantir un contrôle robuste et fiable. À partir de ce constat, nous avons montré que le traitement des indéterminations répondait à une formalisation générique en tant que problème décisionnel basé sur de multiples critères de comparaison. Par rapport aux méthodes alternatives, cette approche se différentie notamment par l'importance qu'elle accorde aux connaissances et préférences qu'un expert est en mesure d'apporter sur le problème traité. À partir de cette intersection novatrice entre le TALN et l'AMCD, nos travaux se sont focalisés sur le développement d'un module décisionnel de contrôle multicritère. L'intégration de ce module au sein d'un système complet de TALN nous a permis d'attester d'une part la faisabilité de notre approche et d'autre part de l'expérimenter sur différents cas concrets d'indétermination.
Cette thèse est le fruit de l'interaction de deux disciplines qui sont la détection de changements dans des images multitemporelles et le raisonnement évidentiel à l'aide de la théorie de Dempster-Shafer (DST). Aborder le problème de détection et d'analyse de changements par la DST nécessite la détermination d'un cadre de discernement exhaustif et exclusif. Ce problème s'avère complexe en l'absence des informations a priori sur les images. L'idée de cet algorithme est la représentation de chaque classe par un nombre varié de centroïdes afin de garantir une meilleure caractérisation de classes. Afin d'assurer l'exhaustivité du cadre de discernement, un nouvel indice de validité de clustering permettant de déterminer le nombre optimal de classes sémantiques est proposé. La troisième contribution consiste à exploiter la position du pixel par rapport aux centroïdes des classes et les degrés d'appartenance afin de définir la distribution de masse qui représente les informations. Nous avons souligné la capacité du conflit évidentiel à indiquer les transformations multi-temporelles. Nous avons porté notre raisonnement sur la décomposition du conflit global et l'estimation des conflits partiels entre les couples des éléments focaux pour mesurer le conflit causé par le changement. Cette stratégie permet d'identifier le couple de classes qui participent dans le changement. Pour quantifier ce conflit, nous avons proposé une nouvelle mesure de changement notée CM. Finalement, nous avons proposé un algorithme permettant de déduire la carte binaire de changements à partir de la carte de conflits partiels.
Dans le cadre des sciences forensiques, le déguisement de la voix révèle un attrait particulier. La plupart des criminels essaie de déguiser leur voix avant de réaliser un appel anonyme ou une revendication terroriste. L'objectif du malfaiteur est de modifier le registre de sa voix afin de changer la perception de son identité ou d'imiter la voix d'un locuteur cible. Cette thèse envisage deux types d'imposture : la transformation de la voix à partir de techniques délibérées et non électroniques, puis de transformations délibérées et électroniques. L'analyse de la transformation est tout d'abord réalisée à partir de paramètres acoustiques en vue de mesurer les variations par rapport à une voix non déguisée. Quatre déguisements parmi les plus commun dans le contexte criminel ont été sélectionnés. La contrainte d'audibilité et d'intelligibilité a été imposée aux locuteurs dans la construction de la base de données. L'analyse acoustique a permis de mettre en évidence des variations spécifiques aux déguisements. L'application de méthodes de classification automatique permet de détecter l'imposture avec un niveau de performance satisfaisant. Ensuite, des techniques de conversion automatique de la voix ont été mises en oeuvre afin d'imiter automatiquement un locuteur cible d'une part, et d'autre part d'envisager la réversibilité du déguisement. L'imposture réalisée comme l'application de la conversion à la réversibilité du déguisement offrent des résultats très satisfaisants.
La médecine fondée sur les preuves a permis de formaliser des guides de pratique clinique qui définissent des flux de travail et des recommandations à suivre pour un domaine clinique concis. Ces guides se sont construits dans le but de standardiser les soins de santé et d'obtenir les meilleurs résultats possibles pour les patients. Néanmoins, les médecins n'adhèrent pas toujours à ces directives en raison de diverses limitations cliniques et de mise-en-œuvre. D'une part, les médecins n'ont pas toujours familiarisés ou en accord avec les lignes directrices des guides de pratique clinique, doutant ainsi de leur efficacité et des résultats attendus par rapport aux pratiques antérieures. D'autre part, maintenir ces guides à jour en incluant les dernières preuves établies requiert une gestion continue d'une documentation établie sur support papier. Les systèmes d'aide à la décision clinique sont ainsi proposés comme aide durant le processus de prise de décision clinique, par la mise en œuvre informatisée des guides pour promouvoir leur consultation et l'adhésion des médecins. Bien que ces systèmes aident à améliorer la conformité des guides, il subsiste certains obstacles hérités des guides sur support papier qui ne sont pas résolus avec leur mise en œuvre informatisée, comme le traitement des cas complexes non-définis dans les directives ou le manque de représentation d'autres facteurs externes qui peuvent influer sur les traitements fournis et faire dévier des recommandations des guides (c.-à-d. les préférences du patient). La présente thèse propose un système avancé d'aide à la décision clinique pour faire face aux limitations du soutien purement basé en guides et aller au-delà des connaissances formalisées en analysant les données cliniques, les résultats et les performances de toutes les décisions prises au fil du temps. Pour atteindre ces objectifs, une approche de modélisation des connaissances et performances cliniques de manière sémantique validée et informatisée a été présentée, en s'appuyant sur une ontologie et avec la formalisation du concept d'Événement Décisionnel. De plus, un cadre indépendant du domaine a été mis en place pour faciliter le processus d'informatisation, de mise à jour et de mise en œuvre des guides de pratique clinique au sein d'un système d'aide à la décision clinique afin de fournir un soutien clinique à pour chaque patient interrogé. Pour répondre aux limites des guides, une méthodologie permettant d'augmenter les connaissances cliniques en utilisant l'expérience a été présentée ainsi qu'une évaluation de la performance clinique et de la qualité au fil du temps, en fonction des différents résultats cliniques étudiés, tels que l'utilisabilité et la fiabilité clinique derrière les connaissances cliniques formalisées. Enfin, les données du monde réel accumulées ont été explorées pour soutenir les cas futurs, promouvoir l'étude de nouvelles hypothèses cliniques et aider à la détection des tendances et des modèles sur les données à l'aide d'outils d'analyse visuelle. Les modules présentés ont été développés et mis en œuvre dans leur majorité dans le cadre du projet européen Horizon 2020 DESIREE, dans lequel le cas d'utilisation était axé sur le soutien des unités de soins du sein au cours du processus décisionnel pour la prise en charge des patientes atteintes d'un cancer du sein primaire, en effectuant une validation technique et clinique sur l'architecture présentée, dont les résultats sont présentés dans cette thèse. Néanmoins, certains des modules ont également été utilisés dans d'autres domaines médicaux tels que le développement des guides de pratique clinique pour le diabète gestationnel, mettant en évidence l'interopérabilité et la flexibilité du travail présenté.
Avec la croissance explosive de la numérisation du patrimoine culturel, de nombreuses patrimoine culturel institutions ont été la conversion des objets physiques du patrimoine culturel dans la représentation numérique ou représentation descriptive. Toutefois, la conversion a donné lieu à plusieurs questions telles que : 1) les documents sont de nature descriptive, 2) l'ambiguïté et de la brièveté des documents, 3) le vocabulaire spécifique est utilisé dans les documents, et 4), il existe également des variations dans les termes utilisés dans le document. En outre, l'utilisation de mots-clés inexactes également entraîné problème de requête court. La plupart du temps, les problèmes sont causés par la faute agrégée en annotant les documents alors que le problème de requête court est causé par l'utilisateur naïf qui a peu de connaissances et d'expérience dans le domaine du patrimoine culturel. Dans cette recherche, l'objectif principal est de modéliser le système d'accès à l'information pour surmonter partiellement les questions soulevées par le processus de documentation et le fond des utilisateurs du patrimoine culturel numérique. Par conséquent, trois types d'outils d'accès aux informations sont introduites et établies à savoir l'information système de recherche, la recherche de contexte, et jeu mobile sur le patrimoine culturel qui permettent à l'utilisateur d'accéder, d'apprendre et d'explorer les informations sur le patrimoine culturel. Fondamentalement, l'idée principale d'information système de recherche et contexte de recherche est d'intégrer la relation de lien entre les termes dans le modèle de la langue par l'extension de Dirichlet lissage pour résoudre les problèmes qui se posent à la fois le processus de documentation et de fond des utilisateurs. En outre, un modèle de préférence est présenté sur la base de la théorie de la charge d'un condensateur de quantifier le contexte cognitif basé sur le temps et les intégrer dans la longue Dirichlet lissage. En outre, un jeu mobile est introduite par l'intégration des éléments des jeux de monopole et chasse au trésor pour atténuer les problèmes découlant de l'arrière-plan des utilisateurs en particulier leur comportement décontracté. Les premier et deuxième approches ont été testées sur le patrimoine culturel dans CLEF (chic) ​​collection qui se compose de questions et de courts documents. Les résultats montrent que l'approche est efficace et donne une meilleure précision lors de la récupération. Enfin, une enquête a été menée pour étudier la troisième approche, et le résultat donne à penser que le jeu est en mesure d'aider les participants à explorer et apprendre les informations sur le patrimoine culturel. En outre, les participants ont également estimé qu'une recherche d'information outil qui est intégré avec le jeu peut fournir plus d'informations à l'utilisateur d'une manière plus pratique tout en jouant le jeu et en visitant les sites du patrimoine dans le match. En conclusion, les résultats montrent que les solutions proposées sont en mesure de résoudre les problèmes posés par le processus de documentation et le fond des utilisateurs du patrimoine culturel numérique.
Cette recherche présente une méthode d'analyse micro-systémique des mots composés thaïs. Les points de vue des autres travaux sont étudiés. Le deuxième chapitre présente les caractéristiques de la langue thaïe qui possède une forme d'écriture typique sans espacement et peut entrainer des difficultés en termes d'ambiguïté dans la traduction. Certaines divergences entre le thaï et le français sont soulignées à l'aide de la théorie micro-systémique du Centre Tesnière. Le quatrième chapitre met en évidence un contrôle modélisé des unités lexicales codées syntaxiquement et sémantiquement afin d'en définir des algorithmes efficaces. Le dernier chapitre conclut sur les résultats des nouveaux algorithmes par leur informatisation. Sont enfin énoncées les perspectives ouvertes par cette nouvelle recherche. Cette étude est présentée comme un travail fiable à l'élimination des ambiguïtés. Fondée sur une méthode hybride, elle nous a permis d'atteindre notre objectif et de trouver ainsi une voie efficace qui nous autorise à traduire automatiquement les mots thaïs vers le français. Le résultat place cet outil comme l'un des plus accessibles à la recherche internationale où le thaï et le français prennent leurs places de choix
La détection de langage abusif en ligne est un problème de classification comportant des défis cruciaux liés à la compréhension automatique du langage naturel et à la variété des contextes complexes et riches par lesquels le langage naturel se manifeste. Les réseaux de neurones et les techniques standard d'apprentissage profond se sont montrés efficaces dans la detection de contenu explicitement offensif dans les conversations mais il est plus difficile de détecter automatiquement des formes plus subtiles, mais aussi plus fréquentes, de *toxicité passive* comme le sarcasme, les comportements passifs-aggressifs et les discours de haine formulés poliment, mais incendiaires. Les modèles d'apprentissage automatique traditionnels et plus récemment les réseaux de neurones convolutifs (CNN) basés sur les mots et sur les caractères et les réseaux de neurones récurrents comme les modèles Long Short-Term Memory (LSTM) et les Gated Recurrent Units (GRUs), couplés à du plongement de mots ont été utilisés pour assister la modération de commentaires et pour detecter les discours de haine. De plus, des travaux ont cherché à interpréter ces réseaux de neurones detectant l'agressivité verbale. Dans le cadre de l'initiative de recherche Conversation AI de Google Jigsaw, notre recherche portera sur la façons dont des méthodes novatrices d'apprentissage profond peuvent saisir la toxicité passive. Conversation AI a de l'expérience dans le développement des modèles théoriques, et des outils pour les utilisateurs et sites web. La thèse s'articulera autour de deux axes complémentaires : les données annotées et les modèles. Pour la classification supervisée, nous avons besoin de grandes quantités de commentaires annotés issus de vraies conversations en ligne. Nous prévoyons de partir des ensembles de données publics fournis par l'équipe Jigsaw / Conversation-AI. Ils sont soit annotés manuellement par production participative soit annotés à partir de features comme les tags ou étiquettes. Ces jeux de données ont montré un nombre significatif de commentaires contenant de la toxicité passive mais caractérisés par une probabilité de toxicité intermédiaire. Nous suivrons l'approche utilisée par l'interface de programmation applicative Perspective, qui consiste à sélectionner un petit nombre de catégories vraissemblables pour annoter les différents types de toxicité passive. Nous exploiterons et améliorerons les modèles Transformer et BERT, qui s'appuient sur des mécanismes d'attention et nous observerons leur comparaison à d'autres modèles d'apprentissage profond. Puisque la toxicité passive se base sur l'ambiguité du langage naturel, nous nous concentrerons sur la modélisation de l'impact émotionnel des messages qui, comme Conversation AI l'a découvert, aboutit à de plus forts niveaux d'accords inter-annotateurs (par rapport aux distinctions sémantiques plus complexes que les document de violation de politique tentent d'établir). Les méthodes d'apprentissage par renforcement (RL) peuvent également se montrer intéressantes dans ce contexte. l'autre aspect de cette recherche est l'analyse et la compréhension des origines et des mécanismes du comportement passif-aggressif en ligne. Ceci peut être fait en mesurant et en visualisant son impact sur les conversations. Du code source ouvert et des publications dans des conférences et journaux sélectifs seront produits au cours de ce doctorat. Les modèles seront implémentés en Python et avec le cadre de développement TensorFlow. Les métriques utilisées pour comparer les architectures incluent la justesse, la précision, le rappel, la spécificité, le taux de faux positifs, le score F1, la fonction d'efficacité du récepteur et son aire sous la courbe (ROC-AUC). Au cours de cette recherche, nous prévoyons d'apporter des éléments de réponse aux différentes questions concernant les conversations sur les réseaux sociaux comme par exemple : comment développer des méthodes d'apprentissage profond pour identifier la toxicité passive dans les conversations en ligne ? L'apprentissage profond peut-il comprendre la pertinence des contributions dans un débat ? Qu'est-ce qui déclenche la toxicité passive dans la conversation ? Que se passe-t-il dans une conversation juste après la détection d'un comportement passif-agressif ? Est-ce un comportement ponctuel ou récurrent des interlocuteurs ? Quelles réponses peuvent désamorcer une conversation et l'empêcher de *mal tourner* ? Dans quelle mesure la détection précoce de toxicité subtile est-elle importante pour garder une conversation en bonne voie ? Nous espérons que les modèles développés performeront suffisement bien pour être finalement intégrés dans l'interface de programmation applicative Perspective, de sorte qu'ils puissent être utilisés directement dans l'industrie, et soutenir un large éventail d'initiatives de recherche supplémentaires. Afin d'amener les internautes en dehors de leurs bulles de filtres en ligne, ils doivent pouvoir dialoguer sur des plateformes à l'abri de l'agressivité, de la violence et de l'ostracisme (phénomène des boucs-émissaires). Les stratégies d'apprentissage automatique, d'apprentissage profond et de traitement automatique du langage naturel peuvent nous aider à *confronter nos opinions divergentes de manière plus constructive* (The Righteous Mind, Jonathan Haidt). Ainsi, cette recherche s'inscrit dans une optique d'accroissement de le participation, de la qualité, et de l'empathie dans les conversations en ligne à grande échelle, en développant de nouveaux modèles appliqués à de nouveaux jeux de données, suggérant des comportements non-biaisés, guidant et éduquant les utilisateurs vers plus d'équité. L'utilisation de nouvelles technologie pour permettre un débat plus rationnel et plus éclairé s'inscrit dans un effort de lutte contre le harcèlement en ligne tout en défendant le débat public, la liberté d'expression, la démocratie et le pluralisme.
Aujourd'hui, l'approche « one-size-fits-all » pour les hypermédias n'est plus applicable. Les Hypermédias Adaptatifs (AH) proposent d'adapter leur comportement aux besoins des utilisateurs. Cependant, en raison de la complexité de leur processus de création et des différentes compétences requises par les auteurs, peu d'entre eux ont été développés. Ces dernières années, de nombreux efforts ont été faits pour proposer d'aider les auteurs à créer leurs propres AH. mais certains problèmes demeurent non résolus. Nous nous sommes intéressées à deux problèmes particuliers. Le premier problème concerne l'intégration des ressources des auteurs dans des systèmes existants. Cela permet aux auteurs de réutiliser directement les adaptations prévues dans un système et de les exécuter sur leurs ressources. Pour répondre à ce problème, nous proposons un processus semi-automatique de fusion / spécialisation pour intégrer le modèle d'un auteur dans un modèle d'un système existant. Notre objectif est double : créer un support pour la définition des correspondances entre les éléments d'un modèle existant et ceux du modèle de l'auteur, et aider à créer un modèle cohérent intégrant les deux modèles et les correspondances entre eux. Le deuxième problème concerne la spécification de l'adaptation, qui est notoirement le processus le plus difficile dans la création des hypermédias adaptatifs. Nous proposons un framework EAP avec trois contributions principales : un ensemble de 22 patrons d'adaptation élémentaires pour l'adaptation de navigation, une typologie organisant les différents patrons d'adaptation élémentaires et un processus pour générer des stratégies d'adaptation basées sur l'utilisation et la combinaison semi-automatique des patrons d'adaptation élémentaires. Nos objectifs sont de permettre de définir facilement des stratégies d'adaptation à un niveau d'abstraction élevé en combinant des stratégies d'adaptation simples. Nous avons comparé l'expressivité du framework EAP à des solutions existantes, identifiant ainsi les avantages et les inconvénients des différentes décisions nécessaires à la définition d'un langage d'adaptation idéal. Nous proposons aussi une vision unifiée de l'adaptation et des langages d'adaptation, basée sur l'analyse de ces solutions et de notre framework, ainsi que sur l'étude de l'expressivité de l'adaptation et de l'interopérabilité entre les différentes solutions analysées. La vision unifiée sur l'adaptation n'est pas limitée aux solutions analysées. Elle peut être utilisée pour comparer et étendre d'autres approches. Outre ces études théoriques qualitatives, cette thèse décrit également des implémentations et des évaluations expérimentales.
Cette thèse concerne la description des constructions possessives en tongugbe, l'un des dialectes de l'éwé, langue parlée au sud-est du Ghana au long du fleuve Volta. La thèse présente une description détaillée des constructions ; et tente de comprendre la relation qui existe entre les constructions possessives propositionnelles et les constructions locatives et existentielles. De plus, ce travail présente une première esquisse de la grammaire de tongugbe. La grammaire présente surtout des résultats préliminaires sur le contraste de durée qui existe au niveau des tons de tongugbe et un système de démonstratifs très riche. Les constructions possessives peuvent être regroupées dans trois catégories : les constructions attributives, les constructions prédicatives et les constructions à possesseur externe. Il est montré que les configurations structurelles des constructions possessives attributives sont motivées par des considérations fonctionnelles. Il est aussi démontré que les variations structurelles des constructions possessives prédicatives et des constructions à possesseur externe correspondent à des différences de sens. Enfin, il est argumenté que, synchroniquement, les constructions possessives propositionnelles et les constructions locatives et existentielles ne peuvent pas être réduites à une structure unique. La proposition soutenue est que chaque construction est une correspondance entre une forme et un sens.
Ce travail de thèse propose une nouvelle approche au traitement des langues naturelles. Plutôt qu'essayer d'estimer directement la probabilité d'une phrase quelconque, nous identifions des structures syntaxiques dans le langage, qui peuvent être utilisées pour modifier et créer de nouvelles phrases à partir d'un échantillon initial. L'étude des structures syntaxiques est accomplie avec des ensembles de substitution Markoviens, ensembles de chaînes de caractères qui peuvent être échangées sans affecter la distribution. Ce point de vue décompose l'analyse du langage en deux parties, une phase de sélection de modèle, où les ensembles de substitution sont sélectionnés, et une phase d'estimation des paramètres, où les fréquences pour chaque ensemble sont estimées. Nous montrons que ces processus constituent des familles exponentielles quand la structure du langage est fixée. Lorsque la structure du langage est inconnue, nous proposons des méthodes pour identifier des ensembles de substitution à partir d'un échantillon, et pour estimer les paramètres de la distribution. Les ensembles de substitution ont quelques relations avec les grammaires hors-Contexte, qui peuvent être utilisées pour aider l'analyse. Nous construisons alors des dynamiques invariantes pour les processus de substitution. Elles peuvent être utilisées pour calculer l'estimateur du maximum de vraisemblance. En effet, les processus de substitution peuvent être vus comme la limite thermodynamique de la mesure invariante d'une dynamique de crossing
Le but de cette étude était de considérer la possibilité de la mise en œuvre de l'approche d'apprentissage par problèmes (APP), Problem Based Learning (PBL), comme une méthodologie d'enseignement, épistémologiquement solide, pour enseigner l'anglais de spécialité (ASP), en particulier, dans le domaine académique de la médecine, English for Academic Medical Purposes (EAMP). Dans un premier temps, l'étude a examiné si PBL est compatible avec l'enseignement des langues et a cherché à déterminer les avantages que cette méthode peut apporter à l'enseignement de l'ASP. L'étude a également tenté de résoudre des problèmes d'apprentissage en anglais qui ont été identifiés dans les Collèges de Santé de l'Année préparatoire (Branche Féminine) au sein de l'Université de Hail, Arabie Saoudite. Une analyse des besoins a été menée dans l'institution pour mieux identifier ces problèmes d'apprentissage. En conséquence, PBL a été mis en œuvre pour déterminer si cette approche est capable de fournir une solution possible à la question, puisque PBL a été initialement mis en œuvre en médecine pour faire face à des problèmes similaires. Cela a entraîné un changement dans les niveaux macro-méthodologique et micro-méthodologique, comme Demaizière (1996 ; 66) les appelle. Dans la partie empirique, une étude longitudinale a été menée avec 13 étudiantes qui ont été observées dans une période de 8 semaines au cours de cinq PBL tutoriels, qui a eu lieu pendant quinze séances. En général, les résultats étaient en faveur de la mise en œuvre de cette approche dans l'enseignement de l'anglais médical. Ils ont également montré que PBL peut améliorer l'autonomie des apprenants ; 
Pour acquérir leur langue maternelle, les bébés doivent a la fois apprendre la forme des mots (par exemple, le mot “chien” en français, “dog” en anglais) et leur sens (la catégorie des chiens). Ces deux aspects de l'apprentissage de la langue ont été typiquement étudiés indépendamment. Des découvertes récentes en psychologie du développement et en apprentissage automatique suggèrent, néanmoins, que la forme et le sens pourraient très bien interagir, et ce, dès les premières étapes du développement. La thèse explore cette piste à travers une étude interdisciplinaire qui combine des outils utilisés dans la technologie de la reconnaissance et la psychologie expérimentale. Dans un premier temps, j'ai développé un modèle computationnel capable d'apprendre, à partir des données naturelles Dans un deuxième temps, j'ai testé la plausibilité cognitive du mécanisme sur des sujets adultes.
L'analyse de ces données repose sur l'apprentissage de réseaux de neurones profonds, qui obtiennent des résultats à l'état de l'art dans de nombreux domaines. En particulier, nous nous concentrons sur la compréhension des intéractions entre les objets ou personnes vivibles dans des images de la vie quotidienne, nommées relations visuelles. Dans un deuxième temps, nous intégrons des connaissances sémantiques à ces réseaux pendant l'apprentissage. Ces connaissances permettent d'obtenir des annotations qui correspondent davantage aux relations visibles. En caractérisant la proximité sémantique entre relations, le modèle apprend ainsi à détecter une relation peu fréquente à partir d'exemples de relations plus largement annotées. Enfin, après avoir montré que ces améliorations ne sont pas suffisantes si le modèle annote les relations sans en distinguer la pertinence, nous combinons des connaissances aux prédictions du réseau de façon à prioriser les relations les plus pertinentes
Le Transport Optimal est une théorie permettant de définir des notions géométriques de distance entre des distributions de probabilité et de trouver des correspondances, des relations, entre des ensembles de points. De cette théorie, à la frontière entre les mathématiques et l'optimisation, découle de nombreuses applications en machine learning. Comment l'adapter lorsque les données sont variées et ne font pas partie d'un même espace métrique ? Cette thèse propose un ensemble d'outils de Transport Optimal pour ces différents cas. Plus largement, nous analysons les propriétés mathématiques des différents outils proposés, nous établissons des solutions algorithmiques pour les calculer et nous étudions leur applicabilité dans de nombreux scenarii de machine learning qui couvrent, notamment, la classification, la simplification, le partitionnement de données structurées, ainsi que l'adaptation de domaines hétérogènes.
Les formations à distance en ligne, en particulier les MOOC, voient leurs effectifs augmenter depuis la démocratisation d'Internet. Malgré leur popularité croissante ces cours manquent encore d'outils permettant aux instructeurs et aux chercheurs de guider et d'analyser finement les apprentissages qui s'y passent. Des tableaux de bord récapitulant l'activité des étudiants sont régulièrement proposés aux instructeurs, mais ils ne leur permettent pas d'appréhender les activités collectives, or du point vue socio-constructiviste, les échanges et les interactions que les instructeurs cherchent généralement dans les forums sont essentiels pour les apprentissages (Stephens, 2014). Jusqu'à présent, les études ont analysé les interactions soit sémantiquement mais à petite échelle, soit statistiquement et à grande échelle mais en ignorant la qualité des interactions. La proposition de cette thèse est une nouvelle approche de détection interactive des activités collectives qui prend en compte à la fois leurs dimensions temporelles, sémantiques et sociales. Nous cherchons un moyen de permettre aux instructeurs d'intervenir et d'encourager les dynamiques collectives qui sont favorables pour les apprentissages. Ce que nous entendons par "dynamique collective", c'est l'évolution des interactions à la fois qualitatives et quantitatives, des apprenants dans des forums. Mais, à la différence des études précédentes, notre approche ne se limite pas à une analyse globale ou centrée sur un individu.
Cette démarche a un gros inconvénient : elle nécessite un investissement lourd qui s'inscrit sur le long terme. Pour palier à ce problème, il est nécessaire de mettre au point des méthodes et des outils informatiques d'aide à la construction de composants linguistiques fins et directement applicables à des textes. Nous nous sommes penché sur le problème des grammaires locales qui décrivent des contraintes précises et locales sous la forme de graphes. Deux questions fondamentales se posent : Comment construire efficacement des grammaires précises, complètes et applicables à des textes ? Comment gérer leur nombre et leur éparpillement ? Comme solution au premier problème, nous avons proposé un ensemble de méthodes simples et empiriques. Nous avons exposé des processus d'analyse linguistique et de représentation à travers deux phénomènes : les expressions de mesure (un immeuble d'une hauteur de 20 mètres) et les adverbes de lieu contenant un nom propre locatif (à l'île de la Réunion), deux points critiques du TAL. Sur la base de M. Gross (1975), nous avons ramené chaque phénomène à une phrase élémentaire. Ceci nous a permis de classer sémantiquement certains phénomènes au moyen de critères formels. Nous avons systématiquement étudié le comportement de ces phrases selon les valeurs lexicales de ses éléments. Au cours de notre travail, nous avons été confronté à des systèmes relationnels de tables syntaxiques pour lesquels la méthode standard de conversion due à E. Roche (1993) ne fonctionnait plus. Nous avons donc élaboré une nouvelle méthode adaptée avec des formalismes et des algorithmes permettant de gérer le cas où les informations sur les graphes à construire se trouvent dans plusieurs tables. En ce qui concerne le deuxième problème, nous avons proposé et implanté un prototype de système de gestion de grammaires locales : une bibliothèque en-ligne de graphes. Le but à terme est de centraliser et de diffuser les grammaires locales construites au sein du réseau RELEX. Nous avons conçu un ensemble d'outils permettant à la fois de stocker de nouveaux graphes et de rechercher des graphes suivant différents critères.
Ce mémoire de thèse traite de la modélisation du discours dans le cadre grammatical des Grammaires Catégorielles Abstraites (Abstract Categorial Grammars, ACGs). Les ACGs offrent un cadre unifié pour la modélisation de la syntaxe et de la sémantique. Nous nous intéressons en particulier aux formalismes discursifs qui utilisent une approche grammaticale pour rendre compte des régularités des structures discursives. Nous proposons en particulier un encodage à l'aide des ACGs de deux formalismes discursifs : G-TAG et D En effet, pour prendre en compte ces connecteurs, G-TAG et D-STAG utilisent une étape extra-grammaticale. Notre encodage offre au contraire une approche purement grammaticale de la prise en compte de ces connecteurs discursifs. Ces encodages se font à l'aide d'ACGs de second ordre. Les grammaires de cette classe ont des propriétés de réversibilité qui nous permettent d'utiliser les mêmes algorithmes polynômiaux aussi bien pour l'analyse discursive que pour la génération de discours.
Un problème central contribuant à la grande difficulté du traitement du langage naturel par des méthodes statistiques est celui de la parcimonie des données, à savoir le fait que dans un corpus d'apprentissage donné, la plupart des évènements linguistiques n'ont qu'un nombre d'occurrences assez faible, et que par ailleurs un nombre infini d'évènements permis par une langue n'apparaitront nulle part dans le corpus. Les modèles neuronaux ont déjà contribué à partiellement résoudre le problème de la parcimonie en inférant des représentations continues de mots. Ces représentations continues permettent de structurer le lexique en induisant une notion de similarité sémantique ou syntaxique entre les mots. Toutefois, les modèles neuronaux actuellement les plus répandus n'offrent qu'une solution partielle au problème de la parcimonie, notamment par le fait que ceux-ci nécessitent une représentation distribuée pour chaque mot du vocabulaire, mais sont incapables d'attribuer une représentation à des mots hors vocabulaire. Ce problème est particulièrement marqué dans des langues morphologiquement riches, ou des processus de formation de mots complexes mènent à une prolifération des formes de mots possibles, et à une faible coïncidence entre le lexique observé lors de l'entrainement d'un modèle, et le lexique observé lors de son déploiement. Aujourd'hui, l'anglais n'est plus la langue majoritairement utilisée sur le Web, et concevoir des systèmes de traduction automatique pouvant appréhender des langues dont la morphologie est très éloignée des langues ouest-européennes est un enjeu important. L'objectif de cette thèse est de développer de nouveaux modèles capables d'inférer de manière non-supervisée les processus de formation de mots sous-jacents au lexique observé, afin de pouvoir de pouvoir produire des analyses morphologiques de nouvelles formes de mots non observées lors de l'entraînement.
La plupart des méthodes statistiques ne sont pas nativement conçues pour fonctionner sur des données incomplètes. L'étude des données incomplètes n'est pas nouvelle et de nombreux résultats ont été établis pour pallier l'incomplétude en amont de l'étude statistique. D'autre part, les méthodes de deep learning sont en général appliquées à des données non structurées de type image, texte ou audio, mais peu de travaux s'intéressent au développement de ce type d'approche sur des données tabulaires, et encore moins sur des données incomplètes. Cette thèse se concentre sur l'utilisation d'algorithmes de machine learning appliqués à des données tabulaires, en présence d'incomplétude et dans un cadre assurantiel. Au travers des contributions regroupées dans ce document, nous proposons différentes façons de modéliser des phénomènes complexes en présence de schémas d'incomplétude. Nous montrons que les approches proposées donnent des résultats de meilleure qualité que l'état de l'art
La factorisation des tenseurs est au coeur des méthodes d'analyse des données massives multidimensionnelles dans de nombreux domaines, dont les systèmes de recommandation, les graphes, les données médicales, le traitement du signal, la chimiométrie, et bien d'autres. Pour toutes ces applications, l'obtention rapide de la décomposition des tenseurs est cruciale pour pouvoir traiter manipuler efficacement les énormes volumes de données en jeu. L'objectif principal de cette thèse est la conception d'algorithmes pour la décomposition de tenseurs multidimensionnels creux, possédant de plusieurs centaines de millions à quelques milliards de coefficients non-nuls. De tels tenseurs sont omniprésents dans les applications citées plus haut. Nous poursuivons cet objectif via trois approches. En premier lieu, nous proposons des algorithmes parallèles à mémoire distribuée, comprenant des schémas de communication point-à-point optimisés, afin de réduire les coûts de communication. Ces algorithmes sont indépendants du partitionnement des éléments du tenseur et des matrices de faible rang. Cette propriété nous permet de proposer des stratégies de partitionnement visant à minimiser le coût de communication tout en préservant l'équilibrage de charge entre les ressources. Nous utilisons des techniques d'hypergraphes pour analyser les paramètres de calcul et de communication de ces algorithmes, ainsi que des outils de partitionnement d'hypergraphe pour déterminer des partitions à même d'offrir un meilleur passage à l'échelle. Deuxièmement, nous étudions la parallélisation sur plate-forme à mémoire partagée de ces algorithmes. Dans ce contexte, nous déterminons soigneusement les tâches de calcul et leur dépendances, et nous les exprimons en termes d'une structure de données idoine, et dont la manipulation permet de révéler le parallélisme intrinsèque du problème. Troisièmement, nous présentons un schéma de calcul en forme d'arbre binaire pour représenter les noyaux de calcul les plus coûteux des algorithmes, comme la multiplication du tenseur par un ensemble de vecteurs ou de matrices donnés. L'arbre binaire permet de factoriser certains résultats intermédiaires, et de les ré-utiliser au fil du calcul. Grâce à ce schéma, nous montrons comment réduire significativement le nombre et le coût des multiplications tenseur-vecteur et tenseur-matrice, rendant ainsi la décomposition du tenseur plus rapide à la fois pour la version séquentielle et la version parallèle des algorithmes. Enfin, le reste de la thèse décrit deux extensions sur des thèmes similaires. La première extension consiste à appliquer le schéma d'arbre binaire à la décomposition des tenseurs denses, avec une analyse précise de la complexité du problème et des méthodes pour trouver la structure arborescente qui minimise le coût total. La seconde extension consiste à adapter les techniques de partitionnement utilisées pour la décomposition des tenseurs creux à la factorisation des matrices non-négatives, problème largement étudié et pour lequel nous obtenons des algorithmes parallèles plus efficaces que les meilleurs actuellement connus. Tous les résultats théoriques de cette thèse sont accompagnés d'implémentations parallèles,aussi bien en mémoire partagée que distribuée. Tous les algorithmes proposés, avec leur réalisation sur plate-forme HPC, contribuent ainsi à faire de la décomposition de tenseurs un outil prometteur pour le traitement des masses de données actuelles et à venir.
Cette thèse s'intéresse aux fonctions des marqueurs discursifs 'you know', 'then' et 'ya'ni' (I mean) en tant que structures autonomes du point de vue syntaxique et en tant qu'unités linguistiques opérationnelles et multifonctionnelles du point de vue conversationnel. Dans une perspective pragmatique, ce travail explore la corrélation entre la position et la fonction en maintenant que la valeur d'un marqueur en position initiale est différente de celle exprimée en position médiane ou en position finale. Dans le contexte des interactions verbales politiques au sein des émissions télévisées en arabe et en anglais, le but de cette thèse est de contribuer aux analyses conversationnelles pragmatiques. À travers une recherche empirique, l'analyse montre que la multifonctionnalité de ces items linguistiques est liée à leur flexibilité syntaxique. Ces marqueurs discursifs assument diverses fonctions contextuelles qui évoluent continuellement sur une échelle pragmatique. L'évolution pragmatique de 'you know', 'then' et 'ya'ni' engendre des expressions complexes au niveau de leur sémantisme. Ces unités pragmatiques deviennent plus ambiguës car elles s'éloignent davantage de leur sens de base en acquérant des significations contextuelles. Ainsi, ces marqueurs évoquent d'autres interprétations et ne peuvent se limiter qu'à leur sens d'origine. En l'occurrence, on leur attribue des équivalents les plus proches selon le contexte de leur occurrence. Les résultats révèlent que 'ya'ni' peut être substitué par d'autres marqueurs de différentes catégories grammaticales en anglais. La variation pragmatique suit le but illocutoire du locuteur, la prise en compte d'autrui et l'organisation de l'interaction verbale. Dans deux situations socioculturelles différentes, l'anglais et l'arabe confirment que ces items linguistiques sont des unités conversationnelles contextuelles et multifonctionnelles. Leur rôle est visiblement actif dans une situation sociale où l'interaction entre le locuteur et l'interlocuteur est un facteur saillant  ; 
Notre recherche doctorale porte sur l'influence des pratiques d'enseignement du code alphabétique sur les progrès des élèves de cours préparatoire. Elle a pour objectif d'identifier des pratiques pédagogiques efficaces et de contribuer à la réflexion sur la formation professionnelle des enseignants. Elle constitue l'un des volets d'une enquête collective de grande ampleur dirigée par Roland Goigoux qui visait à évaluer l'influence des pratiques d'enseignement de la lecture et de l'écriture sur la qualité des apprentissages. La première partie de notre recherche est consacrée à la mise en évidence de relations causales entre les pratiques d'enseignement du code alphabétique et les performances des élèves en décodage et en orthographe. Nous nous intéressons tout d'abord à la question de la planification de l'enseignement, plus précisément à la vitesse d'étude des correspondances entre les graphèmes et les phonèmes (tempo) et à la part déchiffrable des textes utilisés comme supports d'enseignement de la lecture (rendement effectif). Nos résultats soulignent l'influence significative de ces deux variables sur la qualité des apprentissages, cette influence s'exerçant de manière différenciée selon le niveau des élèves à l'entrée du cours préparatoire. En outre, nous proposons une progression de l'étude du code alphabétique fondée sur la fréquence théorique des correspondances graphèmes-phonèmes des textes écrits en français standard pouvant servir de référence aux enseignants. Nous étudions également les effets du temps d'enseignement de l'encodage sur les acquisitions scolaires, effets qui se révèlent significatifs et positifs mais qui varient selon la nature des tâches proposées et les publics ciblés. Dans la seconde partie de notre thèse, nous nous attachons à comprendre et à documenter la conduite de l'activité de maitres expérimentés de cours préparatoire à des fins de formation professionnelle. Nous analysons une situation de référence de l'enseignement du lire-écrire à partir des enregistrements vidéo de trente-six séances de lecture collectives. Puis, nous décrivons des scénarios pédagogiques prototypiques et nous posons les bases d'une formation destinée à développer les compétences professionnelles des enseignants. Nous soulevons notamment la problématique de l'articulation de la résolution de tâches de code et de compréhension et celle de l'autonomie de déchiffrage offerte aux élèves. Nous présentons enfin la plateforme numérique que nous avons élaborée et qui permet de déterminer la part déchiffrable des textes utilisés lors des séances de lecture collectives. Cette plateforme nommée Anagraph aide les enseignants à planifier l'étude des correspondances graphophonémiques et à choisir des textes adaptés à l'enseignement de la lecture
De nombreuses applications qui font maintenant partie de notre vie telles que des assistants personnels, la traduction automatique, ou encore la reconnaissance vocale, reposent sur ces techniques. L'apprentissage par imitation propose de réaliser les procédures d'apprentissage et d'inférence de manière approchée afin de pouvoir exploiter pleinement des structures de dépendance plus riches. Cette thèse explore ce cadre d'apprentissage, en particulier l'algorithme SEARN, à la fois sur le plan théorique ainsi que ses possibilités d'application aux tâches de traitement automatique des langues, notamment aux plus complexes telles que la traduction. Concernant les aspects théoriques, nous présentons un cadre unifié pour les différentes familles d'apprentissage par imitation, qui permet de redériver de manière simple les propriétés de convergence de ces algorithmes ;  concernant les aspects plus appliqués, nous utilisons l'apprentissage par imitation d'une part pour explorer l'étiquetage de séquences en ordre libre ; d'autre part pour étudier des stratégies de décodage en deux étapes pour la traduction automatique.
Ces dernières peuvent, en particulier, prendre une autre dimension dans le contexte de création d'une écriture. Elles offrent ainsi des circonstances propices à cette recherche. L'objectif est de préserver une signification profonde pour le locuteur/scripteur. Pour répondre à cette question, nous proposons de réinvestir une gestuelle porteuse de sens dans le cadre de la conception d'un environnement technique favorisant la création scripturale. Dans un premier temps, cette étude pluridisciplinaire explore En d'autres termes, quels éléments de l'oral peuvent être rapportés à l'écriture. Dans un second temps, elle envisage l'instrument permettant ce transfert. Pour cela, nous employons une démarche phénoménologique entendue comme méthodologie descriptive du point de vue en première personne. Cette méthode s'appuie sur des techniques de verbalisation de l'expérience vécue lors d'entretiens. La construction de la méthode adaptée à la LS française permet d'accéder à des descriptions fines des locuteurs sourds sur leur gestuelle. Le corpus de données est ensuite mis en dialogue avec une analyse en troisième personne établie à l'aide d'études linguistiques et kinésiologiques. Les résultats sur les dimensions du geste sémiotique nous amènent à penser les conditions d'une expérience habilitée, dans une perspective d'appropriation et de création scripturale des LS. Nous suivons pour cela la démarche de conception du design d'expérience utilisateur, du design d'énaction et de l'approche instrumentale pour l'immersion et l'interaction. La conception d'un tel dispositif vient non seulement changer le regard des personnes sourdes sur leur langue, mais également de manière plus générale, changer la relation que tout utilisateur a avec sa production gestuelle.
Les algorithmes de Frank-Wolfe sont des méthodes d'optimisation de problèmes sous contraintes. Elles décomposent un problème non-linéaire en une série de problèmes linéaires. Cela en fait des méthodes de choix pour l'optimisation en grande dimension et notamment explique leur utilisation dans de nombreux domaines appliqués. Ici nous proposons de nouveaux algorithmes de Frank-Wolfe qui convergent plus rapidement vers la solution du problème d'optimisation sous certaines hypothèses structurelles assez génériques. Nous montrons en particulier, que contrairement à d'autres types d'algorithmes, cette famille s'adapte à ces hypothèses sans avoir à spécifier les paramètres qui les contrôlent.
Cette thèse se compose de deux parties indépendantes et la première regroupant deux problématiques distinctes. Dans la première partie, nous étudions d'abord le problème de Principal-Agent dans des systèmes dégénérés, qui apparaissent naturellement dans des environnements à l'observation partielle où l'Agent et le Principal n'observent qu'une partie du système. Nous présentons une approche se basant sur le principe du maximum stochastique, dont le but est d'étendre les travaux existants qui utilisent le principe de la programmation dynamique dans des systèmes non-dégénérés. Ensuite nous utilisons la condition suffisante du problème de l'Agent pour vérifier que le contrat optimal obtenu est bien implémentable. Une étude parallèle est consacrée à l'existence et l'unicité de la solution d'EDSPRs dépendantes de la trajectoire dans le chapitre IV. Enfin, nous étudions le problème de hasard moral avec plusieurs Principals. L'Agent ne peut travailler que pour un seul Principal à la fois et fait donc face à un problème de switching optimal. En utilisant la méthode de randomisation nous montrons que la fonction valeur de l'Agent et son effort optimal sont donnés par un processus d'Itô. Cette représentation nous aide à résoudre ensuite le problème du Principal lorsqu'il y a une infinité de Principals en équilibre selon un jeu à champ-moyen. Nous justifions la formulation à champ-moyen par un argument de propagation de chaos. La deuxième partie de cette thèse est constituée des chapitres V et VI. La motivation de ces travaux est de donner un fondement théorique rigoureux pour la convergence des algorithmes du type descente de gradient très souvent utilisés dans la résolution des problème non-convexes comme la calibration d'un réseau de neurones. Nous montrons que la fonction d'énergie correspondante admet un unique minimiseur qui peut être caractérisé par une condition du premier ordre utilisant la dérivation dans l'espace des mesures au sens de Lions. Nous présentons ensuite une analyse du comportement à long terme de la dynamique de Langevin à champ-moyen, qui possède une structure de flot de gradient dans la métrique de 2-Wasserstein. Nous montrons que le flot de la loi marginale induite par la dynamique de Langevin à champ-moyen converge vers une loi stationnaire en utilisant le principe d'invariance de La Salle, qui est le minimiseur de la fonction d'énergie. Dans le cas des réseaux de neurones profonds, nous les modélisons à l'aide d'un problème de contrôle optimal en temps continu. Nous donnons d'abord la conditiondu premier ordre à l'aide du principe de Pontryagin, qui nous aidera ensuiteà introduire le système d'équation de Langevin à champ-moyen, dont la mesure invariante correspond au minimiseur du problème de contrôle optimal. Enfin, avec la méthode de couplage par réflexion nous montrons que la loi marginale du système de Langevin à champ-moyen converge vers la mesure invariante avec une vitesse exponentielle.
Le langage non-littéral (expressions idiomatiques, métaphores, métonymies, etc.) se révèle être très présent dans nos conversations de la vie de tous les jours. En revanche, très peu de didacticiens se sont interrogés sur les capacités des apprenants à produire du non-littéral dans une langue étrangère à l'exception de Littlemore et al. Dans le but d'avoir une première idée de la façon dont ces deux types de sujets manient le non-littéral, j'analyse tout d'abord le discours d'une petite fille de nationalité anglaise filmée à intervalles réguliers entre l'âge d'un an et quatre ans, puis j'examine les productions écrites en anglais d'apprenants francophones. J'observe ensuite les productions non-littérales d'enfants natifs anglophones âgés de 7, 11 et 15 ans, d'apprenants francophones en classe de seconde, première année de licence d'anglais et deuxième année de master d'anglais, et enfin, d'un groupe contrôle d' Ces formes proviennent principalement d'une carence en ressources lexicales de la langue étrangère et d'expressions figuratives du français que les apprenants ont souhaité transposer en anglais. Cette thèse propose un ensemble d'implications pédagogiques pour la classe de langue dans le but de remédier à ces difficultés.
Les systèmes embarqués temps réel sont de plus en plus omniprésents dans la vie quotidienne. Le cycle de développement des systèmes embarqués temps réel critiques peut prendre des mois voire des années. Par conséquent, la modélisation de ces systèmes doit être analysée à un stade précoce du cycle de développement afin de vérifier si toutes les exigences sont satisfaites, y compris les exigences relatives à la performance temporelle (par exemple, les latences, les délais de bout en bout, etc.). Cette thèse, qui a été financée dans le cadre d'un projet FUI, propose trois contributions majeures. La première contribution porte sur le système de tâches monoprocesseur avec des relations de communication multi-périodique déterministe. Un pattern basé sur les relations de précédence avec sémaphore (SPC) a été étendu afin de supporter les cycles dans le cas d'ordonnancement à priorités dynamiques. Un graphe de dépliage a été également proposé afin de présenter la cyclicité des systèmes à base de SPC et garantir le non-blocage. Le pattern SPC étendu ainsi que le test d'ordonnançabilité correspondant ont été implémentés pour le langage standard AADL.La deuxième contribution de cette thèse consiste en la proposition d'un calcul exact de temps de réponse de bout en bout, en utilisant le formalisme de réseau de Petri temporel, pour les systèmes multiprocesseurs identiques. Il prend en compte la dépendance entre les tâches : la précédence et l'exclusion mutuelle sans protocole de gestion. La troisième contribution porte sur la capitalisation des efforts du processus d'analyse. En effet, de nombreux tests d'analyse ont été proposés, notamment par des chercheurs académiques, basés sur la théorie d'ordonnancement et dédiés aux différentes architectures logicielles et matérielles. Cependant, l'une des principales difficultés rencontrées par les concepteurs est de choisir le test d'analyse le plus approprié permettant de valider et/ou de dimensionner correctement leurs conceptions. Cette difficulté se concrétise, dans le milieu industriel, par le peu de tests d'analyse utilisés malgré la multitude de tests proposés. Cette thèse vise donc à faciliter le processus d'analyse, réduire le fossé sémantique entre le modèle métier et les entrées des tests d'analyse ainsi qu'accélérer le transfert technologique et l'adoption des tests académiques. La thèse propose un référentiel d'analyse jouant le rôle d'un dictionnaire de tests, leurs contextes pour une utilisation correcte, les outils les implémentant, ainsi qu'un mécanisme pour le choix des tests selon le modèle métier d'entrée.
Dans un processus de conception centré sur l'utilisateur, les artefacts évoluent par cycles itératifs jusqu'à ce qu'ils répondent aux exigences des utilisateurs et deviennent ensuite le produit final. Chaque cycle donne l'occasion de réviser la conception et d'introduire de nouvelles exigences qui pourraient affecter les artefacts qui ont été définis dans les phases de développement précédentes. Garder la cohérence des exigences dans tels artefacts tout au long du processus de développement est une activité lourde et longue, surtout si elle est faite manuellement. Actuellement, certains cadres d'applications implémentent le BDD (Développement dirigé par le comportement) et les récits utilisateur comme un moyen d'automatiser le test des systèmes interactifs en construction. Les tests automatisés permettent de simuler les actions de l'utilisateur sur l'interface et, par conséquent, de vérifier si le système se comporte correctement et conformément aux exigences de l'utilisateur. Cependant, les outils actuels supportant BDD requièrent que les tests soient écrits en utilisant des événements de bas niveau et des composants qui n'existent que lorsque le système est déjà implémenté. En conséquence d'un tel bas niveau d'abstraction, les tests BDD peuvent difficilement être réutilisés avec des artefacts plus abstraits. Afin d'éviter que les tests doivent être écrits sur chaque type d'artefact, nous avons étudié l'utilisation des ontologies pour spécifier à la fois les exigences et les tests, puis exécuter des tests dans tous les artefacts partageant les concepts ontologiques. L'ontologie fondée sur le comportement que nous proposons ici vise alors à élever le niveau d'abstraction tout en supportant l'automatisation de tests dans des multiples artefacts. Cette thèse présente tel ontologie et une approche fondée sur BDD et les récits utilisateur pour soutenir la spécification et l'évaluation automatisée des exigences des utilisateurs dans les artefacts logiciels tout au long du processus de développement des systèmes interactifs. Deux études de cas sont également présentées pour valider notre approche. La première étude de cas évalue la compréhensibilité des spécifications des récits utilisateur par une équipe de propriétaires de produit (POs) du département en charge des voyages d'affaires dans notre institut. À l'aide de cette première étude de cas, nous avons conçu une deuxième étude pour démontrer comment les récits utilisateur rédigés à l'aide de notre ontologie peuvent être utilisées pour évaluer les exigences fonctionnelles exprimées dans des différents artefacts, tels que les modèles de tâche, les prototypes d'interface utilisateur et les interfaces utilisateur à part entière. Les résultats ont montré que notre approche est capable d'identifier même des incohérences à grain fin dans les artefacts mentionnés, permettant d'établir une compatibilité fiable entre les différents artefacts de conception de l'interface utilisateur.
Dans l'essor d'internet, les contenus générés par les utilisateurs à partir des services de réseaux sociaux deviennent une source géante d'informations qui peuvent être utile aux entreprises sur l'aspect où les utilisateurs sont considérés comme des clients ou des clients potentiels pour les entreprises. L'exploitation des textes générés par les utilisateurs peut aider à identifier leurs sentiments, leurs intentions, ou réduire l'effort des agents qui sont responsables de recueillir ou de recevoir des informations sur les services de réseaux sociaux. Dans le cadre de cette thèse, les contenues de textes tels que discours, énoncés, conversations issues de la communication interactive sur les plateformes de réseaux sociaux deviennent l'objet de données principales de notre étude. Nous proposons une méthode pour l'extraction d'un arbre de GCC à partir de l'arbre dépendante de la phrase, et une architecture générale pour construire un pont de relation entre les syntaxes et les sémantiques des phrases françaises.
Ma dissertation explore comment l'intégration de petites visualisations contextuelles basées sur des données peut complémenter des documents écrits. Plus spécifiquement, j'identifie et je définis des aspects importants et des directions de recherches pertinentes pour l'intégration de petites visualisations contextuelles basées sur des données textuelles. Cette intégration devra finalement devenir aussi fluide qu'écrire et aussi utile que lire un texte. Je définis les visualisations-mots (Word-Scale Visualizations) comme étant de petites visualisations contextuelles basées sur des données intégrées au texte de documents. Ces visualisations peuvent utiliser de multiples codages visuels incluant les cartes géographiques, les heatmaps, les graphes circulaires, et des visualisations plus complexes. Les visualisations-mots offrent une grande variété de dimensions toujours proches de l'échelle d'un mot, parfois plus grandes, mais toujours plus petites qu'une phrase ou un paragraphe. Les visualisations-mots peuvent venir en aide et être utilisées dans plusieurs formes de discours écrits tels les manuels, les notes, les billets de blogs, les rapports, les histoires, ou même les poèmes. En tant que complément visuel de textes, les visualisations-mots peuvent être utilisées pour accentuer certains éléments d'un document (comme un mot ou une phrase), ou pour apporter de l'information additionnelle. Par exemple, un petit diagramme de l'évolution du cours de l'action d'une entreprise peut être intégré à côté du nom de celle-ci pour apporter de l'information additionnelle sur la tendance passée du cours de l'action. Dans un autre exemple, des statistiques de jeux peuvent être incluses à côté du nom d'équipe de football ou de joueur dans les articles concernant le championnat d'Europe de football. Ces visualisations-mots peuvent notamment aider le lecteur à faire des comparaisons entre le nombre de passes des équipes et des joueurs. Le bénéfice majeur des visualisations-mots réside dans le fait que le lecteur peut rester concentré sur le texte, vu que les visualisations sont dans le texte et non à côté. J'étudie différentes options de placement pour les visualisations-mots et je quantifie leurs effets sur la disposition du texte et sa mise en forme. J'examine aussi comment combiner les visualisations-mots et l'interaction pour soutenir une lecture plus active en proposant des méthodes de collection, d'arrangement et de comparaison de visualisations-mots. Finalement, je propose des considérations de design pour la conception et la création de visualisations-mots et je conclus avec des exemples d'application. En résumé cette dissertation contribue à la compréhension de petites visualisations contextuelles basées sur des données intégrées dans le texte et à leur utilité pour la visualisation d'informations.
Analyse de données géométriques, au delà des convolutionsPour modéliser des interactions entre points, une méthode simple est de se reposer sur des sommes pondérées communément appelées "convolutions". Au cours de la dernière décennie, cette opération est devenue la brique de construction essentielle à la révolution du "deep learning". Le produit de convolution est, toutefois, loin d'être l'alpha et l'oméga des mathématiques appliquées. Pour permettre aux chercheurs d'explorer de nouvelles directions, nous présentons des implémentations robustes et efficaces de trois opérations souvent sous Le transport optimal, qui généralise la notion de "tri" aux espaces de dimension D &gt ; 1.3. Le tir géodésique sur une variété Riemannienne, qui se substitue à l'interpolation linéaire sur des espaces de données où aucune structure vectorielle ne peut être correctement définie. Nos routines PyTorch/NumPy sont compatibles avec la différentiation automatique, et s'exécutent en quelques secondes sur des nuages de plusieurs millions de points. Elle sont de 10 à 1,000 fois plus performantes que des implémentations GPU standards et conservent une empreinte mémoire linéaire. Ces nouveaux outils sont empaquetés dans les bibliothèques "KeOps" et "GeomLoss", avec des applications qui vont de l'apprentissage automatique à l'imagerie médicale. Notre documentation est accessible aux adresses www.kernel-operations.io/keops et /geomloss.
Alors que nous nous représentons le monde au travers de nos sens, de notre langage et de nos interactions, chacun de ces domaines a été historiquement étudié de manière indépendante en apprentissage automatique. Heureusement, ce cloisonnement tend à se défaire grâce aux dernières avancées en apprentissage profond, ce qui a conduit à l'uniformisation de l'extraction des données au travers des communautés. Cependant, les architectures neuronales multimodales n'en sont qu'à leurs premiers balbutiements et l'apprentissage par renforcement profond est encore souvent restreint à des environnements limités. Idéalement, nous aimerions pourtant développer des modèles multimodaux et interactifs afin qu'ils puissent correctement appréhender la complexité du monde réel. Dans cet objectif, cette thèse s'attache à la compréhension du langage combiné à la vision pour trois raisons : (i) ce sont deux modalités longuement étudiées aux travers des différentes communautés scientifiques (ii) nous pouvons bénéficier des dernières avancées en apprentissage profond pour les modèles de langues et de vision (iii) l'interaction entre l'apprentissage du langage et notre perception a été validé en science cognitives. Ainsi, nous avons conçu le jeu GuessWhat?! (KéZaKo) afin d'évaluer la compréhension de langue combiné à la vision de nos modèles : deux joueurs doivent ainsi localiser un objet caché dans une image en posant une série de questions. Enfin, nous explorons comment l'apprentissage par renforcement permet l'apprentissage de la langue et cimente l'apprentissage des représentations multimodales sous-jacentes. Nous montrons qu'un tel apprentissage interactif conduit à des stratégies langagières valides mais donne lieu à de nouvelles problématiques de recherche.
Cette thèse définit un modèle sémantique, non probabiliste et prédictif, pour l'analyse décisionnelle de réseaux sociaux professionnels et institutionnels. Ce modèle, en parallèle à la sociophysique de Galam, intègre des méthodes de traitement sémantique du langage naturel et d'ingénierie des connaissances, des mesures de sociologie statistique et des lois électrodynamiques, appliquées à l'optimisation de la performance économique et du climat social. Il a été développé et expérimenté dans le cadre du projet Socioprise, financé par le Secrétariat d'´Etat à la prospective et au développement de l'économie numérique.
La sclérose en plaques (SEP) est la maladie neurologique évolutive la plus courante chez les jeunes adultes dans le monde et représente donc un problème de santé publique majeur avec environ 90 000 patients en France et plus de 500 000 personnes atteintes de SEP en Europe. Afin d'optimiser les traitements, il est essentiel de pouvoir mesurer et suivre les altérations cérébrales chez les patients atteints de SEP. En fait, la SEP est une maladie aux multiples facettes qui implique différents types d'altérations, telles que les dommages et la réparation de la myéline. Selon cette observation, la neuroimagerie multimodale est nécessaire pour caractériser pleinement la maladie. L'imagerie par résonance magnétique (IRM) est devenue un biomarqueur d'imagerie fondamental pour la sclérose en plaques en raison de sa haute sensibilité à révéler des anomalies tissulaires macroscopiques chez les patients atteints de SEP. L'IRM conventionnelle fournit un moyen direct de détecter les lésions de SEP et leurs changements, et joue un rôle dominant dans les critères diagnostiques de la SEP. De plus, l'imagerie par tomographie par émission de positons (TEP), une autre modalité d'imagerie, peut fournir des informations fonctionnelles et détecter les changements tissulaires cibles au niveau cellulaire et moléculaire en utilisant divers radiotraceurs. Cependant, en milieu clinique, toutes les modalités ne sont pas disponibles pour diverses raisons. Dans cette thèse, nous nous concentrons donc sur l'apprentissage et la prédiction des altérations cérébrales dérivées des modalités manquantes dans la SEP à partir de données de neuroimagerie multimodale.
Cette thèse se propose d'étudier de quelle façon la misogynie peut prendre pour cible des pratiques linguistiques pouvant être perçues comme féminines et prend l'exemple du stéréotype sexiste de la « Valley Girl » . Ce terme, popularisé dans les années 1980 par le titre éponyme de Frank Zappa, a d'abord fait référence à des adolescentes, prétendument futiles et décérébrées, appartenant à la classe moyenne californienne. Bien que ces dernières aient été ridiculisées dans la chanson, cette exposition médiatique a paradoxalement eu pour effet de lancer un effet mode dont l'une des manifestations était linguistique : le Valspeak. Ce dialecte comprend entre autres des marqueurs phonétiques (le California Vowel Shift), prosodiques (l'utilisation d'un contour intonatif montant), lexicaux (fer sure, gag me with a spoon) ou de discours (LIKE). Bien que certains de ces marqueurs n'aient pas été (uniquement) popularisés par les Valley Girls, ils peuvent néanmoins être perçus comme tels, et y avoir recours peut exposer un locuteur à une perception négative de sa personne. Ce travail cherche à interroger les liens pouvant exister entre la stigmatisation des marqueurs du Valspeak et la misogynie, phénomène que nous appelons la « misogynie linguistique » du Valspeak. Dans quelle mesure la potentielle stigmatisation de ces marqueurs peut-elle être due au genre féminin des locutrices prototypiques de ce dialecte ? Trois éléments de réponse à cette question sont proposés. Tout d'abord, une étude de perception dialectale quantitative portant sur trois marqueurs du Valspeak (le California Vowel Shift, le contour intonatif montant et LIKE) est menée auprès de locuteurs de l'anglais américain. Deuxièmement, des entretiens qualitatifs ont pour but d'évaluer quelles idéologies sont associées aux marqueurs linguistiques du Valspeak et à la persona Valley Girl. Enfin, l'analyse porte sur trois représentations télévisuelles humoristiques de personnages féminins dans Parks and Recreation, Les Griffin et Ew ! (un sketch dans The Tonight Show Starring Jimmy Fallon). Il est montré que des marqueurs du Valspeak sont utilisés afin d'orchestrer la stigmatisation de personnages féminins sans faire explicitement appel au stéréotype Valley Girl.
L'accroissement rapide des données numériques vidéographiques fait de la compréhension automatique des vidéos un enjeu de plus en plus important. Comprendre de manière automatique une vidéo recouvre de nombreuses applications, parmi lesquelles l'analyse du contenu vidéo sur le web, les véhicules autonomes,les interfaces homme-machine (eg, Kinect). Cette thèse présente des contributions dans deux problèmes majeurs pour la compréhension automatique des vidéos : la détection d'actions supervisée par des données web, et la localisation d'actions humaines. La détection d'actions supervisées par des données web a pour objectif d'apprendre à reconnaître des actions dans des contenus vidéos sur Internet, sans aucune autre supervision. Nous proposons une approche originale dans ce contexte, qui s'appuie sur la synergie entre les données visuelles (les vidéos) et leur description textuelle associée, et ce dans le but d'apprendre des classifieurs pour les événements sans aucune supervision. Nous montrons l'importance des deux étapes principales, c'est-à-dire la créations des requêtes et l'étape de suppression des vidéos, par des résutats quantitatifs. Notre approche est évaluée dans des conditions difficiles, où aucune annotation manuelle n'est disponible, dénotées EK0 dans les challenges TrecVid. Nous obtenons l'état de l'art sur les bases de données MED 2011 et 2013. Dans la seconde partie de notre thèse, nous nous concentrons sur la localisation des actions humaines, ce qui implique de reconnaître à la fois les actions se déroulant dans la vidéo, comme par exemple "boire" ou "téléphoner", et leur étendues spatio-temporelles. Nous proposons une nouvelle méthode centrée sur la personne, traquant celle-ci dans les vidéos pour en extraire des tubes encadrant le corps entier, même en cas d'occultations ou dissimulations partielles. Deux raisons motivent notre approche. La première est qu'elle permet de gérer les occultations et les changements de points de vue de la caméra durant l'étape de localisation des personnes, car celle-ci estime la position du corps entier à chaque frame. Notre algorithme de tracking connecte les détections temporellement pour extraire des tubes encadrant le corps entier. Nous évaluons notre nouvelle méthode d'extraction de tubes sur une base de données difficile, DALY, et atteignons l'état de l'art.
Cette étude présente les caractéristiques, les écritures et la structure de la langue ouïghoure en faisant une étude linguistique et en proposant de nouveaux modèles expérimentaux qui faciliteront le développement des outils informatiques et le traitement automatique de la langue afin de contribuer à l'informatisation de la langue ouïghoure. Plus précisément, notre étude consiste en quatre parties :  la première partie présente les problématiques d'étude, les caractéristiques de la langue et des écritures, notamment le processus d'unification de l'écriture ouïghoure-latine ;  la deuxième partie expose les notions de base d'extraction d'information et démontre la possibilité d'extraction d'entités nommées en utilisant un outil d'extraction, afin d'expérimenter les conceptions et les théories proposées ;  la troisième partie est consacré à l'étude linguistique notamment sur l'aspect agglutinant de la langue et les règles morphologiques de suffixation qui seront appliquées pendant la réalisation des outils prototypes proposés dans cette thèse ;  enfin la quatrième partie mettre en évidence les problématiques de traitement de la langue ouïghoure dans une situation où les systèmes d'exploitation ne supporte pas la langue ouïghoure. Dans cette partie, nous décrivons les difficultés existantes et nous proposerons des solutions innovantes afin de les résoudre dans les domaines suivants : Unification des polices ouïghoures et création d'une police ouïghoure basée sur l'Unicode, Implémentation des méthodes d'entrées au niveau système et au niveau navigateur, Création des convertisseurs multiécriture, Réalisation d'un dictionnaire ouïghour – anglais en ligne, Mise en place d'un générateur lexical basé sur les règles morphologiques de suffixation de l'ouïghour, Développement d'un analyseur et explorateur de suffixes, Démonstration d'extraction de l'information Implémentation d'un parseur et un correcteur d'orthographe
Les bénéfices engendrés par les études statistiques sur les données personnelles des individus sont nombreux, que ce soit dans le médical, l'énergie ou la gestion du trafic urbain pour n'en citer que quelques-uns. Pour retrouver la confiance des individus, il devient nécessaire de proposer dessolutions de user empowerment, c'est-à-dire permettre à chaque utilisateur de contrôler les paramètres de protection des données personnelles les concernant qui sont utilisées pour des calculs. Cette thèse développe donc un nouveau concept d'anonymisation personnalisé, basé sur la généralisation de données et sur le user empowerment. De plus, nous utilisons une architecture décentralisée basée sur du matériel sécurisé assurant ainsi les garanties de respect de la vie privée tout au long des opérations d'agrégation. En deuxième lieu, ce manuscrit étudie la personnalisations des garanties d'anonymat lors de la publication de jeux de données anonymisés. Nous proposons l'adaptation d'heuristiques existantes ainsi qu'une nouvelle approche basée sur la programmation par contraintes. Des expérimentations ont été menées pour étudier l'impact d'une telle personnalisation sur la qualité des données. Les contraintes d'anonymat ont été construites et simulées de façon réaliste en se basant sur des résultats d'études sociologiques.
La présente thèse propose une description et une étude multidimensionnelles (sociolinguistiques, phonologiques et phonétiques) de la variété d'anglais parlée dans le Greater Manchester. Nous offrons une discussion sur les enjeux méthodologiques et épistémologiques de l'étude du changement linguistique et de l'utilisation des corpus en linguistique. Notre travail est mené dans le cadre du programme PAC (Phonologie de l'Anglais Contemporain : usages, variétés et structure) et au sein du projet LVTI (Langue, Ville, Travail, Identité) sur la base du corpus PAC-LVTI Manchester, constitué de données authentiques et récentes récoltées sur le terrain. Notre analyse se concentre notamment sur le phénomène de nivellement dialectal, qui a été l'objet de nombreuses recherches récentes en sociolinguistique anglaise. Nous nous intéressons en particulier à l'hypothèse de l'expansion d'une variété supralocale dans le nord de l'Angleterre. Notre étude concerne essentiellement les voyelles du Greater Manchester, et repose sur une analyse phonético-acoustique de la production des locuteurs de notre corpus. Nous relevons les caractéristiques majeures de la variété mancunienne, telles qu'elles ont pu être décrites dans les quelques travaux publiés jusqu'ici, et étudions leur corrélation avec des facteurs sociolinguistiques classiques comme l'âge, le genre, ou le niveau socio-économique. Nous explorons également la pertinence des facteurs attitudinaux pour l'étude de nos données. Nous discutons des évolutions du système à la lumière du phénomène de nivellement dialectal, et nous interrogeons sur la pertinence des facteurs internes et externes pour les expliquer.
Cette thèse de doctorat est le résultat de mes travaux de recherche dans le domaine de l'apprentissage automatique, du traitement d'image et du transport intelligent pour résoudre le problème du système de protection des piétons (PPS) multi-tâches comprenant non seulement la classification, la détection et le suivi des piétons, mais aussi l'action des piétons- classification et prédiction des unités, et enfin estimation du risque piéton. De plus, notre système PPS utilise des approches originales d'apprentissage en profondeur inter-modalités. Le but de notre travail de recherche est de développer un composant de protection des piétons intelligent basé uniquement sur un système de vision stéréo unique utilisant une architecture d'apprentissage en profondeur cross-modalité optimale afin de classer l'action piétonne actuelle, de prédire leurs prochaines actions et enfin d'estimer le piéton risque au moment de traverser pour chaque piéton. Premièrement, nous étudions la composante de classification où nous avons analysé comment les représentations d'apprentissage d'une modalité permettraient de reconnaître d'autres modalités au sein de divers apprentissages profonds, un terme comme apprentissage multimodal. Troisièmement, nous analysons la prédiction de l'action des piétons et l'estimation du temps à traverser.
Afin de réduire l'intervalle de temps nécessaire entre la publication de l'information sur le Web et sa consultation par les utilisateurs, les sites Web reposent sur le principe de la Syndication Web. Les fournisseurs d'information diffusent les nouvelles informations à travers des flux RSS auxquels les utilisateurs intéressés peuvent s'abonner. Nous proposons un index basé sur les mots-clés des requêtes utilisateurs permettant de retrouver ceux-ci dans les items des flux. Trois structures d'indexation de souscriptions sont présentées. Un modèle analytique pour estimer le temps de traitement et l'espace mémoire de chaque structure est détaillé. Nous menons une étude expérimentale approfondie de l'impact de plusieurs paramètres sur ces structures. Pour les souscriptions jamais notifiées, nous adaptons les index étudiés pour prendre en considération leur satisfaction partielle.
L'émergence de la technologie des médias digitaux durant les dernières décennies a transformé la manière dont les organisations sont évaluées. Chaque jour, dans de multiples plateformes et sites web, des individus divulguent des informations sur leurs interactions avec des organisations. En comparaison des critiques professionnels traditionnels, les utilisateurs et les consommateurs digitaux tendent à partager des expériences subjectives et partiales, à être moins enclins à être pondérés, et souvent à donner plus d'importance au contenu émotionnel. Alors que davantage de consommateurs s'appuient sur cette information pour leurs choix d'achat, les entreprises dans beaucoup de secteurs se trouvent dans une position où il est difficile d'ignorer les opinions exprimées en ligne par les consommateur. Dans cette thèse, j'étudie la manière dont les stratégies et les comportements des organisations sont influencées par cette « démocratisation » des processus d'évaluation. Le contexte empirique de mes analyses est celui du secteur de la restauration haut-de-gamme. Dans le premier chapitre, j'étudie les commentaires en ligne comme source d'information pour les restaurants, qui peuvent avoir l'opportunité d'apprendre des problèmes et d'améliorations potentielles. J'examine quelles sont les caractéristiques des feedbacks des consommateurs qui ont le plus de chances d'être prises en considération par les restaurants ciblés. A partir d'une expérimentation en ligne dans le secteur de la restauration haut-de-gamme en France, je trouve que les preneurs de décision allouent leur attention aux feedbacks desquels on attend qu'ils aient le plus fort effet sur la réputation et la performance du restaurant. Dans le deuxième chapitre, j'analyse les effets de l'interaction entre les évaluations des amateurs et des experts. En particulier, j'étude l'entrée d'un évaluateur expert (i.e. le guide Michelin) sur le marché, et la manière dont il pousse certaines organisations à faire des choix stratégiques qui signalent leurs aspirations. En construisant sur la littérature sur le statut organisationnel, je trouve que certains restaurants mieux évalués par le guide Michelin font des changements dans leur offre en visant à s'auto-identifier avec le groupe d'élite. Ces changements consistent à adopter ou à exclure certaines caractéristiques affichées dans les menus. De plus, en utilisant les techniques du « topic modeling » appliquées à des commentaires sur Yelp, j'observe que certaines réactions des consommateurs à propos de l'entrée du guide Michelin font que les restaurants apparaissent plus ou moins sensibles aux évaluations de l'expert. Dans le troisième chapitre, je me concentre sur la manière dont les organisations utilisent des réponses publiques adressées aux consommateurs pour répondre aux critiques en ligne. Les études récentes n'offrent pas de conclusion nette sur les bénéfices en termes de réputation des réponses publics aux commentaires. Ces réponses peuvent réduire la probabilité de recevoir des commentaires négatifs dans le futur mais, dans le même temps, elles attirent l'attention sur les problèmes en question. M'appuyant sur la littérature existante sur la réputation et sur l' « impression management » , je propose que les organisations peuvent résoudre cet arbitrage en utilisant stratégiquement les différents types de réponses verbales (ex : l'excuse). Bien que les réponses publiques adressées aux consommateurs puissent être contre-productives, adapter le style des réponses publiques aux caractéristiques des commentaires des consommateurs peut être une stratégie optimale pour les organisations. Dans cette étude, j'analyse les commentaires pour des restaurants situées en France et aux États-Unis en utilisant des modèles économétriques standards appuyés sur des techniques de « supervised learning » .
Cette étude de cas relève de la didactique des langues, et plus particulièrement de l'intégration des outils numériques à la formation pré-professionnelle à la didactique de l'anglais. Elle a pour objectif de décrire et d'analyser les différents processus qui participent à l'émergence de la dimension sociale de l'autonomie. Elle repose sur l'idée que ces processus viennent au premier plan lors de la réalisation d'une activité collective, située et médiatisée. Afin d'étudier ces processus, une tâche-projet basée sur la réalisation de contes multimodaux a été proposée aux apprenant.e.s d'une formation pré-professionnelle au métier d'enseignement de la langue anglaise. Les supports conçus par les apprenant.e.s de première année de licence ont été ensuite testés comme ressources pédagogiques par des professeur.e.s d'écoles primaires dans le but de conférer une dimension plus réelle, plus située, à la production des contes. Une modélisation systémique de l'activité permet d'identifier trois axes qui contribuent à l'émergence de l'autonomie sociale. Le premier est relatif au fonctionnement intra groupal des groupes restreints ayant accompli la tâche-projet. Le deuxième porte sur la contribution de la communauté des professeur.e.s des écoles primaires qui ont testé les supports. Le troisième interroge le rapport aux outils numériques sociaux utilisés par les groupes restreints pour collaborer et échanger ensemble. Les résultats issus des analyses relatives aux trois axes visent à réaliser une modélisation de certaines variables participant à une émergence de l'autonomie sociale.
Les corpus bilingues sont des ressources essentielles pour s'affranchir de la barrière de la langue en traitement automatique des langues (TAL) dans un contexte multilingue. La plupart des travaux actuels utilisent des corpus parallèles qui sont surtout disponibles pour des langues majeurs et pour des domaines spécifiques. Les corpus comparables, qui rassemblent des textes comportant des informations corrélées, sont cependant moins coûteux à obtenir en grande quantité. Plusieurs travaux antérieurs ont montré que l'utilisation des corpus comparables est bénéfique à différentes taches en TAL. En parallèle à ces travaux, nous proposons dans cette thèse d'améliorer la qualité des corpus comparables dans le but d'améliorer les performances des applications qui les exploitent. L'idée est avantageuse puisqu'elle peut être utilisée avec n'importe quelle méthode existante reposant sur des corpus comparables. Nous discuterons en premier la notion de comparabilité inspirée des expériences d'utilisation des corpus bilingues. Cette notion motive plusieurs implémentations de la mesure de comparabilité dans un cadre probabiliste, ainsi qu'une méthodologie pour évaluer la capacité des mesures de comparabilité à capturer un haut niveau de comparabilité. Les mesures de comparabilité sont aussi examinées en termes de robustesse aux changements des entrées du dictionnaire. Les expériences montrent qu'une mesure symétrique s'appuyant sur l'entrelacement du vocabulaire peut être corrélée avec un haut niveau de comparabilité et est robuste aux changements des entrées du dictionnaire. En s'appuyant sur cette mesure de comparabilité, deux méthodes nommées : greedy approach et clustering approach, sont alors développées afin d'améliorer la qualité d'un corpus comparable donnée. L'idée générale de ces deux méthodes est de choisir une sous partie du corpus original qui soit de haute qualité, et d'enrichir la sous-partie de qualité moindre avec des ressources externes. Les expériences montrent que l'on peut améliorer avec ces deux méthodes la qualité en termes de score de comparabilité d'un corpus comparable donnée, avec la méthode clustering approach qui est plus efficace que la method greedy approach. Le corpus comparable ainsi obtenu, permet d'augmenter la qualité des lexiques bilingues en utilisant l'algorithme d'extraction standard. Enfin, nous nous penchons sur la tâche d'extraction d'information interlingue (Cross-Language Information Retrieval, CLIR) et l'application des corpus comparables à cette tâche. Nous développons de nouveaux modèles CLIR en étendant les récents modèles proposés en recherche d'information monolingue. Le modèle CLIR montre de meilleurs performances globales. Les lexiques bilingues extraits à partir des corpus comparables sont alors combinés avec le dictionnaire bilingue existant, est utilisé dans les expériences CLIR, ce qui induit une amélioration significative des systèmes CLIR.
Nous visons la constitution d'un cadre théorique et opérationnel général, permettant la modélisation et l'exploration computationnelle d'une variété significative de telles structures. Nous proposons notamment d'articuler leur analyse autour des trois catégories élémentaires que sont unités, relations et schémas, et envisageons différentes propriétés récurrentes des structures et des mécanismes indiciaires sous-jacents : variabilité du grain, flexibilité, non-linéarité et non-séquentialité potentielles, interactions local/global... Afin de procéder à la description formelle des phénomènes linguistiques étudiés et à l'opérationalisation de leur analyse sur corpus, nous proposons le formalisme CDML (Contraint-based Discourse Modeling Language), qui permet de modéliser des structures discursives par l'expression de contraintes sur des objets textuels de différentes natures (morphologique, syntaxique, sémantique... Un analyseur permet de projeter ces contraintes sur corpus pour identifier les structures décrites. La première porte sur l'hypothèse de l'encadrement du discours de M. Charolles, et la seconde explore les relations de contraste à différentes échelles, entre des objets linguistiques variés.
La présente thèse porte sur l'utilisation et l'interprétation des sujets nuls et pronominaux en portugais brésilien. Son objectif est de comprendre les facteurs sémantiques et discursifs qui peuvent être pertinents pour le choix entre ces expressions anaphoriques et la façon dont ce choix s'articule avec la théorie générale de la résolution de l'anaphore. Le point de départ a été la recherche sur les sujets nuls et réalisés sous la perspective de la grammaire générative, en particulier la théorie paramétrique. Cette thèse démontre que l'analyse proposée dans cette perspective ne peut rendre compte des données observées. Par exemple, la généralisation sur la « pauvreté » de la morphologie verbale directement liée à l'absence, ou à la fréquence réduite, de sujets nuls est contestée avec les données expérimentales ainsi qu'avec la distribution de la fréquence relative des sujets nuls au sein des personnes discursives dans le corpus. Une explication alternative présentée dans la littérature, à savoir l'importance des caractéristiques sémantiques des antécédents – l'Animacité et le Specificité – semble mieux expliquer la distribution constatée. Cette explication n'est cependant pas suffisante pour comprendre le choix des sujets anaphoriques en brésilien, puisque le nombre relatif de sujets nuls animés et spécifiques est relativement plus élevé que dans les langues à expression obligatoire des sujets. Par conséquent, cette thèse soutient que les facteurs discursifs semblent jouer un rôle crucial dans l'utilisation des sujets nuls et réalisés en brésilien. Le premier est un facteur standard dans la littérature sur la résolution de l'anaphore (exprimé par différents termes comme l'accessibilité, la familiarité, etc.), qui permet l'hypothèse d'une relation inverse entre le degré de saillance de l'antécédent et degré explicitation nécessaire dans l'expression anaphorique : plus l'antécédent est saillant, moins l'anaphore doit être explicite. Le second facteur, le contraste, constitue la principale contribution nouvelle de cette thèse : Comme pour d'autres niveaux d'analyse linguistique et d'autres phénomènes dans le langage, le choix de l'expression anaphorique semble être orienté vers l'efficacité. Plus précisément, lorsque l'information d'arrière-plan (background) et l'information assertée (focalisée) dans un énoncé contrastent, il est plus probable qu'un sujet nul soit utilisé. Les caractéristiques d'une grammaire permettant de traiter ces diverses caractéristiques est esquissée : on propose une grammaire à plusieurs niveaux dont les contraintes sémantiques et discursives agissent en parallèle à travers un principe de correspondance probabiliste. Il est ainsi démontré que les sujets nuls sont probables dans certains contextes de co-référence discursifs, puisque dans ces contextes, leurs antécédents sont plus évidents et contrastent plus avec l'information d'arrière-plan. Une contre-preuve apparente à la proposition esquissée ici est analysée : l'interprétation générique des sujets nuls. Cependant, on montre que les mêmes contraintes sémantiques appliquées à d'autres constructions génériques dans plusieurs langues peuvent produire des sujets nuls génériques en brésilien, étant donné l'échec de la mise en arrière-plan prédite par l'approche proposée ici. Enfin, les résultats de trois expériences de mouvements oculaires en lecture, qui étudient l'utilisation et l'interprétation des sujets nuls et pronominaux, sont présentés. Ces résultats corroborent de façon convaincante l'hypothèse selon laquelle les sujets nuls et réalisés ainsi que leur interprétation peuvent être expliqués par la théorie proposée ici, qui les traite en termes de contraintes d'interprétation plutôt qu'en termes de légitimation syntaxique.
Le figement est un obstacle au traitement automatique des langues naturelles. Ce travail est une contribution aux recherches en cours et vise à améliorer le traitement automatique des séquences verbales figées (SVF) comme casser sa pipe. Les notions de degrés de figement et de double structuration guideront notre étude. Pour déterminer le degré de figement des SVF, nous avons analysé leurs structures interne et externe (en tant que prédicats). Ceci permet alors de les inclure dans les classes de prédicats du français selon la théorie des classes d'objets de G. GROSS. Après une présentation des travaux portant sur le figement (verbal), nous identifierons et relèverons les critères de mesure des degrés de figement des SVF par l'étude des séquences à structure [V SN SP]. Puis, nous proposerons des outils formels de reconnaissance automatique de la métaphore. Enfin, nous décrirons deux classes syntactico-sémantiques de prédicats polylexicaux : celles de <déplacement> et d'<états humains>.
La thèse débute par un rappel des étapes historiques principales du développement de l'ethnométhodologie en tant que discipline, depuis les précurseurs européens des années 30 jusqu'à l'explosion aux Etats Unis puis en Europe à partir de 1967. La seconde partie de la thèse est consacrée à l'application des principes développés antérieurement au domaine des stratégies d'innovations technologiques mises en oeuvre en France en vue d'accroître le potentiel de recherche et développement dans le secteur du traitement automatique des langues naturelles. Trois études décrivent successivement les ethnométhodes et les propriétés rationnelles des actions pratiques mises en oeuvre par un groupe de chargés de mission de l'administration, les processus d'élaboration d'une politique d'innovation technologique, les descriptions indexicales du domaine des industries de la langue et de programmes de r et d dans ce secteur. La conclusion s'efforce de montrer comment la puissance des concepts de l'ethnométhodologie et des outils qui en découlent permettent d'accroître la pertinence des analyses stratégiques et l'efficacité des actions de recherche développement
L'apprentissage collaboratif assisté par ordinateur et les technologies d'e-learning devenant de plus en plus populaires et intégrés dans des contextes éducatifs, le besoin se fait sentir de disposer d'outils d'évaluation automatique et d'aide aux enseignants ou tuteurs pour les deux activités, fortement couplées, de compréhension de textes et collaboration entre pairs. Bien qu'une analyse de surface de ces activités est aisément réalisable, une compréhension plus profonde et complète du discours en jeu est nécessaire, complétée par une analyse de l'information méta-cognitive disponible par diverses sources, comme par exemples les auto-explications des apprenants. Plus spécifiquement, nous nous sommes centrés sur la dimension individuelle de l'apprentissage, analysée à partir de l'identification de stratégies de lecture et sur la mise au jour d'un modèle de la complexité textuelle intégrant des facteurs de surface, lexicaux, morphologiques, syntaxiques et sémantiques. En complément, la dimension collaborative de l'apprentissage est centrée sur l'évaluation de l'implication des participants, ainsi que sur l'évaluation de leur collaboration par deux modèles computationnels : un modèle polyphonique, défini comme l'inter-animation de voix selon de multiples perspectives, un modèle spécifique de construction sociale de connaissances, fondé sur le graphe de cohésion et un mécanisme d'évaluation des tours de parole. Notre approche met en œuvre des techniques avancées de traitement automatique de la langue et a pour but de formaliser une évaluation qualitative du processus d'apprentissage. Des validations cognitives de nos différents systèmes d'évaluation automatique ont été réalisées, et nous avons conçu des scénarios d'utilisation de ReaderBench, notre système le plus avancé, dans différents contextes d'enseignement. L'un des buts principaux de notre modèle est de favoriser la compréhension vue en tant que « médiatrice de l'apprentissage » , en procurant des rétroactions automatiques aux apprenants et enseignants ou tuteurs. Leur avantage est triple : leur flexibilité, leur extensibilité et, cependant, leur spécificité, car ils couvrent de multiples étapes de l'activité d'apprentissage, de la lecture de matériel d'apprentissage à l'écriture de synthèses de cours en passant par la discussion collaborative de contenus de cours et la verbalisation métacognitive de jugements de compréhension, afin d'obtenir une perspective complète du niveau de compréhension et de générer des rétroactions appropriées sur le processus d'apprentissage collaboratif.
Cette thèse se place à la croisée du traitement automatique du langage, de l'extraction d'information, de la recherche d'information et des sciences cognitives, dans un contexte de géomatique. Dans ce contexte, nos travaux visent à traiter automatiquement des textes relatant des récits de voyage, avec pour objectif d'interpréter l'information géographique qu'ils contiennent et plus particulièrement les itinéraires qu'ils décrivent. L'approche que nous proposons est une méthode incrémentale de découverte du sens : partant d'une sémantique du niveau du syntagme nominal simple, et passant par une sémantique du niveau de la proposition, nous atteignons une sémantique du niveau du texte. Notre contribution réside donc dans la proposition d'un modèle du concept d'itinéraire adapté à des usages ciblés (pour la RI et pour des usages de conception d'activités pédagogiques), la proposition d'une approche pour caractériser l'itinéraire dans le texte, et enfin la proposition d'un mode opératoire automatisé et entièrement implémenté permettant de relier la caractérisation de l'itinéraire dans le texte à son modèle du concept. Notre démarche étant clairement expérimentale, nous nous sommes appuyé tout au long de nos travaux sur un prototype que nous avons mis au point. Ce prototype (PIIR pour Prototype pour l'Interprétation d'Itinéraires dans des Récits) nous permet de mettre face au traitement automatique les analyses et les algorithmes que nous proposons.
Depuis les années 90, Internet est au coeur du marché du travail. D'abord mobilisée sur des métiers spécifiques, son utilisation s'étend à mesure qu'augmente le nombre d'internautes dans la population. La recherche d'emploi au travers des « bourses à l'emploi électroniques » est devenu une banalité et le e-recrutement quelque chose de courant. Cette explosion d'informations pose cependant divers problèmes dans leur traitement en raison de la grande quantité d'information difficile à gérer rapidement et efficacement pour les entreprises. Nous présentons dans ce mémoire, les travaux que nous avons développés dans le cadre du projet E-Gen, qui a pour but la création d'outils pour automatiser les flux d'informations lors d'un processus de recrutement. Nous nous intéressons en premier lieu à la problématique posée par le routage précis de courriels. La capacité d'une entreprise à gérer efficacement et à moindre coût ces flux d'informations, devient un enjeu majeur de nos jours pour la satisfaction des clients. Nous présentons par la suite les travaux qui ont été menés dans le cadre de l'analyse et l'intégration d'une offre d'emploi par Internet. Le temps étant un facteur déterminant dans ce domaine, nous présentons une solution capable d'intégrer une offre d'emploi d'une manière automatique ou assistée afin de pouvoir la diffuser rapidement. Basé sur une combinaison de systèmes de classifieurs pilotés par un automate de Markov, le système obtient de très bons résultats. Nous proposons également les diverses stratégies que nous avons mises en place afin de fournir une première évaluation automatisée des candidatures permettant d'assister les recruteurs. Nous avons évalué une palette de mesures de similarité afin d'effectuer un classement pertinent des candidatures. L'utilisation d'un modèle de relevance feedback a permis de surpasser nos résultats sur ce problème difficile et sujet à une grande subjectivité.
Nos travaux décrits dans cette thèse portent sur l'apprentissage d'une représentation pour la classification automatique basée sur la découverte de motifs à partir de séries temporelles. L'information pertinente contenue dans une série temporelle peut être encodée temporellement sous forme de tendances, de formes ou de sous-séquences contenant habituellement des distorsions. Des approches ont été développées pour résoudre ces problèmes souvent au prix d'une importante complexité calculatoire. Parmi ces techniques nous pouvons citer les mesures de distance et les représentations de l'information contenue dans les séries temporelles. Nous nous concentrons sur la représentation de l'information contenue dans les séries temporelles. Le framework proposé transforme un ensemble de séries temporelles en un espace d'attributs (feature space) à partir de sous-séquences énumérées des séries temporelles, de mesures de distance et de fonctions d'agrégation. Un cas particulier de ce framework est la méthode notoire des « shapelets » . L'inconvénient potentiel d'une telle approache est le nombre très important de sous-séquences à énumérer en ce qu'il induit un très grand feature space, accompagné d'une très grande complexité calculatoire. Nous montrons que la plupart des sous-séquences présentes dans un jeu de données composé de séries temporelles sont redondantes. De ce fait, un sous-échantillonnage aléatoire peut être utilisé pour générer un petit sous-ensemble de sous-séquences parmi l'ensemble exhaustif, en préservant l'information nécessaire pour la classification et tout en produisant un feature space de taille compatible avec l'utilisation d'algorithmes d'apprentissage automatique de l'état de l'art avec des temps de calculs raisonnable. On démontre également que le nombre de sous-séquences à tirer n'est pas lié avec le nombre de séries temporelles présent dans l'ensemble d'apprentissage, ce qui garantit le passage à l'échelle de notre approche. La combinaison de cette découverte dans le contexte de notre framework nous permet de profiter de techniques avancées (telles que des méthodes de sélection d'attributs multivariées) pour découvrir une représentation de séries temporelles plus riche, en prenant par exemple en considération les relations entre sous-séquences. Ces résultats théoriques ont été largement testés expérimentalement sur une centaine de jeux de données classiques de la littérature, composés de séries temporelles univariées et multivariées. De plus, nos recherches s'inscrivant dans le cadre d'une convention de recherche industrielle (CIFRE) avec Arcelormittal, nos travaux ont été appliqués à la détection de produits d'acier défectueux à partir des mesures effectuées par les capteurs sur des lignes de production.
Ce travail se penche sur les difficultés auxquelles font face les apprenants nigérians de français, précisément ceux de langue maternelle yoruba, quant à ce qui concerne les temps passés du français : l'imparfait et le passé composé. Nous sommes parti des productions des apprenants, deux exercices à trous et un écrit long de type rédaction, pour exposer les erreurs de temps commises. Nous avons découvert, suite à l'analyse des productions, que la plupart de ces erreurs proviennent du système aspectuo-temporel du yoruba, langue ne connaissant pas de conjugaison (désinences verbales) comme le français. D'autre part, l'analyse des deux tests à trous en plus de celle des copies de rédaction montrent que, le manque de connaissance de certaines notions linguistiques est une autre cause des difficultés rencontrées par les apprenants : la notion de discours / récit et celle de premier / arrière En somme, nous pensons qu'un enseignement / apprentissage des temps basé sur la notion d'aspect grammatical, et prenant en compte les notions précédemment mentionnées, sera certainement plus productif. Nos propositions de pistes pour un meilleur enseignement/apprentissage des temps concernés terminent cette recherche. Nous pensons, par ailleurs, qu'en ajoutant à ce que nous venons de dire, les détails que nous ont révélés les analyses linguistiques des systèmes aspectuo-temporels des deux langues, nous pourrons construire par la suite une méthode d'enseignement et apprentissage des temps verbaux du passé pour l'apprenant nigérian.
De multiples problèmes en apprentissage automatique consistent à minimiser une fonction lisse sur un espace euclidien. Pour l'apprentissage supervisé, cela inclut les régressions par moindres carrés et logistique. Dans ce manuscrit, nous considérons le cas particulier de la perte quadratique. Dans une première partie, nous nous proposons de la minimiser grâce à un oracle stochastique. Dans une seconde partie, nous considérons deux de ses applications à l'apprentissage automatique : au partitionnement de données et à l'estimation sous contrainte de forme. Ce nouveau cadre suggère un algorithme alternatif qui combine les aspects positifs du moyennage et de l'accélération. La deuxième contribution est d'obtenir le taux optimal d'erreur de prédiction pour la régression par moindres carrés en fonction de la dépendance au bruit du problème et à l'oubli des conditions initiales. Notre nouvel algorithme est issu de la descente de gradient accélérée et moyennée. La troisième contribution traite de la minimisation de fonctions composites, somme de l'espérance de fonctions quadratiques et d'une régularisation convexe. Nous étendons les résultats existants pour les moindres carrés à toute régularisation et aux différentes géométries induites par une divergence de Bregman. Dans une quatrième contribution, nous considérons le problème du partitionnement discriminatif. Nous proposons sa première analyse théorique, une extension parcimonieuse, son extension au cas multi-labels et un nouvel algorithme ayant une meilleure complexité que les méthodes existantes. La dernière contribution de cette thèse considère le problème de la sériation. Nous adoptons une approche statistique où la matrice est observée avec du bruit et nous étudions les taux d'estimation minimax.
Le domaine de l'intelligence artificielle (IA) a connu un essor sans précédent ces dernières années avec des succès spectaculaires très médiatisés. En réalité, l'IA est utilisée dans de nombreux domaines allant de la vision par ordinateur au traitement automatique des langues. Parmi les nombreuses techniques de l'intelligence artificielle, l'apprentissage profond à base de réseaux de neurones a montré de très bonnes capacités d'apprentissage et des bonnes performances dans de nombreux domaines. La conception et le développement de tels réseau est une tâche ardue qui nécessitent des connaissances pointues dans les architectures parallèles modernes afin de pouvoir utiliser au mieux la puissance de calcul de telles machines. Cet enjeu est d'autant plus important au vu du développement massif de l'IA aussi bien dans le monde académique qu'industriel. L'objectif de la thèse est de définir un environnement de développement de réseaux de neurones pour l'apprentissage profond capable de prendre en compte les évolutions actuelles des architectures de ces réseaux. Cet environnement devra comprendre un langage spécifique de domaine permettant à l'utilisateur de décrire l'architecture de son système sans se préoccuper de la manière dont celui-ci sera déployé. Ce langage de haut-niveau devra reposer sur les principes du parallélisme implicite qui fournit à l'utilisateur des motifs (patterns ou squelettes) permettant de décrire les éléments du réseau de neurones et leur assemblage de manière simple tout en donnant des indications qui permettront de définir une stratégie de parallélisation efficace. L'environnement devra ensuite être capables de dériver à partir de la description de haut niveau, un programme et un déploiement efficaces du réseau de neurones sur l'architecture cible. Pour arriver à cette fin il va falloir détecter et décrire les squelettes de programmation pertinents pour les concepteurs de réseaux de neurones et trouver des stratégies de parallélisation efficace pour chacun de ces squelettes. Par ailleurs des outils d'analyse du réseau d'apprentissage profond permettant de détecter des optimisations en termes de déploiement et de communications seront aussi essentiels pour obtenir des systèmes efficaces. Les résultats auront pour objectif d'intégrer l'environnement MindSpore de Huawei et de contribuer aux produits et solutions IA de Huawei, afin d'explorer toute la puissance potentielle de sa « Compute Architecture for Neural Networks » (CANN)
Les récents progrès dans la numérisation des collections de documents patrimoniaux ont ravivé de nouveaux défis afin de garantir une conservation durable et de fournir un accès plus large aux documents anciens. Les efforts se concentrent autant sur le développement d'outils rapides et automatiques de caractérisation et catégorisation des pages d'ouvrages anciens, capables de classer les pages d'un ouvrage numérisé en fonction de plusieurs critères, notamment la structure des mises en page et/ou les caractéristiques typographiques/graphiques du contenu de ces pages. Ainsi, dans le cadre de cette thèse, nous proposons une approche permettant la caractérisation et la catégorisation automatiques des pages d'un ouvrage ancien. L'approche proposée se veut indépendante de la structure et du contenu de l'ouvrage analysé. Le principal avantage de ce travail réside dans le fait que l'approche s'affranchit des connaissances préalables, que ce soit concernant le contenu du document ou sa structure. Elle est basée sur une analyse des descripteurs de texture et une représentation structurelle en graphe afin de fournir une description riche permettant une catégorisation à partir du contenu graphique (capturé par la texture) et des mises en page (représentées par des graphes). En effet, cette catégorisation s'appuie sur la caractérisation du contenu de la page numérisée à l'aide d'une analyse des descripteurs de texture, de forme, géométriques et topologiques. Cette caractérisation est définie à l'aide d'une représentation structurelle. Dans le détail, l'approche de catégorisation se décompose en deux étapes principales successives. La première consiste à extraire des régions homogènes. La seconde vise à proposer une signature structurelle à base de texture, sous la forme d'un graphe, construite à partir des régions homogènes extraites et reflétant la structure de la page analysée. Cette signature assure la mise en œuvre de nombreuses applications pour gérer efficacement un corpus ou des collections de livres patrimoniaux (par exemple, la recherche d'information dans les bibliothèques numériques en fonction de plusieurs critères, ou la catégorisation des pages d'un même ouvrage). En comparant les différentes signatures structurelles par le biais de la distance d'édition entre graphes, les similitudes entre les pages d'un même ouvrage en termes de leurs mises en page et/ou contenus peuvent être déduites. Ainsi de suite, les pages ayant des mises en page et/ou contenus similaires peuvent être catégorisées, et un résumé/une table des matières de l'ouvrage analysé peut être alors généré automatiquement. Pour illustrer l'efficacité de la signature proposée, une étude expérimentale détaillée a été menée dans ce travail pour évaluer deux applications possibles de catégorisation de pages d'un même ouvrage, la classification non supervisée de pages et la segmentation de flux de pages d'un même ouvrage. En outre, les différentes étapes de l'approche proposée ont donné lieu à des évaluations par le biais d'expérimentations menées sur un large corpus de documents patrimoniaux.
Le travail de recherche présenté dans cette thèse s'inscrit dans le cadre général des travaux sur les humanités numériques qui cherchent, entre autres, à contribuer à l'amélioration des interactions Homme-Machine. L'objectif de l'étude est double. Dans un premier temps, il s'agit d'étudier un corpus de décisions de justice contenues dans la base de données de l'Institut du Droit International des Transports (IDIT) afin de déterminer les contraintes linguistiques du genre judiciaire. Dans un second temps, il est question de proposer des parcours interprétatifs pouvant aider les utilisateurs dans leur accès à l'information juridique recherchée. La problématique de l'aide à l'interprétation est appréhendée à travers l'étude des modalités et des scénarios modaux. Le parti pris de cette recherche est de considérer la pluridisciplinarité comme un atout théorique et méthodologique qui contribue à mieux éclairer un objet d'étude. De ce fait, plusieurs approches (sémantique des modalités, sémantique textuelle, argumentation rhétorique, textométrie) sont convoquées et articulées pour œuvrer ensemble vers les objectifs fixés. L'analyse du corpus a été menée à deux niveaux et selon deux approches. Dans la première partie, l'analyse empirique proposée est quantitative et contrastive. Elle est menée au niveau micro et mésotextuelle dans la mesure où elle se focalise sur l'étude du lexique. Aidée de l'outil TXM, cette première investigation a permis une caractérisation linguistique globale du corpus et un premier aperçu de son profil modal. Elle a également mis en exergue des expressions modales, constructions concessives, routines discursives, etc. qui focalisent sur des moments clés dans le déroulement argumentatif et pouvant donc servir dans le cadre de l'aide à l'interprétation. Dans la seconde partie, l'étude empirique porte sur des analyses modales menées sur des textes complets. Elle est donc abordée dans une approche qualitative et au niveau macrotextuelle. Cette analyse aboutit à la formulation d'un modèle de scénario minutieusement décrit. Il est décomposable en plusieurs niveaux, selon les modalités qui l'ont construit (modalités de premier plan, modalité d'arrière-plan) et selon qu'il caractérise un texte complet ou une zone spécifique de ce texte. Par ailleurs, la présentation schématique proposée pour les scénarios a mis en évidence le rôle que représenterait chaque zone modale dans la perspective d'une aide à l'interprétation.
La première partie de cette thèse présente une méthode d'assistance à la transcription automatique de la parole. Le transcripteur humain dispose de la meilleure hypothèse fournie par le SRAP, et, à chaque correction de sa part, le système propose une nouvelle hypothèse prenant en compte cette correction. Cette dernière est obtenue à partir d'une réévaluation des réseaux de confusion générés par le SRAP. Afin de diminuer le taux d'erreur sur les noms propres, une méthode de phonétisation itérative utilisant les données acoustiques à disposition est proposée dans ce manuscrit. L'utilisation de SMT [Laurent 2009] couplée avec la méthode de phonétisation proposée permet d'observer des gains en terme de taux d'erreur mot (WER) et en terme de taux d'erreur noms propres (PNER).
La linguistique informatique a pour objet de construire un modèle formel des connaissances linguistiques, et d'en tirer des algorithmes permettant le traitement automatique des langues. Pour ce faire, elle s'appuie fréquemment sur des grammaires dites génératives, construisant des phrases valides par l'application successive de règles de réécriture. Une approche alternative, basée sur la théorie des modèles, vise à décrire la grammaticalité comme une conjonction de contraintes de bonne formation, en s'appuyant sur des liens profonds entre logique et automates pour produire des analyseurs efficaces. Notre travail se situe dans ce dernier cadre. En s'appuyant sur plusieurs résultats existants en informatique théorique, nous proposons un outil de modélisation linguistique expressif, conçu pour faciliter l'ingénierie grammaticale. Celui-ci considère dans un premier temps la structure abstraite des énoncés, et fournit un langage logique s'appuyant sur les propriétés lexicales des mots pour caractériser avec concision l'ensemble des phrases grammaticalement correctes. Puis, dans un second temps, le lien entre ces structures abstraites et leurs représentations concrètes (en syntaxe et en sémantique) est établi par le biais de règles de linéarisation qui exploitent la logique et le lambda-calcul. Par suite, afin de valider cette approche, nous proposons un ensemble de modélisations portant sur des phénomènes linguistiques divers, avec un intérêt particulier pour le traitement des langages présentant des phénomènes d'ordre libre (c'est-à-dire qui autorisent la permutation de certains mots ou groupes de mots dans une phrase sans affecter sa signification), ainsi que pour leur complexité algorithmique.
Cette thèse traite de l'extraction d'unités lexicales en chinois contemporain à partir d'un corpus de textes de spécialité. Elle aborde la tâche d'extraction de lexique en chinois en utilisant des techniques se basant sur des caractéristiques linguistiques de la langue chinoise. La thèse traite également de la manière d'évaluer l'extraction de lexique dans un environnement industriel. La première partie de la thèse est consacrée à la description du contexte de l'étude. Nous nous attachons dans un premier à temps à décrire les concepts linguistiques d'unité lexicale et de lexique, et nous donnons une description du processus de construction des unités lexicales en chinois contemporain. Nous faisons ensuite un inventaire des différentes techniques utilisées par la communauté scientifique pour traiter la tâche de l'extraction de lexique en chinois contemporain. Nous concluons cette partie par une description des pratiques d'extraction de lexique en milieu industriel, et nous proposons une formalisation des critères utilisés par les terminographes d'entreprise pour sélectionner les unités lexicales pertinentes. La deuxième partie du mémoire porte sur la description d'une méthode d'extraction de lexique en chinois contemporain et sur son évaluation. Nous introduisons une nouvelle méthode numérique non supervisée s'appuyant sur des caractéristiques structurelles de l'unité lexicale en chinois et sur des particularités syntaxiques du chinois. La méthode comporte un module optionnel permettant une interaction avec un opérateur (i. E. Semi-automatique). Dans la section consacrée à l'évaluation, nous évaluons d'abord le potentiel de la méthode en comparant les résultats de l'extraction avec un standard de référence et une méthode de référence. Nous mettons ensuite en oeuvre une évaluation plus pragmatique de la méthode en mesurant les gains apportés par l'usage de la méthode en comparaison avec l'extraction manuelle de lexique par des terminographes. Les résultats obtenus par notre méthode sont de bonne qualité et sont meilleurs L'évaluation pragmatique montre que la méthode n'améliore pas significativement la productivité des terminographes, mais permet d'extraire des unités lexicales différentes de celles obtenue manuellement.
En biomedicine, le domaine du « Big Data » (l'infobésité) pose le problème de l'analyse de gros volumes de données hétérogènes (i.e. vidéo, audio, texte, image). Les ontologies biomédicales, modèle conceptuel de la réalité, peuvent jouer un rôle important afin d'automatiser le traitement des données, les requêtes et la mise en correspondance des données hétérogènes. Dans un premier temps, les ontologies ont été construites manuellement. Au cours de ces dernières années, quelques méthodes semi-automatiques ont été proposées. Ces techniques semi-automatiques de construction/enrichissement d'ontologies sont principalement induites à partir de textes en utilisant des techniques du traitement du langage naturel (TALN). Les méthodes de TALN permettent de prendre en compte la complexité lexicale et sémantique des données biomédicales : (1) lexicale pour faire référence aux syntagmes biomédicaux complexes à considérer et (2) sémantique pour traiter l'induction du concept et du contexte de la terminologie. Dans cette thèse, afin de relever les défis mentionnés précédemment, nous proposons des méthodologies pour l'enrichissement/la construction d'ontologies biomédicales fondées sur deux principales contributions. La première contribution est liée à l'extraction automatique de termes biomédicaux spécialisés (complexité lexicale) à partir de corpus. De nouvelles mesures d'extraction et de classement de termes composés d'un ou plusieurs mots ont été proposées et évaluées. L'application BioTex implémente les mesures définies. La seconde contribution concerne l'extraction de concepts et le lien sémantique de la terminologie extraite (complexité sémantique). Les évaluations, quantitatives et qualitatives, menées par des experts et non experts, sur des données réelles soulignent l'intérêt de ces contributions.
Cependant, ces améliorations multiplient la quantité de données à traiter avant transmission du signal par un facteur 8. En plus de ce nouveau format, les fournisseurs de contenu doivent aussi encoder les vidéos dans des formats et à des débits différents du fait de la grande variété des systèmes et réseaux utilisés par les consommateurs. Ensuite, dans un second lieu, le design d'une architecture scalable à deux couches, plus conventionnelle, est étudié. Celle-ci est composée d'un encodeur HEVC standard dans la couche de base pour assurer la compatibilité avec les systèmes existants. Un algorithme faisant varier la fréquence image est d'abord proposé. Cet algorithme est capable de détecter localement et de façon dynamique la fréquence image la plus basse n'introduisant pas d'artefacts visibles liés au mouvement. Les algorithmes de fréquence image variable et de résolution spatiale adaptative sont ensuite combinés afin d'offrir un codage scalable à faible complexité des contenus 4KHFR.
La qualité de l'information géographique volontaire est actuellement un sujet qui questionne autant les consommateurs de données géographiques que les producteurs de données d'autorité voulant exploiter les bienfaits de la démarche collaborative. Ce phénomène est un travers de la démarche collaborative, et bien qu'il ne concerne qu'une faible portion des contributions, il peut constituer un obstacle à l'utilisation des données cartographiques participatives. Dans une démarche de qualification de l'information géographique volontaire, ce travail de thèse a plus précisément pour objectif de détecter le vandalisme dans les données collaboratives cartographiques. Dans un premier temps, il s'agit de formaliser une définition du concept de carto Enfin, nos expériences explorent la capacité des méthodes d'apprentissage machine (machine learning) à détecter le carto
Cette thèse porte sur l'intégration de ressources lexicales et syntaxiques du français dans deux tâches fondamentales du Traitement Automatique des Langues [TAL] que sont l'étiquetage morpho-syntaxique probabiliste et l'analyse syntaxique probabiliste. Grâce à des algorithmes d'analyse syntaxique de plus en plus évolués, les performances actuelles des analyseurs sont de plus en plus élevées, et ce pour de nombreuses langues dont le français. Cependant, il existe plusieurs problèmes inhérents aux formalismes mathématiques permettant de modéliser statistiquement cette tâche (grammaire, modèles discriminants,...). La dispersion des données est l'un de ces problèmes, et est causée principalement par la faible taille des corpus annotés disponibles pour la langue. De plus, il est prouvé que la dispersion est en partie un problème lexical, car plus la flexion d'une langue est importante, moins les phénomènes lexicaux sont représentés dans les corpus annotés. Notre première problématique repose donc sur l'atténuation de l'effet négatif de la dispersion lexicale des données sur les performances des analyseurs. Dans cette optique, nous nous sommes intéressé à une méthode appelée regroupement lexical, et qui consiste à regrouper les mots du corpus et des textes en classes. Ces classes réduisent le nombre de mots inconnus et donc le nombre de phénomènes syntaxiques rares ou inconnus, liés au lexique, des textes à analyser. Notre objectif est donc de proposer des regroupements lexicaux à partir d'informations tirées des lexiques syntaxiques du français, et d'observer leur impact sur les performances d'analyseurs syntaxiques. Par ailleurs, la plupart des évaluations concernant l'étiquetage morpho-syntaxique probabiliste et l'analyse syntaxique probabiliste ont été réalisées avec une segmentation parfaite du texte, car identique à celle du corpus évalué. Or, dans les cas réels d'application, la segmentation d'un texte est très rarement disponible et les segmenteurs automatiques actuels sont loin de proposer une segmentation de bonne qualité, et ce, à cause de la présence de nombreuses unités multi-mots (mots composés, entités nommées,...). Dans ce mémoire, nous nous focalisons sur les unités multi-mots dites continues qui forment des unités lexicales auxquelles on peut associer une étiquette morpho-syntaxique, et que nous appelons mots composés. Par exemple, cordon bleu est un nom composé, et tout à fait un adverbe composé. Notre deuxième problématique portera donc sur la segmentation automatique des textes français et son impact sur les performances des processus automatiques. Pour ce faire, nous nous sommes penché sur une approche consistant à coupler, dans un même modèle probabiliste, la reconnaissance des mots composés et une autre tâche automatique. La reconnaissance des mots composés est donc réalisée au sein du processus probabiliste et non plus dans une phase préalable.
La planification est un domaine de l'intelligence artificielle qui a pour but de proposer des méthodes permettant d'automatiser la recherche et l'ordonnancement d'ensembles d'actions afin d'atteindre un objectif donné. Il s'agit de décomposer un problème en plusieurs sous-problèmes (généralement appelés composants) le plus indépendants possibles, et d'assembler des plans pour ces sous-problèmes en un plan pour le problème d'origine. L'intérêt de cette approche est que, pour certaines classes de problèmes de planification, les composants peuvent être bien plus simples à résoudre que le problème initial. La principale différence entre cette méthode et la précédente est qu'il s'agit d'une approche distribuée de la planification modulaire.
Dans cette dissertation, nous discutons comment utiliser les données du regard humain pour améliorer la performance du modèle d'apprentissage supervisé faible dans la classification des images. Le contexte de ce sujet est à l'ère de la technologie de l'information en pleine croissance. En conséquence, les données à analyser augmentent de façon spectaculaire. Étant donné que la quantité de données pouvant être annotées par l'humain ne peut pas tenir compte de la quantité de données elle-même, les approches d'apprentissage supervisées bien développées actuelles peuvent faire face aux goulets d'étranglement l'avenir. Dans ce contexte, l'utilisation de annotations faibles pour les méthodes d'apprentissage à haute performance est digne d'étude. Plus précisément, nous essayons de résoudre le problème à partir de deux aspects : l'un consiste à proposer une annotation plus longue, un regard de suivi des yeux humains, comme une annotation alternative par rapport à l'annotation traditionnelle longue, par exemple boîte de délimitation. L'autre consiste à intégrer l'annotation du regard dans un système d'apprentissage faiblement supervisé pour la classification de l'image. Ce schéma bénéficie de l'annotation du regard pour inférer les régions contenant l'objet cible. Une propriété utile de notre modèle est qu'elle exploite seulement regardez pour la formation, alors que la phase de test est libre de regard. Cette propriété réduit encore la demande d'annotations. Les deux aspects isolés sont liés ensemble dans nos modèles, ce qui permet d'obtenir des résultats expérimentaux compétitifs.
La modélisation sinusoïdale est une des méthodes les plus largement utilisés paramétriques pour la parole et le traitement des signaux audio. Le eaQHM est montré à surperformer aQHM dans l'analyse et la resynthèse de la parole voisée. Sur la base de la eaQHM, un système hybride d'analyse / synthèse de la parole est présenté (eaQHNM), et aussi d'une version hybride de l'aHM (aHNM). En outre, nous présentons la motivation pour une représentation pleine bande de la parole en utilisant le eaQHM, c'est, représentant toutes les parties du discours comme haute résolution des sinusoıdes AM-FM. Les expériences montrent que l'adaptation et la quasi-harmonicité est suffisante pour fournir une qualité de transparence dans la parole non voisée resynthèse. La pleine bande analyse eaQHM et système de synthèse est présenté à côté, ce qui surpasse l'état de l'art des systèmes, hybride ou pleine bande, dans la reconstruction de la parole, offrant une qualité transparente confirmé par des évaluations objectives et subjectives. En ce qui concerne les applications, le eaQHM et l'aHM sont appliquées sur les modifications de la parole (de temps et pas mise à l'échelle). Les modifications qui en résultent sont de haute qualité, et suivent des règles très simples, par rapport à d'autres systèmes de modification état de l'art. Les résultats montrent que harmonicité est préféré au quasi-harmonicité de modifications de la parole du fait de la simplicité de la représentation intégrée. En outre, la pleine bande eaQHM est appliquée sur le problème de la modélisation des signaux audio, et en particulier d'instrument de musique retentit. Le eaQHM est évaluée et comparée à des systèmes à la pointe de la technologie, et leur est montré surpasser en termes de qualité de resynthèse, représentant avec succès l'attaque, transitoire, et une partie stationnaire d'un son d'instruments de musique. Enfin, une autre application est suggéré, à savoir l'analyse et la classification des discours émouvant. Le eaQHM est appliqué sur l'analyse des discours émouvant, offrant à ses paramètres instantanés comme des caractéristiques qui peuvent être utilisés dans la reconnaissance et la quantification vectorielle à base classification du contenu émotionnel de la parole. Bien que les modèles sinusoidaux sont pas couramment utilisés dans ces tâches, les résultats sont prometteurs.
Grâce à la démocratisation des nouvelles technologies de communications nous disposons d'une quantité croissante de ressources textuelles, faisant du Traitement Automatique du Langage Naturel (TALN) une discipline d'importance cruciale tant scientifiquement qu'industriellement. Aisément disponibles, ces données offrent des opportunités sans précédent et, de l'analyse d'opinion à la recherche d'information en passant par l'analyse sémantique de textes les applications sont nombreuses. On ne peut cependant aisément tirer parti de ces données textuelles dans leur état brut et, en vue de mener à bien de telles tâches il semble indispensable de posséder des ressources décrivant les connaissances sémantiques, notamment sous la forme de réseaux lexico-sémantiques comme par exemple celui du projet JeuxDeMots. La constitution et la maintenance de telles ressources restent cependant des opérations difficiles, de part leur grande taille mais aussi à cause des problèmes de polysémie et d'identification sémantique. De plus, leur utilisation peut se révéler délicate car une part significative de l'information nécessaire n'est pas directement accessible dans la ressource mais doit être inférée à partir des données du réseau lexico-sémantique. Nos travaux cherchent à démontrer que les réseaux lexico-sémantiques sont, de par leur nature connexionniste, bien plus qu'une collection de faits bruts et que des structures plus complexes telles que les chemins d'interprétation contiennent davantage d'informations et permettent d'accomplir de multiples opérations d'inférences. En particulier, nous montrerons comment utiliser une base de connaissance pour fournir des explications à des faits de haut niveau. Ces explications permettant a minima de valider et de mémoriser de nouvelles informations. Ce faisant, nous pouvons évaluer la couverture et la pertinence des données de la base ainsi que la consolider. De même, la recherche de chemins se révèle utile pour des problèmes de classification et de désambiguïsation, car ils sont autant de justifications des résultats calculés. Dans le cadre de la reconnaissance d'entité nommées, ils permettent aussi bien de typer les entités et de les désambiguïser (l'occurrence du terme Paris est-il une référence à la ville, et laquelle, ou à une starlette ?) en mettant en évidence la densité des connexions entre les entités ambiguës, leur contexte et leur type éventuel. Enfin nous proposons de tourner à notre avantage la taille importante du réseau JeuxDeMots pour enrichir la base de nouveaux faits à partir d'un grand nombre d'exemples comparables et par un processus d'abduction sur les types de relations sémantiques pouvant connecter deux termes donnés. Chaque inférence s'accompagne d'explications pouvant être validées ou invalidées offrant ainsi un processus d'apprentissage.
Cette thèse s'inscrit dans ce cadre pour aborder le sujet depuis deux points de vue complémentaires. D'une part, celui apparent de la fiabilité, de l'efficacité et de l'utilisabilité de ces interfaces. D'autre part, les aspects de conception et d'implémentation sont étudiés pour apporter des outils de développement aux concepteurs plus ou moins initiés de tels systèmes. A partir des outils et des évolutions dans le domaine, une plate-forme modulaire de dialogue vocal a été agrégée. Une méthode simple, basée sur la comparaison des résultats de traitements parallèles a prouvé son efficacité, tout comme ses limites pour une interaction continue avec l'utilisateur. Les modules de compréhension du langage forment un sous-système interconnecté au sein de la plate-forme. Le choix de la gestion du dialogue basé sur des modèles de tâches hiérarchiques, comme c'est la cas pour la plate-forme, est argumenté. Ce formalisme est basé sur une construction humaine et présente, de fait, des obstacles pour concevoir, implémenter, maintenir et faire évoluer les modèles.
L'objet de cette thèse est de préparer un dictionnaire de médecine vétérinaire français-roumain et roumain-français. A cette fin, dans une première étape, nous avons étudié les méthodes employées au fil du temps par les maîtres de la terminologie. Nos méthodes de travail ont été offertes par nos collègues plus expérimentés et par les normes ISO de terminologie. Ainsi, nos recherches se sont constituées dans un essai de modélisation du travail terminologique, au cours de la recherche des meilleures méthodes. Dans une deuxième étape, nous avons constitué un corpus de recherche, en se basant sur les méthodes de la linguistique de corpus. Nous avons constitué un corpus de 2193 thèses de doctorat vétérinaires VeThèses des 11 années (1998-2008) des quatre écoles vétérinaires françaises, des thèses disponibles en format numérique, autour du thème des vertébrés domestiques. L'étape de dépouillement terminologique a compris aussi un essai de systématisation des termes. Les résultats de ce travail sur le corpus se sont constitués dans une nomenclature et la base de données VeTerm.
Nos travaux de recherche ont pour objectif de résoudre le problème de la géodétection des réseaux enterrés. Plusieurs méthodes sont utilisées actuellement mais présentent des limites dues à la nature du sol, aux matériaux des canalisations et au produit transporté.
A une époque où l'informatique a envahi tous les aspects de notre vie quotidienne, il est tout à fait normal de voir le domaine informatique participer aux travaux en sciences humaines et sociales, et notamment en linguistique où le besoin de développer des logiciels informatiques se fait de plus en plus pressant avec le volume grandissant des corpus traités. D'où notre travail de thèse qui consiste en l'élaboration d'un programme EPL qui étudie le parler arabe libanais blanc. En partant d'un corpus élaboré à partir de deux émissions télévisées enregistrées puis transcrites en lettres arabes, ce programme, élaboré avec le logiciel Access, nous a permis d'extraire les mots et les collocations et de procéder à une analyse linguistique aux niveaux lexical, phonétique, syntaxique et collocationnel. Le fonctionnement de l'EPL ainsi que le code de son développement sont décrits en détails dans une partie informatique à part. Des annexes de taille closent la thèse et rassemblent le produit des travaux de toute une équipe de chercheures venant de maintes spécialités.
La vision par ordinateur est un domaine de l'informatique visant à reproduire et à améliorer la capacité de la vision humaine à comprendre son environnement. Dans cette thèse, nous nous concentrons sur deux domaines de la vision par ordinateur, à savoir la segmentation d'image et l'odométrie visuelle. Nous montrons l'impact positif qu'apporte l'usage d'applications La première partie de cette thèse porte sur l'annotation et la segmentation d'images. Nous définissons dans un premier temps le problème de l'annotation d'images et les défis que cela représente pour des grands ensembles de données. De nombreuses interactions ont été utilisées dans la littérature pour aider les algorithmes de segmentation. Les plus courantes consistent à désigner explicitement des contours, dessiner des boîtes englobantes, ou marquer des traits à l'intérieur et à l'extérieur des objets d'intérêt. Dans un contexte de crowdsourcing, les tâches d'annotation sont déléguées à un public non-expert. Pour cette raison, nous avons mené une étude utilisateur montrant les avantages d'une interaction que nous appelons entourage par rapport aux autres types d'interactions. Un tour d'horizon des fonctionnalités et de son architecture est proposé, ainsi qu'un guide pour le déploiement dans des services de microtâches comme Amazon Mechanical Turk. Cette application est entièrement libre et mise à disposition en ligne. Dans la seconde partie de cette thèse, nous présentons notre bibliothèque libre d'odométrie visuelle directe. Nous fournissons une évaluation comparative montrant que notre approche est aussi performante que les alternatives actuellement disponibles. La formulation du problème d'odométrie visuelle repose sur des outils géométriques et des techniques d'optimisation nécessitant une grosse puissance de calcul pour fonctionner à 25 images par seconde. Puisque nous aspirons à exécuter ces algorithmes sur le Web, nous passons en revue les technologies passées et courantes fournissant des bonnes performances directement au sein du navigateur Web. En particulier, nous détaillons comment cibler une nouvelle plateforme appelée WebAssembly à partir des langages de programmation C++ et Rust. Notre bibliothèque a été implémentée entièrement dans le langage de programmation Rust, ce qui en a facilité le portage vers WebAssembly. Cette propriété nous a permis de mettre en place une application Web d'odométrie visuelle proposant différents types d'interactions. Une barre de temps permet une navigation unidimensionnelle le long de la séquence vidéo. Des paires de points peuvent être sélectionnées sur deux images de la séquence pour réaligner les caméras et corriger l'éventuelle dérive. Des couleurs sont également utilisées pour identifier des parties sélectionnables du nuage de points 3D pour réinitialiser les positions de la caméra. La combinaison de ces interactions permet d'apporter des améliorations sur les résultats du suivi et de la reconstruction du nuage de points 3D.
Notre hypothèse de travail est la suivant : la LSF est une langue et, à ce titre, la traduction automatique lui est applicable. Nous décrivons ensuite les spécifications linguistiques pour le traitement automatique, en fonction des observations mises en avant dans l'état de l'art et des propositions de nos informateurs. Nous détaillons notre méthodologie et présentons l'avancée de nos travaux autour de la formalisation des données linguistiques à partir des spécificités de la LSF dont certaines (model verbal, modification adjectivale et adverbiale, organisation des substantifs, problématiques de l'accord) ont nécessité un traitement plus approfondi. Nous présentons le cadre applicatif dans lequel nous avons travaillé : les systèmes de traduction automatique et d'animation de personnage virtuel de France Telecom R&amp ; D. Puis, après un rapide état de l'art sur les technologies avatar nous décrivons nos modalités de contrôle du moteur de synthèse de geste grâce au format d'échange mis au point. Enfin, nous terminons par nos évaluations et perspectives de recherche et de développements qui pourront suivre cette Thèse. Notre approche a donné ses premiers résultats puisque nous avons atteint notre objectif de faire fonctionner la chaîne complète de traduction : de la saisie d'un énoncé en français jusqu'à la réalisation de l'énoncé correspondant en LSF par un personnage de synthèse.
Il s'agit le plus souvent d'exercices à trous ou de QCM, conçus à l'aide de systèmes auteurs comme Course builder, Hot Potatoes ou Netquizz. Ce type d'activités pose plusieurs problèmes comme la rigidité des logiciels (les données utilisés sont prédéfinies et elles ne peuvent être ni modifiées ni enrichies) et la non adaptabilité des parcours aux compétences linguistiques des apprenants (le cheminement est indépendant de ses réponses à chaque étape, faute de pouvoir les évaluer). L'utilisation du TAL, pour la conception de logiciels d'ALAO, représente un moyen faible pour résoudre ces problèmes. Après plus de deux décennies depuis le début des travaux, l'avancée de recherches dans ce domaine (c'est à dire utilisation de TAL en ALAO) reste insuffisante, à cause de deux facteurs principalement : la méconnaissance du TAL de la part des didacticiens des langues, voire des informaticiens, et le coût des ressources et produits issus du traitement automatique de la langue. ALAO basé sur le TAL pour la langue arabe sont pratiquement inexistants, malgré une riche bibliographie concernant le traitement automatique de l'arabe. Outre les facteurs cités précédemment, la carence concernant la langue arabe dans ce domaine est due au fait qu'elle est une langue difficile à traiter automatiquement. En fonction de cette situation, et désireux d'enrichir les possibilités de création d'activités pédagogiques pour l'arabe, nous avons conçu un étiqueteur, un dérivateur, un conjugueur, un analyseur morphologique ainsi qu'un dictionnaire étiqueté (le plus complet possible) pour la langue arabe. Ensuite nous avons exploité ces outils pour créer bon nombre d'applications pédagogiques pour l'apprentissage de l'arabe.
Cette thèse présente des recherches linguistiques et phonétiques sur le code-switching Français-Arabe Algérien. Un corpus de 7h30 de parole (5h de parole spontané et 2h30 de parole lue) a été constitué en enregistrant 20 hommes et femmes parlant le français et l'arabe algérien. Cette thèse présente également les méthodes de traitement des données orales du code-switching telles que la segmentation de la parole, la segmentation des énoncés de code-switching ainsi que la transcription du français et du dialecte arabe algérien. Cette thèse présente également des méthodes d'alignement automatique de ces données bilingues ainsi qu'un alignement combiné de deux alignements monolingues. Nous avons mené des expériences basées sur l'alignement automatique avec des variations qui traitent de la question de l'influence d'un système phonologique d'une langue A sur des productions phonétiques en code-switching du français et de l'arabe algérien. Nous avons aussi abordé les consonnes emphatiques et l'emphatisation des deux langues. Enfin, nous avons également travaillé sur les géminées et la gémination dans les productions langagières en code-switching. Les résultats ont montré que le code-switching FR-AA se caractérise par des changements de langues très courts qui sont un réel défi pour l'identification des langues dans le code-switching. Le code-switching a un impact sur la variation phonétique des voyelles et des consonnes. La parole du code-switching permet au locuteur de produire moins de variation de voyelles et de consonnes que la parole monolingue.
Le texte généré automatiquement a été utilisé dans de nombreuses occasions à des buts différents. Il peut simplement passer des commentaires générés dans une discussion en ligne à une tâche beaucoup plus malveillante, comme manipuler des informations bibliographiques. Ainsi, cette thèse introduit d'abord différentes méthodes pour générer des textes libres ayant trait à un certain sujet et comment ces textes peuvent être utilisés. Par conséquent, nous essayons d'aborder plusieurs questions de recherche. La première question est comment et quelle est la meilleure méthode pour détecter un document entièrement généré. Ensuite, nous irons un peu plus loin et montrer la possibilité de détecter quelques phrases ou un petit paragraphe de texte généré automatiquement en proposant une nouvelle méthode pour calculer la similarité des phrases en utilisant leur structure grammaticale. La dernière question est comment détecter un document généré automatiquement sans aucun échantillon, ceci est utilisé pour illustrer le cas d'un nouveau générateur ou d'un générateur dont il est impossible de collecter des échantillons dessus. Cette thèse étudie également l'aspect industriel du développement. Un aperçu simple d'un flux de travail de publication d'un éditeur de premier plan est présenté. À partir de là, une analyse est effectuée afin de pouvoir intégrer au mieux notre méthode de détection dans le flux de production. En conclusion, cette thèse a fait la lumière sur de multiples questions de recherche importantes concernant la possibilité de détecter des textes générés automatiquement dans différents contextes. En plus de l'aspect de la recherche, des travaux d'ingénierie importants dans un environnement industriel réel sont également réalisés pour démontrer qu'il est important d'avoir une application réelle pour accompagner une recherche hypothétique.
L'interopérabilité entre les systèmes a été identifiée comme un problème majeur auquel sont confrontées les entreprises lorsqu'ils ont le besoin de collaborer avec d'autres organisations et de participer au sein d'un réseau d'entreprises. Pour atteindre une qualité d'interopérabilité supérieure et garantir une collaboration efficace, un certain nombre d'Exigences d'Interopérabilité (EI) doivent être satisfaites. Ainsi, l'interopérabilité doit être vérifiée et continuellement améliorée. L'Analyse de l'Interopérabilité (ANIN) est une manière de vérifier l'interopérabilité des systèmes. Il a également été identifié que les interdépendances entre les EI ne sont pas explicitement définies. En effet, leurs interdépendances doivent être prises en compte car elles peuvent aider à identifier les impacts sur l'ensemble du système. De plus, la majorité des approches ANIN sont manuelles, ce qui est un processus laborieux et long qui dépend souvent des connaissances « subjectives » des experts. Science Research » (DSR) a été adoptée pour mener à bien la contribution proposée. Pour conceptualiser formellement les connaissances sur l'ANIN, en englobant l'ensemble des EI, les problèmes et solutions d'interopérabilité ainsi que leurs relations, nous avons proposé l'Ontologie de l'Analyse de l'Interopérabilité (OAI). Une approche d'Ingénierie Système basée sur des Modèles a été appliquée pour définir les concepts de l'ontologie. Un prototype du SAIC utilisant l'OAI comme modèle de connaissance a été développé sur une plate La contribution proposée a été évaluée grâce à une étude de cas basée sur une véritable entreprise en réseau
La subjectivité des estimations et des perceptions, la complexité de l'environnement, l'interaction entre sous-systèmes, le manque de données précises, les données manquantes, une faible capacité de traitement de l'information, et l'ambiguïté du langage naturel représentent les principales formes d'incertitude auxquelles les décideurs doivent faire face lorsqu'ils prennent des décisions stratégiques à l'aide de systèmes d'intelligence économique. Cette étude utilise un paradigme de « soft computing » pour identifier et analyser l'incertitude, que nous associons à la notion de facteurs de risque d'information. Les outils de la logique floue ont été également utilisés au niveau des ontologies ( « FuzzOntology » ). FuzzyMatch permet de réduire les problèmes de données manquantes. A l'aide de ces modèles, le processus de décision en intelligence économique bénéficie d'une réduction des risques liés à l'information lors du processus de recherche.
Nous nous intéressons aux conversations écrites en ligne orientées vers la résolution de problèmes. Nous cherchons à utiliser ces actes pour analyser les conversations écrites en ligne. Un cadre et des méthodes bien définies permettant une analyse fine de ce type de conversations en termes d'actes de dialogue représenteraient un socle solide sur lequel pourraient reposer différents systèmes liés à l'aide à la résolution des problèmes et à l'analyse des conversations écrites en ligne. Cependant, les techniques d'identification de la structure des conversations n'ont pas été développées autour des conversations écrites en ligne. Il est nécessaire d'adapter les ressources existantes pour ces conversations. Notre objectif est de modéliser les conversations écrites en ligne orientées vers la résolution de problèmes en termes d'actes de dialogue, et de proposer des outils pour la reconnaissance automatique de ces actes.
Dans le cadre de notre thèse, nous avons proposé une approche générique multilingue d'extraction automatique de connaissances. Nous avons validé l‟approche sur l'extraction des événements de variations des cours pétroliers et l‟extraction des expressions temporelles liées à des référentiels. Notre approche est basée sur la constitution de plusieurs cartes sémantiques par analyse des données non structurées afin de formaliser les traces linguistiques textuelles exprimées par des catégories d'un point de vue de fouille. Nous avons mis en place un système expert permettant d‟annoter la présence des catégories en utilisant des groupes de règles. Deux algorithmes d'annotation AnnotEV et AnnotEC ont été appliqués, dans la plateforme SemanTAS. Le rappel et précision de notre système d'annotation est autour de 80%. Nous avons présenté les résultats aussi sous forme des fiches de synthèses.
L'analyse morphologique automatique du slovaque constitue la première étape d'un système d'analyse automatique du contenu des textes scientifiques et techniques slovaques. Un tel système pourrait être utilisé par des applications, telles que l'indexation automatique des textes, la recherche automatique de la terminologie ou par un système de traduction. Une description des régularités de la langue par un ensemble de règles ainsi que l'utilisation de tous les éléments au niveau de la forme du mot qui rendent possible son interprétation permettent de réduire d'une manière considérable le volume des dictionnaires. Notamment s'il s'agit d'une langue à très riche flexion, comme le slovaque. Les résultats que nous obtenons lors de l'analyse morphologique confirment la faisabilité et la grande fiabilité d'une analyse morphologique basée sur la reconnaissance des formes et ceci pour toutes les catégories lexicales concernées par la flexion.
Ces dernières années, le piratage est devenu une industrie à part entière, augmentant le nombre et la diversité des cyberattaques. Les menaces qui pèsent sur les réseaux informatiques vont des logiciels malveillants aux attaques par déni de service, en passant par le phishing et l'ingénierie sociale. Un plan de cybersécurité efficace ne peut plus reposer uniquement sur des antivirus et des pare-feux pour contrer ces menaces : il doit inclure plusieurs niveaux de défense. Les systèmes de détection d'intrusion (IDS) réseaux sont un moyen complémentaire de renforcer la sécurité, avec la possibilité de surveiller les paquets de la couche 2 (liaison) à la couche 7 (application) du modèle OSI. Les techniques de détection d'intrusion sont traditionnellement divisées en deux catégories : la détection par signatures et la détection par anomalies. La plupart des IDS utilisés aujourd'hui reposent sur la détection par signatures ; ils ne peuvent cependant détecter que des attaques connues. Les IDS utilisant la détection par anomalies sont capables de détecter des attaques inconnues, mais sont malheureusement moins précis, ce qui génère un grand nombre de fausses alertes. Dans ce contexte, la création d'IDS précis par anomalies est d'un intérêt majeur pour pouvoir identifier des attaques encore inconnues. Dans cette thèse, les modèles d'apprentissage automatique sont étudiés pour créer des IDS qui peuvent être déployés dans de véritables réseaux informatiques. Tout d'abord, une méthode d'optimisation en trois étapes est proposée pour améliorer la qualité de la détection : 1/ augmentation des données pour rééquilibrer les jeux de données, 2/ optimisation des paramètres pour améliorer les performances du modèle et 3/ apprentissage ensembliste pour combiner les résultats des meilleurs modèles. Les flux détectés comme des attaques peuvent être analysés pour générer des signatures afin d'alimenter les bases de données d'IDS basées par signatures. Toutefois, cette méthode présente l'inconvénient d'exiger des jeux de données étiquetés, qui sont rarement disponibles dans des situations réelles. L'apprentissage par transfert est donc étudié afin d'entraîner des modèles d'apprentissage automatique sur de grands ensembles de données étiquetés, puis de les affiner sur le trafic normal du réseau à surveiller. Cette méthode présente également des défauts puisque les modèles apprennent à partir d'attaques déjà connues, et n'effectuent donc pas réellement de détection d'anomalies. C'est pourquoi une nouvelle solution basée sur l'apprentissage non supervisé est proposée. Elle utilise l'analyse de l'en-tête des protocoles réseau pour modéliser le comportement normal du trafic. Les anomalies détectées sont ensuite regroupées en attaques ou ignorées lorsqu'elles sont isolées. Enfin, la détection la congestion réseau est étudiée. Le taux d'utilisation de la bande passante entre les différents liens est prédit afin de corriger les problèmes avant qu'ils ne se produisent.
De nos jours, la musique est la plupart du temps diffusée sous forme enregistrée. Un système capable de transformer automatiquement une interprétation musicale en partition serait donc extrêmement bénéfique pour les interprètes et les musicologues. Cette tâche, appelée “automatic music transcription” (AMT), comprend plusieurs sous-tâches qui impliquent notamment la transformation de représentations intermédiaires comme le MIDI, quantifié ou non. Nous d´défendons dans cette thèse l'idée qu'un modèle robuste et bien structure des informations contenues dans une partition musicale aiderait au développement et à l'évaluation de systèmes AMT. En particulier, nous préconisons une s´séparation claire entre le contenu musical encodé dans la partition et l'ensemble des signes et des règles de notation utilisés pour représenter graphiquement ce contenu.
Le Web des données étend le Web en publiant des données structurées et liées en RDF. Un jeu de données RDF est un graphe orienté où les ressources peuvent être des sommets étiquetées dans des langues naturelles. Un des principaux défis est de découvrir les liens entre jeux de données RDF. Étant donnés deux jeux de données, cela consiste à trouver les ressources équivalentes et les lier avec des liens owl :  Cette thèse étudie l'efficacité des ressources linguistiques pour le liage des données exprimées dans différentes langues. Les étiquettes des sommets voisins constituent le contexte d'une ressource. Ceci peut être réalisé à l'aide de la traduction automatique ou de ressources lexicales multilingues. La similarité entre les documents est prise pour la similarité entre les ressources RDF.Nous évaluons expérimentalement différentes méthodes pour lier les données RDF. En particulier, deux stratégies sont explorées : l'application de la traduction automatique et l'usage des banques de données terminologiques et lexicales multilingues. Dans l'ensemble, l'évaluation montre l'efficacité de ce type d'approches. Les méthodes ont été évaluées sur les ressources en anglais, chinois, français, et allemand. L'évaluation montre que la méthode basée sur la similarité peut être appliquée avec succès sur les ressources RDF indépendamment de leur type (entités nommées ou concepts de dictionnaires).
Cette thèse traite de l'adaptation automatique des applications sensibles au contexte par l'utilisation d'informations liées à l'environnement social des utilisateurs afin d'enrichir le service rendu par les applications. Pour cela, notre contribution s'articule autour de la modélisation multidimensionnelle des différents niveaux de contextes sociaux, notamment le poids de la relation entre les acteurs. Plus spécifiquement, nous synthétisons des contextes sociaux non seulement liés à la familiarité mais aussi liés à la similitude des communautés statiques et dynamiques. Deux modèles basés respectivement sur les graphes et les ontologies sont proposés afin de satisfaire l'hétérogénéité des réseaux sociaux de la vie réelle. Nous utilisons les données réelles recueillies sur les réseautages sociaux en ligne pour conduire nos expérimentations et analysons les résultats en vérifiant l'efficacité de ces modèles. En parallèle nous traitons le point de vue de l'application, et nous présentons deux algorithmes utilisant des contextes sociaux pour améliorer la stratégie de transmission des données dans le réseau opportuniste, et particulièrement la contre-mesure aux nœuds égoïstes. Les simulations des scénarios réels confirment les avantages liés à l'introduction des contextes sociaux, en termes de taux de succès et de délais de transmission. Nous effectuons une comparaison avec d'autres algorithmes de transmission traditionnellement décrits dans la littérature pour compléter notre démonstration
Il est communément admis que 70 % des coûts du cycle de vie d'un produit sont engagés dès la phase de spécification. Pour aider les sous-traitants, nous proposons une méthode outillée de synthèse des exigences, laquelle est supportée par un environnement numérique basé sur les sciences des données. Des modèles de classification extraient les exigences des documents. Les exigences sont ensuite analysées au moyen des techniques de traitement du langage naturel afin d'identifier les défauts de qualité qui mettent en péril le reste du cycle de vie. La validation théorique et empirique de notre proposition corrobore l'hypothèse que les sciences des données est un moyen de synthétiser plusieurs centaines ou milliers d'exigences.
Les maladies cardiovasculaires (MCVs) et les cancers constituent la première et la seconde cause de mortalité et de morbidité en France chez les hommes et les femmes, et leur coût annuel est important. La prévention de ces maladies chroniques constitue, avec leur dépistage précoce et leur prise en charge rapide et efficace, un moyen possible de réduire ce coût. Nutrition Santé (PNNS) a été mis en place en France pour aider les Français à avoir une meilleure alimentation, afin de contribuer à réduire l'incidence de ces maladies. L'objectif de cette thèse est la construction d'un système de suggestions personnalisées fondé sur le profil de l'individu et son risque cardiovasculaire. Cette approche nécessite la mise en place d'une démarche interdisciplinaire faisant collaborer des chercheurs dans les domaines de l'informatique, de l'épidémiologie et de la nutrition. L'importance de cette collaboration se justifie par le besoin de produire des suggestions étayées par des recherches attestées dans ces domaines. Le premier apport de cette thèse est l'intégration des technologies du web sémantique dans une nouvelle approche d'évaluation du risque cardiovasculaire qui prend en compte les interactions entre ces facteurs. La création de l'outil de visualisation MCVGraphViz a permis de mettre en oeuvre cette stratégie. Le deuxième apport consiste à proposer une solution pour exploiter les connaissances présentes dans les plans de santé et les recommandations concernant la prévention des maladies cardiovasculaires en France. Ainsi, nous avons opté pour une approche modulaire intégrée dans l'outil MCVGraphViz qui permet de produire des recommandations (alimentation,activité physique, etc.) fondées sur le risque cardiovasculaire évalué et le profil de l'individu (préférences sensorielles, contraintes allergiques, capacité physique, etc.). Le troisième apport concerne la qualification nutritionnelle des recettes de cuisine pour un meilleur suivi : l'approche s'appuie sur des techniques de traitement automatique de la langue et des raisonnements ontologiques pour qualifier d'un point de vue nutritionnel des recettes de cuisine. De nombreuses perspectives sont exposées. La plupart d'entre elles visent à améliorer les systèmes de recommandation et l'expressivité de la base de connaissances sur les maladies cardiovasculaires.
Cette recherche est ancrée à la fois dans la statistique textuelle, et secondairement dans l'analyse du discours, avec une complémentarité quantitatif-qualitatif. L'état de la question présente une vue d'ensemble des recherches linguistiques sur la publicité – linguistique descriptive, analyse du discours, sémiologie/sémiotique et rhétorique/stylistique, en prenant pour point de départ le travail pionnier en linguistique descriptive de Leech (TheLanguage of Advertising, 1966) – et révèle plusieurs constats :  1. les mots de la publicité constituent bien un discours spécialisé 2. la publicité ressemble à la fois à la poésie et à la langue courante 3. l'intertextualité et la participation active d'un récepteur ciblé sont deux données clés de la communication publicitaire 4. la langue publicitaire est émotionnelle, ambiguë, indirecte et implicite 5. de façon simpliste et ludique, la publicité nous dépeint un monde de bonheur parfait 6. simplicité et conventionnalisation du discours publicitaire, et invariants lexicaux. Pour répondre à certaines questions sur le discours, la langue, la communication et la traduction publicitaires, nous avons choisi une approche quantitative et l'approche lexicométrique (ou statistique textuelle, devenue récemment textométrie), développée notamment par le « groupe de Saint-Cloud » et ses collaborateurs, dont André Salem (concepteur de Lexico, le logiciel utilisé pour nos analyses). L'analyse lexicométrique est complétée par l'analyse factorielle des correspondances (AFC) et la méthode de projection géodésique de Viprey (avec le logiciel Astadiag). Grâce à un corpus parallèle (bitexte) d'environ 800 000 occurrences en anglais et un million en français (170 brochures publicitaires compilées à partir de sites Web canadiens de 20 marques, 10 constructeurs, 229 véhicules au total), nous avons réussi à déterminer un profil lexical distinct pour chacun des sept segments du marché automobile : AB, CD, EF, MPV (véhicules multifonctions), PK (pickups/camions), SP (voitures de sport) et SUV (VUS : véhicules utilitaires sports) et montré que dans l'ensemble, les publicités sont traduites assez littéralement.
Leur comportements sont adaptés à chaque cas d'usage par des experts. Permettre au grand public d'enseigner de nouveaux comportements pourrait mener à une meilleure adaptation à moindre coût. Les domiciles sont des mondes ouverts qui ne peuvent pas être prédéterminés. Pepper doit donc, en plus d'apprendre de nouveaux comportements, être capable de découvrir son environnement, et de s'y rendre utile ou de divertir : c'est un scénario riche. L'enseignement de comportements que nous démontrons s'effectue donc dans ces conditions uniques : par le seul langage parlé, dans des scénarios riches et ouverts, et sur un robot Pepper standard. Grâce à la transcription automatique de la parole et au traitement automatique du langage, notre système reconnaît les enseignements de comportement que nous n'avions pas prédéterminés. Les nouveaux comportements peuvent solliciter des entités qui auraient été appris dans d'autres contextes, pour les accepter et s'en servir comme paramètres. Par des expériences de complexité croissante, nous montrons que des conflits entre les comportements apparaissent dans les scénarios riches, et proposons de les résoudre à l'aide de planification de tâche et de règles de priorités.
Le développement de systèmes en traitement automatique des langues (TAL) nécessite de déterminer la qualité de ce qui est produit. Que ce soit pour comparer plusieurs systèmes entre eux ou identifier les points forts et faibles d'un système isolé, l'évaluation suppose de définir avec précision et pour chaque contexte particulier une méthodologie, un protocole, des ressources linguistiques (les données nécessaires à l'apprentissage et au test des systèmes) ou encore des mesures et métriques d'évaluation. C'est à cette condition que l'amélioration des systèmes est possible afin d'obtenir des résultats plus fiables et plus exploitables à l'usage. L'apport de l'évaluation en TAL est important avec la création de nouvelles ressources linguistiques, l'homogénéisation des formats des données utilisées ou la promotion d'une technologie ou d'un système. Nous avons cherché à réduire et à encadrer ces interventions manuelles. Pour ce faire, nous appuyons nos travaux sur la conduite ou la participation à des campagnes d'évaluation comparant des systèmes entre eux, ou l'évaluation de systèmes isolés. Nous avons formalisé la gestion du déroulement de l'évaluation et listé ses différentes phases pour définir un cadre d'évaluation commun, compréhensible par tous. Le point phare de ces phases d'évaluation concerne la mesure de la qualité via l'utilisation de métriques. Cela a imposé trois études successives sur les mesures humaines, les mesures automatiques et les moyens d'automatiser le calcul de la qualité et enfin la méta-évaluation des mesures qui permet d'en évaluer la fiabilité. Dans ce contexte, l'étude des similarités entre les technologies et entre leurs évaluations nous a permis d'observer les points communs et de les hiérarchiser. Nous avons montré qu'un petit ensemble de mesures permet de couvrir une large palette d'applications à des technologies distinctes. Notre objectif final était de définir une architecture d'évaluation générique, c'est-à-dire adaptable à tout type de technologie du TAL, et pérenne, c'est-à-dire permettant la réutilisation de ressources linguistiques, mesures ou méthodes au cours du temps. Notre proposition se fait à partir des conclusions des étapes précédentes afin d'intégrer les phases d'évaluation à notre architecture et d'y incorporer les mesures d'évaluation, sans oublier la place relative à l'utilisation de ressources linguistiques. La définition de cette architecture s'est effectuée en vue d'automatiser entièrement la gestion des évaluations, que ce soit pour une campagne d'évaluation ou l'évaluation d'un système isolé.
Du point de vue du traitement automatique des langues (TAL), l'extraction des événements dans les textes est la forme la plus complexe des processus d'extraction d'information, qui recouvrent de façon plus générale l'extraction des entités nommées et des relations qui les lient dans les textes. Le cas des événements est particulièrement ardu car un événement peut être assimilé à une relation n-aire ou à une configuration de relations. De ce fait, l'adaptation à un nouveau domaine constitue un défi supplémentaire. Cette thèse présente plusieurs stratégies pour améliorer la performance d'un système d'extraction d'événements en utilisant des approches fondées sur les réseaux de neurones et en exploitant les propriétés morphologiques, syntaxiques et sémantiques des plongements de mots. Ceux-ci ont en effet l'avantage de ne pas nécessiter une modélisation a priori des connaissances du domaine et de générer automatiquement un ensemble de traits beaucoup plus vaste pour apprendre un modèle. Nous avons proposé plus spécifiquement différents modèles d'apprentissage profond pour les deux sous-tâches liées à l'extraction d'événements : la détection d'événements et la détection d'arguments. La détection d'événements est considérée comme une sous-tâche importante de l'extraction d'événements dans la mesure où la détection d'arguments est très directement dépendante de son résultat. En préalable à l'introduction de nos nouveaux modèles, nous commençons par présenter en détail le modèle de l'état de l'art qui en constitue la base. Des expériences approfondies sont menées sur l'utilisation de différents types de plongements de mots et sur l'influence des différents hyperparamètres du modèle en nous appuyant sur le cadre d'évaluation ACE 2005, standard d'évaluation pour cette tâche. Nous proposons ensuite deux nouveaux modèles permettant d'améliorer un système de détection d'événements. L'un permet d'augmenter le contexte pris en compte lors de la prédiction d'une instance d'événement (déclencheur d'événement) en utilisant un contexte phrastique, tandis que l'autre exploite la structure interne des mots en profitant de connaissances morphologiques en apparence moins nécessaires mais dans les faits importantes. Nous proposons enfin de reconsidérer la détection des arguments comme une extraction de relation d'ordre supérieur et nous analysons la dépendance de cette détection vis-à-vis de la détection d'événements.
Notre travail concerne les systèmes d'aide à la visite de musée et l'accès au patrimoine culturel. L'objectif est de concevoir des systèmes de recommandation, implémentés sur dispositifs mobiles, pour améliorer l'expérience du visiteur, en lui recommandant les items les plus pertinents et en l'aidant à personnaliser son parcours. Nous considérons essentiellement deux terrains d'application : la visite de musées et le tourisme. Nous proposons une approche de recommandation hybride et sensible au contexte qui utilise trois méthodes différentes : démographique, sémantique et collaborative. Chaque méthode est adaptée à une étape spécifique de la visite de musée. L'approche démographique est tout d'abord utilisée afin de résoudre le problème du démarrage à froid. L'approche sémantique est ensuite activée pour recommander à l'utilisateur des œuvres sémantiquement proches de celles qu'il a appréciées. Enfin l'approche collaborative est utilisée pour recommander à l'utilisateur des œuvres que les utilisateurs qui lui sont similaires ont aimées. La prise en compte du contexte de l'utilisateur se fait à l'aide d'un post-filtrage contextuel, qui permet la génération d'un parcours personnalisé dépendant des œuvres qui ont été recommandées et qui prend en compte des informations contextuelles de l'utilisateur à savoir : l'environnement physique, la localisation ainsi que le temps de visite. Dans le domaine du tourisme, les points d'intérêt à recommander peuvent être de différents types (monument, parc, musée, etc.). La nature hétérogène de ces points d'intérêt nous a poussé à proposer un système de recommandation composite. Chaque recommandation est une liste de points d'intérêt, organisés sous forme de packages, pouvant constituer un parcours de l'utilisateur. L'objectif est alors de recommander les Top-k packages parmi ceux qui satisfont les contraintes de l'utilisateur (temps et coût de visite par exemple). L'évaluation expérimentale du système que nous avons proposé, en utilisant un data-set réel extrait de Tripadvisor démontre sa qualité et sa capacité à améliorer à la fois la précision et la diversité des recommandations.
L'évolution des technologies de l'information et de la communication (TIC) a permis le développement du travail collaboratif dans quasiment tous les secteurs de l'activité humaine. Pour assurer la performance du collectif et minimiser le risque d'erreurs, il est crucial que les individus qui collaborent partagent une même représentation de la situation dans laquelle ils sont engagés. L'avancée de l'étude de la cognition collective, cœur du travail collaboratif, est porteuse d'un potentiel certain qui doit se traduire par des applications concrètes au service de l'optimisation de la gestion et de la réalisation des tâches collectives. L'évaluation en temps réel de la cognition des individus et des équipes permet d'envisager des outils et des systèmes adaptatifs pour gagner en efficacité, en performance et en agilité. Face à ces enjeux, notre objectif, sur commande de la DGA, est de trouver des mesures appropriées qui permettraient une évaluation de la dynamique du partage des consciences de situation, dans le contexte très contraignant des salles de commandement et de contrôle, qui nécessite la plus faible instrumentation possible des opérateurs. Notre contribution au domaine est double. D'une part nous proposons le concept de synchronie des consciences de situation, pour soutenir le développement théorique de l'étude de la dynamique de partage de conscience de situation. Ce travail de doctorat se présente comme une mise en avant de l'intérêt et de l'applicabilité de systèmes d'évaluation du partage de cognition en environnement de travail collaboratif réaliste, et s'accompagne de propositions concernant le futur de la recherche sur le C2.
Dans le cadre de la lutte contre le réchauffement climatique, plusieurs pays du monde notamment le Canada et certains pays européens dont la France, ont établi des mesures afin de réduire les nuisances environnementales. L'un des axes majeurs abordés par les états concerne le secteur du transport et plus particulièrement le développement des systèmes de transport en commun en vue de réduire l'utilisation de la voiture personnelle et les émissions de gaz à effet de serre. A cette fin, les collectivités concernées visent à mettre en place des systèmes de transports urbains plus accessibles, propres et durables. Cette thèse s'inscrit dans un contexte général de valorisation des traces numériques et d'essor du domaine de la science des données (e.g., collecte et stockage des données, développement de méthodes d'apprentissage automatique, etc.). Les travaux comportent trois volets principaux à savoir (i) la prévision long terme de l'affluence des passagers à l'aide de base de données événementielles et de données billettiques, (ii) la prévision court terme de l'affluence des passagers et (iii) la visualisation de l'affluence des passagers dans les transports en commun. Les recherches se basent principalement sur l'utilisation de données billettiques fournies par les opérateurs de transports et ont été menées sur trois cas d'études réels, le réseau de métro et de bus de la ville de Rennes, le réseau ferré et de tramway du quartier d'affaire de la Défense à Paris en France, et le réseau de métro de Montréal, Québec au Canada
La sélection de caractéristiques acoustiques appropriées est essentielle dans tout système de traitement de la parole. Pendant près de 40 ans, la parole a été généralement considérée comme une séquence de signaux quasi-stables (voyelles) séparés par des transitions (consonnes). Bien qu‟un grand nombre d'études documentent clairement l'importance de la coarticulation, et révèlent que les cibles articulatoires et acoustiques ne sont pas indépendantes du contexte, l‟hypothèse que chaque voyelle présente une cible acoustique qui peut être spécifiée d'une manière indépendante du contexte reste très répandue. Ce point de vue implique des limitations fortes. Il est bien connu que les fréquences de formants sont des caractéristiques acoustiques qui présentent un lien évident avec la production de la parole, et qui peuvent participer à la distinction perceptive entre les voyelles. Par conséquent, les voyelles sont généralement décrites avec des configurations articulatoires statiques représentées par des cibles dans l'espace acoustique, généralement par les fréquences des formants correspondants, représentées dans les plans F1-F2 et F2-F3. Les consonnes occlusives peuvent être décrites en termes de point d'articulation, représentés par locus (ou locus équations) dans le plan acoustique. Mais les trajectoires des fréquences de formants dans la parole fluide présentent rarement un état d'équilibre pour chaque voyelle. Elles varient avec le locuteur, l'environnement consonantique (co-articulation) et le débit de parole (relative à un continuum entre hypo et hyper-articulation).
Le contenu généré dans les médias sociaux comme Twitter permet aux utilisateurs d'avoir un aperçu rétrospectif d'évènement et de suivre les nouveaux développements dès qu'ils se produisent. Cependant, bien que Twitter soit une source d'information importante, il est caractérisé par le volume et la vélocité des informations publiées qui rendent difficile le suivi de l'évolution des évènements. L'objectif de cette thèse est de faciliter le suivi d'événement, en fournissant des outils de génération de synthèse adaptés à ce vecteur d'information. Les défis majeurs sous-jacents à notre problématique découlent d'une part du volume, de la vélocité et de la variété des contenus publiés et, d'autre part, de la qualité des tweets qui peut varier d'une manière considérable. La tâche principale dans la notification prospective est l'identification en temps réel des tweets pertinents et non redondants. Le système peut choisir de retourner les nouveaux tweets dès leurs détections où bien de différer leur envoi afin de s'assurer de leur qualité. L'intuition sous-jacente à notre proposition est que la mesure de similarité à base de word embedding est capable de considérer des mots différents ayant la même sémantique ce qui permet de compenser le non-appariement des termes lors du calcul de la pertinence. Deuxièmement, l'estimation de nouveauté d'un tweet entrant est basée sur la comparaison de ses termes avec les termes des tweets déjà envoyés au lieu d'utiliser la comparaison tweet à tweet. Cette méthode offre un meilleur passage à l'échelle et permet de réduire le temps d'exécution. Troisièmement, pour contourner le problème du seuillage de pertinence, nous utilisons un classificateur binaire qui prédit la pertinence. L'approche proposée est basée sur l'apprentissage supervisé adaptatif dans laquelle les signes sociaux sont combinés avec les autres facteurs de pertinence dépendants de la requête. De plus, le retour des jugements de pertinence est exploité pour re-entrainer le modèle de classification. Enfin, nous montrons que l'approche proposée, qui envoie les notifications en temps réel, permet d'obtenir des performances prometteuses en termes de qualité (pertinence et nouveauté) avec une faible latence alors que les approches de l'état de l'art tendent à favoriser la qualité au détriment de la latence. Cette thèse explore également une nouvelle approche de génération du résumé rétrospectif qui suit un paradigme différent de la majorité des méthodes de l'état de l'art. Nous proposons de modéliser le processus de génération de synthèse sous forme d'un problème d'optimisation linéaire qui prend en compte la diversité temporelle des tweets. Les tweets sont filtrés et regroupés d'une manière incrémentale en deux partitions basées respectivement sur la similarité du contenu et le temps de publication. Nous formulons la génération du résumé comme étant un problème linéaire entier dans lequel les variables inconnues sont binaires, la fonction objective est à maximiser et les contraintes assurent qu'au maximum un tweet par cluster est sélectionné dans la limite de la longueur du résumé fixée préalablement.
Durant les dernières décennies, l'animation 3D s'est largement intégrée à notre vie quotidienne, que cela soit dans le domaine du jeu vidéo, du cinéma ou du divertissement plus généralement. Malgré son utilisation rependue, la création d'animation reste réservés à des animateurs expérimentés et n'est pas à la portée de novices. Par ailleurs, le processus de création d'animation doit respecter une pipeline très stricte : après une première phase de storyboarding et de design des personnages, ceux-ci sont ensuite modélisés, riggés et grossièrement positionnés dans l'espace 3D permettant de créer un premier brouillon d'animation. Les actions et mouvements des personnages sont ensuite décomposés en keyframes interpolées, constituant l'animation finale. Le processus de keyframing est difficile et nécessite, de la part des animateurs, beaucoup de temps et d'effort, notamment pour la retouche de chaque courbe d'animation liée à un degré de liberté d'un personnage, et ceci pour chaque action. Ces dernières doivent aussi être correctement séquencées au cours du temps afin de transmettre les intentions des personnages et leur personnalité. Certaines méthodes telle que la motion capture rendent le processus de création plus aisé en transférant le mouvement de véritables acteurs aux mouvements de personnages virtuels. Cependant, ces animations transférées manquent souvent d'expressivité et doivent être rectifiées par des animateurs. Dans cette thèse, nous introduisons une nouvelle méthode permettant de créer aisément des séquences d'animation 3D à partir d'une base de donnée d'animations individuelles comme point de départ. En particulier, notre travail se concentre dans l'animation de personnages virtuels reproduisant une histoire jouée avec des objets physiques tels que des figurines instrumentées par des capteurs. Nous présentons un descripteur de mouvement invariant par translation, rotation et passage à l'échelle permettant à notre système de reconnaître des actions exécutées par l'utilisateur ;  Nous introduisons un nouveau modèle d'animation procédural inférant l'expressivité de la qualité de mouvement de la main en tant qu'Effort de la Laban Temps et Poids. Enfin, nous étendons le système afin de permettre la manipulation de plusieurs personnages en même temps, en détectant et transférant des interactions entre personnages tout en étant fidèle aux qualités de mouvement du narrateur et permettant aux personnages d'agir selon des comportements prédéfinis, laissant l'utilisateur exprimer sa créativité. Nous concluons avec une discussion sur les futures directions de recherche.
Les réseaux sociaux occupent une place de plus en plus importante dans notre vie quotidienne et représentent une part considérable des activités sur le web. Ce succès s'explique par la diversité des services/fonctionnalités de chaque site (partage des données souvent multimédias, tagging, blogging, suggestion de contacts, etc.) incitant les utilisateurs à s'inscrire sur différents sites et ainsi à créer plusieurs réseaux sociaux pour diverses raisons (professionnelle, privée, etc.). Cependant, les outils et les sites existants proposent des fonctionnalités limitées pour identifier et organiser les types de relations ne permettant pas de, entre autres, garantir la confidentialité des utilisateurs et fournir un partage plus fin des données. Particulièrement, aucun site actuel ne propose une solution permettant d'identifier automatiquement les types de relations en tenant compte de toutes les données personnelles et/ou celles publiées. Dans cette étude, nous proposons une nouvelle approche permettant d'identifier les types de relations à travers un ou plusieurs réseaux sociaux. Notre approche est basée sur un framework orientéutilisateur qui utilise plusieurs attributs du profil utilisateur (nom, age, adresse, photos, etc.). Pour cela, nous utilisons des règles qui s'appliquent à deux niveaux de granularité : 1) au sein d'un même réseau social pour déterminer les relations sociales (collègues, parents, amis, etc.) en exploitant principalement les caractéristiques des photos et leurs métadonnées, et, 2) à travers différents réseaux sociaux pour déterminer les utilisateurs co-référents (même personne sur plusieurs réseaux sociaux) en étant capable de considérer tous les attributs du profil auxquels des poids sont associés selon le profil de l'utilisateur et le contenu du réseau social. À chaque niveau de granularité, nous appliquons des règles de base et des règles dérivées pour identifier différents types de relations. Nous mettons en avant deux méthodologies distinctes pour générer les règles de base. Pour les relations sociales, les règles de base sont créées à partir d'un jeu de données de photos créées en utilisant le crowdsourcing. Pour les relations de co-référents, en utilisant tous les attributs, les règles de base sont générées à partir des paires de profils ayant des identifiants de mêmes valeurs. Quant aux règles dérivées, nous utilisons une technique de fouille de données qui prend en compte le contexte de chaque utilisateur en identifiant les règles de base fréquemment utilisées. Nous présentons notre prototype, intitulé RelTypeFinder, que nous avons implémenté afin de valider notre approche. Ce prototype permet de découvrir différents types de relations, générer des jeux de données synthétiques, collecter des données du web, et de générer les règles d'extraction. Nous décrivons les expériementations que nous avons menées sur des jeux de données réelles et syntéthiques. Les résultats montrent l'efficacité de notre approche à découvrir les types de relations.
Les systèmes collaboratifs à large échelle, où un grand nombre d'utilisateurs collaborent pour réaliser une tâche partagée, attirent beaucoup l'attention des milieux industriels et académiques. Dans cette thèse, nous étudions le problème de l'évaluation de la confiance et cherchons à concevoir un modèle de confiance informatique dédiés aux systèmes collaboratifs. Nos travaux s'organisent autour des trois questions de recherche suivantes. 1. Quel est l'effet du déploiement d'un modèle de confiance et de la représentation aux utilisateurs des scores obtenus pour chaque partenaire ? Nous avons conçu et organisé une expérience utilisateur basée sur le jeu de confiance qui est un protocole d'échange d'argent en environnement contrôlé dans lequel nous avons introduit des notes deconfiance pour les utilisateurs. L'analyse détaillée du comportement des utilisateurs montre que : (i) la présentation d'un score de confiance aux utilisateurs encourage la collaboration entre eux de manière significative, et ce, à un niveau similaire à celui de l'affichage du surnom des participants, et (ii) les utilisateurs se conforment au score de confiance dans leur prise de décision concernant l'échange monétaire. Les résultats suggèrent donc qu'un modèle de confiance peut être déployé dans les systèmes collaboratifs afin d'assister les utilisateurs. 2. Comment calculer le score de confiance entre des utilisateurs qui ont déjà collaboré ? Nous avons conçu un modèle de confiance pour les jeux de confiance répétés qui calcule les scores de confiance des utilisateurs en fonction de leur comportement passé. Nous avons validé notre modèle de confiance en relativement à : (i) des données simulées, (ii) de l'opinion humaine et (iii) des données expérimentales réelles. Nous avons appliqué notre modèle de confiance à Wikipédia en utilisant la qualité des articles de Wikipédia comme mesure de contribution. Nous avons proposé trois algorithmes d'apprentissage automatique pour évaluer la qualité des articles de Wikipédia : l'un est basé sur une forêt d'arbres décisionnels tandis que les deux autres sont basés sur des méthodes d'apprentissage profond. 3. Comment prédire la relation de confiance entre des utilisateurs qui n'ont pas encore interagi ? Etant donné un réseau dans lequel les liens représentent les relations de confiance/défiance entre utilisateurs, nous cherchons à prévoir les relations futures. Nous avons proposé un algorithme qui prend en compte les informations temporelles relatives à l'établissement des liens dans le réseau pour prédire la relation future de confiance/défiance des utilisateurs. L'algorithme proposé surpasse les approches de la littérature pour des jeux de données réels provenant de réseaux sociaux dirigés et signés
Face au nombre croissant d'utilisateurs novices d'applications informatiques, le besoin d'une assistance efficace est devenu crucial. Nous proposons d'y répondre à l'aide d'un Agent Conversationnel Assistant (ACA), interface permettant l'usage de la langue naturelle (spontanément employée en cas de problème) et fournissant une présence rassurant les utilisateurs. Une étude préalable décrit la constitution (combinant recueil et utlilisation de thésaurus) d'un corpus de requêtes, dont le besoin est justifié. Ce corpus de 11626 phrases est contrasté à d'autres, et nous montrons qu'il couvre le domaine de l'assistance et contient en plus des requêtes relevant du contrôle de l'application et de discussion avec l'agent. En sortie, les requêtes sont exprimées dans un langage formel (DAFI) dont nous donnons la syntaxe et la sémantique. La chaîne de traitement est évaluée en comparant annotation manuelle et requêtes produites automatiquement, et l'emploi de méthodes d'apprentissage supervisé pour identifier les activités conversationnelles des requêtes est envisagé. L'approche suivie est validée par l'intégration d'un ACA au sein d'une application Web de conception musicale collaborative. Enfin, nous nous intéressons à l'architecture requise pour l'ageTIlt rationnel en charge de définir les réactions à partir des requêtes formelles en DAFT et du modèle de l'application, soulignant le besoin pour celui-ci de disposer d'un modèle cognitif
Ce travail montre que toute modélisation est réductrice de sens. Le moyen employé est la réalisation d'une base de données factuelle et exhaustive d'après des accords d'entreprise. Dans la première partie de ce texte, les notions de documentation et de modélisation sont discutées. Ensuite, les moyens informatiques disponibles pour la réalisation d'applications documentaires sont présentés (systèmes de gestion de bases de données relationnelle et objet, langages de marquage, hypertexte, traitement automatique des langues). Enfin, des applications documentaires actuelles sont présentées. La deuxième partie de ce texte rend compte des tentatives de modélisation qui ont été réalisées. La description des cinq modèles mis au point est précédée par une présentation des accords d'entreprise et par une étude des problèmes structurels inhérents à ces documents. La troisième partie de ce travail décrit les objectifs et la mise en place de la base ACCORD réalisée à l'aide de PostGres et de HTML. Enfin, des éléments méthodologiques généraux pour le traitement de corpus de documents administratifs ou légaux sont dégagés. La conclusion souligne l'intérêt d'un travail collaboratif pour la mise en place de telles applications et sur les transformations dans les pratiques documentaires découlant de l'utilisation des réseaux informatiques.
De plus en plus de médias dans le monde disposent de rubriques ou chroniques dédiées au fact-checking. Elles visent notamment à vérifier la véracité de propos tenus par des responsables politiques. Cette pratique revisite celle née aux États-Unis dans les années 1920, qui consistait à vérifier de manière exhaustive et systématique les contenus avant parution. Ce fact-checking « moderne » incarne une stratégie des rédactions web – en dépit des crises structurelles et conjoncturelles – pour renouer avec la diffusion de contenus mieux vérifiés, ainsi que leur capacité à mettre à profit les outils numériques qui facilitent l'accès à l'information. À travers une trentaine d'entretiens semi-directifs avec des fact-checkeurs français et l'étude de 300 articles et chroniques issus de sept médias différents, ce travail de recherche analyse dans quelle mesure le fact-checking, en tant que genre journalistique, valorise une démarche crédible, mais révèle aussi, en creux, des manquements dans les pratiques professionnelles. Il examine, enfin, comment la promotion de contenus plus qualitatifs et l'éducation aux médias sont de nature à placer le fact-checking au cœur des stratégies éditoriales, destinées à regagner la confiance des publics.
Depuis plusieurs années, un nouveau phénomène lié aux données numériques émerge : des données de plus en plus volumineuses, variées et véloces, apparaissent et sont désormais disponibles, elles sont souvent qualifiées de données complexes. Dans cette thèse, nous focalisons sur un type particulier de données complexes : les séquences complexes d'événements, en posant la question suivante : “comment prédire au plus tôt et influencer l'apparition des événements futurs dans une séquence complexe d'événements ? Nous proposons un algorithme de fouille de règles d'épisode DEER qui a l'originalité de maîtriser l'horizon d'apparition des événements futurs à travers d'une distance imposée au sein de règles extraites. Dans un deuxième temps, nous focalisons sur la détection de l'émergence dans un flux d'événements. Nous proposons l'algorithme EER pour la détection au plus tôt de l'émergence de nouvelles règles. Pour augmenter la fiabilité de nouvelles règles lorsque leur support est très faible, EER s'appuie sur la similarité entre ces règles et les règles déjà connues. Enfin, nous étudions l'impact porté par des événements sur d'autres dans une séquence d'événements. Nous proposons l'algorithme IE qui introduit la notion des “événements influenceurs” et étudie l'influence sur le support, la confiance et la distance à travers de trois mesures d'influence proposées. Ces travaux sont évalués et validés par une étude expérimentale menée sur un corpus de données réelles issues de blogs
Les êtres humains définissent naturellement leur espace quotidien en unités discrètes. Par exemple, nous sommes capables d'identifier le lieu où nous sommes (e.g. Les travaux récents en reconnaissance de lieux sémantiques, visent à doter les robots de capacités similaires. Nous présentons nos travaux dans le domaine de la reconnaissance de lieux sémantiques. Premièrement, ils combinent la caractérisation globale d'une image, intéressante car elle permet de s'affranchir des variations locales de l'apparence des lieux, et les méthodes basées sur les mots visuels, qui reposent sur la classification non-supervisée de descripteurs locaux. Deuxièmement, et de manière intimement reliée, ils tirent parti du flux d'images fourni par le robot en utilisant des méthodes bayésiennes d'intégration temporelle. Dans un premier modèle, nous ne tenons pas compte de l'ordre des images. Le mécanisme d'intégration est donc particulièrement simple mais montre des difficultés à repérer les changements de lieux. Une deuxième version enrichit le formalisme classique du filtrage bayésien en utilisant l'ordre local d'apparition des images. Nous comparons nos méthodes à l'état de l'art sur des tâches de reconnaissance d'instances et de catégorisation, en utilisant plusieurs bases de données. Nous étudions l'influence des paramètres sur les performances et comparons les différents types de codage employés sur une même base. Ces expériences montrent que nos méthodes sont supérieures à l'état de l'art, en particulier sur les tâches de catégorisation.
La Variabilité dans le Big Data se réfère aux données dont la signification change de manière continue. Par exemple, les données des plateformes sociales et les données des applications de surveillance, présentent une grande variabilité. Afin de réaliser cet objectif, les data scientists ont besoin (a) de mesures de comparaison de données pour différentes dimensions telles que l'âge pour les utilisateurs et le sujet pour le traffic réseau, et (b) d'algorithmes efficaces pour la détection de différences à grande échelle. Nous proposons des mesures adaptées à la comparaison de distributions de notes attribuées par les utilisateurs, et des algorithmes efficaces qui permettent, à partir d'une opinion donnée, de trouver les segments qui sont d'accord ou pas avec cette opinion. L'Explication des Différences s'intéresse à fournir une explication succinte de la différence entre deux ensembles de données (ex., les habitudes d'achat de deux ensembles de clients). Nous proposons des fonctions de scoring permettant d'ordonner les explications, et des algorithmes qui guarantissent de fournir des explications à la fois concises et informatives. Enfin, l'Evolution des Différences suit l'évolution d'un ensemble de données dans le temps et résume cette évolution à différentes granularités de temps. Nous proposons une approche basée sur le requêtage qui utilise des mesures de similarité pour comparer des clusters consécutifs dans le temps. Nos index et algorithmes pour l'Evolution des Différences sont capables de traiter des données qui arrivent à différentes vitesses et des types de changements différents (ex., soudains, incrémentaux). L'utilité et le passage à l'échelle de tous nos algorithmes reposent sur l'exploitation de la hiérarchie dans les données (ex., temporelle, démographique).Afin de valider l'utilité de nos tâches analytiques et le passage à l'échelle de nos algorithmes, nous réalisons un grand nombre d'expériences aussi bien sur des données synthétiques que réelles. Nous montrons que l'Exploration des Différences guide les data scientists ainsi que les novices à découvrir l'opinion de plusieurs segments d'internautes à grande échelle. L'Explication des Différences révèle la nécessité de résumer les différences entre deux ensembles de donnes, de manière parcimonieuse et montre que la parcimonie peut être atteinte en exploitant les relations hiérarchiques dans les données. Enfin, notre étude sur l'Evolution des Différences fournit des preuves solides qu'une approche basée sur les requêtes est très adaptée à capturer des taux d'arrivée des données variés à plusieurs granularités de temps. De même, nous montrons que les approches de clustering sont adaptées à différents types de changement.
Les humanités défient les capacités du numérique depuis 60 ans. Les années 90 marquent une rupture, énonçant l'espoir d'une interprétation qualitative automatique de données interopérables devenues « connaissances » . Depuis 2010, une vague de désillusion ternit ces perspectives, le foisonnement des humanités numériques s'intensifie. Cette méthode vise à co-créer des connaissances historiques. À l'utopie d'une modélisation des connaissances qualitatives de l'historien, nous préférons l'heuristique pragmatique : l'interprétation de quantifications du corpus suscite l'émergence de nouvelles certitudes et hypothèses.
La plupart des méthodes de traitement automatique des langues (TAL) peuvent être formalisées comme des problèmes de prédiction, dans lesquels on cherche à choisir automatiquement l'hypothèse la plus plausible parmi un très grand nombre de candidats. Dans ce travail, nous nous intéressons à l'importance du design de l'espace de recherche et étudions l'utilisation de contraintes pour en réduire la taille et la complexité. Une étude de cas sur les modèles exponentiels pour l'analyse morpho-syntaxique montre paradoxalement que cela peut conduire à d'importantes dégradations des résultats, et cela même quand les contraintes associées sont pertinentes. Parallèlement, nous considérons l'utilisation de ce type de contraintes pour généraliser le problème de l'apprentissage supervisé au cas où l'on ne dispose que d'informations partielles et incomplètes lors de l'apprentissage, qui apparaît par exemple lors du transfert cross-lingue d'annotations. Nous étudions deux méthodes d'apprentissage faiblement supervisé, que nous formalisons dans le cadre de l'apprentissage ambigu, appliquées à l'analyse morpho-syntaxiques de langues peu dotées en ressources linguistiques. En effet, il n'est pas possible de considérer l'ensemble factoriel de tous les réordonnancements possibles, et des contraintes sur les permutations s'avèrent nécessaires. Nous comparons différents jeux de contraintes et explorons l'importance de l'espace de réordonnancement dans les performances globales d'un système de traduction. Si un meilleur design permet d'obtenir de meilleurs résultats, nous montrons cependant que la marge d'amélioration se situe principalement dans l'évaluation des réordonnancements plutôt que dans la qualité de l'espace de recherche.
Cette thèse vise à la mise en œuvre et à l'évaluation de techniques d'extraction de relations sémantiques à partir d'un corpus multilingue aligné. D'abord, nos observations porteront sur la comparaison sémantique d'équivalents traductionnels dans des corpus multilingues alignés. A partir des équivalences, nous tâcherons d'extraire des "cliques", ou sous-graphes maximaux complets connexes, dont toutes les unités sont en interrelation, du fait d'une probable intersection sémantique. Ces cliques présentent l'intérêt de renseigner à la fois sur la synonymie et la polysémie des unités, et d'apporter une forme de désambiguïsation sémantique. Ensuite nous tâcherons de relier ces cliques avec un lexique sémantique (de type Wordnet) afin d'évaluer la possibilité de récupérer pour les unités arabes des relations sémantiques définies pour des unités en anglais ou en français. Ces relations permettraient de construire automatiquement un réseau utile pour certaines applications de traitement de la langue arabe, comme les moteurs de question
Les divers intervenants qui décrivent, étudient et réalisent un système complexe ont besoin de points de vue adaptés à leurs préoccupations. Cependant, dans le contexte de l'Ingénierie Dirigée par les Modèles, les moyens pour définir et mettre en œuvre ces points de vue sont, soit trop rigides et inadaptées, soit totalement ad hoc. De plus, ces différents points de vue sont rarement indépendants les uns des autres. Dès lors, il faut s'attacher à identifier puis décrire les liens/les correspondances qui existent entre les points de vue pour enfin pouvoir vérifier que les réponses apportées par les différents intervenants constituent un tout cohérent. Les travaux exposés dans cette thèse permettent la définition de langages dédiés basés sur UML pour les points de vue. Pour cela, une méthode outillée qui analyse la sémantique des descriptions textuelles des concepts du domaine que l'on souhaite projeter sur UML est proposée afin de faciliter la définition de profils UML. Pour définir des points de vue basés sur des profils UML, cette thèse propose une méthode qui permet au méthodologiste d'expliciter le point de vue voulu. Un outil génère ensuite l'outillage qui met en œuvre ce point de vue dans un environnement de modélisation ainsi que le langage dédié correspondant là où la pratique actuelle repose sur une mise en œuvre essentiellement manuelle. Pour assister l'identification des liens entre points de vue, cette thèse propose là aussi d'analyser la sémantique des descriptions textuelles des concepts des langages utilisés par les points de vue. Utilisée en complément des heuristiques syntaxiques existantes, l'approche proposée permet d'obtenir de bons résultats lorsque les terminologies des langages analysés sont éloignées. Un cadre théorique basé sur la théorie des catégories est proposé pour expliciter formellement les correspondances. Pour utiliser ce cadre, une catégorie pour les langages basés sur UML a été proposée. Afin de pouvoir également expliciter les correspondances entre les modèles issus de ces langages, la catégorie des ontologies OWL est utilisée. Une solution est proposée pour caractériser des correspondances plus complexes que la simple équivalence. Ce cadre théorique permet la définition formelle de relations complexes qui permettront de raisonner sur la cohérence de la description de l'architecture. Une fois la description de l'architecture intégrée en un tout en suivant les correspondances formalisées, la question de la cohérence est abordée. Les expérimentations faites sur un cas d'étude concret pour vérifier la cohérence à un niveau syntaxique donnent des résultats pratiques satisfaisants. Les expérimentations menées sur le même cas pour vérifier la cohérence à un niveau sémantique ne donnent pas de résultats pratiques satisfaisants.
Notre recherche porte sur le rôle de la crédibilité des offres dans les sites d'e-commerce et les facteurs qui la déterminent. 294 personnes ont été interrogées à exprimer leur perception sur la crédibilité des offres de livres qui ont été composées en variant la source et les signes de leur réputation. La recherche s'inscrit dans une approche quasi-expérimentale qui vise à mesurer l'impact de ces facteurs sur la crédibilité perçue mais aussi à mesurer l'influence de cette dernière sur l'intérêt accordé à l'offre et en conséquence sur l'intention d'achat. Les analyses sont faites en se basant sur un modèle d'équation structurelle (SEM). Les résultats témoignent le rôle pivot de la crédibilité qui médiatise l'effet des signaux de la qualité de l'offre mis sur l'intérêt qu'elle soulève. Cet intérêt, à son tour, convoie l'impact de la crédibilité sur l'intention d'achat. Nous trouvons également l'effet significatif de la confiance dans la source ainsi que l'intérêt à la catégorie sur la crédibilité. L'effet de la confiance est positif et remarquable tandis que celui de l'intérêt à la catégorie reste négatif. Enfin, ces résultats nous permettent de conclure l'importance de la crédibilité : soigner la crédibilité de l'offre est le chemin au renforcement de l'intérêt du consommateur ainsi qu'aux performances des ventes.
L'intelligence artificielle est la discipline de recherche d'imitation ou de remplacement de fonctions cognitives humaines. À ce titre, l'une de ses branches s'inscrit dans l'automatisation progressive du processus de programmation. Il s'agit alors de transférer de l'intelligence ou, à défaut de définition, de transférer de la charge cognitive depuis l'humain vers le système, qu'il soit autonome ou guidé par l'utilisateur. Dans le cadre de cette thèse, nous considérons les conditions de l'évolution depuis un système guidé par son utilisateur vers un système autonome, en nous appuyant sur une autre branche de l'intelligence artificielle : l'apprentissage artificiel. Pour nos travaux, les requêtes sont exprimées en français, et les actions sont désignées par les commandes correspondantes dans un langage de programmation (ici, R ou bash). L'apprentissage du système est effectué à l'aide d'un ensemble d'exemples constitué par les utilisateurs eux-mêmes lors de leurs interactions. Ce sont donc ces derniers qui définissent, progressivement, les actions qui sont appropriées pour chaque requête, afin de rendre le système de plus en plus autonome. Nous avons collecté plusieurs ensembles d'exemples pour l'évaluation des méthodes d'apprentissage, en analysant et réduisant progressivement les biais induits. Le protocole que nous proposons est fondé sur l'amorçage incrémental des connaissances du système à partir d'un ensemble vide ou très restreint. Cela présente l'avantage de constituer une base de connaissances très représentative des besoins des utilisateurs, mais aussi l'inconvénient de n'aquérir qu'un nombre très limité d'exemples. Nous utilisons donc, après examen des performances d'une méthode naïve, une méthode de raisonnement à partir de cas : le raisonnement par analogie formelle. Nous montrons que cette méthode permet une précision très élevée dans les réponses du système, mais également une couverture relativement faible. L'extension de la base d'exemples par analogie est explorée afin d'augmenter la couverture des réponses données. La durée d'exécution de l'approche par analogie, déjà de l'ordre de la seconde, souffre beaucoup de l'extension de la base et de l'approximation. Enfin, l'assistant opérationnel incrémental fondé sur le raisonnement analogique a été testé en condition incrémentale simulée, afin d'étudier la progression de l'apprentissage du système au cours du temps. On en retient que le modèle permet d'atteindre un taux de réponse stable après une dizaine d'exemples vus en moyenne pour chaque type de commande. Bien que la performance effective varie selon le nombre total de commandes considérées, cette propriété ouvre sur des applications intéressantes dans le cadre incrémental du transfert depuis un domaine riche (la langue naturelle) vers un domaine moins riche (le langage de programmation).
Cette thèse présente le développement d'un framework formel pour la représentation des Langues de Signes (LS), les langages des communautés Sourdes, dans le cadre de la construction d'un système de reconnaissance automatique. Les LS sont de langues naturelles, qui utilisent des gestes et l'espace autour du signeur pour transmettre de l'information. De plus, lors du discours les signeurs utilisent plusieurs parties de leurs corps (articulateurs) simultanément, ce qui est difficile à capturer avec un système de notation écrite. Cette situation difficulté leur représentation dans de taches de Traitement Automatique du Langage Naturel (TALN). Pour représenter les propriétés à vérifier, une logique multi-modale a été choisi : la Logique Propositionnelle Dynamique (PDL). Cette logique a été originalement crée pour la spécification de programmes. De manière plus précise, PDL permit d'utilise des opérateurs modales comme [a] et " a ", représentant " nécessité " et " possibilité ", respectivement. Une variante particulaire a été développée pour les LS : la PDL pour Langue de Signes (PDLSL), qui est interprété sur des STE représentant des corpus. Avec PDLSL, chaque articulateur du corps (comme les mains et la tête) est vu comme un agent indépendant ; cela veut dire que chacun a ses propres actions et propositions possibles, et qu'il peux les exécuter pour influencer une posture gestuelle. L'utilisation du framework proposé peut aider à diminuer deux problèmes importantes qui existent dans l'étude linguistique des LS : hétérogénéité des corpus et la manque des systèmes automatiques d'aide à l'annotation. De ce fait, un chercheur peut rendre exploitables des corpus existants en les transformant vers des STE. Globalement, le système reçoit des vidéos LS et les transforme dans un STE valide. Ensuite, un module fait de la vérification formelle sur le STE, en utilisant une base de données de formules crée par un expert en LS. Le produit de ce processus, est une annotation qui peut être corrigé par des utilisateurs humains, et qui est utilisable dans des domaines d'études tels que la linguistique.
Le lexique est aujourd'hui reconnu comme un composant essentiel de tout système de Traitement Automatique des Langues, et l'utilisation de ressources lexicales est en pleine explosion. Les travaux dédiés à la résolution des ambiguïtés de rattachement prépositionnel, une des tâches les plus délicates à résoudre en analyse syntaxique automatique, utilisent massivement des informations lexicales acquises à partir de corpus portant sur la langue générale. Du côté de l'évaluation, l'efficacité des lexiques est en général testé sur un seul corpus, et la question liée à la nécessité d'adapter le lexique au type de corpus demeure peu explorée. Dans notre étude, nous construisons deux types de lexiques pour le français : l'un est dérivé d'un dictionnaire existant ( Nous faisons émerger des corpus des caractéristiques susceptibles d'éclairer les variations observées dans les résultats de la désambiguïsation. La nécessaire adaptation des ressources au type de corpus est rendue plus manifeste encore lorsque nous confrontons l'utilité du lexique acquis à partir du corpus journalistique à un lexique contenant des informations spécifiques à chacun des cinq corpus de test.
Cette thèse aborde différents aspects de la modélisation de la microstructure du marché et des problèmes de Market Making, avec un accent particulier du point de vue du praticien. Nous souhaitons améliorer la connaissance du LOB pour la communauté de la recherche, proposer de nouvelles idées de modélisation et développer des applications pour les Market Makers. Nous remercions en particuler l'équipe Automated Market Making d'avoir fourni la base de données haute-fréquence de très bonne qualité et une grille de calculs puissante, sans laquelle ces recherches n'auraient pas été possible. Le Chapitre 1 présente la motivation de cette recherche et reprend les principaux résultats des différents travaux. Le Chapitre 2 se concentre entièrement sur le LOB et vise à proposer un nouveau modèle qui reproduit mieux certains faits stylisés. A travers cette recherche, non seulement nous confirmons l'influence des flux d'ordres historiques sur l'arrivée de nouveaux, mais un nouveau modèle est également fourni qui réplique beaucoup mieux la dynamique du LOB, notamment la volatilité réalisée en haute et basse fréquence. Dans le Chapitre 3, l'objectif est d'étudier les stratégies de Market Making dans un contexte plus réaliste. La prédiction à haute fréquence avec la méthode d'apprentissage profond est étudiée dans le Chapitre 4. De nombreux résultats de la prédiction en 1- étape et en plusieurs étapes ont retrouvé la non-linéarité, stationarité et universalité de la relation entre les indicateurs microstructure et le changement du prix, ainsi que la limitation de cette approche en pratique.
La reconnaissance de la langue des signes française (LSF) comme une langue à part entière en 2005 a créé un besoin important de développement d'outils informatiques pour rendre l'information accessible au public sourd. Dans cette perspective, cette thèse a pour cadre la modélisation linguistique pour un système de génération de la LSF. Nous présentons dans un premier temps les différentes approches linguistiques ayant pour but la description linguistique des langues des signes (LS). Nous présentons ensuite les travaux effectués en informatique pour les modéliser. Dans un deuxième temps, nous proposons une approche permettant de prendre en compte les caractéristiques linguistiques propres au LS tout en respectant les contraintes d'un processus de formalisation. En étudiant des liens entre des fonctions sémantiques et leurs formes observées dans les corpus LSF, nous avons identifié plusieurs règles de production. Nous présentons finalement le fonctionnement des règles comme étant un système susceptible de modéliser un énoncé entier en LSF.
Cette thèse de doctorat se place dans le cadre de la reconnaissance de la parole dans des documents audio. Le but de ce travail est d'adapter les principes de l'identification audio pour la reconnaissance de la parole ainsi que concevoir et développer des techniques d'identification robustes. Les systèmes d'identification audio par empreinte (audio fingerprinting) sont conçus pour l'indexation d'extraits de musique mais ne traitent pas des spécificités du signal de parole. Dans un premier temps, différentes méthodes d'identification audio par empreinte sont étudiées ainsi qu'un premier travail d'adaptation à la reconnaissance de la parole. De nouveaux types de sousempreinte basés sur des paramètres usuels de la parole sont alors proposés. Dans un second temps, les différents types de variabilité du signal de parole sont décrits ainsi que les principaux paramètres de représentation acoustique du signal de parole. La robustesse de différents types de sous-empreinte à la variabilité extrinsèque et à la variabilité intrinsèque est évaluée. En présence de perturbations liées à l'environnement et aux conditions de transmission du signal de parole (CTIMIT), un type de sous-empreinte issu de l'identification audio s'avère alors le plus robuste.
L'aide ambiante à la personne (ambiant assisted living) a pour objectif d'accompagner le vieillissement de la population. Cela s'instancie notamment par les maisons intelligentes (smart homes), équipées de multiples capteurs connectés, dont un des objectifs est de prolonger le maintien à domicile des personnes âgées. Le manuscrit s'attache d'abord à introduire la problématique générale des maisons intelligentes, avant de présenter plus avant les trois sous-thématiques qui font plus particulièrement l'objet de la thèse, à savoir la reconnaissance d'activités, la confidentialité et les systèmes de dialogue. La reconnaissance d'activités consiste à déterminer les activités courantes d'une personne ou d'un groupe de personnes, à partir des données (brutes) des capteurs dont est équipée la maison. On peut citer comme exemple la détection de la chute d'une personne. Une maison intelligent repose typiquement sur l'internet des objets (Internet of Things, ou IoT). De nombreuses données sont produites, pouvant contenir des informations privées ou sensibles. Une partie de ces données doit être partagée avec l'extérieur, ce qui peut poser des problèmes de confidentialité. Enfin, pour interragir avec la maison intelligente, un moyen naturel pour l'utilisateur est d'utiliser le dialogue, sujet traité par les systèmes de dialogue. Ce travail de thèse propose des contributions sur ces trois versants, la plupart basées sur l'apprentissage profond.
Le concept de systèmes informatisés complexes rassemble tous les systèmes constitués d'un grand nombre de composantes inter-connectées et gérées par ordinateur. La configuration et la gestion de ces systèmes passe par une multitude de tâches critiques à leur bon fonctionnement et leur évolution. Nous proposons aussi une extension au langage de définition des domaines de planification automatique PDDL afin de modéliser les connaissances des experts du domaine d'application sous forme de méthodes de décomposition des tâches qui serviront à guider l'algorithme de planification HTN. Enfin, nous proposons des critères d'évaluation pour les systèmes en mixed-initiative qui servent de base à la discussion du système MIP.
La présente étude s'intéresse à la terminologie médicale wolof, envisagée dans le cadre de la sémantique lexicale. Nous y abordons des unités terminologiques de type collocation. Ce choix a un lien direct avec notre cadre théorique d'analyse, la Théorie Sens-Texte (TST), qui, à l'heure actuelle, propose l'un des meilleurs outils de description de la collocation avec les Fonctions Lexicales (FL). Les collocations constituent des indices de spécialisation en plus d'avoir un comportement lexicosyntaxique singulier. Nous les analysons, sur la base d'un corpus scientifique compilé, afin d'avoir une perception holistique de la cooccurrence lexicale. Les langues, en Afrique, sont souvent peu dotées du point de vue terminologique. Il s'agit dans cette recherche de s'appuyer sur le modèle d'analyse du lexique qui prend en compte trois paramètres clés : le sens, la forme et la combinatoire, afin de faire une description du lexique wolof qui, à terme, permet d'établir des principes de terminologisation. La portée traductive du travail réside dans l'approche interlinguistique que nous adoptons pour élaborer notre liste de termes. Le versant opératoire de l'étude est la constitution d'un début de corpus médical trilingue (anglais-français La perspective traductive de la terminologie a permis de relever différents procédés de création et de restitutions de termes médicaux (anglais et français) en wolof.
Au cours des 10 dernières années, les téléphones mobiles ont considérablement évolué~ : l'apparition des écrans tactiles et la disparition des claviers physiques ont changé la façon dont nous interagissons au quotidien avec ces dispositifs. Pourtant, la saisie de texte demeure toujours une tâche importante avec des activités telles que la prise de notes, l'envoi de messages textuels ou la communication sur les réseaux sociaux. Cependant, bien que l'utilisation du tactile ait de grands avantages en termes de dynamicité de l'interface et de personnalisation, de tels dispositifs ne sont pas forcément accessibles pour tous. En effet, pour 39 millions de personnes dans le monde touchés par la cécité, les difficultés sont nombreuses avec ces dispositifs du fait que toutes les interactions se fassent au moyen de l'écran dépourvu de tout repère tactile : les interactions avec le dispositif sont possibles, mais elles sont souvent laborieuses et répétitives, ce qui implique alors une charge cognitive trop importante et des problèmes de précision, de mémorisation, et de fatigue. Dans ce travail de doctorat, nous nous sommes intéressés à l'accessibilité de la saisie de texte dans le contexte de la déficience visuelle. Dans un premier temps, nous avons étudié les différentes solutions actuellement existantes pour les utilisateurs en situation de déficience. La problématique principale de ces recherches était d'améliorer la saisie de texte pour permettre aux utilisateurs d'avoir de meilleures performances de saisie. Pour cela, nous avons conçu une solution déductive, appelée DUCK. Cette solution permet aux déficients visuels, de saisir rapidement du texte sans se soucier de la précision de leurs frappes. Un système à base de connaissances linguistiques permet à chaque fin de mot de déduire le mot que l'utilisateur a voulu saisir. Ce dispositif a ensuite été testé auprès d'un échantillon de déficients visuels afin de vérifier l'efficacité de notre solution. La suite des travaux s'est ensuite focalisée sur deux principales optimisations. La première concerne les listes de mots. Nous avons étudié et comparé différentes interactions pour permettre à l'utilisateur de naviguer et choisir des mots de façon efficace et simple lorsqu'il est face à une liste de mots proposée par un système de prédiction ou de déduction. La seconde se focalise sur la saisie des mots couramment utilisés. Nous avons également mené une étude comparative entre différentes propositions d'interaction permettant de saisir un mot court de façon efficace sans avoir recours au système de déduction, trop coûteux en temps pour ce type de mots. Enfin, nous terminons ce projet de doctorat par une étude longitudinale qui présente le clavier DUCK avec l'intégration de ces optimisations. Ce nouveau système a été utilisé par des déficients visuels sur une période de deux semaines afin d'étudier l'efficacité du clavier une fois ce dernier pris en main sur le long terme.
Cette thèse a pour objet d'interroger sur la relation entre les troubles du langage et leur dénomination en orthophonie, comme par exemple en français « dysphasie, troubles spécifiques du langage écrit, aphasie, difficultés du langage écrit, retard de langage » ... La terminologie employée par les orthophonistes pour décrire les pathologies rencontrées chez leurs patients a intégré celle des courants théoriques en évolution, et ce pour apporter des nuances nécessaires à la précision du diagnostic orthophonique. Pour décrire l'inconstance de ce lien entre le terme diagnostique et la réalité de la pathologie étiquetée, l'auteure s'est appuyée sur des considérations épistémologiques, lexicologiques et terminologiques. Le bilan orthophonique permettant l'établissement de ce diagnostic est suivi par convention d'un compte-rendu de bilan orthophonique (CRBO), reflet de la langue de spécialité et révélateur de la représentation de ces troubles. 435 comptes-rendus authentiques ont été explorés au moyen d'une analyse descriptive lexicologique et terminologique semi-automatique grâce à un codage XML, produisant ainsi une « photographie » de l'utilisation des termes concernant l'ensemble des pathologies dont s'occupe l'orthophoniste. L'analyse a permis de distinguer deux niveaux terminologiques (un traitant de la nature du trouble, et un de sa forme), illustrant les nuances nécessaires aux orthophonistes dans des syntagmes que l'on peut qualifier de collocationnels. La dernière phase d'analyse de ces données a permis de tisser la trame d'une proposition de classification orthophonique, la COFOP (Classification Orthophonique FOndée sur la Pratique clinique).
Cette thèse contribue à la compréhension d'un phénomène émergent et complexe : la transformation/smartisation du système de service public urbain. Cela comprend la combinaison de différents secteurs de services tels que les transports, le tourisme,les taxes, etc. Nous établissons trois piliers pour la Science du Service : Système de service (SS), Innovation de Service (IS) et Logiques Institutionnelles de Service (LIS). Nous commençons par proposer une méthode basée sur l'analyse sémantique latente (LSA), l'analyse factorielle (FA), le text mining et la théorie enracinée. Il s'agit de mettre en évidence de manière inductive et interdisciplinaire, 30 années d'évolution de la structure intellectuelle de SS, IS et LIS, de 1986 à 2015. La pensée complexe, comme cadre intégrateur, nous a permis de mettre en évidence le contenu, ou les effets, de la transformation institutionnelle, en considérant le tout et les parties d'un système de service public urbain qui devient smart. L'architecture de la recherche est basée principalement sur la théorie enracinée, l'observation, les archives, l'étude de cas longitudinale multiniveaux (i.e. locale et nationale) via le modèle dialogique (Parmentier-Cajaiba &amp ; Avenier, 2013) et le paradigme épistémologique constructiviste pragmatique (PECP). Aussi, nous utilisons deux hypothèses ontologiques du travail : l'ontologie relationnelle et l'ontologie du devenir. Du point de vue théorique, notre recherche contribue au affinement théorique de la littérature sur SS, IS et LIS. Cette définition est accompagné de trois modèles heuristiques résultants d'une analyse par le processus et par le contenu de la transformation et de la théorie enracinée (Gioia &amp ; Chittipeddi, 1991 ; Gioia et al., 2013). Le premier modèle heuristique contribue à la compréhension du processus de travail institutionnel pour la co-création d'arrangements institutionnels (i.e. standard, normes, les ressources frontières, APIs) entre deux logiques collectives antagonistes et complémentaires (Morin, 2005 ; Smets &amp ; Jarzabkowski, 2013 ; Greenwood et al., 2017) : la logique de service dominant du marché (Lusch &amp ; Nambisan, 2015 ; Vargo &amp ; Lusch, 2016) et la logique du service public (Osborne et al.,2015 ; Osborne, 2017). Le deuxième modèle heuristique met en évidence les composantes structurales d'un système de service public qui devient smart. Le troisième modèle heuristique souligne les moteurs et les freins de la transformation institutionnelle et structurelle.
Tout d'abord perçue comme un phénomène marginal, presque un accident en langue, on considère aujourd'hui que la polysémie fait partie intégrante des systèmes linguistiques. Aussi, à partir de la distinction établie par G. Kleiber (1999), nous considérons deux grands courants selon le rapport établi entre signification, référence et polysémie. Le premier décrit la polysémie en termes de sens premier référentiel dont sont dérivés des sens secondaires (courant objectiviste). Le second l'analyse en termes de potentiel sémantique aréférentiel à partir duquel est obtenu l'ensemble des sens du polysème par spécialisation ou enrichissement contextuel(le) (courant constructiviste). Sur la base des travaux de D. Tuggy (1993), nous déclinons les représentations de la signification des mots à sens multiples le long d'un continuum homonymie-polysémie-multifacialité-indétermination, selon les degrés d'enracinement, de saillance, et les possibilités d'accessibilité et d'activation des différents composants (valeur schématique et élaborations sémantiques). Et, nous mettons ainsi en avant certaines des régularités organisatrices propres aux représentations sémanticoconceptuelles des polysèmes nominaux, ainsi qu'une typologie des sens polysémiques. En grammaire cognitive, nous considérons qu'il s'agit d'un processus non modulaire, compositionnel et dynamique. L'analyse de syntagmes nominaux du type Adj-N et N-Adj révèle en outre certaines régularités dans l'activation des sens polysémiques. Ces régularités sont liées au contexte (place et fonction de l'adjectif par rapport au substantif recteur) et au contexte extralinguistique.
Dans le domaine de l'apprentissage machine, les réseaux de neurones profonds sont devenus la référence incontournable pour un très grand nombre de problèmes. Ces systèmes sont constitués par un assemblage de couches, lesquelles réalisent des traitements élémentaires, paramétrés par un grand nombre de variables. À l'aide de données disponibles pendant une phase d'apprentissage, ces variables sont ajustées de façon à ce que le réseau de neurones réponde à la tâche donnée. Il est ensuite possible de traiter de nouvelles données. Si ces méthodes atteignent les performances à l'état de l'art dans bien des cas, ils reposent pour cela sur un très grand nombre de paramètres, et donc des complexités en mémoire et en calculs importantes. De fait, ils sont souvent peu adaptés à l'implémentation matérielle sur des systèmes contraints en ressources. Par ailleurs, l'apprentissage requiert de repasser sur les données d'entraînement plusieurs fois, et s'adapte donc difficilement à des scénarios où de nouvelles informations apparaissent au fil de l'eau. Dans cette thèse, nous nous intéressons dans un premier temps aux méthodes permettant de réduire l'impact en calculs et en mémoire des réseaux de neurones profonds. Nous proposons dans un second temps des techniques permettant d'effectuer l'apprentissage au fil de l'eau, dans un contexte embarqué.
Entrant dans l'ère de la digitalisation, il est important de transformer les données en connaissances et utiliser celles-ci pour fournir des pistes d'amélioration industrielle. La transformation de données en connaissances pour optimiser la production est aujourd'hui un défi industriel majeur. Nous adresserons les problématiques de l'ordonnancement, de la planification et de l'équilibrage de charges sur et entre les lignes de production. D'un point de vue système, ces aspects sont aujourd'hui considérés comme portés par l'ERP (Enterprise Resource Planning). Les ERP sont des outils présentant une forte rigidité dans leur structure et dans leur fonctionnement, imposant cette rigidité aux organisations. Notre partenaire la société iFAKT, experte en équilibrage de charges nous accompagnera dans ce projet de thèse. Cette thèse aborde deux axes de travail principaux : l'un portant sur l'intégration du Traitement Automatique du Langage Naturel, l'Apprentissage Automatique et l'outil d'équilibrage de charges et l'autre les actions à entreprendre par rapport à cette intégration. Pour la mise en place de ces deux grandes activités, nous contribuerons à la création de méthodologies couplant les techniques et outils cités ci-dessus dans le cadre de l'industrie 4.0
Ce sujet est à la croisée de plusieurs domaines (interaction Humain-Robot, planification automatique, apprentissage artificiel). Il s'agit maintenant d'aller au delà de ces premiers résultats obtenus au cours de mon M2R et de trouver un cadre générique pour la programmation de « cobots » (robots collaboratifs) en milieu industriel. L'approche cobotique consiste à ce qu'un opérateur humain, en tant qu'expert métier directement impliqué dans la réalisation des tâches en ligne, apprenne au robot à effectuer de nouvelles tâches et à utiliser le robot comme assistant « agile » .
Notre thèse se propose d'observer et décrire l'acquisition des marques de la référence en production écrite chez les élèves du CE2 au CM2 (de 9 à 11 ans) dans des textes narratifs. Cette étude permet de dresser une cartographie des compétences des élèves en matière de continuité référentielle en fonction du niveau de classe. Beaucoup de travaux en linguistique se sont intéressés à la question de la référence du point de vue de l'analyse et de la réception, mais très peu du point de vue de la production. En didactique du français langue première, les études qui analysent l'expression référentielle dans les textes d'élèves analysent un échantillon limité de textes. D'autre part, les études psycholinguistiques qui s'intéressent à la référence d'un point de vue développemental sont plus nombreuses pour l'oral que pour l'écrit et concernent généralement les enfants les plus jeunes (de 0 à 3 ans). Dans notre thèse, notre étude s'inscrit dans une perspective de progressivité. Nous considérons la mise en place des compétences rédactionnelles des élèves en étudiant les procédés auxquels ils ont recourt pour introduire et maintenir les référents dans un texte narratif. Nous nous demandons notamment si les élèves du CE2 au CM2 privilégient des relations anaphoriques (Reichler-Béguelin, 1988) ou s'ils construisent des chaînes de références (Schnedecker, 1997), et si le niveau de classe et la nature des référents influent sur le type de résolution choisie. Bonnemaison, 2014) contenant des expressions référentielles exigeant l'introduction de référents de diverses natures (humain/non humain, évènement, indication spatio-temporelle).
Le langage XML est devenu un standard de représentation et d'échange de données à travers le web. Le but de la réplication de données au sein de différents sites est de minimiser le temps d'accès à ces données partagées. Cependant, différents problèmes sont liés à la sécurisation de ces données. Le but de cette thèse est de proposer des modèles de contrôles d'accès XML qui prennent en compte les droits de lecture et de mise-à-jour et qui permettent de surmonter les limites des modèles qui existent. Nous considérons les langages XPath et XQuery Update Facility pour la formalisation des requêtes d'accès et des requêtes de mise-à-jour respectivement. L'autre partie de cette thèse est consacrée à l'étude pratique de nos propositions. Nous présentons notre système appelé SVMAX qui met en oeuvre nos solutions, et nous conduisons une étude expérimentale basée sur une DTD réelle pour montrer son efficacité. Plusieurs systèmes de bases de données natives (systèmes de BDNs) ont été proposés récemment qui permettent une manipulation efficace des données XML en utilisant la plupart des standards du W3C. Nous montrons que notre système SVMAX peut être intégré facilement et efficacement au sein d'un large ensemble de systèmes de BDNs. A nos connaissances, SVMAX est le premier système qui permet la sécurisation des données XML conformes à des DTDs arbitraires (récursives ou non) et ceci en moyennant un fragment significatif de XPath et une classe riche d'opérations de mise-à-jour XML
Au cours du premier millénaire avant notre ère, les bibliothèques, qui apparaissent avec le besoin d'organiser la conservation des textes, sont immédiatement confrontées aux difficultés de l'indexation. Le titre apparaît alors comme une première solution, permettant d'identifier rapidement chaque type d'ouvrage et éventuellement de discerner des ouvrages thématiquement proches. Alors que dans la Grèce Antique, les titres ont une fonction peu informative, mais ont toujours pour objectif d'identifier le document, l'invention de l'imprimerie à caractères mobiles (Gutenberg, XVème siècle) a entraîné une forte augmentation du nombre de documents, offrant désormais une diffusion à grande échelle. Mais comment quelques mots peuvent-ils avoir une si grande influence ? Quelles fonctions les titres doivent-ils remplir en ce début du XXIème siècle ? Comment générer automatiquement des titres respectant ces fonctions ? Le titrage automatique de documents textuels est avant tout un des domaines clés de l'accessibilité des pages Web (standards W3C) tel que défini par la norme proposée par les associations sur le handicap. Côté lecteur, l'objectif est d'augmenter la lisibilité des pages obtenues à partir d'une recherche sur mot-clé(s) et dont la pertinence est souvent faible, décourageant les lecteurs devant fournir de grands efforts cognitifs. Côté producteur de site Web, l'objectif est d'améliorer l'indexation des pages pour une recherche plus pertinente. D'autres intérêts motivent cette étude (titrage de pages Web commerciales, titrage pour la génération automatique de sommaires, titrage pour fournir des éléments d'appui pour la tâche de résumé automatique,). Afin de traiter à grande échelle le titrage automatique de documents textuels, nous employons dans cette étude des méthodes et systèmes de TALN (Traitement Automatique du Langage Naturel). Alors que de nombreux travaux ont été publiés à propos de l'indexation et du résumé automatique, le titrage automatique demeurait jusqu'alors discret et connaissait quelques difficultés quant à son positionnement dans le domaine du TALN. Nous soutenons dans cette étude que le titrage automatique doit pourtant être considéré comme une tâche à part entière. Après avoir défini les problématiques liées au titrage automatique, et après avoir positionné cette tâche parmi les tâches déjà existantes, nous proposons une série de méthodes permettant de produire des titres syntaxiquement corrects selon plusieurs objectifs. En particulier, nous nous intéressons à la production de titres informatifs, et, pour la première fois dans l'histoire du titrage automatique, de titres accrocheurs. Notre système TIT', constitué de trois méthodes (POSTIT, NOMIT et CATIT), permet de produire des ensembles de titres informatifs dans 81% des cas et accrocheurs dans 78% des cas.
Nous vivons dans un monde où une grande quantité de données est généré en continu. Les données sont différentes d'une simple information numérique, mais viennent dans de nombreux format. Cependant, les données prisent isolément n'ont aucun sens. Mais quand ces données sont reliées ensemble on peut en extraire de nouvelles informations. De plus, les données sont sensibles au temps. La façon la plus précise et efficace de représenter les données est de les exprimer en tant que flux de données. Si les données les plus récentes ne sont pas traitées rapidement, les résultats obtenus ne sont pas aussi utiles. Ainsi, un système parallèle et distribué pour traiter de grandes quantités de flux de données en temps réel est un problème de recherche important. Dans cette thèse nous étudions l'opération de jointure sur des flux de données, de manière parallèle et continue. Nous séparons ce problème en deux catégories.
Les grammaires locales constituent un formalisme de description de constructions linguistiques et sont communément représentées sous la forme de graphes orientés. Utilisées pour la recherche et l'extraction de motifs dans un texte, elles trouvent leurs limites dans le traitement de variations non décrites ou fautives ainsi que dans la capacité à accéder à des connaissances exogènes, c'est-à-dire des informations à extraire, au cours de l'analyse, de ressources externes à la grammaire et qui peuvent s'avérer utiles pour normaliser, enrichir, valider ou mettre en relation les motifs reconnus. Premièrement, en ajoutant des fonctions arbitraires à satisfaire, appelées fonctions étendues, qui ne sont pas prédéfinies à l'avance et qui sont évaluées en dehors de la grammaire. Le travail présenté se divise en trois parties : dans un premier temps, nous étudions les principes concernant la construction des grammaires locales étendues. Nous présentons ensuite la mise en œuvre d'un moteur d'analyse textuelle implémentant le formalisme proposé. Enfin, nous étudions quelques applications pour l'extraction de l'information dans des textes bien formés et des textes bruités. Nous nous focalisons sur le couplage des ressources externes et des méthodes non-symboliques dans la construction de nos grammaires en montrant la pertinence de cette approche pour dépasser les limites des grammaires locales classiques
La compréhension automatique de vidéos devrait impacter notre vie de tous les jours dans de nombreux domaines comme la conduite autonome, les robots domestiques, la recherche et le filtrage de contenu, les jeux vidéo, la défense ou la sécurité. Le nombre de vidéos croît plus vite chaque année, notamment sur les plateformes telles que YouTube, Twitter ou Facebook. L'analyse automatique de ces données est indispensable pour permettre à de nouvelles applications de voir le jour. L'analyse vidéo, en particulier en environnement non contrôlé, se heurte à plusieurs problèmes comme la variabilité intra-classe (les échantillons d'un même concept paraissent très différents) ou la confusion inter-classe (les exemples provenant de deux activités distinctes se ressemblent). Bien que ces difficultés puissent être traitées via des algorithmes d'apprentissage supervisé, les méthodes pleinement supervisées sont souvent synonymes d'un coût d'annotation élevé. Dépendant à la fois de la tâche à effectuer et du niveau de supervision requis, la quantité d'annotations nécessaire peut être prohibitive. Dans le cas de la localisation d'actions, une approche pleinement supervisée nécessite les boîtes englobantes de l'acteur à chaque image où l'action est effectuée. Le coût associé à l'obtention d'un telle annotation empêche le passage à l'échelle et limite le nombre d'échantillons d'entraînement. Cette thèse adresse les problèmes évoqués ci-dessus dans le contexte de deux tâches, la classification et la localisation d'actions humaines. La classification consiste à reconnaître l'activité effectuée dans une courte vidéo limitée à la durée de l'action. La localisation a pour but de détecter en temps et dans l'espace des activités effectuées dans de plus longues vidéos. Notre approche pour la classification d'actions tire parti de l'information contenue dans la posture humaine et l'intègre avec des descripteurs d'apparence et de mouvement afin d'améliorer les performances. Notre approche pour la localisation d'actions modélise l'évolution temporelle des actions à l'aide d'un réseau récurrent entraîné à partir de suivis de personnes. Enfin, la troisième méthode étudiée dans cette thèse a pour but de contourner le coût prohibitif des annotations de vidéos et utilise le regroupement discriminatoire pour analyser et combiner différents types de supervision.
Le Traitement Automatique de la Parole (TAP) s'intéresse de plus en plus et progresse techniquement en matière d'étendue de vocabulaire, de gestion de complexité morphosyntaxique, de style et d'esthétique de la parole humaine. L'Affective Computing tend également à intégrer une dimension « émotionnelle » dans un objectif commun au TAP visant à désambiguïser le langage naturel et augmenter la naturalité de l'interaction personne-machine. Dans le cadre de la robotique sociale, cette interaction est modélisée dans des systèmes d'interaction, de dialogue, qui tendent à engendrer une dimension d'attachement dont les effets doivent être éthiquement et collectivement contrôlés. Or la dynamique du langage humain situé met à mal l'efficacité des systèmes automatiques. L'hypothèse de cette thèse propose dans la dynamique des interactions, il existerait une « glu socio-affective » qui ferait entrer en phases synchroniques deux individus dotés chacun d'un rôle social impliqué dans une situation/contexte d'interaction. Cette thèse s'intéresse à des dynamiques interactionnelles impliquant spécifiquement des processus altruistes, orthogonale à la dimension de dominance. Cette glu permettrait ainsi de véhiculer les événements langagiers entre les interlocuteurs, en modifiant constamment leur relation et leur rôle, qui eux même viennent à modifier cette glu, afin d'assurer la continuité de la communication. Un Magicien d'Oz – EmOz – est utilisé afin de contrôler les primitives vocales comme unique support langagier d'un robot majordome d'un habitat intelligent interagissant avec des personnes âgées en isolement relationnel. Cet isolement relationnel permet méthodologiquement d'appréhender les dimensions de la glu socio-affective, en introduisant une situation contrastive dégradée de la glu. Les effets des primitives permettraient alors d'observer les comportements de l'humain à travers des indices multimodaux. Le système automatisé qui découlera des données et des analyses de cette étude permettrait alors d'entraîner les personnes à solliciter pleinement leurs mécanismes de construction relationnelle, afin de redonner l'envie de communiquer avec leur entourage humain. Les analyses du corpus EEE recueilli montrent une évolution de la relation à travers différents indices interactionnels, temporellement organisés. Ces paramètres visent à être intégrés dans une perspective de système de dialogue incrémental – SASI. Les prémisses de ce système sont proposées dans un prototype de reconnaissance de la parole dont la robustesse ne dépendra pas de l'exactitude du contenu langagier reconnu, mais sur la reconnaissance du degré de glu, soit de l'état relationnel entre les locuteurs. Ainsi, les erreurs de reconnaissance tendraient à être compensées par l'intelligence socio-affective adaptative de ce système dont pourrait être doté le robot.
Le cancer de la prostate est le plus courant en France et la 4ième cause de mortalité par cancer. Les méthodes diagnostics de références actuel sont souvent insuffisantes pour détecter et localiser précisément une lésion. Néanmoins, l'interprétation visuelle des multiples séquences IRM n'est pas aisée. Dans ces conditions, un fort intérêt s'est porté sur les systèmes d'aide au diagnostic dont le but est d'assister le radiologue dans ses décisions. Cette thèse présente la conception d'un système d'aide à la détection (CADe) dontl'approche finale est de fournir au radiologue une carte de probabilité du cancer dans la zone périphérique de la prostate. Ce CADe repose sur une base d'images IRM multi-paramétrique (IRM-mp) 1.5T de types Cette thèse met l'accent sur la détection des cancers mais aussisur leur caractérisation dans le but de fournir une carte de probabilité corrélée au grade de Gleason des tumeurs. Nous avons utilisé une méthode d'apprentissage de dictionnaires permettant d'extraire de nouvelles caractéristiques descriptives dont l'objectif est de discriminer chacun des cancers. Ces dernières sont ensuite utilisées par deux classifieurs : régression logistique et séparateur à vaste marge (SVM), permettant de produire une carte de probabilité du cancer. Nous avons concentré nos efforts sur la discrimination des cancers agressifs (Gleason&gt ; 6) et fourni une analyse de la corrélationentre probabilités et scores de Gleason. Les résultats montrent de très bonnes performances de détection des cancers agressifs et l'analyse des probabilités conclue sur une forte capacité du système à séparer les cancers agressifs du reste des tissus mais ne permet pas aisément de distinguer chacundes grades de cancer
La prise de conscience des conséquences du réchauffement climatique a permis de lancer un mouvement de réduction de l'utilisation d'énergie. L'électricité utilisée dans les bâtiments représente une part importante de la consommation d'énergie et doit donc être utilisée de manière efficace. Pour cela, il est nécessaire de pouvoir mesurer et suivre la consommation électrique de chaque appareil au sein d'un bâtiment. Depuis 30 ans, une méthode de suivi des consommations électriques, Non Intrusive Load Monitoring (NILM), propose à partir d'un unique compteur mesurant la consommation totale du bâtiment, de déterminer la contribution de chaque appareil électrique. Cette méthode est basée sur un algorithme de désagrégation des consommations électriques et permet de s'affranchir de l'utilisation d'un compteur de mesure pour chaque appareil électrique du bâtiment. Cette thèse aborde les problèmes algorithmiques que présente le NILM. Ainsi, les principales difficultés du NILM sont : (i) la standardisation de la formulation, (ii) le caractère mal-posé du problème (perte d'information), (iii) les connaissances insuffisantes sur les signaux et (iv) l'implémentation d'un algorithme d'apprentissage. L'objectif principale de cette thèse est de traiter le NILM dans le cadre des grands bâtiments (commerciaux, bureaux, industriels) en utilisant des mesures hautes fréquences du courant et de la tension. Cependant les maisons individuelles et leurs propres types d'appareils électriques ne sont pas exclus de cette étude. Cette thèse est structurée en deux grandes parties. Dans une première partie nous abordons le problème du manque de connaissance des signaux de consommation électriques, à la fois ceux des grands bâtiments et ceux des différents appareils utilisés. La littérature concernant le NILM est principalement orienté sur l'étude des mesures basses fréquences de consommations dans les maisons. Le manque de données de consommations disponibles est également un frein pour le développement du NILM. Pour répondre à cela nous développons un modèle génératif permettant de simuler des données hautes fréquences de courant électrique de bâtiments. A partir d'un nombre limité de données réelles nous réalisons des simulations de bâtiments que nous partageons dans la base de données SHED. Dans une seconde partie, nous abordons le problème de la séparation de source. Grâce à nos résultats d'analyse et par manque de données, nous traitons ce problème à l'aide de techniques d'apprentissage non-supervisées. Pour proposons une nouvelle méthode appartenant à la famille des factorisations de matrice appelée Independent-Variation Matrix Factorization (IVMF), qui permet d'exprimer une matrice d'observation de courant comme le produit de deux matrices : les signatures et les activations. IVMF est le premier algorithme décrit pour le traitement du NILM dans le cadre de données hautes fréquences et de grands bâtiments. Enfin, nous montrons que IVMF atteint de meilleurs résultats pour le problème du NILM que des méthodes classiques de séparation de source comme l'Analyse en Composantes Indépendantes ou encore la Factorisation de Matrice Semi Non-négative.
Depuis 2006, les algorithmes d'apprentissage profond qui s'appuient sur des modèles comprenant plusieurs couches de représentations ont pu surpasser l'état de l'art dans plusieurs domaines. Les modèles profonds peuvent être très efficaces en termes du nombre de paramètres nécessaires pour représenter des opérations complexes. Bien que l'entraînement des modèles profonds ait été traditionnellement considéré comme un problème difficile, une approche réussie a été d'utiliser une étape de pré-entraînement couche par couche, non supervisée, pour initialiser des modèles profonds supervisés. Tout d'abord, l'apprentissage non-supervisé présente de nombreux avantages par rapport à la généralisation car il repose uniquement sur des données non étiquetées qu'il est facile de trouver. Deuxièmement, la possibilité d'apprendre des représentations couche par couche, au lieu de toutes les couches à la fois, améliore encore la généralisation et réduit les temps de calcul. Cependant, l'apprentissage profond pose encore beaucoup de questions relatives à la consistance de l'apprentissage couche par couche, avec de nombreuses couches, et à la difficulté d'évaluer la performance, de sélectionner les modèles et d'optimiser la performance des couches. Dans cette thèse, nous examinons d'abord les limites de la justification variationnelle actuelle pour l'apprentissage couche par couche qui ne se généralise pas bien à de nombreuses couches et demandons si une méthode couche par couche peut jamais être vraiment consistante. Nous constatons que l'apprentissage couche par couche peut en effet être consistant et peut conduire à des modèles génératifs profonds optimaux. Pour ce faire, nous introduisons la borne supérieure de la meilleure probabilité marginale latente (BLM upper bound), un nouveau critère qui représente la log Nous prouvons que la maximisation de ce critère pour chaque couche conduit à une architecture profonde optimale, à condition que le reste de l'entraînement se passe bien. Bien que ce critère ne puisse pas être calculé de manière exacte, nous montrons qu'il peut être maximisé efficacement par des auto-encodeurs quand l'encodeur du modèle est autorisé à être aussi riche que possible. Cela donne une nouvelle justification pour empiler les modèles entraînés pour reproduire leur entrée et donne de meilleurs résultats que l'approche variationnelle. En outre, nous donnons une approximation calculable de la BLM upper bound et montrons qu'elle peut être utilisée pour estimer avec précision la log-vraisemblance finale des modèles. Nous proposons une nouvelle méthode pour la sélection de modèles couche par couche pour les modèles profonds, et un nouveau critère pour déterminer si l'ajout de couches est justifié. Quant à la difficulté d'entraîner chaque couche, nous étudions aussi l'impact des métriques et de la paramétrisation sur la procédure de descente de gradient couramment utilisée pour la maximisation de la vraisemblance. Nous montrons que la descente de gradient est implicitement liée à la métrique de l'espace sous-jacent et que la métrique Euclidienne peut souvent être un choix inadapté car elle introduit une dépendance sur la paramétrisation et peut entraîner une violation de la symétrie. Pour pallier ce problème, nous étudions les avantages du gradient naturel et montrons qu'il peut être utilisé pour restaurer la symétrie, mais avec un coût de calcul élevé. Nous proposons donc qu'une paramétrisation centrée peut rétablir la symétrie avec une très faible surcharge computationnelle.
Cette thèse porte sur la synthèse de séquences de motion capture avec des modèles statistiques. Notre point de départ réside dans deux problèmes principaux rencontrés lors de la synthèse de données de motion capture, assurer le réalisme des positions et des mouvements, et la gestion de la grande variabilité dans ces données. Nous décrivons d'abord une variante de modèles de Markov cachés contextuels pour gérer la variabilité dans les données en conditionnant les paramètres des modèles à une information contextuelle supplémentaire telle que l'émotion avec laquelle un mouvement a été effectué. Nous proposons ensuite une variante d'une méthode de l'état de l'art utilisée pour réaliser une tâche de synthèse de mouvement spécifique appelée Inverse Kinematics, où nous exploitons les processus gaussiens pour encourager le réalisme de chacune des postures d'un mouvement généré. Nos résultats montrent un certain potentiel de ces modèles statistiques pour la conception de systèmes de synthèse de mouvement humain. Pourtant, aucune de ces technologies n'offre la flexibilité apportée par les réseaux de neurones et la récente révolution de l'apprentissage profond et de l'apprentissage Adversarial que nous abordons dans la deuxième partie. La deuxième partie de la thèse décrit les travaux que nous avons réalisés avec des réseaux de neurones et des architectures profondes. Nos travaux s'appuient sur la capacité des réseaux neuronaux récurrents à traiter des séquences complexes et sur l'apprentissage Adversarial qui a été introduit très récemment dans la communauté du Deep Learning pour la conception de modèles génératifs performants pour des données complexes, notamment images. Nous proposons une première architecture simple qui combine l'apprentissage Adversarial et des autoencodeurs de séquences, qui permet de mettre au point des systèmes performants de génération aléatoire de séquences réalistes de motion capture. A partir de cette architecture de base, nous proposons plusieurs variantes d'architectures neurales conditionnelles qui permettent de concevoir des systèmes de synthèse que l'on peut contrôler dans une certaine mesure en fournissant une information de haut niveau à laquelle la séquence générée doit correspondre, par exemple l'émotion avec laquelle une activité est réalisée.
Le traitement informatique des objets qui nous entourent, naturels ou créés par l'homme, demande toujours de passer par une phase de traduction en entités traitables par des programmes. Le choix de ces représentations abstraites est toujours crucial pour l'efficacité des traitements et est le terrain d'améliorations constantes. Mais il est un autre aspect émergeant : le lien entre l'objet à représenter et "sa" représentation n'est pas forcément bijectif ! Ainsi la nature ambiguë de certaines structures discrètes pose problème pour la modélisation ainsi que le traitement et l'analyse à l'aide d'un programme informatique. Le langage dit ``naturel'', et sous sa forme en particulier de représentation textuelle, en est un exemple. Le sujet de cette thèse consiste à explorer cette question, que nous étudions à l'aide de méthodes combinatoires et géométriques. Ces méthodes nous permettent de formaliser le problème d'extraction d'information dans des grands réseaux d'entités ainsi que de construire des représentations géométriques utiles pour le traitement du langage naturel. Dans un premier temps, nous commençons par démontrer des propriétés combinatoires des graphes de séquences intervenant de manière implicite dans les modèles séquentiels. Ces propriétés concernent essentiellement le problème inverse de trouver une séquence représentant un graphe donné. Les algorithmes qui en découlent nous permettent d'effectuer une comparaison expérimentale de différents modèles séquentiels utilisés en modélisation du langage. Dans un second temps, nous considérons une application pour le problème d'identification d'entités nommées. A la suite d'une revue de solutions récentes, nous proposons une méthode compétitive basée sur la comparaison de structures de graphes de connaissances et moins coûteuse en annotations d'exemples dédiés au problème. Nous établissons également une analyse expérimentale d'influence d'entités à partir de relations capitalistiques. Cette analyse suggère l'élargissement du cadre d'application de l'identification d'entités à des bases de connaissances de natures différentes. Ces solutions sont aujourd'hui utilisées au sein d'une librairie logicielle dans le secteur bancaire. Ensuite, nous développons une étude géométrique de représentations de mots récemment proposées, au cours de laquelle nous discutons une conjecture géométrique théoriquement et expérimentalement. Cette étude suggère que les analogies du langage sont difficilement transposables en propriétés géométriques, et nous amène a considérer le paradigme de la géométrie des distances afin de construire de nouvelles représentations. Enfin, nous proposons une méthodologie basée sur le paradigme de la géométrie des distances afin de construire de nouvelles représentations de mots ou d'entités. Nous proposons des algorithmes de résolution de ce problème à grande échelle, qui nous permettent de construire des représentations interprétables et compétitives en performance pour des tâches extrinsèques. Plus généralement, nous proposons à travers ce paradigme un nouveau cadre et piste d'explorations pour la construction de représentations en apprentissage machine.
Cette thèse consiste à analyser conjointement des signaux de mouvement des yeux et d'électroencéphalogrammes (EEG) multicanaux acquis simultanément avec des participants effectuant une tâche de lecture de recueil d'informations afin de prendre une décision binaire-le texte est-il lié à un sujet ou non ? La recherche d'informations textuelles n'est pas un processus homogène dans le temps-ni d'un point de vue cognitif, ni en termes de mouvement des yeux. Au contraire, ce processus implique plusieurs étapes ou phases, telles que la lecture normale, le balayage, la lecture attentive-en termes d'oculométrie-et la création et le rejet d'hypothèses, la confirmation et la décision-en termes cognitifs. Dans une première contribution, nous discutons d'une méthode d'analyse basée sur des chaînes semi-markoviennes cachées sur les signaux de mouvement des yeux afin de mettre en évidence quatre phases interprétables en termes de stratégie d'acquisition d'informations : lecture normale, lecture rapide, lecture attentive et prise de décision. Dans une deuxième contribution, nous lions ces phases aux changements caractéristiques des signaux EEG et des informations textuelles. En utilisant une représentation en ondelettes des EEG, cette analyse révèle des changements de variance et de corrélation des coefficients inter-canaux, en fonction des phases et de la largeur de bande. En utilisant des méthodes de plongement des mots, nous relions l'évolution de la similarité sémantique au sujet tout au long du texte avec les changements de stratégie. Dans une troisième contribution, nous présentons un nouveau modèle dans lequel les EEG sont directement intégrés en tant que variables de sortie afin de réduire l'incertitude des états. Cette nouvelle approche prend également en compte les aspects asynchrones et hétérogènes des données.
L'enseignement de la prosodie en cours de français langue étrangère (FLE) et l'utilisation d'un logiciel de parole, qui permet de visualiser la mélodie des apprenants et de celle d'un modèle francophone, afin de les comparer, font l'objet de cette recherche. Deux tentatives d'expérimentation de la correction de la prosodie de parole, des années soixante, prenant en compte la visualisation, sont évoquées à titre de rappel. Je me suis intéressée à la situation actuelle de l'enseignement de la prosodie du FLE et aux possibilités qu'offre l'outil numérique, afin de l'appréhender. J'ai voulu, entre autres, me rendre compte comment les apprenants réagissaient à un tel outil, et surtout si son utilisation apportait des résultats probants. Le travail avec un manuel, l'entraînement en autonomie avec un ordinateur, avec des explications collectives de l'enseignante, mais parfois aussi avec des explications individuelles, ont été proposés aux apprenants. Les deux premières expérimentations (pilote et générale) ont été effectuées auprès d'un public migrant, la troisième a été faite dans un laboratoire de langues en autonomie, et la dernière, avec des explications, en petits groupes. Les productions du modèle francophone et celles des apprenants ont été analysées. Les enquêtes concernant l'utilisation de l'outil numérique et la visualisation WinPitch (WP) et WinPitch Language Teaching and Learning (WP LTL) ont montré que la plupart des apprenants ont apprécié le travail avec ces logiciels. Pour les deux dernières expérimentations, menées dans un contexte universitaire, j'ai constitué deux groupes de travail : l'un expérimental avec WP, et l'autre de contrôle. Il s'est avéré que les étudiants du groupe expérimental ont eu de meilleurs résultats, en comparaison avec ceux du groupe de contrôle. Pour la dernière expérimentation, des explications concernant la 'grammaire prosodique', en l'occurrence celle, basée sur le modèle de Ph. Les résultats obtenus ont permis de valider les hypothèses de travail. Elles montrent aussi que l'utilisation de visualisation WP, accompagnée des explications de l'enseignant.e, sera bénéfique et justifiée en classe de langue, pour améliorer l'expression orale en français de façon consciente.
Le fil rouge de cette thèse est l'étude des processus de Hawkes. Ces processus ponctuels décryptent l'inter-causalité qui peut avoir lieu entre plusieurs séries d'événements. Concrètement, ils déterminent l'influence qu'ont les événements d'une série sur les événements futurs de toutes les autres séries. Par exemple, dans le contexte des réseaux sociaux, ils décrivent à quel point l'action d'un utilisateur, par exemple un Tweet, sera susceptible de déclencher des réactions de la part des autres. Le premier chapitre est une brève introduction sur les processus ponctuels suivie par un approfondissement sur les processus de Hawkes et en particulier sur les propriétés de la paramétrisation à noyaux exponentiels, la plus communément utilisée. Dans le chapitre suivant, nous introduisons une pénalisation adaptative pour modéliser, avec des processus de Hawkes, la propagation de l'information dans les réseaux sociaux. Cette pénalisation est capable de prendre en compte la connaissance a priori des caractéristiques de ces réseaux, telles que les interactions éparses entre utilisateurs ou la structure de communauté, et de les réfléchir sur le modèle estimé. Notre technique utilise des pénalités pondérées dont les poids sont déterminés par une analyse fine de l'erreur de généralisation. Ensuite, nous abordons l'optimisation convexe et les progrès réalisés avec les méthodes stochastiques du premier ordre avec réduction de variance. Le quatrième chapitre est dédié à l'adaptation de ces techniques pour optimiser le terme d'attache aux données le plus couramment utilisé avec les processus de Hawkes. De plus, de telles fonctions comportent beaucoup de contraintes linéaires qui sont fréquemment violées par les algorithmes classiques du premier ordre, mais, dans leur version duale ces contraintes sont beaucoup plus aisées à satisfaire. Ainsi, la robustesse de notre algorithme est d'avantage comparable à celle des méthodes du second ordre dont le coût est prohibitif en grandes dimensions. Enfin, le dernier chapitre présente une nouvelle bibliothèque d'apprentissage statistique pour Python 3 avec un accent particulier mis sur les modèles temporels. Appelée tick, cette bibliothèque repose sur une implémentation en C++ et les algorithmes d'optimisation issus de l'état de l'art pour réaliser des estimations très rapides dans un environnement multi-cœurs. Publiée sur Github, cette bibliothèque a été utilisée tout au long de cette thèse pour effectuer des expériences.
La cohérence est une propriété qui caractérise un texte comme un tout unifié et interprétatif. Les apprenants chinois de français langue étrangère (FLE) peuvent avoir de la difficulté à produire un texte cohérent en français. Cette thèse vise donc à analyser la gestion de la cohérence textuelle de ces apprenants, et plus précisément, les emplois des connecteurs ainsi que la résolution des anaphores et des chaînes de référence dans les productions écrites d'étudiants chinois de niveau intermédiaire et avancé de FLE en France. Les deux cohortes reçoivent les mêmes consignes et réalisent deux tâches de rédaction (narration et argumentation) en utilisant le logiciel de traitement de texte GenoGraphiX qui enregistre et reconstitue le processus d'écriture.
Dans le domaine de la néologie, différentes approches méthodologiques ont été développées pour la détection et l'extraction de néologismes sémantiques. Ces approches utilisent des stratégies telles que la désambiguïsation sémantique et la modélisation thématique, mais il n'existe aucun système complet de détection de néologismes sémantiques. Ainsi, nous proposons dans cette thèse le développement des algorithmes qui permettent d'identifier et d'extraire les néologismes sémantiques au moyen de méthodes statistiques,d'extraction d'information et d'apprentissage automatique. La méthodologie proposée est basée sur le traitement du processus de détection et d'extraction en tant que problème de classification. Il consiste à analyser la proximité des thèmes entre le champ sémantique de la signification principale d'un terme et son contexte. Pour la construction du système nous avons étudié cinq méthodes de classification automatique supervisée et trois modèles pour la génération de représentations vectorielles de mots par apprentissage profonde. Le corpus d'analyse est composé de néologismes sémantiques du domaine informatique appartenant à la base de données de l'Observatoire de Néologie de l'Université Pompeu Fabra, enregistrés de 1989 à 2015. Nous utilisons ce corpus pour évaluer les différentes méthodes mises en oeuvre par le système : classification automatique, extraction de mots à partir de contextes courts et génération de listes de mots similaires. Cette première approche méthodologique cherche à établir un cadre de référence en termes de détection et d'extraction de néologismes sémantiques.
Les systèmes stochastiques à information partielle permettent de représenter de nombreux systèmes dont les paramètres sont inconnus et dont le fonctionnement dépend de facteurs en dehors de notre contrôle. Dans cette thèse, nous étudions plusieurs problèmes liés à ces systèmes. Le premier est la diagnosticabilité, c'est-à-dire la capacité de décider si un évènement particulier s'est produit. Le second est la classification qui est la capacité de décider à partir d'une trace d'une exécution quel système l'a produite. Enfin, nous nous intéressons aux garanties que l'on peut avoir quand on apprend les probabilités de transition d'un système stochastique.
A travers les siècles, les institutions financières ont façonné le paysage financier et influencé l'activité économique. L'objectif de cette thèse est de mettre en évidence, d'un point de vue théorique, les limites fondamentales des institutions modernes et d'en déduire les implications concernant le futur rôle de ces institutions. Le premier chapitre propose un analyse des Chambres de Compensation. A la suite de la crise financière de 2008, les autorités financières à travers le monde ont mis en place des réglementations imposant la compensation centrale sur la plupart des produits dérivés. Nous montrons que la compensation centrale nécessite un plus grand niveau de liquidité que la compensation bilatérale. Le second chapitre présente un modèle d'apprentissage en temps continu censé Le dernier chapitre introduit et analyse le Lightning Network qui est un réseau de paiement basé sur la Blockchain. Il permet aux utilisateurs de transférer de la valeur de façon instantanée sans avoir recours à un tiers de confiance. Nous discutons des implications à propos de la structure de ce réseau de paiement ainsi que de sa capacité à prendre une place importante dans le paysage financier.
Le trafic maritime est le principal contributeur des bruits sous-marins anthropique : depuis les années 1970, l'augmentation du trafic maritime hauturier a provoqué dans certaines zones une augmentation du bruit ambiant de plus de 10 dB. En réponse à cette préoccupation, la Directive Cadre pour la Stratégie pour le Milieu Marin (DCSMM) recommande un suivi acoustique. Peu d'études s'intéressent à l'activité côtière et aux bruits rayonnés par les petites embarcations ainsi qu'à leurs conséquences sur la faune marine alors que ces environnements côtiers sont les pourvoyeurs de 41.7 % des services écosystémiques produits par les océans. A mi-chemin entre le monde académique et le monde industriel, le travail présenté aux différents questions scientifiques et industrielles sur la thématique du trafic côtier, en termes de l'étude de son influence dans le paysage acoustique et de capacité à détecter et classifier les embarcations côtières. En l'absence d'information sur le trafic maritime côtier, un protocole d'identification visuelle par traitement d'images GoPro ® produisant les mêmes données que l'AIS (position, vitesse, taille et type d'embarcation) est proposé et permet la création de carte du trafic maritime sur un disque de 1.6km de rayon. D'un point de vue acoustique, le trafic est caractérisé par deux descripteurs acoustiques, le SPL lié à la distance du bateau le plus proche et l'ANL caractérisant le nombre de bateaux dans un disque de 500 m de rayon. Le suivi spatio-temporel de ces descripteurs permet d'identifier l'impact du trafic maritime dans le paysage acoustique des environnements côtiers. La détection et la classification sont réalisées après caractérisation individuelle du bruit par un ensemble de paramètres acoustiques et par l'utilisation d'algorithmes d'apprentissage supervisé. Un protocole spécifique pour la création de l'arborescence de classification est proposé par comparaison des données acoustiques aux caractéristiques physiques et contextuelle de chaque bateau. Les travaux présentés sont illustrés sur la flottille d'embarcations côtières présente dans la baie de Calvi (Corse) durant la saison estivale.
Aujourd'hui, la communauté scientifique a l'opportunité de partager des connaissances et d'accéder à de nouvelles informations à travers les documents publiés et stockés dans les bases en ligne du web. Dans ce contexte, la valorisation des données disponibles reste un défi majeur pour permettre aux experts de les réutiliser et les analyser afin de produire de la connaissance du domaine. Pour être valorisées, les données pertinentes doivent être extraites des documents puis structurées. Nos travaux s'inscrivent dans la problématique de la capitalisation des données expérimentales issues des articles scientifiques, sélectionnés dans des bases en ligne, afin de les réutiliser dans des outils d'aide à la décision. Les mesures expérimentales (par exemple, la perméabilité à l'oxygène d'un emballage ou le broyage d'une biomasse) réalisées sur différents objets d'études (par exemple, emballage ou procédé de bioraffinerie) sont représentées sous forme de relations n-aires dans une Ressource Termino-Ontologique (RTO). La RTO est modélisée pour représenter les relations n-aires en associant une partie terminologique et/ou linguistique aux ontologies afin d'établir une distinction claire entre la manifestation linguistique (le terme) et la notion qu'elle dénote (le concept). La thèse a pour objectif de proposer une contribution méthodologique d'extraction automatique ou semi-automatique d'arguments de relations n-aires provenant de documents textuels afin de peupler la RTO avec de nouvelles instances. De manière précise, nous cherchons, dans un premier temps, à extraire des termes, dénotant les concepts d'unités de mesure, réputés difficiles à identifier du fait de leur forte variation typographique dans les textes. Après la localisation de ces derniers par des méthodes de classification automatique, les variants d'unités sont identifiés en utilisant des mesures d'édition originales.
L'hyperlien est une ressource qui permet de relier entre eux des documents textuels et sonores ou des images fixes ou animées, publiés sur le World Wide Web. Il est ainsi devenu le moyen standard par lequel les internautes, les éditeurs de contenus ou les moteurs de recherche élaborent et diffusent des informations. De prime abord, le rapport entre l'hyperlien et le droit de l'Union européenne peut paraître diffus. Pourtant, l'hyperlien soulève une série d'enjeux juridiques qui justifie de poser la question de son appréhension en le droit de l'UE. On peut donc s'attendre à ce que le droit de l'UE favorise, voire protège la création d'hyperliens. D'un autre côté, certains usages de l'hyperlien sont décriés. L'hyperlien participe également activement à la dissémination sur Internet de contenus illicites ou préjudiciables, en opposition avec le droit de l'UE. Au vu de ces enjeux, l'ambition de cette étude est d'évaluer en quoi, et dans quelle mesure, le droit de l'UE permet de délimiter de manière cohérente la liberté de lier et ses limites. Afin de le découvrir, notre thèse s'emploie dans un premier temps à démontrer l'émergence d'une liberté de lier au regard du droit européen de la propriété intellectuelle. L'appréhension de l'hyperlien par le droit de l'UE achoppe toutefois sur le terrain de la cohérence. Afin d'y remédier, des propositions normatives visant à rétablir la stabilité du droit jalonnent cette étude. Cette thèse fournit ainsi un socle de compréhension et de réflexion, commun à l'Union européenne, quant à l'application du droit à l'hyperlien.
Développer des ressources lexico-sémantiques pour le Traitement Automatique des Langues Naturelles est un enjeu majeur du domaine. Ces ressources explicitant notamment des connaissances que seuls les humains possèdent, ont pour but de permettre aux applications de TALNune compréhension de texte assez fine et complète. De nouvelles approches populaires de construction de ces dernières impliquant l'externalisation ouverte (crowdsourcing) émergent en TALN. Dans ce travail de recherche, nous prenons comme exemple d'étude le réseau lexico Ce système se base principalement sur l'enrichissement du réseau par l'inférence et l'annotation de nouvelles relations à partir de celles existantes, ainsi que l'extraction de règles d'inférence permettant de (re)générer une grande partie du réseau.
L'objectif de cette thèse, menée dans un cadre industriel, est d'apparier des contenus textuels médiatiques. Plus précisément, il s'agit d'apparier à des articles de presse en ligne des vidéos pertinentes, pour lesquelles nous disposons d'une description textuelle. Notre problématique relève donc exclusivement de l'analyse de matériaux textuels, et ne fait intervenir aucune analyse d'image ni de langue orale. Surviennent alors des questions relatives à la façon de comparer des objets textuels, ainsi qu'aux critères mobilisés pour estimer leur degré de similarité. L'un de ces éléments est selon nous la similarité thématique de leurs contenus, autrement dit le fait que deux documents doivent relater le même sujet pour former une paire pertinente. Ces problématiques relèvent du domaine de la recherche d'information (ri), dans lequel nous nous ancrons principalement. Le système d'appariement développé dans cette thèse distingue donc différentes étapes qui se complètent. Dans un premier temps, l'indexation des contenus fait appel à des méthodes de traitement automatique des langues (tal) pour dépasser la représentation classique des textes en sac de mots. Ensuite, deux scores sont calculés pour rendre compte du degré de similarité entre deux contenus : l'un relatif à leur similarité thématique, basé sur un modèle vectoriel de ri ;  L'évaluation des performances du système a elle aussi fait l'objet de questionnements dans ces travaux de thèse. Les contraintes imposées par les données traitées et le besoin particulier de l'entreprise partenaire nous ont en effet contraints à adopter une alternative au protocole classique d'évaluation en ri, le paradigme de Cranfield.
Depuis peu, émerge une réelle dynamique de constitution et de diffusion de corpus d'écrits scolaires, notamment francophones. Ces corpus, qui appuient les travaux en didactique de l'écriture, sont souvent de taille restreinte et peu diffusés. Des corpus longitudinaux, c'est-à-dire réalisant le suivi d'une cohorte d'élèves et permettant de s'intéresser à la progressivité des apprentissages, n'existent pas à ce jour pour le français. Par ailleurs, bien que le traitement automatique des langues (TAL) ait outillé des corpus de natures très diverses, peu de travaux se sont intéressés aux écrits scolaires. Ce nouveau champ d'application représente un défi pour le TAL en raison des spécificités des écrits scolaires, et particulièrement les nombreux écarts à la norme qui les caractérisent. Les outils proposés à l'heure actuelle ne conviennent donc pas à l'exploitation de ces corpus. Il y a donc un enjeu pour le TAL à développer des méthodes spécifiques. Cette thèse présente deux apports principaux. D'une part, ce travail a permis la constitution d'un corpus d'écrits scolaires longitudinal (CP-CM2), de grande taille et numérisé, le corpus Scoledit. Par « constitution » , nous entendons le recueil, la numérisation et la transcription des productions, l'annotation des données linguistiques et la diffusion de la ressource ainsi constituée. D'autre part, ce travail a donné lieu à l'élaboration d'une méthode d'exploitation de ce corpus, appelée approche par comparaison, qui s'appuie sur la comparaison entre la transcription des productions et une version normalisée de ces productions pour produire des analyses. Cette méthode a nécessité le développement d'un aligneur de formes, appelé AliScol, qui permet de mettre en correspondance les formes produites par l'élève et les formes normalisées. Cet outil représente un premier niveau d'alignement à partir duquel différentes analyses linguistiques ont été menées (lexicales, morphographiques, graphémiques). La conception d'un aligneur en graphèmes, appelé AliScol_Graph, a été nécessaire pour conduire une étude sur les graphèmes.
La mémoire de travail peut être définie comme la capacité à stocker temporairement et à manipuler des informations de toute nature. Par exemple, imaginez que l'on vous demande d'additionner mentalement une série de nombres. Afin de réaliser cette tâche, vous devez garder une trace de la somme partielle qui doit être mise à jour à chaque fois qu'un nouveau nombre est donné. La mémoire de travail est précisément ce qui permettrait de maintenir (i.e. stocker temporairement) la somme partielle et de la mettre à jour (i.e. manipuler). Dans cette thèse, nous proposons d'explorer les implémentations neuronales de cette mémoire de travail en utilisant un nombre restreint d'hypothèses. Pour ce faire, nous nous plaçons dans le contexte général des réseaux de neurones récurrents et nous proposons d'utiliser en particulier le paradigme du reservoir computing. Ce type de modèle très simple permet néanmoins de produire des dynamiques dont l'apprentissage peut tirer parti pour résoudre une tâche donnée. Dans ce travail, la tâche à réaliser est une mémoire de travail à porte (gated working memory). Le modèle reçoit en entrée un signal qui contrôle la mise à jour de la mémoire. Lorsque la porte est fermée, le modèle doit maintenir son état de mémoire actuel, alors que lorsqu'elle est ouverte, il doit la mettre à jour en fonction d'une entrée. Dans notre approche, cette entrée supplémentaire est présente à tout instant, même lorsqu'il n'y a pas de mise à jour à faire. En d'autres termes, nous exigeons que notre modèle soit un système ouvert, i.e. un système qui est toujours perturbé par ses entrées mais qui doit néanmoins apprendre à conserver une mémoire stable. Dans la première partie de ce travail, nous présentons l'architecture du modèle et ses propriétés, puis nous montrons sa robustesse au travers d'une étude de sensibilité aux paramètres. Celle-ci montre que le modèle est extrêmement robuste pour une large gamme de paramètres. Peu ou prou, toute population aléatoire de neurones peut être utilisée pour effectuer le gating. Par ailleurs, après apprentissage, nous mettons en évidence une propriété intéressante du modèle, à savoir qu'une information peut être maintenue de manière entièrement distribuée, i.e. sans être corrélée à aucun des neurones mais seulement à la dynamique du groupe. Plus précisément, la mémoire de travail n'est pas corrélée avec l'activité soutenue des neurones ce qui a pourtant longtemps été observé dans la littérature et remis en cause récemment de façon expérimentale. Ce modèle vient confirmer ces résultats au niveau théorique. Dans la deuxième partie de ce travail, nous montrons comment ces modèles obtenus par apprentissage peuvent être étendus afin de manipuler l'information qui se trouve dans l'espace latent. Nous proposons pour cela de considérer les conceptors qui peuvent être conceptualisé comme un jeu de poids synaptiques venant contraindre la dynamique du réservoir et la diriger vers des sous-espaces particuliers ; par exemple des sous-espaces correspondants au maintien d'une valeur particulière. Plus généralement, nous montrons que ces conceptors peuvent non seulement maintenir des informations, ils peuvent aussi maintenir des fonctions. Dans le cas du calcul mental évoqué précédemment, ces conceptors permettent alors de se rappeler et d'appliquer l'opération à effectuer sur les différentes entrées données au système. Ces conceptors permettent donc d'instancier une mémoire de type procédural en complément de la mémoire de travail de type déclaratif. Nous concluons ce travail en remettant en perspective ce modèle théorique vis à vis de la biologie et des neurosciences.
La segmentation et le regroupement en locuteurs (SRL) impliquent la détection des locuteurs dans un flux audio et les intervalles pendant lesquels chaque locuteur est actif, c'est-à-dire la détermination de 'qui parle quand'. La première partie des travaux présentés dans cette thèse exploite une approche de modélisation du locuteur utilisant des clés binaires (BKs) comme solution à la SRL. La modélisation BK est efficace et fonctionne sans données d'entraînement externes, car elle utilise uniquement des données de test. Les contributions présentées incluent l'extraction des BKs basée sur l'analyse spectrale multi-résolution, la détection explicite des changements de locuteurs utilisant les BKs, ainsi que les techniques de fusion SRL qui combinent les avantages des BKs et des solutions basées sur un apprentissage approfondi. La tâche de la SRL est étroitement liée à celle de la reconnaissance ou de la détection du locuteur, qui consiste à comparer deux segments de parole et à déterminer s'ils ont été prononcés par le même locuteur ou non. Même si de nombreuses applications pratiques nécessitent leur combinaison, les deux tâches sont traditionnellement exécutées indépendamment l'une de l'autre. La deuxième partie de cette thèse porte sur une application où les solutions de SRL et de reconnaissance des locuteurs sont réunies. La nouvelle tâche, appelée détection de locuteurs à faible latence, consiste à détecter rapidement les locuteurs connus dans des flux audio à locuteurs multiples. Il s'agit de repenser la SRL en ligne et la manière dont les sous-systèmes de SRL et de détection devraient être combinés au mieux.
Les objets connectés ont aujourd'hui pénétré les foyers et, poussés par une société tournée de plus en plus vers le bien-être, ces capteurs mesurent et proposent dorénavant une grande variété de données physiologiques. L'arrivée à maturité des technologies de la réalité virtuelle, couplée avec l'avènement des objets connectés, permet et favorise dès lors de nouvelles perspectives dans la proposition d' Ceux-ci se basent principalement sur du matériel médical, qui possède des contraintes d'utilisation forte, reste souvent encombrant et limite de fait la mobilité des utilisateurs. Nous étudierons en particulier l'influence d'un biofeedback cardiaque, via des capteurs connectés grand public, sur l'engagement utilisateur et le sentiment d'agentivité. Nous avons ainsi mené deux expérimentations nous permettant d'étudier l'impact des différentes modalités de biofeedback sur l'expérience utilisateur. Notre première expérimentation met en place un biofeedback cardiaque dans un jeu d'horreur en réalité virtuelle, permettant d'augmenter le sentiment de peur. Les résultats de cette expérimentationconfortent l'intérêt de l'utilisation de capteurs connectés comme moyen de captation physiologique dans des expériences de réalité virtuelle immersive. Ils mettent également en avant l'impact positif de ce biofeedback sur la dimension d'engagement de l'expérience utilisateur. La deuxième expérience porte sur l'utilisation de l'activité cardiaque Les résultats de cette expérience démontrent la possibilité d'utiliser la dite mécanique pour des expériences virtuelles immersives et indiquent un impact positif sur le sentiment d'agentivité, lié au niveau de compétence des participants. Sur un plan théorique, cette thèse propose une synthèse des modèles de l'expérience utilisateur en environnement virtuel et soumet par ailleurs les bases d'un modèle que nous nommons « l'immersion physiologique » .
Les classificateurs ga (cl.16), ku (cl.17) et mu (cl.18) marquent respectivement les valeurs de «  contact  » , de «  distance  » et d' « intériorité  »   : a) lorsqu'ils sont préfixés à la base ‑úma /endroit/  ; b) lorsqu'ils apparaissent dans le verbe conjugué ou préfixés aux thèmes des déterminants  ; c) lorsqu'ils sont suivis d'un nom en isolation ; d) ou lorsqu'ils ils sont suivis d'un verbe. Bu (cl.14) marque en outre une valeur « abstraite  » lorsqu'il se combine avec des bases lexicales  ; il exprime le temps, la comparaison et la cause, lorsqu'il est préfixé aux thèmes des déterminants  ; il sert en troisième lieu à marquer l'hypothèse lorsqu'il est employé sous la forme d'un morphème libre.
Depuis les années 2000, un progrès significatif est enregistré dans les travaux de recherche qui proposent l'apprentissage de détecteurs d'objets sur des grandes bases de données étiquetées manuellement et disponibles publiquement. Cependant, lorsqu'un détecteur générique d'objets est appliqué sur des images issues d'une scène spécifique les performances de détection diminuent considérablement. Cette diminution peut être expliquée par les différences entre les échantillons de test et ceux d'apprentissage au niveau des points de vues prises par la(les) caméra(s), de la résolution, de l'éclairage et du fond des images. De plus, l'évolution de la capacité de stockage des systèmes informatiques, la démocratisation de la "vidéo-surveillance" et le développement d'outils d'analyse automatique des données vidéos encouragent la recherche dans le domaine du trafic routier. Les buts ultimes sont l'évaluation des demandes de gestion du trafic actuelles et futures, le développement des infrastructures routières en se basant sur les besoins réels, l'intervention pour une maintenance à temps et la surveillance des routes en continu. Par ailleurs, l'analyse de trafic est une problématique dans laquelle plusieurs verrous scientifiques restent à lever. Ces derniers sont dus à une grande variété dans la fluidité de trafic, aux différents types d'usagers, ainsi qu'aux multiples conditions météorologiques et lumineuses. Ainsi le développement d'outils automatiques et temps réel pour l'analyse vidéo de trafic routier est devenu indispensable. Ces outils doivent permettre la récupération d'informations riches sur le trafic à partir de la séquence vidéo et doivent être précis et faciles à utiliser. C'est dans ce contexte que s'insèrent nos travaux de thèse qui proposent d'utiliser les connaissances antérieurement acquises et de les combiner avec des informations provenant de la nouvelle scène pour spécialiser un détecteur d'objet aux nouvelles situations de la scène cible. Dans cette thèse, nous proposons de spécialiser automatiquement un classifieur/détecteur générique d'objets à une scène de trafic routier surveillée par une caméra fixe. Nous présentons principalement deux contributions. Cette formalisation approxime itérativement la distribution cible inconnue au départ, comme étant un ensemble d'échantillons de la base spécialisée à la scène cible. Les échantillons de cette dernière sont sélectionnés à la fois à partir de la base source et de la scène cible moyennant une pondération qui utilise certaines informations a priori sur la scène. La base spécialisée obtenue permet d'entraîner un classifieur spécialisé à la scène cible sans intervention humaine. La deuxième contribution consiste à proposer deux stratégies d'observation pour l'étape mise à jour du filtre SMC. Elles sont utilisées pour la pondération des échantillons cibles. Les différentes expérimentations réalisées ont montré que l'approche de spécialisation proposée est performante et générique. Nous avons pu y intégrer de multiples stratégies d'observation. Elle peut être aussi appliquée à tout type de classifieur. De plus, nous avons implémenté dans le logiciel OD SOFT de Logiroad les possibilités de chargement et d'utilisation d'un détecteur fourni par notre approche. Nous avons montré également les avantages des détecteurs spécialisés en comparant leurs résultats avec celui de la méthode Vu-mètre de Logiroad.
Le discours produit lors d'une visite touristique naît de différentes modalités de communication dont la visite assistée par un dispositif socio-technique et la visite-conférence dirigée par un médiateur. Plusieurs interrogations se posent dont celle d'une taxonomie des genres de discours liés au domaine spécialisé étudié, celle d'une unité de segmentation textuelle s'affranchissant du caractère scriptural ou oral du mode de production du texte, et celles liées à la catégorisation d'un texte dans un genre discursif. En effet, les valeurs des paramètres de caractérisation doivent permettre l'introduction d'un prototype indispensable à la catégorisation et à l'indexation textuelle du genre étudié. En outre, le traitement quantitatif d'une compilation de textes sélectionnés trouve ses fondements au sein même de l'analyse de discours et de la linguistique de corpus. La méthode suivie, qui introduit les règles de segmentation textuelle dont l'annotation manuelle qualitative, et l'analyse quantitative permettent de proposer un modèle d'organisation de chaque genre considéré.
Cette thèse porte sur l'intégration de données brutes provenant de sources hétérogènes sur le Web. L'objectif global est de fournir une architecture générique et modulable capable de combiner, de façon sémantique et intelligente, ces données hétérogènes dans le but de les rendre réutilisables. Dans ce rapport, nous proposons de nouveaux modèles et techniques permettant d'adapter le processus de combinaison et d'intégration à la diversité des sources de données impliquées. Les problématiques sont une gestion transparente et dynamique des sources de données, passage à l'échelle et responsivité par rapport au nombre de sources, adaptabilité au caractéristiques de sources, et finalement, consistance des données produites(données cohérentes, sans erreurs ni doublons). Pour répondre à ces problématiques, nous proposons un méta-modèle pour représenter ces sources selon leurs caractéristiques, liées à l'accès (URI) ou à l'extraction (format) des données, mais aussi au capacités physiques des sources (latence, volume). En s'appuyant sur cette formalisation, nous proposent différentes stratégies d'accès aux données, afin d'adapter les traitements aux spécificités des sources. En se basant sur ces modèles et stratégies, nous proposons une architecture orientée ressource, ou tout les composants sont accessibles par HTTP via leurs URI. Afin d'améliorer la qualité des données produites par notre approches, l'accent est mis sur l'incertitude qui peut apparaître dans les données sur le Web. Nous proposons un modèle, permettant de représenter cette incertitude, au travers du concept de ressource Web incertaines, basé sur un modèle probabiliste ou chaque ressource peut avoir plusieurs représentation possibles, avec une certaine probabilité. Cette approche sera à l'origine d'une nouvelle optimisation de l'architecture pour permettre de prendre en compte l'incertitude pendant la combinaison des données
La France connaît un vieillissement de sa population sans précédent. La part des séniors s'accroît et notre société se doit de repenser son organisation pour tenir compte de ce changement et mieux connaître cette population. De nombreuses cohortes de personnes âgées existent déjà à travers le monde dont quatre en France et, bien que la part de cette population vivant dans des structures d'hébergement collectif (EHPAD, cliniques de soins de suite) augmente, la connaissance de ces seniors reste lacunaire. Aujourd'hui les groupes privés de maisons de retraite et d'établissements sanitaires comme Korian ou Orpéa s'équipent de grandes bases de données relationnelles permettant d'avoir de l'information en temps réel sur leurs patients/résidents. Depuis 2010 les dossiers de tous les résidents Korian sont dématérialisés et accessibles par requêtes. Ils comprennent à la fois des données médico-sociales structurées décrivant les résidents et leurs traitements et pathologies, mais aussi des données textuelles explicitant leur prise en charge au quotidien et saisies par le personnel soignant. Au fil du temps et alors que le dossier résident informatisé (DRI) avait surtout été conçu comme une application de gestion de base de données, il est apparu comme une nécessité d'exploiter cette mine d'informations et de construire un outil d'aide à la décision destiné à améliorer l'efficacité des soins. L'Institut du Bien Vieillir IBV devenu entretemps la Fondation Korian pour le Bien Vieillir a alors choisi, dans le cadre d'un partenariat Public/Privé de financer un travail de recherche destiné à mieux comprendre le potentiel informatif de ces données, d'évaluer leur fiabilité et leur capacité à apporter des réponses en santé publique. Ce travail de recherche et plus particulièrement cette thèse a alors été pensée en plusieurs étapes. - D'abord l'analyse de contenu du data warehouse DRI, l'objectif étant de construire une base de données recherche, avec un versant social et un autre de santé. Ce fut le sujet du premier article. - Ensuite, par extraction directe des informations socio-démographiques des résidents dès leur entrée, de leurs hospitalisations et décès puis, par un processus itératif d'extractions d'informations textuelles de la table des transmissions et l'utilisation de la méthode Delphi, nous avons généré vingt-quatre syndromes, ajouté les hospitalisations et les décès et construit une base de données syndromique, la Base du Bien Vieillir (BBV). Ce système d'informations d'un nouveau type a permis la constitution d'une cohorte de santé publique à partir de la population des résidents de la BBV et l'organisation d'un suivi longitudinal syndromique de celle-ci. La BBV a également été évaluée scientifiquement dans un cadre de surveillance et de recherche en santé publique au travers d'une analyse de l'existant : contenu, périodicité, qualité des données. La cohorte construite a ainsi permis la constitution d'un outil de surveillance. Cet échantillon de population a été suivi en temps réel au moyen des fréquences quotidiennes d'apparitions des 26 syndromes des résidents. La méthodologie d'évaluation était celle des systèmes de surveillance sanitaire proposée par le CDC d'Atlanta et a été utilisée pour les syndromes grippaux et les gastro entérites aiguës. Ce fut l'objet du second article. - Enfin la construction d'un nouvel outil de santé publique : la distribution de chacun des syndromes dans le temps (dates de transmissions) et l'espace (les EHPAD de transmissions) a ouvert le champ de la recherche à de nouvelles méthodes d'exploration des données et permis d'étudier plusieurs problématiques liées à la personne âgée : chutes répétées, cancer, vaccinations et fin de vie.
Les systèmes dynamiques présentent des comportements temporels qui peuvent être exprimés sous diverses formes séquentielles telles que des signaux, des ondes, des séries chronologiques et des suites d'événements. Détecter des motifs sur de tels comportements temporels est une tâche fondamentale pour comprendre et évaluer ces systèmes. Étant donné que de nombreux comportements du système impliquent certaines caractéristiques temporelles, le besoin de spécifier et de détecter des motifs de comportements qui implique des exigences de synchronisation, appelées motifs temporisés, est évidente. Cependant, il s'agit d'une tâche non triviale due à un certain nombre de raisons, notamment la concomitance des sous-systèmes et la densité de temps. La contribution principale de cette thèse est l'introduction et le développement du filtrage par motif temporisé, c'est-à-dire l'identification des segments d'un comportement donné qui satisfont un motif temporisé. Nous proposons des expressions rationnelles temporisées (TRE) et la logique de la boussole métrique (MCL) comme langages de spécification pour motifs temporisés. Nous développons d'abord un nouveau cadre qui abstraite le calcul des aspects liés au temps appelé l'algèbre des relations temporisées. Ensuite, nous fournissons des algorithmes du filtrage hors ligne pour TRE et MCL sur des comportements à temps dense à valeurs discrètes en utilisant ce cadre et étudions quelques extensions pratiques. Il est nécessaire pour certains domaines d'application tels que le contrôle réactif que le filtrage par motif doit être effectué pendant l'exécution réelle du système. Pour cela, nous fournissons un algorithme du filtrage en ligne pour TREs basé sur la technique classique des dérivées d'expressions rationnelles. Nous croyons que la technique sous-jacente qui combine les dérivées et les relations temporisées constitue une autre contribution conceptuelle majeure pour la recherche sur les systèmes temporisés. Nous présentons un logiciel libre Montre qui implémente nos idées et algorithmes. Nous explorons diverses applications du filtrage par motif temporisé par l'intermédiaire de plusieurs études de cas. Enfin, nous discutons des orientations futures et plusieurs questions ouvertes qui ont émergé à la suite de cette thèse.
Cette thèse porte sur les différents facteurs impliqués dans le traitement des relatives, notamment le traitement des relatives sujet et objet ainsi que celui de l'attachement des relatives sujet. Je propose alors dans cette thèse de dépasser les notions de langue avec un avantage pour la relative sujet ou objet, ou avec une préférence avec attachement haut ou bas. Cette description permet de mieux appréhender leur rôle dans le traitement, notamment pour comprendre les différences constatées dans les expériences entre les langues. Je poursuis par une réflexion sur l'approche multifactorielle en tenant compte des facteurs liés aux domaines sémantique et pragmatique. Enfin, pour élargir les facteurs en jeu dans le traitement des relatives au-delà du domaine linguistique, je montre l'influence d'un domaine non linguistique (amorçage mathématique) sur le domaine linguistique (attachement) en français avec des expériences de choix restreints, et de mouvements oculaires avec le paradigme monde visuel.
Dans cette thèse, nous nous sommes intéressés à la gestion dynamique des processus métiers. D'autre part l'objectif est aussi de gérer les processus en prenant en compte les compétences nécessaires à leurs exécutions. Ce travail de thèse s'appuie sur l'approche BPM et plus précisément à sa phase d'exécution. Dans un monde de travail turbulent et en constante évolution, on parle souvent de modèles adaptables ou adaptatifs, de modèles qui s'enrichissent à chaque exécution et ne suivent pas un modèle structuré et prédéfini tel que le cas du BPM (Business Process Management) classique. Ainsi, les acteurs impliqués ne sont pas seulement supportés, ils sont forcés à effectuer les tâches dans des séquences spécifiées. De surcroit, un processus flexible c'est un processus capable de changer que les parties qui ont besoin d'être changées tout en gardant la stabilité de ses autres parties. Dans ce contexte, parmi les approche prometteuses, l'approche orientée services offre aux entreprises une modularité permettant de remplacer facilement un composant par un autre, de le réutiliser et d'étendre son objectif en lui ajoutant un autre composant. De ce fait, dans le but de supporter l'agilité des processus métiers, nous proposons une approche combinant les trois approches suivantes : l'approche BPM, la gestion des compétences et l'approche orientée service dans un environnement social supportant le travail collaboratif.
L'utilisation de termes équivalents ou sémantiquement proches est nécessaire pour augmenter la couverture et la sensibilité d'une application comme la recherche et l'extraction d'information ou l'annotation sémantique de documents. Dans le contexte de l'identification d'effets indésirables susceptibles d'être dûs à un médicament, la sensibilité est aussi recherchée afin de détecter plus exhaustivement les déclarations spontanées et de mieux surveiller le risque médicamenteux. C'est la raison qui motive notre travail. Dans notre travail de thèse, nous cherchons ainsi à détecter des termes sémantiquement proches et à les regrouper en utilisant plusieurs méthodes : des algorithmes de clustering non supervisés, des ressources terminologiques exploitées avec le raisonnement terminologique et des méthodes de Traitement Automatique de la Langue, comme la structuration de terminologies, où nous visons la détection de relations hiérarchiques et synonymiques. Nous avons réalisé de nombreuses expériences et évaluations des clusters générés, qui montrent que les méthodes proposées peuvent contribuer efficacement à la tâche visée.
Ce travail aura pour but d'explorer l'interface sémantique-syntaxe des « constructions » avec les prédicats secondaires, résultatifs et dépictifs. Une attention particulière sera donnée au problème du choix des sujets (ou des hôtes) de prédication pour ces deux types de prédicats, ainsi qu'à la classe aspectuelle lexicale du verbe à la base de la construction. Dans la première partie, nous introduisons divers patterns de la construction résultative et expliquons le principe de base qui régit la syntaxe de ces constructions, à savoir la Restriction sur l'objet direct. Après avoir écarté un nombre de prétendus contre-exemples à la Restriction DOR, nous réaffirmerons sa validité, notamment en tant que diagnostic de l'inaccusativité en anglais. La deuxième partie sera consacré aux prédicats dépictifs, notamment aux contraintes qui pèsent sur le choix du contrôleur pour ce type de prédication secondaire, ainsi qu'aux propriétés des adjectifs dépictifs en comparaison avec d'autres types d'adjoints participant-oriented. Nous étudions la distribution des adjectifs formes longues et formes courtes en russe, conditionnée par les propriétés d'accord qui les distinguent et esquissons un processus historique à l'origine de leur distribution dans la langue d'aujourd'hui.
Dans le domaine de la cartographie cérébrale, nous avons identifié le besoin d'un outil fondé sur la connaissance détaillée des sulci. Dans cette thèse, nous développons un nouvel outil de cartographie cérébrale appelé NeuroLang, qui s'appuie sur la géométrie spatiale du cerveau. Nous avons abordé cette question avec deux approches : premièrement, nous avons fermement fondé notre théorie sur la neuroanatomie classique. Deuxièmement, nous avons conçu et implémenté des méthodes pour les requêtes spécifiques au sulcus dans le langage spécifique au domaine, NeuroLang. Nous avons testé notre méthode sur 52 sujets et évalué les performances de NeuroLang pour des cartographie du cortex spécifiques à la population et au sujet. Ensuite, nous proposons une nouvelle organisation hiérarchique basée sur les données de la stabilité sulcal appuyée sur ces données. Pour conclure, nous avons résumé l'implication de notre méthode dans le domaine actuel, ainsi que notre contribution globale au domaine de la cartographie cérébrale.
L'acquisition du lexique précoce est très importante dans le développement du langage dans la mesure où les mots sont constitutifs des énoncés signifiants de l'enfant mais également car leur développement préfigure dans une certaine mesure les habiletés langagières ultérieures. Il est aujourd'hui admis que l'acquisition du lexique se fait sur la base d'étapes communes mais au sein desquelles il existe de fortes variations inter-individuelles, qui selon les chercheurs seraient d'ordre linguistiques, sociales ou idiosyncrasiques. Cependant, il reste encore des zones d'ombre, notamment sur l'influence possible des méthodes d'évaluation sur les résultats ;  et malgré le fait que certains chercheurs conseillent l'utilisation conjointe de plusieurs méthodes de collecte pour éviter cette influence liée à la méthodologie, cette préconisation est peu suivie. Cette thèse vise à étudier les trajectoires développementales du lexique en production et leurs variations selon les enfants ;  plus spécifiquement, il s'agit de montrer l'apport de méthodes complémentaires et l'importance de l'exploration du contexte de production des mots lors des observations spontanées en milieu naturel pour mieux interpréter les différences inter-individuelles. Globalement, le développement et la composition du vocabulaire des 10 enfants évalués par l'IFDC suivent les tendances observées dans la littérature. Nous nous sommes ensuite focalisés sur 4 de ces enfants pour les stades linguistiques 15-25 ; 50 ; 70-120 mots (corpus CIBLÉ). L'utilisation des deux méthodes – questionnaires parentaux et données spontanées – a permis d'évaluer le développement lexical de manière plus fiable et complète, les avantages d'une méthode permettant de combler les limites de l'autre. Afin de mieux comprendre les divergences de certains résultats entre ces deux méthodes, nous avons poursuivi nos investigations sur les données spontanées des 4 enfants en examinant les contextes situationnels et interactionnels. Nous avons défini et catégorisé les situations présentes dans les enregistrements du corpus TOTAL. Par exemple, il est apparu que les deux enfants dont les effectifs de mots sont les moins élevés au niveau des données spontanées ont été davantage filmés en situation ludique solitaire ; situation où les analyses révèlent que le nombre d'unités lexicales produites est le plus faible. Ensuite, un autre travail a consisté à décrire le contexte interactionnel et plus précisément à comprendre les implications des enfants dans les échanges interactionnels. Beaucoup de différences inter-individuelles sont apparues, dont certaines nous permettent de clarifier les données des enfants. Ainsi, chaque analyse apporte des informations complémentaires – du vocabulaire estimé des questionnaires parentaux, au vocabulaire en usage enregistré en milieu naturel. En dépit du nombre restreint d'enfants qui composent cet échantillon, ces résultats encouragent l'utilisation de méthodes complémentaires. L'analyse des contextes situationnels et interactionnels nous semble aussi cruciale pour comprendre les mesures lexicales des enfants et mieux interpréter les différences intra et inter-individuelles.
L'analyse du domaine vise à identifier et organiser les caractéristiques communes et variables dans un domaine. La contribution générale de cette thèse consiste à adopter et exploiter des techniques de traitement automatique du langage naturel et d'exploration de données pour automatiquement extraire et modéliser les connaissances relatives à la variabilité à partir de documents informels. Nous étudions l'applicabilité de notre idée à travers deux études de cas pris dans deux contextes différents : (1) la rétro-ingénierie des Modèles de Features (FMs) à partir des exigences réglementaires de sûreté dans le domaine de l'industrie nucléaire civil et (2) l'extraction de Matrices de Comparaison de Produits (PCMs) à partir de descriptions informelles de produits. Dans la première étude de cas, nous adoptons des techniques basées sur l'analyse sémantique, le regroupement (clustering) des exigences et les règles d'association. L'évaluation de cette approche montre que 69% de clusters sont corrects sans aucune intervention de l'utilisateur. Les dépendances entre features montrent une capacité prédictive élevée : 95% des relations obligatoires et 60% des relations optionnelles sont identifiées, et la totalité des relations d'implication et d'exclusion sont extraites. Dans la deuxième étude de cas, notre approche repose sur la technologie d'analyse contrastive pour identifier les termes spécifiques au domaine à partir du texte, l'extraction des informations pour chaque produit, le regroupement des termes et le regroupement des informations. Notre étude empirique montre que les PCMs obtenus sont compacts et contiennent de nombreuses informations quantitatives qui permettent leur comparaison. L'expérience utilisateur montre des résultats prometteurs et que notre méthode automatique est capable d'identifier 43% de features correctes et 68% de valeurs correctes dans des descriptions totalement informelles et ce, sans aucune intervention de l'utilisateur. Nous montrons qu'il existe un potentiel pour compléter ou même raffiner les caractéristiques techniques des produits. La principale leçon à tirer de ces deux études de cas, est que l'extraction et l'exploitation de la connaissance relative à la variabilité dépendent du contexte, de la nature de la variabilité et de la nature du texte.
Avec le développement et la multiplication des appareils connectés dans tous les domaines, de nouvelles solutions pour le traitement de flux de données ont vu le jour. Cette thèse s'inscrit dans ce contexte : elle a été réalisée dans le cadre du projet FUI Waves, une plateforme de traitement de flux distribués. Le cas d'usage pour le développement a été la gestion des données provenant d'un réseau de distribution d'eau potable, plus précisément la détection d'anomalie dans les mesures de qualité et leur contextualisation par rapport à des données extérieures. Plusieurs contributions ont été réalisées et intégrées à différentes étapes du projet, leur évaluation et les publications liées témoignant de leur pertinence. Celles-ci se basent sur une ontologie que j'ai spécifiée depuis des échanges avec les experts du domaine travaillant dans chez le partenaire métier du projet. L'utilisation de données géographiques a permis de réaliser un système de profilage visant à améliorer le processus de contextualisation des erreurs. Un encodage de l'ontologie adapté au traitement de flux de données RDF a été développé pour supporter les inférences de RDFS enrichis de owl : sameAs. Conjointement, un formalisme compressé de représentation des flux (PatBin) a été conçu et implanté dans la plateforme. Il se base sur la régularité des motifs des flux entrants. Enfin, un langage de requêtage a été développé à partir de ce formalisme. Il intègre une stratégie de raisonnement se basant sur la matérialisation et la réécriture de requêtes. Enfin, à partir de déductions provenant d'un d'apprentissage automatique, un outil de génération de requêtes a été implanté. Ces différentes contributions ont été évaluées sur des jeux de données concrets du domaine ainsi que sur des jeux d'essais synthétiques.
Cette thèse étudie les méthodes de regroupement basées sur le principe des plus proches voisins partagés (SNN). Comme la plupart des autres approches de clustering à base de graphe, les méthodes SNN sont effectivement bien adaptées à surmonter la complexité des données, l'hétérogénéité et la haute dimensionnalité. La première contribution de la thèse est de revisiter une méthode existante basée sur les voisins partagés en deux points. Nous présentons d'abord un formalisme basé sur la la théorie de décision à contrario. Cela nous permet de tirer des scores de connectivité plus fiable des groupes et une interprétation plus intuitive des voisinage selectionnés optimalement. Nous proposons également un nouveau algorithme de factorisation pour accélérer le calcul intensif nécessaire des matrices des voisins partagés. La deuxième contribution de cette thèse est une généralisation de la classification SNNau cas multi-source. La principale originalité de notre approche est que nous introduisons une étape de sélection des sources d'information optimales dans le calcul de scores de groupes candidats. Chaque groupe est alors associé à son propre sous-ensemble optimal des modalités. Comme le montre le expériences, cette étape de sélection de source rend notre approche largement robuste à la présence de sources locales aberrantes. Cette nouvelle méthode est appliquée à un large éventail de problèmes, y compris la structuration multimodale des collections d'images et dans le regroupement dans des sous-espaces basés sur les projections aléatoires. La troisième contribution de la thèse est une tentative pour étendre les méthodes SNNdans le contexte des graphes biparites. Nous introduisons de nouvelles mesures de pertinence SNNrevisitées pour ce contexte asymétrique et nous montrons qu'elles peuvent être utiliséespour sélectionner localement des voisinages optimales. En conséquence, nous proposons un nouveau algorithme de clustering bipartite SNN qui est appliqué à la découverte d'objets visuels. Les expériences montrent que cette nouvelle méthode est meilleure par rapport aux méthodes de l'état de l'art. Basé sur les objets découverts, nous introduisons également un paradigme de recherche visuelle, c.-à-d les objet basés sur la suggestion de requêtes visuel les.
Depuis plusieurs années, la notion de Big Data s'est largement développée. Afin d'analyser et explorer toutes ces données, il a été nécessaire de concevoir de nouvelles méthodes et de nouvelles technologies. Aujourd'hui, le Big Data existe également dans le domaine de la santé. Les hôpitaux en particulier, participent à la production de données grâce à l'adoption du dossier patient électronique. L'objectif de cette thèse a été de développer des méthodes statistiques réutilisant ces données afin de participer à la surveillance syndromique et d'apporter une aide à la décision. Cette étude comporte 4 axes majeurs. Tout d'abord, nous avons montré que les données massives hospitalières étaient très corrélées aux signaux des réseaux de surveillance traditionnels. Dans un second temps, nous avons établi que les données hospitalières permettaient d'obtenir des estimations en temps réel plus précises que les données du web, et que les modèles SVM et Elastic Net avaient des performances comparables. Puis, nous avons appliqué des méthodes développées aux Etats-Unis réutilisant les données hospitalières, les données du web (Google et Twitter) et les données climatiques afin de prévoir à 2 semaines les taux d'incidence grippaux de toutes les régions françaises. Enfin, les méthodes développées ont été appliquées à la prévision à 3 semaines des cas de gastro-entérite au niveau national, régional, et hospitalier.
Dans un premier temps, nous considérons le problème de la classification avec un grand nombre de classes. Afin de valider cette méthode, nous fournissons une analyse théorique et expérimentale détaillée. Dans la seconde partie, nous approchons le problème de l'apprentissage sur données distribuées en introduisant un cadre asynchrone pour le traitement des données. Nous appliquons ce cadre à deux applications phares : la factorisation de matrice pour les systèmes de recommandation en grande dimension et la classification binaire.
Notre recherche s'inscrit dans le cadre de la phraséologie berbère abordant le figement lexical, notamment l'étude des séquences figées. Elle s'inscrit également dans la continuité, d'une part, de nos deux mémoires de Master 1 « étude thématique des expressions idiomatiques berbères liées au corps humain, parler Ayt Ḥmad Uɛisa, Moyen Atlas » , et de Master 2 consacré à l'étude de la typologie sémantico-syntaxique des expressions idiomatiques relatives au corps humain et, d'autre part, d'autres travaux effectués dans l'aire berbère en général. Il vise à mettre l'accent sur trois différents aspects des expressions idiomatiques en menant une étude thématique et sémantico-syntaxique, et en se focalisant sur la typologie syntaxique et sémantique - étude des comportements syntaxiques et sémantiques de chaque syntagme dans l'expression, les procédés de construction du sens - ainsi que sur le rapport entre le sémantisme des constituants et leur figement syntaxique. L'analyse portera sur un corpus oral que nous comptons collecter à l'aide d'entretiens semi-directs réalisés avec les locuteurs natifs du parler Ayt Ḥmad Uɛisa. Nous analyserons ensuite les unités du corpus en nous référant au cadre théorique général de la phraséologie internationale ainsi que aux travaux effectués dans le domaine de la phraséologie berbère en particulier. L'importance de ce projet réside dans le fait qu'il fournira des éléments linguistiques expliquant le fonctionnement syntaxique et sémantique des expressions idiomatiques berbères, et aidant à leur compréhension, et qu'il contribuera sans doute à conserver ces formes langagières qui constituent une partie importante de la langue berbère par la collecte du corpus et la constitution d'une base de données pouvant être exploitées dans d'autres perspectives, telles que la lexicographie, la phraséodidactique, la traduction et le traitement automatique des langues.
L'objectif de ce travail est de coupler le processus de prétraitement des documents textuels, les techniques de machine learning et les méthodes de visualisation afin de décrire les parcours de soins de cohortes de patients et de mettre en évidence des clusters en fonction de certaines caractéristiques médicales, sociales ou d'organisation territoriale. La modélisation des séquences des états se fera par example par méthode de clustering, Les méthodes de détection des séquences anormales (parcours avec durées anormales, réactions et résultats innatendus sur le traitement, déviations obvservées lors du suivi, etc...). La modélisation des séquences des états sera effectuée par des réseaux de neurones Des méthodes d'apprentissage et de prédictions seront associées aux réseaux de neurones pour optimiser l'ensemble du système.
La présente thèse propose une analyse morphologique automatique des séquences de kanji dans des textes japonais, généraux ou spécialisés. Cette analyse s'appuie sur les particularités graphémiques, morphologiques et syntaxiques du japonais. La première partie décrit le système d'écriture japonais et son codage informatique. La deuxième partie décrit les parties du discours japonais, en particulier les verbes, qualificatifs, particules et suffixes flexionnels, leurs caractéristiques morphosyntaxiques étant essentielles pour l'analyse morphologique. La troisième partie décrit le module d'analyse : identification et formalisation des données pour l'analyse, algorithme de l'analyse et des pré-traitements, formalisation de modèles d'objets pour la manipulation informatique du japonais.
Cette thèse présente une modélisation des principaux aspects syntaxiques de la coordination dans les grammaires d'interaction de Guy Perrier. Les grammaires d'interaction permettent d'expliciter la valence des groupes conjoints. C'est précisément sur cette notion qu'est fondée notre modélisation. Nous présentons également tous les travaux autour de cette modélisation qui nous ont permis d'aboutir à une implantation réaliste : le développement du logiciel XMG et son utilisation pour l'écriture de grammaires lexicalisées, le filtrage lexical par intersection d'automates et l'analyse syntaxique.
Dans cette thèse, nous présentons une nouvelle méthode de cartographie visuelle hybride qui exploite des informations métriques, topologiques et sémantiques. Notre but est de réduire le coût calculatoire par rapport à des techniques de cartographie purement métriques. Comparé à de la cartographie topologique, nous voulons plus de précision ainsi que la possibilité d'utiliser la carte pour le guidage de robots. Cette méthode hybride de construction de carte comprend deux étapes. Ces cartes sont ensuite complétées avec des données métriques aux nœuds correspondant à des sous-séquences d'images acquises quand le robot revenait dans des zones préalablement visitées. La deuxième étape augmente ce modèle en ajoutant des informations sémantiques. Une classification est effectuée sur la base des informations métriques en utilisant des champs de Markov conditionnels (CRF) pour donner un label sémantique à la trajectoire locale du robot (la route dans notre cas) qui peut être "doit", "virage" ou "intersection". La fermeture de boucle n'est réalisée que dans les intersections ce qui accroît l'efficacité du calcul et la précision de la carte. En intégrant tous ces nouveaux algorithmes, cette méthode hybride est robuste et peut être étendue à des environnements de grande taille. Elle peut être utilisée pour la navigation d'un robot mobile ou d'un véhicule autonome en environnement urbain. Nous présentons des résultats expérimentaux obtenus sur des jeux de données publics acquis en milieu urbain pour démontrer l'efficacité de l'approche proposée.
Cette thèse, organisée en deux parties indépendantes, a pour objet la sémantique distributionnelle et la sélection de variables. Dans la première partie, nous introduisons une nouvelle méthode pour l'apprentissage de représentations de mots à partir de grandes quantités de texte brut. Cette méthode repose sur un modèle probabiliste de la phrase, utilisant modèle de Markov caché et arbre de dépendance. Nous évaluons les modèles obtenus sur des taches intrinsèques, telles que prédire des jugements de similarité humains ou catégoriser des mots et deux taches extrinsèques~ : la reconnaissance d'entités nommées et l'étiquetage en supersens. Dans la seconde partie, nous introduisons, dans le contexte des modèles linéaires, une nouvelle pénalité pour la sélection de variables en présence de prédicteurs fortement corrélés. Cette pénalité, appelée trace Lasso, utilise la norm trace des prédicteurs sélectionnés, qui est une relaxation convexe de leur rang, comme critère de complexité. En particulier, lorsque tous les prédicteurs sont orthogonaux, il est égal à la norme ℓ 1, tandis que lorsque tous les prédicteurs sont égaux, il est égal à la norme ℓ 2. Nous proposons deux algorithmes pour calculer la solution du problème de régression aux moindres carrés régularisé par le trace Lasso et réalisons des expériences sur des données synthétiques.
Les architectures d'ordinateur classiques sont optimisées pour le traitement déterministe d'informations pré-formatées et ont donc des difficultés avec des données naturelles bruitées (images, sons, etc.). Comme celles-ci deviennent nombreuses, de nouveaux circuits neuromorphiques (inspirés par le cerveau) tels que les réseaux de neurones émergent. Dans ce travail, nous étudions des memristors basés sur des jonctions tunnel ferroélectriques qui sont composées d'une couche ferroélectrique ultramince entre deux électrodes métalliques. Nous montrons que le renversement de la polarisation de BiFeO3 induit des changements de résistance de quatre ordres de grandeurs et établissons un lien direct entre les états de domaines mixtes et les niveaux de résistance intermédiaires. En alternant les matériaux des électrodes, nous révélons leur influence sur la barrière électrostatique et les propriétés dynamiques des memristors. La combinaison de leur analyse avec de l'imagerie par microscopie à force piézoélectrique nous permet d'établir un modèle dynamique du memristor. Suite à la démonstration de la spike-timing-dependent plasticity, une règle d'apprentissage importante, nous pouvons prédire le comportement de notre synapse artificielle. Ceci représente une avance majeure vers la réalisation de réseaux de neurones sur puce dotés d'un auto-apprentissage non-supervisé.
De nombreuses sources géohistoriques sont aujourd'hui mises à la disposition des chercheurs : plans anciens, bottins, etc. L'objectif de cette thèse est d'apporter une solution à ce verrou par la production de données anciennes de référence. En nous focalisant sur le réseau des rues de Paris entre la fin du XVIIIe et la fin du XIXe siècles, nous proposons plus précisément un modèle multi-représentations de données agrégées permettant, par confrontation d'observations homologues dans le temps, de créer de nouvelles connaissances sur les imperfections des données utilisées et de les corriger.
De manière générale, un réseau reflète les interactions entre les nombreuses entités d'un système. Ces interactions peuvent être de différentes natures, un lien social ou un lien d'amitié dans un réseau social constitué de personnes, un câble dans un réseau de routeurs, une réaction chimique dans un réseau biologique de protéines, un hyperlien dans un réseau de pages Web, etc. Plus encore, la rapide démocratisation du numérique dans nos sociétés, avec Internet notamment, a pour conséquence de produire de nouveaux systèmes qui peuvent être représentés sous forme de réseaux. Aujourd'hui, la science des réseaux est un domaine de recherche à part entière dont l'enjeu principal est de parvenir à décrire et modéliser ces réseaux avec précision afin de révéler leurs caractéristiques générales et de mieux comprendre leurs mécanismes. La plupart des travaux dans ce domaine utilisent le formalisme des graphes qui fournit un ensemble d'outils mathématiques particulièrement adaptés à l'analyse topologique et structurelle des réseaux. Il existe de nombreuses applications dans ce domaine, par exemple des applications concernant la propagation d'épidémie ou de virus informatique, la fragilité du réseau en cas de panne, sa résilience en cas d'attaque, l'étude de la dynamique pour prédire l'apparition de nouveaux liens, la recommandation, etc. La grande majorité des réseaux réels sont caractérisés par des niveaux d'organisation dans leur structure mésoscopique. Du fait de la faible densité globale des réseaux réels couplée à la forte densité locale, on observe la présence de groupes de nœuds fortement liés entre eux et plus faiblement liés avec le reste du réseau, que l'on appelle communautés.
Les dossiers patients électroniques contiennent des informations importantes pour la santé publique. La majeure partie de ces informations est contenue dans des documents rédigés en langue naturelle. Bien que le texte texte soit pertinent pour décrire des concepts médicaux complexes, il est difficile d'utiliser cette source de données pour l'aide à la décision, la recherche clinique ou l'analyse statistique. Parmi toutes les informations cliniques intéressantes présentes dans ces dossiers, la chronologie médicale du patient est l'une des plus importantes. Être capable d'extraire automatiquement cette chronologie permettrait d'acquérir une meilleure connaissance de certains phénomènes cliniques tels que la progression des maladies et les effets à long-terme des médicaments. Dans notre thèse, nous nous concentrons sur la création de ces chronologies médicales en abordant deux questions connexes en traitement automatique des langues : l'extraction d'informations temporelles et la résolution de la coréférence dans des documents cliniques. Concernant l'extraction d'informations temporelles, nous présentons une approche générique pour l'extraction de relations temporelles basée sur des traits catégoriels. Cette approche peut être appliquée sur des documents écrits en anglais ou en français. Puis, nous décrivons une approche neuronale pour l'extraction d'informations temporelles qui inclut des traits catégoriels. Nous décrivons une approche neuronale pour la résolution de la coréférence dans les documents cliniques. Nous menons une étude empirique visant à mesurer l'effet de différents composants neuronaux, tels que les mécanismes d'attention ou les représentations au niveau des caractères, sur la performance de notre approche.
Il est important pour les robots de pouvoir reconnaître les objets rencontrés dans la vie quotidienne afin d'assurer leur autonomie. De nos jours, les robots sont équipés de capteurs sophistiqués permettant d'imiter le sens humain du toucher. Dans cette thèse, notre but est d'exploiter les données haptiques issues de l'interaction robot-objet afin de reconnaître les objets de la vie quotidienne, et cela en utilisant les algorithmes d'apprentissage automatique. Le problème qui se pose est la difficulté de collecter suffisamment de données haptiques afin d'entraîner les algorithmes d'apprentissage supervisé sur tous les objets que le robot doit reconnaître. En effet, les objets de la vie quotidienne sont nombreux et l'interaction physique entre le robot et chaque objet pour la collection des données prend beaucoup de temps et d'efforts. Pour traiter ce problème, nous développons un système de reconnaissance haptique permettant de reconnaître des objets à partir d'aucune, de une seule, ou de plusieurs données d'entraînement. Enfin, nous intégrons la vision afin d'améliorer la reconnaissance d'objets lorsque le robot est équipé de caméras.
Cette étude a pour objectif la description syntaxique et sémantique des constructions transitives non locatives à un complément d'objet direct en grec moderne : N0 V N1. Nous nous sommes appuyée sur le cadre théorique de la grammaire transformationnelle de Zellig À partir de 16 560 entrées verbales morphologiques, nous procédons à la classification des constructions transitives non locatives, à partir de 24 classes distinctes, sur la base de critères formels posés. Un inventaire de 2 934 emplois verbaux à construction transitive non locative à un complément d'objet direct a été ainsi produit et scindé en neuf classes. En outre, la transformation passive est largement interdite pour les emplois verbaux recensés dans la table 32GNM, alors que les tables 32GCV et 32GRA regroupent des verbes acceptant une transformation à verbe support.
Afin d'espérer développer une philosophie politique qui reconnaisse d'emblée notre interdépendance, nous travaillons dans une première partie à établir des hypothèses sur ce qu'on entend par réalité et sur notre accès à cette dernière. Une ontologie événementielle paraît compatible avec l'ontogénèse narrative qui nous constitue individuellement en constituant un « nous » . Il faut pour cela imaginer chacun s'imaginant le monde et apprenant au travers d'histoires, dans une logique inductive qui peut réconcilier la phénoménologie herméneutique d'une part et l'apprentissage statistique de l'autre. De ces histoires chacun tire des universaux, interprétables comme des composantes principales d'une analyse statistique factorielle de ces histoires qui nous constituent. Le temps joue un rôle clef dans la dynamique de cette constitution autant que dans celle des événements rassemblés dans ces histoires. L'enjeu est finalement de partager ces universaux dans une histoire commune, ou, à l'inverse, dans une rupture temporelle qui permet peut-être de mieux accéder à un monde commun. Nous travaillons alors dans une seconde partie la question du vivre ensemble avec les idées républicaines de liberté, d'égalité et de fraternité, et avec celles de pluralité et de confins. L'écologie politique que l'on aperçoit alors est aussi républicaine que libertaire. Dans ce cadre, la justice s'exprime par la justesse, la fidélité, la sensibilité et par une juste démesure. L'impératif catégorique s'y décline dans la nécessité de rendre les autres beaux, libres, et puissants et d'apprendre ensemble. Le Droit apparaît comme s'élaborant dynamiquement dans le temps même où s'élabore la Cité. La possibilité du radicalement nouveau travaillée dans la première partie autorise d'articuler la liberté et les institutions. La logique d'un code d'honneur permet in fine de ne pas s'abandonner à la Raison toute puissante sans pour autant renoncer aux Lumières.
Dans un contexte collaboratif, le tutorat et les outils « d'awareness » constituent des solutions admises pour faire face à l'isolement qui très souvent, mène à l'abandon de l'apprenant. Ainsi, du fait des difficultés rencontrées par le tuteur pour assurer un encadrement et un suivi appropriés à partir des traces de communication (en quantités conséquentes) laissées par les apprenants, nous proposons une approche multi-agents pour analyser les conversations textuelles asynchrones entre apprenants. Ces indicateurs seront déduits à partir des grands volumes d'échanges textuels entre apprenants.
Bluecime a mis au point un système de vidéosurveillance à l'embarquement de télésièges qui a pour but d'améliorer la sécurité des passagers. Ce système est déjà performant, mais il n'utilise pas de techniques d'apprentissage automatique et nécessite une phase de configuration chronophage. L'apprentissage automatique est un sous-domaine de l'intelligence artificielle qui traite de l'étude et de la conception d'algorithmes pouvant apprendre et acquérir des connaissances à partir d'exemples pour une tâche donnée. Une telle tâche pourrait consister à classer les situations sûres ou dangereuses dans les télésièges à partir d'exemples d'images déjà étiquetées dans ces deux catégories, appelés exemples d'entraînement. L'algorithme d'apprentissage automatique apprend un modèle capable de prédire la catégories de nouveaux cas. Depuis 2012, il a été démontré que les modèles d'apprentissage profond sont les modèles d'apprentissage machine les mieux adaptés pour traiter les problèmes de classification d'images lorsque de nombreuses données d'entraînement sont disponibles. Dans ce contexte, cette thèse, financée par Bluecime, vise à améliorer à la fois le coût et l'efficacité du système actuel de Bluecime grâce à l'apprentissage profond.
Entre 2 et 5% des enfants de 6 mois à 5 ans présentent au moins un épisode de Crise d'Épilepsie en contexte Fébrile (CEF). Bien que généralement bénignes, ces crises sont associées à un risque d'urgences neurologiques graves et curables dont l'élimination requiert la réalisation d'examens complémentaires douloureux et/ou irradiants. Actuellement, ce risque est évalué en fonction de trois facteurs : l'âge de l'enfant, le caractère simple ou complexe de la crise, et l'examen clinique. Cette thèse avait pour objectif de tester l'hypothèse que parmi les enfants consultant pour une CEF, seuls ceux avec un examen clinique anormal présentent un risque d'urgence neurologique grave et urgent. Pour ce faire, nous avons créé un outil informatique permettant une recherche exhaustive de cas parmi un million de dossiers médicaux informatisés dans sept services d'urgences pédiatriques entre 2007 et 2011. Nous avons alors identifié : les visites d'enfants présentant une CEF. Nous avons ensuite évalué le risque d'urgence neurologique grave et curable associé à ces visites, notamment lorsque l'examen clinique au décours était normal. Nous n'avons retrouvé aucune urgence neurologique grave et curable parmi les enfants consultant pour une CEF avec un examen clinique normal au décours, quels que soient l'âge et les caractéristiques de la crise. Ce travail de thèse associé aux données de la littérature confirme notre hypothèse et souligne la nécessité de recommandations quant à la prise en charge de ces enfants. Enfin, cette thèse constitue l'occasion de mener une réflexion méthodologique quant à l'utilisation de dossiers médicaux informatisés pour la recherche clinique.
Le concept de “Business Rule Management System ” (BRMS) a été introduit pour faciliter la création, la vérification, le déploiement et l'exécution des politiques commerciales propres à chaque compagnie. Basée sur une approche symbolique, l'idée générale est de permettre aux utilisateurs métier de gérer les changements des règles métier dans le système sans avoir besoin de recourir à des compétences techniques. Il s'agit donc de fournir à ces derniers la possibilité de formuler des politiques commerciales et d'automatiser leur traitement tout en restant proche du langage naturel. De nos jours, avec l'expansion de ce type de systèmes, il faut faire face à des logiques de décision de plus en plus complexes et à de larges volumes de données. Il n'est pas toujours facile d'identifier les causes conduisant à une décision. On constate ainsi un besoin grandissant de justifier et d'optimiser les décisions dans de courts délais qui induit l'intégration à ses systèmes d'une composante d'explication évoluée. Le principal enjeu de ces recherches est de fournir une approche industrialisable de l'explication des processus de décision d'un BRMS et plus largement d'un système à base de règles. Cette approche devra être en mesure d'apporter les informations nécessaires à la compréhension générale de la décision, de faire office de justification auprès d'entités internes et externes ainsi que de permettre l'amélioration des moteurs de règles existants. La réflexion se portera tant sur la génération des explications en elles-mêmes que sur la manière et la forme sous lesquelles elles seront délivrées.
Dans cette thèse, nous abordons le problème de prédiction de liens dans des tenseurs binaires d'ordre trois et quatre contenant des observations positives uniquement. Ce type de tenseur apparaît dans les problèmes de recommandations sur le web, en bio-informatique pour compléter des bases d'interactions entre protéines, ou plus généralement pour la complétion bases de connaissances. Ces dernières nous permettent d'évaluer nos méthodes de complétion à grande échelle et sur des types de graphes relationnels variés. Notre approche est parallèle à celle de la complétion de matrice. Nous résolvons de manière non-convexe un problème de minimisation empirique régularisé sur des tenseurs de faible rangs. Dans un premier temps, nous validons empiriquement notre approche en obtenant des performances supérieures à l'état de l'art sur de nombreux jeux de données. Ces performances ne peuvent être atteintes que pour des rangs trop élevés pour que cette méthode soit applicable à l'échelle de bases de connaissances complètes. Nous nous intéressons dans un second temps à la décomposition Tucker, plus expressive que la décomposition Canonique, mais plus difficile à optimiser. Ces méthodes nous permettent d'améliorer les performances en complétion pour une quantité faible de paramètres par entités. Finalement, nous étudions le cas de base de connaissances temporelles, dans lesquels les prédicats ne sont valides que sur certains intervalles de temps. Nous proposons une formulation faible rang et une régularisation adaptée à la structure du problème, qui nous permet d'obtenir des performances supérieures à l'état de l'art.
En effet,afin de rester compétitives, les entreprises concevant des systèmes cherchent à leur rajouter de plus en plusde fonctionnalités. Cette compétitivité introduit aussi une demande de réactivité lors de la conception desystèmes, pour que le système puisse évoluer lors de sa conception et suivre les demandes du marché. De plus, la langue naturelle est difficile à traiter automatiquement, par exemple, onpeut difficilement déterminer avec un programme informatique que deux exigences en langue naturelle secontredisent. Cependant, la langue naturelle reste indispensable dans les spécifications que nous étudions,car elle reste un moyen de communication pratique et très répandu. Nous cherchons à compléter ces exigences en langue naturelle avec des éléments permettant à la fois de lesrendre moins ambiguës et de faciliter les traitements automatiques. Ces éléments peuvent faire partie demodèles (d'architecture par exemple) et permettent de définir le lexique et la syntaxe utilisés dans lesexigences. Nous avons testé les principes proposés sur des spécifications industrielles réelles et développéun prototype logiciel permettant de réaliser des tests sur une spécification dotée de ces éléments de syntaxeet de lexique.
L'objectif de ce projet est de développer des outils pour analyser et traiter automatiquement la parole enregistrée de patients atteints de la maladie de Huntington, maladie génétique neurodégénérative grave, pour en tirer un marqueur permettant leur suivi clinique. Ce projet portera sur la modélisation conjointe des déficits cognitifs et émotionnels, en utilisant une variété de représentations des signaux vocaux. Nous allons examiner trois niveaux de représentations sur les différents protocoles : Intelligibilité et mesures articulatoires / Représentations acoustiques et prosodique du signal vocal / Représentations linguistiques. En particulier, nous utiliserons des techniques d'apprentissage automatique non supervisées pour détecter les motifs répétés dans le signal, mais aussi quantifier les trajectoires individuelles évolutives des patients dans les différentes dimensions de la maladie. La diversité des aptitudes testées dans la parole des patients nous permettra de rendre compte de l'hétérogénéité de leurs symptômes observée en clinique (troubles cognitifs, comportementaux, émotionnels et moteurs). Ce projet pourra offrir aux cliniciens et aux chercheurs des marqueurs composites reflétant fidèlement l'évolution de la maladie propre à chaque patient.
Ce travail de recherche relève du domaine de la didactique de l'espagnol et porte sur les représentations sociales des étudiants du bassin du Département de Saint-Denis (lle de France) et leurs attitudes envers cette langue. Les effets du dispositif sur les acquisitions langagières ont été mesurés d'une part par le biais d'un pré-test et d'un post-test sommatif pour évaluer les compétences linguistiques en CO, CE et EE courte inspirées des critères du Cecrl, et d'autre part sur une analyse d'un échantillon des productions des étudiants. Les utilisations et les modes d'appropriation de la plate-forme par les étudiants sont mesurés par un traçage sur la plateforme, par les commentaires effectués lors des validations semestrielles et par un questionnaire d'attitude. Nos résultats ont mis en évidence des acquisitions langagières et un changement d'attitude pour deux tiers des étudiants et une progression en fluidité et en précision des énoncés, selon les niveaux de départ et visant le niveau B1-B2 de la passation Gies 1 et 2. Si la majorité des étudiants interrogés évaluent positivement le dispositif et le travail en groupe auxquels ils ont été confrontés, un tiers cependant dit avoir eu des difficultés à assumer la charge de travail imposée et a préféré le travail en situation d'autonomie. L'investissement coordonné des différents acteurs qu'exige ce mode d'apprentissage est une des solutions de partage réclamée par les enseignants de L2 dans l'accompagnement hybride faute de quoi les initiatives dans le domaine de l'accompagnement numérique en enseignement/apprentissage des langues risquent fort de demeurer individuelles et isolées.
La construction montre un retard de productivité par rapport à l'industrie. Pour y remédier, le monde de la construction met en place le processus BIM (Building Information Modeling). Celui-ci repose sur l'utilisation d'une maquette numérique 3D, reproduction du bâtiment, contenant toutes les informations nécessaires à sa réalisation. Cependant, les ingénieurs ont remonté des difficultés dans l'utilisation du BIM. La maquette contient énormément d'éléments provenant de plusieurs intervenants. Le processus BIM propose la même méthode de travail aux différents types de métiers de la construction. Le but est d'offrir un environnement adapté à tout type de métier du bâtiment tout en respectant des procédés de conception actuelle. Ce travail propose une étude sur la visualisation des connaissances métiers ainsi qu'une interaction avec des dispositifs immersifs pour la construction. Dans un premier temps, ce manuscrit propose d'étudier la classification des divers éléments d'une maquette BIM en utilisant l'apprentissage automatique, assister la visualisation des données en utilisant les modèles de saillance et une interface reposant sur la collaboration. Dans un second temps, deux salles de réalité virtuelle dédiées à la construction seront décrites.
Le travail effectué dans cette thèse présente une évaluation des techniques de transformation de voix à base de GMM. Ces techniques de transformation linéaires malgré leurs qualités obtenues, elles ne manquent pas de quelques défauts, on peut noter le sur-lissage, le problème de distorsion spectrale et le sur-apprentissage. Dans un premier volet, nous avons pris en compte ces questions pour adapter la stratégie d'apprentissage des fonctions de conversion. La première c'est la réduction du nombre des paramètres libres de la fonction de conversion. La deuxième considère que les solutions par transformation linéaire sont instables face au peu de données d'apprentissage, d'où le recours aux modèles de transformation non-linéaire de type RBF. Dans un deuxième volet, pour aligner les données non-parallèles des locuteurs source et cible, une solution consiste à correspondre ces données via une représentation récursive d'un arbre binaire.
Les dispositifs matériels mobiles proposent des capacités de mesure à l'aide de capteurs soit embarqués, soit connectés. Ils présentent un caractère critique dans le sens où ces informations doivent être fiables, car potentiellement utilisées dans un contexte exigeant. Malgré une grande demande, peu d'applications proposent d'assister les utilisateurs lors de relevés exploitant ces capacités. Idéalement, ces applications devraient proposer des méthodes de visualisation, de calcul, des procédures de mesure et des fonctions de communications permettant la prise en charge de capteurs connectés ou encore la génération de rapports. La rareté de ces applications se justifie par les connaissances nécessaires pour permettre la définition de procédures de mesure correctes. Ces éléments sont apportés par la métrologie et la théorie de la mesure et sont rarement présents dans les équipes de développement logiciel. Ce postulat apporte la question de recherche à laquelle les travaux présentés répondent : Comment proposer une approche pour la conception d'applications adaptées à des procédures de mesures spécifiques. Les procédures de mesure pouvant être configurées par un utilisateur final La réponse développée est une "plateforme" de conception d'applications d'assistance à la mesure. Elle permet d'assurer la conformité des procédures de mesures sans l'intervention d'expert de la métrologie. Cette expertise comprend des termes et des règles assurant l'intégrité et la cohérence d'une procédure de mesure. Un modèle conceptuel du domaine de la métrologie est proposé. Ce modèle conceptuel est ensuite intégré au processus de développement d'une application. Il permet, la vérification du respect des contraintes inhérentes à la métrologie dans une procédure de mesure. Cette vérification est réalisée en confrontant les procédures de mesures au schéma sous forme de requêtes. Ces requêtes sont décrites à l'aide d'un langage proposé par le schéma. Pour cela, un éditeur d'application est proposé. Ce langage est construit à partir des concepts, formalismes et outils proposés par l'environnement de métamodélisation Diagrammatic Predicate Framework (DPF).
L'influence de l'anglais est évidente sur les langues du monde entier. L'anglais est considéré comme une langue de communication mondiale et est utilisé par un grand nombre de locuteurs du monde entier dans leurs interactions. Il est clair que l'anglais domine dans de nombreux aspects de la vie quotidienne tels que la technologie, la science, les médias et Internet. Toutes les influences observées sur les langues du monde qui sont dues à l'influence de l'anglais relèvent de la notion d'anglicisation, qui couvre tous les niveaux de l'analyse linguistique. Dans ma thèse, j'étudie l'influence de l'anglais sur le grec moderne, qui a été particulièrement forte au cours des deux à trois dernières décennies. J'examine le phénomène de l'anglicisation du grec moderne en tenant compte de l'influence de l'anglais à tous les niveaux de l'analyse linguistique, et particulièrement au niveau lexical, phraséologique et morphosyntaxique. Pour la collecte et l'analyse de mes données, j'utilise des dictionnaires et des grammaires du grec moderne, ainsi que des corpus de textes comme le Trésor National de la Langue Grecque (Hellenic National Corpus), le Corpus des Textes Grecs (Corpus of Greek Texts) et les corpus de textes disponibles via la plateforme Sketch Engine, ainsi qu'un corpus de textes personnalisé que j'ai construit exclusivement pour mon étude via Sketch Engine. De plus, j'étudie les facteurs responsables de l'utilisation des formes non translittérées des emprunts anglais en examinant leur apparition dans des vocabulaires spécialisés du grec moderne tels que le vocabulaire du sport et de la technologie. En ce qui concerne les unités phraséologiques et les structures morphosyntaxiques calquées, je compare la fréquence d'apparition de la structure calquée à la fréquence d'apparition de la structure équivalente en grec moderne. De plus, j'essaie de déterminer la chronologie de l'insertion des emprunts anglais en grec moderne, et, enfin, je tire quelques conclusions générales concernant l'anglicisation du grec moderne à partir des résultats de ma recherche.
Le déploiement massif de capteurs couplé à leur exploitation dans de nombreux secteurs génère une masse considérable de données multivariées qui se sont révélées clés pour la recherche scientifique, les activités des entreprises et la définition de politiques publiques. Plus spécifiquement, les données multivariées qui intègrent une évolution temporelle, c'est-à-dire des séries temporelles, ont reçu une attention toute particulière ces dernières années, notamment grâce à des applications critiques de monitoring (e.g. Cependant, les explications du modèle de remplacement ne peuvent pas être parfaitement exactes au regard du modèle original, ce qui constitue un prérequis pour de nombreuses applications. L'exactitude est cruciale car elle correspond au niveau de confiance que l'utilisateur peut porter aux explications relatives aux prédictions du modèle, c'est-à-dire à quel point les explications reflètent ce que le modèle calcule. Cette thèse propose de nouvelles approches pour améliorer la performance et l'explicabilité des méthodes d'apprentissage automatique de séries temporelles multivariées, et établit de nouvelles connaissances concernant deux applications réelles.
Aujourd'hui, l'apprentissage des langues assisté par ordinateur est de plus en plus répandu, dans les institutions publiques et privées. Cependant, il est encore loin des attentes des enseignants et des apprenants et ne répond pas encore à leurs besoins. Les systèmes d'apprentissage des langues assisté par ordinateur (ALAO) actuels sont plutôt des environnements de tests des connaissances de l'apprenant et ressemblent plus à un support d'apprentissage traditionnel. De plus, le feedback proposé par ces systèmes reste basique et ne peut pas être adapté pour un apprentissage autonome, car, il devrait être en mesure de diagnostiquer les problèmes d'un apprenant avec l'orthographe, la grammaire, la conjugaison,etc., puis générer intelligemment un feedback adéquat selon la situation de l'apprentissage. Cette recherche expose les capacités des outils TAL à apporter des solutions aux limitations des systèmes d'ALAO dans le but d'élaborer un système d'ALAO complet et autonome. Nous présentons une architecture complète d'un système multilingue pour l'apprentissage des langues assisté par ordinateur destiné aux apprenants des langues étrangères, français et arabe. Ce système pourrait être utilisé pour l'apprentissage des langues par les apprenants de la langue en tant que langue seconde ou étrangère. La première partie de nos travaux porte sur l'adaptation des outils et des ressources issues du TAL pour qu'ils soient utilisés dans un environnement d'apprentissage des langues assisté par ordinateur. Parmi ces outils et ressources, il y a les analyseurs morphologiques pour l'arabe et le français, corpus, dictionnaires électroniques, etc. Ensuite, dans la deuxième section, nous présentons la reconnaissance de l'écriture manuscrite en ligne. Dans cette optique, nous exposons une approche statistique basée sur le réseau de neurones, puis, nous présentons la conception de l'architecture du système de reconnaissance ainsi que l'implémentation de l'algorithme de la reconnaissance. La deuxième partie de notre exposé porte sur l'élaboration, l'intégration et l'exploitation des outils TAL utilisés (analyseurs morphologiques, système de reconnaissance de l'écriture, dictionnaires, etc.) dans notre système d'apprentissage des langues assisté par ordinateur. Nous y présentons aussi les modules ajoutés à la plate-forme pour avoir une architecture complète d'un système d'ALAO. Parmi ces modules, figure le générateur de feedback qui permet de corriger les fautes des apprenants et générer un feedback pédagogique pertinent qui permet à l'apprenant de cerner et ses fautes. Enfin, nous décrivons l'outil de génération automatique des activités pédagogiques variées et automatisées.
Le deuxième chapitre (en collaboration avec Xavier Lambin) étudie l'impact de la réputation digitale sur la discrimination ethnique. Le troisième chapitre (rédigé en collaboration avec Rossi Abi Rafeh) développe et estime un modèle de dynamique industrielle. Avec plusieurs entrants en cours, le titulaire du brevet exploite ces externalités en proposant des accords de licence à certains entrants ou en poursuivant des contentieux afin de diminuer le coût des contrats dilatoires offerts aux autres. Le nombre des entrants tardifs augmente en fonction de la solidité du brevet. Le deuxième chapitre montre que les systèmes de réputation peuvent atténuer la discrimination ethnique en permettant les vendeurs appartenant à des minorités ethniques de construire une bonne réputation rapidement, ce qui entraîne les acheteurs d'actualiser leurs convictions. En utilisant une base de données collectée sur une plateforme de covoiturage, nous trouvons qu'en absence d'avis, les conducteurs membres des minorités ethniques gagnent 12% moins de revenue par rapport aux conducteurs non membres des minorités. Pour comprendre le mécanisme derrière ce processus, nous concevons un modèle de 'career concerns' des vendeurs discriminés en présence d'un système de réputation. Les estimations du modèle montrent que les conducteurs appartenant à des minorités ethniques, qui viennent d'entrer dans la plateforme, font face à des convictions trop pessimistes quant à la qualité de leur service. Pour changer ces convictions, ils exercent de grands efforts et proposent des bas prix de lancement, pour renforcer rapidement leur réputation. Des simulations contrefactuelles révèlent que le coût des croyances antérieures erronées est élevé et que le système de réputation bénéficie strictement aux conducteurs des minorités ethniques. Le dernier chapitre étudie l'entrée sur un marché avec un système de réputation et les décisions de formation de prix des vendeurs. Nous proposons un modèle d'oligopole dynamique avec une hétérogénéité des coûts marginaux et des coûts d'opportunité, et avec la réputation individuelle comme une variable d'état. Nous montrons que les nouveaux vendeurs sont généralement moins susceptibles de rejoindre la plateforme par rapport aux anciens, et les vendeurs avec une faible chance de rejoindre dans les périodes suivantes mettent des prix moyens plus élevés. Le mécanisme derrière ces résultats est la sélection selon les coûts marginaux. Notre modèle s'appuie sur une base de données de vendeurs sur une grande plateforme d'un marché de covoiturage. Nous constatons une corrélation négative des utilisateurs expérimentés de la plateforme, mesurée par le nombre d'avis et les prix déterminés par les conducteurs. Cependant, après avoir pris en compte les caractéristiques non observées des conducteurs, dont lesquelles on interprète comme coûts marginaux, nous trouvons une relation positive. Par ailleurs, en étudiant les décisions d'une nouvelle entrée sur la plateforme, nous démontrons la sélection des non-observables. Finalement, nous découvrons la distribution des coûts d'opportunité.
L'objectif général de ce projet doctoral est de proposer des nouvelles approches automatiques non- ou faiblement supervisées visant à caractériser une conversation orale à partir de son enregistrement audio et sa transcription (automatique ou manuelle quand elle est disponible). Dans un premier temps, on s'intéressera à l'identification nommée des différents locuteurs. Sans aucune connaissance a priori sur la qualité et le nombre des participants à la conversation, ce problème peut se découper en deux sous-problèmes : la constitution automatique de la liste des participants à la conversation (grâce à des outils de détection d'entités nommées et d'entity linking, par exemple) suivi de l'attribution automatique de leurs tours de parole respectives (en combinant compréhension de dialogue et reconnaissance du locuteur, par exemple). Dans un second temps, on s'interessera à la caractérisation des conversations selon une typologie permettant d'identifier la nature des échanges (argumentation, débat, altercation, etc.). Cette caractérisation pourra se faire en utilisant des techniques de traitement automatique des langues enrichies d'informations acoustiques (informations prosodiques par exemple) afin d'améliorer les performances. Ces nouvelles approches seront appliquées sur un corpus composé de films (Anna et ses soeurs, les films Harry Potter) et de séries télévisées de différente nature (Lost, Friends, The Big Bang Theory, Game of Thrones, etc.).
L'objectif de cette thèse est de développer un outil qui fédère l'ensemble des recherches lancées à l'IDHN dans le cadre de l'analyse du discours politique numérique par le biais de plateformes, afin de les agréger dans une même interface, et de les faire fonctionner ensemble, dans l'optique d'une analyse globale du phénomène de l'influence. Notre choix se porte sur viky.ai qui est une plateforme open source développée par la société Pertimm, pour créer et partager des agents linguistiques qui analysent des textes dans toutes les langues. La création d'agents d'analyse modifiables et réutilisables, qui peuvent être partagés en collaboration par la communauté, peut contribuer à simplifier considérablement la gestion de l'analyse textuelle sur des problèmes que l'on retrouve de façon similaire dans de nombreux domaines. viky.ai offre une interface utilisateur unique et sans code. C'est une nouvelle façon de créer des modules sémantiques qui sont faciles à intégrer dans n'importe quel système par le biais d'une API. Les briques sémantiques, appelées « agents » de viky.ai, sont des assistants multilingues pour trouver des données pertinentes dans les textes. Ces briques sont constituées d'entités et d'interprétations et peuvent s'assembler à l'infini pour construire des modèles dans tous les domaines de l'analyse sémantique. Beaucoup d'experts du traitement du langage s'accordent à penser que les technologies de TAL alliant à la fois des règles et de l'apprentissage, seront la combinaison gagnante. Plus concrètement, la thèse visera à développer des agents dans deux domaines de recherche déjà esquissés : détection et caractérisation des idéologies, et liens avec le discours émotionnel ; détection des changements de thèmes dans les discours politiques.
Cette thèse en informatique se situe dans le domaine des Environnements Informatiques pour l'Apprentissage Humain (EIAH), et plus particulièrement au sein du projet AGATE (an Approach for Genericity in Assistance To complEx tasks) qui vise à proposer des modèles génériques et des outils unifiés pour permettre la mise en place de systèmes d'assistance dans des applications existantes. Ce système d'assistance peut ensuite être exécuté par le moteur d'assistance de SEPIA pour fournir de l'assistance aux utilisateurs finaux sur les applications Des ingénieurs pédagogiques endossent donc le rôle de concepteurs d'assistance, alors que les apprenants sont les utilisateurs finaux des applications assistées. Nous avons abordé cette problématique de recherche en deux étapes : tout d'abord l'étude d'assistances existantes au sein d'applications utilisées en contexte éducatif, puis l'exploitation et l'enrichissement des modèles et outils du projet AGATE pour les adapter au contexte éducatif. Dans un premier temps, nous avons étudié des applications variées utilisées par des enseignants au sein de leurs cours, ainsi que des travaux existants qui proposent des systèmes d'assistance. Dans un second temps, nous avons confronté les modèles et outils proposés précédemment dans le projet AGATE aux caractéristiques de l'assistance ainsi identifiées dans le contexte éducatif. Les limites des modèles et outils précédents nous ont amené à proposer deux contributions au langage aLDEAS et au système SEPIA pour les adapter au contexte éducatif. Que ce soit dans un contexte éducatif ou non, il est important de pouvoir définir facilement et de manière explicite plusieurs modes d'articulation entre les différents éléments d'un système d'assistance. Nous avons donc proposé un modèle d'articulation entre les règles aLDEAS explicitant le déroulement d'une assistance et permettant de définir des systèmes d'assistance comprenant des éléments qui se déroulent de manière successive, interactive, simultanée, progressive, indépendante. Ce modèle et ce processus ont été implémentés dans SEPIA Elle concernait la complexité à définir un guidage pédagogique proposant un parcours entre différentes activités au sein d'une application existante. Nous avons tout d'abord proposé un modèle d'activité permettant de délimiter les activités au sein des applications...
C'est presque une banalité que de dire qu'une des caractéristiques principales de la parole est sa variabilité : variabilité inter-sexe, inter-locuteur, mais aussi variabilité d'un contexte à un autre ou d'une répétition à une autre pour un même sujet. C'est cette variabilité qui fait à la fois la beauté de la parole mais aussi la complexité de son traitement par les technologies vocales, et la difficulté pour en comprendre les mécanismes. Dans cette thèse nous étudions certains aspects de cette variabilité, avec comme point de départ la variabilité observée chez un locuteur dans la répétition d'un même son dans les mêmes conditions, que nous appelons variabilité intrinsèque. Les modèles de contrôle moteur de la parole abordent principalement la variabilité contextuelle de la parole mais prennent rarement en compte sa variabilité intrinsèque, alors même que l'on sait que c'est cette variabilité qui donne à la parole tout son caractère naturel. L'objectif principal de cette thèse est d'aborder la variabilité intrinsèque et contextuelle de la production de la parole dans un cadre formel intégrateur. Pour cela nous faisons l'hypothèse que la variabilité intrinsèque n'est pas que le résultat d'un bruit d'exécution, mais qu'elle résulte aussi d'une stratégie de contrôle où la variabilité inter-répétition fait partie intégrante de la représentation de la tâche. Méthodologie :  Nous formalisons cette idée dans un cadre computationnel probabiliste, la modélisation Bayésienne, où l'abondance de réalisations possibles d'un même item de parole est représentée naturellement sous la forme d'incertitudes, et où la variabilité est donc manipulée formellement. Nous illustrons la pertinence de cette approche à travers trois contributions. Résultats :  Dans un premier temps, nous reformulons un modèle existant de contrôle optimal de la parole, le modèle GEPPETO, dans le formalisme probabiliste et démontrons que le modèle Bayésien contient GEPPETO comme un cas particulier. Cette étape a nécessité l'élaboration d'hypothèses sur l'intégration des retours sensoriels dans la planification, dont nous avons cherché à évaluer la pertinence en concevant une expérience originale de production Cela est rendu possible grâce à la représentation unifiée des connaissances dans le modèle, qui permet d'intégrer la production et la perception dans un cadre formel unique. L'ensemble de ces travaux illustre la capacité du formalisme Bayésien à proposer une démarche systématique et structurée pour la construction des modèles. Cette démarche facilite le développement des modèles et leur complexification progressive en précisant et explicitant les hypothèses formulées.
Le projet de thèse s'inscrit globalement dans la thématique de l'outillage de la linguistique de terrain. Dans un contexte où la diversité linguistique est menacée par la disparition programmée de près de la moitié des langues qui sont aujourd'hui parlées sur la terre, il devient crucial de doter les spécialistes de linguistique de terrain d'outils automatiques ou semi-automatiques pour recueillir des données linguistiques, les annoter, les enrichir et les archiver, et tenter par là de préserver une partie du patrimoine culturel de l'humanité. Ces problématiques rencontrent un intérêt croissant au sein de la communauté du traitement automatique des langues, dans le cadre de collaboration soutenues avec des équipes de linguistes. Cette thèse se déroulera ainsi dans le cadre d'une collaboration internationale impliquant des équipes de linguistes en France et en Allemagne, avec le soutien de l'Agence Nationale de la Recherche. Du point de vue méthodologique, la première tâche peut être abordée avec des outils de la modélisation statistique, tels que les modèles bayésiens non-paramétriques, dont on étudiera ici l'applicabilité dans un contexte où les données à annoter sont de petite taille, mais où il existe des ressources complémentaires potentiellement mobilisables (lexiques, éléments de description morphologique, etc). Le développement de ces divers types de modèles statistiques sera validée sur les langues du projet au travers d'expérimentations impliquant une collaboration étroite avec des utilisateurs de ces outils.
La compétitivité toujours plus importante et la mondialisation ont mis l'industrie manufacturière au défi de rationaliser les différentes façons de mettre sur le marché de nouveaux produits dans un délai court, avec des prix compétitifs tout en assurant des niveaux de qualité élevés. Le PDP moderne exige simultanément la collaboration de plusieurs groupes de travail qui assurent la création et l'échange d'information avec des points de vue multiples dans et à travers les frontières institutionnelles. Dans ce contexte, des problèmes d'interopérabilité sémantique ont été identifiés en raison de l'hétérogénéité des informations liées à des points de vue différents et leurs relations pour le développement de produits. Le travail présenté dans ce mémoire propose un cadre conceptuel d'interopération pour la conception et la fabrication de produits. Un système expérimental a été proposé à l'aide de l'outil Protégé pour modéliser des ontologies de base et d'une plateforme Java intégrée à Jena pour développer l'interface avec l'utilisateur. Le concept et la mise en œuvre de cette recherche ont été testés par des expériences en utilisant des produits tournants en plastiques. Les résultats ont montré que l'information et ses relations rigoureusement définies peuvent assurer l'efficacité de la conception et la fabrication du produit dans un processus de développement de produits moderne et collaboratif
Cette thèse comprend trois expériences et deux prétests (N = 1135) dans lesquelles sont étudiés trois aspects fondamentaux du design statique des messages sur internet : son format (infographie, audio ou texte), sa couleur et sa typographie, sur la thématique du recyclage des déchets électroniques (études 1 et 2) puis à propos de la migration humaine (étude 3).L'étude des aspects graphiques est pertinente si l'on veut augmenter la force persuasive d'un message. Le format joue un rôle prépondérant (étude 1a), permettant à la fois de changer les attitudes, mais aussi d'ancrer ce changement dans le temps. Les couleurs, par contre, ne semblent pas faire varier la force persuasive du message ou amener les lecteurs à agir en faveur du recyclage (étude 1b). La typographie ne semble pas non plus jouer de rôle dans la dynamique persuasive, qu'elle soit jugée lisible ou peu lisible (étude 2). Des pistes théoriques concernant la personnalité des typographies et leur cohérence avec le contexte sont développées. L'analyse des composantes de l'ELM a révélé, dans chaque étude, le fort lien entre l'attitude des individus et leur sentiment de responsabilité personnelle envers la thématique abordée ainsi que leurs connaissances a priori. Nous avons également vu que les leviers de persuasion ne sont pas systématiquement les mêmes selon le besoin de cognition. Globalement, nous suggérons que les messages persuasifs doivent adopter un format permettant une analyse centrale à faible coût cognitif, utilisant une couleur principale et une typographie toutes deux lisibles et cohérentes avec la thématique développée, avec un argumentaire qui renforce le sentiment de responsabilité des lecteurs.
Le développement d'un environnement collaboratif est un processus complexe. La complexité réside dans le fait que ce développement implique beaucoup de prise de décisions. De multiples compromis doivent être faits pour répondre aux exigences actuelles et futures d'utilisateurs aux profils variés. La prise en compte de cette complexité pose des problèmes aux chercheurs, développeurs et utilisateurs. Les informations et données requises pour prendre des décisions adéquates de conception et évaluer rigoureusement ces décisions sont nombreuses, parfois indéterminées et en constante évolution. Nous en déduisons trois modèles : SyCoW (travail collaboratif synchrone), SyCoE (environnement collaboratif synchrone) et SyCoEE (évaluation environnement collaboratif synchrone). Dans la partie II de cette thèse, nous proposons un processus pour la sélection / développement d'un environnement collaboratif, où nous démontrons comment les modèles SyCoW, SyCoE et SyCoEE structurent ce processus. Les résultats de l'évaluation ont confirmé la convivialité de MT-DT et fournissent des éléments de validation des choix que nous avons faits au cours du développement de MT-DT.
L'apprentissage par transfert de réseaux profonds réduit considérablement les coûts en temps de calcul et en données du processus d'entraînement des réseaux et améliore largement les performances de la tâche cible par rapport à l'apprentissage à partir de zéro. Cependant, l'apprentissage par transfert d'un réseau profond peut provoquer un oubli des connaissances acquises lors de l'apprentissage de la tâche source. Puisque l'efficacité de l'apprentissage par transfert vient des connaissances acquises sur la tâche source, ces connaissances doivent être préservées pendant le transfert. Cette thèse résout ce problème d'oubli en proposant deux schémas de régularisation préservant les connaissances pendant l'apprentissage par transfert. Nous examinons d'abord plusieurs formes de régularisation des paramètres qui favorisent toutes explicitement la similarité de la solution finale avec le modèle initial, par exemple, L1, L2, et Group-Lasso. Nous proposons également les variantes qui utilisent l'information de Fisher comme métrique pour mesurer l'importance des paramètres. Le second schéma de régularisation est basé sur la théorie du transport optimal qui permet d'estimer la dissimilarité entre deux distributions. Nous nous appuyons sur la théorie du transport optimal pour pénaliser les déviations des représentations de haut niveau entre la tâche source et la tâche cible, avec le même objectif de préserver les connaissances pendant l'apprentissage par transfert. Au prix d'une légère augmentation du temps de calcul pendant l'apprentissage, cette nouvelle approche de régularisation améliore les performances des tâches cibles et offre une plus grande précision dans les tâches de classification d'images par rapport aux approches de régularisation des paramètres.
Dans le monde d'aujourd'hui de multiples acteurs de la technologie numérique produisent des quantités infinies de données. Capteurs, réseaux sociaux ou e-commerce, ils génèrent tous de l'information qui s'incrémente en temps-réel selon les 3 V de Gartner : en Volume, en Vitesse et en Variabilité. L'objectif premier de cette étude est de pouvoir établir au moyen de ces approches une vision intégratrice du cycle de vie des données qui s'établit selon 3 étapes, (1) la synthèse des données via la sélection des valeurs-clés des micro-données acquises par les différents opérateurs au niveau de la source, (2) la fusion en faisant le tri des valeurs-clés sélectionnées et les dupliquant suivant un aspect de dé-normalisation afin d'obtenir un traitement plus rapide des données et (3) la transformation en un format particulier de carte de cartes de cartes, via Hadoop dans le processus classique de MapReduce afin d'obtenir un graphe défini dans la couche applicative. Cette réflexion est en outre soutenue par un prototype logiciel mettant en oeuvre les opérateurs de modélisation sus-décrits et aboutissant à une boîte à outils de modélisation comparable à un AGL et, permettant une mise en place assistée d'un ou plusieurs traitements sur BigData
Avec la mise en place d'entrepôts de données cliniques, de plus en plus de données de santé sont disponibles pour la recherche. Si une partie importante de ces données existe sous forme structurée, une grande partie des informations contenues dans les dossiers patients informatisés est disponible sous la forme de texte libre qui peut être exploité pour de nombreuses tâches. Dans ce manuscrit, deux tâches sont explorées~ : la classification multi-étiquette de textes cliniques et la détection de la négation et de l'incertitude. La première est étudiée en coopération avec le centre hospitalier universitaire de Rennes, propriétaire des textes cliniques que nous exploitons, tandis que, pour la seconde, nous exploitons des textes biomédicaux librement accessibles que nous annotons et diffusons gratuitement. Afin de résoudre ces tâches, nous proposons différentes approches reposant principalement sur des algorithmes d'apprentissage profond, utilisés en situations d'apprentissage supervisé et non-supervisé.
Notre recherche porte sur la recommandation de nouvelles offres d'emploi venant d'être postées et n'ayant pas d'historique d'interactions (démarrage à froid). Nous adaptons les systèmes de recommandations bien connus dans le domaine du commerce électronique à cet objectif, en exploitant les traces d'usage de l'ensemble des demandeurs d'emploi sur les offres antérieures. Une des spécificités du travail présenté est d'avoir considéré des données réelles, et de s'être attaqué aux défis de l'hétérogénéité et du bruit des documents textuels. La contribution présentée intègre l'information des données collaboratives pour apprendre une nouvelle représentation des documents textes, requise pour effectuer la recommandation dite à froid d'une offre nouvelle. Cette représentation dite latente vise essentiellement à construire une bonne métrique. L'espace de recherche considéré est celui des réseaux neuronaux. Les réseaux neuronaux sont entraînés en définissant deux fonctions de perte. La première cherche à préserver la structure locale des informations collaboratives, en s'inspirant des approches de réduction de dimension non linéaires. La seconde s'inspire des réseaux siamois pour reproduire les similarités issues de la matrice collaborative. Le passage à l'échelle de l'approche et ses performances reposent sur l'échantillonnage des paires d'offres considérées comme similaires. L'intérêt de l'approche proposée est démontrée empiriquement sur les données réelles et propriétaires ainsi que sur le benchmark publique CiteULike. Enfin, l'intérêt de la démarche suivie est attesté par notre participation dans un bon rang au challenge international RecSys 2017 (15/100 ; un million d'utilisateurs pour un million d'offres).
Durant ces dernières années, les architectures de réseaux de neurones (RN) ont été appliquées avec succès à de nombreuses applications en Traitement Automatique de Langues (TAL), comme par exemple en Reconnaissance Automatique de la Parole (RAP) ainsi qu'en Traduction Automatique (TA). Pour la tâche de modélisation statique de la langue, ces modèles considèrent les unités linguistiques (c'est-à-dire des mots et des segments) à travers leurs projections dans un espace continu (multi-dimensionnel), et la distribution de probabilité à estimer est une fonction de ces projections. Ainsi connus sous le nom de "modèles continus" (MC), la particularité de ces derniers se trouve dans l'exploitation de la représentation continue qui peut être considérée comme une solution au problème de données creuses rencontré lors de l'utilisation des modèles discrets conventionnels. Dans le cadre de la TA, ces techniques ont été appliquées dans les modèles de langue neuronaux (MLN) utilisés dans les systèmes de TA, et dans les modèles continus de traduction (MCT). L'utilisation de ces modèles se sont traduit par d'importantes et significatives améliorations des performances des systèmes de TA. Ils sont néanmoins très coûteux lors des phrases d'apprentissage et d'inférence, notamment pour les systèmes ayant un grand vocabulaire. Afin de surmonter ce problème, l'architecture SOUL (pour "Structured Output Layer" en anglais) et l'algorithme NCE (pour "Noise Contrastive Estimation", ou l'estimation contrastive bruitée) ont été proposés : le premier modifie la structure standard de la couche de sortie, alors que le second cherche à approximer l'estimation du maximum de vraisemblance (MV) par une méthode d'échantillonnage. Toutes ces approches partagent le même critère d'estimation qui est la log-vraisemblance ; pourtant son utilisation mène à une incohérence entre la fonction objectif définie pour l'estimation des modèles, et la manière dont ces modèles seront utilisés dans les systèmes de TA. Cette dissertation vise à concevoir de nouvelles procédures d'entraînement des MC, afin de surmonter ces problèmes. Les contributions principales se trouvent dans l'investigation et l'évaluation des méthodes d'entraînement efficaces pour MC qui visent à : (i) réduire le temps total de l'entraînement, et (ii) améliorer l'efficacité de ces modèles lors de leur utilisation dans les systèmes de TA. D'un côté, le coût d'entraînement et d'inférence peut être réduit (en utilisant l'architecture SOUL ou l'algorithme NCE), ou la convergence peut être accélérée. La dissertation présente une analyse empirique de ces approches pour des tâches de traduction automatique à grande échelle. D'un autre côté, nous proposons un cadre d'apprentissage discriminant qui optimise la performance du système entier ayant incorporé un modèle continu. Les résultats expérimentaux montrent que ce cadre d'entraînement est efficace pour l'apprentissage ainsi que pour l'adaptation des MC au sein des systèmes de TA, ce qui ouvre de nouvelles perspectives prometteuses.
Le concept du web des objets (WOT - web of things) est devenu une réalité avec le développement d'internet, des réseaux, des technologies matérielles et des objets communicants. De nos jours, il existe un nombre croissant d'objets susceptibles d'être utilisés dans des applications spécifiques. Le Monde est ainsi plus étroitement connecté, différents objets pouvant maintenant partager leurs informations et être ainsi utilisés à travers une structure similaire à celle du Web classique. Cependant, même si des objets hétérogènes ont la possibilité de se connecter au Web, ils ne peuvent pas être utilisés dans différentes applications à moins de posséder un modèle de représentation et d'interrogation commun capable de prendre en compte leur hétérogénéité. Dans cette thèse, notre objectif est d'offrir un modèle commun pour décrire les objets hétérogènes et pouvoir ensuite les utiliser pour accéder aux requêtes des utilisateurs. Ceux-ci peuvent avoir différentes demandes, que ce soit pour trouver un objet particulier ou pour réaliser certaines tâches. Nous mettons en évidence deux directions de recherche. Dans un premier temps, nous étudions d'abord les technologies, les applications et les domaines existants où le WOT peut être appliqué. Nous comparons les modèles de description existants dans ce domaine et nous mettons en évidence leurs insuffisances lors d'applications relatives au WOT.
Cette thèse de doctorat est centrée sur l'apprentissage supervisé. Il faut imposer des contraintes sur la variance du problème comme une condition de Bernstein ou de marge. On dit qu'un estimateur est robuste si ce dernier présente certaines garanties théoriques, sous le moins d'hypothèses possibles. Cette problématique de robustesse devient de plus en plus populaire. Ainsi, construire des estimateurs fiables dans cette situation est essentiel. Dans cette thèse nous montrons que le fameux minimiseur du risque empirique (regularisé) associé à une fonction de perte Lipschitz est robuste à des bruits à queues lourde ainsi qu'a des outliers dans les labels. En revanche si la classe de prédicteurs est à queue lourde, cet estimateur n'est pas fiable. Dans ce cas, nous construisons des estimateurs appelé estimateur minmax-MOM, optimal lorsque les données sont à queues lourdes et possiblement corrompues. En grande dimension, certains estimateurs interpolant les données peuvent être bons. En particulier, cette thèse nous étudions le modèle linéaire Gaussien en grande dimension et montrons que l'estimateur interpolant les données de plus petite norme est consistant et atteint même des vitesses rapides.
L'analyse chimique de la composante odorante repose sur une stratégie séparative qui permet d'identifier les différents odorants présents dans l'aliment. Cependant, la perception des odorants en mélange induit des interactions au niveau perceptif qui ne sont pas prises en compte dans les techniques séparatives. Les mécanismes sous-jacents aux interactions perceptives sont mal connus, ce qui limite les possibilités de prédiction de l'odeur d'un aliment sur la base de sa composition chimique. Cependant, les études concernent des odorants seuls et non leurs mélanges. La seconde repose sur la recombinaison d'odorants en mélange après l'étape d'analyse séparative, mais le choix des odorants à associer est essentiellement empirique. Ainsi, deux questions se posent : Comment prédire l'odeur de mélanges de molécules d'après la structure moléculaire des odorants ? Comment améliorer l'analyse de la flaveur dans le but de prédire l'odeur d'aliments complexes composés de plusieurs dizaine d'odorants en mélanges ? Ces deux questions ont été abordées dans cette thèse dont les travaux sont décrits dans ce manuscrit selon deux axes principaux. Le premier axe décrit l'utilisation et le développement d'un modèle basé sur le concept des distances angulaires calculées à partir de la structure moléculaire des odorants avec pour objectif de prédire la similarité perceptive de mélanges plus ou moins complexes d'odorants. Les résultats soulignent l'importance de prendre en compte la dimension d'intensité des odorants afin d'améliorer la qualité de la prédiction. Des perspectives d'amélioration du modèle sont dégagées pour permettre de dépasser la dimension de similarité et prédire des dimensions qualitatives de l'odeur. Le deuxième axe présente une démarche originale d'intégration de connaissances liées à l'expertise dans la procédure d'analyse de la flaveur. Ainsi, trois types de données hétérogènes sont agrégés dans un modèle mathématique global : des données chimiques, des données sensorielles et des connaissances d'experts aromaticiens. L'expertise est intégrée à travers la création d'une ontologie qui est ensuite associée à une approche de logique floue optimisée par algorithme évolutionnaire. Le modèle développé permet de prédire le profil odorant de seize vins rouges sur la base de leur composition en odorants. Au final, l'ensemble des travaux menés dans cette thèse apporte des résultats originaux permettant une meilleure compréhension de la construction des odeurs des aliments et permet d'élaborer des hypothèses quant aux relations sous-jacentes de l'espace perceptif des odeurs en mélanges complexes.
Le comptage des foules est un sujet de recherche important. De nos jours, la population est de plus en plus préoccupée par les problèmes de sécurité. Lorsque la densité de population atteint des pics élevés, les systèmes de comptage se mettent en route et analyse les foules, afin de réorienter le surplus de personnes lorsque le seuil normal est dépassé. Avec ce genre de système, le piétinement du nouvel an de Shanghai ne se reproduirait plus. L'aspect le plus critique pour cette analyse est l'impossibilité d'installer un système de vidéosurveillance intelligent dans certains lieux publics. Dans ces conditions, comment pourrions-nous estimer la densité de population dans ces zones afin d'éviter de futurs accidents ? Face à ces défis, nous proposons la mise en œuvre d'une architecture embarquée reconfigurable en temps réel pour le comptage des personnes dans les zones de regroupement. Premièrement, notre travail intègre les fonctionnalités de HOG et LBP, qui non seulement combinent les informations d'identifications de multiples caractéristiques, mais également la plupart des informations redondantes, réalisant ainsi une compression efficace des informations, économisant ainsi de l'espace mémoire pour le stockage des données. Pour le comptage de personnes dans une foule, nous utilisons plusieurs sources d'informations, à savoir HOG, LBP et le filtrage de CANNY. Ces sources fournissent des estimations distinctes du nombre de personnes comptées et d'autres mesures statistiques de classification, par le biais du vecteur de support machine SVM. Dans le même temps, afin de résoudre efficacement le problème d'extraction des fonctionnalités liées à l'échelle dans le comptage de foules, nous proposons un nouveau environnement M-MCNN basé sur MCNN utilisé pour le comptage de foules sur une seule image. M-MCNN contient non seulement les trois colonnes originales des réseaux de neurones convolutionnels avec différentes tailles de filtres, mais aussi remplace les couches entièrement connectées par une couche convolutionnelle de filtre 1*1, de sorte que l'image d'entrée du modèle peut être de n'importe quelle taille. De plus, pour un échantillon individuel, nous améliorons considérablement l'apprentissage des caractéristiques de l'échantillon en extrayant les caractéristiques de texture d'une seule tête humaine et mieux l'utiliser pour les jeux de données. Enfin, nous implémentons notre nouveau framework M-MCNN sur un FPGA et l'installons sur un drone pour estimer et prévoir la zone de foule à haute densité en temps réel. Notre modèle a obtenu de bons résultats en comptage de personnes dans une foule.
Ce travail s'inscrit dans le projet « Personnalisation des EIAH » étudiant l'utilisation des traces d'activité dans l'évaluation et la personnalisation des situations d' apprentissage médiatisées. L'analyse des traces se fait généralement par des outils d'analyse réalisés par les chercheurs et spécifiques à leurs besoins. Les résultats publiés ne peuvent généralement pas être vérifiés ni comparés, ceci étant dû aux difficultés de partage des corpus et des outils d'analyse. L'objectif de ce travail est de proposer aux chercheurs utilisant les EIAH, une plateforme pour le partage, d'une part des corpus de traces d'interaction contextualisées et les analyses réalisées sur ces corpus, et des outils d'analyse et de visualisation des traces d'autre part. L'hétérogénéité des traces produites par les EIAH, due à la variété des domaines d'apprentissage et des besoins d'analyse, fait que la proposition d'une représentation commune ne peut répondre aux différents besoins de chercheurs pluridisciplinaires. Nous proposons l'approche par « proxy » , une solution participative et incrémentale basée sur une ontologie qui définit trois modèles : un modèle de corpus définissant la structure et les métadonnées de description du corpus et son contenu, un modèle définissant les concepts génériques pouvant être retrouvés dans les corpus, et un modèle opérationnel définissant des opérations assurant l'interopérabilité entre un corpus et un outil d'analyse partagé. En nous basant sur cette approche, nous proposons une architecture de plateforme de partage de corpus de traces et d'outils d'analyse permettant aux chercheurs de partager leurs corpus,d'accéder aux corpus partagés, et de les analyser.
Depuis les débuts du défi DARPA, la conception de voitures autonomes suscite un intérêt croissant. Cet intérêt grandit encore plus avec les récents succès des algorithmes d'apprentissage automatique dans les tâches de perception. Bien que la précision de ces algorithmes soit irremplaçable, il est très difficile d'exploiter leur potentiel. Les contraintes en temps réel ainsi que les problèmes de fiabilité alourdissent le fardeau de la conception de plateformes matériels efficaces. Nous discutons les différentes implémentations et techniques d'optimisation de ces plateformes dans ce travail. Nous abordons le problème de ces accélérateurs sous deux perspectives : performances et fiabilité. Nous proposons deux techniques d'accélération qui optimisent l'utilisation du temps et des ressources. Sur le vol et fiabilité, nous étudions la résilience des algorithmes de Machine Learning face aux fautes matérielles. Nous proposons un outil qui indique si ces algorithmes sont suffisamment fiables pour être employés dans des systèmes critiques avec de fortes critères sécuritaire ou non. Un accélérateur sur processeur associatif résistif est présenté. Cet accélérateur atteint des performances élevées en raison de sa conception en mémoire qui remédie au goulot d'étranglement de la mémoire présent dans la plupart des algorithmes d'apprentissage automatique. Quant à l'approche de multiplication constante, nous avons ouvertla porte à une nouvelle catégorie d'optimisations en concevant des accélérateurs spécifiques aux instances. Les résultats obtenus surpassent les techniques les plus récentes en termes de temps d'exécution et d'utilisation des ressources. Combinés à l'étude de fiabilité que nous avons menée, les systèmes ou la sécurité est de priorité peuvent profiter de ces accélérateurs sans compromettre cette dernière.
L'apprentissage profond a abouti à des développements spectaculaires dans le domaine de l'image et du langage naturel au cours des dernières années. Pourtant, dans de nombreux domaines, les données d'observations ne sont ni des images ni du texte mais des séries temporelles qui représentent l'évolution de grandeurs mesurées ou calculées. Dans cette thèse, nous étudions et proposons différentes représentations de séries temporelles à partir de modèles d'apprentissage profond. Dans un premier temps, dans le domaine du contrôle de véhicules autonomes, nous montrons que l'analyse d'une fenêtre temporelle par un réseau de neurones permet d'obtenir de meilleurs résultats que les méthodes classiques qui n'utilisent pas de réseaux de neurones. Dans un troisième temps, dans un but de génération de mouvements humains, nous proposons des réseaux de neurones génératifs convolutifs 2D où les dimensions temporelles et spatiales sont convoluées de manière jointe. Enfin, dans un dernier temps, nous proposons un plongement où des représentations spatiales de poses humaines sont (ré)organisées dans un espace latent en fonction de leurs relations temporelles.
Les équipements domestiques qui supportent ces protocoles peuvent être détectés automatiquement, configurés et invoqués pour une tâche donnée. Actuellement, plusieurs protocoles coexistent dans la maison, mais les interactions entre les dispositifs ne peuvent pas être mises en action à moins que les appareils supportent le même protocole. En plus, les applications qui orchestrent ces dispositifs doivent connaître à l'avance les noms des services et dispositifs. Or, chaque protocole définit un profil standard par type d'appareil. Par conséquent, deux appareils ayant le même type et les mêmes fonctions mais qui supportent un protocole différent publient des interfaces qui sont souvent sémantiquement équivalentes mais syntaxiquement différentes. Ceci limite alors les applications à interagir avec un service similaire. Dans ce travail, nous présentons une méthode qui se base sur l'alignement d'ontologie et la génération automatique de mandataire pour parvenir à une adaptation dynamique de services.
Malgré les différences typologiques entre le français et le turc, nous posons l'hypothèse que des rapprochements entre ces deux langues peuvent être effectués au niveau des significations et des procédés de construction de significations. Cette approche, fondée sur l'établissement d'un répertoire de significations pourrait être élargie vers d'autres langues ne disposant pas d'outils semblables.
La vision par ordinateur est un domaine qui inclut des méthodes d'acquisition, de traitement, d'analyse et de compréhension des images afin de produire de l'information numérique ou symbolique. Un axe de recherche contribuant au développement de ce domaine consiste à reproduire les capacités de la vision humaine par voie électronique afin de percevoir et de comprendre une image. Nos travaux de thèse s'inscrivent dans cet axe de recherche. Nous proposons plusieurs contributions originales s'inscrivant dans le cadre de résolution des problèmes de la reconnaissance et de la localisation des symboles graphiques en contexte. L'originalité des approches proposées réside dans la proposition d'une alliance intéressante entre l'Analyse Formelle de Concepts et la vision par ordinateur. Pour ce faire, nous nous sommes confrontés à l'étude du domaine de l'AFC et plus précisément l'adaptation de la structure du treillis de concepts et son utilisation comme étant l'outil majeur de nos travaux. La principale particularité de notre travail réside dans son aspect générique vu que les méthodes proposées peuvent être alliées à divers outils autre que le treillis de concepts en gardant les mêmes stratégies adoptées et en suivant une procédure semblable. Le principal avantage du treillis de concepts est l'aspect symbolique qu'il offre. Il présente un espace de recherche concis, précis et souple facilitant ainsi la prise de décision. Nos contributions sont inscrites dans le cadre de la reconnaissance et de localisation de symboles dans les documents graphiques. Nous proposons des chaînes de traitement s'inscrivant dans le domaine de la vision par ordinateur
Des ressources telles que les terminologies ou les ontologies sont utilisees dans differentes applications, notamment dans la description documentaire et la recherche d'information. Differentes methodologies ont ete proposees pour construire ce type de ressources, que ce soit a partir d'entrevues d'experts du domaine ou a partir de corpus textuels. Nous nous interessons dans ce memoire a l'utilisation de methodologies existantes dans le domaine du traitement automatique des langues, destinees a la construction d'ontologies a partir de corpus textuels, pour la construction du type de ressource particulier que sont les ontologies differentielles.
L'extraction de catalogues de sources fiables à partir des images est cruciale pour un large éventail de recherches en astronomie. Cependant, l'efficacité des méthodes de détection de source actuelles est sérieusement limitée dans les champs encombrés, ou lorsque les images sont contaminées par des défauts optiques, électroniques et environnementaux. Les performances en termes de fiabilité et de complétude sont aujourd'hui souvent insuffisantes au regard des exigences scientifiques des grands relevés d'imagerie. Dans cette thèse, nous développons de nouvelles méthodes pour produire des catalogues sources plus robustes et fiables. Nous tirons parti des progrès récents en apprentissage supervisé profond pour concevoir des modèles génériques et fiables basés sur des réseaux de neurones à convolutions (CNNs).Nous présentons MaxiMask et MaxiTrack, deux réseaux de neurones à convolutions que nous avons entrainés pour identifier automatiquement 13 types différents de défauts d'image dans des expositions astronomiques. Nous présentons également un prototype de détecteur de sources multi-échelle et robuste vis-à-vis des défauts d'image, dont nous montrons qu'il surpasse largement les algorithmes existants en terme de performances. Nous discutons des limites actuelles et des améliorations potentielles de notre approche dans le cadre des prochains grands relevés tels que Euclid.
Associé à des images préconçues et pétrit de préjugés, le roman sentimental de type sériel est un produit de consommation qui représente un miroir social et culturel. Présent en France depuis plusieurs dizaines d'années, il a conquis un public hétérogène et fidèle. Quels sont les ressorts de ces romans qu'on dit tous similaires ? Que représentent-ils pour le lectorat ? Comment sont-ils perçus par ceux qui les lisent ? Comment sont-ils lu, appréhendés, interprétés ? Autant de questions auxquelles nous nous efforçons de répondre dans ce travail pluridisciplinaire. Cette recherche a permis de mettre au jour les mécanismes à l'œuvre dans l'évolution du roman sentimental sériel depuis la seconde guerre mondiale, elle a également permis d'observer différents modes de réception et de lecture
Depuis une vingtaine d'années, l'accès et l'utilisation des données médicales sont devenus des enjeux majeurs pour les professionnels de santé comme pour le grand public. Dans ce contexte, plusieurs terminologies médicales spécialisées ont été créées. Ces terminologies ont pour la plupart des formats de représentation et visées différentes : la nomenclature SNOMED 3. 5 pour le codage d'informations cliniques, les classifications CIM10 et CCAM pour le codage épidémiologique puis médico-économique, le thésaurus MeSH pour la bibliographie... Devant ce constat et la nécessité grandissante de permettre la coopération de différents acteurs de la santé et des systèmes d'information associés, il apparaît nécessaire de rendre les terminologies "interopérables". De plus, nous utilisons nos différents algorithmes conjointement avec le métathésaurus UMLS afin d'apporter une plus grande couverture au niveau des relations entre les terminologies. Nous bénéficions, notamment, dans le cadre de cette thèse, d'une expérience riche dans le domaine du Traitement Automatique de la Langue (TAL) issue des précédents travaux de recherche dans les équipes CISMeF et LERTIM.
La recherche en Sciences humaines et sociales repose souvent sur de grandes masses de données textuelles, qu'il serait impossible de lire en détail. Le Traitement automatique des langues (TAL) peut identifier des concepts et des acteurs importants mentionnés dans un corpus, ainsi que les relations entre eux. Ces informations peuvent fournir un aperçu du corpus qui peut être utile pour les experts d'un domaine et les aider à identifier les zones du corpus pertinentes pour leurs questions de recherche. Pour annoter automatiquement des corpus d'intérêt en Humanités numériques, les technologies TAL que nous avons appliquées sont, en premier lieu, le liage d'entités (plus connu sous le nom de Entity Linking), pour identifier les acteurs et concepts du corpus ;  deuxièmement, les relations entre les acteurs et les concepts ont été déterminées sur la base d'une chaîne de traitements TAL, qui effectue un étiquetage des rôles sémantiques et des dépendances syntaxiques, entre autres analyses linguistiques. La partie I de la thèse décrit l'état de l'art sur ces technologies, en soulignant en même temps leur emploi en Humanités numériques. Des outils TAL génériques ont été utilisés. Comme l'efficacité des méthodes de TAL dépend du corpus d'application, des développements ont été effectués, décrits dans la partie II, afin de mieux adapter les méthodes d'analyse aux corpus dans nos études de cas. La partie II montre également une évaluation intrinsèque de la technologie développée, avec des résultats satisfaisants. Les technologies ont été appliquées à trois corpus très différents, comme décrit dans la partie III. Deuxièmement, le corpus PoliInformatics, qui contient des matériaux hétérogènes sur la crise financière américaine de 2007--2008. Enfin, le Bulletin des Négociations de la Terre (ENB dans son acronyme anglais), qui couvre des sommets internationaux sur la politique climatique depuis 1995, où des traités comme le Protocole de Kyoto ou les Accords de Paris ont été négociés. Pour chaque corpus, des interfaces de navigation ont été développées. Ces interfaces utilisateur combinent les réseaux, la recherche en texte intégral et la recherche structurée basée sur des annotations TAL. À titre d'exemple, dans l'interface pour le corpus ENB, qui couvre des négociations en politique climatique, des recherches peuvent être effectuées sur la base d'informations relationnelles identifiées dans le corpus : les acteurs de la négociation ayant discuté un sujet concret en exprimant leur soutien ou leur opposition peuvent être recherchés. Le type de la relation entre acteurs et concepts est exploité, au-delà de la simple co-occurrence entre les termes du corpus. Les interfaces ont été évaluées qualitativement avec des experts de domaine, afin d'estimer leur utilité potentielle pour la recherche dans leurs domaines respectifs. Tout d'abord, il a été vérifié si les représentations générées pour le contenu des corpus sont en accord avec les connaissances des experts du domaine, pour déceler des erreurs d'annotation. Ensuite, nous avons essayé de déterminer si les experts pourraient être en mesure d'avoir une meilleure compréhension du corpus grâce à avoir utilisé les applications, par exemple, s'ils ont trouvé de l'évidence nouvelle pour leurs questions de recherche existantes, ou s'ils ont trouvé de nouvelles questions de recherche. On a pu mettre au jour des exemples où un gain de compréhension sur le corpus est observé grâce à l'interface dédiée au Bulletin des Négociations de la Terre, ce qui constitue une bonne validation du travail effectué dans la thèse. En conclusion, les points forts et faiblesses des applications développées ont été soulignés, en indiquant de possibles pistes d'amélioration en tant que travail futur.
Dans cette thèse, nous décrivons la création du French FrameNet (FFN), une ressource de type FrameNet pour le français créée à partir du FrameNet de l'anglais (Baker et al., 1998) et de deux corpus arborés : le French Treebank (Abeillé et al., 2003) et le Sequoia Treebank (Candito et Seddah, 2012). La ressource séminale, le FrameNet de l'anglais, constitue un modèle d'annotation sémantique de situations prototypiques et de leurs participants. Elle propose à la fois : a) un ensemble structuré de situations prototypiques, appelées cadres, associées à des caractérisations sémantiques des participants impliqués (les rôles) ; b) un lexique de déclencheurs, les lexèmes évoquant ces cadres ; c) un ensemble d'annotations en cadres pour l'anglais. Pour créer le FFN, nous avons suivi une approche « par domaine notionnel » : nous avons défini quatre « domaines » centrés chacun autour d'une notion (cause, communication langagière, position cognitive ou transaction commerciale), que nous avons travaillé à couvrir exhaustivement à la fois pour la définition des cadres sémantiques, la définition du lexique, et l'annotation en corpus. Cette stratégie permet de garantir une plus grande cohérence dans la structuration en cadres sémantiques, tout en abordant la polysémie au sein d'un domaine et entre les domaines. De plus, nous avons annoté les cadres de nos domaines sur du texte continu, sans sélection d'occurrences : nous préservons ainsi la distribution des caractéristiques lexicales et syntaxiques de l'évocation des cadres dans notre corpus. à l'heure actuelle, le FFN comporte 105 cadres et 873 déclencheurs distincts, qui donnent lieu à 1109 paires déclencheur-cadre distinctes, c'est-à-dire 1109 sens. Le corpus annoté compte au total 16167 annotations de cadres de nos domaines et de leurs rôles. La thèse commence par resituer le modèle FrameNet dans un contexte théorique plus large. Nous justifions ensuite le choix de nous appuyer sur cette ressource et motivons notre méthodologie en domaines notionnels. Nous explicitons pour le FFN certaines notions définies pour le FrameNet de l'anglais que nous avons jugées trop floues pour être appliquées de manière cohérente. Nous introduisons en particulier des critères plus directement syntaxiques pour la définition du périmètre lexical d'un cadre, ainsi que pour la distinction entre rôles noyaux et non-noyaux. Nous décrivons ensuite la création du FFN : d'abord, la délimitation de la structure de cadres utilisée pour le FFN, et la création de leur lexique. Nous présentons alors de manière approfondie le domaine notionnel des positions cognitives, qui englobe les cadres portant sur le degré de certitude d'un être doué de conscience sur une proposition. Puis, nous présentons notre méthodologie d'annotation du corpus en cadres et en rôles. à cette occasion, nous passons en revue certains phénomènes linguistiques qu'il nous a fallu traiter pour obtenir une annotation cohérente ;  Enfin, nous présentons des données quantitatives sur le FFN tel qu'il est à ce jour et sur son évaluation. Nous terminons sur des perspectives de travaux d'amélioration et d'exploitation de la ressource créée.
Cette thèse se penche sur l'analyse et les contraintes d'élaboration des composants permettant aux systèmes de répondre à des questions en domaine ouvert. Les systèmes de questions réponses (SQR) sont adaptés à cette tâche, car ils associent une question factuelle à une réponse précise. Il est proposé d'adapter les SQR à une tâche d'interaction afin de leur définir un cadre d'adaptation générique aux systèmes de dialogue. Sont étudiés, les techniques existantes pour les SQR en domaine ouvert, les différents modèles de dialogue homme-machine ainsi que des systèmes à la limite entre le dialogue et la recherche d'informations. Ensuite, est présenté la construction d'un modèle de structure permettant de décrire les interactions entre des questions. Du cadre formel, est déduis une méthode de calcul qui est évalué. Afin de discuter cette évaluation et de voir comment utiliser notre modèle, il est réalisé une analyse des moteurs de recherche pour les systèmes de questions réponses. Dans une dernière partie, il est présenté une utilisation des données calculées pour l'amélioration des résultats de réponses aux questions.
La langue arabe a une morphologie particulière fondée sur des racines et des schèmes. L'Abjad n'est pas seulement un problème culturel, mais aussi un problème informatique.
Le rire est une vocalisation universelle à travers les cultures et les langues. Il est omniprésent dans nos dialogues et utilisé pour un large éventail de fonctions. Le rire a été étudié sous plusieurs angles, mais les classifications proposées sont difficiles à intégrer dans un même système. Malgré le fait qu'il soit crucial dans nos interactions quotidiennes, le rire en conversation a reçu peu d'attention et les études sur la pragmatique du rire en interaction, ses corrélats neuronaux perceptuels et son développement chez l'enfant sont rares. Dans cette thèse, est proposé un nouveau cadre pour l'analyse du rire, fondé sur l'hypothèse cruciale que le rire a un contenu propositionnel, plaidant pour la nécessité de distinguer différentes couches d'analyse, tout comme dans l'étude de la parole : forme, positionnement, sémantique et pragmatique. Des études préliminaires sont menées sur la viabilité d'un mappage forme-fonction du rire basée sur ses caractéristiques acoustiques, ainsi que sur les corrélats neuronaux impliqués dans la perception du rire qui servent différentes fonctions dans un dialogue naturel. Nos résultats donnent lieu à de nouvelles généralisations sur le placement, l'alignement, la sémantique et les fonctions du rire, soulignant le haute niveau des compétences pragmatiques impliquées dans sa production et sa perception. Le développement de l'utilisation sémantique et pragmatique du rire est observé dans une étude de corpus longitudinale de 4 dyades mère-enfant de l'age de 12 à 36 mois, locuteurs d'anglais américain. Les résultats montrent que l'utilisation du rire subit un développement important à chaque niveau analysé et que le rire peut être un indicateur précoce du développement cognitif, communicatif et social.
Nous avons mené différentes études pour étudier le phénomène entre des paires d'inconnus, d'amis de longue date, puis entre des personnes provenant de la même famille. On s'attend à ce que l'amplitude de la convergence soit liée à la distance sociale entre les deux interlocuteurs. On retrouve bien ce résultat. Nous avons ensuite étudié l'impact de la connaissance de la cible linguistique sur l'adaptation. Pour caractériser la convergence phonétique, nous avons développé deux méthodes : la première basée sur une analyse discriminante linéaire entre les coefficients MFCC de chaque locuteur Finalement, nous avons caractérisé la convergence phonétique à l'aide d'une mesure subjective en utilisant un nouveau test de perception basé sur la détection « en ligne » d'un changement de locuteur. Le test a été réalisé à l'aide signaux extraits des interactions mais également avec des signaux obtenus avec une synthèse adaptative basé sur la modélisation HNM.
Les acteurs d'une entreprise doivent collaborer efficacement pour atteindre les objectifs de l'entreprise, et ce dans un environnement évoluant en permanence, alors que les systèmes informatiques actuels sont pour la plupart construits sur la base d'analyses de l'organisation de l'entreprise et d'un recueil des besoins des utilisateurs à un moment donné. Or les médiations par lesquelles les acteurs d'un processus de conception vont échanger des informations, spécifier leurs tâches, se coordonner, évaluer l'activité sont les véritables objets qui doivent former la base des outils informatiques de support des activités coopératives de conception. Ces ajustements passent presque exclusivement par la communication et souvent suivant des circuits qui ne peuvent être définis à l'avance. Les méthodes et outils d'organisation du travail peuvent bénéficier des nouvelles technologies de la communication pour rendre ces adaptations plus efficaces. Un nouveau type d'outillage informatique devient alors nécessaire pour désambigui͏̈ser, faciliter, structurer, améliorer, augmenter, et permettre des recherches évoluées dans les échanges survenant entre les acteurs (humains et informatiques) d'un système sociotechnique coopératif. Les systèmes multi-agents, dynamiquement reconfigurables, peuvent suivre ces évolutions et prendre en compte les spécificités de chacun des acteurs.
Le but de cette thèse est de développer un outil de traitement automatique des langues naturelles visant à analyser et reformuler les problèmes arithmétiques des élèves du 3e cycle de l'école primaire. Plus précisément, cet outil devrait permettre  :  De détecter les éléments de difficultés linguistiques dans les problèmes mathématiques additifs. De classifier des problèmes en fonction des difficultés linguistiques qu'ils présentent. De proposer une reformulation de ces problèmes afin d'en faciliter la compréhension. D'évaluer la reformulation proposée, ce qui peut revenir à comparer deux versions d'un même problème.
Cette thèse s'intéresse à la détection et la reconnaissance du texte arabe incrusté dans les vidéos. Dans ce contexte, nous proposons différents prototypes de détection et d'OCR vidéo (Optical Character Recognition) qui sont robustes à la complexité du texte arabe (différentes échelles, tailles, polices, etc.) Nous introduisons différents détecteurs de texte arabe qui se basent sur l'apprentissage artificiel sans aucun prétraitement. Les détecteurs se basent sur des Réseaux de Neurones à Convolution (ConvNet) ainsi que sur des schémas de boosting pour apprendre la sélection des caractéristiques textuelles manuellement conçus. Nous utilisons différents modèles d'apprentissage profond, regroupant des Auto-Encodeurs, des ConvNets et un modèle d'apprentissage non-supervisé, qui génèrent automatiquement ces caractéristiques. Chaque modèle résulte en un système d'OCR bien spécifique. Le processus de reconnaissance se base sur une approche connexionniste récurrente pour l'apprentissage de l'étiquetage des séquences de caractéristiques sans aucune segmentation préalable. Nos modèles d'OCR proposés sont comparés à d'autres modèles qui se basent sur des caractéristiques manuellement conçues. Nous proposons un schéma de décodage conjoint qui intègre les inférences du LM en parallèle avec celles de l'OCR tout en introduisant un ensemble d'hyper-paramètres afin d'améliorer la reconnaissance et réduire le temps de réponse. Afin de surpasser le manque de corpus textuels arabes issus de contenus multimédia, nous mettons au point de nouveaux corpus manuellement annotés à partir des flux TV arabes. Le corpus conçu pour l'OCR, nommé ALIF et composée de 6,532 images de texte annotées Nos systèmes ont été développés et évalués sur ces corpus. L'étude des résultats a permis de valider nos approches et de montrer leurs efficacité et généricité avec plus de 97% en taux de détection, 88.63% en taux de reconnaissance mots sur le corpus ALIF dépassant ainsi un des systèmes d'OCR commerciaux les mieux connus par 36 points.
Cette thèse de doctorat porte sur le processus de développement d'un logiciel de sous-titrage adapté au contexte de l'enseignement supérieur à partir des pratiques d'une professionnelle de ce secteur. En premier lieu, une révision des fondements théoriques de la Traduction Audiovisuelle et du sous-titrage permet de définir les caractéristiques linguistiques et sémiotiques du texte audiovisuel et de sa traduction. Ce travail se poursuit par l'étude de normes extratextuelles explicites et par l'identification des fonctionnalités qui rendent possible leur application afin que le sous-titreur puisse adapter son projet aux attentes de la culture-cible. Cette thèse aborde également l'aspect professionnel de cette pratique via l'analyse de l'impact des avancées technologiques sur le processus de sous-titrage et des outils dont disposent les sous-titreurs. Elle présente également l'utilisation de matériel audiovisuel sous-titré dans l'enseignement supérieur au moyen d'un exemple d'environnement virtuel d'apprentissage multilingue et multiculturel. L'ensemble de ces analyses a permis de concevoir un nouveau logiciel de sous-titrage, Miro Translate, plateforme hybride en ligne spécialement élaborée pour le sous-titrage de vidéos pédagogiques. Enfin, la qualité de cet outil est étudiée par le biais d'un test d'utilisabilité qui mesure la satisfaction des utilisateurs, son efficience et son efficacité afin d'identifier les actions nécessaires pour son amélioration.
1. Etudes économétriques de mouvements de protestation sur les réseaux sociaux 2. Développement de nouvelles méthodes d'analyse de données liées aux réseaux sociaux 3. Développement de nouvelles méthodes d'analyse de texte appliquées à l'économie
Dans la première partie de la thèse, nous proposons un formalisme d'optimisation général, commun à l'ensemble des méthodes d'apprentissage semi-supervisé et en particulier aux Laplacien Standard, Laplacien Normalisé et PageRank. En utilisant la théorie des marches aléatoires, nous caractérisons les différences majeures entre méthodes d'apprentissage semi-supervisé et nous définissons des critères opérationnels pour guider le choix des paramètres du noyau ainsi que des points étiquetés. Cette application montre de façon édifiante que la famille de méthodes proposée passe parfaitement à l'échelle. Plus précisément, nous proposons des algorithmes randomisés pour la détection rapide des nœuds de grands degrés et des nœuds avec de grandes valeurs de PageRank personnalisé. A la fin de la thèse, nous proposons une nouvelle mesure de centralité, qui généralise à la fois la centralité d'intermédiarité et PageRank. Cette nouvelle mesure est particulièrement bien adaptée pour la détection de la vulnérabilité de réseau.
Considéré comme un historien qui sacrifie rigueur et exactitude à son souci de la rhétorique, Quinte-Curce jouit, et avec lui son histoire « romancée » , d'une réputation en demi-teinte. L'historien se livre alors à une véritable entreprise de démystification qui touche la nature même de cet Orient merveilleux, la fortune providentielle dont se réclame le Macédonien et même le langage. Sont ainsi condamnés la quête effrénée de gloire que poursuit le roi, et son rêve de divinisation : l'Orient est synonyme de renversement généralisé des normes et des valeurs, la fortune une illusion conduisant à un sentiment d'impunité. En filigrane, il propose aussi un idéal du pouvoir qui repose essentiellement sur l'équilibre et sur la responsabilité du prince. Par là, il interroge, au regard des réalités politiques de son temps, la pertinence d'un mythe central dans l'imaginaire politique romain et dont l'ombre plane sur tous les ambitieux, à commencer par les empereurs ou les candidats à l'Empire. Son récit bien mené incite donc à une réflexion réelle sur l'exercice du pouvoir, ses enjeux et ses limites.
L'extraction de la sémantique d'une image est un processus qui nécessite une analyse profonde du contenu de l'image. Elle se réfère à leur interprétation à partir d'un point de vuehumain. Elle consiste à extraire une sémantique simple ou multiple de l'image afin de faciliter sa récupération. Ces objectifs indiquent clairement que l'extraction de la sémantique n'est pas un nouveau domaine de recherche. Cette thèse traite d'une approche d'annotation collaborative et de recherche d'images baséesur les sémantiques émergentes. Il aborde d'une part, la façon dont les annotateurs pourraient décrire et représenter le contenu des images en se basant sur les informations visuelles, et d'autre part comment la recherche des images pourrait être considérablement améliorée grâce aux récentes techniques, notamment le clustering et la recommandation. Pour atteindre ces objectifs, l'exploitation des outils de description implicite du contenu des images, des interactions des annotateurs qui décrivent la sémantique des images et celles des utilisateurs qui utilisent la sémantique produite pour rechercher les images seraient indispensables. Dans cette thèse, nous nous sommes penchés vers les outils duWeb Sémantique, notamment les ontologies pour décrire les images de façon structurée. L'ontologie permet de représenter les objets présents dans une image ainsi que les relations entre ces objets (les scènes d'image). Autrement dit, elle permet de représenter de façon formelle les différents types d'objets et leurs relations. L'ontologie code la structure relationnelle des concepts que l'on peut utiliser pour décrire et raisonner. Cela la rend éminemment adaptée à de nombreux problèmes comme la description sémantique des images qui nécessite une connaissance préalable et une capacité descriptive et normative. La contribution de cette thèse est focalisée sur trois points essentiels : La représentationsémantique, l'annotation sémantique collaborative et la recherche sémantique des images. La représentation sémantique permet de proposer un outil capable de représenter la sémantique des images. Pour capturer la sémantique des images, nous avons proposé une ontologie d'application dérivée d' La recherche sémantique permet de rechercher les images avec les sémantiques fournies par l'annotation sémantique collaborative. Elle est basée sur deux techniques : le clustering et la recommandation. Le clustering permet de regrouper les images similaires à la requête d'utilisateur et la recommandation a pour objectif de proposer des sémantiques aux utilisateurs en se basant sur leurs profils statiques et dynamiques. Elle est composée de trois étapes à savoir : la formation de la communauté des utilisateurs, l'acquisition des profils d'utilisateurs et la classification des profils d'utilisateurs avec l'algèbre de Galois. Des expérimentations ont été menées pour valider les différentes approches proposées dans ce travail.
Les services cloud offrent des coûts réduits, une élasticité et un espace de stockage illimité qui attirent de nombreux utilisateurs. Le partage de fichiers, les plates-formes collaboratives, les plateformes de courrier électroniques, les serveurs de sauvegarde et le stockage de fichiers sont parmi les services qui font du cloud un outil essentiel pour une utilisation quotidienne. Actuellement, la plupart des systèmes d'exploitation proposent des applications de stockage externalisées intégrées, par conception, telles que One Drive et iCloud, en tant que substituts naturels succédant au stockage local. Cependant, de nombreux utilisateurs, même ceux qui sont disposés à utiliser les services susmentionnés, restent réticents à adopter pleinement le stockage et les services sous-traités dans le cloud. Les préoccupations liées à la confidentialité des données augmentent l'incertitude pour les utilisateurs qui conservent des informations sensibles. Il existe de nombreuses violations récurrentes de données à l'échelle mondiale qui ont conduit à la divulgation d'informations sensibles par les utilisateurs. Pour en citer quelques-uns : une violation de Yahoo fin 2014 et annoncé publiquement en Septembre 2016, connue comme la plus grande fuite de données de l'histoire d'Internet, a conduit à la divulgation de plus de 500 millions de comptes utilisateur ; une infraction aux assureurs-maladie, Anthem en février 2015 et Premera BlueCross BlueShield en mars 2015, qui a permis la divulgation de renseignements sur les cartes de crédit, les renseignements bancaires, les numéros de sécurité sociale, pour des millions de clients et d'utilisateurs. Une contre-mesure traditionnelle pour de telles attaques dévastatrices consiste à chiffrer les données des utilisateurs afin que même si une violation de sécurité se produit, les attaquants ne peuvent obtenir aucune information à partir des données. Malheureusement, cette solution empêche la plupart des services du cloud, et en particulier, la réalisation des recherches sur les données externalisées. Les chercheurs se sont donc intéressés à la question suivante : comment effectuer des recherches sur des données chiffrées externalisées tout en préservant une communication, un temps de calcul et un stockage acceptables ? Cette question avait plusieurs solutions, reposant principalement sur des primitives cryptographiques, offrant de nombreuses garanties de sécurité et d'efficacité. Bien que ce problème ait été explicitement identifié pendant plus d'une décennie, de nombreuses dimensions de recherche demeurent non résolues. Dans ce contexte, le but principal de cette thèse est de proposer des constructions pratiques qui sont (1) adaptées aux déploiements dans les applications réelles en vérifiant les exigences d'efficacité nécessaires, mais aussi, (2) en fournissant de bonnes assurances de sécurité. Tout au long de notre recherche, nous avons identifié le chiffrement cherchable (SSE) et la RMA inconsciente (ORAM) comme des deux potentielles et principales primitives cryptographiques candidates aux paramètres des applications réelles. Nous avons identifié plusieurs défis et enjeux inhérents à ces constructions et fourni plusieurs contributions qui améliorent significativement l'état de l'art. Premièrement, nous avons contribué à rendre les schémas SSE plus expressifs en permettant des requêtes booléennes, sémantiques et de sous-chaînes. Cependant, les praticiens doivent faire très attention à préserver l'équilibre entre la fuite d'information et le degré d'expressivité souhaité. Deuxièmement, nous améliorons la bande passante de l'ORAM en introduisant une nouvelle structure récursive de données et une nouvelle procédure d'éviction pour la classe d'ORAM ; nous introduisons également le concept de redimensionnabilibté dans l'ORAM qui est une caractéristique requise pour l'élasticité de stockage dans le cloud.
Dans cette thèse, nous nous intéressons au problème de la détection d'objets faiblement supervisée. Le but est de reconnaître et de localiser des objets dans les images, n'ayant à notre disposition durant la phase d'apprentissage que des images partiellement annotées au niveau des objets. Pour cela, nous avons proposé deux méthodes basées sur des modèles différents. Pour la première méthode, nous avons proposé une amélioration de l'approche " Deformable Part-based Models " (DPM) faiblement supervisée, en insistant sur l'importance de la position et de la taille du filtre racine initial spécifique à la classe. Tout d'abord, un ensemble de candidats est calculé, ceux-ci représentant les positions possibles de l'objet pour le filtre racine initial, en se basant sur une mesure générique d'objectness (par region proposals) pour combiner les régions les plus saillantes et potentiellement de bonne qualité. Ensuite, nous avons proposé l'apprentissage du label des classes latentes de chaque candidat comme un problème de classification binaire, en entrainant des classifieurs spécifiques pour chaque catégorie afin de prédire si les candidats sont potentiellement des objets cible ou non. De plus, nous avons amélioré la détection en incorporant l'information contextuelle à partir des scores de classification de l'image. Enfin, nous avons élaboré une procédure de post-traitement permettant d'élargir et de contracter les régions fournies par le DPM afin de les adapter efficacement à la taille de l'objet, augmentant ainsi la précision finale de la détection. Pour la seconde approche, nous avons étudié dans quelle mesure l'information tirée des objets similaires d'un point de vue visuel et sémantique pouvait être utilisée pour transformer un classifieur d'images en détecteur d'objets d'une manière semi-supervisée sur un large ensemble de données, pour lequel seul un sous-ensemble des catégories d'objets est annoté avec des boîtes englobantes nécessaires pour l'apprentissage des détecteurs. Nous avons proposé de transformer des classifieurs d'images basés sur des réseaux convolutionnels profonds (Deep CNN) en détecteurs d'objets en modélisant les différences entre les deux en considérant des catégories disposant à la fois de l'annotation au niveau de l'image globale et l'annotation au niveau des boîtes englobantes. Nos approches ont été évaluées sur plusieurs jeux de données tels que PASCAL VOC, ImageNet ILSVRC et Microsoft COCO. Ces expérimentations ont démontré que nos approches permettent d'obtenir des résultats comparables à ceux de l'état de l'art et qu'une amélioration significative a pu être obtenue par rapport à des méthodes récentes de détection d'objets faiblement supervisées.
Le domaine de l'apprentissage automatique a récemment été considérablement bouleversé par l'apprentissage profond. Les réseaux neuronaux profonds sont maintenant à la base de l'état de l'art en matière de vision par ordinateur, de reconnaissance vocale, de traitement du langage naturel et de nombreux autres domaines. Bien que très efficaces, ces modèles sont coûteux en termes de calcul et nécessitent de grandes quantités de données pour estimer avec précision leurs nombreux paramètres. Les statistiques bayésiennes offrent un cadre théoriquement bien fondé pour raisonner sur l'incertitude et constituent l'une des pierres angulaires de l'apprentissage automatique moderne. Bien qu'apparemment assez éloigné, il existe un grand potentiel de fertilisation croisée entre l'apprentissage en profondeur et les statistiques bayésiennes. Pourtant, l'interaction entre ces deux paradigmes d'apprentissage est relativement peu exploré jusqu'à présent. Le but de cette thèse est d'apporter théorie et techniques pratiques qui se situent à l'interface de ces deux domaines. Voir version anglais pour sujet de thèse en détail.
Cette thèse examine le phénomène universel de la poly-divergence dans le chinois archaïque avec cinq sous-types : La polygrammaticalisation avec le cas de huò 或 ; la polylexicalisation et l'hybridation de ces deux avec le cas de rán 然 ; la poly-divergence sous l'influence des culte-culture-philosophique avec le cas d'yī 一 ; la poly-divergence de la structure multifonctionnelle avec le cas de « Reference+Comment » ; la poly-divergence de la locution avec le cas de « chiffre+mois » . Ces divergences ont pour effet de réduire et éviter l'opacité causée par la pratique langagière de l'emploi non-obligatoire d'item fonctionnel ou le manque d'item fonctionnel exclusif. Et ces types de poly-divergence en chinois se réalisent via des mécanismes qui sont distincts de ceux des langues occidentales. Enfin, on peut en conclure que, le modèle de la poly-divergence en branches devient le modèle essentiel d'évolution grammaticale dans le chinois archaïque.
Nos travaux ont démontré les performances des systèmes d'intelligence épidémiologique en matière de détection précoce des évènements infectieux au niveau mondial, la valeur ajoutée spécifique de chaque système, la plus grande sensibilité intrinsèque des systèmes modérés et la variabilité du type de source d'information utilisé. La création d'un système virtuel combiné intégrant le meilleur résultat des sept systèmes a démontré les gains en termes de sensibilité et de réactivité, qui résulterait de l'intégration de ces systèmes individuels dans un supra-système. Ils ont illustrés les limites de ces outils et en particulier la faible valeur prédictive positive des signaux bruts détectés, la variabilité les capacités de détection pour une même pathologie, mais également l'influence significative jouée par le type de pathologie, la langue et la région de survenue sur les capacités de détection des évènements infectieux. Ils ont établis la grande diversité des stratégies d'intelligence épidémiologique mises en œuvre par les institutions de santé publique pour répondre à leurs besoins spécifiques et l'impact de ces stratégies sur la nature, l'origine géographique et le nombre des évènements rapportés. Ils ont également montré que dans des conditions proches de la routine, l'intelligence épidémiologique permettait la détection d'évènements infectieux en moyenne une à deux semaines avant leur notification officielle, permettant ainsi d'alerter les autorités sanitaires et d'anticiper la mise en œuvre d'éventuelles mesures de contrôle. Nos travaux ouvrent de nouveaux champs d'investigations dont les applications pourraient être importantes pour les utilisateurs comme pour les systèmes.