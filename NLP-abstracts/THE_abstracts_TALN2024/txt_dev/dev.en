The hypothesis on which our work is based is that the comparison of the lexicalization of body element nouns (henceforth, NEC, Fr. nom d'élément du corps) and the phraseology of the NEC between two languages will make it possible to highlight the differences of conceptualization and culture between two societies. According to this hypothesis, our thesis deals with two main themes. Firstly, we study the Korean NEC (henceforth, NECC, Fr. nom d'élément du corps coréen) focusing on the neutral nouns of human external body elements. The NECC have universal characteristics: lexical richness, elements of the basic vocabulary, sources of the embodiment, physio-conceptual universals and their nature of semantic quasi-predicates. At the same time, the NECC show language-specific semantic, syntactic and morphological characteristics. The comparison of the lexicalization of the NECC and the French NEC shows that even if the elements of the body are physio-conceptual universals, there is no univocal lexical correspondence between the two languages. Secondly, we focus our attention on the NECC's phraseology and its modeling in the Korean Lexical Network, a formal lexicographic model based on a relational conceptualization of the lexicon. We limit the NECC's phraseology to collocations the NECC control (ex. koga oddukhada, 'have a high and pretty nose'). Within the NECC's phraseology, we also take into account the phraseologisation in a word-form (ex. napjakko, 'flat nose'). We denote this morphological phraseme by the term morphologised collocation, as opposed to the lexical collocation. From the examination of lexical and morphologised collocations which NECC control, we can identify the semantic components of the definition of the NECC. After that, we propose a universal definition pattern of the NEC, which is the foundation of the explanatory model of the NEC's phraseology. This model is based on the assumption that recurrent components can be found in the definitions of NEC. Different collocations (of the type Magn, Ver, Bon, Real1, in terms of Lexical Functions of the Meaning-Text Theory) are then generated from the semantism of these components. Finally, we compare the description of the phraseology of the NECC with that of the French NEC, in order to observe the various non-correspondences between the phrasemes of the two languages. This work deepens our understanding of phraseology in general and in specific languages (Korean and French), and highlights cultural differences encoded in both languages. It can also find applications in didactics and translation.
The work presented in this thesis aim at facilitating the development of resources for natural language processing. Resources of this type take different forms, because of the existence of several levels of linguistic description (syntax, morphology, semantics,...) The formalisms featuring different types of structures, a unique description language is not enough: it is necessary to create a domain specific language (or DSL) for every formalism, and to implement a new tool which uses this language, which is a long a complex task. For this reason, we propose in this thesis a method to assemble in a modular way development frameworks specific to tasks of linguistic resource generation. The frameworks assembled thanks to our method are based on the fundamental concepts of the XMG (eXtensible MetaGrammar) approach, allowing the generation of tree based grammars. The method is based on the assembling of a description language from reusable bricks, and according to a unique specification file. The totality of the processing chain for the DSL is automatically assembled thanks to the same specification. In a first time, we validated this approach by recreating the XMG tool from elementary bricks. Some collaborations with linguists also brought us to assemble compilers allowing the description of morphology and semantics.
Air pollution has increasingly become a serious problem in China, more and more journalistic articles and miniblogs (weibo in Chinese, equivalent to tweet), comming from governmental or media websites, social networks, blogs and forums, etc., discuss the issue of "雾 霾" (wumai in Chinese, means smog) in China through several angles: political, ecological, economic, sociological, health, etc. The semantics of the themes addressed in these texts differ significantly from each other according to their textual genre. In the framework of our research, our objectif is double-fold: on the one hand, to identify different themes of a digital propose-bulit corpus relating to wumai ; and on the other hand, to interpret differentially the semantics of these themes. Firstly, we collect the textual data written in chinese and related to wumai. These journalistic articles and weibo deriving from three traditional chinese and the social network are divided into four genres of sub-corpus. Secondly, we constitute our corpus through a series of data processing: data cleaning, word segmentation, normalization, POS tagging, benchmarking and data organization. We study the characteristics of the four genres of sub-corpus through a series of discriminating variables - hyperstructural, lexical, semiotic, rhetorical, modal and syntactic - distributed at the infratextual and intratextual level. After that, based on the characteristics of each textual genre, we identify the main themes exposed in each genre of sub-corpus, and analyze the semantics of these identified themes in a contrastive way. Our analysis results are interpreted from two angles: quantitative and qualitative. All statistical analysis are assisted by textometric tools ; and the semantic interpretations are implemented on several fundamental concepts of SI (Sémantique interprétative) proposed by Rastier (1987).
In this thesis, we introduce a new interactive artifact for the SERP: the "Semantic Snippet". Semantic Snippets rely on the coexistence of the two webs to facilitate the transfer of knowledge to the user thanks to a semantic contextualization of the user's information need. It makes apparent the relationships between the information need and the most relevant entities present in the web page.
This research deals with the study of the problems relating to the use of the upper case letter from the point of view of Natural Language Processing for an automatic spelling correction. The use of the French capital letters suffers from a lack of fixed standardization which inevitably involves that they are used without methodology. This absence reveals on the one hand phenomenon called “majusculite” (abuse of the capital letters) and “minusculite” (abuse of small letters) and on the other hand the presence of spelling variants (la Montagne noire, la montagne Noire, la Montagne Noire, la montagne noire). The current spelling checkers seem unable to say which the good form is. The true direction of upper case letters tends to disappear and their relevance becoming less obvious. Such an amount of doubts, hesitations and fluctuations in the rules of employment, so many differences between the different authors return any attempt of automatic processing very difficult. This wobbly normality more particularly touches the proper nouns known as complex or “dénominations”. The most logical solution so that cease the drift, is to standardize the use of the capital letters. Basing us on various reference works, we worked out clear and logical rules governing the use of the capital letter in order to create a theoretical model of an automatic system checking capital letters. Thus, this solution sees the disappearance of the spelling variants whose existence also constitutes a major problem in research in extraction of fixed forms.
Following recent developments in artificial intelligence and new advances in natural language processing, the number of available chatbots has exploded. Existing systems focus mainly on the functional aspect of chatbots: extraction of keywords, natural language understanding, generation of text in line with the question asked, etc. Although these different aspects are essential for the development of chatbots, the question of social and emotional intelligence is still poorly addressed. Social intelligence is a human quality recognized as one of the most important assets for personal and professional success. Thus, the incorporation of a social and emotional intelligence in the automatic generation of dialogue becomes indispensable for the next generation of chatbot. From a social point of view, this improvement would allow the chatbot to adapt its language with respect to the user, thus enabling it to create an emotional relationship that helps to understand the user behavior in long-term interactions. From a functional point of view, the development of social strategies would make it possible to avoid certain relationship problems related to problems of interaction (annoyance, disinterest, etc.) that occur for example when the chatbot does not understand the requests of the user [17]. These interaction problems can then lead to a discontinuation of the conversation (also called breach of commitment [3]) by the user thus hindering the finalization of the task targeted by the chatbot. From an operational point of view, the lack of emotional intelligence makes robots unusable in many areas. The aim of this thesis is to focus on the social and emotional intelligence of chatbots. Machine learning methods using approaches such as deep learning, and reinforcement learning for the development of socially competent chatbot (selection and generation of utterances in natural language) will be explored.
The notion of data veracity is increasingly getting attention due to the problem of misinformation and fake news. With more and more published online information it is becoming essential to develop models that automatically evaluate information veracity. Moreover, the amount of information that is available nowadays makes this task time-consuming. In this thesis we focus on Truth Discovery models. These approaches address the data veracity problem when conflicting values about the same properties of real-world entities are provided by multiple sources. More precisely, they are unsupervised models that are based on the rationale stating that true information is provided by reliable sources and reliable sources provide true information. The main contribution of this thesis consists in improving Truth Discovery models considering a priori knowledge expressed in ontologies. First of all, we explore the semantic dependencies that may exist among different values, i.e. the ordering of values through certain conceptual relationships. Indeed, two different values are not necessary conflicting. They may represent the same concept, but with different levels of detail. In order to integrate this kind of knowledge into existing approaches, we use the mathematical models of partial order. Then, we consider recurrent patterns that can be derived from ontologies.
This thesis falls in the field of Natural Language Processing. Its main aims is to ensure the exactness of the syntactic and semantic analysis of texts in natural language (technical specifications or texts in under-ressourced languages) and the fidelity of their translation into several other languages. To achieve this, several obstacles must be overcome. First, to master the combinatorics inherent to the complexity of natural languages, the development of analyzers, generators... must be done automatically from a clear and verifiable description. For this purpose, we will improve the Structural Correspondence Static Grammars (GSCS) to make them both more intuitive and able to produce executable modules. Finally to guarantee the meaning, we will produce all possible analyzes and use a dialogue with a human to disambiguate between these different solutions. In addition, to accelerate the development of GSCS grammars, we will apply deep learning-based processing on existing tree banks. The probabilities obtained on the rules will make it possible to organize the dialogue of disambiguation in a natural way. Algorithms capable of measuring the quality of analyzes and translations will be studied and evaluated. Finally, to take into account the highly multilingual character of CS's project, we will choose a semantic pivot architecture. The analyzers produced will thus produce UNL graphs, and the generators will start from these graphs to generate the texts into the target languages. Experiments with other UNL enconverters and deconverters (English, Russian, Hindi, Spanish...) will verify UNL's interoperability capability.
This thesis deals with the formal description and development of an electronic grammar of Arabic language. This work is a prerequisite for the creation of automatic Arabic processing tools. This language presents many challenges for automatic processing. Indeed the order of words in Arabic is relatively free,the morphology is rich and the diacritics are omitted in written texts. Although several research studies have addressed some of these issues, electronic resources useful for the processing of Arabic remain relatively rare or not widely available. In this thesis work, we are interested in the representation of syntax (word order) and the meaning of modern standard Arabic. As a formal system of language representation, we chose the formalism of Tree Adjoining Grammar. The linguistic expert canthus describe the syntax and semantics of the language with abstraction tools facilitating the maintenance and extension of the grammar. The new described grammar has 1074 syntactical rules (not lexicalized) and27 semantic frameworks (predicative relations). This resource was evaluated by analyzing a corpus from excerpts of an Arabic textbook.
This thesis revisits the Constraint games framework by rethinking their solving technique in terms of constraint propagation. Players preferences are considered as global constraints making transparently the integration in constraints solvers. We release our new solver ConGA in open source. Our new complete solver is faster than previous state-of-the-art and is able to find all pure Nash equilibrium for some problems with 200 players or even with 2000 players in graphical games. This problem is solved with a centralized perspective and a decentralized one. The comparison of the two last approaches is really important to evaluate the quality of service in multi-users application, but computationally consuming.
It is the problem of determining which *sense* (meaning) is given to a word according to the context. A famous example is to determine the sense of pen in the following passage (Bar-Hillel 1960): *Little John was looking for his toy box. Finally, he found it. The box was in the pen. John was very happy*. The word pen has multiple meanings according to WordNet list (a writing implement with a point from which ink flows, an enclosure for confining livestock, a portable enclosure in which babies may be left to play, a correctional institution for those convicted of major crimes, female swan).
Machine learning, which is considered as an integral part of artificial intelligence continues to grow with time, and opens unsuspicious horizons. More and more complex structures tend to be studied by this way, raising the available information to the level of exploitable knowledge. This doctoral work proposes to valorize a particular type of data that are the 3D objects (structures) constructed from mesh, by empirically justifying the undeniable contributions of an extraction of sub-parts coming from these last one. This objective is achieved by solving a forecast problem by a new supervised classification approach for information recommendation. Beyond the expected result, a justification is also provided in the form of the visualization of sub-parts extracted discriminant, thus allowing interpretation by the specialist. In the Total Exploration service, this classification need is initially applied to large 3D structures such as geo-models of geological basins, whose relevant elements belong to sub-parts. During the study of a subsoil, geologists try to understand the subsoil by using 3D data reconstructed through acoustic waves. This understanding can be helped by providing a way to detect some types of shapes within these structures. We propose, in order to answer this problem, a classification system of these 3D structures. Thanks to an adaptation of Time series Shapelets and features selection methods, it is possible to only select the most relevant parts for the targeted classification. To summarize, the main idea is to randomly extract a certain number of sub-surfaces from each 3D object of the learning set, then to study its relevance depending on the expected classification, before using the most relevant one for a more traditional learning based on the degree of belonging of the extract in each object. In industrial companies, the lack of justification of results tends to assimilate machine learning techniques to a black box. The proposed method, however, corrects this problem, and allows the understanding of the result of the decision support provided by the classification built. Indeed, in addition to presenting slightly better forecast results than those provided by the state of the art, it offers a visualization of the sub-parts of the most discriminating 3D objects within the framework of the implemented classification model, and therefore the areas that will have mostly allowed to classify the data. Subsequently, we propose an improvement of this method by two main paths: the first one is the contribution of an adaptation of the transfer of knowledge (or transfer learning applied to the previously proposed algorithm; the second one is an innovative method of attribute selection, based on tools derived from fuzzy subset theory, which proves to be potentially applicable to any type of attribute selection challenge in supervised classification. These multiple results confirm the general potential of random selection of candidate attributes, especially in the context of large amounts of data.
At a time when the use of data has reached an unprecedented level, machine learning, and more specifically deep learning based on artificial neural networks, has been responsible for very important practical advances. Their use is now ubiquitous in many fields of application, from image classification, text mining to speech recognition, including time series prediction and text analysis. However, the understanding of many algorithms used in practice is mainly empirical and their behavior remains difficult to analyze. These theoretical gaps raise many questions about their effectiveness and potential risks. Establishing theoretical foundations on which to base numerical observations has become one of the fundamental challenges of the scientific community. The main difficulty that arises in the analysis of most machine learning algorithms is to handle, analytically and numerically, a large number of interacting random variables. In this manuscript, we revisit an approach based on the tools of statistical physics of disordered systems. Developed through a rich literature, they have been precisely designed to infer the macroscopic behavior of a large number of particles from their microscopic interactions. At the heart of this work, we strongly capitalize on the deep connection between the replica method and message passing algorithms in order to shed light on the phase diagrams of various theoretical models, with an emphasis on the potential differences between statistical and algorithmic thresholds. We essentially focus on synthetic tasks and data generated in the teacher-student paradigm. In particular, we apply these mean-field methods to the Bayes-optimal analysis of committee machines, to the worst-case analysis of Rademacher generalization bounds for perceptrons, and to empirical risk minimization in the context of generalized linear models. Finally, we develop a framework to analyze estimation models with structured prior informations, produced for instance by deep neural networks based generative models with random weights.
The adoption of new Information and Communication Technologies (ICT) has enabled the modernization of teaching methods in online learning systems such as e-Learning, intelligent tutorial systems (ITS), etc. These systems provide a remote training that which meets the learner needs. A very important aspect to consider in these systems is the early assessment of the learner in terms of knowledge acquisition. In general, three types of assessment and their relationships are needed during the learning process, namely: (i) diagnostic which is performed before learning to estimate the level of students, (ii) formative evaluation which is applied during learning to test the knowledge evolution and (iii) summative evaluation which is considered after learning to evaluate learner's knowledge acquisition. These methods can be integrated into a semi-automatic, automatic or adapted way in different contexts of formation, for example in the field of languages literary learning such as French, English, etc., hard sciences (mathematics, physics, chemistry, etc.) and programming languages (java, python, sql, etc.). However, the usual evaluation methods are static and are based on linear functions that only take into account the learner's response. They ignore other parameters of their knowledge model that may disclose other performance indicators. For example, the time to solve a problem, the number of attempts, the quality of the response, etc. These elements are used to detect the profile characteristics, behavior and learning disabilitiesof the learner. These additional parameters are seen in our research as learning traces produced by the learner during a given situation or pedagogical context. In this context, we propose in this thesis a learner evaluation approach based on learning traces that can be exploited in an adaptation system of the resource and/or the pedagogic situation. For the learner assessment, we have proposed three generic evaluation models that take into consideration the temporal trace, number of attempts and their combinations. These models are later used as a base metric for our resource adaptation model and/or learning situation. The adaptation model is also based on the three traces mentioned above and on our evaluation models. Our adaptation model automatically generates adapted paths using a state-transition model. The states represent learning situations that consume resources and the transitions between situations express the necessary conditions to pass from one situation to another. These concepts are implemented in a domain ontology and an algorithm that we have developed. The algorithm ensures two types of adaptation: (i) Adaptation of the situation and (ii) Adaptation of resources within a situation. In order to collect traces of training for the implementation of our approaches of learner evaluation and adaptation of resources and learning situations, we conducted experiments on two groups of students in Computer Science (L2). One group in classical training and the other group in adapted training.
The massive amount of documents through the Internet (e.g. web pages, data warehouses and digital or transcribed texts) makes easier the recycling of ideas. Unfortunately, this phenomenon is accompanied by an increase of plagiarism cases. Indeed, claim ownership of content, without the consent of its author and without crediting its source, and present it as new and original, is considered as plagiarism. In addition, the expansion of the Internet, which facilitates access to documents throughout the world (written in foreign languages) as well as increasingly efficient(and freely available) machine translation tools, contribute to spread a new kind of plagiarism: cross-language plagiarism. Cross-language plagiarism means plagiarism by translation, i.e. a text has been plagiarized while being translated (manually or automatically) from its original language into the language of the document in which the plagiarist wishes to include it. After defining the plagiarism and the different concepts discussed during this thesis, we present a state-of-the-art of the different cross-language plagiarism detection approaches. We also present the preexisting corpora for cross-language plagiarism detection and show their limits. Then we describe how we have gathered and built a new dataset, which does not contain most of the limits encountered by the preexisting corpora. Using this new dataset, we conduct arigorous evaluation of several state-of-the-art methods and discover that they behave differently according to certain characteristics of the texts on which they operate. We next present new methods for measuring cross-lingual semantic textual similarities based on word embeddings. We also propose a notion of morphosyntactic and frequency weighting of words, which can be used both within a vector and within a bag-of-words, and we show that its introduction in the new methods increases their respective performance. Our experiments show that we obtain better results than the state-of-the-art in all the sub-corpora studied. We conclude by presenting and discussing the results of these methods obtained during our participation to the cross-lingual Semantic Textual Similarity (STS) task of SemEval-2017, where we ranked 1st on the sub-task that best corresponds to Compilatio's use-case scenario.
Applications in Music Information Retrieval and Computational Musicology have traditionally relied on features extracted from the music content in the form of audio, but mostly ignored the song lyrics. More recently, improvements in fields such as music recommendation have been made by taking into account external metadata related to the song. In this thesis, we argue that extracting knowledge from the song lyrics is the next step to improve the user's experience when interacting with music. To extract knowledge from vast amounts of song lyrics, we show for different textual aspects (their structure, content and perception) how Natural Language Processing methods can be adapted and successfully applied to lyrics. For the structural aspect of lyrics, we derive a structural description of it by introducing a model that efficiently segments the lyrics into its characteristic parts (e.g. intro, verse, chorus). In a second stage, we represent the content of lyrics by means of summarizing the lyrics in a way that respects the characteristic lyrics structure. Finally, on the perception of lyrics we investigate the problem of detecting explicit content in a song text. This task proves to be very hard and we show that the difficulty partially arises from the subjective nature of perceiving lyrics in one way or another depending on the context. Furthermore, we touch on another problem of lyrics perception by presenting our preliminary results on Emotion Recognition. As a result, during the course of this thesis we have created the annotated WASABI Song Corpus, a dataset of two million songs with NLP lyrics annotations on various levels.
Companies are competing to put their products on the market. In this race, knowledge of the quality characteristics that end users require for the product is sometimes presupposed or misunderstood. The result is often a product that does not achieve the purpose for which it was designed and manufactured. In this context, is it possible to guide the development process methodologically in order to ensure the quality of a product? With reference to Systems Engineering, it is at the stage of Concept in the life cycle of the system that the needs of stakeholders are collected, translated first into stakeholder requirements and then into system requirements. This thesis therefore addresses these steps as a priority. It proposes a methodology to ensure that stakeholder needs are well understood and properly translated into system requirements. The proposal complies with the ISO 15288 (2015) quality standard and incorporates the Lean principles. The thesis also proposes a tool that supports the methodology. The results obtained from several case studies developed at the Tecnológico Nacional de México, Instituto Tecnológico de Toluca (ITTol), Mexico, demonstrate the effectiveness of the proposed methodology. Its use increases the likelihood that the delivered product will meet stakeholder expectations, reduces requirement changes due to misidentification of needs and, therefore, the costs incurred by these changes, and ensures faster delivery of the product to the market.
Interestingly this step encompasses the bridge between databases and ML. In this setting, we raise and address three main problems related to data selection for building predictive models. First, the database usually contains more than the data of interest: how to separate the data that the analyst wants from the one she does not want? We propose to see this problem as imbalanced classification between the tuples of interest and the rest of the database. We develop an undersampling method based on the functional dependencies of the database. Second, we discuss the writing of the query returning the tuples of interest. We propose a SQL query completion solution based on data semantics, that starts from a very general query, and helps an analyst to refine it until she selects her data of interest. This process aims at helping the analyst to design the query that will eventually select the data she requires. Third, assuming the data has successfully been extracted from the database, the next natural question follows: is the selected data suited to answer the considered ML problem? Since getting a predictive model from the features to the class to predict amounts to providing a function, we point out that it makes sense to first assess the existence of that function in the data. This existence can be studied through the prism of functional dependencies, and we show how they can be used to understand a model's limitation, and to refine the initial data selection if necessary.
In this study, we show that enunciative selon X are a sub-class of mediative (evidential) selon, and compare them not only with other uses of selon, but also with other adverbials such as d'après X, pour X and with verbal constructions of reported speech. We then describe the situations in which enunciative selon X integrate several sentences in their frame, and we review the clues most commonly involved in the closure of these frames. The study is based on the systematic analysis of an annotated corpus of excerpts from Le Monde diplomatique and quantitative indications on the different usages of selon X are taken into account.
This thesis project aims to study the teaching method of FOS (French on Specific Objectives) catering to foreign cooks who come to work in French restaurants or who have chosen catering as a specialty. The objective of our research is therefore to teach the culinary NAdj phrasemas to foreign A2 level learners. The teaching/learning of phraseology is required in specialty languages and the high frequency of NAdj phrasems has caught our attention. Several questions are then addressed: where to find this specific lexicon? How to extract them? By which approach do we teach the selected phrasems? Finally, we have proposed the three approaches to the use of corpora for the teaching/learning of NAdj phrasems: guided inductive approach, deductive approach, pure inductive approach.
This thesis focuses on automatic and semantic transformation of business rules into formal rules. These business rules are originally drafted in the form of natural language text, tables and images. Our goal is to provide to business experts a set of services allowing them to develop corpora of formal business rules. We carry out this work in the field of building engineering construction. Having formal and executable versions of the business rules enables to perform automatic compliance checking of digital mock-ups of construction projects under design. For this we made available to business experts, the two main contributions of this thesis. The first is the development of a controlled natural language, called RAINS. It allows business experts to rewrite business rules in the form of formal rules. A RAINS rule consists of terms of the business vocabulary and reserved words such as comparison predicates, negation and universal quantification markers and literals. Each RAINS rule has a unique formal semantics which is based on the standards of the Semantic Web. The second major contribution is a service for formalization of business rules. This service implements a formalized approach proposed in this thesis and called FORSA. FORSA uses natural language processing tools and heuristics. To evaluate FORSA, we have set up a benchmark adapted to the formalization of business rules task. The dataset from this benchmark are from norms in the field of Construction
The explosion of widely available relational data in the form of knowledge graphs enabled many applications, including automated personal agents, recommender systems and enhanced web search results. The very large size and notorious incompleteness of these data bases calls for automatic knowledge graph completion methods to make these applications viable. State-of-the-art factorization models propose different trade-offs between modeling expressiveness, and time and space complexity. We introduce a new model, ComplEx---for Complex Embeddings---to reconcile both expressiveness and complexity through the use of complex-valued factorization, and explore its link with unitary diagonalization. We corroborate our approach theoretically and show that all possible knowledge graphs can be exactly decomposed by the proposed model. Our approach based on complex embeddings is arguably simple,as it only involves a complex-valued trilinear product,whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed ComplEx model is scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link-prediction benchmarks. We also demonstrate its ability to learn useful vectorial representations for other tasks,by enhancing word embeddings that improve performances on the natural language problem of entailment recognition between pair of sentences. In the last part of this thesis, we explore factorization models ability to learn relational patterns from observed data. By their vectorial nature, it is not only hard to interpret why this class of models works so well,but also to understand where they fail and how they might be improved. We conduct an experimental survey of state-of-the-art models, not towards a purely comparative end, but as a means to get insight about their inductive abilities. To assess the strengths and weaknesses of each model, we create simple tasks that exhibit first, atomic properties of knowledge graph relations,and then, common inter-relational inference through synthetic genealogies. Based on these experimental results, we propose new research directions to improve on existing models, including ComplEx.
Different disciplines in the humanities, such as philology or palaeography, face complex and time-consuming tasks whenever it comes to examining the data sources. The introduction of computational approaches in humanities makes it possible to address issues such as semantic analysis and systematic archiving. The conceptual models developed are based on algorithms that are later hard coded in order to automate these tedious tasks. We propose a novel representation learning method based on stacking convolutional auto-encoder. The goal is to automatically learn plot representations of the script or the written language.
In recent years, significant advances made in deep neural networks enabled the creation of groundbreaking technologies such as self-driving cars and voice-enabled personal assistants. Almost all successes of deep neural networks are about prediction, whereas the initial breakthroughs came from generative models. This thesis defends the point of view that, instead of trying to eliminate these novelties, we should study them and the generative potential of deep nets to create useful novelty, especially given the economic and societal importance of creating new objects in contemporary societies. The thesis sets out to study novelty generation in relationship with data-driven knowledge models produced by deep generative neural networks. Our first key contribution is the clarification of the importance of representations and their impact on the kind of novelties that can be generated: a key consequence is that a creative agent might need to rerepresent known objects to access various kinds of novelty. We then demonstrate that traditional objective functions of statistical learning theory, such as maximum likelihood, are not necessarily the best theoretical framework for studying novelty generation. We propose several other alternatives at the conceptual level. A second key result is the confirmation that current models, with traditional objective functions, can indeed generate unknown objects. This also shows that even though objectives like maximum likelihood are designed to eliminate novelty, practical implementations do generate novelty. Through a series of experiments, we study the behavior of these models and the novelty they generate. In particular, we propose a new task setup and metrics for selecting good generative models. Finally, the thesis concludes with a series of experiments clarifying the characteristics of models that can exhibit novelty. Experiments show that sparsity, noise level, and restricting the capacity of the net eliminates novelty and that models that are better at recognizing novelty are also good at generating novelty.
In this thesis, we propose to integrate natural language processing (NLP) techniques in image indexing systems. We also address the issue of describing the semantic content of images: we propose an image annotation scheme that relies on extracting relevant named entities from texts coming with the images to annotate.
The general framework of this thesis is semantic indexing and information retrieval, applied to multimedia documents. More specifically, we are interested in the semantic indexing of concepts in images and videos by the active learning approaches that we use to build annotated corpus. Throughout this thesis, we have shown that the main difficulties of this task are often related, in general, to the semantic-gap. Furthermore, they are related to the class-imbalance problem in large scale datasets, where concepts are mostly sparse. For corpus annotation, the main objective of using active learning is to increase the system performance by using as few labeled samples as possible, thereby minimizing the cost of labeling data (e.g. money and time). In this thesis, we have contributed in several levels of multimedia indexing and proposed three approaches that outperform state-of-the-art systems: i) the multi-learner approach (ML) that overcomes the class-imbalance problem in large-scale datasets, ii) a re-ranking method that improves the video indexing, iii) we have evaluated the power-law normalization and the PCA and showed its effectiveness in multimedia indexing. Furthermore, we have proposed the ALML approach that combines the multi-learner with active learning, and also proposed an incremental method that speeds up ALML approach. Moreover, we have proposed the active cleaning approach, which tackles the quality of annotations. The proposed methods were validated through several experiments, which were conducted and evaluated on large-scale collections of the well-known international benchmark, called TrecVid. Finally, we have presented our real-world annotation system based on active learning, which was used to lead the annotations of the development set of TrecVid 2011 campaign, and we have presented our participation at the semantic indexing task of the mentioned campaign, in which we were ranked at the 3rd place out of 19 participants.
At the beginning of the first chapter the interdisciplinary setting is introduced. Then, the notion of corpus is put into focus. Several milestones of corpus design are presented, from pre-digital corpora at the end of the 1950s to web corpora in the 2000s and 2010s. The continuities and changes between the linguistic tradition and web native corpora are exposed. In the second chapter, methodological insights on automated text scrutiny are presented. The state of the art on text quality assessment are described. Text visualization exemplifies corpus processing in the digital humanities framework. Reasons are given to find a balance between quantitative analysis and corpus linguistics. Third, current research on web corpora is summarized. The notion of web corpus preprocessing is introduced and salient steps are discussed. The impact of the preprocessing phase on research results is assessed. I present my work on web corpus construction in the fourth chapter. My analyses concern two main aspects, first the question of corpus sources (or prequalification), and secondly the problem of including valid, desirable documents in a corpus (or document qualification). Last, I present work on corpus visualization consisting of extracting certain corpus characteristics in order to give indications on corpus contents and quality.
Visual impairment and blindness are sources of mobility difficulties for the affected people. In orderto lighten the burden of these difficulties, many mobility aids have been imagined, designed, tested, and more or less adopted. Designers of such assistive systems soon run into the complexity of the issue, which stands at the intersection of three domains that are, by themselves, complex: visual impairment, mobility, and perception. Taken individually, each of these approaches quickly shows its limits, but, together, they draw an interesting portrait of the existing devices. Besides these rather classical approaches, we propose a new model for analyzing assistive systems, which relies on the way these systems take place in a person's perception / mobility process. This model has the advantages of being, a priori, relevant forall assistive systems – in spite of their dissimilarity – and meaningful for both evaluation and classification. The issues of energy consumption and geographical universality, essential for any mobile device, are made explicit and studied. A novelty of the 2SEES resides in its account of reliability issues ; it is thus designed around the complex equilibrium between energy consumption, universality, and reliability. Two aspects of this equilibrium have been further studied. First, the need for robustness has been highlighted by a study of affinities between obstacle sensors and several types of potential obstacle materials. Secondly, we have tried to develop an energy-efficient indoor localization function that islittle dependent on infrastructures, and therefore easily scalable. This function works with embedded sensors (wheel encoder, inertial measurement unit) and a simplified particle filter, which estimates the position by checking the coherence of trajectories derived from sensor data against themap of the location. In addition to this work on the balance between robustness, energy consumption, and universality, we have developed a novel function, named SO2SEES, which allows communication between usersof the 2SEES and smart objects. This function enables users to ask, in natural language, questions tothe 2SEES, which are answered using information coming from surrounding smart objects. In orderto keep the system simple, users do not formulate their own questions, but are instead invited to pick them in a set of predefined questions, which are proposed by the system according to the nearby objects and the information they offer.
Our research is in the field of fuzzy linguistic summaries (FLS) that allow to generate natural language sentences to describe very large amounts of numerical data, providing concise and intelligible views of these data. We first focus on the interpretability of FLS, crucial to provide end-users with an easily understandable text, but hard to achieve due to its linguistic form. In order to guarantee it in the framework of standard fuzzy logic, we introduce a new model of oppositions between increasingly complex sentences. The model allows us to show that these consistency properties can be satisfied by selecting a specific negation approach. Moreover, based on this model, we design a 4-dimensional cube displaying all the possible oppositions between sentences in a FLS and show that it generalises several existing logical opposition structures. We then consider the case of data in the form of numerical series and focus on linguistic summaries about their periodicity: the sentences we propose indicate the extent to which the series are periodic and offer an appropriate linguistic expression of their periods. The proposed extraction method, called DPE, standing for Detection of Periodic Events, splits the data in an adaptive manner and without any prior information, using tools from mathematical morphology. Lastly, DPE returns descriptive sentences of the form ``Approximately every 2 hours, the customer arrival is important''. Experiments with artificial and real data show the relevance of the proposed DPE method. From an algorithmic point of view, we propose an incremental and efficient implementation of DPE, based on established update formulas. This implementation makes DPE scalable and allows it to process real-time streams of data. We also present an extension of DPE based on the local periodicity concept, allowing the identification of local periodic subsequences in a numerical series, using an original statistical test. The method validated on artificial and real data returns natural language sentences that extract information of the form ``Every two weeks during the first semester of the year, sales are high''.
This thesis lies at the frontier of the fields of linguistic research and the automatic processing of language. These two fields intersect for the construction of natural language processing tools, and industrial applications integrating solutions for disambiguation and interpretation of texts. A challenging task, briefly approached and applied, has come to the work of the Techlimed company, that of the automatic analysis of texts written in Arabic. Novel resources have emerged as language lexicons and semantic networks allowing the creation of formal grammars to accomplish this task. An important meta-data for text analysis is "what is being said, and what does it mean". In all cases, our research was tested and validated in a rigorous experimental framework around several formalisms and computer tools. The implementation of our study required the construction of tools for word processing, information retrieval tools. These tools were built by us and are available in Open-source. The success of the application of our work in large scale was concluded by the requirement of having rich and comprehensive semantic resources. Our work has been redirected towards a process of production of such resources, in terms of informationretrieval and knowledge extraction. The tests for this new perspective were favorable to further research and experimentation.
This thesis focuses on the development of a tool for the automatic processing of Modern Standard Arabic, at the morphological and semantic levels, with the final objective of Information Extraction on technological innovations. As far as the morphological analysis is concerned, our tool includes several successive processing stages that allow to label and disambiguate occurrences in texts: a morphological layer (Gibran 1.0), which relies on Arabic pattern as distinctive features; a contextual layer (Gibran 2.0), which uses contextual rules; and a third layer (Gibran 3.0), which uses a machine learning model. Our methodology is evaluated using the annotated corpus Arabic-PADT UD treebank. The evaluations obtain an F-measure of 0.92 and 0.90 for the morphological analyses. These experiments demontrate the possibility of improving such a corpus through linguistic analyses. This approach allowed us to develop a prototype of information extraction on technological innovations for the Arabic language. It is based on the morphological analysis and syntaxico-semantic patterns. This thesis is part of a PhD-entrepreneur course.
The DAP allows an automatic transcription of a speech signal into phonemes, units smaller than words that can potentially maintain the intelligibility of speech. First, we propose a DAP system guided by syllabic information and we highlight the value of integrating this information into the decoding. The second part of our study is built around a keyword spotting engine based on the flow from our phonetic decoding. We suggest a simple and fast method, which is robust to false alarms and avoids the classical computation of the criterion of maximum likelihood. For this, we introduce an appropriate management for errors of phonetization: coarticulation phenomena and false alarms. In particular, we propose techniques for phonetic expansion using phonetic grammars. The system is evaluated throughout the study by the detection of country names in the test corpus ESTER. We present to complete the system currently implemented and integrated in the platform demonstration of Orange Labs dedicated to searching and browsing content.
Or can their use and variation best be explained in terms of competing constructions that can be placed alongside an axis of grammaticalization? The possible applications for the project are mainly theoretical (within Cognitive and Construction Grammar), but could also show to have implications in didactics (the acquisition of these constructions for L2 learners) and computational linguistics (resolving ambiguities between tense, aspect and modality).
My dissertation focuses on sequences "WH-Verb_Construction1 that Verb_Construction 2" (see caption) in which it is traditionally admitted that dependency between the WH- and the embedded VC2 is done at long-distance. This construction has been the subject of numerous studies and all of them conclude that constraints regulate its employment. But these studies are based on either the authors' or speakers' intuition, or on other languages than French. My study is based on 229 utterances extracted from 3M words of spoken French and 9M words of written French. I aim first to confirm the assumptions made in these works (corpus-based). Some are correct: WH- is always a V2 subject, complement or circonstant, V1 is a frequent verb, which is semantically simple and has a modal value. However, there is no prototypique formula and V1 don't exclude factive verbs. In a second step, I examine the syntactic, lexical and pragmatic characteristics of VC1 (corpus-driven). VC1 occurs as "pronominal subject + verb", which could support the hypothesis of performance constraints. But the constancy of VC1 form, its modal meaning and its relationship with the VC2 (hardly pronominalisable) lead me to consider that the whole "VC1 that VC2" is not an embedded structure but a "complex verb", which is a string of verbs similar to sequences in "VC1 infinitive_VC2". The bubble representation proposed by Kahane (2000) clearly reflects that WH- is a local dependent of the complex verb.
The work of this thesis presents the development of algorithms for document classification on the one hand, or complex network analysis on the other hand, based on pretopology, a theory that models the concept of proximity. The first work develops a framework for document clustering by combining Topic Modeling and Pretopology. Our contribution proposes using topic distributions extracted from topic modeling treatment as input for classification methods. In this approach, we investigated two aspects: determine an appropriate distance between documents by studying the relevance of Probabilistic-Based and Vector-Based Measurements and effect groupings according to several criteria using a pseudo-distance defined from pretopology. The second work introduces a general framework for modeling Complex Networks by developing a reformulation of stochastic pretopology and proposes Pretopology Cascade Model as a general model for information diffusion.
Through perception and production tasks, humans are able to manipulate not only high-level units like words or sentences but also low-level units like syllables and phonemes. Studies in phonetics mainly focus on the second type of units. One of the main goal in this field is to understand how humans acquire and manipulate these units and how they are stored in the brain. In this PhD thesis, we address this set of issues by using computer modeling, performing computer simulations with a Bayesian model of communication, named COSMO (“Communicating Objects using Sensory-Motor Operations”). Our studies extend in three ways. In a first part, we investigate the cognitive content of phonetic units. We conclude that motor learning seems rather driven by a linguistic/communication goal than motivated by the reproduction of the stimulus acoustic properties. In a third part, we investigate the nature of phonetic units. In phonetics, there is a debate about the specific status of the syllable vs phoneme in speech communication.
The goal of the thesis is to develop machine learning approaches to credit risk prediction that make use of new sources of data, as well as define a Point-In-Time approach for a better performing estimation of the risks. Those new data sources include for example Bloomberg or Twitter messages, whose predictive power will be explored during this project. Some statistics on the volume of Bloomberg's news about a particular company already exist but they do not analyse the semantic meaning of the message (positive or negative for example). Consequently the methods developed will make use of the specific form of the data, i.e. its sequential and multitask nature as well as its high dimension.
This thesis, "Towards a Micro-Systemic Parsing for a Thai-French Machine Translation: Application to the Serial Verbs", is divided into 6 chapters: Chapter one presents the linguistic and data-processing approaches used in the field of computational linguistics. Chapter two explains the characteristics of the Thai language compared to the French language, the general problems of Thai-French translation, as well as the parsing models of noun phrases in Thai. Chapter three is concerned with trying to parse adjectival and adverbial syntagms of Thai. The hypothesis there presented is the result of successive observations on the general problems of our mother tongue, the Thai language, in particular with regard to natural language processing. This has enabled us to observe that Thai serial verbs play a particular role not only in lexical formation, but also in the syntactic order of the sentence. It is not necessary to say how much the interpretation of the meaning would be obstructed if these verbs were badly analyzed. Quantitatively, Thai serial verbs are not numerous. However, in their pre or post verbal and nominal employment, even at the level of the sentence, the research outcome shows that they play a particular role which deserves to be studied. Chapter five applies the results of chapters 3 and 4 to the implementation of a Thai-French machine translation system in "interactive mode"; we believe that such analysis models for machine translation can be better developed in interactive mode because the problems, which concern the difference of the two distant languages as well as in the lexical formation in syntax, are thereby highlighted. In conclusion, we wish to underline that a Thai-French machine translation system could have many applications in particular in the area of Teaching of French as a Foreign Language for the Thai public or Teaching of Thai as a Foreign Language for French speaking countries
This study is part of a larger research project with educational purpose on student writing. One of its goals is to develop automatic tools for the detection and the resolution of errors, among them errors in Ellipsis constructions.
In order to survive to the unstable and highly changing market-place, modern organisations need to adapt their business processes to be more agile. Such is, particularly, the case of problem solving processes. Problem solving is a key activity that companies perform on a daily basis to improve quality and to obtain sustainable and continuous improvement. Such processes are built following standard rigid frameworks as Plan, Do, Check, Act (PDCA), Define, Measure, Analyse, Improve, Control (DMAIC), or 8 Disciplines (8D)/ 9 Steps (9S). In these methods, the generalization and reuse of knowledge is facilitated by standardization. However, it is sometimes difficult to react to unexpected events due to over-constrained standards. Then, a need arises to define a problem solving process sufficiently structured but not over constrained by standards, which can be reconfigured and adapted to unexpected situations, and that is based on experience feedback principles. This thesis work describes a proposition of an agile problem solving process driven by the reuse of experiences and knowledge. For this purpose, based on Case-Based Reasoning (CBR) principles, the complete lifecycle of an agile problem solving process is proposed. Following the five steps that compose the agile lifecycle, the agile process can be defined, executed and stored in a dedicated knowledge and experience base. An application of the model to a specific problem solving process of a surface treatment company is presented. The process is analysed, deploying the complete agile lifecycle. It is shown how the standard problem solving method used within the company could become more agile through the application of our method.
This thesis studies the different strategies that have allowed listed non-financial corporations to remain profitable while investing less and increasingly distributing funds to shareholders under financialisation. This feeble link between profitability and investment is usually denominated as the profit-investment puzzle. Part 1 of this thesis locates historically and theoretically this puzzle. Whereas the financialisation literature has generally been limited to show the negative effects of the distribution of funds to share holders for capital expenditures, we show that the coexistence of high levels of profits (and payouts) with low levels of investment was possible due to the simultaneous engagement of these non-financial corporations in other activities. The solution to the puzzle in this case implies a shift in the activities of NFCs to financial accumulation and profits. However, throughout this part we provide substantive evidence that rejects this alternative. Part 3 of the thesis moves away from financial accumulation and directs towards the realm of the productive sphere by focusing on production offshoring and intangible accumulation. This part, contrary to the previous one, provides strong and promising results in the explanation of the puzzle.
In recent years, a large number of technologies have been created to help people who have difficulty when reading written texts. The proposed systems integrate speech technologies (reading aloud) or visual aids (setting and/or coloring of fonts or increasing the space between letters and lines). However, it is essential to also propose transformations on the texts' content in order to have simpler and more frequent substitutes. The purpose of this thesis is to contribute to develop a reading aid system that automatically provides a simplified version of a given text while keeping the same meaning of words. The presented work addresses the problem of semantic ambiguity (quite common in natural language processing) and aims to propose solutions for Word Sense Disambiguation (WSD) by using unsupervised and knowledge-based approaches from lexico-semantic resources. Thereafter, we compare various algorithms of WSD in order to get the best of them. Finally, we present our contributions for creating a lexical resource for French that proposes disambiguated and graduated synonyms according to their level of difficulty to be read and understood. We show that our resource is useful and can be integrated in a lexical simplification of texts module.
Our thesis is intended for francophones wishing to learn Arab, its lexicon, its syntax and its semantics. One of the main difficulties which the learner encounters as soon as he enters into contact with our language is the treatment of aspects and of tenses, bath locally and throughout the whole statement or text. The description of aspects and tenses requires us to make a general study of the verbal morphology of the grammatical structures of standard Arab. But we are also brought to work on the same structures in French which is the source language of the learners in order to make a comparative study of the two language-systems. The second face of this work is considerab1y more technical. We have insisted on the interfaces of the lessons that we present on Internet in order to show how they contribute in an active way to the understanding of the contents by the learners
This thesis studies policy iteration methods with linear approximation of the value function for large state space problems in the reinforcement learning context. We first introduce a unified algorithm that generalizes the main stochastic optimal control methods. We show the convergence of this unified algorithm to the optimal value function in the tabular case, and a performance bound in the approximate case when the value function is estimated. We then extend the literature of second-order linear approximation algorithms by proposing a generalization of Least-Squares Policy Iteration (LSPI) (Lagoudakis and Parr, 2003). Our new algorithm, Least-Squares [lambda] Policy Iteration (LS[lambda]PI), adds to LSPI an idea of [lambda]-Policy Iteration (Bertsekas and Ioffe, 1996): the damped (or optimistic) evaluation of the value function, which allows to reduce the variance of the estimation to improve the sampling efficiency. Thus, LS[lambda]PI offers a bias-variance trade-off that may improve the estimation of the value function and the performance of the policy obtained. In a second part, we study in depth the game of Tetris, a benchmark application that several works from the literature attempt to solve. Tetris is a difficult problem because of its structure and its large state space. We provide the first full review of the literature that includes reinforcement learning works, evolutionary methods that directly explore the policy space and handwritten controllers. We observe that reinforcement learning is less successful on this problem than direct policy search approaches such as the cross-entropy method (Szita et Lorincz, 2006). We finally show how we built a controller that outperforms the previously known best controllers, and shortly discuss how it allowed us to win the Tetris event of the 2008 Reinforcement Learning Competition
There is a wide gap between the language of mathematics and its formalized versions. The term "language of mathematics" or"mathematical language" refers to prose that the mathematician uses in authoring textbooks and publications. It mainly consists of natural language, symbolic expressions and notations. It is flexible,structured and semantically well-understood by mathematicians. However, it is very difficult to formalize it automatically. Some of the main reasons are: complex and rich linguistic features of natural language and its inherent ambiguity; intermixing of natural language with symbolic mathematics causing problems which are unique of its kind, and therefore, posing more ambiguity; and the possibility of containing reasoning gaps, which are hard to fill using the current state of art theorem provers (both automated and interactive). One way to work around this problem is to abandon the use of the language of mathematics. Therefore in current state of art of theorem proving, mathematics is formalized manually in very precise, specific and well-defined logical systems. For instance, these languages have non-ambiguous syntax with a limited number of possible syntactic constructions. This enterprise divides the world of mathematics in two groups. The first group consists of a vast majority of mathematicians whose rely on the language of mathematics only. In contrast, the second group consists of a minority of mathematicians. They use formal systems such as theorem provers (interactive ones mostly) in addition to the language of mathematics. To bridge the gap between the language of mathematics and its formalized versions, we may ask the following gigantic question: Can we build a program that understands the language of mathematics used by mathematicians and can we mechanically verify its correctness? This problem can naturally be divided in two sub-problems, both very hard: 1. Parsing mathematical texts (mainly proofs) and translating those parse trees to a formal language after resolving linguistic issues. 2. Verification of this formal version of mathematics. The project MathNat (Mathematics in controlled Natural language) aims at being the first step towards solving this problem, focusing mainly on the first question. First, we develop a Controlled Language for Mathematics (CLM) which is a precisely defined subset of English with restricted grammar and dictionary. To make CLM natural and expressive, we support some rich linguistic features such as anaphoric pronouns and references, rephrasing of a sentence in multiple ways and the proper handling of distributive and collective readings. Second, we translate MathAbs into equivalent first order formulas for verification.
In this paper, in addition to our research results for a French-Chinese machine translation system, we present the theoretical contributions from the SyGULAC theory and from the micro-systemic theory with its calculations as well as the methodologies developed aimed at a secure and reliable application in the context of machine translation. The application covers critical safety areas such as aerospace, medicine and civil security. Then, we explain the problems encountered during our research. The ambiguity, which is the major obstacle to the understandability and to the translatability of a text, is present at all language levels: syntactic, morphological, lexical, nominal and verbal. The identification of the units of a sentence is also a preliminary step for global understanding, whether for human beings or for a translation system. We present an inventory of the divergences between the french and the chinese language in order to achieve an machine translation system. We try to observe the verbal, nominal and vocabulary structure levels, in order to understand their interconnections and their interactions. We also define the obstacles to this research, with a theoretical point of view but also by studying our corpus. The chosen formalism starts from a thorough study of the language used in security protocols. A language is suitable for automatic processing only if this language is formalized. Therefore, An analysis of several French/Chinese bilingual corpora, but also monolingual, from civil security agencies, was conducted. The goal is to find out and present the linguistic characteristics (lexical, syntactic...) which characterize the language of security in general, and to identify all the syntactic structures used by this language. After presenting the formalization of our system, we show the recognition, transfer and generation processes.
As part of the international project InterPhonologie du Français Contemporain (IPFC) (Detey Kawaguchi and 2008; Racine, Detey and Kawaguchi 2012), this study aims to examine acquisition strategies of liaison in French as a foreign language. While developmental models have been proposed for the acquisition of liaison in L1, to date, no hypothesis accounting for L2 liaison learning has received convincing empirical support (Wauquier 2009). This approach shows the presence of common trends and errors in different populations of learners that suggest that, despite the use of lexical strategies, learners are able to develop phonological generalisations of liaison. Taking into account the difficulties that the heterogeneity of liaison presents for second language teaching and acquisition (Racine and Detey in press), learners show weaknesses in both production of liaison and in epilinguistic knowledge (Gombert 1990) of the variation of liaison.
The research presented in this dissertation analyzes topic transition in American English interaction, focusing on audio recordings of spontaneous conversations between friends and relatives. The main object of inquiry is the interactional action of transitioning to a new discourse topic, as well as the different linguistic strategies that participants have at their disposal. Three main types of cues are investigated: questions, discourse markers, and pitch register. Each type of cue is analyzed for its individual contribution to topic transition design, as well as for the way it can combine with, supplement, or contradict other cues. Analyzing different types of cues – verbal and prosodic – creates a composite picture of the various ways in which the topic trajectory of a conversation shapes its grammar – including its prosody. This study uses a mixed-methods approach which draws on the qualitative-oriented theoretical frameworks of Conversation Analysis and Interactional Linguistics, combining them with quantitative methods used in Corpus Linguistics, such as systematic coding and statistics. This multi-domain account is completed by elaborating a comparison between typical and atypical interactions. Persons suffering from schizophrenia can experience difficulties in managing the topics of a conversation, and they can produce non-canonical transitions. Comparing their data with that of typical participants thus sheds light on some of the expectations, preferences and standard formats which can otherwise remain hidden when topic transition goes smoothly.
Apathy is one of the most common neuropsychiatric symptoms in neurocognitive disorders and can be characterized as a significant reduction of goal directed behaviors in three dimensions: Emotions, Cognitions/Behaviors and Social Interactions. Information and Communication Technologies (ICT) can provide new objective measures and new assessment methods such as digital phenotyping, which consists of extracting a digital signature from an individual's digital and physiological measures. These measures can be collected from sensors connected to an application on smartphones, for instance, that can also comprise cognitive tests and questionnaires. As part of this thesis work, we identified existing methods of assessment of apathy and explored new methods using ICT. The first study explores the use of a serious game to assess loss of interest in apathy. Results showed that subjects suffering from apathy had significantly less interest in activities than non-apathetic subjects. In a second study, we explored acoustic speech markers based on an emotional speech task. In the final study presented, we measured facial expressivity from the same sample as the previous study. We extracted automatically facial features, also called action units (AU). Results showed that overall, the more severe the affect symptoms were, the less intense was the facial expressivity. In the single AU comparison, results were different depending on gender and the task's emotional valence (positive or negative). Several emotional and non-emotional AUs were linked to apathy in the upper region of the face (e.g. inner and outer brows movements) and lower region (e.g. lips part). These studies all aimed to explore new measures for apathy and need more validation with larger samples. Future studies should involve multiple types of assessment such as self-questionnaires and physiological measures, to ensure that the results are specific to apathy and no other confounding factors (cognitive decline, depression, stress etc.).
The goal of this thesis is to reduce the lack of available resources and NLP tools for Arabic language in specialised domains by proposing methods allowing the extraction of terms from texts in Modern Standard Arabic. In this context, we first constructed an English-Arabic parallel corous in a specific domain. It is a set of medical texts produced by the US National Library of Medicine (NLM). Thereafter, we have proposed terminological acquisition methods, toextract terms or acquire relations between these terms, for Arabic based on: i) the adaptation of an existing terminology extractor for French or English, ii) the transliteration of English terms in Arabic characters and iii) cross-lingual transfer. Applied at the terminological level, transfer aims to implement a process of term extraction or relationship acquisition between terms in the texts of a source language (here, French or English) and then to transfer the extracted information to target language texts (in this case, Modern Standard Arabic), thereby identifying the same type of terminologicalinformation. We have evaluated the monolingual and bilingual term lists that we have obtained by the experiments we carried out, according to a transparent, direct and semi-automatic method: the extracted term candidates are confronted with a reference terminology before being validated manually. This evaluation follows a protocol that we proposed.
We study in this thesis the joint construction of speech recognition and synthesis systems for new languages, with the goals of accuracy and quick development. The rapid development of voice technologies for new languages is driving scientific ambitions and is now considered strategic by industial players. However, language development research is led by a few research centers, each working on a limited number of languages. However, these technologies share many common points. Our study focuses on building and sharing tools between systems for creating lexicons, learning phonetic rules and taking advantage of imperfect data. Our contributions focus on the selection of relevant data for learning acoustic models, the joint development of phonetizers and pronunciation lexicons for speech recognition and synthesis, and the use of neural models for phonetic transcription from text and speech signal. In addition, we present an approach for automatic detection of phonetic transcript errors in annotated speech signal databases. This study has shown that it is possible to significantly reduce the quantity of data annotation useful for the development of new text-to-speech systems. It naturally helps to reduce data collection time in the process of new systems creation.
This work presents a novel approach to the study of the Jābirian corpus while taking into consideration the existent works and literature and the problems of this peculiar corpus (synonymy, polysemy, dispersion of the knowledge, quotes od other authors, hypertextuality). Section I is an introduction on the historical setting and remarks of the subject of the texts studied, comprising also an excursus on the figure of Jābir Ibn Hayyān and the querelle on his existence; and an explanation of the methodological setting in which this work is settled. The core of the work is represented by the Appendices, divided in four parts: Appendix D comprise a sample of concordances based on the lemmatization of the edition of the first two books of the Tadbīr. Appendix E is the frequency list of the same sample used for the concordances.
The architecture uses the concept of ontology for the descriptionof the environment. We have chosen to use the open source PROTEGE because it allows the definition of the ontology and the fusion and fission engines. Indeed, multimodal inputs will be merged and subdivided into elementary tasks and sent tocontrol the wheelchair with the manipulated arm. This architecture will be validated by specifications and simulations via temporal and stochastic Petri nets.
The ethics of emerging forms of artificial intelligence has become a prolific subject in both academic and public spheres. A great deal of these concerns flow from the need to ensure that these technologies do not cause harm—physical, emotional or otherwise—to the human agents with which they will interact. In the literature, this challenge has been met with the creation of artificial moral agents: embodied or virtual forms of artificial intelligence whose decision procedures are constrained by explicit normative principles, requiring the implementation of what is commonly called artificial morality into these agents. To date, the types of reasoning structures and principles which inform artificial morality have been of two kinds: first, an ethically maximal vision of artificial morality which relies on the strict implementation of traditional moral theories such as Kantian deontology or Utilitarianism, and second, a more minimalist vision which applies stochastic AI techniques to large data sets of human moral preferences so as to illicit or intuit general principles and preferences for the design of artificial morality. We provide an alternative approach to the design of artificial morality, the Ethical Valence Theory, whose purpose is to accommodate this balance, and apply this approach to the case of autonomous vehicles.
In this thesis, we study lexical semantic change: temporal variations in the use and meaning of words, also called extit{diachrony}. These changes are carried by the way people use words, and mirror the evolution of various aspects of society such as its technological and cultural environment. We explore, compare and evaluate methods to build time-varying embeddings from a corpus in order to analyse language evolution. We focus on contextualised word embeddings using pre-trained language models such as BERT. We propose several approaches to extract and aggregate the contextualised representations of words over time, and quantify their level of semantic change. In particular, we address the practical aspect of these systems: the scalability of our approaches, with a view to applying them to large corpora or large vocabularies; their interpretability, by disambiguating the different uses of a word over time; and their applicability to concrete issues, for documents related to COVID19. We evaluate the efficiency of these methods quantitatively using several annotated corpora, and qualitatively by linking the detected semantic variations with real-life events and numerical data. Finally, we extend the task of semantic change detection beyond the temporal dimension. We adapt it to a bilingual setting, to study the joint evolution of a word and its translation in two corpora of different languages; and to a synchronic frame, to detect semantic variations across different sources or communities on top of the temporal variation.
With so many job adverts and candidate profiles available online, the e-recruitment constitutes a rich object of study. One of the difficulties when dealing with this type of raw textual data is being able to grasp the concepts contained in it, which is the problem of standardization that is tackled in this thesis. A nomenclature is by definition a finite set of meaningful concepts, which means that the attributes resulting from standardization are a structured representation of the information. What structure of nomenclature is the best suited for standardization, and how to leverage it? Is it possible to automatically build such a nomenclature from scratch, or to manage the standardization process without one? To illustrate the various obstacles of standardization, the examples we are going to study include the inference of the skills or the category of a job advert, or the level of training of a candidate profile. One of the challenges of e-recruitment is that the concepts are continuously evolving, which means that the standardization must be up-to-date with job market trends. In light of this, we will propose a set of machine learning models that require minimal supervision and can easily adapt to the evolution of the nomenclatures. The questions raised found partial answers using Case Based Reasoning, semi-supervised Learning-to-Rank, latent variable models, and leveraging the evolving sources of the semantic web and social media. The different models proposed have been tested on real-world data, before being implemented in a industrial environment. The resulting standardization is at the core of SmartSearch, a project which provides a comprehensive analysis of the job market.
The challenge of MWEs in this respect is that, in contrast to regular linguistic expressions, they exhibit various irregular properties which make them harder to deal with in natural language processing. In our work, we show that the challenge of the MWE-related irregularities can be turned into an advantage in practical symbolic parsing. Namely, with tree adjoining grammars (TAGs), which provide first-cLass support for MWEs, and A* search strategies, considerable speed-up gains can be achieved by promoting MWE-based analyses with virtually no loss in syntactic parsing accuracy. This is in contrast to purely statistical state-of-the-art parsers, which, despite efficiency, provide no satisfactory support for MWEs. We contribute a TAG-A* -MWE-aware parsing architecture with facilities (grammar compression and feature structures) enabling real-world applications, easily extensible to a probabilistic framework.
Relational data are ubiquitous in the nature and their accessibility has not ceased to increase in recent years. Those data, see as a whole, form a network, which can be represented by a data structure called a graph, where each vertex of the graph is an entity and each edge a connection between pair of vertices. Complex networks in general, such as the Web, communication networks or social network, are known to exhibit common structural properties that emerge through their graphs. In this work we emphasize two important properties called *homophilly* and *preferential attachment* that arise on most of the real-world networks. We firstly study a class of powerful *random graph models* in a Bayesian nonparametric setting, called *mixed-membership model* and we focus on showing whether the models in this class comply with the mentioned properties, after giving formal definitions in a probabilistic context of the latter. Furthermore, we empirically evaluate our findings on synthetic and real-world network datasets. Secondly, we propose a new model, which extends the former Stochastic Mixed-Membership Model, for weighted networks and we develop an efficient inference algorithm able to scale to large-scale networks.
The thesis focuses on the characterization and the identification of foreign accents in French. How many accents may a native French speakers recognize and which cues does (s) he use? Our interest concentrates on French productions from speakers of different mother tongues: English, German, Arabic, Spanish, Italian and Portuguese, also compared with natives French speakers. Using automatic speech processing, our objective is to identify the most reliable acoustic cues in order to distinguish between accents and link these cues with human perception. We measured acoustic parameters such as the duration and voicing rate for consonants, the first two formants values for vowels, the number of schwas produced, the extension of the vowel before a silent schwa, the difference between fundamental frequency of the penultimate vowel and of the final schwa, and the percentages of confusion obtained using automatic alignments with non-standard pronunciation variants. Machine learning techniques were used to select the most discriminating cues differencing between different accents. The results obtained with automatic identification in discriminating between 7 linguistic origins are comparable with those obtained during perceptive tests. With the perceptive tests as well as with automatic identification natives speakers of French are best recognized between seven accents. The cues specific to each of the studied accents can be used in automatic recognition in order to reduce the high error rate.
We tried to explore the notion of translator's style combining the analysis of translation studies and the multilingual textometric methods (quantitatifs textual methods applied in the corpora of aligned texts). Our corpus research is composed of three Chinese translations of one original text in French, namely Jean-Christophe by Romain Rolland (1904-1917), translated by Fu Lei (1952-1953), Han Hulin (2000) and Xu Yuanchong (2000), respectively. After describing the difficulties in building the French-Chinese parallel corpus, we conducted successively various textometric measures on the corpus in order to highlight the specific lexical and syntactic uses of each translator. The re-contextualization in the parallel corpus of the statistical differences of linguistic phenomena between translations and the consideration of socio-cultural factors at each time reveal the indicators of each translator's style. The detailed research on Chinese particles in translations studies, based on textometric comparisons, provides rich results revealing each translator's specific approaches in his work. The results garnered from the comparison of three Chinese translations of the same text between them, then with the French original text lay the groundwork for our proposition of the analytical model on translator's style. We think that our present work offers a methodology for a scientific and systematic exploration to the notion of translator's style within the translation studies.
This thesis studies the relationships between non-standard online writing practices and gender on the American community website Reddit. It is based on a corpus of nearly 20 million tokens which contains the comments written in English by 1,044 internet users, including 300 transgender and non-binary people. Eleven non-standard variables were investigated: six additive processes (emoticons, emojis, letter lengthenings, punctuation lengthenings, all caps and interjections) and five reduction processes (abbreviations, phonetic spellings, g-droppings, apostrophe omissions and lower case spellings of the pronoun “I”). The analyses, which are mainly based on the multiple regression method, provide a nuanced account of the way Redditors use non-standard language to index their gender identity. They also suggest that Hispanic and African-American women play a major role in the spread of non-standard spelling and typography.
This thesis work is at the frontier between multimedia information retrieval and automatic speech processing. During the last years, a new task en speech processing: the rich transcription of an audio document. An important meta-data for rich transcription is the information on sentence type sentence of interrogative or affirmative type). The study on the prosodie differences between these two types of sentences in Vietnamese languaç detection and classification of sentence type in French language and in Vietnamese language is the main subject of this research work. We've realized a system for segmentation and automatic detection of sentence type based on both prosodie and lexica information. The system has been validated on real world spontaneous speech corpus which are recording of conversations via telephone, betwee and a tourism office staff, recruiting interview, project meeting. After this first study on French, we've extended our research in Vietnamese langui language where ail studies until now on prosodie system are still preliminary. We've carried a study on the prosodie differences between interroga affirmative sentences in both production and perception levels. Next, based on these results, a classification motor has been built.
Dung's theory of abstract argumentation is a formalism that represents conflicting information using an argumentation framework. Extension-based semantics have been introduced to determine, given an argumentation framework, the justifiable points of view on the acceptability of the arguments. However, these semantics are not appropriate for some applications. So alternative semantics, called ranking-based semantics, have recently been evolved. Such semantics produces, for a given argumentation framework, a ranking on its arguments from the most acceptable to the least one(s). The overall aim of this thesis is to propose and study ranking-based semantics in the context of abstract argumentation. We first define a new family of ranking-based semantics based on a propagation principle which allow us to control the influence of non-attacked arguments on the acceptability of arguments. We investigate the properties of these semantics, the relationships between them but also with other existing semantics. Then, we provide a thorough analysis of ranking-based semantics in two different ways. The first one is an empirical comparison on randomly generated argumentation frameworks which reveals insights into similarities and differences between ranking-based semantics. The second one is an axiomatic comparison of all these semantics with respect to the proposed properties aiming to better understand the behavior of each semantics. At last, we question the ability of the existing ranking-based semantics to capture persuasion settings and introduce a new parametrized ranking-based semantics which is more appropriate in this context.
Stimulated by many applications such as documents or images annotation, multilabel learning have gained a strong interest during the last decade. But, standard algorithms cannot cope with the volumes of the recent extreme multi-label data (XML) where the number of labels can reach millions. This thesis explores three directions to address the complexity in time and memory of the problem: multi-label dimension reduction, optimization and implementation tricks, and tree-based methods. It proposes to unify the reduction approaches through a typology and two generic formulations and to identify the most efficient ones with an original meta-analysis of the results of the literature. A new approach is developed to analyze the interest of coupling the reduction problem and the classification problem. To reduce the memory complexity of a classical one-vs-rest regression model while maintaining its predictive performances, we also propose an algorithm for estimating the largest useful parameters that follows a strategy inspired by data stream analysis. Finally, we present a new algorithm called CRAFTML that learns an ensemble of diversified decision trees. The contributions of the thesis are completed by the presentation of a software called VIPE that is developed with Orange Labs for multilabel opinion analysis.
Having closely examined the history of Malay and Sanskrit, we have not only shed new light on Malay's drawing on Sanskrit but we have also observed that words borrowed from Sanskrit have been phonologically modified. Furthermore Malay still uses Sanskrit prefixes to generate new words. We propose that Malay be attached to the 'Indo-Malaysian' language branch. We have analyzed canonical sentence structures and have also compared the verbal system of French and Malay. Malay is a highly agglutinative language whilst for French verbs this phenomenon is very rare. We have observed for example that the indirect object is introduced by a preposition in French whilst prepositions are absent in Malay; that the Malay verb “ada” can represent certain meanings of 'be and have' in French (être and avoir); that transitive and aspectual structures are not the same in the two languages, etc. We have developed a formal representation which enables comparing the French and Malay languages systems, this being inspired by S. Cardey's micro-systemic approach which has also enabled creating other verbal microsystems. In doing so, we have clarified the similarities and differences concerning French and Malay structures and verbal systems.
This thesis deals the problem of image annotation extension. Indeed, the fast growth of available visual contents has led a need for indexing and searching of multimedia information methods. Image annotation allows indexing and searching in a large collection of images in an easy and fast way. For automatic image annotation extension, we use probabilistic graphical models. The proposed model is based on a mixture of multinomial distributions and mixtures of Gaussian where we have combined visual and textual characteristics. To reduce the cost of manual annotation and improve the quality of the annotation obtained, we have incorporated user feedback into our model. To reduce the semantic gap problem and to enrich the image annotation, we use a semantic hierarchy by modeling many semantic relationships between keywords. After building the hierarchy, we integrate it into our image annotation model. The model obtained with this hierarchy is a mixture of Bernoulli distributions and
In this work, we propose a new unsupervised process to adapt the language model (LM) and the vocabulary of an automatic speech recognition system to the topic of each spoken document to be transcribed. This process is particularly original since it firstly avoids the use of any a priori knowledge about potentially encountered topics and secondly integrates natural language processing techniques. In order to achieve these goals, we characterize the topic of a spoken document by automatically extracting keywords from a first-pass automatic transcription before building a topic-specific corpus of Web pages. Then, the LM is re-estimated thanks to a terminology acquired from this corpus, and new topic-specific words are added to the vocabulary and integrated to the LM by relying on paradigmatic relations with in-vocabulary words. Experiments done on French-speaking broadcast news show that using these topic-adapted vocabulary and LM lead to recognition accuracy improvements.
This PhD thesis deals with the notion of pedagogical indexation and tackles it from the point of view of searching for and selecting texts for language teaching. This particular problem is set in the field of Computer Assisted Language Learning (CALL) and of the potential contribution of Natural Language Processing (NLP) to this discipline, before being considered within the scope of elements more directly relevant to language didactics, in order to propose an empirical approach. The thesis subsequently revolves around two questionnaires the aim of which is to provide insight into language teachers'declared practices regarding searching for and selecting texts in the context of class planning. The first questionnaire provides data to formalize the notion of pedagogical context, which is later considered through some of its components thanks to the second questionnaire. Finally, these first formalization drafts provide foundations for the definition of a model aiming at taking into account the contextuality of the properties said to be pedagogical, which is inherent to raw resources. Finally, possible leads for implementing this model are suggested through the description of a computerized system.
Automatic speech recognition technologies are now integrated into many systems. The performance of speech recognition systems for non-native speakers, however, continues to suffer high error rates, due to the difference between native and non-speech models trained. The making of recordings in large quantities of non-native speech is typically a very difficult and impractical to represent all the origins of the speakers. This thesis focuses on improving multilingual acoustic models for automatic phonetic transcription of speech such as “multilingual meeting”. To meet these challenges, we propose a process of adaptation of multilingual acoustic models is called "autonomous adaptation". In autonomous adaptation, we studied several approaches for adapting multilingual acoustic models in unsupervised way (spoken languages and the origins of the speakers are not known in advance) and no additional data is used during the adaptation process. The approaches studied are decomposed into two modules. The first module called "the language observer" is to recover the linguistic information (spoken languages and the origins of the speakers) of the segments to be decoded. The second module is to adapt the multilingual acoustic model based on knowledge provided by the language observer. To evaluate the usefulness of autonomous adaptation of multilingual acoustic model, we use the test data, which are extracted from multilingual meeting corpus, containing the native and nonnative speech of three languages: English (EN), French (FR) and Vietnamese (VN). According to the experiment results, the autonomous adaptation shows promising results for non native speech but very slightly degrade performance on native speech. To improve the overall performance of transcription systems for all native and non native speech, we study several approaches for detecting non native speech and propose such a detector cascading with our self-adaptation process (autonomous adaptation). The results thus are the best among all experiments done on our corpus of multilingual meetings.
Since the late 1990s, texts have emerged as a precious source of knowledge for building ontologies that are at times a semantic framework of the Semantic Web and sometimes its bottleneck. In fact, texts carry stabilized and shared knowledge which are easier to access than questioning any expert. The use of texts doesn't replace human expertise but allows the knowledge engineer to understand the domain to be modelled and initiate the work of modelling. One of the challenges of the transformation from texts to ontologies, is to detect a vocabulary of the domain and its structure in the form of a thesaurus before its formalization and these difficulties that are inherent to exploitation of linguistic material and its normalization, caught our attention in this thesis. We propose a normalization method that transforms the linguistic material - as it was extracted from an acquisition corpus by NLP tools - in a semantic network that we call "termino-conceptual network" and that describes a normalized vocabulary of the domain: a disambiguated and structured vocabulary such that it is stabilized in the concerned domain. It is a network of unambiguous terms that are interconnected through taxonomic and associative relationships. It serves not only as the basis for building a domain ontology from texts but also as a thesaurus for annotating documents. This thesis was conducted within the European project ONTORULE (ontology meets business rules). Our approach fits within the overall ontological resources construction TERMINAE method that was initiated by the work of the TIA group (Terminology Intelligence Artificial). The second normalization step transforms the original terminology network into a conceptual network. If the first step can be automated by using extraction tools, the other two require a disambiguation and modeling work that is largely based on human expertise. This thesis helps to refine the method by showing how TERMINAE decomposes the normalization work in different operations, how these operations are enchained and how to control the overall normalization process. It is indeed a difficult step for the knowledge engineer who, after the linguistic extraction phase, is facing a mass units to process, some of them are ambiguous and not all are relevant to the domain. This normalization approach has been experimented to evaluate the main contributions in this thesis. The ontologies created were used in the ONTORULE project for three different use-cases. They served as conceptual vocabularies for writing business rules related to different decision based systems but especially they were used to semantically annotate business documents and to guide the acquisition work of the database business rules from these texts.
This thesis aims to study the difficulties of automatic speech simplification, to improve our understanding of neural speech translation models, and to use them in the context of automatic sub-titling.
The recent progress in artificial neural networks (rebranded as deep learning) has significantly boosted the state-of-the-art in numerous domains of computer vision. In this PhD study, we explore how deep learning techniques can help in the analysis of gender and age from a human face. In particular, two complementary problem settings are considered: (1) gender/age prediction from given face images, and (2) synthesis and editing of human faces with the required gender/age attributes. Firstly, we conduct a comprehensive study which results in an empirical formulation of a set of principles for optimal design and training of gender recognition and age estimation Convolutional Neural Networks (CNNs). On a very challenging internal dataset, our best models reach 98.7% of gender classification accuracy and an average age estimation error of 4.26 years. In order to address the problem of synthesis and editing of human faces, we design and train GA-cGAN, the first Generative Adversarial Network (GAN) which can generate synthetic faces of high visual fidelity within required gender and age categories. Moreover, we propose a novel method which allows employing GA-cGAN for gender swapping and aging/rejuvenation without losing the original identity in synthetic faces. Finally, in order to show the practical interest of the designed face editing method, we apply it to improve the accuracy of an off-the-shelf face verification software in a cross-age evaluation scenario.
The dissemination of digital reading terminals (readers, tablets, mobile phones) has the potential to transform the reading experience, augmenting it with new functionalities (enriching the text with hypertext links, dictionaries, images, music or sounds, etc.), adapting the presentation according to the readers, or making it more social and collaborative, by sharing annotations, quotations, or reading recommendations. These devices also open new perspectives for computer assisted learning, in particular through their ability to embed text overlays that can help, in an adaptive and personalized way, apprentice readers (or pathological readers) to progress in their understanding of texts. The study of reading aids implies a preliminary (or parallel) reflection on evaluation: pedagogical evaluation of the devices, evaluation of the readers, but also evaluation of the reading contents themselves: building automatic recommendation devices or reading pathways adapted to learners (in L1 as in L2) requires the ability to evaluate the works (or, at a finer level, the chapters or passages of works) on a scale of difficulty, which makes it possible to propose readings that are well suited to the abilities of the reader. Although the question of measuring the readability of texts is relatively old, it is still the subject of active research, in particular as regards the automatic measurement of readability, of which we present a brief state of the art below. In the absence of automatic measurement, the only information available to teachers and readers to choose their readings are rough categories which do not accurately reflect the difficulty of the texts. Having inventories on a very large scale, associating works (or passages) with levels of reading difficulty, is therefore essential to guide a novice reader. This project brings together a company specialized in social electronic reading devices (GLOSE) and research teams specialized in automatic language processing (within the LIMSI-CNRS), with the objective of developing automatic tools for measuring reading difficulty in an educational context. Special attention will be paid to two innovative aspects: on the one hand, modelling the evolutionary nature of reading difficulty and, on the other, analysing difficulty in a multilingual context, in which it is important to develop 'universal'measurement techniques that can potentially be applied to a large number of languages.
In the Web, RSS and Atom (feeds) are probably the most popular and highly utilized XML formats which allow web communities, publishing industries, web services, etc. to publish and exchange XML documents. In addition, they allow a user to consume data/information easily without roaming from site to site using software applications. Here, the user registers her favorite feed providers; and each provider sends the list of news items changed since the last download. However, registering a number of feed sources in feed aggregators cause both heterogeneity and information overloading problems. Besides, none of the existing RSS/feed aggregators provide an approach that integrates (merges) feeds from different sources considering similarity, user contexts and preferences. In this research, we provide a formal framework that handles the heterogeneity, integration and querying feeds. The framework is based a tree representation of a feed and has three main components: feed comparator, merger and query processor. The feed comparator addresses the issue of measuring the relatedness between news items using a Knowledge Base, a bottom-up and incremental approaches. We proposed a concept-based similarity measure based on the function of the number of shared and different concepts in their global semantic neighborhoods. We show also how to define and identify the exclusive relationship between any two texts and elements. The feed merger addresses the issue of integrating news items from different sources considering a user context. We show here how to represent a user context and her preferences. Also, we provide a set of predefined set of merging rules that can be extended and adapted by a user. The query processor is based on a formal study on RSS query algebra that uses the notion of semantic similarity over dynamic content. The operators are supported by a set of similarity-based helper functions. We categorize the RSS operators into extraction, set membership and merge operators. The merge operator generalizes the join and the set membership operators. We also provide a set of query rewriting and equivalence rules that would be used during query simplification and optimization. Finally, we present a desktop prototype called Easy RSS Manager (EasyRSSManager) having a semanticaware RSS Reader, and semantic-aware and window-based RSS query components. It is designed to validate, demonstrate and test the practicability of the different proposals of this research. In particular, we test the timing complexity and the relevance of our approaches using both a real and syntactic dataset.
Due to the diversity of alternative contents to choose and the change of users' preferences, real-time prediction of users' preferences in certain users' circumstances becomes increasingly hard for recommender systems. However, most existing context-aware approaches use only current time and location separately, and ignore other contextual information on which users' preferences may undoubtedly depend (e.g. weather, occasion). Furthermore, they fail to jointly consider these contextual information with social interactions between users. On the other hand, solving classic recommender problems (e.g. no seen items by a new user known as cold start problem, and no enough co-rated items with other users with similar preference as sparsity problem) is of significance importance since it is drawn by several works. In our thesis work, we propose a context-based approach that leverages jointly current contextual information and social influence in order to improve items recommendation. In particular, we propose a probabilistic model that aims to predict the relevance of items in respect with the user's current context. We considered several current context elements such as time, location, occasion, week day, location and weather. In order to avoid strong probabilities which leads to sparsity problem, we used Laplace smoothing technique. On the other hand, we argue that information from social relationships has potential influence on users' preferences. Thus, we assume that social influence depends not only on friends' ratings but also on social similarity between users. The user-friend social similarity information may be established based on social interactions between users and their friends (e.g. recommendations, tags, comments). We conducted a comprehensive effectiveness evaluation on real dataset crawled from Pinhole social TV platform. This dataset includes viewer-video accessing history and viewers' friendship networks. In addition, we collected contextual information for each viewer-video accessing history captured by the plat form system. The platform system captures and records the last contextual information to which the viewer is faced while watching such a video. In our evaluation, we adopt Time-aware Collaborative Filtering, Time-Dependent Profile and Social Network-aware Matrix Factorization as baseline models. The evaluation focused on two recommendation tasks. We evaluated the impact of each viewing context element in prediction performance. Experimental results demonstrate that our approach outperforms time-aware and social network-based approaches. In the sparsity and cold start tests, our approach returns consistently accurate predictions at different values of data sparsity.
The general context of this thesis is the quantitative analysis of objects coming from rational language theory. We adapt techniques from the field of analysis of algorithms (average-case complexity, generic complexity, random generation...) to objects and algorithms that involve particular classes of automata. In a first part we study the complexity of Brzozowski's minimisation algorithm. Although the worst-case complexity of this algorithm is bad, it is known to be efficient in practice. Using typical properties of random mappings and random permutations, we show that the generic complexityof Brzozowski's algorithm grows faster than any polynomial in n, where n is the number of states of the automaton. In a second part, we study the random generation of acyclic automata. These automata recognize the finite sets of words, and for this reason they are widely use in applications, especially in natural language processing. We present two random generators, one using a model of Markov chain, the other a ``recursive method", based on a cominatorics decomposition of structures. The first method can be applied in many situations cases but is very difficult to calibrate, the second method is more efficient. Once implemented, this second method allows to observe typical properties of acyclic automata of large size
Graph is a powerful concept for representation of relations between pairs of entities. Data with underlying graph structure can be found across many disciplines, describing chemical compounds, surfaces of three-dimensional models, social interactions, or knowledge bases, to name only a few. However, surprisingly little has been done to explore the applicability of DL on graph-structured data directly. The goal of this thesis is to investigate architectures for DL on graphs and study how to transfer, adapt or generalize concepts working well on sequential and image data to this domain. We concentrate on two important primitives: embedding graphs or their nodes into a continuous vector space representation (encoding) and, conversely, generating graphs from such vectors back (decoding). To that end, we make the following contributions. The method is used to encode graphs with arbitrary and varying structure. Second, we propose SuperPoint Graph, an intermediate point cloud representation with rich edge attributes encoding the contextual relationship between object parts. Based on this representation, ECC is employed to segment large-scale point clouds without major sacrifice in fine details. Third, we present GraphVAE, a graph generator allowing to decode graphs with variable but upper-bounded number of nodes making use of approximate graph matching for aligning the predictions of an autoencoder with its inputs. The method is applied to the task of molecule generation
Modern systems are pressured to adapt in response to their constantly changing environment to remain useful. Traditionally, this adaptation has been handled at down times of the system. There is an increased demand to automate this process and achieve it whilst the system is running. Self-adaptive systems were introduced as a realization of continuously adapting systems. Self-adaptive systems are able to modify at runtime their behavior and/or structure in response to their perception of the environment, the system itself, and their requirements. The focus of this work is on realizing self-configuration, a key and essential property of self-adaptive systems. Self-configuration is the capability of reconfiguring automatically and dynamically in response to changes. This may include installing, integrating, removing and composing/decomposing system elements. This thesis introduces the Dr-BIP framework, an extension of the BIP framework for modeling self-configuring systems that relies on a model-based and component and connector approach to prescribe systems. The combination of both of these approaches exploits the benefits of each, making their combination an ideal methodology to realize complex self-configuring systems. A Dr-BIP system model is a runtime model which captures the running system at three different levels of abstraction namely behavior, configuration, and configuration variants. The system's configuration is captured by component and connectors. In a component and connector system, self-configuration can have three different levels of granularity which includes the ability to add or remove connectors, add or remove components, and add or remove subsystems. Dr-BIP supports explicit addition and removal of both components and subsystems, but implicit addition and removal of connectors. The main advantage of relying on an implicit addition and removal of connectors is the ability to guarantee by construction specific configuration topologies. To capture the three levels of abstraction, we introduce motifs as primary structures to prescribe a self-configuring Dr-BIP system. A motif defines a set of components that evolve according to interaction and reconfiguration rules. A system is composed of multiple motifs that possibly share components and evolve together. Interaction rules dictate how components composing the system can interact and reconfiguration rules dictate how the system configuration can evolve over time. Finally, we show that the proposed framework is both minimal and expressive by modeling four different self-configuring systems. Last but not least, we propose a modeling language to codify the framework concepts and provision an interpreter implementation.
Nowadays, smartphones and smart tablets generate, receive, store and transfer substantial quantities of data, providing services for all possible user needs with easily installable programs, also known as mobile applications. A number of sensors integrated into smartphones allow the devices to collect very precise information about the owner and his environment at any time. The important flow of personal and business data becomes hard to manage. The “Privacy by Design” approach with 7 privacy principles states privacy can be integrated into any system from the software design stage. In Europe, the Data Protection Directive (Directive 95/46/EC) includes “Privacy by Design” principles. The goal of this thesis is to propose pattern-oriented solutions to cope with mobile privacy problems, such as lack of transparency, lack of consent, poor security and disregard for purpose limitation, thus giving mobile systems more Privacy by (re) Design
Indeed, topic structuring is considered as the starting point of several applications such as information retrieval, summarization and topic modeling. In this thesis, we proposed a generic topic structuring system i.e. that has the ability to deal with any TV Broadcast News. Our system contains two steps: topic segmentation and title assignment. Topic segmentation consists in splitting the document into thematically homogeneous fragments. The latter are generally identified by anonymous labels and the last step has to assign a title to each segment. Several original contributions are proposed like the use of a joint exploitation of the distribution of speakers and words (speech cohesion) and also the use of diachronic semantic relations. Finally, we proposed the evaluation of two new metrics, the first is dedicated to the topic segmentation and the second to title assignment. They consisted of 168 TV Broadcast News from 10 French channels automatically transcribed.
This Ph.D focuses on the analysis of textual data in the health domain and in particular on the supervised multi-class classification of data from biomedical literature and social media. One of the major difficulties when exploring such data by supervised learning methods is to have a sufficient number of data sets for models training. Indeed, it is generally necessary to label manually the data before performing the learning step. The large size of the data sets makes this labellisation task very expensive, which should be reduced with semi-automatic systems. In this context, active learning, in which the Oracle intervenes to choose the best examples to label, is promising. The intuition is as follows: by choosing the smartly the examples and not randomly, the models should improve with less effort for the oracle and therefore at lower cost (i.e. with less annotated examples). In this PhD, we will evaluate different active learning approaches combined with recent deep learning models. In addition, when small annotated data set is available, one possibility of improvement is to artificially increase the data quantity during the training phase, by automatically creating new data from existing data. More precisely, we inject knowledge by taking into account the invariant properties of the data with respect to certain transformations. The augmented data can thus cover an unexplored input space, avoid overfitting and improve the generalization of the model. In this Ph.D, we will propose and evaluate a new approach for textual data augmentation.
Language is the most preserved cognitive function from the effects of aging. Unlike typical aging, Alzheimer's disease (AD) is characterized by an early impairment. Analysis of discourse may reveal more difficulties than other tests since it mobilizes a large set of cognitive functions. This work aims to analyze various discourses in AD patients and participants with typical aging, in relation to various cognitive tests as well as neuroimaging data. One study focused on a memory-based discourse. A second study compared a memory-based and in a picture-based discourse. Les résultats montrent que les difficultés rencontrées lors d'un discours d'événements vécus seraient corrélées à l'atteinte mnésique des patients. A l'inverse, elles seraient associées à l'atteinte exécutive et lexicale dans un discours imagé. A final study focused on discourse variability in the general aging population. A cluster analysis revealed four profiles of speakers. Among them, an "off-topic" profile could reflect a grey zone between normal and pathological aging. This work allowed us to shed light on deficit markers and compensatory strategies in prodromal AD, in relation with cognitive and cerebral changes. In particular, many difficulties may actually be related to memory or executive impairment, regardless of a language alteration.
This thesis aims to better understand how scientific workshops led by researchers at the university try to motivate students from primary school to high school to pursue a university science career. Our major challenge was to analyse the scientific workshops with a didactical goal by taking into account the complexity of interactions between its actors and their expectations. As such, this work is based on a analysis of the workshops, as these are seen as a complex dynamic system. We articulate a qualitative approach consisting in a “complex core of analysis” and a systemic triangulation of the system: the context, the strategies and the production. In this perspective, this thesis reflets on the culture of assessment in education by proposing a modelisation of a “multireferencial assessment” based on three criteria: the efficacy, the relevance and the efficiency. To do that, different levels of analyses are proposed based on questionnaires and interviews of students, mediators and teachers who were engaged in workshops. The results have especially highlighted the gap between the objectives of the workshops' participants and a those of the institutional presented for these workshops. For the mediators, the teachers and the students, these workshops are primarily meant to enjoy the activities. More, mediators who took part in the study express little interest in showing a positive view of their own academic background and their career, whereas students see, before anything else, researchers' career as a vocation.
Supervised, semi-supervised or unsupervised deep learning approaches seem to offer favorable premises for achieving the goals of the project. Nevertheless, they must be reconsidered in a pragmatic and consistent manner in order to be able to provide concrete, global solutions, that may be exploitable in an industrial context. Based on a generic artificial intelligence (AI) platform, integrating different analysis metrics, according to the different media channels that can be considered, a benchmark of existing technologies on both generic and specific bases (i.e., France Télévisions specific contents) will be realized, in order to objectively evaluate the performances and to retain the most promising methods. An analysis of the results with advantages and limitations will be conducted to identify areas for improvement/adaptation/optimization. Subsequent developments will concern the fusion of the different media. To achieve this goal, it is essential to identify and exploit, for recommendation purposes, combinations of modalities that can lead to relevant results. The level of spatio-temporal granularity is also to be considered and taken into account. The main technological and scientific obstacles that are tackled in this thesis project are the following: - the specification of an intelligent taxonomy adapted to the different types of programs, which can be identified and learned on the fly, - taking into account, for specific categorization purposes, a self-adaptable multi-modal dimension, - the definition of appropriate levels of spatio-temporal and semantic granularities, - the adaptive selection of the different dimensions involved in multi-modality, - the automatic detection of semantic audio-visual projections.
In this corpus-based study, we are concerned with the formation of two types of denominal adjectives in Slovak: adjectives suffixed with -ský (NskýA) and compound adjectives formed by an adjective and a noun (ANA). The obtained results are verified and tested in two surveys conducted with Slovak native speakers. The Slovak morphological constructions and compared with semantically equivalent expressions in French. The formation of ANA (MODROOKÝA "blue-eyed") obeys a set of three types of semantic constraints: (i) between ANA and the Head-Noun (HdN), e.g. modrookéA dievcaHdN "blue-eyed girl", (ii) between the noun-component (N) and the HdN (OKON "eye" et dievca "girl"), and (iii) between the adjective-component (A) and the N (MODRÝA "blue" et OKON "eye"). An ANA refers to an inherent property of an individual denoted by HdN. The interpretation in force is a Part-Whole Relation: the N denotes a constitutive and visible part of the concrete entity referred to by the HdN. The suffixation by means of -ský applies mainly to proper nouns denoting either a portion of space (toponym) or a human (anthroponym). In Slovak, the semantic interpretation of an NskýA adjective is normally the same as that of the base noun. Toponymic adjectives like LIBANONSKÝA "lebanese" can refer to the geographical space as well as to the inhabitants of this space. Only the HdN's semantic value is able to disambiguate the meaning of such a toponymic NskýA. As for anthroponymic NskýA like STALINSKÝA "stalinien", they are likely to appear, besides the relational use, i.e. "concerning Stalin", in qualifying contexts: "comparable to Staline".
Using queries to explore corpora is today part of the routine of not only researchers of various fields with an empirical approach to discourse, but also of non-specialists who use search engines or concordancers for language learning purposes.
In this thesis we study several machine learning problems that are all linked with the minimization of a noisy function, which will often be convex. Inspired by real-life applications we focus on sequential learning problems which consist in treating the data ``on the fly'', or in an online manner. The first part of this thesis is thus devoted to the study of three different sequential learning problems which all face the classical ``exploration vs. exploitation''trade-off. We demonstrate that all of these problems can be studied under the scope of stochastic convex optimization, and we propose and analyze algorithms to solve them. In the second part of this thesis we focus on the analysis of the Stochastic Gradient Descent algorithm, which is likely one of the most used stochastic optimization algorithms in machine learning. We provide an exhaustive analysis in the convex setting and in some non-convex situations by studying the associated continuous-time model, and obtain new optimal convergence results.
This thesis is a reflection upon the notions of linguistic norm(s) and usage(s) within the domain of Air Traffic Control. Whenever the limits of phraseology are reached, pilots and controllers revert to a more natural form of language which has only recently been described in this domain: plain language. However, several problems relating to the implementation of plain language and phraseology have been identified by professional Aviation English teachers. To meet the needs of the French civil aviation university (ENAC), a survey of the different uses of the English language by French controllers and international pilots is presented. The method of analysis consists in a comparative study between two corpora: one representing the prescribed norm and the other corresponding to real pilot-controller communications. This comparative analysis allows the description and categorization of the different types of languages used on the frequency in air navigation typical situations. Some differences are found between the two corpora. Depending on the situation, pilots and controllers'messages can indeed vary from a lexical, semantic or syntactic and discursive point of view. Some variations reflect the influence of natural language on the speakers while others can be considered as a strategy to “humanise” radiotelephony communications.
Cyber-Physical Systems form a class of complex, large-scale systems of frequently safety-critical nature in various industrial applications. Formal verification approaches can provide performance and safety guarantees for these systems. They require three elements: a formal model, a formal verification method, and a set of formal specifications. However, industrial models are typically non-formal, they are analyzed in non-formal simulation environments, and their specifications are described in non-formal natural language. In this thesis, we aim to facilitate the integration of formal verification into the industrial model-based design process. Our first key contribution is a model transformation methodology. Starting with a standard simulation model, we transform it into an equivalent verification model, particularly a network of hybrid automata. The transformation process addresses differences in syntax, semantics, and other aspects of modeling. For this class of formal models, so-called reachability algorithms can be applied to verify safety properties. An obstacle is that scalable algorithms exist for piecewise affine (PWA) models, but not for nonlinear ones. To obtain PWA over-approximations of nonlinear dynamics, we propose a compositional syntactic hybridization technique. The second key contribution is an approach to encode rich formal specifications so that they can be interpreted by tools for reachability. Herein, we consider specifications expressed by pattern templates since they are close to natural language and can be easily understood by non-expert users. We provide (i) formal definitions for select patterns that respect the semantics of hybrid automata, and (ii) monitors which encode the properties as the reachability of an error state. By composing these monitors with the formal model under study, the properties can be checked by off-the-shelf fully automated verification tools. Furthermore, we provide a semi-automated toolchain and present results from case studies conducted in collaboration with industrial partners.
Lexical items known as discourse particles («particules énonciatives») are mostly studied in languages with a strong morpho-syntactic constraint, and as a result they do not seem to fit the descriptive frameworks that do not always highlight their discourse functions. We investigate such functions in Burmese, a language that is sometimes laboured, a little too fast, as a language «with little grammar», for which it is not suitable to describe using the notions of the grammar of Indo-European languages. It is indispensable to take into account characteristics that are particular to Burmese, such as the significant role played by the discourse particles. It is important to identify their status in the language use, at syntactic as well as discourse levels. Our objective is to examine the use of a selected range of particles, in order to identify the relationship between grammar and discourse functions, and more precisely, to bring out their discourse functions in Burmese. This study, using a large corpus (over 250 000 word-syllables) of spoken discourse, consisting of different genres and by different speakers, tempts to identify sociolinguistic aspects of Burmese.
The aim of this research, which draws on a descriptive approach to translation and uses a corpus-based methodology, is to explore D.H. Lawrence's style through his use of body-related metaphors. This thesis falls into three parts: first of all, I explore different theoretical frameworks from Aristotle to more recent studies, notably those carried out by Lakoff and Johnson. This step confirms that metaphors are a relevant tool of communication. Secondly, in order to weave a link between metaphors and Lawrence's writing in The Rainbow, I examine style in Translation Studies and beyond. Metaphorising and translating are two closely related processes that revolve around a common aspect, movement. Thirdly, I conduct a qualitative and quantitative analysis of 35 excerpts from the censored and the unabridged edition of The Rainbow (1915) with their French translations by Albine Loisy (1939) and Jacqueline Gouirand-Rousselon (2002) in order to highlight convergences and divergences in the style and metaphorical representations of the body. The recurring use of metaphor in The Rainbow is a means of conceptualising Lawrence's vision of the world. Both translators had to overcome at least two challenges: to preserve the metaphorical images and to opt for a style that reflects the complexity of the Lawrencian writing, while respecting the stylistic norms of the French language. Differences in translations pave the way for new interpretations that could take shape through future retranslations.
This doctoral thesis deals with an examination of the expression of discusivecoherence from the uses of anaphors and connectives by secondary and high school chidren in Congo. How do these linguistic tools exhibit textual cohesion? To make sense of the way in which certain cohesive markers organize the discursve cohernce, a contextualized and dynamic answer is provid in three parts. In forst party, the presentation of theories, context and methodogy is made. At this level the argument centres on three chapters. The first chapter focuses on the data from type of instruction that the pupils have receved. The third chapter presents our research methodology. Several operations are identified at this level. The second party concentrates on the "anaphoric appraoches". In chapter four, an examination of pronominal anaphors is carried out. In the fifth chapter, lexical anaphors are studied. In the light of Jean Michel Adam's (2008), Georges Kleiber's theories (1994), the uses of faithful, unfaithful, hyperononic, resomption and associatives by the learners. We devote two chapters here to takle the premiment pnenomina. Chapter six deals with enumerationand conclusion markers. In chapter seven, the argument centers on the justication and opposition connectives. These two types of connective are widely used in the congolese learners'papers.
Automatic knowledge acquisition from text ideally consists in generating a structured representation of a corpus, which a human or a machine should be able to query. The first part of this dissertation studies the two main approaches to the problem: automatic terminology retrieval, and model driven knowledge acquisition. The second part studies the mostly implicit theoretical foundations of natural language processing i. E. Logical positivism and componential lexical semantics. We offer an alternative inspired from the work of charles sanders peirce, ludwig wittgenstein and georges kleiber, i. E. A semantics based on the notions of sign, usage and reference. The third part is devoted to a detailed semantic analysis of a medical corpus. Reference is studied through two notions, denomination and denotation. Denominations allow for arbitrary, preconstructed and opaque reference; denotations, for discursive, constructed and transparent reference. In the fourth part, we manually construct a detailed representation of a fragment of the corpus. The aim is to study the relevance of the theoretical analysis and to set precise objectives to the system. The fifth part focuses on implementation. It is devoted to the construction of a terminological knowledge base capable of representing a domain corpus, and sufficiently structured for use by applications in terminology or domain modelling for example. In a nutshell, this dissertation examines automatic knowledge acquisition from text from a theoretical and technical point of view, with the technology setting the guidelines for the theoretical discussions.
This document follows a multi-paper model in which each connected study contributes to answer our main research inquiry. Our first study, Chapter 2, allowed us to understand that institutional pressures have an influence into the mental adoption of S.Scan. Chapter 3 and 4 focus into trial and allow the improvement of Target method aiding the identification of information needs for S.Scan. In addition, these improvements let to introduce two new concepts to S.Scan, helping practitioners identify their scan priorities. In Chapter 5, we studied the interactions in collective targeting meetings in order to understand the contributions of such activities to develop organizational absorptive capacity. This work allowed the understanding of the main themes to be negotiated in order to ease the activity of targeting and to produce results, which represent the information needs of the organization as a whole. Finally, we present our: theoretical, research, and practice contributions.
In recent years, deep neural networks have been widely used to address sequence-to-sequence (S2S) learning. The most common models are composed of an encoder that reads the full input sequence before the decoder produces the corresponding output sequence. This implies a latency equal to the length of the input sequence. Such an approach cannot be used in a truly interactive scenario, in particular by a speech-handicapped person to communicate *orally*. The goal of this project is to develop a general methodology for incremental sequence-to-sequence mapping, with application to speech synthesis and speech enhancement.
This thesis concerns a lexical study of the Book of animals Al-Gâhiz in a diachronic perspective to trace the evolution of the verbal lexicon of Arabic. We adopted for this study a comparative approach between two states of language, Arabic medieval represented by the book of the animals and the modern Arab represented by Arabic-corpus. We, as part of this thesis focused our study on the verbal lexemes, they are indeed eminently relational lexical units, we have demonstrated their variational behavior of multiple meanings, both in synchrony in diachrony. The semantics of verbs is determined by syntactic sémantico-factors, relational acquiring the differential value according to construction. We have adopted a varied but essentially theoretical framework based on the theory of tropes (metaphor, metonymy), as well as the diagram of arguments developed by (J. Dichy: 1999-2000). This is to clarify the possible environments for each word, specifying the syntactic constructions and indicating the semantic environments through specifiers traits. We illustrate the implications of such an approach based on the changes that have suffered diachronic studied verbs, the objective being to demonstrate how buildings interact changes a verb especially in the creation of meaning. And to highlight the regularities in the change of the Arab verbal lexicon. The study of lexical or morphosyntactic variations in the texts studied, allows us to appreciate the patterns that fall in own common language of medieval, modern language, and other patterns which fall on them of the individual. In a perspective for second, to identify some aspects of language al-Gâhiz and realize its creative dynamic.
Latent linear models are powerful probabilistic tools for extracting useful latent structure from otherwise unstructured data and have proved useful in numerous applications such as natural language processing and computer vision. However, the estimation and inference are often intractable for many latent linear models and one has to make use of approximate methods often with no recovery guarantees. An alternative approach, which has been popular lately, are methods based on the method of moments. These methods often have guarantees of exact recovery in the idealized setting of an infinite data sample and well specified models, but they also often come with theoretical guarantees in cases where this is not exactly satisfied. In this thesis, we focus on moment matchingbased estimation methods for different latent linear models. Using a close connection with independent component analysis, which is a well studied tool from the signal processing literature, we introduce several semiparametric models in the topic modeling context and for multi-view models and develop moment matching-based methods for the estimation in these models. These methods come with improved sample complexity results compared to the previously proposed methods. The models are supplemented with the identifiability guarantees, which is a necessary property to ensure their interpretability. This is opposed to some other widely used models, which are unidentifiable.
Absorption, Distribution, Metabolism, Elimination (ADME) and Toxicity (Tox) properties are crucial for the success of clinical trials of a drug candidate. During this process, chemoinformatics is regularly used to predict the ADME-Tox profile of bioactive compounds and to improve their pharmacokinetic properties. In this thesis, we first built a database containing 150,000 data points for about 50 ADME-Tox properties. In order to valorize all this data, we then proposed an automatic platform for creating predictive models. This platform, called MetaPredict, has been designed to optimize each step of model development, in order to improve their quality and robustness. Third,, we promoted the statistical models using the online application of MetaPredict platform. This application has been developed to facilitate the use of newly built models, to provide a simplified interpretation of the results and to modulate the obtained observations according to the needs of the researchers. Finally, this platform provides an easy access to the ADME-Tox models for the scientific community.
The underlying technology of current speech synthesis systems is called corpus synthesis. It relies on the selection of an optimal sequence of acoustic units with respect to the synthesis context. This approach which minimizes the concatenation effort allows to generate natural speech as far as we consider only a reading style. However, the true acceptability of the speech synthesis technology depends on the ability to reproduce expressive patterns and various vocal qualities. In order to fulfill these expectations, more in depth studies in speech signal characterization must be carried out. This thesis deals with an explicit introduction of speech production mechanisms in synthesis. The first part handles decomposition of speech into a source component, namely the glottal wave, resulting from vocal fold vibration and a filter component modeling the vocal tract. To solve this deconvolution problem, we rely on an ARX-LF model which introduces in a linear speech production model prior information on the glottal wave using an LF model (Liljencrants Fant). The estimation of the model parameters in a least square sense results in a complex non-linear optimization problem. We made up an efficient method based on decoupling parameter estimation and on several algorithmic optimizations. Estimation results are very promising. First, the proposed deconvolution method leads to a better estimation of glottal closure instants compared to existing approaches. Then, estimated glottal waves have been corroborated with electroglottographic measures. The second part of this thesis deals with speech synthesis and modifications based on an ARX-LF model. A special care has been dedicated to controlling the time domain envelop of the residual signal in speech modifications. Results in fundamental frequency and duration modifications prove the relevance of the proposed method compared to other modification methods.
We create an innovative English/Arabic translation aid tool to meet the growing need for online translation tools centered on the Arabic language. This tool combines dictionaries appropriate to the specificities of the Arabic language and a bilingual concordancer derived from parallel corpora. Given its agglutinative and unvoweled nature, Arabic words require specific treatment. For this reason, and to construct our dictionary resources, we base on Buckwalter's morphological analyzer which, on the one hand, allows a morphological analysis taking into account the complex composition of the Arabic word (proclitic, prefix, stem, suffix, enclitic), and on the other hand, provides translational resources enabling rehabilitation in a translation system. Furthermore, this morphological analyzer is compatible with the approach defined around the DIINAR database (DIctionnaire Informatisé de l'Arabe - Computerized Dictionary for Arabic), which was constructed, among others, by members of our research team. In response to the contextual issue in translation, a bilingual concordancer was developed from parallel corpora. The latter represent a novel linguistic resource with multiple uses, in this case aid for translation. We therefore closely analyse these corpora, their alignment methods, and we proposed a mixed approach that significantly improves the quality of sub-sentential alignment of English-Arabic corpora. Several technologies have been used for the implementation of this translation aid tool which have been made available online (tarjamaan.com) and which allow the user to search the translation of millions of words and expressions while visualizing their original contexts. An evaluation of this tool has been made with a view to its optimization and its enlargement to support other language pairs.
In many application areas, data elements can be high-dimensional. This raises the problem of dimensionality reduction. The dimensionality reduction techniques can be classified based on their aim: dimensionality reduction for optimal data representation and dimensionality reduction for classification, as well as based on the adopted strategy: feature selection and feature extraction. The set of features resulting from feature extraction methods is usually uninterpretable. Thereby, the first scientific problematic of the thesis is how to extract interpretable latent features? The dimensionality reduction for classification aims to enhance the classification power of the selected subset of features. We see the development of the task of classification as the task of trigger factors identification that is identification of those factors that can influence the transfer of data elements from one class to another. The second scientific problematic of this thesis is how to automatically identify these trigger factors? We aim at solving both scientific problematics within the recommender systems application domain. We propose to interpret latent features for the matrix factorization-based recommender systems as real users. We design an algorithm for automatic identification of trigger factors based on the concepts of contrast analysis. Through experimental results, we show that the defined patterns indeed can be considered as trigger factors