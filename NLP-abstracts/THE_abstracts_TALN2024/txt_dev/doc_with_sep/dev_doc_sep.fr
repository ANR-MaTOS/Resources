L'hypothèse sur laquelle repose notre travail est que la comparaison de la lexicalisation des noms d'éléments du corps (dorénavant, NEC) et de la phraséologie des NEC entre deux langues va permettre de mettre en évidence des différences de conceptualisation et de culture entre deux sociétés.<sep> En fonction de cette hypothèse, notre thèse aborde deux thèmes principaux.<sep> Premièrement, nous étudions les NEC coréens (dorénavant, NECC) en nous focalisant sur les noms neutres d'éléments externes du corps humain.<sep> Les NECC ont des caractéristiques universelles : richesse lexicale, éléments du vocabulaire basique, source de l' « embodiment » , universel physio-conceptuel et nature de quasi-prédicats sémantiques.<sep> En même temps, les NECC montrent des particularités sémantiques, syntaxiques et morphologiques liées aux spécificités de la langue coréenne.<sep> La comparaison de la lexicalisation des NECC et des NEC français montre que même si les éléments du corps sont des universels physio-conceptuels, il n'y a pas de correspondance lexicale univoque entre les deux langues.<sep> Deuxièmement, nous focalisons notre attention sur la phraséologie des NECC et sa modélisation dans le Réseau Lexical du Coréen, une modélisation lexicographique formelle fondée sur une conception relationnelle du lexique.<sep> Nous bornons la phraséologie des NECC aux collocations contrôlées par les NECC (par ex. koga oddukhada, litt. nez+SUB être.haut 'avoir un nez haut et beau').<sep> Dans la phraséologie des NECC, nous prenons aussi en compte la phraséologisation dans un mot-forme (par ex. napjakko, 'nez aplati ').<sep> Nous appelons collocation morphologisée ce type de phrasème morphologique par opposition à la collocation lexicale.<sep> À partir de l'examen des collocations non seulement lexicales mais aussi morphologisées contrôlées par NEC, nous pouvons obtenir les composantes sémantiques de la définition de la base, le NEC.<sep> Après cela, nous proposons un patron universel de définition des NEC, qui est le fondement du modèle explicatif de la phraséologie des NEC.<sep> Ce modèle s'appuie sur l'hypothèse selon laquelle on peut trouver dans les définitions des NEC des composantes récurrentes.<sep> Différentes collocations (du type Magn, Ver, Bon, Real1, en termes de fonctions lexicales de la Théorie Sens-Texte) sont alors générées relativement au sémantisme de ces composantes.<sep> Finalement, nous comparons la description de la phraséologie des NECC à celle des NEC français, afin d'observer les diverses non-correspondances entre les phrasèmes des deux langues.<sep> Ce travail approfondit notre compréhension de la phraséologie aussi bien en général, qu'en tant qu'elle est appliquée au coréen et au français, et met en relief des différences culturelles encodées dans les deux langues.<sep> Il peut également trouver des applications en didactique et en traductologie.
Les travaux présentés dans cette thèse visent à faciliter le développement de ressources pour le traitement automatique des langues.<sep> Les ressources de ce type prennent des formes très diverses, en raison de l'existence de différents niveaux d'étude de la langue (syntaxe, morphologie, sémantique,.<sep> Les formalismes faisant intervenir différents types de structures, un unique langage de description n'est pas suffisant : il est nécessaire pour chaque formalisme de créer un langage dédié (ou DSL), et d'implémenter un nouvel outil utilisant ce langage, ce qui est une tâche longue et complexe.<sep> Pour cette raison, nous proposons dans cette thèse une méthode pour assembler modulairement, et adapter, des cadres de développement spécifiques à des tâches de génération de ressources langagières.<sep> Les cadres de développement créés sont construits autour des concepts fondamentaux de l'approche XMG (eXtensible MetaGrammar), à savoir disposer d'un langage de description permettant la définition modulaire d'abstractions sur des structures linguistiques, ainsi que leur combinaison non-déterministe (c'est à dire au moyen des opérateurs logiques de conjonction et disjonction).<sep> La méthode se base sur l'assemblage d'un langage de description à partir de briques réutilisables, et d'après un fichier unique de spécification.<sep> L'intégralité de la chaîne de traitement pour le DSL ainsi défini est assemblée automatiquement d'après cette même spécification.<sep> Nous avons dans un premier temps validé cette approche en recréant l'outil XMG à partir de briques élémentaires.<sep> Des collaborations avec des linguistes nous ont également amené à assembler des compilateurs permettant la description de la morphologie de l'Ikota (langue bantoue) et de la sémantique (au moyen de la théorie des frames).
Au fur et à mesure de la dégradation de la qualité de l'air en Chine, de plus en plus d'articles journalistiques et de microblogues (weibo en chinois, équivalent de tweet), provenant de sites web gouvernementaux, médiatiques, de réseaux sociaux, de forums ou de blogs, traitent le problème du « 雾霾 » (wumai en chinois, pour désigner le brouillard de pollution) en Chine sous plusieurs angles : politique, écologique, économique, sociologique, sanitaire, etc.<sep> La sémantique des thèmes abordés dans ces textes diffère sensiblement en fonction de leur genre textuel.<sep> Dans cette thèse, nous avons pour objectif d'une part, de relever les différents thèmes d'un corpus numérique traitant du wumai et spécifiquement construit à cette fin, et d'autre part, d'interpréter de façon différentielle la sémantique de ces thèmes.<sep> Dans un premier temps, nous collectons les données textuelles en langue chinoise relatives au wumai.<sep> Ces textes provenant de trois sites web chinois traditionnels et du réseau social sont divisés en quatre genres textuels.<sep> Après une série de traitements préparatoires : nettoyage, segmentation, normalisation, annotation, balisage et organisation, nous étudions les caractéristiques des quatre genres textuels du corpus à partir d'une série de variables discriminantes - hyperstructurelles, lexicales, sémiotiques, rhétoriques, modales et syntaxiques - réparties au niveau infratextuel et intratextuel.<sep> Ensuite, en nous basant sur les caractéristiques de chaque genre textuel, nous relevons les thèmes principaux exposés dans chaque genre de sous-corpus, et analysons de manière contrastive la sémantique de ces thèmes récupérés.<sep> Les résultats d'étude sont interprétés de manière quantitative et qualitative.<sep> Les analyses quantitatives s'effectuent à l'aide d'outils textométriques, les interprétations sémantiques s'inscrivent dans le cadre théorique de la sémantique interprétative (SI) proposée par Rastier (1987).
Dans cette thèse, nous introduisons un nouvel artefact interactif pour le SERP : le "Snippet sémantique".<sep> Les snippets sémantiques s'appuient sur la coexistence des deux Webs pour faciliter le transfert des connaissances aux utilisateurs grâce a une contextualisation sémantique du besoin d'information de l'utilisateur.<sep> Ils font apparaître les relations entre le besoin d'information et les entités les plus pertinentes présentes dans la page Web.
Cette recherche repose sur l'étude des problèmes relatifs à l'emploi de la majuscule dans une perspective de traitement automatique des langues en vue d'une correction automatique.<sep> L'usage des majuscules en français souffre d'une absence de norme fixe et universelle qui entraîne inévitablement leur placement aléatoire et souvent injustifié.<sep> Cette absence fait apparaître d'une part des phénomènes appelés majusculite (abus des majuscules) et minusculite (abus des minuscules) et d'autre part la présence de variantes orthographiques (la Montagne noire, la montagne Noire, la Montagne Noire, la montagne noire).<sep> Les correcticiels actuels semblent incapables de dire quelle est la bonne orthographe.<sep> Le véritable sens des majuscules tend à disparaître et leur pertinence à devenir moins évidente.<sep> Tant d'incertitudes, d'hésitations et de flottements dans les règles d'usage, tant de différences de traitement d'un ouvrage à un autre rendent toute tentative d'automatisation très difficile.<sep> Cette normalité bancale touche plus particulièrement les noms propres dits complexes ou dénominations.<sep> La solution la plus logique pour que cesse la dérive, est de normaliser l'emploi des majuscules.<sep> En nous basant sur un certain nombre d'ouvrages de référence, nous avons élaboré des règles claires et logiques régissant l'emploi de la majuscule afin de créer un modèle théorique à la base d'un système de vérification automatique des majuscules.<sep> Cette solution voit ainsi la disparition des variantes orthographiques dont l'existence constitue également un problème majeur dans la recherche en extraction de formes figées.
Suite aux récents développements de l'intelligence artificielle et aux nouvelles avancées dans le traitement du langage naturel, le nombre d'agents relationnels (chatbots) disponibles a explosé.<sep> Les systèmes existants se concentrent principalement sur l'aspect fonctionnel des chatbots : extraction des mots clefs, compréhension du langage, génération de texte en accord avec la question posée, etc.<sep> Bien que ces différents aspects soient indispensables pour le développement des chatbots, la question de l'intelligence sociale et émotionnelle est encore peu traitée.<sep> L'intelligence sociale est une qualité humaine reconnue comme l'un des atouts les plus importants pour la réussite personnelle et professionnelle [4].<sep> Ainsi, l'incorporation d'une intelligence sociale et émotionnelle dans la génération automatique de dialogue devient indispensable pour les prochaines générations de chatbot.<sep> D'un point de vue social, cette amélioration permettrait au chatbot d'adapter son langage vis à vis de l'utilisateur, lui permettant ainsi de créer une relation émotionnelle aidant à la compréhension de l'utilisateur sur le plus long terme.<sep> D'un point de vue fonctionnel, le développement de stratégies sociales permettrait d'éviter certains problèmes relationnels liés à des problèmes d'interaction (énervement, désintérêt, etc..) qui ont lieu par exemple lorsque le chatbot ne comprend pas les requêtes de l'utilisateur [17].<sep> Ces problèmes d'interaction peuvent alors mener à un abandon de la conversation (appelée également rupture d'engagement [3]) par l'utilisateur nuisant ainsi à la finalisation de la tâche visée par le chatbot.<sep> D'un point de vue opérationnel, l'absence d'intelligence émotionnelle rend les robots inutilisables dans de nombreux domaines.<sep> L'objectif de cette thèse est de se concentrer sur l'intelligence sociale et émotionnelle des chatbots.<sep> Des méthodes d'apprentissage automatique utilisant des approches telles que l'apprentissage profond, et l'apprentissage par renforcement pour le développement de chatbot socialement compétents (sélection et génération d'énoncés en langage naturel) seront étudiées.
Face au danger de la désinformation et de la prolifération de fake news (fausses nouvelles) sur le Web, la notion de véracité des données constitue un enjeu crucial.<sep> Dans ce contexte, il devient essentiel de développer des modèles qui évaluent de manière automatique la véracité des informations.<sep> De plus, la quantité d'informations disponibles sur le Web rend cette tâche quasiment impossible.<sep> Dans cette thèse, nous nous concentrons sur les modèles de découverte de la vérité.<sep> Ces approches analysent les assertions émises par différentes sources afin de déterminer celle qui est la plus fiable et digne de confiance.<sep> Plus précisément, les modèles de la littérature sont des modèles non supervisés qui reposent sur un postulat : les informations exactes sont principalement fournies par des sources fiables et des sources fiables fournissent des informations exactes.<sep> Les approches existantes faisaient jusqu'ici abstraction de la connaissance a priori d'un domaine.<sep> Nous insistons principalement sur deux approches : la prise en compte de la hiérarchisation des concepts de l'ontologie et l'identification de motifs dans les connaissances qui permet, en exploitant certaines règles d'association, de renforcer la confiance dans certaines assertions.<sep> Dans le premier cas, deux valeurs différentes ne seront plus nécessairement considérées comme contradictoires ; <sep> elles peuvent, en effet, représenter le même concept mais avec des niveaux de détail différents.<sep> Pour intégrer cette composante dans les approches existantes, nous nous basons sur les modèles mathématiques associés aux ordres partiels.<sep> Dans le second cas, nous considérons des modèles récurrents (modélisés en utilisant des règles d'association) qui peuvent être dérivés à partir des ontologies et de bases de connaissances existantes.
Cette thèse se situe dans le domaine du Traitement Automatique des Langues Naturelles.<sep> Elle vise à pouvoir garantir l'exactitude de l'analyse syntaxique et sémantique de textes en langue naturelle (des spécifications techniques ou des textes dans des langues peu dotées) et la fidélité de leur traduction dans plusieurs autres langues.<sep> Pour y parvenir, plusieurs verrous doivent être levés.<sep> D'abord, pour maîtriser la combinatoire inhérente à la complexité des langues naturelles, le développement des analyseurs, des générateurs doit être réalisé automatiquement à partir d'une description claire et vérifiable.<sep> Dans ce but, nous ferons évoluer les Grammaires Statiques de Correspondances Structurales (GSCS) pour les rendre à la fois plus intuitives et capables de produire des modules exécutables.<sep> Pour finir de garantir le sens, nous produirons toutes les analyses possibles et utiliserons un dialogue avec un humain pour désambiguïser entre ces différentes solutions.<sep> Par ailleurs, pour accélérer le développement des grammaires GSCS, nous appliquerons un traitement à base d'apprentissage profond sur des banques d'arbres existantes.<sep> Les probabilités obtenues sur les règles permettront d'organiser le dialogue de désambiguïsation de manière naturelle.<sep> Des algorithmes capables de mesurer la qualité des analyses et des traductions seront étudiés et évalués.<sep> Enfin, pour prendre en compte le caractère fortement multilingue du project de CS, nous choisirons une architecture à base de pivot sémantique.<sep> Les analyseurs réalisés produiront ainsi des graphes UNL, et les générations partiront de ces graphes pour générer les textes en langue cible.<sep> Des expérimentations avec d'autres enconvertisseurs et déconvertisseurs UNL (anglais, russe, hindi, espagnol...) permettront de vérifier la capacité d'interopérabilité d'UNL.
Cette thèse traite de la description formelle et du développement d'une grammaire électronique de la langue arabe.<sep> Ce travail est un prérequis à la création d'outils de traitement automatique de l'arabe.<sep> Cette langue présente de nombreux défis pour un traitement automatique.<sep> En effet l'ordre de mots en arabe est relativement libre, la morphologie y est riche et les diacritiques sont omis dans les textes écrits.<sep> Bien que plusieurs travaux de recherche aient abordé certaines de ces problématiques, les ressources électroniques utiles pour le traitement de l'arabe demeurent relativement rares ou encore peu disponibles.<sep> Dans ce travail de thèse, nous nous sommes intéressés à la représentation de la syntaxe (ordre des mots) et du sens de l'arabe standard moderne.<sep> Comme système formel de représentation de la langue, nous avons choisi le formalisme des grammaires d'arbres adjoints (Tree Adjoining Grammar).<sep> L'expert linguiste peut ainsi décrire la syntaxe et sémantique de la langue avec des outils d'abstraction facilitant la maintenance et l'extension de la grammaire.<sep> La grammaire ainsi décrite compte 1074 règles syntaxiques (non lexicalisées) et 27 cadres sémantiques (relations prédicatives).<sep> Cette ressource a été évaluée en analysant un corpus issu d'extraits d'un manuel scolaire d'apprentissage de l'arabe.
Cette thèse présente de nouvelles techniques pour les Constraint Games.<sep> La manière de résoudre un Constraint Game est repensée en terme de propagation de contraintes.<sep> Notre nouveau solveur ConGA est diffusé en open source.<sep> Celui-ci est plus rapide que les travaux connexes et est capable de trouver tous les équilibres de Nash, et cela même dans des jeux avec 200 joueurs voir 2000 pour certains jeux graphiques.<sep> Les aspects centralisé et décentralisé ont été étudiés.<sep> La comparaison de ces derniers est très importante pour évaluer la qualité de service dans les applications multi-utilisateurs.
Il s'agit de déterminer quel "sens" (signification) est donné à un mot en fonction du contexte.<sep> Un exemple célèbre consiste à déterminer le sens du "pen" dans le passage suivant (Bar-Hillel 1960) : "Little John was looking for his toy box. Finally, he found it. The box was in the pen. John was very happy".<sep> Le mot "pen" a de multiples significations selon la liste de WordNet (un instrument d'écriture avec une pointe d'où coule de l'encre, une enceinte pour confiner le bétail, une enceinte portable dans laquelle les bébés peuvent être laissés à jouer, un établissement correctionnel pour les personnes condamnées pour des crimes majeurs, un cygne femelle).
L'apprentissage automatique, considéré comme une partie intégrante de l'intelligence artificielle, voit son importance grandir de jour en jour, et ouvre des horizons insoupçonnés.<sep> Des structures toujours plus complexes tendent à être étudiées par ce biais, haussant ainsi l'information disponible au rang de connaissance exploitable.<sep> Ce travail de doctorat propose de valoriser un certain type de données que sont les objets (structures) 3D construits à partir de maillage, en justifiant empiriquement les apports indéniables d'une extraction de sous-parties issues de ces derniers.<sep> Cet objectif est atteint en résolvant un problème de prévision par une nouvelle approche de classification supervisée pour la recommandation d'information.<sep> Au delà du résultat attendu, une justification de ce dernier est également fournie sous forme de la visualisation de sous-parties extraites discriminantes, permettant ainsi l'interprétation par le spécialiste.<sep> Au sein du service Exploration de Total, ce besoin de classification s'applique initialement aux grandes structures 3D telles que les géo-modèles de bassins géologiques, dont les éléments pertinents tiennent effectivement de sous-parties.<sep> Lors de l'étude d'un sous-sol, les géologues cherchent, à partir de données 3D reconstituées grâce à des ondes acoustiques, à comprendre le sous-sol.<sep> Cette compréhension peut être aidée en fournissant un moyen de détecter certains types de formes au sein de ces structures.<sep> Nous proposons logiquement, afin de répondre à ce problème, un système de classification de ces structures 3D.<sep> Grâce à une adaptation des Time Series Shapelets et des méthodes de sélection de features, on parvient à ne sélectionner que les parties les plus pertinentes pour la classification souhaitée.<sep> L'idée maîtresse consiste à extraire aléatoirement un certain nombre de sous-surfaces de chaque objet 3D du jeu d'apprentissage, puis à en étudier la pertinence pour la classification souhaitée, avant d'utiliser les plus pertinents pour un apprentissage plus classique basé sur le degré d'imbrication de l'extrait dans chaque objet.<sep> En entreprise, l'absence de justification des résultats tend à assimiler l'apprentissage automatique à une boite noire.<sep> La méthode proposée, quant à elle, corrige ce problème, et permet la compréhension du résultat de l'aide à la décision fournie par la classification mise en place.<sep> En effet, en plus de présenter des résultats de prévision légèrement meilleurs que ceux de l'état de l'art, elle offre une visualisation des sous-parties d'objets 3D les plus discriminantes et donc les zones qui auront le plus d'influence sur la classification des données.<sep> Par la suite, nous proposons une amélioration de cette méthode sur deux axes : le premier est l'apport d'une adaptation du transfert de connaissances (ou transfer learning) appliqué à l'algorithme précédemment proposé ; le second est la mise en œuvre d'une méthode novatrice de sélection d'attributs, basée sur des outils issus de la théorie des sous-ensembles flous, est introduite. Cette dernière s'avère être potentiellement applicable à tout type de sélection d'attributs en classification supervisée.<sep> Cette dernière s'avère être potentiellement applicable à tout type de sélection d'attributs en classification supervisée.
À une époque où l'utilisation des données a atteint un niveau sans précédent, l'apprentissage machine, et plus particulièrement l'apprentissage profond basé sur des réseaux de neurones artificiels, a été responsable de très importants progrès pratiques.<sep> Leur utilisation est désormais omniprésente dans de nombreux domaines d'application, de la classification d'images à la reconnaissance vocale en passant par la prédiction de séries temporelles et l'analyse de texte.<sep> Pourtant, la compréhension de nombreux algorithmes utilisés en pratique est principalement empirique et leur comportement reste difficile à analyser.<sep> Ces lacunes théoriques soulèvent de nombreuses questions sur leur efficacité et leurs potentiels risques.<sep> Établir des fondements théoriques sur lesquels asseoir les observations numériques est devenu l'un des défis majeurs de la communauté scientifique.<sep> La principale difficulté qui se pose lors de l'analyse de la plupart des algorithmes d'apprentissage automatique est de traiter analytiquement et numériquement un grand nombre de variables aléatoires en interaction.<sep> Dans ce manuscrit, nous revisitons une approche basée sur les outils de la physique statistique des systèmes désordonnés.<sep> Développés au long d'une riche littérature, ils ont été précisément conçus pour décrire le comportement macroscopique d'un grand nombre de particules, à partir de leurs interactions microscopiques.<sep> Au cœur de ce travail, nous mettons fortement à profit le lien profond entre la méthode des répliques et les algorithmes de passage de messages pour mettre en lumière les diagrammes de phase de divers modèles théoriques, en portant l'accent sur les potentiels écarts entre seuils statistiques et algorithmiques.<sep> Nous nous concentrons essentiellement sur des tâches et données synthétiques générées dans le paradigme enseignant-élève.<sep> En particulier, nous appliquons ces méthodes à champ moyen à l'analyse Bayes-optimale des machines à comité, à l'analyse des bornes de généralisation de Rademacher pour les perceptrons, et à la minimisation du risque empirique dans le contexte des modèles linéaires généralisés.<sep> Enfin, nous développons un cadre pour analyser des modèles d'estimation avec des informations à priori structurées, produites par exemple par des réseaux de neurones génératifs avec des poids aléatoires.
L'adoption des Nouvelles Technologies de l'Information et de la Communication (NTIC) a permis la modernisation des méthodes d'enseignement dans les systèmes d'apprentissage en ligne comme l'e-Learning, les systèmes tutoriels intelligents, etc.<sep> Ces derniers assurent une formation à distance qui répond aux besoins des apprenants.<sep> Un aspect très important à prendre en considération dans ces systèmes est l'évaluation précoce de l'apprenant en termes d'acquisition des connaissances.<sep> En général, trois types d'évaluation et leurs relations sont nécessaires durant le processus d'apprentissage, à savoir : (i) diagnostic qui est exécuté avant l'apprentissage pour estimer le niveau des élèves, (ii) évaluation formative qui est appliquée lors de l'apprentissage pour tester l'évolution des connaissances et (iii) évaluation sommative qui est considérée après l'apprentissage pour évaluer l'acquisition des connaissances.<sep> Ces méthodes peuvent être intégrées d'une manière semi-automatique, automatique ou adaptée aux différents contextes de formation, par exemple dans le domaine d'apprentissage des langues (français, anglais, etc.), des sciences fondamentales (mathématiques, physique, chimie, etc.) et langages de programmation (java, python, sql, etc.)<sep> Cependant, les méthodes d'évaluation usuelles sont statiques et se basent sur des fonctions linéaires qui ne prennent en considération que la réponse de l'apprenant.<sep> Elles ignorent, en effet, d'autres paramètres de son modèle de connaissances qui peuvent divulguer d'autres indicateurs de performance.<sep> Par exemple, le temps de résolution d'un problème, le nombre de tentatives, la qualité de la réponse, etc.<sep> Ces éléments servent à détecter les traits du profil, le comportement ainsi que les troubles d'apprentissage de l'apprenant.<sep> Ces paramètres additionnels sont vus dans nos travaux de recherche comme des traces d'apprentissage produites par l'apprenant durant une situation ou un contexte pédagogique donné.<sep> Dans ce cadre, nous proposons dans cette thèse une approche d'évaluation de l'apprenant à base des traces d'apprentissage qui peut être exploitée dans un système d'adaptation de la ressource et/ou de la situation pédagogique.<sep> Pour l'évaluation de l'apprenant, nous avons proposé trois modèles génériques d'évaluation qui prennent en considération la trace temporelle, le nombre de tentatives et leurs combinaisons.<sep> Ces modèles ont servi, par la suite, comme métrique de base à notre modèle d'adaptation de la ressource et/ou de la situation d'apprentissage.<sep> Le modèle d'adaptation est également fondé sur les trois traces susmentionnées et sur nos modèles d'évaluation.<sep> Notre modèle d'adaptation génère automatiquement des trajectoires d'apprentissage adaptées en utilisant un modèle d'état-transition.<sep> Les états présentent des situations d'apprentissage qui consomment des ressources et les transitions entre situations expriment les conditions nécessaires à remplir pour passer d'une situation à une autre.<sep> Ces concepts sont aussi implémentés dans une ontologie du domaine et un algorithme d'adaptation a été également proposé.<sep> L'algorithme assure deux types d'adaptation : (i) Adaptation de la situation et (ii) Adaptation des ressources dans une situation.<sep> Afin de collecter les traces d'apprentissage pour la mise en œuvre de notre approche d'évaluation de l'apprenant et d'adaptation de ressources et de situations d'apprentissage, nous avons effectué des expérimentations sur deux groupes d'étudiants en Licence Informatique (L2).<sep> Un groupe en apprentissage classique et un groupe en apprentissage adapté.
La mise à disposition massive de documents via Internet (pages Web, entrepôts de données,documents numériques, numérisés ou retranscrits, etc.) rend de plus en plus aisée la récupération d'idées.<sep> Malheureusement, ce phénomène s'accompagne d'une augmentation des cas de plagiat.<sep> En effet, s'approprier du contenu, peu importe sa forme, sans le consentement de son auteur (ou de ses ayants droit) et sans citer ses sources, dans le but de le présenter comme sa propre œuvre ou création est considéré comme plagiat.<sep> De plus, ces dernières années, l'expansion d'Internet a également facilité l'accès à des documents du monde entier (écrits dans des langues étrangères) et à des outils de traduction automatique de plus en plus performants, accentuant ainsi la progression d'un nouveau type de plagiat : le plagiat translingue.<sep> Ce plagiat implique l'emprunt d'un texte tout en le traduisant (manuellement ou automatiquement) de sa langue originale vers la langue du document dans lequel le plagiaire veut l'inclure.<sep> Après avoir défini le plagiat et les différents concepts abordés au cours de cette thèse, nous établissons un état de l'art des différentes approches de détection du plagiat translingue.<sep> Nous présentons également les différents corpus déjà existants pour la détection du plagiat translingue et exposons leurs limites.<sep> Nous présentons ensuite le corpus que nous avons constitué et qui ne possède pas la plupart des limites rencontrées par les différents corpus déjà existants.<sep> Nous menons,à l'aide de ce nouveau corpus, une évaluation de plusieurs méthodes de l'état de l'art et découvrons que ces dernières se comportent différemment en fonction de certaines caractéristiques des textes sur lesquelles elles opèrent.<sep> Ensuite, nous présentons des nouvelles méthodes de mesure de similarités textuelles sémantiques translingues basées sur des représentations continues de mots(word embeddings).<sep> Nous proposons également une notion de pondération morphosyntaxique et fréquentielle de mots, qui peut aussi bien être utilisée au sein d'un vecteur qu'au sein d'un sac de mots, et nous montrons que son introduction dans ces nouvelles méthodes augmente leurs performances respectives.<sep> Nous obtenons ainsi de meilleurs résultats que l'état de l'art dans la totalité des sous-corpus étudiés.<sep> Nous terminons en présentant et discutant les résultats de ces méthodes lors de notre participation à la tâche de similarité textuelle sémantique (STS) translingue de la campagne d'évaluation SemEval 2017, où nous nous sommes classés 1er à la sous-tâche correspondant le plus au scénario industriel de Compilatio.
Les applications en Recherche d'Information Musicale et en musicologie computationnelle reposent traditionnellement sur des fonctionnalités extraites du contenu musical sous forme audio, mais ignorent la plupart du temps les paroles des chansons.<sep> Plus récemment, des améliorations dans des domaines tels que la recommandation de musique ont été apportées en tenant compte des métadonnées externes liées à la chanson.<sep> Dans cette thèse, nous soutenons que l'extraction des connaissances à partir des paroles des chansons est la prochaine étape pour améliorer l'expérience de l'utilisateur lors de l'interaction avec la musique.<sep> Pour extraire des connaissances de vastes quantités de paroles de chansons, nous montrons pour différents aspects textuels (leur structure, leur contenu et leur perception) comment les méthodes de Traitement Automatique des Langues peuvent être adaptées et appliquées avec succès aux paroles.<sep> Pour l'aspect structurel des paroles, nous en dérivons une description structurelle en introduisant un modèle qui segmente efficacement les paroles en leurs parties caractéristiques (par exemple, intro, couplet, refrain).<sep> Puis, nous représentons le contenu des paroles en résumant les paroles d'une manière qui respecte la structure caractéristique des paroles.<sep> Enfin, sur la perception des paroles,nous étudions le problème de la détection de contenu explicite dans un texte de chanson.<sep> Cette tâche s'est avèree très difficile et nous montrons que la difficulté provient en partie de la nature subjective de la perception des paroles d'une manière ou d'une autre selon le contexte.<sep> De plus, nous abordons un autre problème de perception des paroles en présentant nos résultats préliminaires sur la reconnaissance des émotions.<sep> L'un des résultats de cette thèse a été de créer un corpus annoté, le WASABI Song Corpus, un ensemble de données de deux millions de chansons avec des annotations de paroles TAL à différents niveaux.
Les entreprises sont en compétition pour mettre avant les autres leurs produits sur le marché.<sep> Dans cette course, la connaissance des caractéristiques de qualité qu'un utilisateur final souhaite pour le produit est parfois présupposée ou mal comprise.<sep> Il en résulte souvent un produit qui n'atteint pas l'objectif pour lequel il a été conçu et fabriqué.<sep> Dans ce contexte, est-il possible de guider méthodologiquement le processus de développement afin d'assurer la qualité d'un produit ?<sep> En nous référant à l'Ingénierie des Systèmes, c'est à l'étape de la définition du Concept dans le cycle de vie du système que les besoins des parties prenantes sont recueillis, traduits tout d'abord en exigences des parties prenantes puis en exigences sur le système.<sep> Cette thèse adresse donc prioritairement ces étapes.<sep> Elle propose une méthodologie visant à assurer que les besoins des parties prenantes sont bien compris et correctement traduits en exigences système.<sep> La proposition est conforme à la norme de qualité ISO 15288 (2015) et intègre les principes du Lean.<sep> La thèse propose également un outillage qui supporte la méthodologie.<sep> Les résultats obtenus sur plusieurs études de cas développés à Tecnológico Nacional de México, Instituto Tecnológico de Toluca (ITTol), Mexique, démontrent l'efficacité de la méthodologie proposée.<sep> Son utilisation augmente les chances que le produit livré réponde aux attentes des parties prenantes, réduit les changements d'exigences dus à une mauvaise identification des besoins et, par conséquent les coûts induits par ces changements, et assure une livraison plus rapide du produit sur le marché.
Il est intéressante de noter que cet étape fait le pont entre les bases de données et l'apprentissage.<sep> Dans ce contexte, nous soulevons et considérons trois problèmes liés à la sélection de données pour les modèles prédictifs.<sep> Premièrement, la base de données contient généralement plus que les données d'intérêt : comment séparer les données que l'analyste veut de celles qu'elle ne veut pas ?<sep> Nous proposons de voir ce problème comme une classification déséquilibrée entre les tuples d'intérêt et le reste de la base de données.<sep> Nous développons une méthode de sous-échantillonnage basée sur les dépendances fonctionnelles de la base de données.<sep> Deuxièmement, nous discutons de l'écriture de la requête renvoyant les tuples d'intérêt.<sep> Nous proposons une solution de complétion de requête SQL basée sur la sémantique des données, qui part d'une requête très générale, et aide un analyste à l'affiner jusqu'à ce qu'elle sélectionne ses données d'intérêt.<sep> Ce processus vise à aider l'analyste à concevoir la requête qui finira par sélectionner les données dont elle a besoin.<sep> Troisièmement, en supposant que les données ont été extraites avec succès de la base de données, on peut se poser la question suivante : les données sélectionnées sont-elles adaptées pour répondre au problème d'apprentissage considéré ?<sep> Puisque construire un modèle prédictif est équivalent à déterminer une fonction, nous soulignons qu'il est logique de d'abord évaluer l'existence de cette fonction dans les données.<sep> Cette existence peut être étudiée à travers le prisme des dépendances fonctionnelles, et nous montrons comment elles peuvent être utilisées pour comprendre les limitations d'un modèle et affiner la sélection initiale des données si nécessaire.
Dans ce travail, nous caractérisons les selon X énonciatifs comme une sous-classe des emplois médiatifs (évidentiels) de selon, et les situons par rapport aux autres emplois, exophrastiques (médiatifs) et endophrastiques de selon et à leurs principaux concurrents adverbiaux (d'après X, pour X) et verbaux (verbes introducteurs de discours rapporté).<sep> Ensuite, nous établissons les conditions dans lesquelles les selon X énonciatifs sont aptes à porter sur plusieurs phrases et passons en revue les principaux indices et réseaux d'indices intervenant dans la clôture des cadres énonciatifs en cours.<sep> L'étude, qui repose sur l'analyse systématique d'un corpus issu du Monde diplomatique, prend en considération des données quantitatives sur les différents emplois de selon X. Les résultats sont formulés de façon à permettre l'implémentation de règles dans des systèmes de Traitement Automatique des Langues Naturelles, règles propres à repérer les selon X énonciatifs dans les textes et à délimiter leur portée.
Ce projet de thèse s'inscrit dans le cadre de l'enseignement du FOS (Français sur Objectifs Spécifiques) à des cuisiniers étrangers venus travailler dans des restaurants français ou ayant choisi la restauration comme spécialité.<sep> L'objectif de notre recherche est donc d'enseigner les phrasèmes NAdj du domaine culinaire auprès d'apprenants étrangers niveau A2.<sep> L'enseignement/apprentissage de la phraséologie s'avère nécessaire dans les langues de spécialités et la haute fréquence des phrasèmes NAdj a attiré notre attention.<sep> Plusieurs questions sont alors abordées : où trouver ce lexique spécifique ?<sep> Comment les extraire ?<sep> Par quelle approche enseignons-nous les phrasèmes sélectionnés ?<sep> Enfin, nous avons proposé les trois approches d'utilisation des corpus pour l'enseignement/apprentissage des phrasèmes NAdj : approche inductive guidée, approche déductive, approche inductive pure.
Cette thèse porte sur la transformation automatique et sémantique de règles métiers en des règles formelles.<sep> Ces règles métiers sont originellement rédigées sous la forme de textes en langage naturel, de tableaux et d'images.<sep> L'objectif est de mettre à la disposition des experts métiers, un ensemble de services leur permettant d'élaborer des corpus de règles métiers formelles.<sep> Le domaine de la Construction est le champ d'application de ces travaux.<sep> Disposer d'une version formelle et exécutable de ces règles métiers servira à effectuer des contrôles de conformité automatique sur les maquettes numériques des projets de construction en cours de conception.<sep> Pour cela, nous avons mis à disposition des experts métiers les deux principales contributions de cette thèse.<sep> La première est la mise sur pied d'un langage naturel contrôlé, dénommé RAINS.<sep> Il permet aux experts métiers de réécrire les règles métiers sous la forme de règles formelles.<sep> Les règles RAINS se composent de termes du vocabulaire métier et de mots réservés tels que les fonctions de comparaisons, les marques de négation et de quantification universelle et les littéraux.<sep> Chaque règle RAINS a une sémantique formelle unique qui s'appuie sur les standards du web sémantique.<sep> La seconde contribution majeure est un service de formalisation des règles métiers.<sep> Ce service implémente une approche de formalisation proposée dans le cadre de cette thèse et dénommée FORSA.<sep> FORSA fait appel à des outils du traitement automatique du langage naturel et à des heuristiques.<sep> Pour évaluer FORSA, nous avons mis sur pied un benchmark adapté à la tâche de formalisation des règles métiers.<sep> Les données de ce benchmark sont issues de normes du domaine de la Construction
L'explosion de données relationnelles largement disponibles sous la forme de graphes de connaissances a permisle développement de multiples applications, dont les agents personnels automatiques,les systèmes de recommandation et l'amélioration des résultats de recherche en ligne.<sep> La grande taille et l'incomplétude de ces bases de données nécessite le développement de méthodes de complétion automatiques pour rendre ces applications viables.<sep> Les modèles de factorisation existants proposent différents compromis entre leur expressivité, et leur complexité en temps et en espace.<sep> Nous proposons un nouveau modèle appelé ComplEx, pour "Complex Embeddings", pour réconcilier expressivité et complexité par l'utilisation d'une factorisation en nombre complexes, dont nous explorons le lien avec la diagonalisation unitaire.<sep> Nous corroborons notre approche théoriquement en montrant que tous les graphes de connaissances possibles peuvent être exactement décomposés par le modèle proposé.<sep> Notre approche, basées sur des embeddings complexes reste simple, car n'impliquant qu'un produit trilinéaire complexe, là où d'autres méthodes recourent à des fonctions de composition de plus en plus compliquées pour accroître leur expressivité.<sep> Le modèle proposé ayant une complexité linéaire en temps et en espace est passable à l'échelle, tout en dépassant les approches existantes sur les jeux de données de référence pour la prédiction de liens.<sep> Nous démontrons aussi la capacité de ComplEx à apprendre des représentations vectorielles utiles pour d'autres tâches,en enrichissant des embeddings de mots, qui améliorentles prédictions sur le problème de traitement automatique du langage d'implication entre paires de phrases.<sep> Dans la dernière partie de cette thèse, nous explorons les capacités de modèles de factorisation à apprendre les structures relationnelles à partir d'observations.<sep> De part leur nature vectorielle, il est non seulement difficile d'interpréter pourquoi cette classe de modèles fonctionne aussi bien, mais aussi où ils échouent et comment ils peuvent être améliorés.<sep> Nous conduisons une étude expérimentale sur les modèles de l'état de l'art, non pas simplement pour les comparer, mais pour comprendre leur capacités d'induction.<sep> Pour évaluer les forces et faiblesses de chaque modèle, nous créons d'abord des tâches simples représentant des propriétés atomiques des propriétés des relations des graphes de connaissances ; <sep> À partir de ces résultats expérimentaux, nous proposons de nouvelles directionsde recherches pour améliorer les modèles existants, y compris ComplEx.
Différentes disciplines des sciences humaines telles la philologie ou la paléographie font face à des tâches complexes et fastidieuses pour l'examen des sources de données.<sep> La proposition d'approches computationnelles en humanités permet d'adresser les problématiques rencontrées telles que la lecture, l'analyse et l'archivage de façon systématique.<sep> Les modèles conceptuels élaborés reposent sur des algorithmes et ces derniers donnent lieu à des implémentations informatiques qui automatisent ces tâches fastidieuses.<sep> Notre approche propose un empilement de réseaux de neurones auto-encodeurs afin de fournir une représentation alternative des données reçues en entrée.
Des avancées significatives sur les réseaux de neurones profonds ont récemment permis le développement de technologies importantes comme les voitures autonomes et les assistants personnels intelligents basés sur la commande vocale.<sep> La plupart des succès en apprentissage profond concernent la prédiction, alors que les percées initiales viennent des modèles génératifs.<sep> Cette thèse défends le point de vue que, plutôt que d'éliminer ces nouveautés, on devrait les étudier et étudier le potentiel génératif des réseaux neuronaux pour créer de la nouveauté utile - particulièrement sachant l'importance économique et sociétale de la création d'objets nouveaux dans les sociétés contemporaines.<sep> Cette thèse a pour objectif d'étudier la génération de la nouveauté et sa relation avec les modèles de connaissance produits par les réseaux neurones profonds génératifs.<sep> Notre première contribution est la démonstration de l'importance des représentations et leur impact sur le type de nouveautés qui peuvent être générées : une conséquence clé est qu'un agent créatif a besoin de re-représenter les objets connus et utiliser cette représentation pour générer des objets nouveaux.<sep> Ensuite, on démontre que les fonctions objectives traditionnelles utilisées dans la théorie de l'apprentissage statistique, comme le maximum de vraisemblance, ne sont pas nécessairement les plus adaptées pour étudier la génération de nouveauté.<sep> On propose plusieurs alternatives à un niveau conceptuel.<sep> Un deuxième résultat clé est la confirmation que les modèles actuels - qui utilisent les fonctions objectives traditionnelles<sep> Cela montre que même si les fonctions objectives comme le maximum de vraisemblance s'efforcent à éliminer la nouveauté, les implémentations en pratique échouent à le faire.<sep> A travers une série d'expérimentations, on étudie le comportement de ces modèles ainsi que les objets qu'ils génèrent.<sep> En particulier, on propose une nouvelle tâche et des métriques pour la sélection de bons modèles génératifs pour la génération de la nouveauté.<sep> Finalement, la thèse conclue avec une série d'expérimentations qui clarifie les caractéristiques des modèles qui génèrent de la nouveauté.<sep> Les expériences montrent que la sparsité, le niveaux du niveau de corruption et la restriction de la capacité des modèles tuent la nouveauté et que les modèles qui arrivent à reconnaître des objets nouveaux arrivent généralement aussi à générer de la nouveauté.
Nous nous intéressons, dans cette thèse, à l'usage du traitement automatique des langues (TAL) dans les systèmes d'indexation d'images.<sep> Au niveau de la description du contenu sémantique des images, nous proposons une méthode d'annotation d'images basée sur l'extraction d'entités nommées pertinentes dans des textes accompagnant les images à annoter.
Le cadre général de cette thèse est l'indexation sémantique et la recherche d'informations, appliquée à des documents multimédias.<sep> Plus précisément, nous nous intéressons à l'indexation sémantique des concepts dans des images et vidéos par les approches d'apprentissage actif, que nous utilisons pour construire des corpus annotés.<sep> Tout au long de cette thèse, nous avons montré que les principales difficultés de cette tâche sont souvent liées, en général, à l'fossé sémantique.<sep> En outre, elles sont liées au problème de classe-déséquilibre dans les ensembles de données à grande échelle, où les concepts sont pour la plupart rares.<sep> Pour l'annotation de corpus, l'objectif principal de l'utilisation de l'apprentissage actif est d'augmenter la performance du système en utilisant que peu d'échantillons annotés que possible, ainsi minimisant les coûts de l'annotations des données (par exemple argent et temps).<sep> Dans cette thèse, nous avons contribué à plusieurs niveaux de l'indexation multimédia et nous avons proposé trois approches qui succèdent des systèmes de l'état de l'art : i) l'approche multi-apprenant (ML) qui surmonte le problème de classe-déséquilibre dans les grandes bases de données, ii) une méthode de reclassement qui améliore l'indexation vidéo, iii) nous avons évalué la normalisation en loi de puissance et de l'APC et a montré son efficacité dans l'indexation multimédia.<sep> En outre, nous avons proposé l'approche ALML qui combine le multi-apprenant avec l'apprentissage actif, et nous avons également proposé une méthode incrémentale qui accélère l'approche proposé (ALML).<sep> En outre, nous avons proposé l'approche de nettoyage actif, qui aborde la qualité des annotations.<sep> Les méthodes proposées ont été tous validées par plusieurs expériences, qui ont été menées et évaluées sur des collections à grande échelle de l'indice de benchmark internationale bien connue, appelés TRECVID.<sep> Enfin, nous avons présenté notre système d'annotation dans le monde réel basé sur l'apprentissage actif, qui a été utilisé pour mener les annotations de l'ensemble du développement de la campagne TRECVID en 2011, et nous avons présenté notre participation à la tâche d'indexation sémantique de cette campagne, dans laquelle nous nous sommes classés à la 3ème place sur 19 participants.
Le premier chapitre s'ouvre par un description du contexte interdisciplinaire.<sep> Ensuite, le concept de corpus est présenté en tenant compte de l'état de l'art.<sep> Plusieurs étapes clés de la construction de corpus sont retracées, des corpus précédant l'ère digitale à la fin des années 1950 aux corpus web des années 2000 et 2010.<sep> Les continuités et changements entre la tradition en linguistique et les corpus tirés du web sont exposés.<sep> Le second chapitre rassemble des considérations méthodologiques.<sep> L'état de l'art concernant l'estimation de la qualité de textes est décrit.<sep> Enfin, la visualisation de textes démontre l'intérêt de l'analyse de corpus pour les humanités numériques.<sep> Les raisons de trouver un équilibre entre analyse quantitative et linguistique de corpus sont abordées.<sep> Le troisième chapitre résume l'apport de la thèse en ce qui concerne la recherche sur les corpus tirés d'internet.<sep> La notion de prétraitement des corpus web est introduite, ses étapes majeures sont brossées.<sep> L'impact des prétraitements sur le résultat est évalué.<sep> La quatrième partie décrit l'apport de la thèse du point de vue de la construction de corpus proprement dite, à travers la question des sources et le problèmes des documents invalides ou indésirables.<sep> Enfin, les travaux sur la visualisation de corpus sont abordés : extraction de caractéristiques à l'échelle d'un corpus afin de donner des indications sur sa composition et sa qualité.
Malvoyance et cécité sont sources d'importantes difficultés de mobilité chez les personnes qu'elles touchent.<sep> Pour tenter d'alléger la charge que font peser ces difficultés, des dispositifs d'assistance variés ont été imaginés, conçus, testés, et parfois adoptés.<sep> La conception de tels dispositifs d'assistance à la mobilité se heurte à l'ampleur de la problématique, située à l'intersection de trois domaines déjà individuellement complexes : malvoyance, mobilité, et perception.<sep> Pris ensemble, ils ont permis de dresser un portrait général des dispositifs existants.<sep> À côté de ces approches assez classiques, nous proposons un modèle d'analyse des dispositifs selon la manière dont ils s'insèrent dans le processus perception / mobilité des personnes.<sep> Ce modèle présente l'intérêt de pouvoir s'appliquer, a priori, à l'ensemble des dispositifs et d'être à la fois pertinent dans leur évaluation et leur classification.<sep> Les problématiques d'autonomie et d'universalité, primordiales dans tout dispositif électronique mobile, sont explicitées et approfondies.<sep> Une nouveauté est la prise en compte des problématiques de robustesse : le 2SEES est donc construit autour de l'équilibre complexe entre autonomie, universalité, et robustesse, notions relativement peu visibles dans les travaux existants.<sep> Deux aspects de cet équilibre ont été plus spécifiquement étudiés.<sep> D'une part, la nécessité de robustesse a été mise en évidence par une analyse des affinités entre plusieurs types de capteurs d'obstacles et différents matériaux constituants potentiels d'obstacles.<sep> D'autre part, nous avons développé un prototype de fonctionnalité de localisation peu dépendante d'infrastructures, et donc déployable rapidement, ainsi qu'économe en énergie.<sep> Cette fonctionnalité repose principalement sur des capteurs embarqués (encodeur de roue, capteurs inertiels) et sur un filtre particulaire simplifié, qui estime la position de la personne en vérifiant la cohérence de trajectoires dérivées des données capteurs avec la carte du lieu.<sep> Outre la recherche de cet équilibre entre robustesse, autonomie, et universalité, nous avons développé une fonctionnalité novatrice de communication avec les objets intelligents, nommée SO2SEES.<sep> Cette fonctionnalité permet à l'utilisateur de poser des questions en langage naturel au système 2SEES, auxquelles ce dernier répond en exploitant les informations mises à disposition par les objets intelligents environnants.<sep> Afin de simplifier le système, la personne ne pose pas ses propres questions, mais les sélectionne dans un ensemble proposé en fonction du contexte formé parles objets présents et les données qu'ils mettent à disposition.
Nos travaux s'inscrivent dans le domaine des résumés linguistiques flous (RLF) qui permettent la génération de phrases en langage naturel, descriptives de données numériques, et offrent ainsi une vision synthétique et compréhensible de grandes masses d'information.<sep> Nous nous intéressons d'abord à l'interprétabilité des RLF, capitale pour fournir une vision simplement appréhendable de l'information à un utilisateur humain et complexe du fait de sa formulation linguistique.<sep> Afin de la garantir dans le cadre de la logique floue standard, nous introduisons une formalisation originale de l'opposition entre phrases de complexité croissante.<sep> Ce formalisme nous permet de démontrer que les propriétés de cohérence sont vérifiables par le choix d'un modèle de négation spécifique.<sep> D'autre part, nous proposons sur cette base un cube en 4 dimensions mettant en relation toutes les oppositions possibles entre les phrases d'un RLF et montrons que ce cube généralise plusieurs structures d'opposition logiques existantes.<sep> Nous considérons ensuite le cas de données sous forme de séries numériques et nous intéressons à des résumés linguistiques portant sur leur périodicité : les phrases que nous proposons indiquent à quel point une série est périodique et proposent une formulation linguistique appropriée de sa période.<sep> La méthode d'extraction proposée, nommée DPE pour Detection of Periodic<sep> Enfin, DPE génère des phrases comme « Environ toutes les 2 heures, l'afflux de client est important » .<sep> Des expériences sur des données artificielles et réelles confirment la pertinence de l'approche.<sep> D'un point de vue algorithmique, nous proposons une implémentation incrémentale et efficace de DPE, basée sur l'établissement de formules permettant le calcul de mises à jour des variables.<sep> Cette implémentation permet le passage à l'échelle de la méthode ainsi que l'analyse en temps réel de flux de données.<sep> Nous proposons également une extension de DPE basée sur le concept de périodicité locale permettant d'identifier les sous-séquences périodiques d'une série temporelle par l'utilisation d'un test statistique original.<sep> La méthode, validée sur des données artificielles et réelles, génère des phrases en langage naturel permettant d'extraire des informations du type « 
Cette thèse se situe à l'intersection des domaines de la recherche en linguistique et du traitement automatique de la langue.<sep> Ces deux domaines se croisent pour la construction d'outils de traitement de texte, et des applications industrielles intégrant des solutions de désambiguïsation et d'interprétation de la langue.<sep> Une tâche difficile et très peu abordée et appliqué est arrivée sur les travaux de l'entreprise Techlimed, celle de l'analyse automatique des textes écrits en arabe.<sep> De nouvelles ressources sont apparues comme les lexiques de langues et les réseaux sémantiques permettant à la création de grammaires formelles d'accomplir cette tâche.<sep> Une métadonnée importante pour l'analyse de texte est de savoir « qu'est-ce qui est dit, et que signifie-t-il ?<sep> Dans tous les cas, nos travaux de recherche ont été testés et validés dans un cadre expérimental rigoureux autour de plusieurs formalismes et outils informatiques.<sep> La mise en place de notre étude a requis la construction d'outils de traitement de texte et d'outils de recherche d'information.<sep> Ces outils ont été construits par nos soins et sont disponible en Open-source.<sep> La réussite de l'application de nos travaux à grande échelle s'est conclue par la condition d'avoir de ressources sémantiques riches et exhaustives.<sep> Nous travaux ont été redirigés vers une démarche de production de telles ressources, en termes de recherche d'informations et d'extraction de connaissances.<sep> Les tests menés pour cette nouvelle perspective ont étéfavorables à d'avantage de recherche et d'expérimentation.
Cette recherche porte sur le développement d'un outil de traitement automatique de la langue arabe standard moderne, au niveau morphologique et sémantique, avec comme objectif final l'extraction d'information dans le domaine de l'innovation technologique en entreprise.<sep> En ce qui concerne l'analyse morphologique, notre outil comprend plusieurs traitements successifs qui permettent d'étiqueter et de désambiguïser les occurrences dans les textes : une couche morphologique (Gibran 1.0), qui s'appuie sur les schèmes arabes comme traits distinctifs ; une couche contextuelle (Gibran 2.0), qui fait appel à des règles contextuelles ; et une troisième couche (Gibran 3.0) qui fait appel à un modèle d'apprentissage automatique.<sep> Notre méthodologie est évaluée sur le corpus annoté Arabic-PADT UD treebank.<sep> Les évaluations obtiennent une F-mesure de 0,92 et 0,90 pour les analyses morphologiques.<sep> Ces expérimentations montrent, entre autres, la possibilité d'améliorer une telle ressource par les analyses linguistiques.<sep> Cette approche nous a permis de développer un prototype d'extraction d'information autour de l'innovation technologique pour la langue arabe.<sep> Il s'appuie sur l'analyse morphologique et des patrons syntaxico-sémantiques.<sep> Cette thèse s'inscrit dans un parcours docteur-entrepreneur.
Le DAP permet de transcrire automatiquement un signal de parole en phonèmes, unités plus petites que les mots mais permettant potentiellement de conserver l'intelligibilité du discours.<sep> Nous mettons en évidence, dans un premier temps, l'intérêt d'intégrer l'information syllabique dans un système de DAP.<sep> La seconde partie de notre étude s'articule autour d'un moteur de détection de mots-clés basé sur le flux phonétique issu de notre DAP.<sep> Nous proposons un système simple, rapide, et robuste aux fausses alarmes, s'affranchissant d'un calcul classique du critère de maximum de vraisemblance.<sep> Nous introduisons pour cela des méthodes adaptées de gestion des erreurs de phonétisation, des phénomènes de coarticulation et de filtrage des fausses alarmes.<sep> Nous proposons en particulier des techniques d'expansion phonétique par utilisation de grammaires.<sep> Le système est évalué tout au long de cette étude par la détection de noms de pays dans le corpus de test ESTER.<sep> Nous présentons pour terminer le système complet actuellement implémenté et intégré dans la plateforme de démonstration de Orange Labs dédiée à la recherche et à la navigation dans les contenus.
Ou est-ce que l'usage ainsi que la variation de ces constructions sont mieux expliqués en termes de concurrantes dans une axe de grammaticalisation ?<sep> Les applications de ce projet sont surtoût théoriques (dans le cadre de la Linguistique Cognitive et la Grammaire des Constructions), mais des implications pourront s'avérer en didactique des langues et en traduction automatique.
Notre étude porte sur les séquences « QU- Construction_Verbale_1 que Construction_Verbale_2 » (cf. sous-titre), dans lesquelles on considère traditionnellement que la dépendance entre l'élément QU- et le V2 enchâssé se fait à longue distance.<sep> Cette construction a fait l'objet de nombreux travaux et tous concluent que des contraintes en règlent les emplois.<sep> Mais ces travaux s'appuient soit sur l'intuition des auteurs ou des locuteurs, soit sur de langues autres que le français.<sep> Dans un premier temps, notre étude, basée sur 229 occurrences relevées dans 3M de mots d'oral et 9M de mots d'écrit, cherche à vérifier les hypothèses formulées dans ces travaux (corpus-based).<sep> Certaines se révèlent justes : l'élément QU- est toujours sujet, objet ou ajout de V2, le V1 est un verbe fréquent, sémantiquement simple et de valeur modale.<sep> En revanche, il n'existe pas de formule prototypique et les V1 n'excluent pas les verbes factifs.<sep> Dans un second temps, nous examinons les propriétés syntaxiques, lexicales et pragmatiques de la CV1, jusque là peu étudiée (corpus-driven).<sep> Celle-ci se présente sous la forme « sujet pronominal + verbe » , ce qui pourrait conforter l'hypothèse de contraintes liées à la performance.<sep> Mais la constance de la forme CV1, son sens modal et sa relation avec la CV2 (difficilement pronominalisable), nous invite à considérer que l'ensemble « CV1 que CV2 » est irréductible à un enchâssement et forme un « complexe verbal » , une chaîne de verbes comparable à celle des séquences « CV1 CV2_infinitif » .<sep> La représentation par bulle proposée par Kahane (2000) rend compte de façon particulièrement naturelle du fait que l'élément QU- est un dépendant local du complexe verbal.
Les travaux de cette thèse présentent le développement d'algorithmes de classification de documents d'une part, ou d'analyse de réseaux complexes d'autre part, en s'appuyant sur la prétopologie, une théorie qui modélise le concept de proximité.<sep> Le premier travail développe un cadre pour la classification de documents en combinant une approche de topicmodeling et la prétopologie.<sep> Notre contribution propose d'utiliser des distributions de sujets extraites à partir d'un traitement topic-modeling comme entrées pour des méthodes de classification.<sep> Dans cette approche, nous avons étudié deux aspects : déterminer une distance adaptée entre documents en étudiant la pertinence des mesures probabilistes et des mesures vectorielles, et effet réaliser des regroupements selon plusieurs critères en utilisant une pseudo-distance définie à partir de la prétopologie.<sep> Le deuxième travail introduit un cadre général de modélisation des Réseaux Complexes en développant une reformulation de la prétopologie stochastique, il propose également un modèle prétopologique de cascade d'informations comme modèle général de diffusion.
A travers les tâches de perception et de production, les humains peuvent manipuler non seulement des mots et des phrases mais également des unités de plus bas niveau tels des syllabes et des phonèmes.<sep> Les études en phonétique sont principalement focalisées sur ces seconds types d'unitées.<sep> Un des objectif majeur dans ce domaine et de comprendre comment les humains acquiert et manipulent ces unités.<sep> Dans cette thèse, nous nous intéressons à cette question à travers l'utilisation de la modélisation computationnelle en réalisant des simulation informatiques à l'aide d'un modèle bayésien de la communication nommé COSMO (“Communicating Objects using Sensory-Motor Operations”).<sep> Nos études s'étendres à trois aspects.<sep> Dans une première partie, nous étudions les représentations cognitives des unités phonétiques.<sep> En comparant plusieurs conditions de développement, nous établissons qu'elles s'acquiert à travers un processus de reproduction des catégories plutôt qu'à une répétition des sons.<sep> Dans une troisième partie, nous analysons la nature des catégories phonétiques.<sep> En phonétique, il y a un débat autour du statut des syllabes vs. des phonèmes dans la communication de la parole.
L'objectif de la thèse est de développer des techniques d'apprentissage de modèles prédictifs du risque de crédit permettant d'exploiter de nouvelles sources de données afin de définir une approche Point In Time pour une estimation plus performante des risques afférents.<sep> Ces sources originales de données incluent par exemple les messages Bloomberg ou Twitter, dont le caractère éventuellement prédictif sera exploré à travers le projet.<sep> Des statistiques sur le volume de news Bloomberg citant une société existent déjà, mais ne proposent pas de traitement qualitatif du message (positif ou négatif).<sep> Aussi les méthodes développées s'adapteront à la structure particulière des données, i.e.
La thèse intitulée « vers une analyse micro-systémique en vue d'une traduction automatique thaï-français : application aux verbes sériels » , s'articule en 6 chapitres : <sep> Le premier présente les approches linguistiques et informatiques utilisées dansle domaine du traitement automatique du langage.<sep> Le deuxième aborde les caractéristiques de la langue thaïe par rapport aufrançais, les problèmes généraux de traduction thaï-français, ainsi que les modèles d'analyse du syntagme nominal du thaï.<sep> Le troisième concerne un essai d'analyse des syntagmes adjectivaux et adverbiaux du thaï.<sep> L'hypothèse est née de nos réflexionssuccessives sur les problèmes généraux de notre langue maternelle, le thaï, notamment en ce qui concerne le traitement automatique des langues.<sep> Nous avons constaté que les verbes sériels jouent un rôle particulier non seulement dans la formation lexicale, mais aussi dans l'ordre syntaxique de la phrase.<sep> Nul n'est besoin de dire combien ilspourraient faire obstacle à l'interprétation du sens, s'ils étaient mal analysés.<sep> Sur le plan quantitatif, les verbes sériels en thaï ne sont pas nombreux.<sep> Pourtant, en emploi pré ou post verbal et nominal, voire au niveau de la phrase, nous trouvons qu'ils occupent une place particulière qui mérite d'être étudiée.<sep> Le cinquième chapitre applique les résultats des chapitres 3 et 4 pour la réalisation d'un système de traduction thaï-français en « mode interactif » : nous démontrons que de telles analyses pour une traduction automatique peuvent être mieux développées en mode interactif car ainsi sont mis en évidence les problèmes qui relèvent de la différence de deux langues éloignées tant dans la formation lexicale que syntaxique.<sep> Dans notre conclusion, nous soulignons qu'un système de traduction automatique thaï-français pourrait avoir de nombreuses applications notamment dans le cadre de l'enseignement du français pour le public thaï ou l'enseignement du thaï pour le public francophone
Cette thèse s'intéresse particulièrement aux constructions coordonnées dans les rédactions des étudiants.<sep> Ainsi, les principaux objectifs de cette étude sont la construction d'un grand corpus de textes d'étudiants comportant des erreurs syntaxiques et le développement d'un outil de reconnaissance automatique des constructions coordonnées erronées.
Les organisations d'aujourd'hui ont besoin d'être plus agiles afin de survivre dans des marchés fluctuants et instables.<sep> C'est le cas particulier des processus de résolution de problèmes.<sep> La résolution de problèmes est une activité clé que les entreprises réalisent quotidiennement afin d'améliorer leur qualité et de réussir l'amélioration continue globale.<sep> Ces processus sont construits à partir des standards cadrés tels que le Plan, Do, Check, Act (PDCA), Define, Measure, Analyse, Improve, Control (DMAIC), ou le 8 Disciplines (8D)/ 9 Steps (9S).<sep> Dans ces méthodes, la généralisation et la réutilisation des connaissances sont facilitées par la standardisation.<sep> Ainsi, le besoin de processus de résolution de problèmes suffisamment structurés mais pas sur-contraints par des standards apparaît.<sep> Un tel processus doit pouvoir être reconfiguré et adapté à des situations inattendues et se baser sur des méthodes de retour d'expérience.<sep> Cette thèse décrit la proposition d'un processus agile de résolution de problèmes guidé par le retour d'expériences et les connaissances.<sep> A cet effet, le cycle de vie d'un processus agile de résolution de problèmes, basé sur les principes du Case-Based Reasoning (CBR), est proposé.<sep> Au travers des cinq étapes d'un cycle de vie agile, le processus peut être défini, réalisé et stocké dans des bases d'expériences et de connaissances spécifiques à des fins de réutilisation.<sep> L'application du modèle à un processus de résolution de problèmes dans une entreprise de traitement de surface est présentée.<sep> Le processus est analysé en déployant le cycle de vie agile.<sep> Il est montré comment la méthode standard de résolution de problèmes utilisée au sein de l'entreprise peut devenir plus agile grâce à l'application de notre méthode.
Cette thèse étudie par quels moyens les sociétés non financières cotées ont pu demeurer profitables malgré un investissement en déclin et une distribution accrue de leurs fonds aux actionnaires, à l'heure de la financiarisation.<sep> Ce faible lien entre profit et investissement est couramment dénommé le problème du profit sans l'investissement.<sep> La première partie de la thèse situe historiquement et théoriquement ce problème.<sep> Alors que la littérature sur la financiarisation se contente de montrer les effets négatifs de la distribution de fonds aux actionnaires sur l'investissement, cette thèse montre que la coexistence de hauts niveaux de profits (et de paiements financiers) avec de faibles niveaux d'investissement a été rendu possible par l'engagement simultané des sociétés non financières dans d'autres types d'activités.<sep> La solution au problème du profit sans l'investissement implique dans ce cas un déplacement des activités des sociétés non financières vers l'accumulation d'actifs et de profits financiers.<sep> Cependant, dans cette partie, nous fournissons des preuves empiriques substantielles qui rejettent cette alternative.<sep> La troisième partie de la thèse se focalise non plus sur la sphère financière mais productive, et porte sur la délocalisation de la production et l'accumulation d'actifs intangibles.<sep> Cette partie, contrairement à la précédente, fournit des résultats probants et prometteurs dans l'explication du problème du profit sans investissement.
Ces dernières années, un nombre important de technologies ont été créées pour venir en aide aux personnes ayant des difficultés pour lire des textes écrits.<sep> Les systèmes proposés intègrent des technologies de la parole (lecture à « voix haute » ) ou des aides visuelles (paramétrage et/ou mise en couleur des polices ou augmentation de l'espace entre lettres et lignes).<sep> Cependant, il est essentiel de proposer aussi des transformations sur le contenu afin d'avoir des substituts de mots plus simples et plus fréquents.<sep> Le but de cette thèse est de contribuer à un système d'aide à la lecture permettant de proposer automatiquement une version simplifiée d'un texte donné tout en gardant le même sens des mots.<sep> Le travail présenté adresse le problème de l'ambiguïté sémantique (très courant en traitement automatique des langues) et vise à proposer des solutions pour la désambiguïsation sémantique à l'aide de méthodes non supervisées et à base de connaissances provenant de ressources lexico-sémantiques.<sep> Par la suite, nous comparons divers algorithmes de désambiguïsation sémantique afin d'en tirer le meilleur.<sep> Enfin, nous présentons nos contributions pour la création d'une ressource lexicale pour le français proposant des synonymes désambiguïsés et gradués en fonction de leur niveau de difficulté de lecture et compréhension.<sep> Nous montrons que cette ressource est utile et peut être intégrée dans un module de simplification lexicale de textes.
Notre thèse s'adresse aux francophones qui veulent apprendre l'arabe, son lexique, sa syntaxe et sa sémantique.<sep> L'une des difficultés majeures que rencontre l'apprenant dès qu'il entre en contact avec notre langue, est le traitement des aspects et des temps, localement et dans l'ensemble de l'énoncé.<sep> Dans le cadre, d'une description des aspects et des temps nous sommes contraints de faire une étude générale de la morphologie verbale et des catégories grammaticales de l'arabe standard.<sep> Mais nous devons aussi travailler sur les mêmes structures en français qui est la langue source de nos apprenants afin de mettre en regard les deux systèmes linguistiques.<sep> Le second visage de ce travail est nettement plus technologique.<sep> Nous avons insisté sur les interfaces des leçons que nous présentons sur Internet afin de montrer comment elles participent de façon active à la compréhension des contenus par les apprenants.
Cette thèse s'intéresse aux méthodes d'itération sur les politiques dans l'apprentissage par renforcement à grand espace d'états avec approximation linéaire de la fonction de valeur.<sep> Nous proposons d'abord une unification des principaux algorithmes du contrôle optimal stochastique.<sep> Nous montrons la convergence de cette version unifiée vers la fonction de valeur optimale dans le cas tabulaire, ainsi qu'une garantie de performances dans le cas où la fonction de valeur est estimée de façon approximative.<sep> Nous étendons ensuite l'état de l'art des algorithmes d'approximation linéaire du second ordre en proposant une généralisation de Least-Squares Policy Iteration (LSPI) (Lagoudakis et Parr, 2003).<sep> Notre nouvel algorithme, Least-Squares [lambda] Policy Iteration (LS[lambda]PI), ajoute à LSPI un concept venant de [lambda]-Policy Iteration (Bertsekas et Ioffe, 1996) : l'évaluation amortie (ou optimiste) de la fonction de valeur, qui permet de réduire la variance de l'estimation afin d'améliorer l'efficacité de l'échantillonnage.<sep> LS[lambda]PI propose ainsi un compromis biais-variance réglable qui peut permettre d'améliorer l'estimation de la fonction de valeur et la qualité de la politique obtenue.<sep> Dans un second temps, nous nous intéressons en détail au jeu de Tetris, une application sur laquelle se sont penchés plusieurs travaux de la littérature.<sep> Tetris est un problème difficile en raison de sa structure et de son grand espace d'états.<sep> Nous proposons pour la première fois une revue complète de la littérature qui regroupe des travaux d'apprentissage par renforcement, mais aussi des techniques de type évolutionnaire qui explorent directement l'espace des politiques et des algorithmes réglés à la main.<sep> Nous constatons que les approches d'apprentissage par renforcement sont à l'heure actuelle moins performantes sur ce problème que des techniques de recherche directe de la politique telles que la méthode d'entropie croisée (Szita et Lorincz, 2006).<sep> Nous expliquons enfin comment nous avons mis au point un joueur de Tetris qui dépasse les performances des meilleurs algorithmes connus jusqu'ici et avec lequel nous avons remporté l'épreuve de Tetris de la Reinforcement Learning Competition 2008
Le langage mathématique courant et les langages mathématiques formels sont très éloignés.<sep> Par "langage mathématique courant" nous entendons la prose que le mathématicien utilise tous les jours dansses articles et ses livres.<sep> C'est une langue naturelle avec des expressions symboliques et des notations spécifiques.<sep> Cette langue est à la fois flexible et structurée mais reste sémantiquement intelligible par tous les mathématiciens.<sep> Cependant, il est très difficile de formaliser automatiquement cette langue.<sep> Les raisons principales sont : la complexité et l'ambiguïté des langues naturelles en général, le mélange inhabituel entre langue naturelle et notations symboliques tout aussi ambiguë et les sauts dans le raisonnement qui sont pour l'instant bien au-delà des capacités des prouveurs de théorèmes automatiques ou interactifs.<sep> Pour contourner ce problème, les assistants de preuves actuels utilisent des langages formels précis dans un système logique bien déterminé, imposant ainsi de fortes restrictions par rapport aux langues naturelles.<sep> En général ces langages ressemblent à des langages de programmation avec un nombre limité de constructions possibles et une absence d'ambiguïté.<sep> Ainsi, le monde des mathématiques est séparé en deux, la vaste majorité qui utilise la langue naturelle et un petit nombre utilisant aussi des méthodes formelles comme des assistants de preuves.<sep> Pour résoudre ce problème, on peut se demander : est-il possible d'écrire un programme qui comprend la langue naturelle mathématique et qui la traduit vers un langage formel afin de permettre sa validation ?<sep> Ce problème se subdivise naturellement en deux sous-problèmes tous les deux très difficiles : 1. l'analyse grammaticale des textes mathématiques et leur traduction dans un langage formel, 2. la validation des preuves écrites dans ce langage formel.<sep> Le but du projet MathNat (Mathematics in controlled Natural languages) est de faire un premier pas pour répondre à cette question très difficile, en se concentrant essentiellement sur la première question.<sep> Pour cela, nous développons CLM (Controlled Language for Mathematics) qui est un sous-ensemble de l'anglais avec une grammaire et un lexique restreint, mais qui inclut tout de même quelques ingrédients importants des langues naturelles comme les pronoms anaphoriques, les références, la possibilité d'écrire la même chose de plusieurs manières, des adjectifs distributifs ou non,...<sep> Pour le second problème, nous avons réalisé une petite expérience en traduisant MathAbs vers une liste de formules en logique du premier ordre dont la validité garantit la correction de la preuve.
Dans ce mémoire, nous présentons outre les résultats de recherche en vue d'un système de traduction automatique français–chinois, les apports théoriques à partir de la théorie SyGULAC et de la théorie micro-systémique avec ses calculs ainsi que les méthodologies élaborées tendant à une application sure et fiable dans le cadre de la traduction automatique.<sep> L'application porte sur des domaines de sécurité critique tels que l'aéronautique, la médecine, la sécurité civile.<sep> Nous expliquons ensuite les problématiques rencontrées au cours de notre recherche.<sep> L'ambigüité, obstacle majeur pour la compréhensibilité et la traductibilité d'un texte, se situe à tous les niveaux de la langue : syntaxique, morphologique, lexical, nominal ou encore verbal.<sep> L'identification des unités d'une phrase est aussi une étape préalable à la compréhension globale, que cela soit pour un être humain ou un système de traduction.<sep> Nous dressons un état des lieux de la divergence entre la langue française et la langue chinoise en vue de réaliser un système de traduction automatique.<sep> Nous essayons d'observer la structure aux niveaux verbal, nominal et lexical, de comprendre leurs liens et leurs interactions.<sep> Egalement nous définissons les obstacles sources d'entrave à la réalisation de cette recherche, avec un point de vue théorique mais aussi en étudiant notre corpus concret.<sep> Le formalisme pour lequel nous avons opté part d'une étude approfondie de la langue utilisée dans les protocoles de sécurité.<sep> Une langue ne se prête au traitement automatique que si elle est formalisée.<sep> De ce fait, nous avons procédé à l'analyse de plusieurs corpus bilingues français/chinois mais aussi monolingues émanant d'organismes de sécurité civile.<sep> Le but est de dégager les particularités linguistiques (lexicales, syntaxiques, …) qui caractérisent la langue de la sécurité en général et de recenser toutes les structures syntaxiques qu'utilise cette langue.<sep> Après avoir présenté la formalisation de notre système, nous montrons les processus de reconnaissance, de transfert et de génération.
Dans le cadre du projet « InterPhonologie du Français Contemporain » (IPFC) (Racine, Detey et Kawaguchi 2012) ce travail se propose d'examiner les stratégies d'acquisition de la liaison en L2.<sep> Si des modèles développementaux ont été proposés pour l'acquisition de la liaison en L1, aucune hypothèse pour rendre compte de l'apprentissage en L2 n'a reçu à ce jour un appui empirique convaincant (Wauquier 2009).<sep> Cette approche a permis de mettre en évidence la présence de tendances et d'erreurs communes qui suggèrent que, malgré le recours à des stratégies lexicales, les apprenants sont en mesure de développer des généralisations phonologiques de la liaison.<sep> De plus, compte-tenu des difficultés que l'hétérogénéité de la liaison pose dans l'enseignement et l'acquisition en français L2 (Racine et Detey sous presse), les apprenants montrent des faiblesses tant au niveau de la production de la liaison qu'au niveau des connaissances épilinguistiques (Gombert 1990) de la variation de la liaison.
Cette thèse analyse la transition topicale en anglais américain à l'aide d'un corpus audio de conversations spontanées entre proches.<sep> L'objet d'étude principal est l'action interactionnelle qui consiste à changer de topique discursif, ainsi que les diverses stratégies linguistiques que les participants ont à leur disposition.<sep> Trois modalités de marquage sont prises en compte : les questions, les marqueurs de discours, et le registre de la voix.<sep> Chaque modalité est analysée pour sa contribution individuelle, ainsi que pour les associations avec d'autres modalités qu'elle peut occasionner.<sep> Se pencher sur différentes modalités de marquage crée une vue d'ensemble composite de l'influence que la trajectoire topicale d'une conversation a sur sa grammaire et sa prosodie.<sep> Dans le cadre d'une approche mixte mêlant analyses qualitatives et quantitatives, cette étude se situe à la croisée de plusieurs cadres théoriques, empruntant tant à l'analyse conversationnelle et à la linguistique interactionnelle pour l'analyse qualitative située, qu'à la linguistique de corpus de par ses méthodes quantitatives telles que le codage systématique des données et le recours aux statistiques.<sep> Ce projet multi-domaines est complété par une comparaison entre conversations typiques et atypiques.<sep> Les personnes schizophrènes peuvent connaître des difficultés dans la gestion des topiques d'une conversation, ce qui peut occasionner des transitions non-canoniques.<sep> Comparer ce type de données à celles de participants typiques apporte un éclairage supplémentaire sur certaines des attentes, préférences et standards, par ailleurs moins visibles lorsque la transition topicale est plus aisée.
L'apathie est un trouble neuropsychiatrique qui se caractérise par une réduction quantifiable et significative des comportements dirigés vers un but dans trois domaines : les cognitions/comportements, les émotions et les interactions sociales.<sep> Les Technologies de l'Information et de la Communication (TIC) offrent la possibilité de développer de nouveaux outils d'évaluation notamment par le biais du phénotypage numérique ou digital phenotyping. Ce dernier consiste à extraire un profil ou une signature numérique à partir de données de mesures physiologiques.<sep> Ces supports offrent la possibilité d'intégrer entre autres des questionnaires ou des tests cognitifs numérisés et de les associer aux données capteurs.<sep> Dans le cadre de ce travail de thèse, nous avons recensé les méthodes d'évaluation déjà existantes de l'apathie et exploré de nouvelles méthodes en employant les TIC.<sep> La première étude est centrée sur le développement d'une application évaluant les centres d'intérêts des patients par le biais d'un serious game (jeu ludique).<sep> Les résultats ont montré que les sujets apathiques avaient significativement moins de centres d'intérêts que les sujets non apathiques.<sep> Dans une seconde étude, nous avons exploré les marqueurs vocaux acoustiques à partir d'une tâche de discours émotionnel.<sep> Enfin, dans la dernière étude, nous avons exploré les marqueurs faciaux impliqués dans l'apathie dans la même tâche que l'étude précédente.<sep> Nous avons pu extraire automatiquement l'intensité et la fréquence de 17 expressions faciales ou Action Units (AU).<sep> Les résultats ont montré que, globalement, plus les symptômes de réduction d'affect sont importants moins sont intenses les expressions faciales.<sep> En comparant les AUs un à un, les résultats sont différents en fonction de la valence de l'histoire (positive ou négative) et du sexe.<sep> Plusieurs AUs émotionnels et non émotionnels sont impliqués au niveau de la partie supérieure (ex. froncement des sourcils) et inférieure (ex. plissement des lèvres) du visage, certains sont également impliqués dans la dépression (le sourire).<sep> Ces études portent toutes les trois sur de nouvelles mesures et nécessitent des recherches supplémentaires avec de plus gros échantillons pour valider nos résultats.<sep> Elles devront impliquer également des méthodes d'évaluations différentes (auto-questionnaire, mesures physiologiques telles que les données d'électroencéphalogramme, imagerie cérébrale, conductance cutanée) afin de s'assurer que ces résultats sont spécifiques à l'apathie.
L'objectif de cette thèse est de lever les verrous que constituent le manque de disponibilité de ressources ou d'outils TAL pour la langue arabe dans les domaines de spécialité en proposant des méthodes permettant l'extraction de termes à partir de textes en arabe standard moderne.<sep> Dans ce contexte, nous avons d'abord construit un corpus parallèle anglais-arabe dans un domaine de spécialité.<sep> Il s'agit d'un ensemble de textes médicaux produits par la bibliothèque nationale de médecine américaine (NLM).<sep> Par la suite, nous avons proposé des méthodes d'acquisition terminologique, permettant d'extraire des termes ou d'acquérir des relations entre ces termes, pour la langue arabe en se basant sur : i)adaptation d'un extracteur terminologique existant pour la languefrançaise ou anglaise, ii) l'exploitation de la translittération des termes anglais en caractères arabes et iii) l'application de la la notion de transfert translingue.<sep> Appliqué au niveau terminologique, le transfert consiste à mettre en œuvre un processus d'extraction de termes ou d'acquisition de relations entre termes sur des textes d'une langue source (ici, le français ou l'anglais) puis à transférer les informations extraites sur des textes d'une langue cible (ici, l'arabe standard moderne) pour ainsi identifier le même type d'informations terminologiques.<sep> Nous avons évalué les listes de termes monolingues et bilingues obtenues lors des différentes expériences que nous avons réalisées, suivant une méthode transparente, directe et semi-automatique : les termes candidats extraits sont confrontés à une terminologie de référence avant d'être vérifiés manuellement.<sep> Cette évaluation suit un protocole que nous avons proposé.
Nous étudions dans cette thèse la construction mutualisée de systèmes de reconnaissance et de synthèse de la parole pour de nouvelles langues, avec un objectif de performance et de rapidité de développement.<sep> Le développement rapide des technologies vocales pour de nouvelles langues anime des ambitions scientifiques et est aujourd'hui considéré comme stratégique par les acteurs industriels.<sep> Cependant, le développement des langues est conduit de manière morcelée par quelques centres de recherche travaillant chacun sur un nombre réduit de langues.<sep> Or, ces technologies partagent de nombreux points communs.<sep> Notre étude se concentre sur la construction et la mutualisation d'outils pour la création de lexiques, l'apprentissage de règles de phonétisation et l'exploitation de données imparfaites.<sep> Nos contributions portent sur la sélection de données pertinentes pour l'apprentissage de modèles acoustiques, le développement conjoint de phonétiseurs et de lexiques de prononciation pour la reconnaissance et la synthèse de la parole, et l'exploitation de modèles neuronaux pour la transcription phonétique à partir du texte et du signal de parole.<sep> De plus, nous présentons une approche de détection automatique des erreurs de transcriptions phonétiques dans les bases de données annotées de signal de parole.<sep> Cette étude a montré qu'il était possible de réduire de manière importante la quantité de données à annoter manuellement lors du développement de nouveaux systèmes de synthèse de la parole.<sep> Cela contribue naturellement à réduire le temps de collecte de données pour la création de nouveaux systèmes.
Cet élaborât veut donner un nouvel approche à l'étude du corpus jābirien, en prenant compte de la littérature et des études précédents et des problématiques inhérentes à ce particulier corpus (synonymie, polysémie, dispersion du savoir, citations d'autres auteurs, hypertextualité).<sep> La première section de la thèse comprend une introduction historique, prenant compte de la figure de Jābir et de son travail, de la querelle sur son existence et de l'histoire de la science Arabe médiévale ; et méthodologique, qui explique les outils utilisés pour l'analyse.<sep> Le noyau de ce travail est représenté cependant par les Appendices, divisées en quatre parties : <sep> L'Appendice D est un échantillon de concordances qui se base sur la lemmatisation des deux premiers livres du Tadbīr.<sep> L'Appendice E est une liste de fréquence du même échantillon utilisé pour la création des concordances.
L'architecture proposée utilise le concept d'Ontologie du domaine pour la description de l'environnement.<sep> Nous avons choisi d'utiliser l'outil open source PROTEGE qui va nous permettre de définir l'ontologie ainsi que les moteurs de fusion et de fission.<sep> Les entrées multimodales seront fusionnées puis subdivisées en tâches élémentaires et envoyées comme commandes au fauteuil roulant muni d'un bras manipulateur.<sep> Cette architecture sera validée par des spécifications et des simulations via des réseaux de Pétri temporels et stochastiques.
Les problèmes éthiques liés à l'arrivée de formes d'intelligence artificielles différentes a sollicité beaucoup d'attention aussi bien académique que publique.<sep> Cependant, ces inquiétudes se concentrent sur un problème particulier : comment assurer que les décisions prises par les agents artificiels comme des voitures autonomes ne nuisent pas aux êtres humains présents dans leur environnement ?<sep> Cette question a incité la création de ceux qui sont communément appelés les agents moraux artificiels dans la littérature, la prise de décision desquels est contrainte par une moralité artificielle : un système de principes normatifs implémenté dans le processus de raisonnement de la machine.<sep> A ce jour, la forme que prend cette moralité artificielle relève de deux approches différentes : soit une forme maximalement éthique, qui dépend de l'implémentation stricte des théories morales préexistantes comme la déontologie Kantienne ou l'Utilitarisme, soit une forme minimaliste, qui applique des techniques de l'IA stochastique à l'analyse et agrégation de données portant sur les préférences morales d'une population, afin d'en tirer des principes généraux mobilisés ensuite dans la prise de décision des machines.<sep> Nous proposons une approche alternative à la moralité artificielle, la théorie des valences éthiques, qui s'efforce d'accommoder ce genre de pondération, et nous l'appliquons au cas du véhicule autonome.
Dans cette thèse, nous étudions les changements lexico-sémantiques : les variations temporelles dans l'usage et la signification des mots, également appelé extit{diachronie}.<sep> Ces changements reflètent l'évolution de divers aspects de la société tels que l'environnement technologique et culturel.<sep> Nous explorons et évaluons des méthodes de construction de plongements lexicaux variant dans le temps afin d'analyser l'évolution du language.<sep> Nous utilisont notamment des plongements contextualisés à partir de modèles de langue pré-entraînés tels que BERT.<sep> Nous proposons plusieurs approches pour extraire et agréger les représentations contextualisées des mots dans le temps, et quantifier leur degré de changement sémantique.<sep> En particulier, nous abordons l'aspect pratique de ces systèmes : le passage à l'échelle de nos approches, en vue de les appliquer à de grands corpus ou de larges vocabulaire ; leur interprétabilité, en désambiguïsant les différents usages d'un mot au cours du temps ; et leur applicabilité à des problématiques concrètes, pour des documents liés au COVID19 et des corpus du domaine financier.<sep> Nous évaluons l'efficacité de ces méthodes de manière quantitative, en utilisant plusieurs corpus annotés, et de manière qualitative, en liant les variations détectées dans des corpus avec des événements de la vie réelle et des données numériques.<sep> Enfin, nous étendons la tâche de détection de changements sémantiques au-delà de la dimension temporelle.<sep> Nous l'adaptons à un cadre bilingue, pour étudier l'évolution conjointe d'un mot et sa traduction dans deux corpus de langues différentes ; et à un cadre synchronique, pour détecter des variations sémantiques entre différentes sources ou communautés en plus de la variation temporelle.
Sachant qu'une grande partie des offres d'emplois et des profils candidats est en ligne, le e-recrutement constitue un riche objet d'étude.<sep> Déduire de tels attributs structurés à partir de donnée textuelle brute est le problème abordé dans cette thèse, sous le nom de normalisation.<sep> Ce traitement traduit donc chaque document en un language commun, ce qui permet d'agréger l'ensemble des données dans un format exploitable et compréhensible.<sep> Quelle structure de nomenclature est la plus adaptée à la normalisation, et comment l'exploiter ?<sep> Est-il possible de construire automatiquement une telle nomenclature de zéro, ou de normaliser sans en avoir une ?<sep> Pour illustrer le problème de la normalisation, nous allons étudier par exemple la déduction des compétences ou de la catégorie professionelle d'une offre d'emploi, ou encore du niveau d'étude d'un profil de candidat.<sep> Un défi du e-recrutement est que les concepts évoluent continuellement, de sorte que la normalisation se doit de suivre les tendances du marché.<sep> A la lumière de cela, nous allons proposer un ensemble de modèles d'apprentissage statistique nécessitant le minimum de supervision et facilement adaptables à l'évolution des nomenclatures.<sep> Les questions posées ont trouvé des solutions dans le raisonnement à partir de cas, le learning-to-rank semi-supervisé, les modèles à variable latente, ainsi qu'en bénéficiant de l'Open Data et des médias sociaux.<sep> Les différents modèles proposés ont été expérimentés sur des données réelles, avant d'être implémentés industriellement.<sep> La normalisation résultante est au coeur de SmartSearch, un projet qui fournit une analyse exhaustive du marché de l'emploi.
Le défi que posent les EP dans ce contexte, par rapport aux expressions linguistiques régulières, provient de leurs propriétés parfois inattendues qui les rendent difficiles à gérer dans te traitement automatique des langues.<sep> Dans nos travaux, nous montrons qu'il est pourtant possible de profiter de ce cette caractéristique des EP afin d'améliorer les résultats d'analyse syntaxique.<sep> Notamment, avec les grammaires d'arbres adjoints (TAGs), qui fournissent un cadre naturel et puissant pour la modélisation des EP, ainsi qu'avec des stratégies de recherche basées sur l'algorithme A*, il est possible d'obtenir des gains importants au niveau de la vitesse sans pour autant détériorer la qualité de l'analyse syntaxique.<sep> Cela contraste avec des méthodes purement statistiques qui, malgré l'efficacité, ne fournissent pas de solutions satisfaisantes en ce qui concerne les EP.<sep> Nous proposons un analyseur syntaxique novateur qui combine les grammaires TAG avec La technique A*, axé sur la prédiction des EP, dont les fonctionnalités permettent des applications à grande échelle, facilement extensible au contexte probabiliste.
Les données relationnelles sont omniprésentes dans la nature et leur accessibilité ne cesse d'augmenter depuis ces dernières années.<sep> Ces données, vues comme un tout, forment un réseau qui peut être représenté par une structure de données appelée graphe où chaque nœud du graphe est une entité et chaque arête représente une relation ou connexion entre ces entités.<sep> Les réseaux complexes en général, tels que le Web, les réseaux de communications ou les réseaux sociaux sont connus pour exhiber des propriétés structurelles communes qui émergent aux travers de leurs graphes.<sep> Dans cette thèse, nous mettons l'accent sur deux importantes propriétés appelées *homophilie* et *attachement préférentiel* qui se produisent dans un grand nombre de réseaux réels.<sep> Dans une première phase, nous étudions une classe de modèles de graphes aléatoires dans un contexte Bayésien non-paramétrique, appelé *modèle de composition mixée*, et nous nous concentrons à montrer si ces modèles satisfont ou non les propriétés mentionnées, après avoir proposé des définitions formelles pour ces dernières.<sep> Nous conduisons ensuite une évaluation empirique pour mettre à l'épreuve nos résultats sur des jeux de données de réseaux synthétiques et réels.<sep> Dans une seconde phase, nous proposons un nouveau modèle, qui généralise un précédent modèle à composition mixée stochastique, adapté pour les réseaux pondérés et nous développons un algorithme d'inférence efficace capable de s'adapter à des réseaux de grande échelle.
Cette thèse a pour sujet la caractérisation et l'identification des accents étrangers en français.<sep> Combien d'accents un natif français peut-il reconnaître et quels sont les indices qu'il utilise ?<sep> Notre intérêt se porte sur les productions en français de locuteurs anglais, allemand, arabes, espagnols, italiens et portugais, également comparés avec des natifs du français.<sep> L'objectif est d'identifier grâce au traitement automatique de la parole les indices acoustiques les plus fiables pour distinguer entre les accents et mettre en relation ces indices avec la perception humaine.<sep> Nous avons mesuré des paramètres acoustiques tels que la durée et le taux de voisement pour les consonnes, les valeurs des deux premiers formants pour les voyelles, le nombre de e muets réalisés, l'allongement de la voyelle précédant un e muet, le différence de fréquence fondamentale entre la voyelle pénultième et le e muet final, ainsi que les taux de confusions obtenus lors des alignements automatiques avec variantes de prononciation non standard.<sep> Des techniques d'apprentissage automatique ont été utilisées pour sélectionner les traits les plus discriminants afin de différencier entre les différents accents.<sep> Les résultats d'identification automatique pour la discrimination entre 7 origines linguistiques sont comparables avec les résultats obtenus lors de tests perceptifs.<sep> Autant lors des tests perceptifs qu'en identification automatique, les locuteurs français sont les mieux reconnus.<sep> Les traits spécifiques à chacun des accents étudiés peuvent être utilisées en reconnaissance automatique, dans la perspective de diminuer les taux d'erreurs.
Nous avons tenté d'explorer la notion de style du traducteur en articulant les analyses traductologiques et les méthodes de la textométrie multilingue (méthodes d'analyse quantitatives textuelles appliquées à des corpus de textes alignés).<sep> Notre corpus d'étude est constitué par trois traductions chinoises d'une oeuvre littéraire française, Jean-Christophe de Romain Rolland (1904-1917), réalisées respectivement par Fu Lei (1952-1953), Han Hulin(2000) et Xu Yuanchong (2000).<sep> Après une description des difficultés inhérentes à la construction d'un corpus parallèle français-chinois, nous effectuons successivement diverses mesures textométriques sur ce corpus, dans le but de mettre en évidence des usages lexicaux et syntaxiques propres à chacun des traducteurs.<sep> La remise en contexte dans le corpus parallèle des différences statistiques des phénomènes linguistiques entre traductions et l'examen des facteurs socioculturels relatifs à chacune des époques font ressortir des indicateurs du style de chaque traducteur.<sep> La recherche détaillée de type traductologique, portant sur les particules chinoises, appuyée sur des comparaisons textométriques, fournit une série d'indices révélant des approches spécifiques à chacun des traducteurs dans son travail.<sep> Les résultats de cette enquête, menée à travers la comparaison des trois versions chinoises entre elles, puis avec le texte original français jettent les bases d'une proposition de modèle d'analyse centré sur le style du traducteur.<sep> Nous pensons que notre travail ouvre une voie à une exploration scientifique et systématique de la notion de style du traducteur dans le cadre traductologique.
Cette thèse étudie les relations entre les pratiques d'écriture en ligne et le genre sur le site communautaire américain Reddit.<sep> Elle s'appuie sur un corpus de près de 20 millions de tokens comprenant les commentaires en anglais de 1044 internautes, qui inclut les contributions de 300 personnes transgenres et non binaires.<sep> Les variables linguistiques étudiées comprennent 11 variations par rapport à la langue écrite standard : 6 procédés d'ajout (émoticônes, émojis, étirements de lettres, étirements de ponctuation, mots en majuscules et interjections) et 5 procédés de réduction (abréviations, graphies phonétiques, g-droppings, omissions d'apostrophe et omissions de la majuscule du pronom personnel I).<sep> Les analyses, qui s'appuient principalement sur la méthode de la régression multiple, montrent notamment que femmes et hommes transgenres s'alignent rarement sur les femmes et hommes cisgenres.<sep> Nos résultats suggèrent ainsi que les femmes afro-américaines et hispaniques jouent un rôle de premier plan dans la diffusion des formes innovantes de la CMC.
Cette thèse se situe à la frontière des domaines du traitement automatique de la parole et de la recherche d'informations multimédia.<sep> Ces dernièn une nouvelle tâche est apparue dans le domaine du traitement automatique de la parole : la transcription enrichie d'un document audio.<sep> Parmi les informations extra-linguistiques transportées par la parole, une meta-donnée importante pour la transcription enrichie concerne l'information sur l, des phrases parlées (c'est-à-dire les phrases sont-elles du type interrogatif ou affirmatif ou autre).<sep> Notre étude a principalement porté sur la différ prosodique entre les phrases de type affirmatif et de type interrogatif pour les langues française et vietnamienne, la détection et la classification a du type de phrase pour chacune des deux langues et la comparaison des stratégies spécifiques à chacune des deux langues.<sep> Nous avons ainsi réalisé un système de segmentation et détection automatique de type de phrases basé sur l"information prosodique et sur l"information lexicale.<sep> Le système a été validé sur des corpus de parole spontanée de la vie courante qui sont l'enregistrement de conversations téléphoniques entre un client et une agence de tourisme, des entretiens d'embauche ou des réunions de projet.<sep> Cette première étude sur la langue française, nous avons élargit notre recherche en travaillant sur la langue vietnamienne, une langue où les étud, sur le système prosodique sont encore toutes préliminaires.<sep> Nous avons d'abord poursuivi une étude pour identifier la différence prosodique entre phrases interrogatives et affirmatives à la fois sur le plan de production et sur le plan de perception.<sep> Ensuite, sur la base de ces résultats, un mot, classification a été construit.
La théorie de l'argumentation abstraite de Dung est un formalisme permettant d'utiliser un système d'argumentation afin de représenter des informations conflictuelles.<sep> Des sémantiques à base d'extensions ont d'abord été introduites dans le but de déterminer quels arguments peuvent être conjointement acceptés.<sep> Cependant, ces sémantiques ne sont pas appropriées pour certaines applications, c'est pourquoi des sémantiques à base de classement, permettant de classer les arguments du plus acceptable au moins acceptable, ont été introduites.<sep> Le but de cette thèse est donc de proposer et d'étudier ces sémantiques à base de classement dans le contexte de l'argumentation abstraite.<sep> Nous définissons d'abord une nouvelle famille de sémantiques à base de classement basées sur un principe de propagation permettant de contrôler l'influence des arguments non-attaqués sur l'acceptabilité des arguments.<sep> Nous étudions les propriétés de ces sémantiques, les relations entre elles ainsi qu'avec d'autres sémantiques existantes.<sep> Nous proposons ensuite deux méthodes pour comparer les sémantiques à base de classement.<sep> La première est une comparaison empirique sur des systèmes d'argumentation générés aléatoirement donnant un aperçu des similitudes et des différences entre ces sémantiques.<sep> La seconde est une comparaison axiomatique de toutes ces sémantiques à la lumière des propriétés proposées visant à mieux comprendre le comportement de chaque sémantique.<sep> Enfin, nous remettons en question la capacité des sémantiques existantes à capturer certains principes de persuasion et introduisons une nouvelle sémantique paramétrée à base de classement plus appropriée pour ce contexte précis.
Stimulé par des applications comme l'annotation de documents ou d'images, l'apprentissage multi-label a connu un fort développement cette dernière décennie.<sep> Mais les algorithmes classiques se heurtent aux nouveaux volumes des données multi-label extrême (XML) où le nombre de labels peut atteindre le million.<sep> Cette thèse explore trois directions pour aborder la complexité en temps et en mémoire du problème : la réduction de dimension multi-label, les astuces d'optimisation et d'implémentation et le découpage arborescent.<sep> Elle propose d'unifier les approches de réduction à travers une typologie et deux formulations génériques et d'identifier des plus performantes avec une méta-analyse originale des résultats de la littérature.<sep> Une nouvelle approche est développée pour analyser l'apport du couplage entre le problème de réduction et celui de classification.<sep> Pour réduire la complexité mémoire en maintenant les capacités prédictives, nous proposons également un algorithme d'estimation des plus grands paramètres utiles d'un modèle classique de régression one-vs-rest qui suit une stratégie inspirée de l'analyse de données en flux.<sep> Enfin, nous présentons un nouvel algorithme CRAFTML qui apprend un ensemble d'arbres de décision diversifiés.<sep> Les apports de la thèse sont complétés par la présentation d'un outil logiciel VIPE développé avec Orange Labs pour l'analyse d'opinions multi-label.
En parcourant l'histoire des deux langues malaise et sanscrite, nous avons non seulement mis encore plus en lumière que le malais puise dans la langue sanscrite mais également remarqué que les mots sanscrits empruntés ont subi des modifications au niveau phonologique.<sep> Par ailleurs, le préfixe sanscrit est toujours utilisé pour créer de nouveaux mots.<sep> Nous avons proposé que le malais soit alors rattaché à la branche « Indo-malaisienne » .<sep> Nous mettons en évidence dans ce travail des structures phrastiques canoniques d'une part du malais et d'autre part nous comparons les systèmes verbaux du français et du malais.<sep> Le malais présente un degré d'agglutination élevé alors que pour le français le phénomène est très rare.<sep> Nous voyons par exemple aussi qu'en français le complément indirect est introduit par une préposition alors que le malais omet la préposition, que le verbe « ada » en malais exprime certains sens des verbes « être et avoir » , que les structures de transitivité du verbe et les structures aspectuelles ne sont pas identiques dans les deux langues, etc.<sep> Nous avons mis en place un formalisme qui permet de comparer les deux systèmes en nous inspirant des microsystèmes établis par S. Cardey et pour créer d'autres microsystèmes verbaux.<sep> Nous voyons ainsi mieux les convergences et divergences au niveau des structures et des systèmes verbaux du français et du malais.
Cette thèse traite le problème d'extension d'annotation d'images.<sep> En effet, la croissance rapide des archives de contenus visuels disponibles a engendré un besoin en techniques d'indexation et de recherche d'information multimédia.<sep> L'annotation d'images permet l'indexation et la recherche dans des grandes collections d'images d'une façon facile et rapide.<sep> Pour l'extension automatique d'annotation d'images, nous avons utilisé les modèles graphiques probabilistes.<sep> Le modèle proposé est un mélange de distributions multinomiales et de mélanges de Gaussiennes où nous avons combiné des caractéristiques visuelles et textuelles.<sep> Pour réduire le coût de l'annotation manuelle et améliorer la qualité de l'annotation obtenue, nous avons intégré des retours utilisateur dans notre modèle.<sep> Pour combler le problème du fossé sémantique et enrichir l'annotation d'images, nous avons utilisé une hiérarchie sémantique en modélisant de nombreuses relations sémantiques entre les mots-clés d'annotation.<sep> Après la construction de la hiérarchie, nous l'avons intégré dans notre modèle d'annotation d'images.<sep> Le modèle obtenu avec la hiérarchie est un mélange de distributions de Bernoulli et de mélanges de Gaussiennes
Dans cette thèse, nous proposons un processus d'adaptation thématique non supervisée qui vise à spécialiser le modèle de langue (ML) et le vocabulaire d'un système de reconnaissance de la parole (RAP) en fonction du thème de chaque document à transcrire.<sep> Ce processus a comme singularité de ne nécessiter aucune connaissance a priori sur les éventuels thèmes rencontrés et d'intégrer des techniques de traitement automatique des langues.<sep> Il consiste à caractériser le thème d'un document audio par des mots-clés extraits automatiquement et à construire un corpus de pages Web du même thème.<sep> Nous ré-estimons alors le ML en fonction d'une terminologie issue de ce corpus, puis intégrons au système de nouveaux mots propres au thème en assimilant ceux-ci à d'autres avec lesquels il partagent des relations paradigmatiques.<sep> Nos expériences sur le corpus ESTER montrent que l'utilisation des ML et vocabulaire ainsi adaptés produit des améliorations du taux de reconnaissance d'un système de RAP.
Cette thèse traite la notion d'indexation pédagogique et l'aborde sous l'angle de la recherche et du choix de textes pour l'enseignement des langues.<sep> Ce problème est replacé dans le champ disciplinaire de l'Apprentissage des Langues Assisté par Ordinateur (ALAO) et des apports potentiels du Traitement Automatique des Langues (TAL) à cette discipline, avant d'être confronté à des éléments provenant plus directement de la didactique des langues, pour proposer une approche empirique.<sep> La thèse s'articule ensuite autour de deux questionnaires visant à connaitre les pratiques déclarées des enseignants quant à la recherche et au choix de textes dans le cadre de la tâche de planification des cours.<sep> Le premier questionnaire permet la formalisation de la notion de contexte pédagogique, qui est ultérieurement appréhendée à travers certaines de ses composantes grâce au second questionnaire.<sep> Enfin, ces premières ébauches de formalisation servent de fondement à la définition d'un modèle dont l'objectif est de rendre compte de la contextualité des propriétés dites pédagogiques quand elles sont appliquées à des ressources brutes.<sep> Des pistes d'implantation du modèle sont finalement proposées dans le cadre de la description d'un système informatique.
Les technologies de reconnaissance automatique de la parole sont désormais intégrées dans de nombreux systèmes.<sep> La performance des systèmes de reconnaissance vocale pour les locuteurs non natifs continue cependant à souffrir de taux d'erreur élevés, en raison de la différence entre la parole non native et les modèles entraînés.<sep> La réalisation d'enregistrements en grande quantité de parole non native est généralement une tâche très difficile et peu réaliste pour représenter toutes les origines des locuteurs.<sep> Ce travail de thèse porte sur l'amélioration des modèles acoustiques multilingues pour la transcription phonétique de la parole de type « réunion multilingue » .<sep> Pour répondre à ces défis, nous proposons un processus d'adaptation de modèles acoustiques multilingues que nous appelons « adaptation autonome » .<sep> Dans l'adaptation autonome, nous étudions plusieurs approches pour adapter les modèles acoustiques multilingues de manière non supervisée (les langues parlées et les origines des locuteurs ne sont pas connues à l'avance) et qui n'utilise aucune donnée supplémentaire lors du processus d'adaptation.<sep> Les approches étudiées sont décomposées selon deux modules.<sep> Le premier module qui s'appelle « l'observateur de langues » consiste à récupérer les caractéristiques linguistiques (les langues parlées et les origines des locuteurs) des segments à décoder.<sep> Le deuxième module consiste à adapter le modèle acoustique multilingue en fonction des connaissances fournies par l'observateur de langue.<sep> Pour évaluer l'utilité de l'adaptation autonome d'un modèle acoustique multilingue, nous utilisons les données de test, qui sont extraites de réunions multilingues, contenant de la parole native et non native de trois langues : l'anglais (EN), le français (FR) et le vietnamien (VN).<sep> Selon les résultats d'expérimentation, l'adaptation autonome donne des résultats prometteurs pour les paroles non natives mais dégradent très légèrement les performances sur de la parole native.<sep> Afin d'améliorer la performance globale des systèmes de transcription pour toutes les paroles natives et non natives, nous étudions plusieurs approches de détection de parole non native et proposons de cascader un tel détecteur avec notre processus d'adaptation autonome.<sep> Les résultats obtenus ainsi, sont les meilleurs parmi toutes les expériences réalisées sur notre corpus de réunions multilingues.
Les textes se sont imposés depuis la fin des années 1990 comme une source précieuse de connaissances pour la construction de ces ontologies qui constituent à la fois l'ossature sémantique du web sémantique et son goulot d'étranglement.<sep> Les textes sont en effet porteurs de connaissances stabilisées et partagées qui sont plus faciles d'accès que les experts qu'on pourrait vouloir interroger.<sep> Le recours aux textes ne remplace pas l'expertise humaine mais elle permet à l'ingénieur de la connaissance de prendre connaissance du domaine à modéliser et d'amorcer le travail de modélisation.<sep> L'un des enjeux du passage des textes à des ontologies est l'identification du vocabulaire du domaine et sa structuration sous la forme d'un thésaurus avant sa formalisation et ce sont les difficultés inhérentes à cette exploitation du matériau linguistique et à sa normalisation qui ont retenu notre attention dans ce travail de thèse.<sep> Nous proposons une méthode de normalisation qui permet de transformer le matériau linguistique – tel qu'il a été extrait d'un corpus d'acquisition par des outils de TAL – en un réseau sémantique que nous appelons « réseau termino-conceptuel » et qui décrit le vocabulaire normalisé du domaine, c'est-à-dire le vocabulaire désambiguïsé et structuré tel qu'il est stabilisé dans le domaine en question.<sep> C'est un réseau de termes non ambigus qui sont interconnectés à travers des relations taxonomiques et associatives.<sep> Il sert de base pour la construction d'une ontologie de domaine à partir de textes mais aussi de thesaurus pour l'annotation des documents.<sep> Cette thèse a été conduite dans le cadre du projet européen ONTORULE (ontology meets business rules).<sep> Notre approche s'inscrit dans le cadre global de la méthode de construction de ressources ontologiques TERMINAE qui a été initiée par les travaux du groupe TIA (Terminologie Intelligence Artificielle).<sep> La deuxième étape de normalisation permet de transformer le réseau terminologique initial en un réseau termino-conceptuel.<sep> Si la première étape peut être automatisée par des outils d'extraction, les deux autres nécessitent un travail de désambiguïsation et de modélisation qui repose en grande partie sur l'expertise humaine.<sep> Cette thèse a permis d'affiner la méthode TERMINAE en montrant comment décomposer le travail de normalisation en différentes opérations, comment enchaîner ces opérations et comment contrôler le processus global de normalisation.<sep> C'est en effet une étape difficile pour l'ingénieur de la connaissance qui se retrouve, à l'issue de la phase d'extraction linguistique, face à une masse d'unités à traiter, dont certaines sont ambiguës et qui ne sont pas toutes pertinentes pour le domaine.<sep> Cette approche de la normalisation a été testée dans le cadre d'expérimentations visant à évaluer les principales contributions dans cette thèse.<sep> Les ontologies créées ont été utilisées dans le cadre du projet ONTORULE sur trois cas d'usage différents.<sep> Elles ont servi de vocabulaires conceptuels pour l'écriture des règles métier de différents systèmes d'aide à la décision mais elles ont surtout été utilisées pour annoter sémantiquement les textes réglementaires et ainsi guider le travail d'acquisition des base de règles métier à partir de ces textes.
Cette thèse vise à étudier les difficultés de la simplification automatique de parole dans le but de progresser dans la compréhension des modèles neuronaux de traduction de parole, et de les appliquer à des tâches de sous-titrage automatique.
Les progrès récents des réseaux de neurones artificiels (plus connus sous le nom d'apprentissage profond) ont permis d'améliorer l'état de l'art dans plusieurs domaines de la vision par ordinateur.<sep> Dans cette thèse, nous étudions des techniques d'apprentissage profond dans le cadre de l'analyse du genre et de l'âge à partir du visage humain.<sep> En particulier, deux problèmes complémentaires sont considérés : (1) la prédiction du genre et de l'âge, et (2) la synthèse et l'édition du genre et de l'âge.<sep> D'abord, nous effectuons une étude détaillée qui permet d'établir une liste de principes pour la conception et l'apprentissage des réseaux de neurones convolutifs (CNNs) pour la classification du genre et l'estimation de l'âge.<sep> Nos meilleurs CNNs obtiennent une précision moyenne de 98.7% pour la classification du genre et une erreur moyenne de 4.26 ans pour l'estimation de l'âge sur un corpus interne particulièrement difficile.<sep> Ensuite, afin d'adresser le problème de la synthèse et de l'édition d'images de visages, nous concevons un modèle nommé GA-cGAN : le premier réseau de neurones génératif adversaire (GAN) qui produit des visages synthétiques réalistes avec le genre et l'âge souhaités.<sep> Enfin, nous proposons une nouvelle méthode permettant d'employer GA-cGAN pour le changement du genre et de l'âge tout en préservant l'identité dans les images synthétiques.<sep> Cette méthode permet d'améliorer la précision d'un logiciel sur étagère de vérification faciale en présence d'écarts d'âges importants.
La diffusion des terminaux de lecture de lecture numérique (liseuses, tablettes, téléphones mobiles) a le potentiel de renouveler l'expérience de lecture, en l'augmentant de nouvelles fonctionnalités (enrichissement du texte par des liens hypertextes, des dictionnaires, des images, de la musique ou des sons, etc), en en adaptant la présentation en fonction des lecteurs, ou en la rendant plus sociale et collaborative, via le partage de d'annotations, de citations, ou de recommandations de lecture.<sep> Ces terminaux ouvrent également un champ nouveau en matière d'apprentissage de la lecture, en particulier de par leur capacité à embarquer des surcouches textuelles qui peuvent aider, de manière adaptative et personnalisée, les apprentis lecteurs (ou des lecteurs pathologiques) à progresser dans leur compréhension des textes.<sep> Comme tout dispositif, la question de l'aide à lecture implique en préalable (ou en parallèle) une réflexion sur l'évaluation : évaluation pédagogique des dispositifs, évaluation des lecteurs, mais également celle de l'évaluation des contenus de lecture eux-mêmes : construire des dispositifs de recommandation automatique ou des parcours de lecture adaptés à des apprenants (en L1 comme en L2) demande de pouvoir évaluer les œuvres (ou, à un niveau plus fin, les chapitres ou les passages d'œuvres) sur une échelle de difficulté, qui permette de proposer des lectures qui soient bien en adéquation avec les capacités du lecteur.<sep> Si la question de la mesure de la lisibilité des textes est relativement ancienne, elle fait encore l'objet de nombreuses recherches, en particulier pour ce qui concerne la mesure automatique de la lisibilité, dont nous présentons un bref état de l'art ci-dessous.<sep> Faute de mesure automatique, les seuls éléments d'information dont disposent les maîtres et les lecteurs pour choisir leurs lectures sont des catégorisations grossières, souvent datées, qui ne rendent pas finement compte de la difficulté des textes.<sep> Disposer d'inventaires à très grande échelle associant œuvres (ou passages) à des niveaux de difficulté de lecture, est donc indispensable pour accompagner un lecteur novice dans ses apprentissages de la lecture.<sep> Ce projet rassemble une entreprise spécialisée dans les dispositifs de lecture électroniques sociaux (GLOSE) et des équipes de recherche spécialisées en traitement automatique des langues (au sein du LIMSI-CNRS), avec pour objectif de produire des outils de mesure automatique de la difficulté de la lecture dans un contexte pédagogique.<sep> Deux aspects innovants seront l'objets d'une attention particulière : d'une part la modélisation du caractère évolutif de la difficulté de lecture, d'autre part l'analyse de la difficulté dans un contexte multilingue, dans lequel il importe d'élaborer des techniques de mesure « universelles » , pouvant potentiellement valoir pour un grand nombre de langues.
Dans le monde du Web, on retrouve les formats RSS et Atom (feeds) qui sont, sans doute, les formats XML les plus populaires et les plus utilisés. Ces formats permettent aux, entre autres, communautés Web, industriels, et services web de publier et d'échanger des documents XML.<sep> En outre, ils permettent à un utilisateur de consulter librement des données/informations sans avoir à basculer d'un site à un autre, et cela à l'aide d'applications logicielles.<sep> Dans ce cas, l'utilisateur enregistre ses fournisseurs de flux favoris, chaque fournisseur diffuse la liste des nouveaux éléments qui ont été modifiés depuis le dernier téléchargement.<sep> Cependant, l'enregistrement d'un certain nombre de sources de flux dans un agrégateur de flux engendre à la fois des problèmes d'hétérogénéité (à cause des différences structurelles et de contenu) et des problèmes de surcharges d'information.<sep> Par ailleurs, aucun des agrégateurs de flux existants n'offre une approche qui intègre (ou fusionne) les flux en tenant compte de leurs similarités, du contexte de l'utilisateur et de ses préférences.<sep> Dans cette thèse, nous proposons un framework formel qui permet de traiter l'hétérogénéité, l'intégration et l'interrogation des flux d'actualités.<sep> Ce framework est fondé sur une représentation arborescente d'un flux et possède trois éléments principaux qui sont les suivants : comparateur de flux, intégrateur de flux, et processeur de requêtes.<sep> Le comparateur de flux permet de mesurer le degré de similarité entre deux éléments/flux en utilisant une base de connaissance intégrant une approche ascendante et progressive.<sep> Nous proposons une mesure de similarité à base de concept capable de calculer la similarité entre les flux selon le nombre de leurs concepts communs (et différents) et leurs proximités sémantiques.<sep> Nous montrons également comment définir et identifier la relation exclusive entre deux textes ou éléments.<sep> L'intégrateur de flux permet de fusionner plusieurs flux provenant de différentes sources tout en tenant compte du contexte de l'utilisateur.<sep> Nous montrons dans notre étude comment représenter le contexte d'utilisateur ainsi que ses préférences.<sep> Nous fournissons un ensemble prédéfini de règles de fusion qui peuvent être enrichies et adaptées par chaque utilisateur.<sep> Quant au processeur de requêtes, il se base sur une étude formelle et plus précisément sur une algèbre dédiée à la fusion des flux continus d'actualités que nous proposons ici.<sep> Les opérateurs proposés dans cette algèbre sont aidés par des fonctions à base de similarité.<sep> Nous catégorisons les opérateurs de flux selon trois catégories : opérateurs d'extraction, opérateurs ensemblistes et opérateur de fusion.<sep> Nous montrons que l'opérateur de fusion généralise l'opération de jointure et les opérateurs ensemblistes.<sep> Nous fournissons également un ensemble de règles de réécriture et d'équivalence de requêtes pour la simplification et l'optimisation des requêtes.<sep> Enfin, nous présentons un prototype nommé « Easy RSS Manager » (EasyRSSManager). Ce prototype est un lecteur sémantique de flux et un composant sémantique pour l'interrogation des fenêtres de flux.<sep> Ce prototype est un lecteur sémantique de flux et un composant sémantique pour l'interrogation des fenêtres de flux.<sep> EasyRSSManager a été utilisé pour valider, démontrer et tester la faisabilité des différentes propositions de notre étude.
La diversité des contenus recommandation et la variation des contextes des utilisateurs rendent la prédiction en temps réel des préférences des utilisateurs de plus en plus difficile mettre en place.<sep> Toutefois, la plupart des approches existantes n'utilisent que le temps et l'emplacement actuels séparément et ignorent d'autres informations contextuelles sur lesquelles dépendent incontestablement les préférences des utilisateurs (par exemple, la météo, l'occasion).<sep> En outre, ils ne parviennent pas considérer conjointement ces informations contextuelles avec les interactions sociales entre les utilisateurs.<sep> D'autre part, la résolution de problèmes classiques de recommandation (par exemple, aucun programme de télévision vu par un nouvel utilisateur connu sous le nom du problème de démarrage froid et pas assez d'items co-évalués par d'autres utilisateurs ayant des préférences similaires, connu sous le nom du problème de manque de donnes) est d'importance significative puisque sont attaqués par plusieurs travaux.<sep> Dans notre travail de thèse, nous proposons un modèle probabiliste qui permet exploiter conjointement les informations contextuelles actuelles et l'influence sociale afin d'améliorer la recommandation des items.<sep> En particulier, le modèle probabiliste vise prédire la pertinence de contenu pour un utilisateur en fonction de son contexte actuel et de son influence sociale.<sep> Nous avons considérer plusieurs éléments du contexte actuel des utilisateurs tels que l'occasion, le jour de la semaine, la localisation et la météo.<sep> Nous avons utilisé la technique de lissage Laplace afin d'éviter les fortes probabilités.<sep> D'autre part, nous supposons que l'information provenant des relations sociales a une influence potentielle sur les préférences des utilisateurs.<sep> Ainsi, nous supposons que l'influence sociale dépend non seulement des évaluations des amis mais aussi de la similarité sociale entre les utilisateurs.<sep> Les similarités sociales utilisateur-ami peuvent être établies en fonction des interactions sociales entre les utilisateurs et leurs amis (par exemple les recommandations, les tags, les commentaires).<sep> Nous avons mené une série d'expérimentations en utilisant un ensemble de donnes réelles issues de la plateforme de TV sociale Pinhole.<sep> Cet ensemble de donnes inclut les historiques d'accès des utilisateurs-vidéos et les réseaux sociaux des téléspectateurs.<sep> En outre, nous collectons des informations contextuelles pour chaque historique d'accès utilisateur-vidéo saisi par le système de formulaire plat.<sep> Le système de la plateforme capture et enregistre les dernières informations contextuelles auxquelles le spectateur est confronté en regardant une telle vidéo.<sep> Dans notre évaluation, nous adoptons le filtrage collaboratif axé sur le temps, le profil dépendant du temps et la factorisation de la matrice axe sur le réseau social comme tant des modèles de référence.<sep> L'évaluation a port sur deux tâches de recommandation.<sep> Nous avons évalué l'impact de chaque élément du contexte de visualisation dans la performance de prédiction.<sep> Les résultats expérimentaux démontrent que notre modèle surpasse les approches de l'état de l'art fondes sur le facteur temps et sur les réseaux sociaux.<sep> Dans les tests des problèmes de manque de donnes et de démarrage froid, notre modèle renvoie des prédictions cohérentes différentes valeurs de manque de données.
Le cadre générale de cette thèse est l'analyse quantitative des objets issus de la théorie des langages rationnels.<sep> On adapte des techniques d'analyse d'algorithmes (complexité en moyenne, complexité générique, génération aléatoire,...) à des objets et à des algorithmes qui font intervenir des classes particulières d'automates.<sep> Dans une première partie nous étudions la complexité de l'algorithme de minimisation de Brzozowski.<sep> Bien qu'ayant une mauvaise complexité dans le pire des cas, cet algorithme a la réputation d'être efficace en pratique.<sep> En utilisant les propriétés typiques des applications et des permutations aléatoires, nous montrons que la complexité générique de l'algorithme de Brzozowski appliqué à un automate déterministe croît plus vite que tout polynôme en n, où n est le nombre d'états de l'automate.<sep> Dans une seconde partie nous nous intéressons à la génération aléatoire d'automates acycliques.<sep> Ces automates sont ceux qui reconnaissent les ensembles finis de mots et sont de ce fait utilisés dans de nombreuses applications, notamment en traitement automatique des langues.<sep> Le premier utilise le modèle des chaînes de Markov, et le second utilise la "méthode récursive", qui tire partie des décompositions combinatoires des objets pour faire de la génération.<sep> La première méthode est souple mais difficile à calibrer, la seconde s'avère plutôt efficace.<sep> Une fois implantée, cette dernière nous a notamment permis d'observer les propriétés typiques des grands automates acycliques aléatoires
Le graphe est un concept puissant pour la représentation des relations entre des paires d'entités.<sep> Les données ayant une structure de graphes sous-jacente peuvent être trouvées dans de nombreuses disciplines, décrivant des composés chimiques, des surfaces des modèles tridimensionnels, des interactions sociales ou des bases de connaissance, pour n'en nommer que quelques-unes.<sep> Cependant, étonnamment peu de choses ont été faites pour explorer l'applicabilité de DL directement sur des données structurées sous forme des graphes.<sep> L'objectif de cette thèse est d'étudier des architectures de DL sur des graphes et de rechercher comment transférer, adapter ou généraliser à ce domaine des concepts qui fonctionnent bien sur des données séquentielles et des images.<sep> Nous nous concentrons sur deux primitives importantes : le plongement de graphes ou leurs nœuds dans une représentation de l'espace vectorielle continue (codage) et, inversement, la génération des graphes à partir de ces vecteurs (décodage).<sep> Nous faisons les contributions suivantes.<sep> La méthode est utilisée pour coder des graphes avec une structure arbitraire et variable.<sep> Deuxièmement, nous proposons SuperPoint Graph, une représentation intermédiaire de nuages de points avec de riches attributs des arêtes codant la relation contextuelle entre des parties des objets.<sep> Sur la base de cette représentation, l'ECC est utilisé pour segmenter les nuages de points à grande échelle sans sacrifier les détails les plus fins.<sep> Troisièmement, nous présentons GraphVAE, un générateur de graphes permettant de décoder des graphes avec un nombre de nœuds variable mais limité en haut, en utilisant la correspondance approximative des graphes pour aligner les prédictions d'un auto-encodeur avec ses entrées.<sep> La méthode est appliquée à génération de molécules
Les systèmes modernes subissent des pressions pour s'adapter à leur environnement en constante évolution afin de rester utiles.<sep> Traditionnellement, cette adaptation a été gérée lors des temps morts du système.<sep> Il y a une demande croissante d'automatiser ce processus et de le réaliser pendant le fonctionnement du système.<sep> Les systèmes auto-adaptatifs ont été introduits en tant que réalisation de systèmes s'adaptant en permanence.<sep> Les systèmes auto-adaptatifs peuvent modifier au moment de l'exécution leur comportement et / ou leur structure en fonction de leur perception de l'environnement, du système lui-même et de leurs exigences.<sep> L'objectif de ce travail est de réaliser l'auto-configuration, une propriété essentielle et essentielle des systèmes auto-adaptatifs.<sep> L'auto-configuration est la capacité de reconfiguration automatique et dynamique en réponse aux changements.<sep> Cela peut inclure l'installation, l'intégration, le retrait et la composition / décomposition d'éléments du système.<sep> Cette thèse présente le framework Dr-BIP, une extension du framework BIP pour la modélisation de systèmes à configuration automatique qui repose sur une approche basée sur un modèle et basée sur des composants et des connecteurs pour prescrire des systèmes.<sep> La combinaison de ces deux approches exploite les avantages de chacune d'elles, faisant de leur combinaison une méthodologie idéale pour la réalisation de systèmes complexes à configuration automatique.<sep> Un modèle de système Dr-BIP est un modèle d'exécution qui capture le système en cours d'exécution à trois niveaux d'abstraction différents, à savoir les variantes de comportement, de configuration et de configuration.<sep> La configuration du système est capturée par le composant et les connecteurs.<sep> Dans un système de composants et de connecteurs, la configuration automatique peut avoir trois niveaux de granularité différents, notamment la possibilité d'ajouter ou de supprimer des connecteurs, d'ajouter ou de supprimer des composants et d'ajouter ou de supprimer des sous-systèmes.<sep> Dr-BIP prend en charge l'ajout et le retrait explicites de composants et de sous-systèmes, mais l'ajout et le retrait implicites de connecteurs.<sep> Le principal avantage de compter sur une addition et une suppression implicites de connecteurs est la possibilité de garantir, par la construction, des topologies de configuration spécifiques.<sep> Pour capturer les trois niveaux d'abstraction, nous introduisons des motifs en tant que structures principales pour prescrire un système Dr-BIP à configuration automatique.<sep> Un motif définit un ensemble de composants qui évoluent en fonction de règles d'interaction et de reconfiguration.<sep> Un système est composé de plusieurs motifs pouvant éventuellement partager des composants et évoluer ensemble.<sep> Les règles d'interaction dictent la manière dont les composants composant le système peuvent interagir, tandis que les règles de reconfiguration dictent l'évolution de la configuration du système.<sep> Enfin, nous montrons que le cadre proposé est à la fois minimal et expressif en modélisant quatre systèmes différents à configuration automatique.<sep> Enfin, nous proposons un langage de modélisation pour codifier les concepts du cadre et fournir une implémentation d'interprète.
De nos jours, les smartphones et les tablettes génèrent, reçoivent, mémorisent et transfèrent vers des serveurs une grande quantité de données en proposant des services aux utilisateurs via des applications mobiles facilement téléchargeables et installables.<sep> Le grand nombre de capteurs intégrés dans un smartphone lui permet de collecter de façon continue des informations très précise sur l'utilisateur et son environnement.<sep> Cette importante quantité de données privées et professionnelles devient difficile à superviser.<sep> L'approche « Privacy by Design » , qui inclut sept principes, propose d'intégrer la notion du respect des données privées dès la phase de la conception d'un traitement informatique.<sep> En Europe, la directive européenne sur la protection des données privées (Directive 95/46/EC) intègre des notions du « Privacy by Design » .<sep> L'objectif de cette thèse est de proposer des solutions pour améliorer la transparence des utilisations des données personnelles mobiles, la visibilité sur les systèmes informatiques, le consentement et la sécurité pour finalement rendre les applications et les systèmes mobiles plus conforme au « Privacy by (re)Design » 
La structuration en thèmes est un domaine de recherche très prisé dans le traitement automatique du langage naturel car elle est le point de départ de plusieurs applications comme la recherche d'information, le résumé automatique et la modélisation des thèmes.<sep> Dans cette thèse, nous avons proposé un système de structuration automatique des journaux d'informations.<sep> Notre système contient deux modules : segmentation thématique et titrage.<sep> La segmentation thématique consiste à effectuer un pavage de l'émission en segments thématiquement homogènes.<sep> Ces derniers, sont généralement identifiés par des étiquettes anonymes, c'est alors le rôle du module de titrage d'affecter un titre à chaque segment.<sep> Ces travaux ont permis plusieurs contributions originales tel que l'exploitation conjointe de la distribution des mots et des locuteurs (cohésion de la parole) ainsi que l'utilisation des relations sémantiques de type diachronique.<sep> Finalement, nous avons proposé deux nouvelles métriques d'évaluation, l'une pour la segmentation thématique et l'autre pour le titrage.<sep> Ils sont constitués de 168 journaux télévisés issus de 10 chaînes françaises transcrits automatiquement.
Cette thèse porte sur l'analyse de données textuelles dans le domaine de la santé et en particulier sur la classification supervisée multi-classes de données issues de la littérature biomédicale et des médias sociaux.<sep> Une des difficultés majeures lors de l'exploration de telles données par des méthodes d'apprentissage supervisées est de posséder un jeu de données suffisant en nombre d'exemples pour l'entraînement des modèles.<sep> En effet, il est généralement nécessaire de catégoriser les données manuellement avant de réaliser l'étape d'apprentissage.<sep> La taille importante des jeux de données rend cette tâche de catégorisation très coûteuse, qu'il convient de réduire par des systèmes semi-automatiques.<sep> Dans ce contexte, l'apprentissage actif, pendant lequel l'oracle intervient pour choisir les meilleurs exemples à étiqueter, s'avère prometteur.<sep> L'intuition est la suivante : en choisissant les exemples intelligemment et non aléatoirement, les modèles devraient s'améliorer avec moins d'efforts pour l'oracle et donc à moindre coût (c'est-a-dire avec moins d'exemples annotés).<sep> Dans cette thèse, nous évaluerons différentes approches d'apprentissage actif combinées avec des modèles d'apprentissage profond récents.<sep> Par ailleurs, lorsque l'on dispose de peu de données annotées, une possibilité d'amélioration est d'augmenter artificiellement la quantité de données pendant la phase d'entraînement du modèle, en créant de nouvelles données de manière automatique à partir des données existantes.<sep> Plus précisément, il s'agit d'injecter de la connaissance en tenant compte des propriétés invariantes des données par rapport à certaines transformations.<sep> Les données augmentées peuvent ainsi couvrir un espace d'entrée inexploré, éviter le sur-apprentissage et améliorer la généralisation du modèle.<sep> Dans cette thèse, nous proposerons et évaluerons une nouvelle approche d'augmentation de données textuelles.
Le langage demeure préservé des effets du vieillissement mais précocement atteint dans la Maladie d'Alzheimer (MA).<sep> Le discours, en mobilisant l'ensemble des fonctions cognitives, pourrait mettre en évidence davantage de difficultés.<sep> Ce travail vise à analyser des productions de patients et de participants âgés typiques en lien avec les changements cognitifs et cérébraux.<sep> Une étude s'est intéressée à la production de discours portant sur des événements personnellement vécus.<sep> Une deuxième visait à comparer ce type de discours à un discours sur support imagé.<sep> Les résultats montrent que les difficultés rencontrées lors d'un discours d'événements vécus seraient corrélées à l'atteinte mnésique des patients.<sep> Une dernière étude s'est focalisée sur la variabilité existant chez les participants âgés "typiques".<sep> Une analyse en cluster a montré quatre profils de locuteurs, dont un profil "hors-sujet" qui pourrait refléter une zone ambiguë entre vieillissement normal et pathologique.<sep> Ce travail pointe des marqueurs de MA prodromale, en lien avec les modifications cognitives et cérébrales.<sep> Certaines difficultés seraient liées à une atteinte mnésique ou exécutive indépendamment de l'altération langagière.
Cette thèse vise à mieux comprendre de quelle manière les ateliers scientifiques animés par des chercheurs à l'université essayent de motiver les élèves de l'école primaire au lycée à suivre une carrière scientifique universitaire.<sep> Notre défi majeur a été d'analyser ces ateliers scientifiques à une fin didactique en prenant en compte la complexité d'interactions des acteurs et leurs attentes.<sep> Ainsi, ce travail repose sur une analyse des ateliers en les considérant comme un système dynamique complexe.<sep> Nous articulons une approche qualitative qui consiste en un « noyau d'analyse complexe » d'une triangulation systémique entre le contexte, la stratégie et la production.<sep> Dans cette perspective, cette thèse apporte une réflexion sur la culture des évaluations dans le domaine de l'éducation en proposant une modélisation d'une « évaluation multiréférentielle » basée sur trois critères : l'efficacité, la pertinence et l'efficience.<sep> Pour ce faire, différents niveaux d'analyse sont proposés à partir de questionnaires et d'entretiens auprès d'élèves, de médiateurs et d'enseignants, ayant participé à des ateliers.<sep> Les résultats ont mis en évidence des décalages entre les objectifs des acteurs et les visées institutionnelles affichées pour ces ateliers.<sep> L'enjeu pour les médiateurs, les enseignants et les élèves se situe avant tout dans un registre du plaisir.<sep> En plus, les médiateurs interrogés valorisent peu leur parcours scolaire et universitaire, alors que les élèves voient avant tout le parcours du chercheur comme relevant d'une vocation.
Les approches d'apprentissage profond supervisés, semi-supervisés ou encore non supervisés semblent offrir des prémices favorables pour atteindre ces objectifs.<sep> Néanmoins, ils doivent être reconsidérées d'une manière pragmatique et cohérente pour pouvoir apporter de solutions concrètes, globales et donc exploitables dans un contexte industriel.<sep> En s'appuyant sur une plate-forme générique d'intelligence artificielle (IA), intégrant différentes métriques d'analyse, selon les différents canaux médias qui puissent être considérés, un benchmark des technologies existantes sur des bases à la fois génériques et sur des contenus spécifiques de France Télévisions sera réalisé, afin d'évaluer objectivement les performances et retenir les méthodes les plus prometteuses.<sep> Une analyse des résultats avec avantages et limitations sera conduite afin de déceler les pistes d'amélioration/adaptation/optimisation.<sep> Les développements ultérieurs concerneront la fusion des différents médias possibles/envisageables.<sep> Pour atteindre cet objectif, il est essentiel d'identifier et exploiter, à des objectifs de recommandation, les combinaisons de modalités qui puissent conduire à des résultats pertinents.<sep> Le niveau de granularité spatio-temporelle est également à gérer et à prendre en compte.<sep> Les principaux verrous technologiques et scientifiques auxquels s'attaque ce projet de thèse sont les suivants : - la spécification d'une taxonomie intelligente et adaptée aux différents types de programmes, qui puisse être identifiée et apprise à la volée, - la vraie prise en compte, pour des objectifs spécifiques de catégorisation, d'une dimension multi-modale auto-adaptable, - la définition de niveaux appropriés de granularités spatio-temporelles et sémantiques, - la sélection adaptative des différentes dimensions intervenant dans la multi-modalité, - la détection automatique des saillances sémantiques audio-visuelles.
Ce travail, qui s'appuie sur une étude de corpus, analyse deux types d'adjectifs dénominaux en slovaque : les adjectifs suffixés en -ský (NskýA) et les adjectifs composés d'un adjectif et d'un nom (ANA).<sep> Les résultats obtenus pour le slovaque sont mis en correspondance, et seulement quand cela est pertinent, avec les réalisations sémantiquement équivalentes du français. Les analyses pour le slovaque sont vérifiées et testées à chaque fois à l'aide d'une expérience réalisée sous forme de questionnaires soumis aux locuteurs slovaques.<sep> La formation des ANA (MODROOKÝA "ayant les yeux bleus") répond à un triple faisceau de contraintes (i) entre l'ANA et le nom recteur (Nr), e.g. modrookéA dievcaNr "fille aux yeux bleus", (ii) entre le composant nominal (N) et le Nr (OKON "oeil" et dievca "fille"), et (iii) entre le composant adjectival (A) et le N (MODRÝA "bleu" et OKON "oeil").<sep> Ces adjectifs désignent une propriété inhérente de l'entité à laquelle réfère le Nr.<sep> L'interprétation sémantique majoritaire est la relation méronymique : le N dénote une partie constitutive et visible de l'entité désignée par le Nr.<sep> La suffixation en -ský sélectionne essentiellement les noms propres de lieu (toponymes) et de personne (anthroponymes) en position de base.<sep> Le contenu sémantique de ces adjectifs est identique à celui de leurs bases.<sep> Les NskýA toponymiques comme LIBANONSKÝA "libanais" peuvent référer au lieu ainsi qu'aux habitants d'un lieu.<sep> Seule la valeur du Nr permet de désambiguïser la référence d'un NskýA toponymique.<sep> Les NskýA anthroponymiques comme STALINSKÝA "stalinien" peuvent avoir, outre l'interprétation relationnelle : "de Staline", un sens qualifiant : "comparable à Staline".
L'exploration de corpus à travers des requêtes fait aujourd'hui partie de la routine de nombreux chercheurs adoptant une approche empirique de la langue, mais aussi de non-spécialistes qui utilisent des moteurs de recherche ou des concordanciers dans le cadre de l'apprentissage d'une langue.
Dans cette thèse nous étudions plusieurs problèmes d'apprentissage automatique qui sont tous liés à la minimisation d'une fonction bruitée, qui sera souvent convexe.<sep> Du fait de leurs nombreuses applications nous nous concentrons sur des problèmes d'apprentissage séquentiel, qui consistent à traiter des données ``à la volée'', ou en ligne.<sep> La première partie de cette thèse est ainsi consacrée à l'étude de trois différents problèmes d'apprentissage séquentiel dans lesquels nous rencontrons le compromis classique ``exploration vs.<sep> Nous étudions tous ces problèmes à l'aide de techniques d'optimisation stochastique convexe, et nous proposons et analysons des algorithmes pour les résoudre.<sep> Dans la deuxième partie de cette thèse nous nous concentrons sur l'analyse de l'algorithme de descente de gradient stochastique qui est vraisemblablement l'un des algorithmes d'optimisation stochastique les plus utilisés en apprentissage automatique.<sep> Nous en présentons une analyse complète dans le cas convexe ainsi que dans certaines situations non convexes en étudiant le modèle continu qui lui est associé, et obtenons de nouveaux résultats de convergence optimaux.
Dans cette thèse, nous menons une réflexion sur les notions de norme(s) et d'usage(s) langagiers dans le cadre du domaine du contrôle aérien.<sep> Lorsque la phraséologie ne suffit pas, ces derniers ont recours à une forme langagière plus naturelle, le "plain language".<sep> De nombreux problèmes relatifs à la mise en œuvre de ce dernier ont rapidement vu le jour et ont suscité des interrogations, notamment chez les professionnels de l'enseignement de l'anglais de l'aviation.<sep> Pour répondre aux besoins spécifiques de l'ENAC, cette thèse dresse un panorama des usages faits de la langue anglaise par les contrôleurs français et les pilotes étrangers lors de leurs communications radiotéléphoniques.<sep> Notre méthode d'analyse consiste en une étude comparative entre un corpus de référence, représentant la norme, et un corpus de communications réelles, représentant les usages.<sep> Cette analyse comparative nous permet de repérer, de décrire et de catégoriser les formes langagières employées lors de situations routinières de la navigation aérienne.<sep> Certaines différences sont ainsi repérées entre les deux corpus.<sep> En fonction de la situation, les pilotes et les contrôleurs peuvent procéder à des variations d'ordre lexical, sémantique, syntaxique et discursif.<sep> Alors que certains semblent subir l'influence de la langue naturelle, d'autres semblent mettre en œuvre une stratégie communicationnelle pour tenter d'"humaniser" ou de modaliser le contenu de leurs messages.
Les systèmes cyber-physiques sont une classe de systèmes complexe, de grande échelle, souvent critiques de sûreté, qui apparaissent dans des applications industrielles variées.<sep> Des approches de vérification formelle sont capable de fournir des garanties pour la performance et la sûreté de ces systèmes.<sep> Elles nécessitent trois éléments : un modèle formel, une méthode de vérification, ainsi qu'un ensemble de spécifications formelles.<sep> En revanche, les modèles industriels sont typiquement informels, ils sont analysés dans des environnements de simulation informels et leurs spécifications sont décrits dans un langage naturel informel.<sep> Dans cette thèse, nous visons à faciliter l'intégration de la vérification formelle dans le processus industriel de la conception basé sur modèle.<sep> Notre première contribution clé est une méthodologie de transformation de modèle.<sep> A partir d'un modèle de simulation standard, nous le transformons en un modèle de vérification équivalent, plus précisément en un réseau d'automates hybrides.<sep> Le processus de transformation prend en compte des différences de syntaxes, sémantique et d'autres aspects de la modélisation.<sep> Pour cette classe de modèle formel, des algorithmes d'atteignabilité peuvent être appliqués pour vérifier des propriétés de sûreté.<sep> Un obstacle est que des algorithmes d'atteignabilité se mettent à l'échelle pour des modèles affines par morceaux, mais pas pour des modèles non linéaires.<sep> Pour obtenir des surapproximations affines par morceaux des dynamiques non linéaires, nous proposons une technique compositionnelle d'hybridisation syntaxique.<sep> La seconde contribution clé est une approche pour encoder des spécifications formelles riches de façon à ce qu'elles peuvent être interprétées par des outils d'atteignabilité.<sep> Nous prenons en compte des spécifications exprimées sous forme d'un gabarit de motif (pattern template), puisqu'elles sont proche au langage naturel et peuvent être compris facilement par des utilisateurs non experts.<sep> Nous fournissons (i) des définitions formelles pour des motifs choisis, qui respectent la sémantique des automates hybrides, et (ii) des observateurs qui encodes les propriétés en tant qu'atteignabilité d'un état d'erreur.<sep> En composant ces observateurs avec le modèle formel, les propriétés peuvent être vérifiées par des outils standards de vérification qui sont automatisés.<sep> Finalement, nous présentons une chaîne d'outils semi-automatisée ainsi que des études de cas menées en collaboration avec des partenaires industriels.
Les particules dites énonciatives sont d'ordinaire étudiées dans les langues à forte contrainte morpho-syntaxique, et il en résulte qu'elles paraissent sortir des cadres descriptifs qui ne mettent pas toujours en lumière les fonctions énonciatives.<sep> Nous les étudions en birman, une langue dont on dit, un peu vite, qu'elle a « peu de grammaire » , et pour laquelle nous ne pouvons utiliser des théories qui conviennent pour les langues Indo-Européennes.<sep> Nous devons, en revanche, tenir compte des caractéristiques propres au birman, langue dans laquelle les particules énonciatives jouent un rôle considérable.<sep> Nous devons également préciser leur statut, tant au niveau grammatical que discursif.<sep> Notre but est d'examiner l'emploi d'une gamme choisie de ces particules, d'observer si leurs caractéristiques permettent de les ordonner du discursif au grammatical, et de faire apparaître les possibilités énonciatives de la langue birmane.<sep> Pour réaliser cette étude, nous utilisons un corpus assez vaste (de plus de 250 000 mot-syllabes), qui parcourt un champ de situations étendu (dialogues spontanés ou simulés et narrations) afin d'identifier plusieurs domaines importants pour une sociolinguistique du birman.
Ce travail de recherche examine le style en traduction à travers le prisme de la métaphore corporelle dans le roman censuré The Rainbow (1915) de D.H. Lawrence et ses deux traductions françaises par Albine Loisy (1939) et Jacqueline Gouirand-Rousselon (2002).<sep> Nous avons articulé notre travail autour de trois parties : en parcourant différents cadres théoriques allant d'Aristote jusqu'aux études plus récentes.<sep> La métaphore est en effet un outil de communication redoutable.<sep> Nous avons, par la suite, exploré la notion de style en traductologie afin de tisser un lien entre la métaphore et le style dans l'écriture lawrencienne.<sep> Métaphoriser et traduire sont deux processus sensiblement proches qui tournent autour d'un point commun, celui du mouvement.<sep> L'analyse détaillée des 35 exemples extraits de The Rainbow et de leurs traductions en français nous a permis de détecter les convergences et les divergences au niveau du style et des représentations métaphoriques du corps.<sep> L'emploi récurrent de la métaphore chez Lawrence n'est pas anodin. Il s'agit d'un moyen pour conceptualiser la philosophie de l'auteur.<sep> Les traductrices ont dû surmonter au moins deux défis : préserver la charge métaphorique et opter pour un style qui reflète la complexité de l'écriture lawrencienne, tout en respectant les normes stylistiques de la langue française.<sep> Les écarts constatés au niveau des traductions ouvrent la voie à des interprétations qui pourraient prendre forme grâce à de futures retraductions.
Cette thèse de doctorat porte sur une lecture de l'expression de la cohérence discursive à partir des usages des anaphores et des connecteurs par les collégiens et les lycéens du Congo.<sep> Comment ces outils linguistiques manifestent-ils la cohésion textuelle ?<sep> Pour apprécier la façon dont certains marqueurs de cohésion organisent la cohérence discursive, une réponse contextualisée et dynamique est apportée en trois parties.<sep> Ainsi dans la première partie, une présentation des théories, du contexte et de la méthodologie est faite.<sep> Trois chapitres organisent la réflexion à ce niveau.<sep> Le premier chapitre porte essentiellement sur les types d'enseignements reçus par les élèves.<sep> Le troisième chapitre présente la méthodologie de notre recherche.<sep> Plusieurs opérations sont identifiées à ce niveau.<sep> La deuxième partie porte sur "les approches anaphoriques".<sep> Dans le chapitre 4, une lecture des anaphores pronominales et menée.<sep> Dans le chapitre 5, ce sont les anaphores lexicales qui sont étudiées.<sep> A partir des de la théorisation de Jean Michel Adam (2008), Georges Kleiber (1994), les usages des anaphores fidèles, infidèles, hypéronymiques, résomptives et associatives sont décrits et commentés.<sep> Deux chapitres permettent d'aborder les phénomènes saillants.<sep> Le chapitre 6 aborde les marqueurs d'énumération et de conclusion.<sep> Dans le chapitre 7, le réflexion porte sur les connecteurs de justification et d'opposition.<sep> ces deux types de connecteurs sont très employés dans les copies des apprenants congolais.
L'acquisition automatique de connaissances a partir de textes consiste, idealement, a generer une representation structuree d'un corpus fourni en entree a un systeme informatique.<sep> Elle consiste en une vue structuree des deux grandes familles d'approches du probleme de l'acquisition de connaissances : l'extraction automatique de terminologie, et l'acquisition de connaissances par projection de modeles conceptuels.<sep> Une seconde partie etudie les fondement souvent implicites du traitement automatique des langues, c'est-a-dire le positivisme logique et la semantique lexicale componentielle.<sep> En guise d'alternative a la componentialite, nous proposons une semantique du signe, de l'usage et de la reference inspiree de charles sanders peirce, de ludwig wittgenstein et de georges kleiber.<sep> Dans la troisieme partie, il est procede a l'analyse semantique referentielle d'un corpus de textes medicaux.<sep> Nous y definissons deux types de reference : la denomination et la denotation.<sep> La premiere consiste en une reference arbitraire, preconstruite, et opaque ; la seconde, en une reference discursive, construite, et transparente.<sep> Dans la quatrieme partie, nous construisons manuellement une representation detaillee d'un fragment du corpus afin d'examiner la pertinence pratique de l'analyse theorique, et de fixer des objectifs precis au systeme.<sep> Enfin, la cinquieme partie est consacree a la construction aussi automatisee que possible d'une base de connaissances terminologiques capable de representer un corpus de textes techniques ou scientifiques, et qui soit suffisamment structuree pour permettre des usages applicatifs par exemple en terminologie ou en modelisation de domaines.<sep> En somme, ce travail examine le probleme de l'acquisition automatique de connaissances en liant intimement la theorie et la pratique, la finalite technologique donnant une ligne directrice aux discussions theoriques.
Ce travail suit un modèle multipapier dans lequel chaque étude contribue à répondre à la question de recherche principale.<sep> Notre première étude (chapitre 2) nous a permis de comprendre que les pressions institutionnelles ont une influence sur la décision mentale d'adopter ou pas une VS.<sep> Les chapitres 3 et 4 ont contribué à essayer et à améliorer une méthode de ciblage permettant d'identifier les besoins en information pour la VS.<sep> En outre, ces améliorations ont permis l'introduction de deux nouveaux concepts pour aider les praticiens à identifier leurs priorités pour faire de la VS.<sep> Dans le chapitre 5, nous avons étudié les interactions dans les réunions collectives de ciblage afin de comprendre la contribution de ces activités au développement de la capacité d'absorption organisationnelle.<sep> Nos travaux ont permis d'identifier les thèmes à négocier afin de faciliter l'activité de ciblage et de produire des résultats qui représentent les besoins d'information de l'organisation dans son ensemble.<sep> Enfin, nous présentons nos contributions théoriques, à la recherche, et à la pratique.
Ces dernières années, les réseaux neuronaux profonds ont été largement utilisés pour aborder l'apprentissage des modèles séquence à séquence (S2S).<sep> Les modèles les plus courants sont composés d'un encodeur qui lit la séquence d'entrée complète avant que le décodeur ne produise la séquence de sortie correspondante.<sep> Ceci implique une latence égale à la longueur de la séquence d'entrée.<sep> Une telle approche ne peut être utilisée dans un scénario véritablement interactif, en particulier par une personne handicapée utilisant un TTS pour communiquer oralement.<sep> L'objectif de ce projet est de développer une méthodologie générale pour réaliser un mapping séquence-à-séquence de façon incrémentale, avec application à la synthèse et à la restauration de la parole.
Cette thèse a pour objet une étude lexicologique du Livre des animaux d'al-Gâhiz, dans une perspective diachroniques visant à tracer l'évolution du lexique verbale de l'arabe.<sep> Nous avons adopté pour cette étude une approche comparative entre deux états de langue, l'arabe médiéval représentée par le livre des animaux et l'arabe moderne représentée par Arabic-corpus.<sep> Nous avons, dans le cadre de cette thèse, ciblé notre étude sur les lexèmes verbaux, ils représentent en effet des unités lexicales éminemment relationnelles, nous avons démontré leurs comportements variationnels de polysémie, aussi bien en synchronie qu'en diachronie.<sep> Le sémantisme des verbes est déterminé par des facteurs sémantico-syntaxiques, relationnels acquérant de la valeur différentielle selon la construction.<sep> Nous avons adopté un cadre théorique varié mais basé essentiellement sur la théorie des tropes (métaphore, métonymie), ainsi que sur le Schéma d'arguments élaboré par (J. Dichy : 1999-2000).<sep> Ceci consiste à préciser les environnements possibles pour chaque mot, en spécifiant les constructions syntaxiques et en indiquant la nature sémantique des environnements à travers les traits spécificateurs.<sep> Nous illustrons les implications d'une telle approche sur la base des changements qu'ont subis en diachronie les verbes étudiés, l'objectif étant de démontrer comment interagissent les changements de constructions d'un verbe en particulier dans la création du sens.<sep> Ainsi que de souligner les régularités dans le changement du lexique verbal arabe.<sep> L'étude des variations lexicales ou morphosyntaxiques dans les textes étudiés, nous permet d'apprécier les régularités qui relèvent en propre du commun de la langue de l'époque médiévale, de la langue moderne, et d'autres régularités qui relèvent quant à elles de l'individuel.<sep> Dans une perspective visant en second lieu, à cerner quelques aspects du langage d'al-Gâhiz et de rendre compte de sa dynamique créative.
Les modèles linéaires latents sont des modèles statistique puissants pour extraire la structure latente utile à partir de données non structurées par ailleurs. Ces modèles sont utiles dans de nombreuses applications telles que le traitement automatique du langage naturel et la vision artificielle.<sep> Pourtant, l'estimation et l'inférence sont souvent impossibles en temps polynomial pour de nombreux modèles linéaires latents et on doit utiliser des méthodes approximatives pour lesquelles il est difficile de récupérer les paramètres.<sep> Plusieurs approches, introduites récemment, utilisent la méthode des moments.<sep> Elles permettent de retrouver les paramètres dans le cadre idéalisé d'un échantillon de données infini tiré selon certains modèles, mais ils viennent souvent avec des garanties théoriques dans les cas où ce n'est pas exactement satisfait.<sep> Dans cette thèse, nous nous concentrons sur les méthodes d'estimation fondées sur l'appariement de moment pour différents modèles linéaires latents.<sep> L'utilisation d'un lien étroit avec l'analyse en composantes indépendantes, qui est un outil bien étudié par la communauté du traitement du signal, nous présentons plusieurs modèles semiparamétriques pour la modélisation thématique et dans un contexte multi-vues. Nous présentons des méthodes à base de moment ainsi que des algorithmes pour l'estimation dans ces modèles.<sep> Nous prouvons pour ces méthodes des résultats de complexité améliorée par rapport aux méthodes existantes.<sep> Nous donnons également des garanties d'identifiabilité, contrairement à d'autres modèles actuels. C'est une propriété importante pour assurer leur interprétabilité.
Dans le cadre de la recherche pharmaceutique, les propriétés relatives à l'Absorption, la Distribution, le Métabolisme, l'Elimination (ADME) et la Toxicité (Tox) sont cruciales pour le succès des phases cliniques lors de la conception de nouveaux médicaments.<sep> Durant ce processus, la chémoinformatique est régulièrement utilisée afin de prédire le profil ADME-Tox des molécules bioactives et d'améliorer leurs propriétés pharmacocinétiques.<sep> Au cours de cette thèse, nous avons d'abord constitué une base de données contenant 150 000 mesures pour une cinquantaine de propriétés ADME-Tox.<sep> Afin de valoriser l'ensemble de ces données, nous avons dans un deuxième temps proposé une plateforme automatique de création de modèles de prédiction QSAR.<sep> Cette plateforme, nommée MetaPredict, a été conçue afin d'optimiser chacune des étapes de création d'un modèle statistique, dans le but d'améliorer leur qualité et leur robustesse.<sep> Nous avons dans un troisième temps valorisé les modèles obtenus grâce à la plateforme MetaPredict en proposant une application en ligne.<sep> Cette application a été développée pour faciliter l'utilisation des modèles, apporter une interprétation simplifiée des résultats et moduler les observations obtenues en fonction des spécificités d'un projet de recherche.<sep> Finalement, MetaPredict permet de rendre les modèles ADME-Tox accessibles à l'ensemble des chercheurs.
Les systèmes actuels de synthèse vocale sont basés sur la technologie dite de synthèse par corpus.<sep> Celle-ci repose sur la sélection d'une séquence optimale d'unités acoustiques au sens du contexte de synthèse.<sep> Cette approche qui minimise l'effort de concaténation conduit à une restitution jugée naturelle mais uniquement pour un style de parole lue.<sep> L'acceptabilité réelle d'une brique technologique de synthèse vocale dépend cependant de la réponse à deux attentes principales : la capacité du système à restituer d'une part des formes expressives et d'autre part des qualités de voix différentes.<sep> Pour satisfaire ce double objectif, un travail de caractérisation des signaux de parole est nécessaire.<sep> Cette thèse traite de la prise en compte explicite des mécanismes de production de la parole en synthèse.<sep> Dans une première partie, nous nous intéressons à la décomposition d'un signal de parole en une composante de source – l'onde de débit glottique (ODG) produite lors de la vibration des cordes vocales – et une composante filtre caractérisant le conduit vocal.<sep> Pour résoudre ce problème de déconvolution, nous proposons un modèle ARX-LF qui consiste à introduire, dans un processus linéaire de production de la parole, de l'information a priori sur l'ODG en utilisant un modèle LF (Liljencrants Fant).<sep> L'estimation des paramètres du modèle ARX-LF selon un critère des moindres carrés résulte en un problème d'optimisation non-linéaire complexe.<sep> Nous introduisons donc une solution efficace basée sur un découplage de l'estimation des paramètres et sur de nombreuses optimisations algorithmiques.<sep> Les résultats d'estimation sont très encourageants.<sep> D'une part, la méthode d'inversion proposée conduit à une meilleure estimation des instants de fermeture que les méthodes existantes.<sep> D'autre part, les ODG estimées ont pu être corroborées par des mesures électroglottographiques.<sep> Dans une seconde partie, nous avons proposé une méthode de synthèse et de modification de signaux de parole basée sur le modèle ARX-LF.<sep> Nous nous sommes particulièrement attachés à la modélisation de la composante résiduelle et avons introduit une nouvelle méthode de contrôle explicite de l'enveloppe temporelle du résidu lors de la modification de signaux de parole.<sep> Des résultats en modification de durée et de fréquence fondamentale permettent de comparer favorablement la méthode proposée aux techniques existantes.
Dans cette thèse, nous abordons la réalisation d'un outil innovant d'aide à la traduction anglais/arabe pour répondre au besoin croissant en termes d'outils en ligne d'aide à la traduction centrés sur la langue arabe.<sep> Cet outil combine des dictionnaires adaptés aux spécificités de la langue arabe et un concordancier bilingue issu des corpus parallèles.<sep> Compte tenu de sa nature agglutinante et non voyellée, le mot arabe nécessite un traitement spécifique.<sep> C'est pourquoi, et pour construire nos ressources lexicales, nous nous sommes basés sur l'analyseur morphologique de Buckwalter qui, d'une part, permet une analyse morphologique en tenant compte de la composition complexe du mot arabe (proclitique, préfixe, radical, suffixe, enclitique), et qui, d'autre part, fournit des ressources traductionnelles permettant une réadaptation au sein d'un système de traduction.<sep> Par ailleurs, cet analyseur morphologique est compatible avec l'approche définie autour de la base de données DIINAR (DIctionnaire Informatisé de l'Arabe), qui a été construite, entre autres, par des membres de notre équipe de recherche.<sep> Pour répondre à la problématique du contexte dans la traduction, un concordancier bilingue a été développé à partir des corpus parallèles<sep> Ces derniers représentent une ressource linguistique très intéressante et ayant des usages multiples, en l'occurrence l'aide à la traduction.<sep> Nous avons donc étudié de près ces corpus, leurs méthodes d'alignement, et nous avons proposé une approche mixte qui améliore significativement la qualité d'alignement sous-phrastique des corpus parallèles anglais-arabes.<sep> Plusieurs technologies informatiques ont été utilisées pour la mise en œuvre de cet outil d'aide à la traduction qui est disponible en ligne (tarjamaan.com), et qui permet à l'utilisateur de chercher la traduction de millions de mots et d'expressions tout en visualisant leurs contextes originaux.<sep> Une évaluation de cet outil a été faite en vue de son optimisation et de son élargissement pour prendre en charge d'autres paires de langues.
Dans de nombreux domaines, les données peuvent être de grande dimension.<sep> Ça pose le problème de la réduction de dimension.<sep> Les techniques de réduction de dimension peuvent être classées en fonction de leur but : techniques pour la représentation optimale et techniques pour la classification, ainsi qu'en fonction de leur stratégie : la sélection et l'extraction des caractéristiques.<sep> L'ensemble des caractéristiques résultant des méthodes d'extraction est non interprétable.<sep> Ainsi, la première problématique scientifique de la thèse est comment extraire des caractéristiques latentes interprétables ?<sep> La réduction de dimension pour la classification vise à améliorer la puissance de classification du sous-ensemble sélectionné.<sep> Nous voyons le développement de la tâche de classification comme la tâche d'identification des facteurs déclencheurs, c'est-à-dire des facteurs qui peuvent influencer le transfert d'éléments de données d'une classe à l'autre.<sep> La deuxième problématique scientifique de cette thèse est comment identifier automatiquement ces facteurs déclencheurs ?<sep> Nous visons à résoudre les deux problématiques scientifiques dans le domaine d'application des systèmes de recommandation.<sep> Nous proposons d'interpréter les caractéristiques latentes de systèmes de recommandation basés sur la factorisation de matrices comme des utilisateurs réels.<sep> Nous concevons un algorithme d'identification automatique des facteurs déclencheurs basé sur les concepts d'analyse par contraste.<sep> Au travers d'expérimentations, nous montrons que les motifs définis peuvent être considérés comme des facteurs déclencheurs