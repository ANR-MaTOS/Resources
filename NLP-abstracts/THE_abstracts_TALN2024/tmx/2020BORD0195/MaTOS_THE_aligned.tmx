<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020BORD0195. segId begin by 1, tuid = segId</note>
        <docid>2020BORD0195</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>La mémoire de travail peut être définie comme la capacité à stocker temporairement et à manipuler des informations de toute nature.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Working memory can be defined as the ability to temporarily store and manipulate information of any kind.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Par exemple, imaginez que l'on vous demande d'additionner mentalement une série de nombres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For example, imagine that you are asked to mentally add a series of numbers.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Afin de réaliser cette tâche, vous devez garder une trace de la somme partielle qui doit être mise à jour à chaque fois qu'un nouveau nombre est donné.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In order to accomplish this task, you need to keep track of the partial sum that needs to be updated every time a new number is given.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>La mémoire de travail est précisément ce qui permettrait de maintenir (i.e. stocker temporairement) la somme partielle et de la mettre à jour (i.e. manipuler).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The working memory is precisely what would make it possible to maintain (i.e. temporarily store) the partial sum and to update it (i.e. manipulate).</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Dans cette thèse, nous proposons d'explorer les implémentations neuronales de cette mémoire de travail en utilisant un nombre restreint d'hypothèses.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this thesis, we propose to explore the neuronal implementations of this working memory using a limited number of hypotheses.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Pour ce faire, nous nous plaçons dans le contexte général des réseaux de neurones récurrents et nous proposons d'utiliser en particulier le paradigme du reservoir computing.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To do this, we place ourselves in the general context of recurrent neural networks and we propose to use in particular the reservoir computing paradigm.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Ce type de modèle très simple permet néanmoins de produire des dynamiques dont l'apprentissage peut tirer parti pour résoudre une tâche donnée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This type of very simple model nevertheless makes it possible to produce dynamics that learning can take advantage of to solve a given task.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Dans ce travail, la tâche à réaliser est une mémoire de travail à porte (gated working memory).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this job, the task to be performed is a gated working memory task.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Le modèle reçoit en entrée un signal qui contrôle la mise à jour de la mémoire.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The model receives as input a signal which controls the update of the memory.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Lorsque la porte est fermée, le modèle doit maintenir son état de mémoire actuel, alors que lorsqu'elle est ouverte, il doit la mettre à jour en fonction d'une entrée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>When the door is closed, the model should maintain its current memory state, while when open, it should update it based on an input.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Dans notre approche, cette entrée supplémentaire est présente à tout instant, même lorsqu'il n'y a pas de mise à jour à faire.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In our approach, this additional input is present at all times, even when there is no update to do.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>En d'autres termes, nous exigeons que notre modèle soit un système ouvert, i.e. un système qui est toujours perturbé par ses entrées mais qui doit néanmoins apprendre à conserver une mémoire stable.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In other words, we require our model to be an open system, i.e. a system which is always disturbed by its inputs but which must nevertheless learn to keep a stable memory.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Dans la première partie de ce travail, nous présentons l'architecture du modèle et ses propriétés, puis nous montrons sa robustesse au travers d'une étude de sensibilité aux paramètres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the first part of this work, we present the architecture of the model and its properties, then we show its robustness through a parameter sensitivity study.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Celle-ci montre que le modèle est extrêmement robuste pour une large gamme de paramètres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This shows that the model is extremely robust for a wide range of parameters.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Peu ou prou, toute population aléatoire de neurones peut être utilisée pour effectuer le gating.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>More or less, any random population of neurons can be used to perform gating.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Par ailleurs, après apprentissage, nous mettons en évidence une propriété intéressante du modèle, à savoir qu'une information peut être maintenue de manière entièrement distribuée, i.e. sans être corrélée à aucun des neurones mais seulement à la dynamique du groupe.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Furthermore, after learning, we highlight an interesting property of the model, namely that information can be maintained in a fully distributed manner, i.e. without being correlated to any of the neurons but only to the dynamics of the group.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Plus précisément, la mémoire de travail n'est pas corrélée avec l'activité soutenue des neurones ce qui a pourtant longtemps été observé dans la littérature et remis en cause récemment de façon expérimentale.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>More precisely, working memory is not correlated with the sustained activity of neurons, which has nevertheless been observed for a long time in the literature and recently questioned experimentally.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>Ce modèle vient confirmer ces résultats au niveau théorique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This model confirms these results at the theoretical level.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>Dans la deuxième partie de ce travail, nous montrons comment ces modèles obtenus par apprentissage peuvent être étendus afin de manipuler l'information qui se trouve dans l'espace latent.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the second part of this work, we show how these models obtained by learning can be extended in order to manipulate the information which is in the latent space.</seg>
            </tuv>
        </tu>
        <tu tuid="20">
            <tuv xml:lang="FR">
                <seg>Nous proposons pour cela de considérer les conceptors qui peuvent être conceptualisé comme un jeu de poids synaptiques venant contraindre la dynamique du réservoir et la diriger vers des sous-espaces particuliers ; par exemple des sous-espaces correspondants au maintien d'une valeur particulière.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We therefore propose to consider conceptors which can be conceptualized as a set of synaptic weights which constrain the dynamics of the reservoir and direct it towards particular subspaces; for example subspaces corresponding to the maintenance of a particular value.</seg>
            </tuv>
        </tu>
        <tu tuid="21">
            <tuv xml:lang="FR">
                <seg>Plus généralement, nous montrons que ces conceptors peuvent non seulement maintenir des informations, ils peuvent aussi maintenir des fonctions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>More generally, we show that these conceptors can not only maintain information, they can also maintain functions.</seg>
            </tuv>
        </tu>
        <tu tuid="22">
            <tuv xml:lang="FR">
                <seg>Dans le cas du calcul mental évoqué précédemment, ces conceptors permettent alors de se rappeler et d'appliquer l'opération à effectuer sur les différentes entrées données au système.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the case of mental arithmetic mentioned previously, these conceptors then make it possible to remember and apply the operation to be carried out on the various inputs given to the system.</seg>
            </tuv>
        </tu>
        <tu tuid="23">
            <tuv xml:lang="FR">
                <seg>Ces conceptors permettent donc d'instancier une mémoire de type procédural en complément de la mémoire de travail de type déclaratif.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These conceptors therefore make it possible to instantiate a procedural working memory in addition to the declarative working memory.</seg>
            </tuv>
        </tu>
        <tu tuid="24">
            <tuv xml:lang="FR">
                <seg>Nous concluons ce travail en remettant en perspective ce modèle théorique vis à vis de la biologie et des neurosciences.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We conclude this work by putting this theoretical model into perspective with respect to biology and neurosciences.</seg>
            </tuv>
        </tu>
    </body>
</tmx>