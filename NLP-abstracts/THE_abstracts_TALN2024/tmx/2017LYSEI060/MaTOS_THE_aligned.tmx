<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2017LYSEI060. segId begin by 1, tuid = segId</note>
        <docid>2017LYSEI060</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>L'estimation de la pose humaine et la reconnaissance des activités humaines sont des étapes importantes dans de nombreuses applications comme la robotique, la surveillance et la sécurité, etc.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Estimating human pose and recognizing human activities are important steps in many applications, such as human computer interfaces (HCI), health care, smart conferencing, robotics, security surveillance etc.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Actuellement abordées dans le domaine, ces tâches ne sont toujours pas résolues dans des environnements non-coopératifs particulièrement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Despite the ongoing effort in the domain, these tasks remained unsolved in unconstrained and non cooperative environments in particular.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Dans un premier temps, nous nous sommes concentrés sur la reconnaissance des actions complexes depuis des vidéos.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>As a second problem, we tackled the human pose estimation task in particular settings where multiple visual sensors are available and allowed to collaborate.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Pour ceci, nous avons introduit une représentation spatio-temporelle indépendante du point de vue.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the first part, we focused on indoor action recognition from videos and we consider complex activities.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Plus précisément, nous avons capturé le mouvement de la personne en utilisant un capteur de profondeur et l'avons encodé en 3D pour le représenter.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To this end, we explored several methodologies and eventually introduced a 3D spatio-temporal representation for a video sequence that is viewpoint independent.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Un descripteur 3D a ensuite été utilisé pour la classification des séquences avec la méthodologie bag-of-words.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A 3D feature descriptor was employed afterwards to build a codebook and classify the actions with the bag-of-words approach.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Pour la deuxième partie, notre objectif était l'estimation de pose articulée, qui est souvent une étape intermédiaire pour la reconnaissance de l'activité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>As for the second part, we concentrated on articulated pose estimation, which is often an intermediate step for activity recognition.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Notre motivation était d'incorporer des informations à partir de capteurs multiples et de les fusionner pour surmonter le problème de l'auto-occlusion.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our motivation was to incorporate information from multiple sources and views and fuse them early in the pipeline to overcome the problem of self-occlusion, and eventually obtain robust estimations.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Nous avons démontré que les contraintes géométriques et les paramètres de cohérence d'apparence sont efficaces pour renforcer la cohérence entre les points de vue, aussi que les paramètres classiques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In addition to the single-view appearance of the human body and its kinematic priors, we demonstrated that geometrical constraints and appearance-consistency parameters are effective for boosting the coherence between the viewpoints in a multi-view setting.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Finalement, nous avons évalué ces nouvelles méthodes sur des datasets publics, qui vérifie que l'utilisation de représentations indépendantes de la vue et l'intégration d'informations à partir de points de vue multiples améliore la performance pour les tâches ciblées dans le cadre de cette manuscrit.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Both methods that we proposed was evaluated on public benchmarks and showed that the use of view-independent representations and integrating information from multiple viewpoints improves the performance of action recognition and pose estimation tasks, respectively.</seg>
            </tuv>
        </tu>
    </body>
</tmx>