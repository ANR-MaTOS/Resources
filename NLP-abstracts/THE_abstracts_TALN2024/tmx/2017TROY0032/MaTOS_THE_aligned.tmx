<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2017TROY0032. segId begin by 1, tuid = segId</note>
        <docid>2017TROY0032</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Cette thèse porte sur l’étude de méthodes aléatoires pour l’apprentissage de données en grande dimension.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis deals with the study of random methods for learning large-scale data.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Nous proposons d'abord une approche non supervisée consistant en l'estimation des composantes principales, lorsque la taille de l'échantillon et la dimension de l'observation tendent vers l'infini.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Firstly, we propose an unsupervised approach consisting in the estimation of the principal components, when the sample size and the observation dimension tend towards infinity.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Cette approche est basée sur les matrices aléatoires et utilise des estimateurs consistants de valeurs propres et vecteurs propres de la matrice de covariance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This approach is based on random matrices and uses consistent estimators of eigenvalues and eigenvectors of the covariance matrix.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Ensuite, dans le cadre de l’apprentissage supervisé, nous proposons une approche qui consiste à, d'abord réduire la dimension grâce à une approximation de la matrice de données originale, et ensuite réaliser une LDA dans l’espace réduit.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Then, in the case of supervised learning, we propose an approach which consists in reducing the dimension by an approximation of the original data matrix and then realizing LDA in the reduced space.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>La réduction de dimension est basée sur l’approximation de matrices de rang faible par l’utilisation de matrices aléatoires.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Dimension reduction is based on low–rank approximation matrices by the use of random matrices.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Un algorithme d'approximation rapide de la SVD, puis une version modifiée permettant l’approximation rapide par saut spectral sont développés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A fast approximation algorithm of the SVD and a modified version as fast approximation by spectral gap are developed.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Les approches sont appliquées à des données réelles images et textes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Experiments are done with real images and text data.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Elles permettent, par rapport à d’autres méthodes, d’obtenir un taux d’erreur assez souvent optimal, avec un temps de calcul réduit.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Compared to other methods, the proposed approaches provide an error rate that is often optimal, with a small computation time.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Enfin, dans le cadre de l’apprentissage par transfert, notre contribution consiste en l’utilisation de l'alignement des sous-espaces caractéristiques et l’approximation de matrices de rang faible par projections aléatoires.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, our contribution in transfer learning consists in the use of the subspace alignment and the low-rank approximation of matrices by random projections.</seg>
            </tuv>
        </tu>
    </body>
</tmx>