<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2016LORR0350. segId begin by 1, tuid = segId</note>
        <docid>2016LORR0350</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Ce document propose d’apprendre le comportement d’un système à partir d’un ensemble de dialogues annotés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This document proposes to learn the behaviour of the dialogue manager of a spoken dialogue system from a set of rated dialogues.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Le système apprend un comportement optimal via l’apprentissage par renforcement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This learning is performed through reinforcement learning.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Nous montrons qu’il n’est pas nécessaire de définir une représentation de l’espace d’état ni une fonction de récompense.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our method does not require the definition of a representation of the state space nor a reward function.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>En effet, ces deux paramètres peuvent être appris à partir du corpus de dialogues annotés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These two high-level parameters are learnt from the corpus of rated dialogues.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Nous montrons qu’il est possible pour un développeur de systèmes de dialogue d’optimiser la gestion du dialogue en définissant seulement la logique du dialogue ainsi qu’un critère à maximiser (par exemple, la satisfaction utilisateur).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It is shown that the spoken dialogue designer can optimise dialogue management by simply defining the dialogue logic and a criterion to maximise (e.g user satisfaction).</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>La première étape de la méthodologie que nous proposons consiste à prendre en compte un certain nombre de paramètres de dialogue afin de construire une représentation de l’espace d’état permettant d’optimiser le critère spécifié par le développeur.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The methodology suggested in this thesis first considers the dialogue parameters that are necessary to compute a representation of the state space relevant for the criterion to be maximized.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Par exemple, si le critère choisi est la satisfaction utilisateur, il est alors important d’inclure dans la représentation des paramètres tels que la durée du dialogue et le score de confiance de la reconnaissance vocale.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For instance, if the chosen criterion is user satisfaction then it is important to account for parameters such as dialogue duration and the average speech recognition confidence score.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>L’espace d’état est modélisé par une mémoire sparse distribuée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The state space is represented as a sparse distributed memory.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Notre modèle, Genetic Sparse Distributed Memory for Reinforcement Learning (GSDMRL), permet de prendre en compte de nombreux paramètres de dialogue et de sélectionner ceux qui sont importants pour l’apprentissage par évolution génétique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The Genetic Sparse Distributed Memory for Reinforcement Learning (GSDMRL) accommodates many dialogue parameters and selects the parameters which are the most important for learning through genetic evolution.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>L’espace d’état résultant ainsi que le comportement appris par le système sont aisément interprétables.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The resulting state space and the policy learnt on it are easily interpretable by the system designer.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Ces deux méthodes interprètent le critère à optimiser comme étant la récompense globale pour chaque dialogue.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These two algorithms consider the criterion to be the return for the entire dialogue.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Nous comparons ces deux fonctions sur un ensemble de dialogues simulés et nous montrons que l’apprentissage est plus rapide avec ces fonctions qu’en utilisant directement le critère comme récompense finale.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These functions are discussed and compared on simulated dialogues and it is shown that the resulting functions enable faster learning than using the criterion directly as the final reward.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Nous avons développé un système de dialogue dédié à la prise de rendez-vous et nous avons collecté un corpus de dialogues annotés avec ce système.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A spoken dialogue system for appointment scheduling was designed during this thesis, based on previous systems, and a corpus of rated dialogues with this system were collected.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Ce corpus permet d’illustrer la capacité de mise à l’échelle de la représentation de l’espace d’état GSDMRL et constitue un bon exemple de système industriel sur lequel la méthodologie que nous proposons pourrait être appliquée</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This corpus illustrates the scaling capability of the state space representation and is a good example of an industrial spoken dialogue system upon which the methodology could be applied</seg>
            </tuv>
        </tu>
    </body>
</tmx>