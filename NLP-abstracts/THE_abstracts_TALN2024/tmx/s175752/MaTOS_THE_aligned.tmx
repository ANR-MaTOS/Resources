<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-s175752. segId begin by 1, tuid = segId</note>
        <docid>s175752</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Les modèles graphiques probabilistes codent les dépendances cachées entre les variables aléatoires pour la modélisation des données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Probabilistic graphical models encode the hidden dependencies between random variables for data modelling.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>L'estimation des paramètres est une partie cruciale et nécessaire du traitement de ces modèles probabilistes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Parameter estimation is a crucial and necessary part of handling such probabilistic models.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Ces modèles très généraux ont été utilisés dans de nombreux domaines tels que la vision par ordinateur, le traitement du signal, le traitement du langage naturel et bien d'autres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These very general models have been used in plenty of fields such as computer vision, signal processing, natural language processing and many more.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Nous nous sommes surtout concentrés sur les modèles log-supermodulaires, qui constituent une partie spécifique des distributions familiales exponentielles, où la fonction potentielle est supposée être négative d'une fonction sous-modulaire.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We mostly focused on log-supermodular models, which is a specific part of exponential family distributions, where the potential function is assumed to be negative of a submodular function.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Cette propriété sera très pratique pour le maximum d'estimations a posteriori et d'apprentissage des paramètres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This property will be very handy for the maximum a posteriori and parameter learning estimations.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Malgré la restriction apparente des modèles d'intérêt, ils couvrent une grande partie des familles exponentielles, puisqu'il y a beaucoup de fonctions qui sont sous-modulaires, par exemple, les coupes graphiques, l'entropie et autres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Despite the seemed restriction of the models of interest, they cover a broad part of exponential families, since there are plenty of functions that are submodular, e.g., graph cuts, entropy and others.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Il est bien connu que le traitement probabiliste est un défi pour la plupart des modèles, mais nous avons été en mesure de relever certains des défis au moins approximativement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It is well known that probabilistic treatment is a challenging way for most of all models, however we were able to tackle some of the challenges at least approximately.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Dans ce manuscrit, nous exploitons les idées perturb-and-MAP pour l'approximation de la fonction de partition et donc un apprentissage efficace des paramètres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this manuscript, we exploit the perturb-and-MAP ideas for the partition function approximation and thus an efficient parameter learning.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>De plus, le problème peut également être interprété comme une tâche d'apprentissage de structure, où chaque paramètre ou poids estimé représente l'importance du terme correspondant.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Moreover, the problem can be also interpreted as a structure learning task, where each estimated parameter or weight represents the importance of the corresponding term.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Nous proposons une méthode d'estimation et d'inférence approximative des paramètres pour les modèles où l'apprentissage et l'inférence exacts sont insolubles dans le cas général en raison de la complexité du calcul des fonctions de partition.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We propose a way of approximate parameter estimation and inference for the models where exact learning and inference is intractable in general case due to the partition function calculation complexity.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>La première partie de la thèse est consacrée aux garanties théoriques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first part of the thesis is dedicated to theoretical guarantees.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Étant donné les modèles log-supermodulaires, nous tirons parti de la propriété de minimisation efficace liée à la sous-modularité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Given the log-supermodular models, we take advantage of the efficient minimization property related to submodularity.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>En introduisant et en comparant deux limites supérieures existantes de la fonction de partition, nous sommes en mesure de démontrer leur relation en prouvant un résultat théorique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Introducing and comparing two existing upper bounds of the partition function, we are able to demonstrate their relation by proving a theoretical result.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Nous introduisons une approche pour les données manquantes comme sous-routine naturelle de la modélisation probabiliste.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We introduce an approach for missing data as a natural subroutine of probabilistic modelling.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Il semble que nous puissions appliquer une technique stochastique à l'approche d'approximation par perturbation et carte proposée tout en maintenant la convergence tout en la rendant plus rapide dans la pratique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It appears that we can apply a stochastic technique over proposed perturb-and-map approximation approach and still maintain convergence while make it faster in practice.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>La deuxième contribution principale de cette thèse est une généralisation efficace et évolutive de l'approche de l'apprentissage paramétrique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second main contribution of this thesis is an efficient and scalable generalization of the parameter learning approach.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Dans cette section, nous développons de nouveaux algorithmes pour effectuer l'estimation des paramètres pour diverses fonctions de perte, différents niveaux de supervision et nous travaillons également sur l'évolutivité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this section we develop new algorithms to perform the parameter estimation for various loss functions, different levels of supervision and we also work on the scalability.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>En particulier, en travaillant principalement avec des coupes graphiques, nous avons pu intégrer différentes techniques d'accélération.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In particular, working with mostly graph cuts, we were able to incorporate various acceleration techniques.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>Nous traitons d'un problème général d'apprentissage des signaux continus.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>As a third contribution we deal with a general problem of learning continuous signals.</seg>
            </tuv>
        </tu>
        <tu tuid="20">
            <tuv xml:lang="FR">
                <seg>Dans cette partie, nous nous concentrons sur les représentations de modèles graphiques clairsemés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this part, we focus on the sparse graphical models representations.</seg>
            </tuv>
        </tu>
        <tu tuid="21">
            <tuv xml:lang="FR">
                <seg>Nous utilisons des régularisateurs à faible densité commune comme potentiels basés sur les antécédents.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We consider common sparsity-inducing regularizers as prior-based potentials.</seg>
            </tuv>
        </tu>
        <tu tuid="22">
            <tuv xml:lang="FR">
                <seg>Les techniques de débruitage proposées ne nécessitent pas le choix d'un redresseur précis à l'avance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The proposed denoising techniques do not require choosing any precise regularizer in advance.</seg>
            </tuv>
        </tu>
        <tu tuid="23">
            <tuv xml:lang="FR">
                <seg>Pour effectuer l'apprentissage de la représentation clairsemée, la communauté utilise souvent les pertes symétriques comme l1, mais nous proposons de paramétrer la perte et d'apprendre le poids de chaque composante de perte à partir des données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To perform sparse representation learning, community often use symmetric losses as l1, but we propose to parameterize the loss and learn the weight of each loss component from the data.</seg>
            </tuv>
        </tu>
        <tu tuid="24">
            <tuv xml:lang="FR">
                <seg>C'est possible grâce à l'approche que nous avons proposée dans les sections précédentes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This is feasible via the approach we proposed in the previous sections.</seg>
            </tuv>
        </tu>
        <tu tuid="25">
            <tuv xml:lang="FR">
                <seg>Pour tous les aspects de l'estimation des paramètres mentionnés ci-dessus, nous avons effectué les calculs suivants des expériences nationales visant à approuver l'idée ou à la comparer à des repères existants, et à démontrer sa performance dans la pratique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For all sides of the parameter estimation mentioned above we performed computational experiments to approve the idea or compare with existing benchmarks, and demonstrate its performance in practice.</seg>
            </tuv>
        </tu>
    </body>
</tmx>