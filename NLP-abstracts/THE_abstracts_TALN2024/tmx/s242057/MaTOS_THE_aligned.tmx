<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-s242057. segId begin by 1, tuid = segId</note>
        <docid>s242057</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Cette thèse sera consacrée à l'étude d'utilisation du traitement de langage naturel en finance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis will be devoted to the use of natural language processing methods in finance.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Nous étudierons d'abord les méthodes d'analyse du sentiment dans le cadre d'application sur les nouvelles financières.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We will first study sentiment analysis methods with the aim of applying them on financial news.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Avant l'ère d'intelligence artificielle, les méthodes feature engineering et statistiques étaient les outils dominant dans ce domaine, par exemple, Naive Bayse Classifier et TF-IDF.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Before the era of artificial intelligence, feature engineering and statistical approaches were the dominant tools in this area, e.g., Naïve Bayes Classifier, TF-IDF.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Cependant, la plupart de ces méthodes sont basées sur les embedding statiques et elles ignorent les informations contextuelles dans les corpus.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However, most of these approaches are based on static embeddings and ignore contextual information in a sentence.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Par conséquent, nous allons étudier les améliorations en précision par adopter les embeddings contextualisés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Hence, we are going to study possible improvements in accuracy by introducing contextualized embeddings into the models.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Nous étudierons aussi les méthodes qui nous permettent de générer les embeddings contextualisés, soit par définir et entraîner un modèle de zéro, soit par fine-tuner une modèle existante, par exemple, BERT ou XLNet.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We will also methods enabling us to generate contextualized embedding, either by defining and training an embedding model from scratch or fine-tuning an existing model, such as BERT or XLNet.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Nous allons ensuite focaliser sur l'analyse du sentiment sur les textes longs.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We will then focus on sentiment analysis on long texts.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>En finance, il y a de différents types de documents longs, par exemple, les rapports annuel et les transcriptions de conférence.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In finance, there are different types of such documents, for example annual reports or conference transcripts.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Etant donné la longueur de ces corpus, nous ne pouvons pas appliquer directement les méthodes classiques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>All these documents being much longer than a title or a summary, we cannot directly apply standard methods on these corpus.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>En conséquence, nous proposons de le résumer dans un premier temps et ensuite d'entraîner un modèle de classification.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Thus, we propose summarizing the text in the first place and then training a classification model.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Les méthodes existantes comme LDA (Latent Dirichlet Allocation) ou BERTSum fonctionnent bien sur les textes plutôt uniformes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Existing summarization methods such as, LDA (Latent Dirichlet Allocation) or BERTSum work well on somehow uniform texts.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Cependant, ils ont les performances limitées sur ces documents financiers avec les formats différents.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However they have limited performance on these financial documents with various formats and characteristics.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Nous allons investiguer la méthode transfer learning pour cette tâche.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We will investigate the transfer learning approach in this task.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Nous allons d'abord entraîner un modèle avec tous les corpus.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We will first train a general model based on all available texts.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Ensuite, pour chaque type de document, nous allons considérer un modèle plus parcimonieux avec un nombre limité de donnée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Then, for each type of corpus, we will consider a much more parsimonious model with limited amount of data.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Notre résultat final sera obtenu par agréger les sorties des deux modèles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our final result will be obtained by aggregating the outputs of the general model and the specialized model.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Finalement, nous sommes intéressés par la possibilité d'appliquer les méthodes NLP sur la tâche de réduction de dimension des séries temporelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, we will be interested in the possibility of applying NLP methods on time series dimension reduction task.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>Les méthodes NLP fréquemment utilisées comme Auto-Encodeur, Generative Adversarial Network (GAN) et Variational Auto-Encoder (VAE) contiennent toutes une partie encodeur, qui nous permet de résumer les embeddings haut-dimensionnels en plusieurs vecteurs.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Widely used methods in NLP such as Auto-encoder, Generative Adversarial Network (GAN) and Variational Auto-Encoder (VAE) all have an encoder part, which transforms high-dimensional embedding into multiple vectors as a summary.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>Par conséquent, en basant sur cette idée, nous allons chercher une structure qui prend les séries temporelles haut-dimensionnelles comme l'entrée et qui donne les séries temporelles basse-dimensionnelles comme la sortie.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Hence, based on this idea, we would like to design a model structure which takes in high-dimensional time series instead of texts and outputs a lower-dimensional time series.</seg>
            </tuv>
        </tu>
    </body>
</tmx>